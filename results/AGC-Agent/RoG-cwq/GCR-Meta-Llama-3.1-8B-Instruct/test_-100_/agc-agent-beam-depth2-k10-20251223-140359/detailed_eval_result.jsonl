{"id": "WebQTest-100_524908899a8aa334a18a0ac00f8f2fe6", "prediction": ["# Reasoning Path:\nNord-Est Department -> location.administrative_division.first_level_division_of -> Haiti -> location.country.languages_spoken -> Haitian Creole\n# Answer:\nHaiti", "# Reasoning Path:\nNord-Est Department -> location.administrative_division.first_level_division_of -> Haiti -> location.country.languages_spoken -> French\n# Answer:\nHaiti", "# Reasoning Path:\nNord-Est Department -> location.administrative_division.first_level_division_of -> Haiti -> location.country.official_language -> French\n# Answer:\nHaiti", "# Reasoning Path:\nNord-Est Department -> location.location.containedby -> Haiti -> location.country.languages_spoken -> Haitian Creole\n# Answer:\nHaiti", "# Reasoning Path:\nNord-Est Department -> location.administrative_division.first_level_division_of -> Haiti -> location.country.official_language -> Haitian Creole\n# Answer:\nHaiti"], "ground_truth": ["Haitian Creole", "French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-100_bf66fd89b6e9fc5fcb96c8b3f7a0e616", "prediction": ["# Reasoning Path:\nNord-Ouest Department -> location.administrative_division.first_level_division_of -> Haiti -> location.country.languages_spoken -> Haitian Creole\n# Answer:\nHaiti", "# Reasoning Path:\nNord-Ouest Department -> location.administrative_division.first_level_division_of -> Haiti -> location.country.languages_spoken -> French\n# Answer:\nHaiti", "# Reasoning Path:\nNord-Ouest Department -> location.administrative_division.first_level_division_of -> Haiti -> location.country.official_language -> French\n# Answer:\nHaiti", "# Reasoning Path:\nNord-Ouest Department -> location.administrative_division.country -> Haiti -> location.country.languages_spoken -> Haitian Creole\n# Answer:\nHaiti", "# Reasoning Path:\nNord-Ouest Department -> location.administrative_division.first_level_division_of -> Haiti -> location.country.official_language -> Haitian Creole\n# Answer:\nHaiti"], "ground_truth": ["Haitian Creole", "French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-100_de15ac1f762e3ec1e1261f6d9c81ebf9", "prediction": ["# Reasoning Path:\nLa Dessalinienne -> music.composition.recordings -> Haiti -> music.recording.artist -> Nicolas Geffrard\n# Answer:\nHaiti"], "ground_truth": ["Haitian Creole", "French"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1091_d3301dedbdcb2d171a75be1631e8cec5", "prediction": ["# Reasoning Path:\nLumbini -> religion.place_of_worship.religion -> Buddhism -> religion.religion.sacred_sites -> Lumbini\n# Answer:\nBuddhism", "# Reasoning Path:\nLumbini -> common.topic.notable_types -> Place of worship -> type.type.domain -> Religion\n# Answer:\nPlace of worship", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Alabama -> location.location.containedby -> United States of America\n# Answer:\nAlabama"], "ground_truth": ["Buddhism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.3333333333333333, "path_recall": 0.5, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-1119_8c41904eaef9fce2d95afd27eb9150ec", "prediction": ["# Reasoning Path:\nBentley B. Rayburn -> business.board_member.leader_of -> m.0hprvkd -> organization.leadership.person -> Bentley B. Rayburn\n# Answer:\nm.0hprvkd"], "ground_truth": ["College/University"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121_333b95bd45af3e6e31e328bc8c24d84f", "prediction": ["# Reasoning Path:\nChase Lane Primary School -> education.educational_institution.students_graduates -> m.04dyl8v -> education.education.student -> David Beckham\n# Answer:\nm.04dyl8v"], "ground_truth": ["LA Galaxy"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1251_cbf2f20f6caf754bc49d672ca7b150b7", "prediction": ["# Reasoning Path:\nEast German mark -> finance.currency.countries_formerly_used -> East Germany -> location.location.containedby -> Central and Eastern Europe\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_used -> East German mark\n# Answer:\nEast Germany", "# Reasoning Path:\nEast German mark -> finance.currency.countries_formerly_used -> East Germany -> location.country.official_language -> German Language\n# Answer:\nEast Germany", "# Reasoning Path:\nEast German mark -> finance.currency.countries_used -> East Germany -> location.location.containedby -> Central and Eastern Europe\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_formerly_used -> East German mark\n# Answer:\nEast Germany"], "ground_truth": ["East Germany"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1260_5dd0eeca79ae03b7711252c032849eb2", "prediction": ["# Reasoning Path:\nSuper bowl -> freebase.type_profile.equivalent_topic -> Super Bowl -> time.recurring_event.instances -> Super Bowl X\n# Answer:\nSuper Bowl", "# Reasoning Path:\nRise -> sports.mascot.team -> Baltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII\n# Answer:\nBaltimore Ravens", "# Reasoning Path:\nSuper bowl -> freebase.type_profile.equivalent_topic -> Super Bowl -> sports.sports_championship.events -> Super Bowl X\n# Answer:\nSuper Bowl", "# Reasoning Path:\nRise -> sports.mascot.team -> Baltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV\n# Answer:\nBaltimore Ravens"], "ground_truth": ["Super Bowl XXXV"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-12_68d745a0657c86906382873e57294d6a", "prediction": ["# Reasoning Path:\nGovernor -> base.firsts.achievement.firsts -> m.04zr93l -> base.firsts.first_achievement.category -> United States of America\n# Answer:\nm.04zr93l", "# Reasoning Path:\nOhio -> book.book_subject.works -> Dale Loves Sophie to Death -> book.written_work.subjects -> Ohio\n# Answer:\nDale Loves Sophie to Death"], "ground_truth": ["Return J. Meigs, Jr.", "John Kasich"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1320_c5498ca807d2e1ec30d4c8fdd41f0bf7", "prediction": ["# Reasoning Path:\nHank -> sports.mascot.team -> Milwaukee Brewers -> sports.sports_team.arena_stadium -> Miller Park\n# Answer:\nMilwaukee Brewers"], "ground_truth": ["Miller Park"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1348_3bddf705b2a346fbdc2f00e9a02b84f3", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.team -> Indianapolis Colts\n# Answer:\nm.0j4z5bh", "# Reasoning Path:\nPeyton Manning -> sports.drafted_athlete.drafted -> m.04vb3pp -> sports.sports_league_draft_pick.team -> Indianapolis Colts\n# Answer:\nm.04vb3pp", "# Reasoning Path:\nViktor the Viking -> sports.mascot.team -> Minnesota Vikings -> sports.sports_team.league -> m.0crt4x2\n# Answer:\nMinnesota Vikings", "# Reasoning Path:\nMale -> medicine.risk_factor.diseases -> Kyphosis -> medicine.risk_factor.diseases -> Lordosis\n# Answer:\nKyphosis", "# Reasoning Path:\nMale -> medicine.risk_factor.diseases -> Kyphosis -> medicine.disease.causes -> Major trauma\n# Answer:\nKyphosis"], "ground_truth": ["Minnesota Vikings"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.28571428571428575, "path_precision": 0.2, "path_recall": 0.5, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1384_744a496b907e407b16bc5d7c197dc3f0", "prediction": ["# Reasoning Path:\nOvadia Yosef -> people.person.religion -> Judaism -> religion.religion.deities -> Elohim\n# Answer:\nJudaism", "# Reasoning Path:\nOvadia Yosef -> people.person.religion -> Judaism -> religion.religion.deities -> God\n# Answer:\nJudaism", "# Reasoning Path:\nOvadia Yosef -> people.person.religion -> Haredi Judaism -> religion.religion.is_part_of -> Judaism\n# Answer:\nHaredi Judaism", "# Reasoning Path:\nOvadia Yosef -> people.person.religion -> Judaism -> religion.religion.beliefs -> Entering Heaven alive\n# Answer:\nJudaism", "# Reasoning Path:\nOvadia Yosef -> people.person.ethnicity -> Israelis -> people.ethnicity.included_in_group -> Semitic people\n# Answer:\nIsraelis"], "ground_truth": ["Judaism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1528_25853c768670cd164d7793f094ba7cbb", "prediction": ["# Reasoning Path:\ng.122sg15v\n# Answer:\ng.122sg15v", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.03jq21w -> film.performance.actor -> Logan Lerman\n# Answer:\nm.03jq21w", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.03jq21w -> film.performance.film -> The Butterfly Effect\n# Answer:\nm.03jq21w"], "ground_truth": ["Percy Jackson: Sea of Monsters", "Percy Jackson & the Olympians: The Lightning Thief"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1686_29e74083744b3631541b29b4094fb273", "prediction": ["# Reasoning Path:\nGovernor -> base.firsts.achievement.firsts -> m.04zr93l -> base.firsts.first_achievement.category -> United States of America\n# Answer:\nm.04zr93l", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y42 -> government.government_position_held.office_holder -> Ernest McFarland\n# Answer:\nm.04j8y42", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y46 -> government.government_position_held.office_holder -> Paul Fannin\n# Answer:\nm.04j8y46", "# Reasoning Path:\nGovernor -> base.ikariam.ikariam_research.category -> Military branch -> book.periodical_subject.periodicals -> CAT\n# Answer:\nMilitary branch", "# Reasoning Path:\nGovernor -> base.ikariam.ikariam_research.category -> Military branch -> book.periodical_subject.periodicals -> After the Battle\n# Answer:\nMilitary branch"], "ground_truth": ["Janet Napolitano"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1785_1c178196ae53ffd4b09e5787a35c3950", "prediction": ["# Reasoning Path:\nKim Richards -> tv.tv_actor.starring_roles -> m.0bngb3y -> tv.regular_tv_appearance.series -> Hello, Larry\n# Answer:\nm.0bngb3y"], "ground_truth": ["Nanny and the Professor"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1797_68a33792b0a1e18937dcd4b3f50d941e", "prediction": ["# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> location.location.containedby -> North America\n# Answer:\nConfederate States of America", "# Reasoning Path:\nMontgomery -> location.location.containedby -> Alabama -> location.location.containedby -> Contiguous United States\n# Answer:\nAlabama"], "ground_truth": ["Confederate States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1812_c29807a955ea46fa79cdc9f7aeacba18", "prediction": ["# Reasoning Path:\nRihanna: Live in Concert Tour -> music.concert_tour.artist -> Rihanna -> music.artist.origin -> Saint Michael Parish\n# Answer:\nRihanna", "# Reasoning Path:\nRihanna: Live in Concert Tour -> music.concert_tour.artist -> Rihanna -> music.artist.origin -> Barbados\n# Answer:\nRihanna", "# Reasoning Path:\nRihanna: Live in Concert Tour -> music.concert_tour.artist -> Rihanna -> people.person.ethnicity -> Afro-Guyanese\n# Answer:\nRihanna"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-361_e24533e28da40db99eb4b25773f9d38f", "prediction": ["# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Kunsthistorisches Museum -> travel.tourist_attraction.near_travel_destination -> Vienna\n# Answer:\nKunsthistorisches Museum"], "ground_truth": ["Kunsthistorisches Museum"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-397_c7ee26d5c3f80107d7ae5fede489209a", "prediction": ["# Reasoning Path:\nHarvard Art Museums -> visual_art.art_owner.artworks_owned -> m.075vbgc -> visual_art.artwork_owner_relationship.owner -> Harvard Art Museums\n# Answer:\nm.075vbgc"], "ground_truth": ["Crimson"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-484_ac67410188d0f2258139a3c84773885e", "prediction": ["# Reasoning Path:\nCentral Western Time Zone -> time.time_zone.locations_in_this_time_zone -> Australia -> location.country.languages_spoken -> Lojban\n# Answer:\nAustralia", "# Reasoning Path:\nCentral Western Time Zone -> time.time_zone.locations_in_this_time_zone -> Australia -> location.country.languages_spoken -> Esperanto Language\n# Answer:\nAustralia", "# Reasoning Path:\nCentral Western Time Zone -> time.time_zone.locations_in_this_time_zone -> Australia -> location.country.languages_spoken -> English Language\n# Answer:\nAustralia"], "ground_truth": ["English Language", "Esperanto Language", "Lojban"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-538_49b4e9304f18a0a1cbe37bb162f61131", "prediction": ["# Reasoning Path:\nWest Reading El Ctr -> education.educational_institution.students_graduates -> m.0ycxbfr -> education.education.student -> Taylor Swift\n# Answer:\nm.0ycxbfr"], "ground_truth": ["Valentine's Day", "The Giver", "Jonas Brothers: The Concert Experience", "Hannah Montana: The Movie", "The Lorax"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-538_92e606ef9c0429ad6820797ad2950730", "prediction": ["# Reasoning Path:\nOnce-ler -> film.film_character.portrayed_in_films -> m.0gwnx81 -> film.performance.film -> The Lorax\n# Answer:\nm.0gwnx81", "# Reasoning Path:\nTaylor Swift -> music.composer.compositions -> 22 -> music.recording.canonical_version -> 22\n# Answer:\n22"], "ground_truth": ["The Lorax"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-576_01e2da60a2779c4ae4b5d1547499a4f8", "prediction": ["# Reasoning Path:\nCountry -> fictional_universe.type_of_fictional_setting.settings -> Ruritania -> fictional_universe.fictional_setting.contained_by -> Europe\n# Answer:\nRuritania", "# Reasoning Path:\nCentral America -> location.location.contains -> Guatemala -> location.country.form_of_government -> Constitutional republic\n# Answer:\nGuatemala", "# Reasoning Path:\nCountry -> fictional_universe.type_of_fictional_setting.settings -> Grand Fenwick -> fictional_universe.fictional_setting.contained_by -> Europe\n# Answer:\nGrand Fenwick", "# Reasoning Path:\nAlta Verapaz Department -> location.statistical_region.population -> g.11bv5vs59b\n# Answer:\ng.11bv5vs59b"], "ground_truth": ["Guatemala"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.02040816326530612, "path_precision": 0.25, "path_recall": 0.010638297872340425, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-576_8dfecb6548586cf236340abadabeb86c", "prediction": ["# Reasoning Path:\nCountry -> fictional_universe.type_of_fictional_setting.settings -> Grand Fenwick -> fictional_universe.fictional_setting.contained_by -> Europe\n# Answer:\nGrand Fenwick", "# Reasoning Path:\nCountry -> fictional_universe.type_of_fictional_setting.settings -> Ruritania -> fictional_universe.fictional_setting.contained_by -> Europe\n# Answer:\nRuritania", "# Reasoning Path:\nCentral America -> location.location.contains -> Panama -> location.country.form_of_government -> Constitutional republic\n# Answer:\nPanama", "# Reasoning Path:\nUnited States Dollar -> common.topic.webpage -> m.04lsxjt -> common.webpage.resource -> m.0bk1n5x\n# Answer:\nm.04lsxjt"], "ground_truth": ["Panama", "El Salvador"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.3333333333333333, "ans_precission": 0.25, "ans_recall": 0.5, "path_f1": 0.029411764705882353, "path_precision": 0.25, "path_recall": 0.015625, "path_ans_f1": 0.3333333333333333, "path_ans_precision": 0.25, "path_ans_recall": 0.5}
{"id": "WebQTest-576_906ad6be7bec9d208f4dde4f7721c261", "prediction": ["# Reasoning Path:\nCentral America -> organization.organization_scope.organizations_with_this_scope -> Exclaimer -> organization.organization.geographic_scope -> Europe\n# Answer:\nExclaimer", "# Reasoning Path:\nCountry -> fictional_universe.type_of_fictional_setting.settings -> Ruritania -> fictional_universe.fictional_setting.contained_by -> Europe\n# Answer:\nRuritania", "# Reasoning Path:\nCountry -> fictional_universe.type_of_fictional_setting.settings -> Grand Fenwick -> fictional_universe.fictional_setting.contained_by -> Europe\n# Answer:\nGrand Fenwick", "# Reasoning Path:\nCentral America -> organization.organization_scope.organizations_with_this_scope -> Exclaimer -> organization.organization.geographic_scope -> Central America\n# Answer:\nExclaimer", "# Reasoning Path:\nCentral America -> location.location.contains -> Panama -> base.aareas.schema.administrative_area.administrative_area_type -> Sovereign state\n# Answer:\nPanama"], "ground_truth": ["Costa Rica", "Panama", "Guatemala", "Belize", "El Salvador", "Honduras"], "ans_acc": 0.16666666666666666, "ans_hit": 1, "ans_f1": 0.1818181818181818, "ans_precission": 0.2, "ans_recall": 0.16666666666666666, "path_f1": 0.006430868167202573, "path_precision": 0.2, "path_recall": 0.0032679738562091504, "path_ans_f1": 0.1818181818181818, "path_ans_precision": 0.2, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTest-590_6aad73acb74f304bc9acae44314164be", "prediction": ["# Reasoning Path:\nLibya, Libya, Libya -> music.composition.language -> Arabic Language -> language.human_language.countries_spoken_in -> Libya\n# Answer:\nArabic Language", "# Reasoning Path:\nLibya, Libya, Libya -> music.composition.composer -> Abdul Wahab -> people.person.profession -> Composer\n# Answer:\nAbdul Wahab", "# Reasoning Path:\nPrime Minister of Libya -> government.government_office_or_title.office_holders -> m.0105y7bt -> government.government_position_held.office_holder -> Abdullah al-Thani\n# Answer:\nm.0105y7bt", "# Reasoning Path:\nLibya, Libya, Libya -> music.composition.composer -> Abdul Wahab -> people.person.children -> Ahmed Mohammed Abdel Wahab\n# Answer:\nAbdul Wahab"], "ground_truth": ["Abdullah al-Thani"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666666, "path_precision": 0.25, "path_recall": 0.125, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-61_09020cbb000c86fda5910ec084690246", "prediction": ["# Reasoning Path:\nMecca -> religion.place_of_worship.religion -> Islam -> religion.religion.types_of_places_of_worship -> Mosque\n# Answer:\nIslam", "# Reasoning Path:\nMecca -> travel.travel_destination.tourist_attractions -> Al Ja\u00e1ranah Mosque -> travel.tourist_attraction.near_travel_destination -> Mecca\n# Answer:\nAl Ja\u00e1ranah Mosque", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf2_w -> location.religion_percentage.religion -> Islam\n# Answer:\nm.03xf2_w"], "ground_truth": ["Islam"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.6666666666666666, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-61_8f0d44a78a2e50f47729db23c5850da0", "prediction": ["# Reasoning Path:\nIndonesia -> location.country.official_language -> Indonesian Language -> language.human_language.countries_spoken_in -> Indonesia\n# Answer:\nIndonesian Language", "# Reasoning Path:\nMary -> people.person.religion -> Catholicism -> religion.religion.deities -> Jesus Christ\n# Answer:\nCatholicism", "# Reasoning Path:\nMary -> people.person.religion -> Catholicism -> religion.religion.deities -> God\n# Answer:\nCatholicism", "# Reasoning Path:\nMary -> people.person.religion -> Catholicism -> religion.religion.deities -> Holy Spirit\n# Answer:\nCatholicism", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf2_w -> location.religion_percentage.religion -> Islam\n# Answer:\nm.03xf2_w"], "ground_truth": ["Catholicism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-626_47b990334f91b6d3bd042f82b81740f6", "prediction": ["# Reasoning Path:\nDennis Daugaard -> people.person.nationality -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> South Dakota\n# Answer:\nUnited States of America", "# Reasoning Path:\nDennis Daugaard -> common.image.appears_in_topic_gallery -> Dennis Daugaard -> people.person.nationality -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nDennis Daugaard -> government.politician.government_positions_held -> m.0jskk6b -> government.government_position_held.jurisdiction_of_office -> South Dakota\n# Answer:\nm.0jskk6b", "# Reasoning Path:\nMissouri River -> geography.body_of_water.bridges -> ASB Bridge -> common.topic.notable_for -> g.125hbd7yd\n# Answer:\nASB Bridge", "# Reasoning Path:\nDennis Daugaard -> common.image.appears_in_topic_gallery -> Dennis Daugaard -> government.politician.government_positions_held -> m.0jskk6b\n# Answer:\nm.0jskk6b"], "ground_truth": ["South Dakota"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3076923076923077, "path_precision": 0.4, "path_recall": 0.25, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-626_8c6b952c6bd963f0ece4e401c9eb731a", "prediction": ["# Reasoning Path:\nCentral Time Zone -> common.topic.image -> Timezoneswest -> common.image.appears_in_topic_gallery -> Central Time Zone\n# Answer:\nTimezoneswest", "# Reasoning Path:\nMissouri River -> common.topic.image -> Missouri River near Yankton 1 -> common.image.appears_in_topic_gallery -> Missouri River\n# Answer:\nMissouri River near Yankton 1"], "ground_truth": ["Nebraska", "Kansas", "Iowa", "Missouri", "South Dakota", "North Dakota"], "ans_acc": 0.16666666666666666, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.5, "ans_recall": 0.16666666666666666, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.25, "path_ans_precision": 0.5, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTest-626_bd2c9bc1c0a692218e7c4182a1d52f85", "prediction": ["# Reasoning Path:\nMike Johanns -> government.politician.government_positions_held -> m.05kfds4 -> government.government_position_held.district_represented -> Nebraska\n# Answer:\nm.05kfds4", "# Reasoning Path:\nMike Johanns -> common.image.appears_in_topic_gallery -> Mike Johanns -> government.politician.government_positions_held -> m.05kfds4\n# Answer:\nm.05kfds4", "# Reasoning Path:\nMike Johanns -> common.image.appears_in_topic_gallery -> Mike Johanns -> government.politician.government_positions_held -> m.04l4prr\n# Answer:\nm.04l4prr", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Nebraska -> government.governmental_jurisdiction.governing_officials -> m.04l4prr\n# Answer:\nNebraska", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Missouri -> location.location.adjoin_s -> m.02tgvb_\n# Answer:\nMissouri"], "ground_truth": ["Nebraska"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.4, "path_recall": 0.3333333333333333, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-712_6099ea03f4fd2476605c4a513318d29c", "prediction": ["# Reasoning Path:\nLiberal Government of New Zealand -> government.political_appointer.appointees -> m.0wzvl9f -> government.government_position_held.jurisdiction_of_office -> New Zealand\n# Answer:\nm.0wzvl9f", "# Reasoning Path:\nLiberal Government of New Zealand -> government.political_appointer.appointees -> m.0wpq63h -> government.government_position_held.jurisdiction_of_office -> New Zealand\n# Answer:\nm.0wpq63h", "# Reasoning Path:\nCountry -> type.type.properties -> Administrative Divisions -> type.property.master_property -> Country\n# Answer:\nAdministrative Divisions", "# Reasoning Path:\nOceania -> location.location.contains -> New Zealand -> base.aareas.schema.administrative_area.administrative_children -> Auckland Region\n# Answer:\nNew Zealand"], "ground_truth": ["New Zealand"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.3, "path_precision": 0.75, "path_recall": 0.1875, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-743_0a8cdba29cf260283b7c890b3609c0b9", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.parents -> John F. Kennedy\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nMale -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm -> medicine.risk_factor.diseases -> Intracranial aneurysm\n# Answer:\nAbdominal aortic aneurysm", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.sibling_s -> m.0pbycd1\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.sibling_s -> m.03hs9fc -> people.sibling_relationship.sibling -> John F. Kennedy\n# Answer:\nm.03hs9fc"], "ground_truth": ["Robert F. Kennedy"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-832_c334509bb5e02cacae1ba2e80c176499", "prediction": ["# Reasoning Path:\nLou Seal -> sports.mascot.team -> San Francisco Giants -> sports.sports_team.championships -> 2014 World Series\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nLou Seal -> sports.mascot.team -> San Francisco Giants -> sports.sports_team.championships -> 2010 World Series\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nLou Seal -> sports.mascot.team -> San Francisco Giants -> time.participant.event -> 2010 World Series\n# Answer:\nSan Francisco Giants"], "ground_truth": ["2014 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-96_11da03aa9cec8b011619c8ea0dbfdcf9", "prediction": ["# Reasoning Path:\nEast Whittier Elementary School -> education.educational_institution.students_graduates -> m.0_sp3dv -> education.education.student -> Richard Nixon\n# Answer:\nm.0_sp3dv"], "ground_truth": ["New York City"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-983_ccda690fb745939d0a62c3fbcf3e3769", "prediction": ["# Reasoning Path:\nThe Maneater -> education.school_newspaper.school -> University of Missouri -> education.educational_institution.faculty -> m.0k7ct71\n# Answer:\nUniversity of Missouri", "# Reasoning Path:\nThe Maneater -> book.newspaper.circulation_areas -> Columbia -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nColumbia", "# Reasoning Path:\nCollege/University -> type.type.domain -> Education -> freebase.domain_profile.users -> a_olympia\n# Answer:\nEducation", "# Reasoning Path:\nTennessee Williams -> book.author.works_written -> 27 Wagons Full of Cotton -> media_common.adapted_work.adaptations -> Baby Doll\n# Answer:\n27 Wagons Full of Cotton", "# Reasoning Path:\nThe Maneater -> book.newspaper.circulation_areas -> Columbia -> base.biblioness.bibs_location.state -> Missouri\n# Answer:\nColumbia"], "ground_truth": ["University of Missouri"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.13333333333333333, "path_precision": 0.2, "path_recall": 0.1, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-998_f693ae2cb9aa6b8ffdf0f103a6777b62", "prediction": ["# Reasoning Path:\nJames L. Dolan -> sports.sports_team_owner.teams_owned -> New York Knicks -> sports.sports_team.championships -> 1973 NBA Finals\n# Answer:\nNew York Knicks", "# Reasoning Path:\nJames L. Dolan -> sports.sports_team_owner.teams_owned -> New York Knicks -> sports.sports_team.championships -> 1970 NBA Finals\n# Answer:\nNew York Knicks", "# Reasoning Path:\nJames L. Dolan -> organization.organization_founder.organizations_founded -> The Madison Square Garden Company -> sports.sports_team_owner.teams_owned -> New York Knicks\n# Answer:\nThe Madison Square Garden Company"], "ground_truth": ["1973 NBA Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTrn-1023_1e7110e48c30a2cef3caf291e3b8d394", "prediction": ["# Reasoning Path:\nBuenos Aires -> film.film_location.featured_in_films -> M -> film.film.language -> Spanish Language\n# Answer:\nM", "# Reasoning Path:\nBuenos Aires -> location.administrative_division.country -> Argentina -> periodicals.newspaper_circulation_area.newspapers -> Clar\u00edn\n# Answer:\nArgentina", "# Reasoning Path:\nBuenos Aires -> film.film_location.featured_in_films -> M -> film.film.genre -> Documentary film\n# Answer:\nM", "# Reasoning Path:\nBuenos Aires -> film.film_location.featured_in_films -> Awesome Fuckers -> film.film.language -> Spanish Language\n# Answer:\nAwesome Fuckers", "# Reasoning Path:\nBuenos Aires -> periodicals.newspaper_circulation_area.newspapers -> La Raz\u00f3n -> common.topic.notable_types -> Newspaper\n# Answer:\nLa Raz\u00f3n"], "ground_truth": ["Argentine peso"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-1077_0c34ca057060e35aa5c74fbbca682dee", "prediction": ["# Reasoning Path:\nBadakhshan Province -> location.location.containedby -> Afghanistan -> location.statistical_region.religions -> m.0493b56\n# Answer:\nAfghanistan", "# Reasoning Path:\nBadakhshan Province -> location.location.containedby -> Afghanistan -> location.location.containedby -> Asia\n# Answer:\nAfghanistan", "# Reasoning Path:\nBadakhshan Province -> location.location.containedby -> Afghanistan -> location.statistical_region.religions -> m.0493b5d\n# Answer:\nAfghanistan", "# Reasoning Path:\nBadakhshan Province -> base.aareas.schema.administrative_area.administrative_parent -> Afghanistan -> location.statistical_region.religions -> m.0493b56\n# Answer:\nAfghanistan", "# Reasoning Path:\nBadakhshan Province -> location.location.containedby -> Afghanistan -> location.location.partially_contains -> Amu Darya\n# Answer:\nAfghanistan"], "ground_truth": ["Shia Islam", "Sunni Islam"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-1077_f4a9e5f1e0dcfb82cbadf4771eda7bb5", "prediction": ["# Reasoning Path:\nAfghan National Anthem -> music.composition.language -> Pashto language -> language.human_language.countries_spoken_in -> Pakistan\n# Answer:\nPashto language", "# Reasoning Path:\nAfghan National Anthem -> music.composition.language -> Pashto language -> language.human_language.main_country -> Pakistan\n# Answer:\nPashto language"], "ground_truth": ["Shia Islam", "Sunni Islam"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-124_028bb5f442b37a4af9f9fd9fa0bc5e9a", "prediction": ["# Reasoning Path:\nWilliam O. Schaefer Elementary School -> education.educational_institution.students_graduates -> m.0w17_7x -> education.education.student -> Angelina Jolie\n# Answer:\nm.0w17_7x"], "ground_truth": ["In the Land of Blood and Honey", "By the Sea", "Unbroken", "A Place in Time"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-124_f3990dc9aa470fa81ec4cf2912a9924f", "prediction": ["# Reasoning Path:\nAjla -> film.film_character.portrayed_in_films -> m.0gw7h9w -> film.performance.film -> In the Land of Blood and Honey\n# Answer:\nm.0gw7h9w", "# Reasoning Path:\nAjla -> common.topic.notable_types -> Film character -> type.type.domain -> Film\n# Answer:\nFilm character", "# Reasoning Path:\nAjla -> common.topic.notable_types -> Film character -> type.type.expected_by -> Character\n# Answer:\nFilm character", "# Reasoning Path:\nAngelina Jolie -> film.director.film -> In the Land of Blood and Honey -> film.film.starring -> m.0gw7h9w\n# Answer:\nIn the Land of Blood and Honey", "# Reasoning Path:\nAngelina Jolie -> film.director.film -> Unbroken -> common.topic.notable_for -> g.1yl5y8gg2\n# Answer:\nUnbroken"], "ground_truth": ["In the Land of Blood and Honey"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTrn-1259_1997cb4922db71983be26e6a509950f4", "prediction": ["# Reasoning Path:\nCountry Nation World Tour -> music.concert_tour.artist -> Brad Paisley -> people.person.education -> m.0h3d7qb\n# Answer:\nBrad Paisley", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> Universities -> type.property.schema -> Cityscape\n# Answer:\nUniversities", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> University -> organization.organization_type.organizations_of_this_type -> University of Oxford\n# Answer:\nUniversity", "# Reasoning Path:\nBachelor's degree -> common.topic.webpage -> m.09wyykm -> common.webpage.topic -> Bachelor's degree\n# Answer:\nm.09wyykm", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> University -> organization.organization_type.organizations_of_this_type -> Dallas Baptist University\n# Answer:\nUniversity"], "ground_truth": ["Belmont University"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-1394_dc71f08a76d71762a0cfd30f11f66535", "prediction": ["# Reasoning Path:\nAnnaba Province -> location.administrative_division.country -> Algeria -> location.country.languages_spoken -> Arabic Language\n# Answer:\nAlgeria", "# Reasoning Path:\nAnnaba Province -> base.aareas.schema.administrative_area.administrative_parent -> Algeria -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAlgeria", "# Reasoning Path:\nAnnaba Province -> location.administrative_division.country -> Algeria -> location.location.containedby -> Africa\n# Answer:\nAlgeria", "# Reasoning Path:\nAnnaba Province -> location.location.containedby -> Algeria -> location.country.languages_spoken -> Arabic Language\n# Answer:\nAlgeria", "# Reasoning Path:\nAnnaba Province -> base.aareas.schema.administrative_area.administrative_parent -> Algeria -> location.location.containedby -> Africa\n# Answer:\nAlgeria"], "ground_truth": ["Algeria"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.4, "path_recall": 0.3333333333333333, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTrn-1557_edfd3507d32929ce0e9d844f7a2674de", "prediction": ["# Reasoning Path:\nCharles R. Drew -> people.person.education -> m.03l72ff -> education.education.institution -> Columbia University\n# Answer:\nm.03l72ff", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> Universities -> type.property.schema -> Cityscape\n# Answer:\nUniversities", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> Institution -> type.property.expected_type -> College/University\n# Answer:\nInstitution", "# Reasoning Path:\nLe D\u00e9lit fran\u00e7ais -> education.school_newspaper.school -> McGill University -> education.educational_institution.mascot -> McGill University Marty the Martlet\n# Answer:\nMcGill University", "# Reasoning Path:\nCharles R. Drew -> people.person.education -> m.02wmxnx -> education.education.student -> Charles R. Drew\n# Answer:\nm.02wmxnx"], "ground_truth": ["McGill University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.22222222222222224, "path_precision": 0.2, "path_recall": 0.25, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTrn-1770_8db36acba886620a06031d39165d78de", "prediction": ["# Reasoning Path:\nI Am... World Tour -> music.concert_tour.artist -> Beyonc\u00e9 Knowles -> music.composer.compositions -> Ave Maria\n# Answer:\nBeyonc\u00e9 Knowles"], "ground_truth": ["Blue Ivy"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-1812_5fb6c8c4d53d55c03445e4b2454ef810", "prediction": ["# Reasoning Path:\nCountry -> fictional_universe.type_of_fictional_setting.settings -> Ruritania -> fictional_universe.fictional_setting.contained_by -> Europe\n# Answer:\nRuritania", "# Reasoning Path:\nEast Caribbean dollar -> finance.currency.countries_used -> Dominica -> location.location.contains -> Isla Aves\n# Answer:\nDominica", "# Reasoning Path:\nCountry -> fictional_universe.type_of_fictional_setting.settings -> Grand Fenwick -> fictional_universe.fictional_setting.contained_by -> Europe\n# Answer:\nGrand Fenwick", "# Reasoning Path:\nCaribbean -> location.location.events -> Action of 6 December 1782 -> base.culturalevent.event.entity_involved -> Jean-Charles de Borda\n# Answer:\nAction of 6 December 1782"], "ground_truth": ["Saint Kitts and Nevis", "Saint Lucia", "Anguilla"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-1817_89670933168c3f4e5195a241f9d46e76", "prediction": ["# Reasoning Path:\nTi\u1ebfn Qu\u00e2n Ca -> music.composition.language -> Vietnamese Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nVietnamese Language", "# Reasoning Path:\nTi\u1ebfn Qu\u00e2n Ca -> music.composition.language -> Vietnamese Language -> language.human_language.main_country -> Vietnam\n# Answer:\nVietnamese Language"], "ground_truth": ["Single-party state", "Socialist state", "Communist state"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-1841_6bf772ee1b388c1b3c226b02c8244aa7", "prediction": ["# Reasoning Path:\n2014 Eurocup Finals -> sports.sports_championship_event.champion -> Valencia BC -> sports.sports_team.championships -> 2014 Eurocup Finals\n# Answer:\nValencia BC", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Valencia BC -> sports.sports_team.championships -> 2014 Eurocup Finals\n# Answer:\nValencia BC", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Real Madrid Baloncesto -> sports.sports_team.colors -> Black\n# Answer:\nReal Madrid Baloncesto", "# Reasoning Path:\n2014 Eurocup Finals -> sports.sports_championship_event.runner_up -> BC UNICS -> sports.sports_team.roster -> m.0sxg3vf\n# Answer:\nBC UNICS"], "ground_truth": ["Valencia BC"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTrn-1841_b8df00139e3fa59b8633ef551ed8ca9f", "prediction": ["# Reasoning Path:\n2010 FIFA World Cup -> base.x2010fifaworldcupsouthafrica.fifa_world_cup.champion -> Spain national football team -> base.x2010fifaworldcupsouthafrica.world_cup_participation.world_cup -> m.079pkv4\n# Answer:\nSpain national football team", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Spain national football team -> base.x2010fifaworldcupsouthafrica.world_cup_participation.world_cup -> m.079pkv4\n# Answer:\nSpain national football team", "# Reasoning Path:\n2010 FIFA World Cup -> time.event.locations -> South Africa -> sports.sports_team_location.teams -> South Africa national football team\n# Answer:\nSouth Africa", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Joventut Badalona -> sports.sports_team.location -> Spain\n# Answer:\nJoventut Badalona", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Joventut Badalona -> sports.sports_team.sport -> Basketball\n# Answer:\nJoventut Badalona"], "ground_truth": ["Spain national football team"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTrn-1864_67ecd1c247c3b2c9545fbcf1ad8d9d00", "prediction": ["# Reasoning Path:\nAnadyr Time Zone -> time.time_zone.locations_in_this_time_zone -> Asia -> base.locations.continents.countries_within -> Laos\n# Answer:\nAsia", "# Reasoning Path:\nCountry -> common.topic.image -> Europe topography map\n# Answer:\nEurope topography map", "# Reasoning Path:\nAnadyr Time Zone -> time.time_zone.locations_in_this_time_zone -> Asia -> base.locations.continents.countries_within -> Afghanistan\n# Answer:\nAsia", "# Reasoning Path:\nAnadyr Time Zone -> time.time_zone.locations_in_this_time_zone -> Asia -> base.locations.continents.countries_within -> Iraq\n# Answer:\nAsia"], "ground_truth": ["India"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-1864_9dc4e22121d3a46d45b8f9bd9e8c7013", "prediction": ["# Reasoning Path:\nCountry -> fictional_universe.type_of_fictional_setting.settings -> Ruritania -> fictional_universe.fictional_setting.setting_type -> Country\n# Answer:\nRuritania", "# Reasoning Path:\nCountry -> fictional_universe.type_of_fictional_setting.settings -> Grand Fenwick -> fictional_universe.fictional_setting.setting_type -> Country\n# Answer:\nGrand Fenwick", "# Reasoning Path:\nASEAN Common Time Zone -> freebase.valuenotation.has_no_value -> DST offset from UTC -> rdf-schema#range -> Floating Point Number\n# Answer:\nDST offset from UTC"], "ground_truth": ["India"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-1938_7322a2a4d46bf36b95bfab4418c9a32b", "prediction": ["# Reasoning Path:\nNorthern District -> location.administrative_division.first_level_division_of -> Israel -> location.country.form_of_government -> Parliamentary system\n# Answer:\nIsrael"], "ground_truth": ["Parliamentary system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTrn-2026_d059b24adec4064377b957ca598769be", "prediction": ["# Reasoning Path:\nSanto Domingo -> base.biblioness.bibs_location.country -> Dominican Republic -> location.location.containedby -> Greater Antilles\n# Answer:\nDominican Republic", "# Reasoning Path:\nSanto Domingo -> location.location.contains -> Central Santo Domingo -> location.location.containedby -> Dominican Republic\n# Answer:\nCentral Santo Domingo", "# Reasoning Path:\nSanto Domingo -> location.location.contains -> Central Santo Domingo -> location.location.containedby -> Santo Domingo\n# Answer:\nCentral Santo Domingo"], "ground_truth": ["Dominican peso"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-2057_b64f996ceca972c0d6fea3de2705ce63", "prediction": ["# Reasoning Path:\nAlta Verapaz Department -> base.aareas.schema.administrative_area.administrative_parent -> Guatemala -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nGuatemala", "# Reasoning Path:\nAlta Verapaz Department -> base.aareas.schema.administrative_area.administrative_parent -> Guatemala -> location.country.currency_used -> Guatemalan quetzal\n# Answer:\nGuatemala", "# Reasoning Path:\nAlta Verapaz Department -> location.administrative_division.country -> Guatemala -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nGuatemala", "# Reasoning Path:\nAlta Verapaz Department -> location.administrative_division.country -> Guatemala -> location.country.currency_used -> Guatemalan quetzal\n# Answer:\nGuatemala"], "ground_truth": ["Guatemalan quetzal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTrn-2069_0fa727f3b282196eb1097410b4be6818", "prediction": ["# Reasoning Path:\nMichelle Bachelet -> people.person.nationality -> Chile -> location.country.official_language -> Spanish Language\n# Answer:\nChile", "# Reasoning Path:\nMichelle Bachelet -> people.person.nationality -> Chile -> location.country.languages_spoken -> Spanish Language\n# Answer:\nChile", "# Reasoning Path:\nMichelle Bachelet -> people.person.ethnicity -> Chileans -> people.ethnicity.languages_spoken -> Spanish Language\n# Answer:\nChileans", "# Reasoning Path:\nMichelle Bachelet -> people.person.ethnicity -> Chileans -> people.ethnicity.languages_spoken -> Chilean Spanish\n# Answer:\nChileans"], "ground_truth": ["Spanish Language", "Rapa Nui Language", "Puquina Language", "Mapudungun Language", "Aymara language"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.31578947368421056, "path_ans_precision": 0.75, "path_ans_recall": 0.2}
{"id": "WebQTrn-2152_3cdf60c15a8355981dd92e3c57ac2eed", "prediction": ["# Reasoning Path:\nAmerican League West -> sports.sports_league.teams -> m.0crtd3v -> sports.sports_league_participation.team -> California Angels\n# Answer:\nm.0crtd3v", "# Reasoning Path:\nAmerican League West -> sports.sports_league.teams -> m.0crtd18 -> sports.sports_league_participation.team -> Oakland Athletics\n# Answer:\nm.0crtd18", "# Reasoning Path:\nAmerican League West -> baseball.baseball_division.teams -> Seattle Mariners -> sports.sports_team.team_mascot -> Mariner Moose\n# Answer:\nSeattle Mariners", "# Reasoning Path:\nMariner Moose -> sports.mascot.team -> Seattle Mariners -> sports.professional_sports_team.draft_picks -> m.04d0n1b\n# Answer:\nSeattle Mariners", "# Reasoning Path:\nAmerican League West -> baseball.baseball_division.teams -> Seattle Pilots -> sports.sports_team.arena_stadium -> Sick's Stadium\n# Answer:\nSeattle Pilots"], "ground_truth": ["Seattle Mariners"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTrn-2152_92fba37c9723caee68665ad9a5e4a468", "prediction": ["# Reasoning Path:\nAmerican League West -> baseball.baseball_division.teams -> Texas Rangers -> sports.professional_sports_team.owner_s -> Ray Davis\n# Answer:\nTexas Rangers", "# Reasoning Path:\nAmerican League West -> baseball.baseball_division.teams -> Texas Rangers -> sports.professional_sports_team.owner_s -> Tom Hicks\n# Answer:\nTexas Rangers", "# Reasoning Path:\nAmerican League West -> sports.sports_league.teams -> m.0crtd3v -> sports.sports_league_participation.team -> California Angels\n# Answer:\nm.0crtd3v", "# Reasoning Path:\nTom Hicks -> sports.sports_team_owner.teams_owned -> Texas Rangers -> baseball.baseball_team.current_manager -> Tim Bogar\n# Answer:\nTexas Rangers", "# Reasoning Path:\nAmerican League West -> sports.sports_league.teams -> m.0crtd2r -> sports.sports_league_participation.team -> Los Angeles Angels of Anaheim\n# Answer:\nm.0crtd2r"], "ground_truth": ["Texas Rangers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTrn-2189_f4440609f5cecb091bf8e86adb47be25", "prediction": ["# Reasoning Path:\nBilady, Bilady, Bilady -> music.composition.language -> Arabic Language -> language.human_language.dialects -> Arabic, Sudanese Spoken Language\n# Answer:\nArabic Language", "# Reasoning Path:\nBilady, Bilady, Bilady -> music.composition.language -> Arabic Language -> language.human_language.dialects -> Egyptian Arabic\n# Answer:\nArabic Language", "# Reasoning Path:\nBilady, Bilady, Bilady -> music.composition.language -> Arabic Language -> language.human_language.dialects -> Bedawi Arabic\n# Answer:\nArabic Language", "# Reasoning Path:\nBilady, Bilady, Bilady -> music.composition.language -> Arabic Language -> language.human_language.countries_spoken_in -> Israel\n# Answer:\nArabic Language", "# Reasoning Path:\nBilady, Bilady, Bilady -> music.composition.language -> Arabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire\n# Answer:\nArabic Language"], "ground_truth": ["Modern Standard Arabic"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-21_660138373d19bbffdd3d3f7a30234e4a", "prediction": ["# Reasoning Path:\nDire Dawa -> government.governmental_jurisdiction.governing_officials -> m.0115lk6q -> government.government_position_held.office_holder -> Mersha Nahusenay\n# Answer:\nm.0115lk6q", "# Reasoning Path:\nDire Dawa -> government.governmental_jurisdiction.governing_officials -> m.0115lk6q -> government.government_position_held.basic_title -> Governor\n# Answer:\nm.0115lk6q", "# Reasoning Path:\nPrime minister -> book.book_subject.works -> The Prime Minister -> book.book.editions -> Prime Minister\n# Answer:\nThe Prime Minister", "# Reasoning Path:\nDire Dawa -> location.administrative_division.country -> Ethiopia -> location.country.languages_spoken -> Amharic Language\n# Answer:\nEthiopia"], "ground_truth": ["Hailemariam Desalegn"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-2209_c1374f388d9cc7a78365860c91218362", "prediction": ["# Reasoning Path:\nBrad Stevens -> basketball.basketball_coach.team -> Boston Celtics -> sports.sports_team.championships -> 1961 NBA Finals\n# Answer:\nBoston Celtics", "# Reasoning Path:\nBrad Stevens -> basketball.basketball_coach.team -> Boston Celtics -> sports.sports_team.championships -> 1986 NBA Finals\n# Answer:\nBoston Celtics", "# Reasoning Path:\nBrad Stevens -> basketball.basketball_coach.team -> Boston Celtics -> sports.sports_team.championships -> 1963 NBA Finals\n# Answer:\nBoston Celtics"], "ground_truth": ["1959 NBA Finals", "1965 NBA Finals", "1984 NBA Finals", "1961 NBA Finals", "1962 NBA Finals", "1969 NBA Finals", "2008 NBA Finals", "1976 NBA Finals", "1964 NBA Finals", "1960 NBA Finals", "1986 NBA Finals", "1974 NBA Finals", "1963 NBA Finals", "1957 NBA Finals", "1966 NBA Finals", "1968 NBA Finals", "1981 NBA Finals"], "ans_acc": 0.17647058823529413, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3, "path_precision": 1.0, "path_recall": 0.17647058823529413, "path_ans_f1": 0.3, "path_ans_precision": 1.0, "path_ans_recall": 0.17647058823529413}
{"id": "WebQTrn-2215_11c4cd5a25fd84f3980d7013c0329bad", "prediction": ["# Reasoning Path:\nBeat This Summer Tour -> music.concert_tour.artist -> Brad Paisley -> people.person.place_of_birth -> Glen Dale\n# Answer:\nBrad Paisley"], "ground_truth": ["Glen Dale"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTrn-2286_e0906c845ea8b2e22e08e1b0e6eb9b43", "prediction": ["# Reasoning Path:\nFrankfort -> base.biblioness.bibs_location.state -> Kentucky -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nKentucky", "# Reasoning Path:\nFrankfort -> base.biblioness.bibs_location.state -> Kentucky -> location.location.containedby -> Contiguous United States\n# Answer:\nKentucky", "# Reasoning Path:\nFrankfort -> location.location.containedby -> Kentucky -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nKentucky", "# Reasoning Path:\nFrankfort -> base.biblioness.bibs_location.state -> Kentucky -> location.location.containedby -> United States of America\n# Answer:\nKentucky", "# Reasoning Path:\nFrankfort -> location.location.containedby -> Kentucky -> location.location.containedby -> Contiguous United States\n# Answer:\nKentucky"], "ground_truth": ["United States, with Territories", "Contiguous United States", "United States of America"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.22222222222222224, "path_precision": 0.2, "path_recall": 0.25, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTrn-241_dfb6c97ac9bf2f0ac07f27dd80f9edc2", "prediction": ["# Reasoning Path:\nNijmegen -> film.film_location.featured_in_films -> A Bridge Too Far -> film.film.featured_film_locations -> Germany\n# Answer:\nA Bridge Too Far", "# Reasoning Path:\nNijmegen -> location.administrative_division.second_level_division_of -> Netherlands -> location.location.containedby -> Kingdom of the Netherlands\n# Answer:\nNetherlands", "# Reasoning Path:\nNijmegen -> location.location.containedby -> Gelderland -> location.location.containedby -> Kingdom of the Netherlands\n# Answer:\nGelderland", "# Reasoning Path:\nFrance -> location.location.containedby -> Europe -> location.location.partially_contains -> France\n# Answer:\nEurope", "# Reasoning Path:\nFrance -> film.film_location.featured_in_films -> Amen -> film.film.featured_film_locations -> France\n# Answer:\nAmen"], "ground_truth": ["Germany"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.052631578947368425, "path_precision": 0.2, "path_recall": 0.030303030303030304, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTrn-2444_8f2cd432b509e5b8fe681bb55bca2767", "prediction": ["# Reasoning Path:\n1946 World Series -> sports.sports_championship_event.champion -> St. Louis Cardinals -> sports.sports_team.arena_stadium -> Busch Stadium\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\n1946 World Series -> time.event.locations -> Sportsman's Park -> location.location.containedby -> St. Louis\n# Answer:\nSportsman's Park"], "ground_truth": ["Busch Stadium"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTrn-2444_8fb3f377318c9e61f47779b2c188167b", "prediction": ["# Reasoning Path:\n1931 World Series -> sports.sports_championship_event.champion -> St. Louis Cardinals -> sports.sports_team.arena_stadium -> Busch Stadium\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\n1931 World Series -> time.event.locations -> Shibe Park -> location.location.events -> 1931 World Series\n# Answer:\nShibe Park"], "ground_truth": ["Busch Stadium"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTrn-2444_e5059ff268415917df330817b9c8ef8c", "prediction": ["# Reasoning Path:\nWilliam DeWitt, Jr. -> sports.sports_team_owner.teams_owned -> St. Louis Cardinals -> sports.sports_team.arena_stadium -> Busch Stadium\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\nWilliam DeWitt, Jr. -> sports.sports_team_owner.teams_owned -> St. Louis Cardinals -> sports.sports_team.arena_stadium -> Roger Dean Stadium\n# Answer:\nSt. Louis Cardinals"], "ground_truth": ["Busch Stadium"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTrn-2478_3e09b108c3448248556d4c34acae3cf7", "prediction": ["# Reasoning Path:\nEric F. Spina -> common.topic.notable_types -> Organization leader -> freebase.type_profile.kind -> Role\n# Answer:\nOrganization leader"], "ground_truth": ["Syracuse University Otto the Orange"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-2570_374d1789f1735b6f08e1a829c0d075a2", "prediction": ["# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.05rln07 -> event.speech_or_presentation.speaker_s -> Franklin D. Roosevelt\n# Answer:\nm.05rln07", "# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.05rpn5h -> event.speech_or_presentation.speaker_s -> Harry S. Truman\n# Answer:\nm.05rpn5h", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03q2h_9 -> government.government_position_held.office_holder -> Franklin D. Roosevelt\n# Answer:\nm.03q2h_9"], "ground_truth": ["Harry S. Truman"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3333333333333333, "path_precision": 0.3333333333333333, "path_recall": 0.3333333333333333, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTrn-2664_471b83eade9707a4dba68e201bf29d73", "prediction": ["# Reasoning Path:\nChina -> location.statistical_region.places_exported_to -> m.04bccq0 -> location.imports_and_exports.exported_to -> Sierra Leone\n# Answer:\nm.04bccq0", "# Reasoning Path:\nChina -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> China\n# Answer:\nEnglish Language", "# Reasoning Path:\nGreenwich Mean Time Zone -> time.time_zone.day_dst_begins -> Last Sunday in March -> time.day_of_year.calendar_system -> Gregorian calendar\n# Answer:\nLast Sunday in March"], "ground_truth": ["Sierra Leone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4, "path_precision": 0.3333333333333333, "path_recall": 0.5, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTrn-2722_8babdaa9ecd05a72e3227b43b1f98771", "prediction": ["# Reasoning Path:\nDewitt High School -> organization.organization.headquarters -> m.0dhqb2h -> location.mailing_address.postal_code -> 48820\n# Answer:\nm.0dhqb2h", "# Reasoning Path:\nDewitt High School -> education.educational_institution.students_graduates -> m.0w5lr60 -> education.education.student -> Jordyn Wieber\n# Answer:\nm.0w5lr60", "# Reasoning Path:\nGold medal -> common.topic.webpage -> m.09w7y6w -> common.webpage.topic -> Gold medal\n# Answer:\nm.09w7y6w"], "ground_truth": ["Gymnastics at the 2012 Summer Olympics \u2013 Women's artistic team all-around"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-2834_6a27420dcf0528ae017dd74e40cfd38a", "prediction": ["# Reasoning Path:\nTamil Nadu Thowheed Jamath -> religion.religious_organization.associated_with -> Islam -> religion.religion.deities -> Ramdev Pir\n# Answer:\nIslam", "# Reasoning Path:\nTamil Nadu Thowheed Jamath -> religion.religious_organization.associated_with -> Islam -> religion.religion.deities -> God\n# Answer:\nIslam", "# Reasoning Path:\nTamil Nadu Thowheed Jamath -> religion.religious_organization.associated_with -> Islam -> religion.religion.deities -> Allah\n# Answer:\nIslam", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Abenaki language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nAbenaki language"], "ground_truth": ["Islam"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.6, "path_precision": 0.75, "path_recall": 0.5, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTrn-2871_d7d303efc1f901f14e6aae2bb469743c", "prediction": ["# Reasoning Path:\nMountain Time Zone -> common.topic.image -> Timezoneswest -> common.image.appears_in_topic_gallery -> Mountain Time Zone\n# Answer:\nTimezoneswest", "# Reasoning Path:\nGrand Canyon -> film.film.featured_film_locations -> Grand Canyon National Park -> location.location.contains -> Grand Canyon\n# Answer:\nGrand Canyon National Park", "# Reasoning Path:\nGrand Canyon -> travel.tourist_attraction.near_travel_destination -> Phoenix -> travel.travel_destination.tourist_attractions -> Grand Canyon\n# Answer:\nPhoenix", "# Reasoning Path:\nGrand Canyon -> travel.tourist_attraction.near_travel_destination -> Phoenix -> travel.travel_destination.tourist_attractions -> Castle Hot Springs\n# Answer:\nPhoenix", "# Reasoning Path:\nGrand Canyon -> travel.tourist_attraction.near_travel_destination -> Phoenix -> common.topic.image -> Downtown Phoenix\n# Answer:\nPhoenix"], "ground_truth": ["Lake Powell", "Phoenix"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.6, "ans_recall": 0.5, "path_f1": 0.3, "path_precision": 0.6, "path_recall": 0.2, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.6, "path_ans_recall": 0.5}
{"id": "WebQTrn-3100_143c89d70679c3e5257c93d8e2bc4c67", "prediction": ["# Reasoning Path:\nUTC\u221205:00 -> common.topic.notable_for -> g.125fnb_r1\n# Answer:\ng.125fnb_r1", "# Reasoning Path:\nDominican Republic -> location.location.containedby -> Americas -> location.location.contains -> Dominican Republic\n# Answer:\nAmericas", "# Reasoning Path:\nDominican Republic -> location.location.containedby -> Greater Antilles -> location.location.containedby -> Antilles\n# Answer:\nGreater Antilles", "# Reasoning Path:\nDominican Republic -> location.location.containedby -> Latin America -> location.location.containedby -> Americas\n# Answer:\nLatin America", "# Reasoning Path:\nDominican Republic -> location.location.containedby -> Americas -> location.location.contains -> Latin America\n# Answer:\nAmericas", "# Reasoning Path:\nDominican Republic -> location.location.containedby -> Latin America -> location.location.contains -> Dominican Republic\n# Answer:\nLatin America"], "ground_truth": ["Greater Antilles"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 0.16666666666666666, "ans_recall": 1.0, "path_f1": 0.2857142857142857, "path_precision": 0.16666666666666666, "path_recall": 1.0, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTrn-3136_a2debf685e0c50491e35a9cf7a1e9ade", "prediction": ["# Reasoning Path:\nDown District Council -> location.location.containedby -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nDown District Council -> location.administrative_division.country -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nDown District Council -> location.administrative_division.country -> Northern Ireland -> location.administrative_division.country -> United Kingdom\n# Answer:\nNorthern Ireland"], "ground_truth": ["Northern Ireland"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTrn-3251_d8cddfe5e947e414b7735780ef1efff8", "prediction": ["# Reasoning Path:\nNorthern Colorado Bears football -> sports.school_sports_team.school -> University of Northern Colorado -> location.location.containedby -> Colorado\n# Answer:\nUniversity of Northern Colorado", "# Reasoning Path:\nNorthern Colorado Bears football -> sports.school_sports_team.school -> University of Northern Colorado -> location.location.containedby -> Greeley\n# Answer:\nUniversity of Northern Colorado", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> University -> organization.organization_type.organizations_of_this_type -> University of California, Berkeley\n# Answer:\nUniversity", "# Reasoning Path:\nGreeley -> location.location.contains -> Aims Community College -> education.university.domestic_tuition -> m.0h6z43s\n# Answer:\nAims Community College", "# Reasoning Path:\nGreeley -> location.location.contains -> University of Northern Colorado -> location.location.containedby -> Colorado\n# Answer:\nUniversity of Northern Colorado"], "ground_truth": ["University of Northern Colorado"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.631578947368421, "path_precision": 0.6, "path_recall": 0.6666666666666666, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTrn-3376_0619d288bbed0ca782e60c6f841a6051", "prediction": ["# Reasoning Path:\nCastlemont High School -> education.educational_institution.students_graduates -> m.0gy5x50 -> education.education.student -> Kevin Hart\n# Answer:\nm.0gy5x50"], "ground_truth": ["Castlemont High School", "Temple University"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTrn-372_a806813629e86776ae3bfe26a3e000f8", "prediction": ["# Reasoning Path:\nBarcelona Cathedral -> religion.place_of_worship.religion -> Catholicism -> religion.religion.deities -> The Father\n# Answer:\nCatholicism", "# Reasoning Path:\nBarcelona Cathedral -> religion.place_of_worship.religion -> Catholicism -> religion.religion.deities -> God\n# Answer:\nCatholicism", "# Reasoning Path:\nBarcelona Cathedral -> religion.place_of_worship.religion -> Catholicism -> religion.religion.deities -> Holy Spirit\n# Answer:\nCatholicism"], "ground_truth": ["Jesus Christ", "God", "The Father", "Holy Spirit"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.8571428571428571, "path_precision": 1.0, "path_recall": 0.75, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTrn-3744_26401b4c33bcb760afd734acaa0a1869", "prediction": ["# Reasoning Path:\nRussell Wilson -> people.person.education -> m.0hpny0z -> education.education.institution -> North Carolina State University\n# Answer:\nm.0hpny0z", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> School -> type.type.domain -> Education\n# Answer:\nSchool", "# Reasoning Path:\nThe Daily Cardinal -> education.school_newspaper.school -> University of Wisconsin-Madison -> education.educational_institution.sports_teams -> Wisconsin Badgers football\n# Answer:\nUniversity of Wisconsin-Madison", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> Universities -> type.property.expected_type -> College/University\n# Answer:\nUniversities", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> Institution -> owl#inverseOf -> Departments\n# Answer:\nInstitution"], "ground_truth": ["University of Wisconsin-Madison"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.20000000000000004, "path_precision": 0.2, "path_recall": 0.2, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTrn-3744_b9bd90569bb7912ec3ea180bf164663c", "prediction": ["# Reasoning Path:\nWisconsin Badgers men's basketball -> sports.school_sports_team.school -> University of Wisconsin-Madison -> organization.organization_founder.organizations_founded -> Association of American Universities\n# Answer:\nUniversity of Wisconsin-Madison", "# Reasoning Path:\nWisconsin Badgers men's basketball -> sports.school_sports_team.school -> University of Wisconsin-Madison -> education.university.fraternities_and_sororities -> Alpha Delta Phi\n# Answer:\nUniversity of Wisconsin-Madison", "# Reasoning Path:\nWisconsin Badgers men's basketball -> sports.sports_team.arena_stadium -> Kohl Center -> sports.sports_facility.teams -> Wisconsin Badgers men's basketball\n# Answer:\nKohl Center", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> Universities -> type.property.expected_type -> College/University\n# Answer:\nUniversities", "# Reasoning Path:\nWisconsin Badgers men's basketball -> sports.school_sports_team.school -> University of Wisconsin-Madison -> education.university.fraternities_and_sororities -> Alpha Sigma Phi\n# Answer:\nUniversity of Wisconsin-Madison"], "ground_truth": ["University of Wisconsin-Madison"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.3, "path_precision": 0.6, "path_recall": 0.2, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTrn-557_960c16ffdb29e173df0577fc76c7455d", "prediction": ["# Reasoning Path:\nWinged Monkey #7 -> film.film_character.portrayed_in_films -> m.0yj6tr7 -> film.performance.actor -> Jack Paul\n# Answer:\nm.0yj6tr7", "# Reasoning Path:\nDorothy Gale -> film.film_character.portrayed_in_films -> m.02vd7y2 -> film.performance.actor -> Romola Remus\n# Answer:\nm.02vd7y2"], "ground_truth": ["Judy Garland"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-567_693feb48c0515cd069014e7ca2846b37", "prediction": ["# Reasoning Path:\nRon Howard -> film.producer.film -> A Beautiful Mind -> media_common.netflix_title.netflix_genres -> 20th Century Period Pieces\n# Answer:\nA Beautiful Mind", "# Reasoning Path:\nRon Howard -> film.producer.film -> Dr. Seuss' How the Grinch Stole Christmas -> film.film.produced_by -> Ron Howard\n# Answer:\nDr. Seuss' How the Grinch Stole Christmas", "# Reasoning Path:\nTeklel Hafouli -> film.film_character.portrayed_in_films -> m.01064ngt -> film.performance.character -> Teklel Hafouli\n# Answer:\nm.01064ngt", "# Reasoning Path:\nRon Howard -> film.director.film -> A Beautiful Mind -> media_common.netflix_title.netflix_genres -> 20th Century Period Pieces\n# Answer:\nA Beautiful Mind", "# Reasoning Path:\nRon Howard -> film.producer.film -> Angels & Demons -> film.film.executive_produced_by -> Dan Brown\n# Answer:\nAngels & Demons"], "ground_truth": ["The Journey"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-568_d54918e8e89ad97237bce821087a9818", "prediction": ["# Reasoning Path:\nNBC Nightside -> tv.tv_program.filming_locations -> Charlotte -> location.location.containedby -> Mecklenburg County\n# Answer:\nCharlotte", "# Reasoning Path:\nNBC Nightside -> tv.tv_program.filming_locations -> Charlotte -> location.location.nearby_airports -> Charlotte Douglas International Airport\n# Answer:\nCharlotte", "# Reasoning Path:\nNBC Nightside -> tv.tv_program.country_of_origin -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nNBC Nightside -> tv.tv_program.country_of_origin -> United States of America -> location.country.administrative_divisions -> Alabama\n# Answer:\nUnited States of America"], "ground_truth": ["Mecklenburg County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTrn-60_0b79f757e511e73c7aca0bb6f6057f2d", "prediction": ["# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.languages_spoken -> Portuguese Language\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.official_language -> Portuguese Language\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.form_of_government -> Unitary state\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Mozambique -> location.country.languages_spoken -> Portuguese Language\n# Answer:\nMozambique", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Timor-Leste -> location.country.form_of_government -> Democracy\n# Answer:\nTimor-Leste"], "ground_truth": ["Macau"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-60_22027d5bb310c0d956b5f22248a6811c", "prediction": ["# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.official_language -> Portuguese Language\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.languages_spoken -> Portuguese Language\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Andorra -> location.country.languages_spoken -> Portuguese Language\n# Answer:\nAndorra", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.form_of_government -> Unitary state\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Mozambique -> location.country.languages_spoken -> Portuguese Language\n# Answer:\nMozambique"], "ground_truth": ["Macau"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-60_39ba4faa87698cb0767d1a5ee7ce1827", "prediction": ["# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.official_language -> Portuguese Language\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.languages_spoken -> Portuguese Language\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Mozambique -> location.country.languages_spoken -> Portuguese Language\n# Answer:\nMozambique", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.location.containedby -> Africa\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Mozambique -> location.country.form_of_government -> Constitutional republic\n# Answer:\nMozambique"], "ground_truth": ["South Africa"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-60_68f0d0ad309d64a4af858a5ef4fb5713", "prediction": ["# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Mozambique -> location.country.languages_spoken -> Portuguese Language\n# Answer:\nMozambique", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.administrative_divisions -> Luanda Province\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Mozambique -> location.location.containedby -> Africa\n# Answer:\nMozambique", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Mozambique -> location.country.form_of_government -> Constitutional republic\n# Answer:\nMozambique"], "ground_truth": ["Mozambique"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTrn-60_990f6babd500d25e3746174e6da58c84", "prediction": ["# Reasoning Path:\nBrazil -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Brazil\n# Answer:\nConstitutional republic", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.form_of_government -> Presidential system\n# Answer:\nAngola", "# Reasoning Path:\nBrazil -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Angola\n# Answer:\nPresidential system", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Andorra -> location.country.languages_spoken -> Spanish Language\n# Answer:\nAndorra"], "ground_truth": ["Angola"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTrn-634_000f72b60ccaedff5f056f522ee06e98", "prediction": ["# Reasoning Path:\nMansfeld -> location.location.people_born_here -> Arthur Gaebelein -> common.topic.notable_types -> Football player\n# Answer:\nArthur Gaebelein"], "ground_truth": ["Writer", "Monk", "Priest", "Physician", "Theologian", "Professor"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-662_7a992044f94b39edfc37ac5dcfcb3c26", "prediction": ["# Reasoning Path:\n2008 FIFA Club World Cup Final -> sports.sports_championship_event.champion -> Manchester United F.C. -> sports.sports_team.colors -> Red\n# Answer:\nManchester United F.C."], "ground_truth": ["Newton Heath L&YR F.C."], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-710_c264a6d11d7956741926d417b94327e2", "prediction": ["# Reasoning Path:\nLarry Baer -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale"], "ground_truth": ["2014 World Series"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-750_bb35e9b8023fbf9c05df55b4245a4775", "prediction": ["# Reasoning Path:\n1988 World Series -> time.event.locations -> Dodger Stadium -> location.location.events -> 1988 World Series\n# Answer:\nDodger Stadium", "# Reasoning Path:\n1988 World Series -> sports.sports_championship_event.champion -> Los Angeles Dodgers -> sports.sports_team.championships -> 1981 World Series\n# Answer:\nLos Angeles Dodgers", "# Reasoning Path:\n1988 World Series -> time.event.locations -> Dodger Stadium -> location.location.events -> 1963 World Series\n# Answer:\nDodger Stadium", "# Reasoning Path:\n1988 World Series -> sports.sports_championship_event.champion -> Los Angeles Dodgers -> sports.sports_team.championships -> 1963 World Series\n# Answer:\nLos Angeles Dodgers", "# Reasoning Path:\n1988 World Series -> time.event.locations -> Dodger Stadium -> location.location.events -> 1981 World Series\n# Answer:\nDodger Stadium"], "ground_truth": ["1959 World Series", "1988 World Series", "1981 World Series", "1963 World Series", "1965 World Series"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.7692307692307693, "path_precision": 1.0, "path_recall": 0.625, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 1.0, "path_ans_recall": 0.6}
{"id": "WebQTrn-810_a188aff4a054e1ec66fafba1b8021f67", "prediction": ["# Reasoning Path:\nBill Neukom -> sports.sports_team_owner.teams_owned -> San Francisco Giants -> sports.sports_team.championships -> 2014 World Series\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nBill Neukom -> sports.sports_team_owner.teams_owned -> San Francisco Giants -> sports.sports_team.championships -> 2010 World Series\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nBill Neukom -> sports.sports_team_owner.teams_owned -> San Francisco Giants -> sports.sports_team.championships -> 2012 World Series\n# Answer:\nSan Francisco Giants"], "ground_truth": ["2014 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTrn-810_c334509bb5e02cacae1ba2e80c176499", "prediction": ["# Reasoning Path:\nLou Seal -> sports.mascot.team -> San Francisco Giants -> sports.sports_team.championships -> 2014 World Series\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nLou Seal -> sports.mascot.team -> San Francisco Giants -> sports.sports_team.championships -> 2010 World Series\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nLou Seal -> sports.mascot.team -> San Francisco Giants -> sports.sports_team.championships -> 2012 World Series\n# Answer:\nSan Francisco Giants"], "ground_truth": ["2014 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTrn-810_e3d40457273785e46c5b71732713a5f4", "prediction": ["# Reasoning Path:\nCrazy Crab -> sports.mascot.team -> San Francisco Giants -> sports.sports_team.championships -> 2014 World Series\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nCrazy Crab -> sports.mascot.team -> San Francisco Giants -> sports.sports_team.championships -> 2010 World Series\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nCrazy Crab -> sports.mascot.team -> San Francisco Giants -> sports.sports_team.championships -> 2012 World Series\n# Answer:\nSan Francisco Giants"], "ground_truth": ["2014 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTrn-846_a29552911617e890ca2e1d6564e0990e", "prediction": ["# Reasoning Path:\nThe Avril Lavigne Tour -> music.concert_tour.artist -> Avril Lavigne -> people.person.place_of_birth -> Belleville\n# Answer:\nAvril Lavigne", "# Reasoning Path:\nThe Avril Lavigne Tour -> music.concert_tour.artist -> Avril Lavigne -> music.artist.origin -> Greater Napanee\n# Answer:\nAvril Lavigne", "# Reasoning Path:\nThe Avril Lavigne Tour -> time.event.people_involved -> Avril Lavigne -> people.person.place_of_birth -> Belleville\n# Answer:\nAvril Lavigne", "# Reasoning Path:\nThe Avril Lavigne Tour -> time.event.people_involved -> Avril Lavigne -> music.artist.origin -> Greater Napanee\n# Answer:\nAvril Lavigne", "# Reasoning Path:\nThe Avril Lavigne Tour -> time.event.locations -> Arena at Gwinnett Center -> location.location.events -> The Avril Lavigne Tour\n# Answer:\nArena at Gwinnett Center"], "ground_truth": ["Belleville"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTrn-849_1eb0f1ddd5074471fbe3a7f6b575f202", "prediction": ["# Reasoning Path:\nGermany -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics -> olympics.olympic_games.participating_countries -> Denmark\n# Answer:\n1896 Summer Olympics", "# Reasoning Path:\nGermany -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Denmark\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nGermany -> location.location.events -> Winter Time in Europe, 2010-2011 -> time.event.locations -> Denmark\n# Answer:\nWinter Time in Europe, 2010-2011", "# Reasoning Path:\nCopenhagen -> location.location.contains -> Amager East -> common.topic.notable_types -> Location\n# Answer:\nAmager East", "# Reasoning Path:\nCountry -> type.type.properties -> Administrative Divisions -> rdf-schema#domain -> Country\n# Answer:\nAdministrative Divisions"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.1935483870967742, "path_precision": 0.6, "path_recall": 0.11538461538461539, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTrn-849_586aae7703d62aa44eb79759e1563309", "prediction": ["# Reasoning Path:\nCountry -> base.descriptive_names.names.descriptive_name -> m.012l2b96 -> base.schemastaging.plural_form.language -> Turkish\n# Answer:\nm.012l2b96", "# Reasoning Path:\nLejre Municipality -> location.administrative_division.second_level_division_of -> Denmark -> location.location.events -> Winter Time in Europe, 2010-2011\n# Answer:\nDenmark", "# Reasoning Path:\nLejre Municipality -> location.administrative_division.country -> Denmark -> location.location.containedby -> Europe\n# Answer:\nDenmark", "# Reasoning Path:\nGermany -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Denmark\n# Answer:\nEurope", "# Reasoning Path:\nLejre Municipality -> location.location.containedby -> Denmark -> location.location.containedby -> Europe\n# Answer:\nDenmark"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.06451612903225806, "path_precision": 0.2, "path_recall": 0.038461538461538464, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTrn-857_9392f3f06e288ee4e3437a74f6bf5a37", "prediction": ["# Reasoning Path:\nRamble Elementary -> education.educational_institution.students_graduates -> m.0b6xs_g -> education.education.student -> Bill Clinton\n# Answer:\nm.0b6xs_g"], "ground_truth": ["Al Gore"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-962_032f61bfcfed69da8b215bb8f058c24e", "prediction": ["# Reasoning Path:\nJenny's Father -> film.film_character.portrayed_in_films -> m.0y54dnx -> film.performance.actor -> Kevin Mangan\n# Answer:\nm.0y54dnx", "# Reasoning Path:\nForrest Gump -> film.film_character.portrayed_in_films -> m.02xgww5 -> film.performance.film -> Forrest Gump\n# Answer:\nm.02xgww5", "# Reasoning Path:\nForrest Gump -> film.film_character.portrayed_in_films -> m.02xgww5 -> film.performance.actor -> Michael Connor Humphreys\n# Answer:\nm.02xgww5"], "ground_truth": ["Michael Connor Humphreys"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4, "path_precision": 0.3333333333333333, "path_recall": 0.5, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
