{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Costa Rica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.region -> Americas\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Louisiana\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1900 Galveston hurricane -> meteorology.tropical_cyclone.affected_areas -> Atlantic Canada\n# Answer:\n1900 Galveston hurricane"], "ground_truth": ["Jamaican English", "Jamaican Creole English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nm.04j60kh", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.district_represented -> Tennessee's 9th congressional district\n# Answer:\nm.04j60kh", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8 -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nm.04469y8", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j5sk8 -> government.government_position_held.office_position_or_title -> Governor of Tennessee\n# Answer:\nm.04j5sk8", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Representative\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> United States Representative\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> freebase.type_profile.kind -> Title\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8 -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nm.04469y8", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j5sk8 -> government.government_position_held.basic_title -> Governor\n# Answer:\nm.04j5sk8"], "ground_truth": ["Speaker of the United States House of Representatives", "Governor of Tennessee", "United States Representative"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5, "path_ans_precision": 0.4, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.main_country -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> Martinique\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.main_country -> Puerto Rico\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole -> language.human_language.main_country -> Puerto Rico\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_1j97\n# Answer:\ng.1245_1j97", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.countries_spoken_in -> Cuba\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.region -> Americas\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole -> language.human_language.countries_spoken_in -> Cuba\n# Answer:\nHaitian Creole"], "ground_truth": ["Haitian Creole", "French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1000", "prediction": ["# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.location.contains -> Line Islands\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Australia -> location.country.currency_used -> Australian dollar\n# Answer:\nAustralia", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.location.contains -> Phoenix Islands\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Australia -> location.country.languages_spoken -> English Language\n# Answer:\nAustralia", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.location.contains -> Tarawa\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.country.official_language -> English Language\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Australia -> sports.sport_country.multi_event_tournaments_participated_in -> 2010 Commonwealth Games\n# Answer:\nAustralia", "# Reasoning Path:\nOceania -> location.location.containedby -> DVD Region 4 -> location.location.contains -> South America\n# Answer:\nDVD Region 4", "# Reasoning Path:\nOceania -> location.location.contains -> Adams Seamount -> common.topic.article -> m.02q9sc5\n# Answer:\nAdams Seamount"], "ground_truth": ["Tonga", "Marshall Islands", "Solomon Islands", "Fiji", "New Zealand", "New Caledonia", "Papua New Guinea", "Kiribati", "Samoa", "Cook Islands", "Indonesia", "Guam", "Vanuatu", "Northern Mariana Islands", "Australia"], "ans_acc": 0.13333333333333333, "ans_hit": 1, "ans_f1": 0.2285714285714286, "ans_precission": 0.8, "ans_recall": 0.13333333333333333, "path_f1": 0.2285714285714286, "path_precision": 0.8, "path_recall": 0.13333333333333333, "path_ans_f1": 0.2285714285714286, "path_ans_precision": 0.8, "path_ans_recall": 0.13333333333333333}
{"id": "WebQTest-1001", "prediction": ["# Reasoning Path:\nNicki Minaj -> people.person.nationality -> United States of America -> location.country.currency_used -> United States Dollar\n# Answer:\nUnited States of America", "# Reasoning Path:\nNicki Minaj -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> New York City\n# Answer:\nUnited States of America", "# Reasoning Path:\nNicki Minaj -> people.person.place_of_birth -> Saint James, Trinidad and Tobago -> location.location.people_born_here -> Lincoln Phillips\n# Answer:\nSaint James, Trinidad and Tobago", "# Reasoning Path:\nNicki Minaj -> people.person.place_of_birth -> Saint James, Trinidad and Tobago -> common.topic.image -> Music Truck In Trinidad Carnival\n# Answer:\nSaint James, Trinidad and Tobago", "# Reasoning Path:\nNicki Minaj -> people.person.ethnicity -> African American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nAfrican American", "# Reasoning Path:\nNicki Minaj -> people.person.place_of_birth -> Saint James, Trinidad and Tobago -> common.topic.notable_for -> g.125532q8s\n# Answer:\nSaint James, Trinidad and Tobago", "# Reasoning Path:\nNicki Minaj -> people.person.ethnicity -> African American -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nAfrican American", "# Reasoning Path:\nNicki Minaj -> people.person.place_of_birth -> Saint James, Trinidad and Tobago -> location.location.people_born_here -> Siobhan Cropper\n# Answer:\nSaint James, Trinidad and Tobago", "# Reasoning Path:\nNicki Minaj -> people.person.ethnicity -> African American -> common.topic.notable_types -> Ethnicity\n# Answer:\nAfrican American", "# Reasoning Path:\nNicki Minaj -> people.person.ethnicity -> African American -> people.ethnicity.languages_spoken -> Spanish Language\n# Answer:\nAfrican American"], "ground_truth": ["United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1002", "prediction": ["# Reasoning Path:\nProfessor Albus Dumbledore -> film.film_character.portrayed_in_films -> m.03bvlq0 -> film.performance.actor -> Richard Harris\n# Answer:\nm.03bvlq0", "# Reasoning Path:\nProfessor Albus Dumbledore -> film.film_character.portrayed_in_films -> m.0jz6c8 -> film.performance.actor -> Richard Harris\n# Answer:\nm.0jz6c8", "# Reasoning Path:\nProfessor Albus Dumbledore -> film.film_character.portrayed_in_films -> m.02tb7yy -> film.performance.actor -> Michael Gambon\n# Answer:\nm.02tb7yy", "# Reasoning Path:\nProfessor Albus Dumbledore -> film.film_character.portrayed_in_films -> m.03bvlq0 -> film.performance.film -> Harry Potter and the Philosopher's Stone\n# Answer:\nm.03bvlq0", "# Reasoning Path:\nProfessor Albus Dumbledore -> film.film_character.portrayed_in_films -> m.0jz6c8 -> film.performance.film -> Harry Potter and the Chamber of Secrets\n# Answer:\nm.0jz6c8", "# Reasoning Path:\nProfessor Albus Dumbledore -> cvg.game_character.games -> m.0h57m_z -> cvg.game_performance.game -> Harry Potter and the Goblet of Fire\n# Answer:\nm.0h57m_z", "# Reasoning Path:\nProfessor Albus Dumbledore -> common.topic.notable_types -> Film character -> freebase.type_profile.kind -> Annotation\n# Answer:\nFilm character", "# Reasoning Path:\nProfessor Albus Dumbledore -> common.topic.notable_types -> Film character -> type.type.expected_by -> Character\n# Answer:\nFilm character", "# Reasoning Path:\nProfessor Albus Dumbledore -> cvg.game_character.games -> m.0h57m_z -> cvg.game_performance.performance_type -> Voice\n# Answer:\nm.0h57m_z", "# Reasoning Path:\nProfessor Albus Dumbledore -> film.film_character.portrayed_in_films -> m.02tb7yy -> film.performance.film -> Harry Potter and the Order of the Phoenix\n# Answer:\nm.02tb7yy"], "ground_truth": ["Michael Gambon", "Toby Regbo", "Dane Farwell", "Richard Harris"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.28571428571428564, "path_precision": 0.3, "path_recall": 0.2727272727272727, "path_ans_f1": 0.37499999999999994, "path_ans_precision": 0.3, "path_ans_recall": 0.5}
{"id": "WebQTest-1003", "prediction": ["# Reasoning Path:\nPrime Minister of Pakistan -> government.government_office_or_title.office_holders -> m.088yy7q -> government.government_position_held.office_holder -> Nawaz Sharif\n# Answer:\nm.088yy7q", "# Reasoning Path:\nPrime Minister of Pakistan -> government.government_office_or_title.office_holders -> m.088yy7z -> government.government_position_held.office_holder -> Nawaz Sharif\n# Answer:\nm.088yy7z", "# Reasoning Path:\nPrime Minister of Pakistan -> government.government_office_or_title.office_holders -> m.0k_kpqx -> government.government_position_held.office_holder -> Nawaz Sharif\n# Answer:\nm.0k_kpqx", "# Reasoning Path:\nPrime Minister of Pakistan -> government.government_office_or_title.office_holders -> m.088yy7q -> government.government_position_held.basic_title -> Prime minister\n# Answer:\nm.088yy7q", "# Reasoning Path:\nPrime Minister of Pakistan -> common.topic.notable_for -> g.1255j1742\n# Answer:\ng.1255j1742", "# Reasoning Path:\nPrime Minister of Pakistan -> government.government_office_or_title.office_holders -> m.088yy7q -> government.government_position_held.jurisdiction_of_office -> Pakistan\n# Answer:\nm.088yy7q", "# Reasoning Path:\nPrime Minister of Pakistan -> government.government_office_or_title.office_holders -> m.088yy7z -> government.government_position_held.basic_title -> Prime minister\n# Answer:\nm.088yy7z", "# Reasoning Path:\nPrime Minister of Pakistan -> common.topic.notable_types -> Government Office or Title -> type.type.domain -> Government\n# Answer:\nGovernment Office or Title", "# Reasoning Path:\nPrime Minister of Pakistan -> government.government_office_or_title.office_holders -> m.0k_kpqx -> government.government_position_held.jurisdiction_of_office -> Pakistan\n# Answer:\nm.0k_kpqx", "# Reasoning Path:\nPrime Minister of Pakistan -> common.topic.notable_types -> Government Office or Title -> common.topic.article -> m.02xlj1v\n# Answer:\nGovernment Office or Title"], "ground_truth": ["Moeenuddin Ahmad Qureshi", "Nawaz Sharif"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4, "path_precision": 0.3, "path_recall": 0.6, "path_ans_f1": 0.37499999999999994, "path_ans_precision": 0.3, "path_ans_recall": 0.5}
{"id": "WebQTest-1004", "prediction": ["# Reasoning Path:\nChristopher Plummer -> award.award_nominee.award_nominations -> m.010nysm3 -> award.award_nomination.award -> Primetime Emmy Award for Best Actor in a Single Performance\n# Answer:\nm.010nysm3", "# Reasoning Path:\nChristopher Plummer -> common.resource.annotations -> m.0944d05 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.0944d05", "# Reasoning Path:\nChristopher Plummer -> award.award_nominee.award_nominations -> m.07shq4d -> award.award_nomination.nominated_for -> The Big Event\n# Answer:\nm.07shq4d", "# Reasoning Path:\nChristopher Plummer -> award.award_nominee.award_nominations -> m.07cyt_y -> award.award_nomination.nominated_for -> Our Fathers\n# Answer:\nm.07cyt_y", "# Reasoning Path:\nChristopher Plummer -> award.award_nominee.award_nominations -> m.010nysm3 -> freebase.valuenotation.has_value -> Nominated work\n# Answer:\nm.010nysm3", "# Reasoning Path:\nChristopher Plummer -> tv.tv_personality.tv_regular_appearances -> m.0103cmz2 -> tv.tv_regular_personal_appearance.appearance_type -> Narrator\n# Answer:\nm.0103cmz2", "# Reasoning Path:\nChristopher Plummer -> award.award_nominee.award_nominations -> m.07cyt_y -> award.award_nomination.ceremony -> 12th Screen Actors Guild Awards\n# Answer:\nm.07cyt_y", "# Reasoning Path:\nChristopher Plummer -> award.award_nominee.award_nominations -> m.07cyt_y -> award.award_nomination.award -> Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Miniseries or Television Movie\n# Answer:\nm.07cyt_y", "# Reasoning Path:\nChristopher Plummer -> tv.tv_personality.tv_regular_appearances -> m.0103cmz2 -> tv.tv_regular_personal_appearance.program -> Moguls and Movie Stars\n# Answer:\nm.0103cmz2", "# Reasoning Path:\nChristopher Plummer -> tv.tv_personality.tv_regular_appearances -> m.06jv4_f -> tv.tv_regular_personal_appearance.program -> The Final Chapter?\n# Answer:\nm.06jv4_f"], "ground_truth": ["Kali the Little Vampire", "Where the Heart Is", "Rumpelstiltskin", "Wind Across the Everglades", "Woman Wanted", "The Insider", "Wolf", "On Golden Pond", "Already Dead", "The Girl with the Dragon Tattoo", "Souvenir", "Lucky Break", "Highpoint", "Babes in Toyland", "Closing the Ring", "Stage Struck", "Secrets", "Remember", "International Velvet", "The New World", "Possessed", "Starcrash", "Up", "Syriana", "The Forger", "The Assignment", "Lock Up Your Daughters", "Triple Cross", "Blizzard", "The Boy in Blue", "The Scarlet and the Black", "Firehead", "National Treasure", "Ordeal by Innocence", "Dolores Claiborne", "Barrymore", "Battle of Britain", "Dark Descent of the Forgotten Empress", "12 Monkeys", "Five Good Years", "Shadow Dancing", "Nicholas Nickleby", "Blackheart", "Caesar and Cleopatra", "The Lake House", "The Amateur", "My Dog Tulip", "Ararat", "Priest", "Oedipus the King", "When the Circus Came to Town", "American Tragedy", "The Night of the Generals", "Star Trek VI: The Undiscovered Country", "Dracula 2000", "The Sound of Music", "Rock-a-Doodle", "Skeletons", "Riel", "Alexander", "Conduct Unbecoming", "Our Fathers", "The Man Who Would Be King", "Vampire in Venice", "Liar's Edge", "A Beautiful Mind", "A Hazard of Hearts", "Harrison Bergeron", "Must Love Dogs", "Eyewitness", "The Gnomes' Great Adventure", "Gandahar", "The Return of the Pink Panther", "The Last Station", "Dreamscape", "The Tempest", "Madeline: Lost in Paris", "A Doll's House", "Lily in Love", "Malcolm X", "9", "Muhammad Ali's Greatest Fight", "The Gospel of John", "Somewhere in Time", "The Spiral Staircase", "Night Flight", "The Legend of Sarila", "Murder by Decree", "Hanover Street", "Winchell", "The Boss' Wife", "Hector and the Search for Happiness", "Elsa & Fred", "Inside Daisy Clover", "An American Tail", "Impolite", "Hidden Agenda", "Money", "Aces High", "Full Disclosure", "The Conspiracy of Fear", "Waterloo", "The Silent Partner", "Cold Creek Manor", "Beginners", "I Love N.Y.", "The First Circle", "The Imaginarium of Doctor Parnassus", "The Day That Shook the World", "Four Minutes", "Heidi", "Silver Blaze", "Inside Man", "Little Gloria... Happy at Last", "Young Catherine", "The Fall of the Roman Empire", "Hamlet at Elsinore", "Emotional Arithmetic", "Imagine", "The Man Who Planted Trees", "Nobody Runs Forever", "Dragnet", "The Royal Hunt of the Sun", "The Clown at Midnight", "Man in the Chair", "The Dinosaur Hunter", "The Pyx", "Prototype", "A Marriage: Georgia O'Keeffe and Alfred Stieglitz"], "ans_acc": 0.015503875968992248, "ans_hit": 1, "ans_f1": 0.014388489208633093, "ans_precission": 0.1, "ans_recall": 0.007751937984496124, "path_f1": 0.03636363636363636, "path_precision": 0.1, "path_recall": 0.022222222222222223, "path_ans_f1": 0.028776978417266185, "path_ans_precision": 0.2, "path_ans_recall": 0.015503875968992248}
{"id": "WebQTest-1005", "prediction": ["# Reasoning Path:\nMorocco -> location.country.capital -> Rabat -> common.topic.notable_types -> City/Town/Village\n# Answer:\nRabat", "# Reasoning Path:\nMorocco -> location.country.capital -> Rabat -> travel.travel_destination.tourist_attractions -> Mausoleum of Mohammed V\n# Answer:\nRabat", "# Reasoning Path:\nMorocco -> location.country.capital -> Rabat -> location.location.containedby -> Rabat-Sal\u00e9-Zemmour-Zaer\n# Answer:\nRabat", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Chaouia-Ouardigha -> location.location.time_zones -> Western European Time Zone\n# Answer:\nChaouia-Ouardigha", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Doukkala-Abda -> location.location.people_born_here -> Bouchaib Rmail\n# Answer:\nDoukkala-Abda", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Chaouia-Ouardigha -> common.topic.article -> m.0d1426\n# Answer:\nChaouia-Ouardigha", "# Reasoning Path:\nMorocco -> location.country.administrative_divisions -> Chaouia-Ouardigha -> location.location.time_zones -> Western European Time Zone\n# Answer:\nChaouia-Ouardigha", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Doukkala-Abda -> common.topic.notable_types -> Administrative Division\n# Answer:\nDoukkala-Abda", "# Reasoning Path:\nMorocco -> location.country.administrative_divisions -> Chaouia-Ouardigha -> common.topic.article -> m.0d1426\n# Answer:\nChaouia-Ouardigha", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Doukkala-Abda -> common.topic.article -> m.0d129p\n# Answer:\nDoukkala-Abda"], "ground_truth": ["Rabat"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1006", "prediction": ["# Reasoning Path:\nOmar Epps -> film.actor.film -> m.0k0669 -> film.performance.film -> Love & Basketball\n# Answer:\nm.0k0669", "# Reasoning Path:\nOmar Epps -> film.actor.film -> m.0k66h_ -> film.performance.film -> The Program\n# Answer:\nm.0k66h_", "# Reasoning Path:\nOmar Epps -> film.actor.film -> m.0k0669 -> film.performance.character -> Quincy McCall\n# Answer:\nm.0k0669", "# Reasoning Path:\nOmar Epps -> film.actor.film -> m.02vbbx0 -> film.performance.film -> The Mod Squad\n# Answer:\nm.02vbbx0", "# Reasoning Path:\nOmar Epps -> award.award_nominee.award_nominations -> m.0z9x7cw -> award.award_nomination.nominated_for -> Love & Basketball\n# Answer:\nm.0z9x7cw", "# Reasoning Path:\nOmar Epps -> award.award_nominee.award_nominations -> m.0_r73q0 -> award.award_nomination.nominated_for -> House\n# Answer:\nm.0_r73q0", "# Reasoning Path:\nOmar Epps -> film.actor.film -> m.0k66h_ -> film.performance.character -> Darnell Jefferson\n# Answer:\nm.0k66h_", "# Reasoning Path:\nOmar Epps -> award.award_nominee.award_nominations -> m.0z9x7cw -> award.award_nomination.award -> Teen Choice Award for Film - Choice Actor\n# Answer:\nm.0z9x7cw", "# Reasoning Path:\nOmar Epps -> award.award_nominee.award_nominations -> m.0_rc0gv -> award.award_nomination.nominated_for -> House\n# Answer:\nm.0_rc0gv", "# Reasoning Path:\nOmar Epps -> award.award_nominee.award_nominations -> m.0_r73q0 -> award.award_nomination.award -> NAACP Image Award for Outstanding Supporting Actor in a Drama Series\n# Answer:\nm.0_r73q0"], "ground_truth": ["Major League II", "A Day in the Life", "Breakfast of Champions", "Deadly Voyage", "Don't Be a Menace to South Central While Drinking Your Juice in the Hood", "Daybreak", "Alfie", "Juice", "The Mod Squad", "Scream 2", "Big Trouble", "Brother", "The Program", "The Wood", "Conviction", "Perfume", "First Time Felon", "Against the Ropes", "Higher Learning", "Dracula 2000", "In Too Deep", "MTV 20: Jams", "Love & Basketball"], "ans_acc": 0.13043478260869565, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2, "path_precision": 0.4, "path_recall": 0.13333333333333333, "path_ans_f1": 0.19672131147540983, "path_ans_precision": 0.4, "path_ans_recall": 0.13043478260869565}
{"id": "WebQTest-1007", "prediction": ["# Reasoning Path:\nNancy Pelosi -> people.person.place_of_birth -> Baltimore -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nNancy Pelosi -> government.politician.government_positions_held -> m.0239jrx -> government.government_position_held.district_represented -> California\u2019s 5th congressional district\n# Answer:\nm.0239jrx", "# Reasoning Path:\nNancy Pelosi -> government.politician.government_positions_held -> m.03nbxkd -> government.government_position_held.district_represented -> California\u2019s 8th congressional district\n# Answer:\nm.03nbxkd", "# Reasoning Path:\nNancy Pelosi -> government.politician.government_positions_held -> m.0239jrx -> government.government_position_held.legislative_sessions -> 100th United States Congress\n# Answer:\nm.0239jrx", "# Reasoning Path:\nNancy Pelosi -> government.politician.government_positions_held -> m.03hcgxq -> government.government_position_held.legislative_sessions -> 110th United States Congress\n# Answer:\nm.03hcgxq", "# Reasoning Path:\nNancy Pelosi -> government.politician.government_positions_held -> m.03hcgxq -> government.government_position_held.governmental_body -> United States House of Representatives\n# Answer:\nm.03hcgxq", "# Reasoning Path:\nNancy Pelosi -> government.politician.government_positions_held -> m.0239jrx -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nm.0239jrx", "# Reasoning Path:\nNancy Pelosi -> tv.tv_actor.guest_roles -> m.0bnfw3w -> tv.tv_guest_role.episodes_appeared_in -> Nancy Pelosi, Democrats Gain Control Of Congress\n# Answer:\nm.0bnfw3w", "# Reasoning Path:\nNancy Pelosi -> government.politician.government_positions_held -> m.0239jrx -> government.government_position_held.legislative_sessions -> 101st United States Congress\n# Answer:\nm.0239jrx", "# Reasoning Path:\nNancy Pelosi -> government.politician.government_positions_held -> m.03hcgxq -> government.government_position_held.legislative_sessions -> 111th United States Congress\n# Answer:\nm.03hcgxq"], "ground_truth": ["California\u2019s 12th congressional district", "California\u2019s 8th congressional district", "California\u2019s 5th congressional district"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.26666666666666666, "path_precision": 0.2, "path_recall": 0.4, "path_ans_f1": 0.30769230769230765, "path_ans_precision": 0.2, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1008", "prediction": ["# Reasoning Path:\nWalton -> location.location.containedby -> Indiana -> location.location.containedby -> Contiguous United States\n# Answer:\nIndiana", "# Reasoning Path:\nWalton -> location.location.containedby -> Cass County -> location.location.containedby -> Indiana\n# Answer:\nCass County", "# Reasoning Path:\nWalton -> location.location.containedby -> Tipton Township -> location.location.containedby -> Indiana\n# Answer:\nTipton Township", "# Reasoning Path:\nWalton -> location.location.containedby -> Indiana -> location.location.containedby -> East North Central States\n# Answer:\nIndiana", "# Reasoning Path:\nWalton -> location.location.containedby -> Indiana -> location.location.containedby -> Eastern United States\n# Answer:\nIndiana", "# Reasoning Path:\nWalton -> location.location.containedby -> Tipton Township -> location.location.containedby -> Cass County\n# Answer:\nTipton Township", "# Reasoning Path:\nWalton -> location.statistical_region.population -> g.11b66g7n0c\n# Answer:\ng.11b66g7n0c", "# Reasoning Path:\nWalton -> location.location.containedby -> Indiana -> fictional_universe.fictional_setting.contains -> Eagleton\n# Answer:\nIndiana", "# Reasoning Path:\nWalton -> location.location.containedby -> Indiana -> government.political_district.representatives -> m.05kcncx\n# Answer:\nIndiana", "# Reasoning Path:\nWalton -> location.location.containedby -> Cass County -> location.us_county.county_seat -> Logansport\n# Answer:\nCass County"], "ground_truth": ["Tipton Township", "Cass County", "United States of America", "Indiana"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.5833333333333334, "path_precision": 0.7, "path_recall": 0.5, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-1009", "prediction": ["# Reasoning Path:\nGeorge H. W. Bush -> people.person.education -> m.03k_sf2 -> education.education.institution -> University Yale\n# Answer:\nm.03k_sf2", "# Reasoning Path:\nGeorge H. W. Bush -> people.person.education -> m.0sw3d4k -> education.education.institution -> Davenport College\n# Answer:\nm.0sw3d4k", "# Reasoning Path:\nGeorge H. W. Bush -> people.person.education -> m.04hdfwr -> education.education.institution -> Phillips Academy\n# Answer:\nm.04hdfwr", "# Reasoning Path:\nGeorge H. W. Bush -> people.person.profession -> Businessperson -> common.topic.subjects -> Billy Sorrentino\n# Answer:\nBusinessperson", "# Reasoning Path:\nGeorge H. W. Bush -> government.political_appointer.appointees -> m.048zk8w -> government.government_position_held.office_holder -> James Baker\n# Answer:\nm.048zk8w", "# Reasoning Path:\nGeorge H. W. Bush -> government.political_appointer.appointees -> m.04c_v9g -> government.government_position_held.office_holder -> Wayne A. Budd\n# Answer:\nm.04c_v9g", "# Reasoning Path:\nGeorge H. W. Bush -> people.person.profession -> Businessperson -> common.topic.subjects -> Krishna Mali\n# Answer:\nBusinessperson", "# Reasoning Path:\nGeorge H. W. Bush -> people.person.profession -> Businessperson -> common.topic.subject_of -> Billy Sorrentino\n# Answer:\nBusinessperson", "# Reasoning Path:\nGeorge H. W. Bush -> people.person.profession -> Entrepreneur -> common.topic.subject_of -> Krishna Mali\n# Answer:\nEntrepreneur", "# Reasoning Path:\nGeorge H. W. Bush -> government.political_appointer.appointees -> m.048zk8w -> government.government_position_held.office_position_or_title -> White House Chief of Staff\n# Answer:\nm.048zk8w"], "ground_truth": ["University Yale"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1010", "prediction": ["# Reasoning Path:\nCaitlyn Jenner -> olympics.olympic_athlete.medals_won -> m.07ygz5n -> olympics.olympic_medal_honor.event -> Athletics at the 1976 Summer Olympics - Men's Decathlon\n# Answer:\nm.07ygz5n", "# Reasoning Path:\nCaitlyn Jenner -> olympics.olympic_athlete.medals_won -> m.07ygz5n -> olympics.olympic_medal_honor.country -> United States of America\n# Answer:\nm.07ygz5n", "# Reasoning Path:\nCaitlyn Jenner -> film.actor.film -> m.01z02gc -> film.performance.film -> Can't Stop the Music\n# Answer:\nm.01z02gc", "# Reasoning Path:\nCaitlyn Jenner -> film.actor.film -> m.0_vcp39 -> film.performance.film -> The Hungover Games\n# Answer:\nm.0_vcp39", "# Reasoning Path:\nCaitlyn Jenner -> film.actor.film -> m.0cgnzpc -> film.performance.film -> The Big Tease\n# Answer:\nm.0cgnzpc", "# Reasoning Path:\nCaitlyn Jenner -> people.person.children -> Brandon Jenner -> people.person.sibling_s -> m.010_srbf\n# Answer:\nBrandon Jenner", "# Reasoning Path:\nCaitlyn Jenner -> film.actor.film -> m.0_vcp39 -> film.performance.character -> Skip Bayflick\n# Answer:\nm.0_vcp39", "# Reasoning Path:\nCaitlyn Jenner -> people.person.children -> Burt Jenner -> common.topic.notable_types -> Person\n# Answer:\nBurt Jenner", "# Reasoning Path:\nCaitlyn Jenner -> people.person.children -> Brandon Jenner -> people.person.sibling_s -> m.0113v0_l\n# Answer:\nBrandon Jenner", "# Reasoning Path:\nCaitlyn Jenner -> people.person.children -> Brandon Jenner -> people.person.profession -> Musician\n# Answer:\nBrandon Jenner"], "ground_truth": ["Athletics at the 1976 Summer Olympics - Men's Decathlon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1011", "prediction": ["# Reasoning Path:\nStandard Tibetan -> language.human_language.countries_spoken_in -> China -> biology.breed_origin.breeds_originating_here -> Balikun horse\n# Answer:\nChina", "# Reasoning Path:\nStandard Tibetan -> base.schemastaging.context_name.pronunciation -> m.013160gt\n# Answer:\nm.013160gt", "# Reasoning Path:\nStandard Tibetan -> language.human_language.countries_spoken_in -> China -> biology.breed_origin.breeds_originating_here -> Black Telescope\n# Answer:\nChina", "# Reasoning Path:\nStandard Tibetan -> language.human_language.countries_spoken_in -> China -> location.location.partially_contains -> Amur River\n# Answer:\nChina", "# Reasoning Path:\nStandard Tibetan -> language.human_language.countries_spoken_in -> China -> biology.breed_origin.breeds_originating_here -> Chinese Chongqing Dog\n# Answer:\nChina", "# Reasoning Path:\nStandard Tibetan -> language.human_language.countries_spoken_in -> China -> location.location.partiallycontains -> m.011sdwlt\n# Answer:\nChina", "# Reasoning Path:\nStandard Tibetan -> language.human_language.main_country -> China -> biology.breed_origin.breeds_originating_here -> Balikun horse\n# Answer:\nChina", "# Reasoning Path:\nStandard Tibetan -> language.human_language.main_country -> Tibet -> location.location.containedby -> China\n# Answer:\nTibet", "# Reasoning Path:\nStandard Tibetan -> language.human_language.countries_spoken_in -> China -> location.location.partially_contains -> Argun River\n# Answer:\nChina", "# Reasoning Path:\nStandard Tibetan -> language.human_language.countries_spoken_in -> China -> location.location.partially_contains -> Arun River, China-Nepal\n# Answer:\nChina"], "ground_truth": ["China", "Tibet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1012", "prediction": ["# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Universal Studios Hollywood -> location.location.containedby -> Universal City\n# Answer:\nUniversal Studios Hollywood", "# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Universal CityWalk -> location.location.containedby -> 91608\n# Answer:\nUniversal CityWalk", "# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Universal Studios Hollywood -> location.location.events -> 16th People's Choice Awards\n# Answer:\nUniversal Studios Hollywood", "# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Dolby Theatre -> location.location.containedby -> Los Angeles\n# Answer:\nDolby Theatre", "# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Universal Studios Hollywood -> location.location.containedby -> California\n# Answer:\nUniversal Studios Hollywood", "# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Universal CityWalk -> location.location.containedby -> Los Angeles County\n# Answer:\nUniversal CityWalk", "# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Universal CityWalk -> common.topic.article -> m.025shr3\n# Answer:\nUniversal CityWalk", "# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Universal Studios Hollywood -> location.location.events -> 17th People's Choice Awards\n# Answer:\nUniversal Studios Hollywood", "# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Dolby Theatre -> location.location.containedby -> United States of America\n# Answer:\nDolby Theatre", "# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Dolby Theatre -> location.location.events -> 1st Annual Governors Awards\n# Answer:\nDolby Theatre"], "ground_truth": ["Hollywood Sign", "Hollywood Heritage Museum", "Griffith Observatory", "Universal Studios Hollywood", "Universal CityWalk", "Grauman's Egyptian Theatre", "TCL Chinese Theatre", "Hollywood Walk of Fame", "Dolby Theatre", "Hollywood Wax Museum"], "ans_acc": 0.3, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 1.0, "ans_recall": 0.3, "path_f1": 0.4615384615384615, "path_precision": 1.0, "path_recall": 0.3, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 1.0, "path_ans_recall": 0.3}
{"id": "WebQTest-1014", "prediction": ["# Reasoning Path:\nKim Kardashian -> people.person.place_of_birth -> Los Angeles -> base.popstra.location.arrestee -> m.064jg0c\n# Answer:\nLos Angeles", "# Reasoning Path:\nKim Kardashian -> people.person.place_of_birth -> Los Angeles -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nLos Angeles", "# Reasoning Path:\nKim Kardashian -> base.popstra.celebrity.dated -> m.065q3r2 -> base.popstra.dated.participant -> Nick Cannon\n# Answer:\nm.065q3r2", "# Reasoning Path:\nKim Kardashian -> base.popstra.celebrity.dated -> m.065q25_ -> base.popstra.dated.participant -> Nick Lachey\n# Answer:\nm.065q25_", "# Reasoning Path:\nKim Kardashian -> base.popstra.celebrity.dated -> m.064hndx -> base.popstra.dated.participant -> Reggie Bush\n# Answer:\nm.064hndx", "# Reasoning Path:\nKim Kardashian -> base.popstra.celebrity.vacations_in -> m.065pcc2 -> base.popstra.vacation_choice.location -> New Orleans\n# Answer:\nm.065pcc2", "# Reasoning Path:\nKim Kardashian -> base.popstra.celebrity.vacations_in -> m.064sx7c -> base.popstra.vacation_choice.location -> Monaco\n# Answer:\nm.064sx7c", "# Reasoning Path:\nKim Kardashian -> base.popstra.celebrity.vacations_in -> m.064hmxl -> base.popstra.vacation_choice.location -> Cabo San Lucas\n# Answer:\nm.064hmxl"], "ground_truth": ["Los Angeles"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1015", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> base.wikipedia_infobox.settlement.area_code -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.people_born_here -> Jeremy Bieber\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Canada\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Ontario\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.people_born_here -> Pattie Mallette\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> common.topic.notable_types -> City/Town/Village\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Canada\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nm.0102z0vx"], "ground_truth": ["Stratford", "Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.4, "ans_recall": 0.5, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1017", "prediction": ["# Reasoning Path:\nThe Santa Clause -> film.film.starring -> m.0k6ckh -> film.performance.character -> Charlie Calvin\n# Answer:\nm.0k6ckh", "# Reasoning Path:\nThe Santa Clause -> film.film.starring -> m.0k6ckh -> film.performance.actor -> Eric Lloyd\n# Answer:\nm.0k6ckh", "# Reasoning Path:\nThe Santa Clause -> film.film.starring -> m.02tbbmx -> film.performance.actor -> Peter Boyle\n# Answer:\nm.02tbbmx", "# Reasoning Path:\nThe Santa Clause -> media_common.netflix_title.netflix_genres -> Ages 11-12\n# Answer:\nAges 11-12", "# Reasoning Path:\nThe Santa Clause -> film.film.starring -> m.0dltq_0 -> film.performance.actor -> Jesse Collins\n# Answer:\nm.0dltq_0", "# Reasoning Path:\nThe Santa Clause -> common.topic.webpage -> m.09syg4y -> common.webpage.resource -> Review of Santa Clause, The by Hollis Chacona at Austin Chronicle\n# Answer:\nm.09syg4y", "# Reasoning Path:\nThe Santa Clause -> media_common.netflix_title.netflix_genres -> Comedies\n# Answer:\nComedies", "# Reasoning Path:\nThe Santa Clause -> film.film.starring -> m.02tbbmx -> film.performance.character -> Mr. Whittle\n# Answer:\nm.02tbbmx", "# Reasoning Path:\nThe Santa Clause -> film.film.starring -> m.0dltq_0 -> film.performance.character -> Ad Executive\n# Answer:\nm.0dltq_0", "# Reasoning Path:\nThe Santa Clause -> common.topic.webpage -> m.09syg4y -> common.webpage.category -> Review\n# Answer:\nm.09syg4y"], "ground_truth": ["Eric Lloyd"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t8hv2 -> tv.regular_tv_appearance.actor -> Yvonne Craig\n# Answer:\nm.02t8hv2", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t91b7 -> tv.regular_tv_appearance.actor -> Melissa Gilbert\n# Answer:\nm.02t91b7", "# Reasoning Path:\nBarbara Gordon -> common.topic.notable_for -> g.12596ymdk\n# Answer:\ng.12596ymdk", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> film.film_character.portrayed_in_films -> m.012hbb3h\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t91b7 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nm.02t91b7", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.03ljbs2 -> tv.regular_tv_appearance.actor -> Angie Harmon\n# Answer:\nm.03ljbs2", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> book.book_subject.works -> A Cold Christmas\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer -> base.descriptive_names.names.descriptive_name -> m.0101pjk9\n# Answer:\nLawyer", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer -> people.profession.specializations -> Entertainment Lawyer\n# Answer:\nLawyer", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> fictional_universe.fictional_job_title.fictional_characters_with_this_job -> m.0110s7g9\n# Answer:\nDetective"], "ground_truth": ["Ilyssa Fradin", "Hannah Gunn", "Melinda McGraw"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1020", "prediction": ["# Reasoning Path:\nR. Kelly -> people.person.education -> m.0h3355s -> education.education.institution -> Kenwood Academy\n# Answer:\nm.0h3355s", "# Reasoning Path:\nR. Kelly -> music.artist.track_contributions -> g.11bypnyn2g\n# Answer:\ng.11bypnyn2g", "# Reasoning Path:\nR. Kelly -> music.artist.track_contributions -> m.010_sxwg -> music.track_contribution.role -> Vocals\n# Answer:\nm.010_sxwg", "# Reasoning Path:\nR. Kelly -> music.artist.track_contributions -> m.01100cnr -> music.track_contribution.track -> Hotel\n# Answer:\nm.01100cnr", "# Reasoning Path:\nR. Kelly -> music.producer.tracks_produced -> Age Ain't Nothing But a Number -> music.single.versions -> Age Ain't Nothing but a Number\n# Answer:\nAge Ain't Nothing But a Number", "# Reasoning Path:\nR. Kelly -> music.producer.tracks_produced -> Age Ain't Nothing But a Number -> music.recording.releases -> Age Ain't Nothing but a Number\n# Answer:\nAge Ain't Nothing But a Number", "# Reasoning Path:\nR. Kelly -> music.artist.track_contributions -> m.010_sxwg -> music.track_contribution.track -> I'm Your Angel\n# Answer:\nm.010_sxwg", "# Reasoning Path:\nR. Kelly -> music.producer.tracks_produced -> Age Ain't Nothing But a Number -> music.recording.contributions -> m.0rqjwzt\n# Answer:\nAge Ain't Nothing But a Number", "# Reasoning Path:\nR. Kelly -> music.producer.tracks_produced -> At Your Best -> music.recording.releases -> Age Ain't Nothing but a Number\n# Answer:\nAt Your Best", "# Reasoning Path:\nR. Kelly -> music.artist.track_contributions -> m.01100cnr -> music.track_contribution.role -> Vocals\n# Answer:\nm.01100cnr"], "ground_truth": ["Kenwood Academy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1021", "prediction": ["# Reasoning Path:\nBenghazi -> location.location.events -> 2013 Benghazi conflict -> common.topic.article -> m.0w1f1w0\n# Answer:\n2013 Benghazi conflict", "# Reasoning Path:\nBenghazi -> location.location.events -> 2013 Benghazi conflict -> time.event.included_in_event -> Factional violence in Libya\n# Answer:\n2013 Benghazi conflict", "# Reasoning Path:\nBenghazi -> location.location.events -> First Battle of Benghazi -> time.event.locations -> Al Bayda'\n# Answer:\nFirst Battle of Benghazi", "# Reasoning Path:\nBenghazi -> location.location.events -> 2012 U.S. diplomatic missions attacks -> time.event.locations -> Antwerp\n# Answer:\n2012 U.S. diplomatic missions attacks", "# Reasoning Path:\nBenghazi -> location.location.events -> 2012 U.S. diplomatic missions attacks -> common.topic.article -> m.0m2t_23\n# Answer:\n2012 U.S. diplomatic missions attacks", "# Reasoning Path:\nBenghazi -> location.location.events -> 2012 U.S. diplomatic missions attacks -> time.event.included_in_event -> 2012 Benghazi attack\n# Answer:\n2012 U.S. diplomatic missions attacks", "# Reasoning Path:\nBenghazi -> location.location.events -> First Battle of Benghazi -> time.event.included_in_event -> Libyan Civil War\n# Answer:\nFirst Battle of Benghazi", "# Reasoning Path:\nBenghazi -> location.location.events -> 2012 U.S. diplomatic missions attacks -> time.event.locations -> Cairo\n# Answer:\n2012 U.S. diplomatic missions attacks", "# Reasoning Path:\nBenghazi -> location.location.events -> First Battle of Benghazi -> common.topic.notable_for -> g.1255z37qq\n# Answer:\nFirst Battle of Benghazi", "# Reasoning Path:\nBenghazi -> location.location.events -> 2012 U.S. diplomatic missions attacks -> time.event.locations -> Chennai\n# Answer:\n2012 U.S. diplomatic missions attacks"], "ground_truth": ["2012 U.S. diplomatic missions attacks", "2013 Benghazi conflict", "First Battle of Benghazi", "Operation Dignity Battle", "2012 Benghazi attack", "Battle of Benina Airport", "Second Battle of Benghazi"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 1.0, "ans_recall": 0.42857142857142855, "path_f1": 0.6, "path_precision": 1.0, "path_recall": 0.42857142857142855, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 1.0, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-1022", "prediction": ["# Reasoning Path:\nNike, Inc. -> base.schemastaging.organization_extra.phone_number -> m.0s95m1v -> base.schemastaging.phone_sandbox.service_location -> United States of America\n# Answer:\nm.0s95m1v", "# Reasoning Path:\nNike, Inc. -> organization.organization.headquarters -> m.09rzrss -> location.mailing_address.country -> United States of America\n# Answer:\nm.09rzrss", "# Reasoning Path:\nNike, Inc. -> base.schemastaging.organization_extra.phone_number -> m.0pm55rd -> base.schemastaging.phone_sandbox.service_location -> United States of America\n# Answer:\nm.0pm55rd", "# Reasoning Path:\nNike, Inc. -> base.schemastaging.organization_extra.phone_number -> m.0s95m8k -> base.schemastaging.phone_sandbox.service_location -> United States of America\n# Answer:\nm.0s95m8k", "# Reasoning Path:\nNike, Inc. -> organization.organization.headquarters -> m.09rzrss -> location.mailing_address.citytown -> Beaverton\n# Answer:\nm.09rzrss", "# Reasoning Path:\nNike, Inc. -> base.schemastaging.organization_extra.phone_number -> m.0s95m1v -> base.schemastaging.phone_sandbox.service_location -> Earth\n# Answer:\nm.0s95m1v", "# Reasoning Path:\nNike, Inc. -> base.schemastaging.organization_extra.phone_number -> m.0s95m1v -> base.schemastaging.phone_sandbox.service_language -> English Language\n# Answer:\nm.0s95m1v", "# Reasoning Path:\nNike, Inc. -> base.schemastaging.organization_extra.phone_number -> m.0s95m1v -> base.schemastaging.phone_sandbox.contact_category -> Media relations\n# Answer:\nm.0s95m1v", "# Reasoning Path:\nNike, Inc. -> organization.organization.headquarters -> m.09rzrss -> location.mailing_address.postal_code -> 97005\n# Answer:\nm.09rzrss", "# Reasoning Path:\nNike, Inc. -> base.schemastaging.organization_extra.phone_number -> m.0pm55rd -> base.schemastaging.phone_sandbox.caller_category -> Consumer Service\n# Answer:\nm.0pm55rd"], "ground_truth": ["United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1024", "prediction": ["# Reasoning Path:\nMexico -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> North America\n# Answer:\nEarth", "# Reasoning Path:\nMexico -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> United States of America\n# Answer:\nEarth", "# Reasoning Path:\nMexico -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> Sovereign state\n# Answer:\nEarth", "# Reasoning Path:\nMexico -> location.location.containedby -> Latin America -> location.location.containedby -> Americas\n# Answer:\nLatin America", "# Reasoning Path:\nMexico -> location.location.containedby -> Latin America -> location.location.contains -> Cuauht\u00e9moc, D.F.\n# Answer:\nLatin America", "# Reasoning Path:\nMexico -> location.location.containedby -> Americas -> location.location.contains -> Latin America\n# Answer:\nAmericas", "# Reasoning Path:\nMexico -> location.location.containedby -> North America -> base.locations.continents.countries_within -> Guatemala\n# Answer:\nNorth America", "# Reasoning Path:\nMexico -> location.location.containedby -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nMexico -> location.location.containedby -> Americas -> location.location.contains -> Guatemala\n# Answer:\nAmericas", "# Reasoning Path:\nMexico -> location.location.containedby -> Latin America -> organization.organization_scope.organizations_with_this_scope -> AMREP Mexico\n# Answer:\nLatin America"], "ground_truth": ["Mexican peso"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1025", "prediction": ["# Reasoning Path:\nSteve Spurrier -> sports.pro_athlete.teams -> m.0hpcpht -> sports.sports_team_roster.team -> San Francisco 49ers\n# Answer:\nm.0hpcpht", "# Reasoning Path:\nSteve Spurrier -> sports.pro_athlete.teams -> m.0hpcphg -> sports.sports_team_roster.team -> Tampa Bay Buccaneers\n# Answer:\nm.0hpcphg", "# Reasoning Path:\nSteve Spurrier -> sports.pro_athlete.teams -> m.0hpcpht -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hpcpht", "# Reasoning Path:\nSteve Spurrier -> sports.pro_athlete.teams -> m.0hpcn0z -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nm.0hpcn0z", "# Reasoning Path:\nSteve Spurrier -> people.person.education -> m.03p7x9l -> education.education.institution -> University of Florida\n# Answer:\nm.03p7x9l", "# Reasoning Path:\nSteve Spurrier -> sports.pro_athlete.teams -> m.0hpcphg -> sports.sports_team_roster.position -> Punter\n# Answer:\nm.0hpcphg", "# Reasoning Path:\nSteve Spurrier -> people.person.profession -> Athlete -> people.profession.specializations -> Alpine skier\n# Answer:\nAthlete", "# Reasoning Path:\nSteve Spurrier -> people.person.profession -> Athlete -> common.topic.notable_types -> Profession\n# Answer:\nAthlete", "# Reasoning Path:\nSteve Spurrier -> people.person.education -> m.0n1lxwh -> education.education.institution -> Science Hill High School\n# Answer:\nm.0n1lxwh", "# Reasoning Path:\nSteve Spurrier -> sports.pro_athlete.teams -> m.0hpcphg -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.0hpcphg"], "ground_truth": ["San Francisco 49ers", "Tampa Bay Buccaneers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1027", "prediction": ["# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> United Center -> location.location.events -> 2002 NCAA Men's Division I Basketball Tournament\n# Answer:\nUnited Center", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> United Center -> sports.sports_facility.teams -> Chicago Bulls\n# Answer:\nUnited Center", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> United Center -> business.sponsored_recipient.sponsorship -> m.05_g6fp\n# Answer:\nUnited Center", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> United Center -> location.location.events -> 2002 NCAA Men's Division I Basketball Tournament- East Regional First Round\n# Answer:\nUnited Center", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> Chicago Stadium -> location.location.containedby -> 60612\n# Answer:\nChicago Stadium", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> United Center -> location.location.events -> 2002 NCAA Men's Division I Basketball Tournament- East Regional Second Round\n# Answer:\nUnited Center", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.location -> Chicago -> travel.travel_destination.tourist_attractions -> Adler Planetarium\n# Answer:\nChicago", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> Chicago Stadium -> location.location.containedby -> Cook County\n# Answer:\nChicago Stadium", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> Chicago Stadium -> base.politicalconventions.convention_venue.presidential_nomination_conventions_hosted -> 1932 Democratic National Convention\n# Answer:\nChicago Stadium", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.location -> Chicago -> travel.travel_destination.tourist_attractions -> Cadillac Palace Theatre\n# Answer:\nChicago"], "ground_truth": ["United Center"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1028", "prediction": ["# Reasoning Path:\nJohn Connor -> film.film_character.portrayed_in_films -> m.04m5b3q -> film.performance.actor -> Christian Bale\n# Answer:\nm.04m5b3q", "# Reasoning Path:\nJohn Connor -> film.film_character.portrayed_in_films -> m.02vbxnf -> film.performance.actor -> Edward Furlong\n# Answer:\nm.02vbxnf", "# Reasoning Path:\nJohn Connor -> film.film_character.portrayed_in_films -> m.04m5b3q -> film.performance.film -> Terminator Salvation\n# Answer:\nm.04m5b3q", "# Reasoning Path:\nJohn Connor -> film.film_character.portrayed_in_films -> m.03n9_pb -> film.performance.actor -> Nick Stahl\n# Answer:\nm.03n9_pb", "# Reasoning Path:\nJohn Connor -> film.film_character.portrayed_in_films -> m.02vbxnf -> film.performance.film -> T2 3-D:Battle Across Time\n# Answer:\nm.02vbxnf", "# Reasoning Path:\nJohn Connor -> common.topic.webpage -> m.09w5r38 -> common.webpage.resource -> Christian Bale to star in 'Terminator'\n# Answer:\nm.09w5r38", "# Reasoning Path:\nJohn Connor -> film.film_character.portrayed_in_films -> m.03n9_pb -> film.performance.film -> Terminator 3: Rise of the Machines\n# Answer:\nm.03n9_pb", "# Reasoning Path:\nJohn Connor -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Terminator -> fictional_universe.fictional_universe.works_set_here -> Skynet\n# Answer:\nTerminator", "# Reasoning Path:\nJohn Connor -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Terminator -> people.person.gender -> Male\n# Answer:\nTerminator", "# Reasoning Path:\nJohn Connor -> common.topic.webpage -> m.09wd0s2 -> common.webpage.resource -> 'Terminator Salvation': The shocking, bummer of an ending you didn't see!\n# Answer:\nm.09wd0s2"], "ground_truth": ["Edward Furlong", "Christian Bale", "Michael Edwards", "Dalton Abbott", "Nick Stahl"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3529411764705882, "path_precision": 0.3, "path_recall": 0.42857142857142855, "path_ans_f1": 0.48, "path_ans_precision": 0.4, "path_ans_recall": 0.6}
{"id": "WebQTest-1029", "prediction": ["# Reasoning Path:\nMexico -> location.country.languages_spoken -> Awakateko Language -> language.human_language.main_country -> Guatemala\n# Answer:\nAwakateko Language", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Awakateko Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nAwakateko Language", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Awakateko Language -> common.topic.notable_types -> Human Language\n# Answer:\nAwakateko Language", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_0s31\n# Answer:\ng.1245_0s31", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Guatemala\n# Answer:\nSpanish Language", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.11b60v4rn6\n# Answer:\ng.11b60v4rn6", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Spanish Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Chichimeca Jonaz language -> language.human_language.region -> Americas\n# Answer:\nChichimeca Jonaz language", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_38p7\n# Answer:\ng.1245_38p7"], "ground_truth": ["Ch\u2019ol language", "Tzeltal language", "O'odham language", "Chochotec Language", "Cuicatec, Teutila Language", "K'iche' language", "Chontal, Tabasco Language", "Tataltepec Chatino Language", "Chichimeca Jonaz language", "Pima Bajo Language", "Mixtecan languages", "Mazatecan languages", "Huamelula Language", "Mayo Language", "Chuj language", "Matlatzinca language", "Tepehua languages", "Chinantecan languages", "Ixil, San Juan Cotzal Language", "Awakateko Language", "Ixcatec Language", "Maya, Yucat\u00e1n Language", "Pur\u00e9pecha language", "Nahuatl languages", "Italian Language", "Pame language", "Tzotzil language", "Tojolabal Language", "Spanish Language", "Seri Language", "Jakaltek language", "Huave language", "Huastec, Veracruz Language", "Texistepec language", "Yaqui Language", "Q'eqchi' Language", "Tarahumara language", "Huichol Language", "Chicomuceltec Language", "Cora, El Nayar Language", "Cocopah Language", "Mocho Language", "Trique language", "Paipai Language", "Kumiai Language", "Tepehuan, Southeastern Language", "Guerrero Amuzgo language", "Kiliwa Language", "Huarijio Language", "Q\u2019anjob\u2019al language", "Tlapanec, Azoy\u00fa Language", "Cochimi Language", "Totonac, Highland Language", "Lacandon Language", "Mazahua language", "Mixe languages", "Otomi language", "Kickapoo Language"], "ans_acc": 0.05172413793103448, "ans_hit": 1, "ans_f1": 0.09633027522935779, "ans_precission": 0.7, "ans_recall": 0.05172413793103448, "path_f1": 0.09633027522935779, "path_precision": 0.7, "path_recall": 0.05172413793103448, "path_ans_f1": 0.09633027522935779, "path_ans_precision": 0.7, "path_ans_recall": 0.05172413793103448}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nm.04qjyzm", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qpksh -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nm.04qpksh", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.season -> 1992\u201393 NBA season\n# Answer:\nm.04qjyzm", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nm.04q9532", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.team -> Orlando Magic\n# Answer:\nm.02kbc3w", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qpksh -> basketball.basketball_player_stats.season -> 1994\u201395 NBA season\n# Answer:\nm.04qpksh", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player -> common.topic.notable_for -> g.125ctmqvt\n# Answer:\nPlayer", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.season -> 2003\u201304 NBA season\n# Answer:\nm.04q9532", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc58 -> sports.sports_award.team -> Los Angeles Lakers\n# Answer:\nm.02kbc58", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player -> music.recording.releases -> You cant STOP the REIGN\n# Answer:\nPlayer"], "ground_truth": ["Orlando Magic", "Miami Heat", "Cleveland Cavaliers", "LSU Tigers men's basketball", "Phoenix Suns", "Los Angeles Lakers", "Boston Celtics"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.5, "path_recall": 0.16666666666666666, "path_ans_f1": 0.36363636363636365, "path_ans_precision": 0.5, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-1030", "prediction": ["# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Europeans -> people.ethnicity.included_in_group -> White people\n# Answer:\nEuropeans", "# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Europeans -> people.ethnicity.includes_groups -> Czech American\n# Answer:\nEuropeans", "# Reasoning Path:\nItalian people -> common.topic.image -> Italian -> base.rosetta.local_name.languoid -> Italian Language\n# Answer:\nItalian", "# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Europeans -> people.ethnicity.includes_groups -> Danes\n# Answer:\nEuropeans", "# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Latin European peoples -> people.ethnicity.includes_groups -> Andalusians\n# Answer:\nLatin European peoples", "# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Latin European peoples -> common.topic.image -> LatinEuropeCountries\n# Answer:\nLatin European peoples", "# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Europeans -> people.ethnicity.languages_spoken -> Italian Language\n# Answer:\nEuropeans", "# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Europeans -> people.ethnicity.includes_groups -> Dutch people\n# Answer:\nEuropeans", "# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Europeans -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nEuropeans", "# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Latin European peoples -> common.topic.notable_for -> g.1255l0h1c\n# Answer:\nLatin European peoples"], "ground_truth": ["Latin European peoples", "Europeans"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1031", "prediction": ["# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Model -> base.lightweight.profession.similar_professions -> Actor\n# Answer:\nModel", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Model -> common.topic.notable_types -> Profession\n# Answer:\nModel", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Film director -> freebase.type_profile.published -> Published\n# Answer:\nFilm director", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Actor -> people.profession.specializations -> Voice Actor\n# Answer:\nActor", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Actor -> owl#inverseOf -> Film performances\n# Answer:\nActor", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Film director -> freebase.type_hints.included_types -> Person\n# Answer:\nFilm director", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Actor -> people.profession.specializations -> Narrator\n# Answer:\nActor", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Actor -> common.topic.subjects -> Chris Free\n# Answer:\nActor", "# Reasoning Path:\nAngelina Jolie -> award.award_nominee.award_nominations -> m.010wr37v -> award.award_nomination.nominated_for -> Maleficent\n# Answer:\nm.010wr37v", "# Reasoning Path:\nAngelina Jolie -> book.author.book_editions_published -> Notes from My Travels -> common.topic.notable_types -> Book\n# Answer:\nNotes from My Travels"], "ground_truth": ["Model", "Actor", "Screenwriter", "Voice Actor", "Film director", "Writer", "Author", "Film Producer"], "ans_acc": 0.625, "ans_hit": 1, "ans_f1": 0.5106382978723405, "ans_precission": 0.8, "ans_recall": 0.375, "path_f1": 0.5106382978723405, "path_precision": 0.8, "path_recall": 0.375, "path_ans_f1": 0.7377049180327869, "path_ans_precision": 0.9, "path_ans_recall": 0.625}
{"id": "WebQTest-1032", "prediction": ["# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0hhzbby -> film.performance.actor -> Chris Pine\n# Answer:\nm.0hhzbby", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8g09 -> film.performance.actor -> William Shatner\n# Answer:\nm.02h8g09", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8ffj -> film.performance.actor -> William Shatner\n# Answer:\nm.02h8ffj", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0hhzbby -> film.performance.film -> Star Trek Into Darkness\n# Answer:\nm.0hhzbby", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8g09 -> film.performance.film -> Star Trek VI: The Undiscovered Country\n# Answer:\nm.02h8g09", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8ffj -> film.performance.film -> Star Trek: The Motion Picture\n# Answer:\nm.02h8ffj", "# Reasoning Path:\nJames T. Kirk -> common.topic.notable_types -> Film character -> freebase.type_profile.kind -> Annotation\n# Answer:\nFilm character", "# Reasoning Path:\nJames T. Kirk -> common.topic.notable_types -> Film character -> type.type.expected_by -> Character\n# Answer:\nFilm character", "# Reasoning Path:\nJames T. Kirk -> common.topic.webpage -> m.09wcz0z -> common.webpage.resource -> Wolverines! Chris Hemsworth nabs lead in 'Red Dawn'\n# Answer:\nm.09wcz0z", "# Reasoning Path:\nJames T. Kirk -> common.topic.webpage -> m.09wcz0z -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09wcz0z"], "ground_truth": ["William Shatner"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.1904761904761905, "path_precision": 0.2, "path_recall": 0.18181818181818182, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1033", "prediction": ["# Reasoning Path:\nDana Scully -> tv.tv_character.appeared_in_tv_program -> m.02nv3nm -> tv.regular_tv_appearance.actor -> Gillian Anderson\n# Answer:\nm.02nv3nm", "# Reasoning Path:\nDana Scully -> tv.tv_character.appeared_in_tv_program -> m.02nv3nm -> tv.regular_tv_appearance.series -> The X-Files\n# Answer:\nm.02nv3nm", "# Reasoning Path:\nDana Scully -> tv.tv_character.appeared_in_tv_program -> m.02nv3nm -> tv.regular_tv_appearance.seasons -> The X-Files Season 8\n# Answer:\nm.02nv3nm", "# Reasoning Path:\nDana Scully -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The X-Files Universe -> common.topic.notable_types -> Fictional Universe\n# Answer:\nThe X-Files Universe", "# Reasoning Path:\nDana Scully -> tv.tv_character.appeared_in_tv_episodes -> m.06zyt5d -> tv.tv_guest_role.actor -> Gillian Anderson\n# Answer:\nm.06zyt5d", "# Reasoning Path:\nDana Scully -> tv.tv_character.appeared_in_tv_program -> m.02nv3nm -> tv.regular_tv_appearance.seasons -> The X-Files season 9\n# Answer:\nm.02nv3nm", "# Reasoning Path:\nDana Scully -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The X-Files Universe -> fictional_universe.fictional_universe.species -> Manitou\n# Answer:\nThe X-Files Universe", "# Reasoning Path:\nDana Scully -> tv.tv_character.appeared_in_tv_episodes -> m.06zyt5d -> tv.tv_guest_role.episodes_appeared_in -> Ghost in the Machine\n# Answer:\nm.06zyt5d", "# Reasoning Path:\nDana Scully -> tv.tv_character.appeared_in_tv_episodes -> m.06zyy_x -> tv.tv_guest_role.episodes_appeared_in -> War of the Coprophages\n# Answer:\nm.06zyy_x", "# Reasoning Path:\nDana Scully -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The X-Files Universe -> fictional_universe.fictional_universe.works_set_here -> The Lone Gunmen\n# Answer:\nThe X-Files Universe"], "ground_truth": ["Gillian Anderson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.07142857142857142, "path_precision": 0.2, "path_recall": 0.043478260869565216, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1035", "prediction": ["# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Novelist -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nNovelist", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Novelist -> people.profession.specialization_of -> Writer\n# Answer:\nNovelist", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Writer -> people.profession.specializations -> Novelist\n# Answer:\nWriter", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Novelist -> common.topic.notable_types -> Profession\n# Answer:\nNovelist", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Writer -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nWriter", "# Reasoning Path:\nF. Scott Fitzgerald -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Male\n# Answer:\nheart attack", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Writer -> people.profession.specializations -> Screenwriter\n# Answer:\nWriter", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Writer -> common.topic.notable_types -> Profession\n# Answer:\nWriter", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Poet -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nPoet", "# Reasoning Path:\nF. Scott Fitzgerald -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.parent_disease -> Cardiovascular disease\n# Answer:\nheart attack"], "ground_truth": ["Writer", "Novelist", "Poet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1036", "prediction": ["# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Costa Rica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.region -> Americas\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Louisiana\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1900 Galveston hurricane -> meteorology.tropical_cyclone.affected_areas -> Atlantic Canada\n# Answer:\n1900 Galveston hurricane"], "ground_truth": ["Jamaican English"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1037", "prediction": ["# Reasoning Path:\nNew York Knicks -> basketball.basketball_team.head_coach -> Derek Fisher -> basketball.basketball_player.player_statistics -> m.04q9tdk\n# Answer:\nDerek Fisher", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.coaches -> m.010r7q18 -> sports.sports_team_coach_tenure.coach -> Derek Fisher\n# Answer:\nm.010r7q18", "# Reasoning Path:\nNew York Knicks -> basketball.basketball_team.head_coach -> Derek Fisher -> tv.tv_actor.guest_roles -> m.09nch_2\n# Answer:\nDerek Fisher", "# Reasoning Path:\nNew York Knicks -> basketball.basketball_team.head_coach -> Derek Fisher -> basketball.basketball_player.player_statistics -> m.04qc3sv\n# Answer:\nDerek Fisher", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.coaches -> m.010r7q18 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.010r7q18", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.coaches -> m.0112zxyh -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0112zxyh", "# Reasoning Path:\nNew York Knicks -> basketball.basketball_team.head_coach -> Derek Fisher -> basketball.basketball_player.player_statistics -> m.04qhk3g\n# Answer:\nDerek Fisher", "# Reasoning Path:\nNew York Knicks -> basketball.basketball_team.head_coach -> Derek Fisher -> tv.tv_actor.guest_roles -> m.09nch_7\n# Answer:\nDerek Fisher", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.coaches -> m.0qt2p64 -> sports.sports_team_coach_tenure.coach -> Mike Woodson\n# Answer:\nm.0qt2p64", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.venue -> m.0wz1zjx -> sports.team_venue_relationship.venue -> Madison Square Garden\n# Answer:\nm.0wz1zjx"], "ground_truth": ["Derek Fisher"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1038", "prediction": ["# Reasoning Path:\nWesley Snipes -> people.person.profession -> Dancer -> base.descriptive_names.names.descriptive_name -> m.0102lxg3\n# Answer:\nDancer", "# Reasoning Path:\nWesley Snipes -> people.person.profession -> Dancer -> common.topic.notable_for -> g.125h67_9m\n# Answer:\nDancer", "# Reasoning Path:\nWesley Snipes -> people.person.profession -> Singer -> common.topic.notable_types -> Profession\n# Answer:\nSinger", "# Reasoning Path:\nWesley Snipes -> common.topic.notable_for -> g.1254x6509\n# Answer:\ng.1254x6509", "# Reasoning Path:\nWesley Snipes -> people.person.profession -> Dancer -> common.topic.image -> Dancers\n# Answer:\nDancer", "# Reasoning Path:\nWesley Snipes -> people.person.profession -> Actor -> people.profession.specializations -> Narrator\n# Answer:\nActor", "# Reasoning Path:\nWesley Snipes -> people.person.profession -> Dancer -> base.descriptive_names.names.descriptive_name -> m.0102lxkn\n# Answer:\nDancer", "# Reasoning Path:\nWesley Snipes -> people.person.profession -> Dancer -> base.descriptive_names.names.descriptive_name -> m.0102lxys\n# Answer:\nDancer", "# Reasoning Path:\nWesley Snipes -> tv.tv_actor.guest_roles -> m.09nmg5k -> tv.tv_guest_role.character -> Duke\n# Answer:\nm.09nmg5k", "# Reasoning Path:\nWesley Snipes -> people.person.profession -> Actor -> freebase.equivalent_topic.equivalent_type -> Film actor\n# Answer:\nActor"], "ground_truth": ["Actor", "Singer", "Businessperson", "Dancer", "Martial Artist", "Film Producer"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6153846153846154, "ans_precission": 0.8, "ans_recall": 0.5, "path_f1": 0.6153846153846154, "path_precision": 0.8, "path_recall": 0.5, "path_ans_f1": 0.6428571428571429, "path_ans_precision": 0.9, "path_ans_recall": 0.5}
{"id": "WebQTest-1039", "prediction": ["# Reasoning Path:\nSherrod Brown -> people.person.places_lived -> m.0wk6dss -> people.place_lived.location -> Avon\n# Answer:\nm.0wk6dss", "# Reasoning Path:\nSherrod Brown -> people.person.place_of_birth -> Mansfield -> location.statistical_region.population -> g.11b66g3p9g\n# Answer:\nMansfield", "# Reasoning Path:\nSherrod Brown -> people.person.place_of_birth -> Mansfield -> location.hud_foreclosure_area.total_residential_addresses -> m.07dcp5y\n# Answer:\nMansfield", "# Reasoning Path:\nSherrod Brown -> people.person.place_of_birth -> Mansfield -> location.statistical_region.population -> g.11bc8563lx\n# Answer:\nMansfield", "# Reasoning Path:\nSherrod Brown -> people.person.place_of_birth -> Mansfield -> location.location.people_born_here -> Amy Douglass\n# Answer:\nMansfield", "# Reasoning Path:\nSherrod Brown -> people.person.places_lived -> m.04hcytr -> people.place_lived.location -> Mansfield\n# Answer:\nm.04hcytr", "# Reasoning Path:\nSherrod Brown -> people.person.place_of_birth -> Mansfield -> location.statistical_region.population -> g.11x1clr1_\n# Answer:\nMansfield", "# Reasoning Path:\nSherrod Brown -> people.person.place_of_birth -> Mansfield -> location.location.people_born_here -> Antonio Graves\n# Answer:\nMansfield", "# Reasoning Path:\nSherrod Brown -> people.person.place_of_birth -> Mansfield -> location.location.people_born_here -> Bentley B. Gilbert\n# Answer:\nMansfield", "# Reasoning Path:\nSherrod Brown -> people.person.parents -> Charles Gailey Brown -> people.person.gender -> Male\n# Answer:\nCharles Gailey Brown"], "ground_truth": ["Avon", "Mansfield"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5833333333333334, "ans_precission": 0.7, "ans_recall": 0.5, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> travel.travel_destination.tourist_attractions -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> location.location.contains -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9 -> award.award_honor.award -> Emmy Award for Outstanding Variety, Music or Comedy Series\n# Answer:\nm.04fk_g9", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.04lybjz\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.image -> Mulberry Street NYC c1900 LOC 3g04637u edit\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.09wfyz1\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Chandler Muriel Bing\n# Answer:\nScottish American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9 -> award.award_honor.honored_for -> The Tonight Show with Jay Leno\n# Answer:\nm.04fk_g9", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.0n2qyfk -> award.award_honor.award -> People's Choice Award for Favorite Late Night Talk Show Host\n# Answer:\nm.0n2qyfk"], "ground_truth": ["New Rochelle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1040", "prediction": ["# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Belize\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Chile\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Achawa language -> language.human_language.language_family -> Macro-Arawakan languages\n# Answer:\nAchawa language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> Belize\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.statistical_region.gdp_nominal_per_capita -> g.11b60ywwvv\n# Answer:\ng.11b60ywwvv", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Cuba\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language"], "ground_truth": ["Ponares Language", "Yucuna Language", "Macaguaje Language", "Ticuna language", "Carijona Language", "Andaqui Language", "Desano Language", "Inga Language", "Coyaima Language", "Tunebo, Western Language", "Guahibo language", "Cocama language", "Macagu\u00e1n Language", "Guayabero Language", "Anserma Language", "Tomedes Language", "Romani, Vlax Language", "Hupd\u00eb Language", "Ocaina Language", "Piapoco Language", "Providencia Sign Language", "Catio language", "Quechua, Napo Lowland Language", "Natagaimas Language", "Tanimuca-Retuar\u00e3 Language", "Cagua Language", "P\u00e1ez language", "Kogi Language", "Tinigua language", "Malayo Language", "Kuna, Border Language", "Piratapuyo Language", "Puinave Language", "Baudo language", "Siona Language", "Tuyuca language", "Tunebo, Barro Negro Language", "Curripaco Language", "Waimaj\u00e3 Language", "Guambiano Language", "Cubeo Language", "Macuna Language", "Palenquero Language", "Chipiajes Language", "Guanano Language", "Totoro Language", "Spanish Language", "Tunebo, Angosturas Language", "Minica Huitoto", "Coxima Language", "Piaroa Language", "Bora Language", "Bar\u00ed Language", "Cumeral Language", "Runa Language", "Barasana Language", "Cof\u00e1n Language", "Muinane Language", "Inga, Jungle Language", "Yukpa Language", "Andoque Language", "Islander Creole English", "Murui Huitoto language", "Nukak language", "Carabayo Language", "Arhuaco Language", "Playero language", "Nheengatu language", "Tunebo, Central Language", "Koreguaje Language", "Cuiba language", "Colombian Sign Language", "Tama Language", "Tucano Language", "Nonuya language", "Wayuu Language", "Cams\u00e1 Language", "Omejes Language", "Siriano Language", "Achawa language", "Ember\u00e1, Northern Language", "Cabiyar\u00ed Language", "Pijao Language", "S\u00e1liba Language", "Awa-Cuaiquer Language", "Uwa language"], "ans_acc": 0.023255813953488372, "ans_hit": 1, "ans_f1": 0.04534005037783375, "ans_precission": 0.9, "ans_recall": 0.023255813953488372, "path_f1": 0.045283018867924525, "path_precision": 0.6, "path_recall": 0.023529411764705882, "path_ans_f1": 0.04534005037783375, "path_ans_precision": 0.9, "path_ans_recall": 0.023255813953488372}
{"id": "WebQTest-1041", "prediction": ["# Reasoning Path:\nUnited Kingdom -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> England\n# Answer:\nPound sterling", "# Reasoning Path:\nEngland -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> United Kingdom\n# Answer:\nPound sterling", "# Reasoning Path:\nEngland -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Scotland\n# Answer:\nPound sterling", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0hq0kmy\n# Answer:\nm.0hq0kmy", "# Reasoning Path:\nUnited Kingdom -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Kingdom of England\n# Answer:\nPound sterling", "# Reasoning Path:\nUnited Kingdom -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Northern Ireland\n# Answer:\nPound sterling", "# Reasoning Path:\nEngland -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Kingdom of England\n# Answer:\nPound sterling", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0mv3kv1\n# Answer:\nm.0mv3kv1", "# Reasoning Path:\nUnited Kingdom -> location.location.containedby -> Eurasia -> common.topic.notable_types -> Location\n# Answer:\nEurasia", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0njwhvf\n# Answer:\nm.0njwhvf"], "ground_truth": ["Pound sterling"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1042", "prediction": ["# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.0k6pxpv -> tv.regular_tv_appearance.actor -> Lacey Chabert\n# Answer:\nm.0k6pxpv", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.04d4q86 -> tv.regular_tv_appearance.actor -> Lacey Chabert\n# Answer:\nm.04d4q86", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntq_4 -> tv.regular_tv_appearance.actor -> Mila Kunis\n# Answer:\nm.02ntq_4", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.0k6pxpv -> tv.regular_tv_appearance.series -> Family Guy\n# Answer:\nm.0k6pxpv", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.04d4q86 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nm.04d4q86", "# Reasoning Path:\nMeg Griffin -> cvg.game_character.games -> m.0ypsv3n -> cvg.game_performance.voice_actor -> Mila Kunis\n# Answer:\nm.0ypsv3n", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntq_4 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.02ntq_4", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntq_4 -> tv.regular_tv_appearance.seasons -> Family Guy - Season 10\n# Answer:\nm.02ntq_4", "# Reasoning Path:\nMeg Griffin -> cvg.game_character.games -> m.0ypsv3n -> cvg.game_performance.performance_type -> Voice\n# Answer:\nm.0ypsv3n", "# Reasoning Path:\nMeg Griffin -> film.film_character.portrayed_in_films -> m.0h10hz6 -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0h10hz6"], "ground_truth": ["Lacey Chabert"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1043", "prediction": ["# Reasoning Path:\nMali -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Algeria\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nMali -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Burkina Faso\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nMali -> location.country.form_of_government -> Semi-presidential system -> common.topic.notable_types -> Form of Government\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nMali -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Cuba\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nMali -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d0tc16\n# Answer:\ng.11b6d0tc16", "# Reasoning Path:\nMali -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Cuba\n# Answer:\nUnitary state", "# Reasoning Path:\nMali -> location.country.form_of_government -> Unitary state -> common.topic.notable_types -> Form of Government\n# Answer:\nUnitary state", "# Reasoning Path:\nMali -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6flnr\n# Answer:\ng.12tb6flnr", "# Reasoning Path:\nMali -> location.country.form_of_government -> Republic -> common.topic.image -> Giovanni Battista Tiepolo 080\n# Answer:\nRepublic", "# Reasoning Path:\nMali -> location.country.form_of_government -> Republic -> base.ontologies.ontology_instance.equivalent_instances -> m.07nfqv3\n# Answer:\nRepublic"], "ground_truth": ["Republic", "Semi-presidential system", "Unitary state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1044", "prediction": ["# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nm.010dzr92", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.match -> 2014 Real Madrid CF vs. CA Osasuna football match\n# Answer:\nm.010dzr92", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzrlj -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nm.010dzrlj", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010p4jhc -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nm.010p4jhc", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w8w78v -> soccer.football_player_stats.team -> Real Madrid C.F.\n# Answer:\nm.0w8w78v", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzrlj -> soccer.football_goal.match -> 2014 Real Madrid CF vs. CA Osasuna football match\n# Answer:\nm.010dzrlj", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010p4jhc -> soccer.football_goal.match -> 2014 UEFA Champions League Final\n# Answer:\nm.010p4jhc", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w8w78v -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0w8w78v", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w9gdc1 -> soccer.football_player_stats.team -> Manchester United F.C.\n# Answer:\nm.0w9gdc1", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w8w7f0 -> soccer.football_player_stats.team -> Portugal national football team\n# Answer:\nm.0w8w7f0"], "ground_truth": ["Real Madrid C.F.", "Portugal national football team"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3333333333333333, "path_precision": 0.5, "path_recall": 0.25, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1045", "prediction": ["# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26501 -> location.location.contains -> Harmony Grove Meeting House\n# Answer:\n26501", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26501 -> location.postal_code.country -> United States of America\n# Answer:\n26501", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26508 -> location.location.geometry -> m.058x11n\n# Answer:\n26508", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26501 -> location.location.geolocation -> m.03dz41t\n# Answer:\n26501", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26507 -> common.topic.notable_for -> g.125dzdth5\n# Answer:\n26507", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26501 -> location.location.contains -> Little Indian Creek Wildlife Management Area\n# Answer:\n26501", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26508 -> common.topic.notable_types -> Postal Code\n# Answer:\n26508", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26501 -> location.location.contains -> Morgantown Lock and Dam\n# Answer:\n26501", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26507 -> location.postal_code.country -> United States of America\n# Answer:\n26507", "# Reasoning Path:\nMorgantown -> location.location.people_born_here -> Alan Henderson -> sports.pro_athlete.sports_played_professionally -> m.0c54chs\n# Answer:\nAlan Henderson"], "ground_truth": ["26504", "26508", "26505", "26502", "26506", "26501", "26507"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5806451612903225, "ans_precission": 0.9, "ans_recall": 0.42857142857142855, "path_f1": 0.5806451612903225, "path_precision": 0.9, "path_recall": 0.42857142857142855, "path_ans_f1": 0.5806451612903225, "path_ans_precision": 0.9, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1046", "prediction": ["# Reasoning Path:\nJacksonville -> location.location.time_zones -> Eastern Time Zone -> common.topic.notable_for -> g.125bn2wcf\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nJacksonville -> location.location.time_zones -> Eastern Time Zone -> common.topic.webpage -> m.03hc_rb\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nJacksonville -> location.location.time_zones -> Eastern Time Zone -> common.topic.article -> m.02hcvg\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nJacksonville -> location.location.time_zones -> Eastern Time Zone -> common.topic.article -> m.0j44rkj\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nJacksonville -> location.citytown.postal_codes -> 32099 -> location.postal_code.country -> United States of America\n# Answer:\n32099", "# Reasoning Path:\nJacksonville -> location.location.time_zones -> Eastern Time Zone -> common.topic.article -> m.0j62c65\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nJacksonville -> location.location.adjoin_s -> m.02xjbkj -> location.adjoining_relationship.adjoins -> St. Johns County\n# Answer:\nm.02xjbkj", "# Reasoning Path:\nJacksonville -> location.citytown.postal_codes -> 32099 -> common.topic.notable_for -> g.1259f0k4c\n# Answer:\n32099", "# Reasoning Path:\nJacksonville -> location.citytown.postal_codes -> 32201 -> common.topic.notable_for -> g.125dnh3dj\n# Answer:\n32201", "# Reasoning Path:\nJacksonville -> location.citytown.postal_codes -> 32099 -> common.topic.notable_types -> Postal Code\n# Answer:\n32099"], "ground_truth": ["Eastern Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1047", "prediction": ["# Reasoning Path:\nDodgeBall: A True Underdog Story -> film.film.starring -> m.02kdl29 -> film.performance.character -> Steve the Pirate\n# Answer:\nm.02kdl29", "# Reasoning Path:\nAmy Stiller -> people.person.sibling_s -> m.04dq69h -> people.sibling_relationship.sibling -> Ben Stiller\n# Answer:\nm.04dq69h", "# Reasoning Path:\nDodgeBall: A True Underdog Story -> film.film.starring -> m.03jp9kl -> film.performance.character -> Dwight\n# Answer:\nm.03jp9kl", "# Reasoning Path:\nDodgeBall: A True Underdog Story -> film.film.starring -> m.03jqjf8 -> film.performance.character -> Blazer\n# Answer:\nm.03jqjf8", "# Reasoning Path:\nDodgeBall: A True Underdog Story -> film.film.starring -> m.02kdl29 -> film.performance.actor -> Alan Tudyk\n# Answer:\nm.02kdl29", "# Reasoning Path:\nAmy Stiller -> film.actor.film -> m.063wc2g -> film.performance.character -> Flower Girl/Carol\n# Answer:\nm.063wc2g", "# Reasoning Path:\nDodgeBall: A True Underdog Story -> film.film.starring -> m.03jp9kl -> film.performance.actor -> Chris Williams\n# Answer:\nm.03jp9kl", "# Reasoning Path:\nDodgeBall: A True Underdog Story -> film.film.starring -> m.03jqjf8 -> film.performance.actor -> Brandon Molale\n# Answer:\nm.03jqjf8", "# Reasoning Path:\nDodgeBall: A True Underdog Story -> film.film.language -> English Language -> organization.organization_sector.organizations_in_this_sector -> dragon-class.com\n# Answer:\nEnglish Language", "# Reasoning Path:\nDodgeBall: A True Underdog Story -> common.topic.webpage -> m.03lsm_4 -> common.webpage.resource -> m.0bkhc4d\n# Answer:\nm.03lsm_4"], "ground_truth": ["Keno Waitress"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1048", "prediction": ["# Reasoning Path:\nHenry VII of England -> people.person.children -> Arthur, Prince of Wales -> people.person.place_of_birth -> Winchester Cathedral Priory\n# Answer:\nArthur, Prince of Wales", "# Reasoning Path:\nHenry VII of England -> people.person.children -> Arthur, Prince of Wales -> people.person.parents -> Elizabeth of York\n# Answer:\nArthur, Prince of Wales", "# Reasoning Path:\nHenry VII of England -> people.person.children -> Henry VIII of England -> people.person.children -> Mary I of England\n# Answer:\nHenry VIII of England", "# Reasoning Path:\nHenry VII of England -> common.topic.notable_for -> g.125c04cnp\n# Answer:\ng.125c04cnp", "# Reasoning Path:\nHenry VII of England -> people.person.children -> Arthur, Prince of Wales -> people.person.sibling_s -> m.04jt6xb\n# Answer:\nArthur, Prince of Wales", "# Reasoning Path:\nHenry VII of England -> people.person.children -> Henry VIII of England -> people.person.children -> Elizabeth I of England\n# Answer:\nHenry VIII of England", "# Reasoning Path:\nHenry VII of England -> people.person.children -> Edmund Tudor, Duke of Somerset -> common.topic.notable_for -> g.125fhsz99\n# Answer:\nEdmund Tudor, Duke of Somerset", "# Reasoning Path:\nHenry VII of England -> people.person.children -> Henry VIII of England -> people.person.children -> Edward VI of England\n# Answer:\nHenry VIII of England", "# Reasoning Path:\nHenry VII of England -> people.person.children -> Edmund Tudor, Duke of Somerset -> people.family_member.family -> Tudor dynasty\n# Answer:\nEdmund Tudor, Duke of Somerset", "# Reasoning Path:\nHenry VII of England -> people.person.children -> Henry VIII of England -> common.topic.webpage -> m.09x2qmh\n# Answer:\nHenry VIII of England"], "ground_truth": ["Edward Tudor", "Margaret Tudor", "Elizabeth Tudor", "Mary Tudor, Queen of France", "Arthur, Prince of Wales", "Katherine Tudor", "Henry VIII of England", "Roland de Velville", "Edmund Tudor, Duke of Somerset"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.48648648648648646, "ans_precission": 0.9, "ans_recall": 0.3333333333333333, "path_f1": 0.45000000000000007, "path_precision": 0.9, "path_recall": 0.3, "path_ans_f1": 0.48648648648648646, "path_ans_precision": 0.9, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.main_country -> Myanmar\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> media_common.netflix_genre.titles -> Shutter\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.region -> Asia\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Lao Language -> language.human_language.countries_spoken_in -> Laos\n# Answer:\nLao Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> language.human_language.region -> Asia\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yg\n# Answer:\ng.12tb6f6yg", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Mlabri Language -> common.topic.notable_types -> Human Language\n# Answer:\nMlabri Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Lao Language -> language.human_language.region -> Asia\n# Answer:\nLao Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> common.topic.notable_types -> Human Language\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Lao Language -> common.topic.notable_types -> Human Language\n# Answer:\nLao Language"], "ground_truth": ["Mon Language", "Mlabri Language", "Hmong language", "Khmer language", "Saek language", "Thai Language", "Malay, Pattani Language", "Lao Language", "Vietnamese Language", "Nyaw Language", "Cham language", "Phu Thai language", "Akha Language"], "ans_acc": 0.3076923076923077, "ans_hit": 1, "ans_f1": 0.45859872611464964, "ans_precission": 0.9, "ans_recall": 0.3076923076923077, "path_f1": 0.33333333333333337, "path_precision": 0.6, "path_recall": 0.23076923076923078, "path_ans_f1": 0.45859872611464964, "path_ans_precision": 0.9, "path_ans_recall": 0.3076923076923077}
{"id": "WebQTest-1050", "prediction": ["# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> Meg Griffin\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> Peter Griffin\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> common.topic.notable_for -> g.1255sxsvl\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> Adam West\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.works_set_here -> Family Guy Video Game!\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.0k6pxpv -> tv.regular_tv_appearance.character -> Meg Griffin\n# Answer:\nm.0k6pxpv", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.02ntq_4 -> tv.regular_tv_appearance.character -> Meg Griffin\n# Answer:\nm.02ntq_4", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.0k6pxpv -> tv.regular_tv_appearance.actor -> Lacey Chabert\n# Answer:\nm.0k6pxpv", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.010btxt8 -> tv.regular_tv_appearance.actor -> Brandon Richardson\n# Answer:\nm.010btxt8", "# Reasoning Path:\nFamily Guy -> common.topic.webpage -> m.02sfx_3 -> common.webpage.category -> Topic Webpage\n# Answer:\nm.02sfx_3"], "ground_truth": ["Meg Griffin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1051", "prediction": ["# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> common.topic.subjects -> San Francisco Sightseeing\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> base.schemastaging.context_name.pronunciation -> g.125_nh8xf\n# Answer:\ng.125_nh8xf", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> common.topic.subjects -> http://www.sanfranciscosightseeing.com/\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> location.location.containedby -> California\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> common.topic.subject_of -> Memory-Song Painted Gold: for the Blue Yusef Lateef (1920-2013)\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> location.location.containedby -> Area codes 415 and 628\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> location.location.containedby -> San Francisco Bay Area\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> common.topic.subject_of -> Robin Duke\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> common.topic.subject_of -> San Francisco SEO\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> common.topic.notable_types -> Disaster -> type.type.domain -> Event\n# Answer:\nDisaster"], "ground_truth": ["1906-04-18"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1052", "prediction": ["# Reasoning Path:\nThe Omen -> film.film.starring -> m.0k610p -> film.performance.actor -> Seamus Davey-Fitzpatrick\n# Answer:\nm.0k610p", "# Reasoning Path:\nThe Omen -> film.film.starring -> m.0k610p -> film.performance.character -> Damien Thorn\n# Answer:\nm.0k610p", "# Reasoning Path:\nThe Omen -> film.film.starring -> m.05h9v78 -> film.performance.actor -> Tomas Wooler\n# Answer:\nm.05h9v78", "# Reasoning Path:\nThe Omen -> film.film.starring -> m.05h9v78 -> film.performance.character -> Damien Thorn\n# Answer:\nm.05h9v78", "# Reasoning Path:\nThe Omen -> film.film.sequel -> Damien: Omen II -> film.film.sequel -> Omen III: The Final Conflict\n# Answer:\nDamien: Omen II", "# Reasoning Path:\nThe Omen -> film.film.starring -> m.02tb9t2 -> film.performance.character -> Keith Jennings\n# Answer:\nm.02tb9t2", "# Reasoning Path:\nThe Omen -> film.film.sequel -> Damien: Omen II -> film.film.story_by -> David Seltzer\n# Answer:\nDamien: Omen II", "# Reasoning Path:\nThe Omen -> film.film.sequel -> Damien: Omen II -> media_common.netflix_title.netflix_genres -> Cult Horror\n# Answer:\nDamien: Omen II", "# Reasoning Path:\nThe Omen -> film.film.sequel -> Damien: Omen II -> media_common.netflix_title.netflix_genres -> Cult Horror Movies\n# Answer:\nDamien: Omen II", "# Reasoning Path:\nThe Omen -> common.resource.annotations -> m.094b9pw -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.094b9pw"], "ground_truth": ["Seamus Davey-Fitzpatrick", "Tomas Wooler"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1054", "prediction": ["# Reasoning Path:\nFour Christmases -> film.film.starring -> m.06483k3 -> film.performance.actor -> Jon Favreau filmography\n# Answer:\nm.06483k3", "# Reasoning Path:\nFour Christmases -> film.film.starring -> m.06483k3 -> film.performance.character -> Denver\n# Answer:\nm.06483k3", "# Reasoning Path:\nFour Christmases -> film.film.starring -> m.06483jz -> film.performance.actor -> Dwight Yoakam\n# Answer:\nm.06483jz", "# Reasoning Path:\nFour Christmases -> film.film.starring -> m.04ddq2f -> film.performance.actor -> Reese Witherspoon\n# Answer:\nm.04ddq2f", "# Reasoning Path:\nFour Christmases -> film.film.starring -> m.06483jz -> film.performance.character -> Pastor Phil\n# Answer:\nm.06483jz", "# Reasoning Path:\nFour Christmases -> film.film.starring -> m.04ddq2f -> film.performance.character -> Kate\n# Answer:\nm.04ddq2f", "# Reasoning Path:\nFour Christmases -> film.film.costume_design_by -> Sophie Carbonell -> film.film_costumer_designer.costume_design_for_film -> All About the Benjamins\n# Answer:\nSophie Carbonell", "# Reasoning Path:\nFour Christmases -> film.film.costume_design_by -> Sophie Carbonell -> film.film_costumer_designer.costume_design_for_film -> Chasing Mavericks\n# Answer:\nSophie Carbonell", "# Reasoning Path:\nFour Christmases -> film.film.costume_design_by -> Sophie Carbonell -> common.topic.article -> m.0bb15yz\n# Answer:\nSophie Carbonell", "# Reasoning Path:\nFour Christmases -> film.film.film_set_decoration_by -> Jan Pascale -> film.film_set_designer.film_sets_designed -> A Gathering of Old Men\n# Answer:\nJan Pascale"], "ground_truth": ["Jon Favreau filmography"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1055", "prediction": ["# Reasoning Path:\nEssex -> travel.travel_destination.tourist_attractions -> Colchester Zoo -> common.topic.article -> m.05zjnk\n# Answer:\nColchester Zoo", "# Reasoning Path:\nEssex -> travel.travel_destination.tourist_attractions -> Colchester Zoo -> common.topic.image -> A Wolf at Colchester Zoo\n# Answer:\nColchester Zoo", "# Reasoning Path:\nEssex -> travel.travel_destination.tourist_attractions -> Colchester Zoo -> location.location.containedby -> Colchester\n# Answer:\nColchester Zoo", "# Reasoning Path:\nEssex -> travel.travel_destination.tourist_attractions -> Beth Chatto Gardens -> symbols.namesake.named_after -> Beth Chatto\n# Answer:\nBeth Chatto Gardens", "# Reasoning Path:\nEssex -> base.aareas.schema.administrative_area.capital -> Chelmsford -> travel.travel_destination.tourist_attractions -> Chelmsford Cathedral\n# Answer:\nChelmsford", "# Reasoning Path:\nEssex -> base.aareas.schema.administrative_area.administrative_area_type -> English non metropolitan county -> base.aareas.schema.administrative_area_type.level -> State/Province\n# Answer:\nEnglish non metropolitan county", "# Reasoning Path:\nEssex -> travel.travel_destination.tourist_attractions -> Beth Chatto Gardens -> common.topic.notable_types -> Tourist attraction\n# Answer:\nBeth Chatto Gardens", "# Reasoning Path:\nEssex -> base.aareas.schema.administrative_area.capital -> Chelmsford -> travel.travel_destination.tourist_attractions -> Essex Police Museum\n# Answer:\nChelmsford", "# Reasoning Path:\nEssex -> travel.travel_destination.tourist_attractions -> Beth Chatto Gardens -> common.topic.notable_for -> g.1255mqjcw\n# Answer:\nBeth Chatto Gardens", "# Reasoning Path:\nEssex -> travel.travel_destination.tourist_attractions -> Cudmore Grove Country Park -> common.topic.notable_for -> g.1258j1cvq\n# Answer:\nCudmore Grove Country Park"], "ground_truth": ["Marsh Farm Country Park", "RHS Garden, Hyde Hall", "Mistley Place Park", "Cudmore Grove Country Park", "Thorndon Country Park", "Waltham Abbey Royal Gunpowder Mills", "Stansted Mountfitchet Castle", "Beth Chatto Gardens", "Colchester Zoo", "Green Island Gardens"], "ans_acc": 0.3, "ans_hit": 1, "ans_f1": 0.42, "ans_precission": 0.7, "ans_recall": 0.3, "path_f1": 0.42, "path_precision": 0.7, "path_recall": 0.3, "path_ans_f1": 0.42, "path_ans_precision": 0.7, "path_ans_recall": 0.3}
{"id": "WebQTest-1056", "prediction": ["# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.roster -> m.011461z_ -> sports.sports_team_roster.player -> LeBron James\n# Answer:\nm.011461z_", "# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.roster -> m.0csn6xn -> sports.sports_team_roster.player -> LeBron James\n# Answer:\nm.0csn6xn", "# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.roster -> m.011461z_ -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.011461z_", "# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.roster -> m.0102thh3 -> sports.sports_team_roster.player -> Scotty Hopson\n# Answer:\nm.0102thh3", "# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.roster -> m.0csn6xn -> sports.sports_team_roster.position -> Shooting guard\n# Answer:\nm.0csn6xn", "# Reasoning Path:\nCleveland Cavaliers -> sports.professional_sports_team.draft_picks -> m.04_byvx -> sports.sports_league_draft_pick.player -> JJ Hickson\n# Answer:\nm.04_byvx", "# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.roster -> m.0102thh3 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0102thh3", "# Reasoning Path:\nCleveland Cavaliers -> sports.professional_sports_team.draft_picks -> m.0110q23v -> sports.sports_league_draft_pick.player -> Andrew Wiggins\n# Answer:\nm.0110q23v", "# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.coaches -> m.010qc996 -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nm.010qc996", "# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.roster -> m.0csn6xn -> sports.sports_team_roster.position -> Small forward\n# Answer:\nm.0csn6xn"], "ground_truth": ["Dion Waiters", "LeBron James", "Anthony Bennett", "Shannon Brown", "JJ Hickson", "Luke Jackson", "Andrew Wiggins"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.34782608695652173, "path_precision": 0.4, "path_recall": 0.3076923076923077, "path_ans_f1": 0.4137931034482759, "path_ans_precision": 0.4, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1057", "prediction": ["# Reasoning Path:\nAnne Frank -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Bergen-Belsen concentration camp -> location.location.containedby -> Germany\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Bergen-Belsen concentration camp -> people.place_of_interment.interred_here -> Abel Herzberg\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Bergen-Belsen concentration camp -> common.topic.image -> 450px-Anne_frank_memorial_bergen_belsen.jpg\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> people.deceased_person.place_of_death -> Bergen-Belsen concentration camp -> people.place_of_interment.interred_here -> Abel Herzberg\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Bergen-Belsen concentration camp -> people.place_of_interment.interred_here -> Abraham Asscher\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> people.deceased_person.place_of_death -> Bergen-Belsen concentration camp -> common.topic.article -> m.0139dt\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Bergen-Belsen concentration camp -> people.place_of_interment.interred_here -> Adrien de Noailles\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Bergen-Belsen concentration camp -> common.topic.image -> 610px-Bergen_Belsen_Liberation_05.jpg\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> people.deceased_person.place_of_death -> Bergen-Belsen concentration camp -> people.place_of_interment.interred_here -> Abraham Asscher\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> people.deceased_person.place_of_death -> Bergen-Belsen concentration camp -> common.topic.image -> 450px-Anne_frank_memorial_bergen_belsen.jpg\n# Answer:\nBergen-Belsen concentration camp"], "ground_truth": ["Bergen-Belsen concentration camp"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1059", "prediction": ["# Reasoning Path:\nTimor-Leste -> location.country.languages_spoken -> Bekais -> common.topic.notable_types -> Human Language\n# Answer:\nBekais", "# Reasoning Path:\nTimor-Leste -> location.country.languages_spoken -> Bekais -> common.topic.notable_for -> g.125b9xx03\n# Answer:\nBekais", "# Reasoning Path:\nTimor-Leste -> location.statistical_region.internet_users_percent_population -> g.11b60w2nsc\n# Answer:\ng.11b60w2nsc", "# Reasoning Path:\nTimor-Leste -> location.country.languages_spoken -> Makalero dialect -> common.topic.notable_for -> g.12561xxm2\n# Answer:\nMakalero dialect", "# Reasoning Path:\nTimor-Leste -> location.location.contains -> Aileu -> location.location.containedby -> Aileu District\n# Answer:\nAileu", "# Reasoning Path:\nTimor-Leste -> location.statistical_region.internet_users_percent_population -> g.1245_qfdq\n# Answer:\ng.1245_qfdq", "# Reasoning Path:\nTimor-Leste -> location.country.languages_spoken -> Makalero dialect -> common.topic.notable_types -> Human Language\n# Answer:\nMakalero dialect", "# Reasoning Path:\nTimor-Leste -> location.statistical_region.internet_users_percent_population -> g.1245_w5d4\n# Answer:\ng.1245_w5d4", "# Reasoning Path:\nTimor-Leste -> location.country.languages_spoken -> Bunak Language -> common.topic.notable_types -> Human Language\n# Answer:\nBunak Language", "# Reasoning Path:\nTimor-Leste -> location.country.languages_spoken -> Bunak Language -> base.rosetta.languoid.local_name -> Bunak\n# Answer:\nBunak Language"], "ground_truth": ["Kawaimina languages", "Portuguese Language", "Idalaka", "Makalero dialect", "Tetun Language", "Makuv'a Language", "Kemak Language", "Tukudede Language", "Fataluku Language", "English Language", "Dawan", "Makasae Language", "Bunak Language", "Indonesian Language", "Habu Language", "Mambai Language", "Galoli Language", "Tetun-Terik", "Wetarese", "Bekais"], "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0.24, "ans_precission": 0.6, "ans_recall": 0.15, "path_f1": 0.24, "path_precision": 0.6, "path_recall": 0.15, "path_ans_f1": 0.24, "path_ans_precision": 0.6, "path_ans_recall": 0.15}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Programmer\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Entrepreneur\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.parents -> Edward Zuckerberg\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Businessperson\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Social networking service -> film.film_subject.films -> Internet Rising\n# Answer:\nSocial networking service", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.parents -> Karen Kempner\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.category -> Social networking service\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> influence.influence_node.influenced -> andyforsberg\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.category -> Social Network (Consumer)\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> influence.influence_node.influenced -> Austin Cameron\n# Answer:\nMark Zuckerberg"], "ground_truth": ["Tyler Winklevoss", "Cameron Winklevoss"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1060", "prediction": ["# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> common.topic.notable_types -> Currency\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\ng.11b71mdn0q", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Belgium\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Finland\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\ng.12tb6gbb1", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37cxr\n# Answer:\ng.1hhc37cxr", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Nearco -> biology.organism.sex -> Male\n# Answer:\nNearco", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Ribot -> base.thoroughbredracing.thoroughbred_racehorse.sire -> Tenerani\n# Answer:\nRibot", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Ribot -> biology.organism.sex -> Male\n# Answer:\nRibot"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1061", "prediction": ["# Reasoning Path:\nGreek Language -> language.human_language.language_family -> Indo-European languages -> language.language_family.sub_families -> Goidelic languages\n# Answer:\nIndo-European languages", "# Reasoning Path:\nGreek Language -> base.rosetta.languoid.parent -> Attic Group -> base.rosetta.languoid.parent -> Greek Group\n# Answer:\nAttic Group", "# Reasoning Path:\nGreek Language -> language.human_language.language_family -> Indo-European languages -> common.topic.article -> m.03v0m\n# Answer:\nIndo-European languages", "# Reasoning Path:\nGreek Language -> language.human_language.language_family -> Hellenic languages -> common.topic.article -> m.05pd8n2\n# Answer:\nHellenic languages", "# Reasoning Path:\nGreek Language -> language.human_language.language_family -> Indo-European languages -> language.language_family.sub_families -> Germanic languages\n# Answer:\nIndo-European languages", "# Reasoning Path:\nGreek Language -> language.human_language.language_family -> Indo-European languages -> base.schemastaging.context_name.pronunciation -> g.125_r38g5\n# Answer:\nIndo-European languages", "# Reasoning Path:\nGreek Language -> base.rosetta.languoid.parent -> Attic Group -> base.rosetta.languoid.languoid_class -> Group\n# Answer:\nAttic Group", "# Reasoning Path:\nGreek Language -> language.human_language.language_family -> Indo-European languages -> language.language_family.sub_families -> Balto-Slavic languages\n# Answer:\nIndo-European languages", "# Reasoning Path:\nGreek Language -> language.human_language.language_family -> Hellenic languages -> language.language_family.sub_families -> Ancient Greek dialects\n# Answer:\nHellenic languages", "# Reasoning Path:\nGreek Language -> language.human_language.language_family -> Hellenic languages -> language.language_family.languages -> Ancient Macedonian language\n# Answer:\nHellenic languages"], "ground_truth": ["Attic Group"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1062", "prediction": ["# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> common.topic.notable_types -> Currency\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Belgium\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\ng.11b71mdn0q", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Finland\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.location.containedby -> Eurasia -> base.locations.continents.countries_within -> Turkey\n# Answer:\nEurasia", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\ng.12tb6gbb1", "# Reasoning Path:\nItaly -> location.location.containedby -> Eurasia -> base.locations.continents.countries_within -> France\n# Answer:\nEurasia", "# Reasoning Path:\nItaly -> location.location.containedby -> Eurasia -> location.location.events -> World War II\n# Answer:\nEurasia", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37cxr\n# Answer:\ng.1hhc37cxr"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1063", "prediction": ["# Reasoning Path:\nAudrey Hepburn -> people.person.place_of_birth -> Ixelles -> location.location.people_born_here -> Ad\u00e8le Christiaens\n# Answer:\nIxelles", "# Reasoning Path:\nAudrey Hepburn -> people.person.place_of_birth -> Ixelles -> location.location.contains -> La Cambre Abbey\n# Answer:\nIxelles", "# Reasoning Path:\nAudrey Hepburn -> people.person.place_of_birth -> Ixelles -> location.location.people_born_here -> Agn\u00e8s Varda\n# Answer:\nIxelles", "# Reasoning Path:\nAudrey Hepburn -> people.person.place_of_birth -> Ixelles -> common.topic.image -> Ixelles (Elsene) municipality in the Brussels-Capital Region\n# Answer:\nIxelles", "# Reasoning Path:\nAudrey Hepburn -> people.person.place_of_birth -> Ixelles -> location.location.people_born_here -> Alfred Bastien\n# Answer:\nIxelles", "# Reasoning Path:\nAudrey Hepburn -> people.person.place_of_birth -> Ixelles -> common.topic.image -> La Cambre Abbey in Ixelles.\n# Answer:\nIxelles", "# Reasoning Path:\nAudrey Hepburn -> award.ranked_item.appears_in_ranked_lists -> m.09p5yb2 -> award.ranking.list -> AFI's 100 Years...100 Stars\n# Answer:\nm.09p5yb2", "# Reasoning Path:\nAudrey Hepburn -> award.award_winner.awards_won -> m.059wntr -> award.award_honor.ceremony -> 47th Golden Globe Awards\n# Answer:\nm.059wntr", "# Reasoning Path:\nAudrey Hepburn -> award.award_winner.awards_won -> m.06zyf62 -> award.award_honor.award -> Presidential Medal of Freedom\n# Answer:\nm.06zyf62", "# Reasoning Path:\nAudrey Hepburn -> award.award_winner.awards_won -> m.059wntr -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nm.059wntr"], "ground_truth": ["Ixelles"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1065", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.actor -> James Earl Jones\n# Answer:\nm.03jr0x9", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zsqt -> film.performance.actor -> David Prowse\n# Answer:\nm.0j7zsqt", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.special_performance_type -> Voice\n# Answer:\nm.03jr0x9", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.film -> The Benchwarmers\n# Answer:\nm.03jr0x9", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zsqt -> film.performance.film -> The Making of Star Wars\n# Answer:\nm.0j7zsqt", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.part_of_series -> Jedi Quest\n# Answer:\nPath to Truth"], "ground_truth": ["Abraham Benrubi", "Matt Lanter", "Zac Efron", "Dr. Smoov", "James Earl Jones"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.1904761904761905, "path_precision": 0.2, "path_recall": 0.18181818181818182, "path_ans_f1": 0.20000000000000004, "path_ans_precision": 0.2, "path_ans_recall": 0.2}
{"id": "WebQTest-1067", "prediction": ["# Reasoning Path:\nDouglas MacArthur -> people.person.education -> m.04hdcyk -> education.education.institution -> United States Military Academy\n# Answer:\nm.04hdcyk", "# Reasoning Path:\nDouglas MacArthur -> people.person.education -> m.0n0mx8r -> education.education.institution -> TMI \u2014 The Episcopal School of Texas\n# Answer:\nm.0n0mx8r", "# Reasoning Path:\nDouglas MacArthur -> symbols.name_source.namesakes -> Douglas MacArthur High School -> education.educational_institution.school_type -> High school\n# Answer:\nDouglas MacArthur High School", "# Reasoning Path:\nDouglas MacArthur -> symbols.name_source.namesakes -> Douglas MacArthur High School -> organization.organization.headquarters -> m.0dhdwr7\n# Answer:\nDouglas MacArthur High School", "# Reasoning Path:\nDouglas MacArthur -> symbols.name_source.namesakes -> MacArthur Causeway -> common.topic.article -> m.09tmvf\n# Answer:\nMacArthur Causeway", "# Reasoning Path:\nDouglas MacArthur -> people.person.quotations -> A general is just as good or just as bad as the troops under his command make him. -> common.topic.notable_types -> Quotation\n# Answer:\nA general is just as good or just as bad as the troops under his command make him.", "# Reasoning Path:\nDouglas MacArthur -> symbols.name_source.namesakes -> Douglas MacArthur High School -> education.educational_institution.students_graduates -> m.012dxr1b\n# Answer:\nDouglas MacArthur High School", "# Reasoning Path:\nDouglas MacArthur -> symbols.name_source.namesakes -> MacArthur Bridge -> transportation.bridge.body_of_water_spanned -> Detroit River\n# Answer:\nMacArthur Bridge", "# Reasoning Path:\nDouglas MacArthur -> people.person.quotations -> A general is just as good or just as bad as the troops under his command make him. -> common.topic.notable_for -> g.125b9dw19\n# Answer:\nA general is just as good or just as bad as the troops under his command make him.", "# Reasoning Path:\nDouglas MacArthur -> people.person.quotations -> Age wrinkles the body. Quitting wrinkles the soul. -> common.topic.notable_types -> Quotation\n# Answer:\nAge wrinkles the body. Quitting wrinkles the soul."], "ground_truth": ["TMI \u2014 The Episcopal School of Texas", "United States Military Academy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1068", "prediction": ["# Reasoning Path:\nBarbados -> location.location.contains -> Saint Michael Parish -> location.location.people_born_here -> Rihanna\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nBarbados -> location.location.contains -> Saint Michael Parish -> location.location.people_born_here -> Anderson Sealy\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nBarbados -> location.location.contains -> Saint Michael Parish -> common.topic.webpage -> m.0b47dqc\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nBarbados -> location.location.contains -> Saint Michael Parish -> location.location.people_born_here -> Barto Bartlett\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nBarbados -> location.location.contains -> Saint Michael Parish -> common.topic.image -> Map of Barbados showing the Saint Michael parish\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nBarbados -> base.popstra.location.vacationers -> m.064_m37 -> base.popstra.vacation_choice.vacationer -> Rihanna\n# Answer:\nm.064_m37", "# Reasoning Path:\nBarbados -> location.statistical_region.official_development_assistance -> g.1hhc37pj3\n# Answer:\ng.1hhc37pj3", "# Reasoning Path:\nBarbados -> location.location.contains -> 3Ws Oval -> common.topic.article -> m.02656zh\n# Answer:\n3Ws Oval", "# Reasoning Path:\nBarbados -> base.popstra.location.vacationers -> m.06445pj -> base.popstra.vacation_choice.vacationer -> Simon Cowell\n# Answer:\nm.06445pj", "# Reasoning Path:\nBarbados -> location.statistical_region.official_development_assistance -> g.1hhc382cm\n# Answer:\ng.1hhc382cm"], "ground_truth": ["William Maynard Gomm", "Murr", "Thomas Jordan", "Eyre Sealy", "Frank Leslie Walcott", "Ron King", "Hugh Gordon Cummins", "Zeeteah Massiah", "Dwayne Stanford", "Kirk Corbin", "William T. Shorey", "William Cleeve", "Michael Stoute", "Hugh Springer", "Cynthia Rosalina La Touche Andersen", "Rickey A. Walcott", "Hal Linton", "Kyle Gibson", "Shaquana Quintyne", "Omari Eastmond", "Karen Lord", "Campbell Foster", "Pamela Lavine", "Charlie Griffith", "William Perkins", "Percy Tarilton", "Deandra Dottin", "Monica Braithwaite", "Gregory Goodridge", "Stephen Griffith", "Jim Hendy", "Alana Shipp", "Rashidi Boucher", "Barry Skeete", "Raheim Sargeant", "Brandy Fenty", "Roy Callender", "John Holder", "Romario Harewood", "Tristan Parris", "Sam Seale", "Seymour Nurse", "Kyle Hope", "Kadeem Latham", "Kyle Corbin", "Menelik Shabazz", "Arturo Tappin", "Dwight James", "Renaldo Fenty", "Kyle Mayers", "Henry Doorly", "Ronald Fenty", "George Moe", "Tony White", "Jennifer Gibbons", "Woodie Blackman", "Armando Lashley", "Richard Moody", "Christopher Codrington", "Annalee Davis", "Kerry Holder", "Greg Armstrong", "Anthony Alleyne", "Kycia Knight", "Prof Edwards", "Xavier Marcus", "Shane Dowrich", "Malcolm Marshall", "Romell Brathwaite", "Ashley Bickerton", "Richard Lavine", "Adriel Brathwaite", "Edwin Lascelles, 1st Baron Harewood", "Shawn Terry Cox", "Martin Nurse", "Goodridge Roberts", "Magnet Man", "Hilary Beckles", "Neville Goddard", "Carl Joseph", "Alison Sealy-Smith", "John Lucas", "Wilfred Wood", "Nevada Phillips", "Trevor W. Payne", "Roger Blades", "Sheridan Grosvenor", "Philo Wallace", "Mia Mottley", "Andy Straughn", "Ivor Germain", "John Richard Farre", "Winston Reid", "Renn Dickson Hampden", "Robert Callender", "Wyndham Gittens", "Priya Balachandran", "Branford Taitt", "Kemar Headley", "Jaicko", "Chai Lloyd", "Rashida Williams", "Alan Emtage", "Crystal Fenty", "Anthony Forde", "Float Woods", "Gordon Greenidge", "Robin Bynoe", "Douglas Dummett", "Micky Welch", "Kyshona Knight", "Clennell Wickham", "Adrian Chase", "Lene Hall", "Owen Arthur", "Wendell White", "Lloyd A. Thompson", "Dwayne Griffith", "George Alleyne", "James Waithe", "Omar Archer", "Marita Payne", "Shakera Reece", "Clyde Mascoll", "Frank L. White", "Colin Forde", "David Comissiong", "Jon Rubin", "Chris Braithwaite", "Anthony Kellman", "Avinash Persaud", "Jonathan Straker", "Anne Cools", "Emmerson Trotman", "Kat Flint", "Ryan Hinds", "Keith Griffith", "James Wedderburn", "Tony Cordle", "Hal Padmore", "Horace Stoute", "Craig Worrell", "Shai Hope", "Norman Forde", "Richard B. Moore", "Peter Lashley", "Mabel Keaton Staupers", "Austin Clarke", "Hugh Laing", "Nick Nanton", "Albert Beckles", "Richard Pile", "Dennis Archer", "Dadrian Forde", "Ricardo Morris", "Jomo Brathwaite", "Jackie Roberts", "Sir William Randolph Douglas", "Lionel Paul", "Glenville Lovell", "Sylvester Braithwaite", "Jason Boxhill", "Samuel Hinds", "Conrad Hunte", "Henry Honychurch Gorringe", "Dave Wilkins", "Kandy Fenty", "Jabarry Chandler", "Hadan Holligan", "Arnold Josiah Ford", "Ryan Wiggins", "Ricardo Ellcock", "George Codrington", "Denzil H. Hurley", "Javon Searles", "Louis Eugene King", "Anderson Cummins", "Jason Carter", "Robert Bailey", "Odimumba Kwamdela", "Carlos Brathwaite", "Jason Holder", "Bryan Neblett", "Diquan Adamson", "Tom Adams", "John Shepherd", "Seibert Straughn", "Frank Collymore", "Curtis Odle", "Agymah Kamau", "Don Kinch", "Cecil Foster", "Chris Jordan", "Denys Williams", "June Gibbons", "Colin Young", "Samuel Jackman Prescod", "John Goddard", "Riviere Williams", "Tim Thorogood", "Earl Maynard", "Wayne Sobers", "Andre Bourne", "Nita Barrow", "Tony Cozier", "Mario Harte", "Edward Evelyn Greaves", "Alvin Rouse", "Ella Jane New", "Ramuel Miller", "Jamal Chandler", "Richard Roett", "Hartley Alleyne", "Lunden De'Leon", "Arthur Hendy", "Tony Reid", "Billie Miller", "G. Aubrey Goodman", "John Parris", "Kirk Edwards", "Rayvon", "George Blackman", "David Holford", "Kensley Reece", "Shakera Selman", "Redd Pepper", "Sylvesta Stuart", "Renaldo Gilkes", "Ricky Hoyte", "Bentley Springer", "Lloyd Erskine Sandiford"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1069", "prediction": ["# Reasoning Path:\nChina -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> France\n# Answer:\nEarth", "# Reasoning Path:\nChina -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> Asia\n# Answer:\nEarth", "# Reasoning Path:\nChina -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> Sovereign state\n# Answer:\nEarth", "# Reasoning Path:\nChina -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> United States of America\n# Answer:\nEarth", "# Reasoning Path:\nChina -> location.country.currency_used -> Renminbi -> common.topic.notable_types -> Currency\n# Answer:\nRenminbi", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Balikun horse -> common.topic.notable_types -> Organism Classification\n# Answer:\nBalikun horse", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Black Telescope -> biology.animal_breed.breed_of -> Goldfish\n# Answer:\nBlack Telescope", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Bone-mouth -> common.topic.notable_types -> Organism Classification\n# Answer:\nBone-mouth", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Black Telescope -> biology.organism_classification.rank -> Breed\n# Answer:\nBlack Telescope"], "ground_truth": ["Renminbi"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> common.topic.notable_types -> Profession\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> fictional_universe.character_occupation.characters_with_this_occupation -> Indiana Jones\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Farmer -> common.topic.notable_types -> Profession\n# Answer:\nFarmer", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> people.profession.specialization_of -> Scientist\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Architect -> common.topic.notable_types -> Profession\n# Answer:\nArchitect", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> common.topic.notable_types -> Book -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nBook", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Polly Jefferson\n# Answer:\nPolly Jefferson", "# Reasoning Path:\nThomas Jefferson -> common.topic.notable_types -> Book -> freebase.type_hints.included_types -> Topic\n# Answer:\nBook", "# Reasoning Path:\nThomas Jefferson -> common.topic.notable_types -> Film character -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nFilm character"], "ground_truth": ["Philosopher", "Architect", "Inventor", "Lawyer", "Writer", "Author", "Teacher", "Statesman", "Archaeologist", "Farmer"], "ans_acc": 0.3, "ans_hit": 1, "ans_f1": 0.37499999999999994, "ans_precission": 0.5, "ans_recall": 0.3, "path_f1": 0.37499999999999994, "path_precision": 0.5, "path_recall": 0.3, "path_ans_f1": 0.37499999999999994, "path_ans_precision": 0.5, "path_ans_recall": 0.3}
{"id": "WebQTest-1070", "prediction": ["# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.gender -> Male\n# Answer:\nMichael Joseph Jackson, Jr.", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Paris-Michael Katherine Jackson -> people.person.parents -> Debbie Rowe\n# Answer:\nParis-Michael Katherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.nationality -> United States of America\n# Answer:\nMichael Joseph Jackson, Jr.", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Prince Michael Jackson II -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nPrince Michael Jackson II", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Prince Michael Jackson II -> freebase.valuenotation.has_no_value -> Profession\n# Answer:\nPrince Michael Jackson II", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.profession -> Actor\n# Answer:\nMichael Joseph Jackson, Jr.", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Prince Michael Jackson II -> common.topic.notable_for -> g.125dq4cts\n# Answer:\nPrince Michael Jackson II", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Prince Michael Jackson II -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nPrince Michael Jackson II", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Paris-Michael Katherine Jackson -> common.topic.article -> m.0j38d_7\n# Answer:\nParis-Michael Katherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.profession -> TV Personality\n# Answer:\nMichael Joseph Jackson, Jr."], "ground_truth": ["Prince Michael Jackson II", "Michael Joseph Jackson, Jr.", "Paris-Michael Katherine Jackson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1072", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y42 -> government.government_position_held.office_holder -> Ernest McFarland\n# Answer:\nm.04j8y42", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y46 -> government.government_position_held.office_holder -> Paul Fannin\n# Answer:\nm.04j8y46", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.0hz834l -> government.government_position_held.office_holder -> Jan Brewer\n# Answer:\nm.0hz834l", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y42 -> government.government_position_held.office_position_or_title -> Governor of Arizona\n# Answer:\nm.04j8y42", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y46 -> government.government_position_held.basic_title -> Governor\n# Answer:\nm.04j8y46", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w1gvc", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.0hz834l -> government.government_position_held.basic_title -> Governor\n# Answer:\nm.0hz834l", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.0hz834l -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nm.0hz834l", "# Reasoning Path:\nArizona -> location.statistical_region.religions -> m.04403h9 -> location.religion_percentage.religion -> Pentecostalism\n# Answer:\nm.04403h9", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.resource -> ATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!\n# Answer:\nm.09w1gvc"], "ground_truth": ["Jan Brewer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1074", "prediction": ["# Reasoning Path:\nLamar Odom -> sports.pro_athlete.teams -> m.0j2gtf_ -> sports.sports_team_roster.team -> Los Angeles Lakers\n# Answer:\nm.0j2gtf_", "# Reasoning Path:\nLamar Odom -> sports.pro_athlete.teams -> m.010b9r96 -> sports.sports_team_roster.team -> New York Knicks\n# Answer:\nm.010b9r96", "# Reasoning Path:\nLamar Odom -> sports.pro_athlete.teams -> m.0_qrcms -> sports.sports_team_roster.team -> Saski Baskonia\n# Answer:\nm.0_qrcms", "# Reasoning Path:\nLamar Odom -> basketball.basketball_player.player_statistics -> m.04qrr58 -> basketball.basketball_player_stats.team -> Los Angeles Clippers\n# Answer:\nm.04qrr58", "# Reasoning Path:\nLamar Odom -> base.schemastaging.athlete_extra.salary -> m.0j2l84k -> base.schemastaging.athlete_salary.team -> Dallas Mavericks\n# Answer:\nm.0j2l84k", "# Reasoning Path:\nLamar Odom -> basketball.basketball_player.player_statistics -> m.04q8xv5 -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nm.04q8xv5", "# Reasoning Path:\nLamar Odom -> sports.pro_athlete.teams -> m.0j2gtf_ -> sports.sports_team_roster.position -> Power forward\n# Answer:\nm.0j2gtf_", "# Reasoning Path:\nLamar Odom -> basketball.basketball_player.player_statistics -> m.04qc3tr -> basketball.basketball_player_stats.team -> Los Angeles Clippers\n# Answer:\nm.04qc3tr", "# Reasoning Path:\nLamar Odom -> base.schemastaging.athlete_extra.salary -> m.0j2l84k -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0j2l84k", "# Reasoning Path:\nLamar Odom -> basketball.basketball_player.player_statistics -> m.04qrr58 -> basketball.basketball_player_stats.season -> 1999\u20132000 NBA season\n# Answer:\nm.04qrr58"], "ground_truth": ["Saski Baskonia", "Los Angeles Clippers", "New York Knicks", "Dallas Mavericks", "Los Angeles Lakers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5384615384615384, "path_precision": 0.7, "path_recall": 0.4375, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1075", "prediction": ["# Reasoning Path:\nGreat Pyramid of Giza -> architecture.building.building_complex -> Giza Plateau -> location.location.containedby -> Giza\n# Answer:\nGiza Plateau", "# Reasoning Path:\nGreat Pyramid of Giza -> location.location.containedby -> Giza Plateau -> location.location.containedby -> Giza\n# Answer:\nGiza Plateau", "# Reasoning Path:\nGreat Pyramid of Giza -> location.location.containedby -> Egypt -> location.location.events -> Battle of al-Babein\n# Answer:\nEgypt", "# Reasoning Path:\nGreat Pyramid of Giza -> location.location.containedby -> Egypt -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nEgypt", "# Reasoning Path:\nGreat Pyramid of Giza -> architecture.building.building_complex -> Giza Plateau -> location.location.contains -> Pyramid of Menkaure\n# Answer:\nGiza Plateau", "# Reasoning Path:\nGreat Pyramid of Giza -> location.location.containedby -> Egypt -> base.militaryinfiction.location_in_fiction.contains -> Ancient Egypt\n# Answer:\nEgypt", "# Reasoning Path:\nGreat Pyramid of Giza -> architecture.building.building_complex -> Giza Plateau -> common.topic.notable_for -> g.125bpgq0f\n# Answer:\nGiza Plateau", "# Reasoning Path:\nGreat Pyramid of Giza -> location.location.containedby -> Egypt -> location.location.events -> Greco-Persian Wars\n# Answer:\nEgypt", "# Reasoning Path:\nGreat Pyramid of Giza -> location.location.containedby -> Giza Plateau -> location.location.contains -> Pyramid of Menkaure\n# Answer:\nGiza Plateau", "# Reasoning Path:\nGreat Pyramid of Giza -> location.location.containedby -> Giza Plateau -> common.topic.notable_for -> g.125bpgq0f\n# Answer:\nGiza Plateau"], "ground_truth": ["Giza Plateau"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1077", "prediction": ["# Reasoning Path:\nRussia -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.countries_spoken_in -> Kazakhstan\n# Answer:\nUkrainian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.main_country -> Ukraine\n# Answer:\nUkrainian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Russian Language -> language.human_language.countries_spoken_in -> Kazakhstan\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.countries_spoken_in -> Soviet Union\n# Answer:\nUkrainian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.region -> Europe\n# Answer:\nUkrainian Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> language.human_language.countries_spoken_in -> Kazakhstan\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Russian Language -> common.topic.notable_types -> Human Language\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.11b60vv5th\n# Answer:\ng.11b60vv5th", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.countries_spoken_in -> Ukraine\n# Answer:\nUkrainian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Russian Language -> language.human_language.region -> Eurasia\n# Answer:\nRussian Language"], "ground_truth": ["Lezgi Language", "Azerbaijani language", "Lak Language", "Erzya Language", "Ingush Language", "Nogai Language", "Ukrainian Language", "Udmurt Language", "Osetin Language", "Aghul language", "Yiddish Language", "Buryat language", "Mari language", "Rutul language", "Tatar Language", "Tabassaran Language", "Adyghe Language", "Moksha Language", "Tsakhur Language", "Crimean Turkish Language", "Bashkir Language", "Kalmyk-Oirat Language", "Altai language", "Yakut Language", "Kumyk Language", "Abaza Language", "Komi language", "Karachay-Balkar Language", "Tuvin Language", "Chechen Language", "Russian Language", "Khakas Language", "Kabardian Language", "Avar Language", "Dargwa Language"], "ans_acc": 0.05714285714285714, "ans_hit": 1, "ans_f1": 0.10746268656716418, "ans_precission": 0.9, "ans_recall": 0.05714285714285714, "path_f1": 0.1043478260869565, "path_precision": 0.6, "path_recall": 0.05714285714285714, "path_ans_f1": 0.10746268656716418, "path_ans_precision": 0.9, "path_ans_recall": 0.05714285714285714}
{"id": "WebQTest-1078", "prediction": ["# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.main_country -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\ng.11b60qdnlp", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\ng.11b60vfr15", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\ng.1245_2hrg", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> Japan\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_55mr\n# Answer:\ng.1245_55mr", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_2hqk\n# Answer:\ng.1245_2hqk"], "ground_truth": ["Lojban", "English Language", "Esperanto Language"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.5, "ans_recall": 0.6666666666666666, "path_f1": 0.5714285714285715, "path_precision": 0.5, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.5, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1079", "prediction": ["# Reasoning Path:\nKV62 -> common.topic.article -> m.01qg6v\n# Answer:\nm.01qg6v", "# Reasoning Path:\nKV62 -> common.topic.notable_types -> Location -> freebase.type_hints.included_types -> Topic\n# Answer:\nLocation", "# Reasoning Path:\nKV62 -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> The Treasures of Tutankhamun -> exhibitions.exhibition.subjects -> Tutankhamun\n# Answer:\nThe Treasures of Tutankhamun", "# Reasoning Path:\nKV62 -> common.topic.notable_types -> Location -> type.type.properties -> Adjectival form\n# Answer:\nLocation", "# Reasoning Path:\nKV62 -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Tutankhamun and the Golden Age of the Pharaohs -> exhibitions.exhibition.subjects -> Tutankhamun\n# Answer:\nTutankhamun and the Golden Age of the Pharaohs", "# Reasoning Path:\nKV62 -> common.topic.notable_types -> Location -> type.type.properties -> Adjoins\n# Answer:\nLocation", "# Reasoning Path:\nKV62 -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> The Treasures of Tutankhamun -> exhibitions.exhibition.subjects -> Amarna\n# Answer:\nThe Treasures of Tutankhamun", "# Reasoning Path:\nKV62 -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> The Treasures of Tutankhamun -> exhibitions.exhibition.exhibition_types -> Artefact exhibition\n# Answer:\nThe Treasures of Tutankhamun", "# Reasoning Path:\nKV62 -> common.topic.notable_types -> Location -> type.type.properties -> Area\n# Answer:\nLocation", "# Reasoning Path:\nKV62 -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> The Treasures of Tutankhamun -> common.topic.article -> m.03cpt8c\n# Answer:\nThe Treasures of Tutankhamun"], "ground_truth": ["Egypt"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 3: 1844-1846\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Darwin -> book.written_work.subjects -> Biology\n# Answer:\nDarwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Darwin -> book.written_work.subjects -> Evolution\n# Answer:\nDarwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nEngland"], "ground_truth": ["Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Darwin Reader First Edition", "The living thoughts of Darwin", "The Descent of Man, and Selection in Relation to Sex", "From Darwin's unpublished notebooks", "Die geschlechtliche Zuchtwahl", "To the members of the Down Friendly Club", "Notebooks on transmutation of species", "Die fundamente zur entstehung der arten", "The Structure And Distribution of Coral Reefs", "La vie et la correspondance de Charles Darwin", "The Variation of Animals and Plants under Domestication", "The Correspondence of Charles Darwin, Volume 4", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Kleinere geologische Abhandlungen", "Metaphysics, Materialism, & the evolution of mind", "Les moyens d'expression chez les animaux", "The Correspondence of Charles Darwin, Volume 8: 1860", "The autobiography of Charles Darwin, 1809-1882", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "La facult\u00e9 motrice dans les plantes", "The Correspondence of Charles Darwin, Volume 12", "Monographs of the fossil Lepadidae and the fossil Balanidae", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The Orgin of Species", "On Natural Selection", "H.M.S. Beagle in South America", "Questions about the breeding of animals", "On the origin of species by means of natural selection", "The Origin of Species (World's Classics)", "The Origin of Species (Oxford World's Classics)", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The Formation of Vegetable Mould through the Action of Worms", "The Essential Darwin", "The Correspondence of Charles Darwin, Volume 9", "ontstaan der soorten door natuurlijke teeltkeus", "The Origin of Species (Collector's Library)", "The Correspondence of Charles Darwin, Volume 1", "Diary of the voyage of H.M.S. Beagle", "vari\u00eberen der huisdieren en cultuurplanten", "The descent of man and selection in relation to sex.", "The Power of Movement in Plants", "Part I: Contributions to the Theory of Natural Selection / Part II", "The collected papers of Charles Darwin", "The Correspondence of Charles Darwin, Volume 17: 1869", "Darwin-Wallace", "The foundations of the Origin of species", "On evolution", "The Expression of the Emotions in Man and Animals", "The Correspondence of Charles Darwin, Volume 15: 1867", "The Expression of the Emotions in Man And Animals", "Voyage of the Beagle", "The Voyage of the Beagle (Unabridged Classics)", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "Volcanic Islands", "The Correspondence of Charles Darwin, Volume 18: 1870", "The descent of man, and selection in relation to sex", "Charles Darwin's marginalia", "Evolution and natural selection", "The Origin of Species (Variorum Reprint)", "The Correspondence of Charles Darwin, Volume 15", "red notebook of Charles Darwin", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Beagle letters", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Darwin en Patagonia", "From So Simple a Beginning", "Darwin", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "The principal works", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "The Correspondence of Charles Darwin, Volume 10: 1862", "The descent of man, and selection in relation to sex.", "Autobiography of Charles Darwin", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "Darwin and Henslow", "Voyage d'un naturaliste autour du monde", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "Darwin for Today", "Les mouvements et les habitudes des plantes grimpantes", "The Origin of Species", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "Fertilisation of Orchids", "The Correspondence of Charles Darwin, Volume 12: 1864", "monograph on the sub-class Cirripedia", "The Origin of Species (Great Minds Series)", "The Correspondence of Charles Darwin, Volume 3", "The expression of the emotions in man and animals.", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "Voyage of the Beagle (NG Adventure Classics)", "On the Movements and Habits of Climbing Plants", "The Autobiography of Charles Darwin [EasyRead Edition]", "The expression of the emotions in man and animals", "Darwin's Ornithological notes", "Reise um die Welt 1831 - 36", "The Voyage of the Beagle (Great Minds Series)", "Darwin's insects", "The Correspondence of Charles Darwin, Volume 11: 1863", "The Autobiography of Charles Darwin", "Leben und Briefe von Charles Darwin", "The Correspondence of Charles Darwin, Volume 10", "Origin of Species", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "The Autobiography Of Charles Darwin", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "The autobiography of Charles Darwin", "Darwin's journal", "Tesakneri tsagume\u030c", "Rejse om jorden", "Reise eines Naturforschers um die Welt", "The Correspondence of Charles Darwin, Volume 5", "Darwin Darwin", "Cartas de Darwin 18251859", "The Origin of Species (Great Books : Learning Channel)", "Charles Darwin", "The Life of Erasmus Darwin", "genese\u014ds t\u014dn eid\u014dn", "A student's introduction to Charles Darwin", "The action of carbonate of ammonia on the roots of certain plants", "The Autobiography of Charles Darwin, and selected letters", "Evolution by natural selection", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "The structure and distribution of coral reefs.", "The structure and distribution of coral reefs", "The Descent of Man and Selection in Relation to Sex", "Darwin on humus and the earthworm", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "The Correspondence of Charles Darwin, Volume 9: 1861", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "On the tendency of species to form varieties", "Del Plata a Tierra del Fuego", "The voyage of Charles Darwin", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "The\u0301orie de l'e\u0301volution", "The origin of species", "Motsa ha-minim", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "El Origin De Las Especies", "The Darwin Reader Second Edition", "The Correspondence of Charles Darwin, Volume 6", "The Origin of Species (Enriched Classics)", "Voyage of the Beagle (Harvard Classics, Part 29)", "Voyage Of The Beagle", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The Voyage of the Beagle (Adventure Classics)", "The Origin of Species (Mentor)", "A Darwin Selection", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "The Voyage of the Beagle (Mentor)", "Diario del Viaje de Un Naturalista Alrededor", "Geological Observations on South America", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "Notes on the fertilization of orchids", "The Correspondence of Charles Darwin, Volume 2", "The Structure and Distribution of Coral Reefs", "The Autobiography of Charles Darwin (Large Print)", "Het uitdrukken van emoties bij mens en dier", "The Voyage of the Beagle (Everyman Paperbacks)", "Works", "The Correspondence of Charles Darwin, Volume 16: 1868", "Gesammelte kleinere Schriften", "The origin of species : complete and fully illustrated", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Opsht\u0323amung fun menshen", "The Correspondence of Charles Darwin, Volume 7", "The education of Darwin", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "The Correspondence of Charles Darwin, Volume 8", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Darwin's notebooks on transmutation of species", "Darwinism stated by Darwin himself", "The Expression Of The Emotions In Man And Animals", "Voyage of the Beagle (Dover Value Editions)", "The portable Darwin", "The Autobiography of Charles Darwin (Great Minds Series)", "From so simple a beginning", "The Voyage of the Beagle", "The Correspondence of Charles Darwin, Volume 11", "Memorias y epistolario i\u0301ntimo", "The Autobiography of Charles Darwin (Dodo Press)", "More Letters of Charles Darwin", "Darwin Compendium", "Wu zhong qi yuan", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Origins", "Charles Darwin on the routes of male humble bees", "The Different Forms of Flowers on Plants of the Same Species", "The voyage of the Beagle.", "Human nature, Darwin's view", "On a remarkable bar of sandstone off Pernambuco", "The Origin Of Species", "The geology of the voyage of H.M.S. Beagle", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Origin of Species (Everyman's University Paperbacks)", "Insectivorous Plants", "Charles Darwin's natural selection", "The Correspondence of Charles Darwin, Volume 13: 1865", "Charles Darwin's letters", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "The Correspondence of Charles Darwin, Volume 14", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The Correspondence of Charles Darwin, Volume 13", "Proiskhozhdenie vidov", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "Evolution", "The Correspondence of Charles Darwin, Volume 14: 1866", "Origin of Species (Harvard Classics, Part 11)", "Resa kring jorden"], "ans_acc": 0.07009345794392523, "ans_hit": 1, "ans_f1": 0.027397260273972608, "ans_precission": 0.6, "ans_recall": 0.014018691588785047, "path_f1": 0.29411764705882354, "path_precision": 1.0, "path_recall": 0.1724137931034483, "path_ans_f1": 0.13100436681222707, "path_ans_precision": 1.0, "path_ans_recall": 0.07009345794392523}
{"id": "WebQTest-1080", "prediction": ["# Reasoning Path:\nAbraham -> people.person.place_of_birth -> Mesopotamia -> book.book_subject.works -> A History of the Ancient Near East\n# Answer:\nMesopotamia", "# Reasoning Path:\nAbraham -> people.person.place_of_birth -> Mesopotamia -> location.location.contains -> Ur Ka\u015bdim\n# Answer:\nMesopotamia", "# Reasoning Path:\nAbraham -> people.deceased_person.place_of_death -> Canaan -> common.topic.article -> m.0d9fc\n# Answer:\nCanaan", "# Reasoning Path:\nAbraham -> people.person.place_of_birth -> Mesopotamia -> book.book_subject.works -> Ancient Iraq\n# Answer:\nMesopotamia", "# Reasoning Path:\nAbraham -> people.person.place_of_birth -> Mesopotamia -> location.location.events -> Anastasian War\n# Answer:\nMesopotamia", "# Reasoning Path:\nAbraham -> people.person.place_of_birth -> Mesopotamia -> book.book_subject.works -> Ancient Mesopotamia\n# Answer:\nMesopotamia", "# Reasoning Path:\nAbraham -> people.person.place_of_birth -> Mesopotamia -> location.location.events -> Antony's Parthian War\n# Answer:\nMesopotamia", "# Reasoning Path:\nAbraham -> people.deceased_person.place_of_death -> Canaan -> common.topic.notable_for -> g.1257y3_z1\n# Answer:\nCanaan", "# Reasoning Path:\nAbraham -> people.person.place_of_birth -> Mesopotamia -> location.location.events -> Armenian\u2013Parthian War\n# Answer:\nMesopotamia", "# Reasoning Path:\nAbraham -> people.deceased_person.place_of_death -> Canaan -> common.topic.image -> Amarnamap\n# Answer:\nCanaan"], "ground_truth": ["Mesopotamia", "Canaan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1081", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.education -> m.0n1916z -> education.education.institution -> Simpson College\n# Answer:\nm.0n1916z", "# Reasoning Path:\nGeorge Washington Carver -> people.person.education -> m.0ztr7gt -> education.education.institution -> Iowa State University\n# Answer:\nm.0ztr7gt", "# Reasoning Path:\nGeorge Washington Carver -> people.person.education -> m.04hdfv4 -> education.education.institution -> Iowa State University\n# Answer:\nm.04hdfv4", "# Reasoning Path:\nGeorge Washington Carver -> people.person.education -> m.0ztr7gt -> education.education.degree -> Master's Degree\n# Answer:\nm.0ztr7gt", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_for -> g.1255959wd\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.education -> m.0n1916z -> freebase.valuenotation.has_value -> Degree\n# Answer:\nm.0n1916z", "# Reasoning Path:\nGeorge Washington Carver -> people.person.education -> m.04hdfv4 -> education.education.degree -> Bachelor of Science\n# Answer:\nm.04hdfv4", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> astronomy.extraterrestrial_location.on_celestial_object -> Moon\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.education -> m.0n1916z -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nm.0n1916z", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_types -> Extraterrestrial location\n# Answer:\nCarver"], "ground_truth": ["Iowa State University", "Simpson College"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1082", "prediction": ["# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Obradoiro CAB -> sports.sports_team.sport -> Basketball\n# Answer:\nObradoiro CAB", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> CB Avenida -> sports.sports_team.sport -> Basketball\n# Answer:\nCB Avenida", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> CB Sevilla -> sports.sports_team.sport -> Basketball\n# Answer:\nCB Sevilla", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Obradoiro CAB -> common.topic.notable_types -> Basketball Team\n# Answer:\nObradoiro CAB", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\ng.1hhc37x4q", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60xs9d2\n# Answer:\ng.11b60xs9d2", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> CB Avenida -> sports.sports_team.colors -> Blue\n# Answer:\nCB Avenida", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> CB Sevilla -> sports.sports_team.location -> Seville\n# Answer:\nCB Sevilla", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc38hhk\n# Answer:\ng.1hhc38hhk", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> CB Sevilla -> common.topic.notable_types -> Basketball Team\n# Answer:\nCB Sevilla"], "ground_truth": ["Ros Casares Valencia", "Spain men's national volleyball team", "CB Breog\u00e1n", "CB 1939 Canarias", "CB Estudiantes", "Saski Baskonia", "San Sebasti\u00e1n Gipuzkoa BC", "Spain women's national volleyball team", "Joventut Badalona", "B\u00e0squet Manresa", "Real Madrid Baloncesto", "CB Sant Josep", "CB Sevilla", "CB Avenida", "Valencia BC", "CB Murcia", "Spain national football team", "Obradoiro CAB", "Basket Zaragoza 2002", "Spain national beach handball team", "Baloncesto M\u00e1laga", "Spain women's national handball team", "Spain national handball team"], "ans_acc": 0.13043478260869565, "ans_hit": 1, "ans_f1": 0.21989528795811517, "ans_precission": 0.7, "ans_recall": 0.13043478260869565, "path_f1": 0.21989528795811517, "path_precision": 0.7, "path_recall": 0.13043478260869565, "path_ans_f1": 0.21989528795811517, "path_ans_precision": 0.7, "path_ans_recall": 0.13043478260869565}
{"id": "WebQTest-1083", "prediction": ["# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> location.location.containedby -> Hawaii\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> base.biblioness.bibs_location.state -> Hawaii\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> location.location.containedby -> United States of America\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Writer's Journey to Selma, Alabama -> book.written_work.subjects -> African American\n# Answer:\nA Writer's Journey to Selma, Alabama", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Writer's Journey to Selma, Alabama -> book.written_work.author -> Aberjhani\n# Answer:\nA Writer's Journey to Selma, Alabama", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> Countdown of 10 Amazing Moments from the Year 2011: No. 2 President Obama -> book.written_work.subjects -> The Year 2011\n# Answer:\nCountdown of 10 Amazing Moments from the Year 2011: No. 2 President Obama", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Writer's Journey to Selma, Alabama -> book.written_work.subjects -> African-American history\n# Answer:\nA Writer's Journey to Selma, Alabama", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Writer's Journey to Selma, Alabama -> book.written_work.subjects -> Black History Month\n# Answer:\nA Writer's Journey to Selma, Alabama", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Writer's Journey to Selma, Alabama -> book.review.work_reviewed -> Selma\n# Answer:\nA Writer's Journey to Selma, Alabama"], "ground_truth": ["Honolulu"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1084", "prediction": ["# Reasoning Path:\nPoland -> sports.sports_team_location.teams -> Asseco Gdynia -> sports.sports_team.sport -> Basketball\n# Answer:\nAsseco Gdynia", "# Reasoning Path:\nPoland -> sports.sports_team_location.teams -> Basket Zielona G\u00f3ra -> sports.sports_team.sport -> Basketball\n# Answer:\nBasket Zielona G\u00f3ra", "# Reasoning Path:\nPoland -> sports.sports_team_location.teams -> Asseco Gdynia -> common.topic.notable_types -> Basketball Team\n# Answer:\nAsseco Gdynia", "# Reasoning Path:\nPoland -> sports.sports_team_location.teams -> Basket Zielona G\u00f3ra -> sports.sports_team.colors -> White\n# Answer:\nBasket Zielona G\u00f3ra", "# Reasoning Path:\nPoland -> sports.sports_team_location.teams -> GT\u017b Grudzi\u0105dz -> sports.sports_team.sport -> Motorcycle speedway\n# Answer:\nGT\u017b Grudzi\u0105dz", "# Reasoning Path:\nPoland -> location.statistical_region.size_of_armed_forces -> g.12cp_j3lx\n# Answer:\ng.12cp_j3lx", "# Reasoning Path:\nPoland -> sports.sports_team_location.teams -> Basket Zielona G\u00f3ra -> common.topic.notable_types -> Basketball Team\n# Answer:\nBasket Zielona G\u00f3ra", "# Reasoning Path:\nPoland -> location.statistical_region.size_of_armed_forces -> g.1hhc385gk\n# Answer:\ng.1hhc385gk", "# Reasoning Path:\nPoland -> sports.sports_team_location.teams -> GT\u017b Grudzi\u0105dz -> sports.sports_team.colors -> Blue\n# Answer:\nGT\u017b Grudzi\u0105dz", "# Reasoning Path:\nPoland -> location.statistical_region.size_of_armed_forces -> g.1hhc39krm\n# Answer:\ng.1hhc39krm"], "ground_truth": ["\u015al\u0105sk Wroc\u0142aw", "Poland men's national volleyball team", "Basket Zielona G\u00f3ra", "KK W\u0142oc\u0142awek", "Asseco Gdynia", "Poland women's national handball team", "Poland women's national volleyball team", "Poland national football team", "GT\u017b Grudzi\u0105dz", "Stal Gorz\u00f3w Wielkopolski", "Tur\u00f3w Zgorzelec", "Poland national handball team"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.3684210526315789, "ans_precission": 0.7, "ans_recall": 0.25, "path_f1": 0.3684210526315789, "path_precision": 0.7, "path_recall": 0.25, "path_ans_f1": 0.3684210526315789, "path_ans_precision": 0.7, "path_ans_recall": 0.25}
{"id": "WebQTest-1085", "prediction": ["# Reasoning Path:\nDemi Lovato -> music.composer.compositions -> Behind Enemy Lines -> music.composition.composer -> Joe Jonas\n# Answer:\nBehind Enemy Lines", "# Reasoning Path:\nDemi Lovato -> base.popstra.celebrity.dated -> m.065q3fy -> base.popstra.dated.participant -> Joe Jonas\n# Answer:\nm.065q3fy", "# Reasoning Path:\nDemi Lovato -> music.composer.compositions -> Don't Forget -> music.composition.composer -> Joe Jonas\n# Answer:\nDon't Forget", "# Reasoning Path:\nDemi Lovato -> music.artist.album -> Demi -> common.topic.article -> m.0sghhtd\n# Answer:\nDemi", "# Reasoning Path:\nDemi Lovato -> base.popstra.celebrity.dated -> m.063gd6q -> base.popstra.dated.participant -> Cody Linley\n# Answer:\nm.063gd6q", "# Reasoning Path:\nDemi Lovato -> music.composer.compositions -> Behind Enemy Lines -> music.composition.composer -> Kevin Jonas\n# Answer:\nBehind Enemy Lines", "# Reasoning Path:\nDemi Lovato -> music.composer.compositions -> Behind Enemy Lines -> music.composition.language -> English Language\n# Answer:\nBehind Enemy Lines", "# Reasoning Path:\nDemi Lovato -> base.popstra.celebrity.dated -> m.064cvl8 -> base.popstra.dated.participant -> Nicholas Braun\n# Answer:\nm.064cvl8", "# Reasoning Path:\nDemi Lovato -> music.composer.compositions -> Behind Enemy Lines -> music.composition.composer -> Nick Jonas\n# Answer:\nBehind Enemy Lines", "# Reasoning Path:\nDemi Lovato -> music.composer.compositions -> Don't Forget -> music.composition.composer -> Kevin Jonas\n# Answer:\nDon't Forget"], "ground_truth": ["Wilmer Valderrama", "Cody Linley", "Joe Jonas", "Nicholas Braun"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.29629629629629634, "path_precision": 0.4, "path_recall": 0.23529411764705882, "path_ans_f1": 0.6, "path_ans_precision": 0.5, "path_ans_recall": 0.75}
{"id": "WebQTest-1087", "prediction": ["# Reasoning Path:\nScarlett Johansson -> award.award_nominee.award_nominations -> m.0z83g54 -> award.award_nomination.nominated_for -> We Bought a Zoo\n# Answer:\nm.0z83g54", "# Reasoning Path:\nScarlett Johansson -> award.award_nominee.award_nominations -> m.0z9m97f -> award.award_nomination.nominated_for -> The Prestige\n# Answer:\nm.0z9m97f", "# Reasoning Path:\nScarlett Johansson -> award.award_nominee.award_nominations -> m.0gm4_rg -> award.award_nomination.nominated_for -> Lost in Translation\n# Answer:\nm.0gm4_rg", "# Reasoning Path:\nScarlett Johansson -> award.award_nominee.award_nominations -> m.0z9m97f -> award.award_nomination.nominated_for -> The Black Dahlia\n# Answer:\nm.0z9m97f", "# Reasoning Path:\nScarlett Johansson -> award.award_nominee.award_nominations -> m.0z83g54 -> award.award_nomination.ceremony -> 2012 Teen Choice Awards\n# Answer:\nm.0z83g54", "# Reasoning Path:\nScarlett Johansson -> award.award_nominee.award_nominations -> m.0z9m97f -> award.award_nomination.ceremony -> 2007 Teen Choice Awards\n# Answer:\nm.0z9m97f", "# Reasoning Path:\nScarlett Johansson -> award.award_nominee.award_nominations -> m.0gm4_rg -> award.award_nomination.nominated_for -> The Perfect Score\n# Answer:\nm.0gm4_rg", "# Reasoning Path:\nScarlett Johansson -> award.award_nominee.award_nominations -> m.0gm4_rg -> award.award_nomination.award -> Teen Choice Award for Choice Movie Breakout Star - Female\n# Answer:\nm.0gm4_rg", "# Reasoning Path:\nScarlett Johansson -> award.award_nominee.award_nominations -> m.0gm4_rg -> award.award_nomination.ceremony -> 2004 Teen Choice Awards\n# Answer:\nm.0gm4_rg", "# Reasoning Path:\nScarlett Johansson -> tv.tv_actor.guest_roles -> m.0bvtz__ -> tv.tv_guest_role.episodes_appeared_in -> Ryan Reynolds / Lady Gaga\n# Answer:\nRyan Reynolds / Lady Gaga"], "ground_truth": ["Chef", "Ghost World", "Lost in Translation", "Captain America: The Winter Soldier", "The Horse Whisperer", "The Nanny Diaries", "A Good Woman", "The Black Dahlia", "The SpongeBob SquarePants Movie", "The Island", "The Other Boleyn Girl", "Her", "He's Just Not That Into You", "Manny & Lo", "An American Rhapsody", "The Avengers", "The Jungle Book", "Lucy", "We Bought a Zoo", "The Man Who Wasn't There", "The Perfect Score", "The Avengers: Age of Ultron", "Buck", "Just Cause", "North", "My Brother the Pig", "The Prestige", "Vicky Cristina Barcelona", "Iron Man 2", "Scoop", "The Spirit", "Home Alone 3", "Girl with a Pearl Earring", "Match Point", "Eight Legged Freaks", "Don Jon", "In Good Company", "Fall", "If Lucy Fell", "Hitchcock", "Captain America: Civil War", "A Love Song for Bobby Long", "Under the Skin"], "ans_acc": 0.11627906976744186, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.09345794392523363, "path_precision": 0.5, "path_recall": 0.05154639175257732, "path_ans_f1": 0.18867924528301888, "path_ans_precision": 0.5, "path_ans_recall": 0.11627906976744186}
{"id": "WebQTest-1088", "prediction": ["# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.0k6pxpv -> tv.regular_tv_appearance.actor -> Lacey Chabert\n# Answer:\nm.0k6pxpv", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.02ntq_4 -> tv.regular_tv_appearance.character -> Meg Griffin\n# Answer:\nm.02ntq_4", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.0k6pxpv -> tv.regular_tv_appearance.character -> Meg Griffin\n# Answer:\nm.0k6pxpv", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.02ntq_4 -> tv.regular_tv_appearance.actor -> Mila Kunis\n# Answer:\nm.02ntq_4", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.010btxt8 -> tv.regular_tv_appearance.actor -> Brandon Richardson\n# Answer:\nm.010btxt8", "# Reasoning Path:\nFamily Guy -> common.topic.webpage -> m.02sfx_3 -> common.webpage.category -> Topic Webpage\n# Answer:\nm.02sfx_3", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.010btxt8 -> tv.regular_tv_appearance.seasons -> Family Guy - Season 12\n# Answer:\nm.010btxt8", "# Reasoning Path:\nFamily Guy -> common.topic.webpage -> m.07ldx61 -> common.webpage.resource -> m.0bjysbj\n# Answer:\nm.07ldx61", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.02ntq_4 -> tv.regular_tv_appearance.seasons -> Family Guy - Season 10\n# Answer:\nm.02ntq_4", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.010btxt8 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nm.010btxt8"], "ground_truth": ["Lacey Chabert"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1089", "prediction": ["# Reasoning Path:\nBad Teacher -> film.film.starring -> m.0b798jt -> film.performance.actor -> Lucy Punch\n# Answer:\nm.0b798jt", "# Reasoning Path:\nBad Teacher -> film.film.starring -> m.0b798jt -> film.performance.character -> Amy Squirrel\n# Answer:\nm.0b798jt", "# Reasoning Path:\nBad Teacher -> film.film.starring -> m.0b798j0 -> film.performance.actor -> Jason Segel\n# Answer:\nm.0b798j0", "# Reasoning Path:\nBad Teacher -> film.film.starring -> m.0b798j5 -> film.performance.actor -> Cameron Diaz\n# Answer:\nm.0b798j5", "# Reasoning Path:\nBad Teacher -> film.film.starring -> m.0b798j0 -> film.performance.character -> Russell Gettis\n# Answer:\nm.0b798j0", "# Reasoning Path:\nBad Teacher -> film.film.other_crew -> m.0glzm9q -> film.film_crew_gig.crewmember -> Laurene Alvarado\n# Answer:\nm.0glzm9q", "# Reasoning Path:\nBad Teacher -> film.film.other_crew -> m.0glzm9v -> film.film_crew_gig.film_crew_role -> Key Hair Stylist\n# Answer:\nm.0glzm9v", "# Reasoning Path:\nBad Teacher -> film.film.starring -> m.0b798j5 -> film.performance.character -> Elizabeth Halsey\n# Answer:\nm.0b798j5", "# Reasoning Path:\nBad Teacher -> film.film.other_crew -> m.0glzm9q -> film.film_crew_gig.film_crew_role -> Makeup Artist\n# Answer:\nm.0glzm9q", "# Reasoning Path:\nBad Teacher -> film.film.other_crew -> m.0glzmb2 -> film.film_crew_gig.film_crew_role -> Key Makeup Artist\n# Answer:\nm.0glzmb2"], "ground_truth": ["Lucy Punch"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nm.0hpc6gc", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hpc6gc", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.team -> New York Jets\n# Answer:\nm.04nb7z0", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0j5dkn6 -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nm.0j5dkn6", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> book.written_work.author -> Nathan Whitaker\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.04nb7z0", "# Reasoning Path:\nTim Tebow -> people.person.education -> m.05v4mnf -> education.education.institution -> University of Florida\n# Answer:\nm.05v4mnf", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.notable_types -> Book\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0j5dkn6 -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0j5dkn6", "# Reasoning Path:\nTim Tebow -> people.person.education -> m.05v4mnf -> education.education.major_field_of_study -> Family, Youth and Community Sciences\n# Answer:\nm.05v4mnf"], "ground_truth": ["Florida Gators football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1090", "prediction": ["# Reasoning Path:\nBoeing -> organization.organization.headquarters -> m.02_6pm_ -> location.mailing_address.citytown -> Chicago\n# Answer:\nm.02_6pm_", "# Reasoning Path:\nBoeing -> organization.organization.place_founded -> Seattle -> location.location.containedby -> United States of America\n# Answer:\nSeattle", "# Reasoning Path:\nBoeing -> organization.organization.headquarters -> m.02_6pm_ -> location.mailing_address.country -> United States of America\n# Answer:\nm.02_6pm_", "# Reasoning Path:\nBoeing -> organization.organization.headquarters -> m.02_6pm_ -> freebase.valuenotation.has_no_value -> Street Address 2\n# Answer:\nm.02_6pm_", "# Reasoning Path:\nBoeing -> organization.organization.place_founded -> Seattle -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nSeattle", "# Reasoning Path:\nBoeing -> business.business_operation.current_liabilities -> m.090887v -> measurement_unit.dated_money_value.currency -> United States Dollar\n# Answer:\nm.090887v", "# Reasoning Path:\nBoeing -> organization.organization.headquarters -> m.02_6pm_ -> freebase.valuenotation.has_no_value -> Street Address 3\n# Answer:\nm.02_6pm_", "# Reasoning Path:\nBoeing -> organization.organization.headquarters -> m.02_6pm_ -> freebase.valuenotation.has_no_value -> Street Address 4\n# Answer:\nm.02_6pm_"], "ground_truth": ["Chicago"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1091", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.administrative_divisions -> Alaska -> location.location.containedby -> United States, with Territories\n# Answer:\nAlaska", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Alaska -> location.location.containedby -> United States, with Territories\n# Answer:\nAlaska", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\ng.125_r6z32", "# Reasoning Path:\nUnited States of America -> location.country.administrative_divisions -> Alabama -> location.location.containedby -> Contiguous United States\n# Answer:\nAlabama", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nm.010r7f0z", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Arizona -> location.location.containedby -> Contiguous United States\n# Answer:\nArizona", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Arizona -> freebase.valuenotation.is_reviewed -> Minimum wage\n# Answer:\nArizona", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Arizona -> location.location.containedby -> United States, with Territories\n# Answer:\nArizona", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Arizona -> location.location.partially_contains -> Colorado River\n# Answer:\nArizona", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Arizona -> location.location.partially_contains -> Santa Cruz River\n# Answer:\nArizona"], "ground_truth": ["Christianity", "Islam", "Atheism", "Hinduism", "Unitarian Universalism", "Judaism", "Buddhism"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1092", "prediction": ["# Reasoning Path:\nViggo Mortensen -> film.actor.film -> m.0k5s9k -> film.performance.character -> Aragorn\n# Answer:\nm.0k5s9k", "# Reasoning Path:\nViggo Mortensen -> film.actor.film -> g.11b6bnm19y\n# Answer:\ng.11b6bnm19y", "# Reasoning Path:\nViggo Mortensen -> people.person.profession -> Actor\n# Answer:\nActor", "# Reasoning Path:\nViggo Mortensen -> film.actor.film -> g.11b6ntbrrh\n# Answer:\ng.11b6ntbrrh", "# Reasoning Path:\nViggo Mortensen -> film.actor.film -> m.0k5s9k -> film.performance.film -> The Lord of the Rings: The Fellowship of the Ring\n# Answer:\nm.0k5s9k", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> film.film.starring -> m.02vc6p4 -> film.performance.character -> Meriadoc Brandybuck\n# Answer:\nm.02vc6p4", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> film.film.other_crew -> m.0cc3qg7 -> film.film_crew_gig.crewmember -> Margaret Aston\n# Answer:\nm.0cc3qg7", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_winning_work.awards_won -> m.03mlpqj -> award.award_honor.award -> Academy Award for Best Picture\n# Answer:\nm.03mlpqj", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> film.film.other_crew -> m.0cc3qgd -> film.film_crew_gig.film_crew_role -> Makeup Artist\n# Answer:\nm.0cc3qgd", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> award.award_nominated_work.award_nominations -> m.010bdgd6 -> award.award_nomination.award -> NME Award for Best Film\n# Answer:\nm.010bdgd6"], "ground_truth": ["Aragorn"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.0909090909090909, "path_precision": 0.1, "path_recall": 0.08333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1094", "prediction": ["# Reasoning Path:\nIsaac Newton -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.0101ddrk\n# Answer:\nPhysicist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Physicist -> people.profession.part_of_professional_field -> Physics\n# Answer:\nPhysicist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Philosopher -> people.profession.specializations -> Logician\n# Answer:\nPhilosopher", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.01066smw\n# Answer:\nPhysicist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Daniel Faraday\n# Answer:\nPhysicist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Chemist -> base.descriptive_names.names.descriptive_name -> m.01066g6n\n# Answer:\nChemist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Chemist -> base.schemastaging.context_name.pronunciation -> g.125_m09_2\n# Answer:\nChemist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.01066sxg\n# Answer:\nPhysicist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Chemist -> common.topic.notable_types -> Profession\n# Answer:\nChemist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Philosopher -> fictional_universe.character_rank.characters_of_this_rank -> Heraclitus\n# Answer:\nPhilosopher"], "ground_truth": ["Philosopher", "Physicist", "Astronomer", "Scientist", "Mathematician", "Chemist"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-1095", "prediction": ["# Reasoning Path:\nWilliam Shakespeare -> people.person.parents -> John Shakespeare -> people.person.children -> Anne Shakespeare\n# Answer:\nJohn Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Hamnet Shakespeare -> people.person.parents -> Anne Hathaway\n# Answer:\nHamnet Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Judith Quiney -> people.person.parents -> Anne Hathaway\n# Answer:\nJudith Quiney", "# Reasoning Path:\nWilliam Shakespeare -> people.person.parents -> John Shakespeare -> people.person.children -> Edmund Shakespeare\n# Answer:\nJohn Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.spouse_s -> m.02wtqtm -> people.marriage.spouse -> Anne Hathaway\n# Answer:\nm.02wtqtm", "# Reasoning Path:\nWilliam Shakespeare -> people.person.parents -> John Shakespeare -> people.person.profession -> Politician\n# Answer:\nJohn Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.parents -> Mary Shakespeare -> people.person.children -> Anne Shakespeare\n# Answer:\nMary Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.parents -> John Shakespeare -> people.person.children -> Gilbert Shakespeare\n# Answer:\nJohn Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.parents -> John Shakespeare -> freebase.valuenotation.has_value -> Cause of death\n# Answer:\nJohn Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.parents -> Mary Shakespeare -> people.person.children -> Edmund Shakespeare\n# Answer:\nMary Shakespeare"], "ground_truth": ["Anne Hathaway"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1096", "prediction": ["# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0z9rhy8 -> award.award_nomination.nominated_for -> Last Holiday\n# Answer:\nm.0z9rhy8", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0b3t_h2 -> award.award_nomination.nominated_for -> Chicago\n# Answer:\nm.0b3t_h2", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0z9rhy8 -> award.award_nomination.ceremony -> 2006 Teen Choice Awards\n# Answer:\nm.0z9rhy8", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0z9rhy8 -> award.award_nomination.award_nominee -> LL Cool J\n# Answer:\nm.0z9rhy8", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0x0zxs3 -> award.award_nomination.nominated_for -> Chicago\n# Answer:\nm.0x0zxs3", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0b3t_h2 -> award.award_nomination.ceremony -> 2003 MTV Movie Awards\n# Answer:\nm.0b3t_h2", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0x0zxs3 -> award.award_nomination.nominated_for -> Bringing Down the House\n# Answer:\nm.0x0zxs3", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0x0zxs3 -> award.award_nomination.nominated_for -> Brown Sugar\n# Answer:\nm.0x0zxs3", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0x0zxs3 -> award.award_nomination.award -> BET Award for Best Actress\n# Answer:\nm.0x0zxs3", "# Reasoning Path:\nQueen Latifah -> award.award_winner.awards_won -> m.05bl1y_ -> award.award_honor.honored_for -> U.N.I.T.Y.\n# Answer:\nm.05bl1y_"], "ground_truth": ["Ice Age: The Meltdown", "Zac Efron's Pool Party", "Kung Faux: Vol. 3", "Mama Flora's Family", "Kung Faux: Vol. 1", "Living Out Loud", "Through the Years of Hip-Hop: Vol. 1: Graffiti", "Sphere", "Stranger than Fiction", "Kung Faux: Vol. 4", "Joyful Noise", "Arctic Tale", "Queen Latifah - Unauthorized", "Steel Magnolias", "The Cookout", "Mad Money", "Jungle Fever", "22 Jump Street", "Bringing Out the Dead", "Juice", "The Country Bears", "House Party 2", "Hairspray", "Taxi", "We Are Family", "The Secret Life of Bees", "My Life", "Just Wright", "What Happens in Vegas", "Chicago", "Beauty Shop", "Set It Off", "Ice Age: A Mammoth Christmas", "Kung Faux: Vol. 2", "Valentine's Day", "Brown Sugar", "Life in the 310: Vol. 3", "Kung Faux: Vol. 5", "The Dilemma", "Bessie", "Barbershop 2: Back in Business", "Hairspray 2: White Lipstick", "Barbershop 3", "The Bone Collector", "Hoodlum", "Breaking Out: The Alcatraz Concert", "Last Holiday", "Bringing Down the House", "We Are One: The Obama Inaugural Celebration at the Lincoln Memorial", "Scary Movie 3", "The Making of What's Going On: An All-Star Tribute", "Ice Age: Dawn of the Dinosaurs", "One Love: The Bob Marley All-Star Tribute", "The Muppets' Wizard of Oz", "g.11bc95kg2q", "Living with the Dead", "Life Support", "The Perfect Holiday", "Ice Age: Continental Drift"], "ans_acc": 0.06779661016949153, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.07207207207207207, "path_precision": 0.4, "path_recall": 0.039603960396039604, "path_ans_f1": 0.11940298507462686, "path_ans_precision": 0.5, "path_ans_recall": 0.06779661016949153}
{"id": "WebQTest-1097", "prediction": ["# Reasoning Path:\nMichelle Obama -> people.person.education -> m.02wn0g2 -> education.education.institution -> Princeton University\n# Answer:\nm.02wn0g2", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.0n1db15 -> education.education.institution -> Whitney M. Young Magnet High School\n# Answer:\nm.0n1db15", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.02wpt7p -> education.education.institution -> Harvard Law School\n# Answer:\nm.02wpt7p", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.02wn0g2 -> education.education.major_field_of_study -> Sociology\n# Answer:\nm.02wn0g2", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.02wn0g2 -> freebase.valuenotation.has_value -> Specialization\n# Answer:\nm.02wn0g2", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.02wn0g2 -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nm.02wn0g2", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.0n1db15 -> freebase.valuenotation.has_value -> Degree\n# Answer:\nm.0n1db15", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.02wpt7p -> education.education.degree -> Juris Doctor\n# Answer:\nm.02wpt7p", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.02wpt7p -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nm.02wpt7p", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.0n1db15 -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nm.0n1db15"], "ground_truth": ["Harvard Law School", "Whitney M. Young Magnet High School", "Princeton University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1098", "prediction": ["# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.genre.artists -> Aaron Copland\n# Answer:\nArt song", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Opera -> music.compositional_form.superforms -> Vocal music\n# Answer:\nOpera", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.genre.artists -> Albert Roussel\n# Answer:\nArt song", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.compositional_form.compositions -> Beau soir\n# Answer:\nArt song", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Chamber music -> music.genre.albums -> Acts of Beauty/Exit no Exit\n# Answer:\nChamber music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Chamber music -> music.genre.parent_genre -> Classical music\n# Answer:\nChamber music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.genre.artists -> Alexander Borodin\n# Answer:\nArt song", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.genre.albums -> 8 Lust Songs: I Sonetti Lussuriosi\n# Answer:\nArt song", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Chamber music -> common.topic.notable_types -> Musical genre\n# Answer:\nChamber music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Opera -> common.topic.notable_types -> Composition type\n# Answer:\nOpera"], "ground_truth": ["Chamber music", "Romantic music", "Opera", "Classical music", "Art song"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 1.0, "ans_recall": 0.6, "path_f1": 0.7499999999999999, "path_precision": 1.0, "path_recall": 0.6, "path_ans_f1": 0.888888888888889, "path_ans_precision": 1.0, "path_ans_recall": 0.8}
{"id": "WebQTest-1099", "prediction": ["# Reasoning Path:\nDavid Luiz -> people.person.nationality -> Brazil -> sports.sports_team_location.teams -> Brazil national football team\n# Answer:\nBrazil", "# Reasoning Path:\nDavid Luiz -> soccer.football_player.statistics -> m.0w9d9nx -> soccer.football_player_stats.team -> Brazil national football team\n# Answer:\nm.0w9d9nx", "# Reasoning Path:\nDavid Luiz -> people.person.nationality -> Brazil -> base.aareas.schema.administrative_area.administrative_children -> Amazonas\n# Answer:\nBrazil", "# Reasoning Path:\nDavid Luiz -> people.person.nationality -> Brazil -> base.aareas.schema.administrative_area.administrative_children -> Acre\n# Answer:\nBrazil", "# Reasoning Path:\nDavid Luiz -> soccer.football_player.statistics -> m.0w8zrhb -> soccer.football_player_stats.team -> S.L. Benfica\n# Answer:\nm.0w8zrhb", "# Reasoning Path:\nDavid Luiz -> people.person.nationality -> Brazil -> location.country.first_level_divisions -> Amazonas\n# Answer:\nBrazil", "# Reasoning Path:\nDavid Luiz -> people.person.nationality -> Brazil -> base.aareas.schema.administrative_area.administrative_children -> Alagoas\n# Answer:\nBrazil", "# Reasoning Path:\nDavid Luiz -> people.person.nationality -> Brazil -> location.country.first_level_divisions -> Acre\n# Answer:\nBrazil", "# Reasoning Path:\nDavid Luiz -> soccer.football_player.statistics -> m.0w9dq88 -> soccer.football_player_stats.team -> Chelsea F.C.\n# Answer:\nm.0w9dq88", "# Reasoning Path:\nDavid Luiz -> sports.pro_athlete.teams -> m.04mg4fm -> sports.sports_team_roster.team -> S.L. Benfica\n# Answer:\nm.04mg4fm"], "ground_truth": ["Brazil national football team"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nm.0j5d2kv", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nm.0jh9fcj", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0jh9fcj", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.0j5d2kv", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.team -> Indianapolis Colts\n# Answer:\nm.0j4z5bh", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nm.0j5d2kv", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0zs5mvy", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nm.0zs5mvy", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nm.0j5d2kv"], "ground_truth": ["Denver Broncos"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1100", "prediction": ["# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Polly Jefferson\n# Answer:\nPolly Jefferson", "# Reasoning Path:\nThomas Jefferson -> influence.influence_node.influenced -> Ayn Rand -> influence.influence_node.influenced_by -> Aristotle\n# Answer:\nAyn Rand", "# Reasoning Path:\nThomas Jefferson -> book.book_subject.works -> Jefferson -> location.statistical_region.population -> g.11bc87zp8k\n# Answer:\nJefferson", "# Reasoning Path:\nThomas Jefferson -> influence.influence_node.influenced -> Ayn Rand -> influence.influence_node.influenced_by -> Carl Menger\n# Answer:\nAyn Rand", "# Reasoning Path:\nThomas Jefferson -> book.book_subject.works -> Jefferson -> location.statistical_region.population -> g.11bc88twss\n# Answer:\nJefferson", "# Reasoning Path:\nThomas Jefferson -> book.book_subject.works -> Jefferson -> location.location.time_zones -> Central Time Zone\n# Answer:\nJefferson", "# Reasoning Path:\nThomas Jefferson -> influence.influence_node.influenced -> Benjamin Barber -> people.person.nationality -> United States of America\n# Answer:\nBenjamin Barber", "# Reasoning Path:\nThomas Jefferson -> influence.influence_node.influenced -> Ayn Rand -> influence.influence_node.influenced_by -> Immanuel Kant\n# Answer:\nAyn Rand", "# Reasoning Path:\nThomas Jefferson -> influence.influence_node.influenced -> Ayn Rand -> people.person.profession -> Author\n# Answer:\nAyn Rand"], "ground_truth": ["Republican notes on religion ; and, An act establishing religious freedom, passed in the assembly of Virginia, in the year 1786", "The President's speech", "The Papers of Thomas Jefferson, Volume 5: February 1781 to May 1781", "United States Declaration of Independence", "Jefferson's Germantown letters", "Thomas Jefferson's Farm book", "Minor Vocabularies of Nanticoke-Conoy", "The Papers of Thomas Jefferson, Volume 36: 1 December 1801 to 3 March 1802", "The Papers of Thomas Jefferson, Retirement Series: Volume 4: 18 June 1811 to 30 April 1812", "Jefferson's proposed instructions to the Virginia delegates, 1744", "Papers", "The life and letters of Thomas Jefferson", "Jefferson abroad", "The Papers of Thomas Jefferson, Volume 21: Index, Vols. 1-20", "The Papers of Thomas Jefferson, Volume 15: March 1789 to November 1789", "Thomas Jefferson Travels", "The family letters of Thomas Jefferson", "Thomas Jefferson, political writings", "The Works of Thomas Jefferson", "Elementary School Support Kit/Bulletin Boards", "Jefferson's parliamentary writings", "The complete Jefferson", "The Papers of Thomas Jefferson, Volume 1: 14 January 1760 to 6 December 1776", "Junior Fact Summer 2007 Bundle", "The Papers of Thomas Jefferson, Volume 12: August 1787 to March 1788", "An appendix to the Notes on Virginia relative to the murder of Logan's family", "Thomas Jefferson", "The Statute of Virginia for Religious Freedom", "Il pensiero politico e sociale di Thomas Jefferson", "The essential Jefferson", "Memoir, correspondence, and miscellanies", "The four versions of Jefferson's letter to Mazzei", "The Papers of Thomas Jefferson, Volume 30: 1 January 1798 to 31 January 1799", "The anas of Thomas Jefferson", "The best letters of Thomas Jefferson", "Jefferson's literary commonplace book", "The essence of Jefferson", "The proceedings of the government of the United States, in maintaining the public right to the beach of the Missisipi", "Thomas Jefferson correspondence", "Account of Louisiana", "John Dewey presents the living thoughts of Thomas Jefferson", "Jefferson in love", "To the girls and boys", "Letters of Thomas Jefferson", "The Papers of Thomas Jefferson, Volume 7: March 1784 to February 1785", "The Papers of Thomas Jefferson, Retirement Series: Volume 2: 16 November 1809 to 11 August 1810", "The Papers of Thomas Jefferson, Volume 35: 1 August to 30 November 1801", "A Summary View of the Rights of British America", "Speech of Thomas Jefferson, president of the United States, delivered at his instalment, March 4, 1801, at the city of Washington", "The Papers of Thomas Jefferson, Volume 13: March 1788 to October 1788", "Thomas Jefferson's architectural drawings", "Master thoughts of Thomas Jefferson", "Speech of Thomas Jefferson, president of the United States", "The quotable Jefferson", "The political writings of Thomas Jefferson", "The Papers of Thomas Jefferson, Volume 2: January 1777 to 18 June 1779", "Jefferson Bible", "Jefferson's extracts from the Gospels", "Memorandums taken on a journey from Paris into the southern parts of France and Northern Italy, in the year 1787", "A Manual of Parliamentary Practice for the Use of the Senate of the United States", "Speech of Thomas Jefferson, president of the United States, delivered at his inauguration, March 4, 1801", "Basic writings of Thomas Jefferson", "The portable Thomas Jefferson", "The life and selected writings of Thomas Jefferson", "The Jefferson-Dunglison letters", "Catalogue", "The Papers of Thomas Jefferson, Volume 8: February 1785 to October 1785", "Jefferson the man", "The Papers of Thomas Jefferson, Volume 24: 1 June to 31 December 1792", "The inaugural addresses of President Thomas Jefferson, 1801 and 1805", "The inaugural speeches and messages of Thomas Jefferson, Esq", "The republic of letters", "Speech of Thomas Jefferson, president of the United States, delivered in the Senate chamber, March 4th, 1801", "Documents Relating To The Purchase And Exploration Of Louisiana", "Autobiography of Thomas Jefferson", "The Papers of Thomas Jefferson, Volume 4: October 1780 to February 1781", "Crusade against ignorance", "Letters", "Jefferson on Jefferson", "The papers of Thomas Jefferson. Index", "Unpublished correspondence between Thomas Jefferson and some American Jews", "Correspondence between His Excellency Thomas Jefferson, President of the United States and James Monroe, Esq., late American ambassador to the Court of St. James", "The Papers of Thomas Jefferson, Volume 11: January 1787 to August 1787", "The correspondence of Jefferson and Du Pont de Nemours", "The Papers of Thomas Jefferson, Volume 18: 4 November 1790 to 24 January 1791", "The Papers of Thomas Jefferson, Retirement Series: Volume 1: 4 March 1809 to 15 November 1809", "Jefferson and Madison on the Separation of Church and State", "Revolutionary Philosopher", "The Papers of Thomas Jefferson, Volume 25: 1 January to 10 May 1793", "Calendar of the correspondence of Thomas Jefferson", "Public and private papers", "The living thoughts of Thomas Jefferson", "The Papers of Thomas Jefferson, Volume 14: October 1788 to March 1789", "The Papers of Thomas Jefferson, Volume 6: May 1781 to March 1784", "The address of Thomas Jefferson", "Foundations of Freedom", "The Papers of Thomas Jefferson, Volume 34: 1 May to 31 July 1801", "The Papers of Thomas Jefferson, Volume 17: July 1790 to November 1790", "Jeffersonian principles", "The Papers of Thomas Jefferson, Volume 9: November 1785 to June 1786", "The Papers of Thomas Jefferson, Volume 31: 1 February 1799 to 31 May 1800", "The Papers of Thomas Jefferson, Volume 20: April 1791 to August 1791", "The Papers of Thomas Jefferson, Volume 26: 11 May to 31 August 1793", "The Papers of Thomas Jefferson, Volume 16: November 1789 to July 1790", "The Papers of Thomas Jefferson, Volume 10: June 1786 to December 1786", "The Papers of Thomas Jefferson, Volume 29: 1 March 1796 to 31 December 1797", "Jefferson's ideas on a university library", "An essay towards facilitating instruction in the Anglo-Saxon and modern dialects of the English language. For the use of the University of Virginia", "A Jefferson profile as revealed in his letters", "The Papers of Thomas Jefferson, Volume 22: 6 August to 31 December 1791", "Responsibility Skills", "The Papers of Thomas Jefferson, Volume 32: 1 June 1800 to 16 February 1801", "The speech of Logan", "The Papers of Thomas Jefferson, Retirement Series: Volume 3: 12 August 1810 to 17 June 1811", "The writings of Thomas Jefferson", "An American Christian Bible", "The Literary Bible of Thomas Jefferson", "\\\"Ye will say I am no Christian\\\"", "The Commonplace book of Thomas Jefferson", "A supplementary note on the mould board described in a letter to Sir John Sinclair, of March 23, 1798", "The religious and moral wisdom of Thomas Jefferson", "The Papers of Thomas Jefferson, Volume 28: 1 January 1794 to 29 February 1796", "Jefferson himself", "Citizen Jefferson", "Thomas Jefferson, his words and vision", "The wisdom of Thomas Jefferson", "The Papers of Thomas Jefferson, Retirement Series: Volume 5: 1 May 1812 to 10 March 1813", "Letters and addresses of Thomas Jefferson", "State of the Union Addresses of Thomas Jefferson", "The Papers of Thomas Jefferson, Volume 23: 1 January to 31 May 1792", "The Papers of Thomas Jefferson, Volume 19: January 1791 to March 1791", "Light and liberty", "The Papers of Thomas Jefferson, Volume 3: June 1779 to September 1780", "Notes on the State of Virginia", "Manual de pra\u0301ctica parlamentaria", "The Papers of Thomas Jefferson, Retirement Series: Volume 6: 11 March to 27 November 1813", "The Papers of Thomas Jefferson, Volume 33: 17 February to 30 April 1801", "The Papers of Thomas Jefferson, Volume 27: 1 September to 31 December 1793", "The life & morals of Jesus Christ of Nazareth"], "ans_acc": 0.007194244604316547, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.020833333333333332, "path_precision": 1.0, "path_recall": 0.010526315789473684, "path_ans_f1": 0.014285714285714287, "path_ans_precision": 1.0, "path_ans_recall": 0.007194244604316547}
{"id": "WebQTest-1101", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.activism.activism_issue.activists -> Abbie Hoffman\n# Answer:\nAnti-Apartheid", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.nobelprizes.nobel_subject_area.nobel_awards -> m.063k0hx\n# Answer:\nAnti-Apartheid", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Humanitarian -> common.topic.notable_for -> g.1256fnzxx\n# Answer:\nHumanitarian", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.profession -> Clergy -> people.profession.specialization_of -> Religious Leader\n# Answer:\nClergy", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.activism.activism_issue.activists -> Albert Lutuli\n# Answer:\nAnti-Apartheid", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Humanitarian -> common.topic.notable_types -> Profession\n# Answer:\nHumanitarian", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.profession -> Humanitarian -> common.topic.notable_for -> g.1256fnzxx\n# Answer:\nHumanitarian", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.activism.activism_issue.activists -> Coretta Scott King\n# Answer:\nAnti-Apartheid", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.activism.activism_issue.supporting_political_parties -> African National Congress\n# Answer:\nAnti-Apartheid", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> African Americans' rights -> base.activism.activism_issue.activists -> Abbie Hoffman\n# Answer:\nAfrican Americans' rights"], "ground_truth": ["Humanitarian", "Writer", "Minister", "Clergy", "Civil rights activist", "Pastor"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.4, "ans_recall": 0.3333333333333333, "path_f1": 0.3157894736842105, "path_precision": 0.3, "path_recall": 0.3333333333333333, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.4, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1102", "prediction": ["# Reasoning Path:\nVictoria -> location.location.containedby -> British Columbia -> location.location.containedby -> Canada\n# Answer:\nBritish Columbia", "# Reasoning Path:\nVictoria -> location.location.containedby -> British Columbia -> base.aareas.schema.administrative_area.administrative_area_type -> Canadian province\n# Answer:\nBritish Columbia", "# Reasoning Path:\nVictoria -> location.location.containedby -> Canada -> location.location.containedby -> North America\n# Answer:\nCanada", "# Reasoning Path:\nVictoria -> common.image.size -> m.02dtr85\n# Answer:\nm.02dtr85", "# Reasoning Path:\nVictoria -> location.location.containedby -> Canada -> base.locations.countries.continent -> North America\n# Answer:\nCanada", "# Reasoning Path:\nVictoria -> location.location.containedby -> British Columbia -> book.book_subject.works -> A Ghost in the Water\n# Answer:\nBritish Columbia", "# Reasoning Path:\nVictoria -> location.location.containedby -> British Columbia -> book.book_subject.works -> A child is not a toy\n# Answer:\nBritish Columbia", "# Reasoning Path:\nVictoria -> location.location.containedby -> Canada -> location.location.partially_contains -> Alsek River\n# Answer:\nCanada", "# Reasoning Path:\nVictoria -> location.location.events -> 1996 Victoria Film Festival -> time.event.instance_of_recurring_event -> Victoria Film Festival\n# Answer:\n1996 Victoria Film Festival", "# Reasoning Path:\nVictoria -> location.location.containedby -> British Columbia -> book.book_subject.works -> Beloved land\n# Answer:\nBritish Columbia"], "ground_truth": ["British Columbia", "Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1103", "prediction": ["# Reasoning Path:\nMary Mary -> music.artist.album -> Go Get It -> music.composition.composer -> Erica Campbell\n# Answer:\nGo Get It", "# Reasoning Path:\nMary Mary -> music.artist.album -> A Mary Mary Christmas -> common.topic.notable_for -> g.1258xk87v\n# Answer:\nA Mary Mary Christmas", "# Reasoning Path:\nMary Mary -> music.artist.album -> Go Get It -> music.composition.composer -> Warryn Campbell\n# Answer:\nGo Get It", "# Reasoning Path:\nMary Mary -> music.artist.album -> A Mary Mary Christmas -> music.album.release_type -> Album\n# Answer:\nA Mary Mary Christmas", "# Reasoning Path:\nMary Mary -> music.artist.album -> Go Get It -> music.composition.composer -> Tina Campbell\n# Answer:\nGo Get It", "# Reasoning Path:\nMary Mary -> award.award_winner.awards_won -> m.0_r9jdp -> freebase.valuenotation.has_value -> Winning work\n# Answer:\nm.0_r9jdp", "# Reasoning Path:\nMary Mary -> music.artist.album -> Go Get It -> music.album.album_content_type -> Compilation album\n# Answer:\nGo Get It", "# Reasoning Path:\nMary Mary -> music.artist.album -> Help 2.0 -> music.album.genre -> Contemporary R&B\n# Answer:\nHelp 2.0", "# Reasoning Path:\nMary Mary -> award.award_winner.awards_won -> m.0_r9jdp -> award.award_honor.award -> NAACP Image Award for Outstanding Duo or Group\n# Answer:\nm.0_r9jdp", "# Reasoning Path:\nMary Mary -> award.award_winner.awards_won -> m.0_r9tvb -> award.award_honor.honored_for -> Go Get It\n# Answer:\nm.0_r9tvb"], "ground_truth": ["Tina Campbell", "Erica Campbell"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.2, "path_recall": 0.3333333333333333, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1104", "prediction": ["# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> French Polynesia\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> New Caledonia\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> Wallis and Futuna\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Andorra\n# Answer:\nEuro", "# Reasoning Path:\nFrance -> location.country.currency_used -> Euro -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nEuro", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.11b60tv_69\n# Answer:\ng.11b60tv_69", "# Reasoning Path:\nFrance -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Martinique\n# Answer:\nEuro", "# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> Andorra\n# Answer:\nFrench franc", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6gn9g\n# Answer:\ng.12tb6gn9g", "# Reasoning Path:\nFrance -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Belgium\n# Answer:\nEuro"], "ground_truth": ["CFP franc", "Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1106", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nm.0w9plqp", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m -> soccer.football_player_stats.team -> LA Galaxy\n# Answer:\nm.0w8w79m", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9021c -> soccer.football_player_stats.team -> A.C. Milan\n# Answer:\nm.0w9021c", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nm.0j2zzj_", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.02nr829 -> sports.sports_team_roster.team -> LA Galaxy\n# Answer:\nm.02nr829", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0j2zzj_", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nm.0k4ytw5", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.02nr829 -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nm.02nr829", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.05ckhjp -> sports.sports_team_roster.team -> A.C. Milan\n# Answer:\nm.05ckhjp", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0k4ytw5"], "ground_truth": ["LA Galaxy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.47058823529411764, "path_precision": 0.4, "path_recall": 0.5714285714285714, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1107", "prediction": ["# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0z9v3c6 -> award.award_nomination.nominated_for -> Love Don't Cost a Thing\n# Answer:\nm.0z9v3c6", "# Reasoning Path:\nNick Cannon -> film.actor.film -> m.062wcnd -> film.performance.film -> Ball Don't Lie\n# Answer:\nm.062wcnd", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0b4d586 -> award.award_nomination.nominated_for -> Bobby\n# Answer:\nm.0b4d586", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0z9v3c6 -> award.award_nomination.ceremony -> 2004 Teen Choice Awards\n# Answer:\nm.0z9v3c6", "# Reasoning Path:\nNick Cannon -> film.actor.film -> m.0ncf55j -> film.performance.film -> Men in Black II\n# Answer:\nm.0ncf55j", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0z9v3c6 -> award.award_nomination.award -> Teen Choice Award for Choice Movie: Chemistry\n# Answer:\nm.0z9v3c6", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.010g2z6m -> award.award_nomination.nominated_for -> Disney Parks Christmas Day Parade\n# Answer:\nm.010g2z6m", "# Reasoning Path:\nNick Cannon -> film.actor.film -> g.11bttggxl1\n# Answer:\ng.11bttggxl1", "# Reasoning Path:\nNick Cannon -> film.actor.film -> m.062wcnd -> film.performance.character -> Mico\n# Answer:\nm.062wcnd", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0b4d586 -> award.award_nomination.award -> Screen Actors Guild Award for Outstanding Performance by a Cast in a Motion Picture\n# Answer:\nm.0b4d586"], "ground_truth": ["Weapons", "Garfield: The Movie", "The Adventures of Brer Rabbit", "Drumline", "A Very School Gyrls Holla-Day", "Underclassman", "Bobby", "Even Money", "Ball Don't Lie", "Whatever It Takes", "Goal II: Living the Dream", "Drumline: A New Beat", "Men in Black II", "Shall We Dance?", "Chiraq", "Love Don't Cost a Thing", "Monster House", "Roll Bounce", "Day of the Dead", "The Killing Room", "American Son"], "ans_acc": 0.19047619047619047, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.1951219512195122, "path_precision": 0.4, "path_recall": 0.12903225806451613, "path_ans_f1": 0.25806451612903225, "path_ans_precision": 0.4, "path_ans_recall": 0.19047619047619047}
{"id": "WebQTest-1108", "prediction": ["# Reasoning Path:\nNPR -> organization.organization.headquarters -> m.02hr995 -> location.mailing_address.citytown -> Washington, D.C.\n# Answer:\nm.02hr995", "# Reasoning Path:\nNPR -> organization.organization.headquarters -> m.02hr995 -> common.topic.notable_for -> g.1258r1rck\n# Answer:\nm.02hr995", "# Reasoning Path:\nNPR -> organization.organization.headquarters -> m.02hr995 -> location.location.geolocation -> m.04nm5hc\n# Answer:\nm.02hr995", "# Reasoning Path:\nNPR -> award.award_winner.awards_won -> m.0103rsl0 -> award.award_honor.ceremony -> 2014 Peabody Awards Ceremony\n# Answer:\nm.0103rsl0", "# Reasoning Path:\nNPR -> broadcast.radio_network.affiliates -> m.010gb2_0 -> broadcast.radio_affiliation_duration.station -> WDET-FM\n# Answer:\nm.010gb2_0", "# Reasoning Path:\nNPR -> award.award_winner.awards_won -> m.0103rsl0 -> award.award_honor.award_winner -> Michele Norris\n# Answer:\nm.0103rsl0", "# Reasoning Path:\nNPR -> award.award_winner.awards_won -> m.03wllq5 -> award.award_honor.award -> National Medal of Arts\n# Answer:\nm.03wllq5", "# Reasoning Path:\nNPR -> broadcast.radio_network.affiliates -> m.010gb2ws -> broadcast.radio_affiliation_duration.station -> WBHM\n# Answer:\nm.010gb2ws", "# Reasoning Path:\nNPR -> award.award_winner.awards_won -> m.0103rsl0 -> award.award_honor.honored_for -> Morning Edition\n# Answer:\nm.0103rsl0", "# Reasoning Path:\nNPR -> award.award_winner.awards_won -> m.05v0spb -> freebase.valuenotation.is_reviewed -> Award winner\n# Answer:\nm.05v0spb"], "ground_truth": ["WNYC"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1109", "prediction": ["# Reasoning Path:\nTom Landry -> sports.pro_athlete.teams -> m.0hpglgc -> sports.sports_team_roster.team -> New York Giants\n# Answer:\nm.0hpglgc", "# Reasoning Path:\nTom Landry -> sports.pro_athlete.teams -> m.0hpglgc -> sports.sports_team_roster.position -> Cornerback\n# Answer:\nm.0hpglgc", "# Reasoning Path:\nTom Landry -> sports.pro_athlete.teams -> m.0hpglgp -> sports.sports_team_roster.team -> New York Yankees\n# Answer:\nm.0hpglgp", "# Reasoning Path:\nTom Landry -> sports.pro_athlete.teams -> m.0hph6vx -> sports.sports_team_roster.team -> Texas Longhorns football\n# Answer:\nm.0hph6vx", "# Reasoning Path:\nTom Landry -> symbols.name_source.namesakes -> Landry Jones -> sports.pro_athlete.teams -> m.0h_j38w\n# Answer:\nLandry Jones", "# Reasoning Path:\nTom Landry -> sports.pro_athlete.teams -> m.0hpglgp -> sports.sports_team_roster.position -> Defensive back\n# Answer:\nm.0hpglgp", "# Reasoning Path:\nTom Landry -> symbols.name_source.namesakes -> Landry Jones -> sports.pro_athlete.teams -> m.0vyl_2v\n# Answer:\nLandry Jones", "# Reasoning Path:\nTom Landry -> symbols.name_source.namesakes -> Landry Jones -> common.topic.image -> Jones winding up to pass in a game againt Tulsa in 2009.\n# Answer:\nLandry Jones", "# Reasoning Path:\nTom Landry -> sports.pro_athlete.teams -> m.0hpglgp -> freebase.valuenotation.has_value -> To\n# Answer:\nm.0hpglgp", "# Reasoning Path:\nTom Landry -> sports.pro_athlete.teams -> m.0hph6vx -> freebase.valuenotation.has_value -> To\n# Answer:\nm.0hph6vx"], "ground_truth": ["New York Yankees", "New York Giants", "Texas Longhorns football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Hungary -> base.locations.countries.continent -> Europe\n# Answer:\nHungary", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Hungary -> location.location.containedby -> Eurasia\n# Answer:\nHungary", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Serbia -> location.location.containedby -> Eurasia\n# Answer:\nSerbia", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Eastern European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.partially_contains -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Menchul -> common.topic.notable_for -> g.11b764th4s\n# Answer:\nMenchul", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Czech Republic\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Serbia -> base.locations.countries.continent -> Europe\n# Answer:\nSerbia"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1110", "prediction": ["# Reasoning Path:\nJohn Jay -> people.person.place_of_birth -> New York City -> media_common.quotation_subject.quotations_about_this_subject -> \\\"History dressed up in the glow of love\u2019s kiss turned grief into beauty.\\\"\n# Answer:\nNew York City", "# Reasoning Path:\nJohn Jay -> people.person.place_of_birth -> New York City -> periodicals.newspaper_circulation_area.newspapers -> Morgen Freiheit\n# Answer:\nNew York City", "# Reasoning Path:\nJohn Jay -> common.topic.image -> John Jay, einer der Gr\u00fcnderv\u00e4ter der Vereinigten Staaten, war direkter Vorfahre Trotts. -> common.image.appears_in_topic_gallery -> Adam von Trott zu Solz\n# Answer:\nJohn Jay, einer der Gr\u00fcnderv\u00e4ter der Vereinigten Staaten, war direkter Vorfahre Trotts.", "# Reasoning Path:\nJohn Jay -> government.politician.party -> m.0lr1zhl -> government.political_party_tenure.party -> Federalist Party\n# Answer:\nm.0lr1zhl", "# Reasoning Path:\nJohn Jay -> people.person.place_of_birth -> New York City -> periodicals.newspaper_circulation_area.newspapers -> China Daily\n# Answer:\nNew York City", "# Reasoning Path:\nJohn Jay -> common.topic.image -> John Jay, einer der Gr\u00fcnderv\u00e4ter der Vereinigten Staaten, war direkter Vorfahre Trotts. -> common.image.size -> m.02bg03x\n# Answer:\nJohn Jay, einer der Gr\u00fcnderv\u00e4ter der Vereinigten Staaten, war direkter Vorfahre Trotts.", "# Reasoning Path:\nJohn Jay -> people.person.place_of_birth -> New York City -> periodicals.newspaper_circulation_area.newspapers -> New York World\n# Answer:\nNew York City", "# Reasoning Path:\nJohn Jay -> common.topic.image -> John Jay, einer der Gr\u00fcnderv\u00e4ter der Vereinigten Staaten, war direkter Vorfahre Trotts. -> common.image.appears_in_topic_gallery -> Federalist No. 2\n# Answer:\nJohn Jay, einer der Gr\u00fcnderv\u00e4ter der Vereinigten Staaten, war direkter Vorfahre Trotts.", "# Reasoning Path:\nJohn Jay -> common.topic.image -> John Jay, einer der Gr\u00fcnderv\u00e4ter der Vereinigten Staaten, war direkter Vorfahre Trotts. -> common.image.appears_in_topic_gallery -> Federalist No. 3\n# Answer:\nJohn Jay, einer der Gr\u00fcnderv\u00e4ter der Vereinigten Staaten, war direkter Vorfahre Trotts."], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 0.4444444444444444, "ans_recall": 1.0, "path_f1": 0.6153846153846153, "path_precision": 0.4444444444444444, "path_recall": 1.0, "path_ans_f1": 0.6153846153846153, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 1.0}
{"id": "WebQTest-1111", "prediction": ["# Reasoning Path:\nMount Fuji -> location.location.containedby -> Honshu -> location.location.containedby -> Japan\n# Answer:\nHonshu", "# Reasoning Path:\nMount Fuji -> location.location.containedby -> Fuji-Hakone-Izu National Park -> location.location.containedby -> Kant\u014d region\n# Answer:\nFuji-Hakone-Izu National Park", "# Reasoning Path:\nMount Fuji -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nJapan", "# Reasoning Path:\nMount Fuji -> base.schemastaging.context_name.pronunciation -> g.125_ptxg0\n# Answer:\ng.125_ptxg0", "# Reasoning Path:\nMount Fuji -> location.location.containedby -> Honshu -> location.location.geolocation -> m.0dbffb_\n# Answer:\nHonshu", "# Reasoning Path:\nMount Fuji -> location.location.containedby -> Fuji-Hakone-Izu National Park -> common.topic.image -> Mt. Fuji and Ashinoko Lake from Motohakone\n# Answer:\nFuji-Hakone-Izu National Park", "# Reasoning Path:\nMount Fuji -> location.location.containedby -> Honshu -> location.location.contains -> Abu\n# Answer:\nHonshu", "# Reasoning Path:\nMount Fuji -> location.location.containedby -> Fuji-Hakone-Izu National Park -> common.topic.notable_types -> Protected Site\n# Answer:\nFuji-Hakone-Izu National Park", "# Reasoning Path:\nMount Fuji -> location.location.containedby -> Honshu -> location.location.contains -> Aichi Prefecture\n# Answer:\nHonshu", "# Reasoning Path:\nMount Fuji -> geography.mountain.mountain_type -> Stratovolcano -> geography.geographical_feature_category.features -> Akita-Yake-Yama\n# Answer:\nStratovolcano"], "ground_truth": ["Asia", "Japan", "Ch\u016bbu region", "Honshu", "Fuji-Hakone-Izu National Park"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6857142857142857, "ans_precission": 0.8, "ans_recall": 0.6, "path_f1": 0.509090909090909, "path_precision": 0.7, "path_recall": 0.4, "path_ans_f1": 0.6857142857142857, "path_ans_precision": 0.8, "path_ans_recall": 0.6}
{"id": "WebQTest-1113", "prediction": ["# Reasoning Path:\nToussaint Louverture -> people.person.place_of_birth -> Saint-Domingue -> common.topic.notable_types -> Location\n# Answer:\nSaint-Domingue", "# Reasoning Path:\nToussaint Louverture -> common.topic.notable_types -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nToussaint Louverture -> people.person.place_of_birth -> Saint-Domingue -> location.location.people_born_here -> Dominique You\n# Answer:\nSaint-Domingue", "# Reasoning Path:\nToussaint Louverture -> common.topic.notable_types -> Politician -> freebase.type_hints.included_types -> Topic\n# Answer:\nPolitician", "# Reasoning Path:\nToussaint Louverture -> common.topic.notable_types -> Politician -> freebase.type_profile.kind -> Title\n# Answer:\nPolitician", "# Reasoning Path:\nToussaint Louverture -> people.person.place_of_birth -> Saint-Domingue -> location.location.people_born_here -> Jean Lafitte\n# Answer:\nSaint-Domingue", "# Reasoning Path:\nToussaint Louverture -> common.topic.notable_types -> Politician -> type.type.expected_by -> politician\n# Answer:\nPolitician", "# Reasoning Path:\nToussaint Louverture -> people.person.place_of_birth -> Saint-Domingue -> location.location.events -> Action of 1 January 1800\n# Answer:\nSaint-Domingue", "# Reasoning Path:\nToussaint Louverture -> people.person.place_of_birth -> Saint-Domingue -> location.location.people_born_here -> Jean-Louis Michel\n# Answer:\nSaint-Domingue", "# Reasoning Path:\nToussaint Louverture -> people.person.place_of_birth -> Saint-Domingue -> location.location.events -> Battle of Cap-Fran\u00e7ais\n# Answer:\nSaint-Domingue"], "ground_truth": ["Saint-Domingue"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1114", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> base.aareas.schema.administrative_area.administrative_children -> Anglesey\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> base.aareas.schema.administrative_area.administrative_children -> Blaenau Gwent\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> location.country.first_level_divisions -> Anglesey\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nm.0ghc35h", "# Reasoning Path:\nUnited Kingdom -> location.statistical_region.long_term_unemployment_rate -> g.12cp_jvpx\n# Answer:\ng.12cp_jvpx", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Down\n# Answer:\nNorthern Ireland"], "ground_truth": ["England", "Wales", "Scotland", "Northern Ireland"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6153846153846154, "ans_precission": 0.8, "ans_recall": 0.5, "path_f1": 0.2727272727272727, "path_precision": 0.3, "path_recall": 0.25, "path_ans_f1": 0.6153846153846154, "path_ans_precision": 0.8, "path_ans_recall": 0.5}
{"id": "WebQTest-1115", "prediction": ["# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73505 -> location.location.geolocation -> m.03dw8k7\n# Answer:\n73505", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73505 -> location.postal_code.country -> United States of America\n# Answer:\n73505", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73507 -> location.location.containedby -> Comanche County\n# Answer:\n73507", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73505 -> location.location.containedby -> Comanche County\n# Answer:\n73505", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73502 -> common.topic.notable_types -> Postal Code\n# Answer:\n73502", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73507 -> common.topic.notable_types -> Postal Code\n# Answer:\n73507", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73507 -> location.location.geolocation -> m.03dw8kf\n# Answer:\n73507", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73502 -> location.postal_code.country -> United States of America\n# Answer:\n73502", "# Reasoning Path:\nLawton -> location.hud_county_place.county -> Comanche County -> location.location.containedby -> Oklahoma\n# Answer:\nComanche County", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73502 -> common.topic.notable_for -> g.1258s3mpy\n# Answer:\n73502"], "ground_truth": ["73507", "73505", "73506", "73501", "73502"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7200000000000001, "ans_precission": 0.9, "ans_recall": 0.6, "path_f1": 0.7200000000000001, "path_precision": 0.9, "path_recall": 0.6, "path_ans_f1": 0.7200000000000001, "path_ans_precision": 0.9, "path_ans_recall": 0.6}
{"id": "WebQTest-1116", "prediction": ["# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> base.aareas.schema.administrative_area.administrative_parent -> Community of Madrid\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\ng.1hhc37x4q", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> periodicals.newspaper_circulation_area.newspapers -> P\u00fablico\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> periodicals.newspaper_circulation_area.newspapers -> ADN\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc38hhk\n# Answer:\ng.1hhc38hhk", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> periodicals.newspaper_circulation_area.newspapers -> Diario AS\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_children -> Andalusia -> location.location.contains -> Serran\u00eda de Ronda\n# Answer:\nAndalusia", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_children -> Aragon -> location.location.containedby -> Iberian Peninsula\n# Answer:\nAragon", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc3dvnk\n# Answer:\ng.1hhc3dvnk", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_children -> Andalusia -> base.aareas.schema.administrative_area.administrative_children -> Province of Seville\n# Answer:\nAndalusia"], "ground_truth": ["Madrid"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1117", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Eschen\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.country.first_level_divisions -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Andreas Bielau\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria -> location.country.languages_spoken -> Bavarian Language\n# Answer:\nAustria", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Gamprin\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.location.partially_contains -> Alps\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Liechtenstein\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.country.first_level_divisions -> Eschen\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Andreas Diebitz\n# Answer:\nEast Germany"], "ground_truth": ["Czech Republic", "Cyprus", "Second Polish Republic", "Belgium", "West Germany", "Germany", "Vatican City", "Luxembourg", "Denmark", "Canada", "East Germany", "South Africa", "Austria", "Switzerland", "Liechtenstein"], "ans_acc": 0.26666666666666666, "ans_hit": 1, "ans_f1": 0.41142857142857137, "ans_precission": 0.9, "ans_recall": 0.26666666666666666, "path_f1": 0.32727272727272727, "path_precision": 0.9, "path_recall": 0.2, "path_ans_f1": 0.4210526315789474, "path_ans_precision": 1.0, "path_ans_recall": 0.26666666666666666}
{"id": "WebQTest-1118", "prediction": ["# Reasoning Path:\nMark McGwire -> baseball.baseball_player.batting_stats -> m.06rkf_h -> baseball.batting_statistics.season -> 1988 Major League Baseball Season\n# Answer:\nm.06rkf_h", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.batting_stats -> m.06rkfx7 -> baseball.batting_statistics.season -> 1986 Major League Baseball Season\n# Answer:\nm.06rkfx7", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.batting_stats -> m.06rkfyq -> baseball.batting_statistics.season -> 1987 Major League Baseball Season\n# Answer:\nm.06rkfyq", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.batting_stats -> m.06rkf_h -> baseball.batting_statistics.team -> Oakland Athletics\n# Answer:\nm.06rkf_h", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.position_s -> First baseman -> sports.sports_position.players -> g.11bxv84dh5\n# Answer:\nFirst baseman", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.position_s -> First baseman -> common.topic.notable_types -> Baseball Position\n# Answer:\nFirst baseman", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.batting_stats -> m.06rkfx7 -> baseball.batting_statistics.team -> Oakland Athletics\n# Answer:\nm.06rkfx7", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.batting_stats -> m.06rkfyq -> baseball.batting_statistics.team -> Oakland Athletics\n# Answer:\nm.06rkfyq", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.position_s -> First baseman -> sports.sports_position.sport -> Baseball\n# Answer:\nFirst baseman", "# Reasoning Path:\nMark McGwire -> award.award_nominee.award_nominations -> m.0sgpgnt -> award.award_nomination.award -> Kids' Choice Award for Favorite Male Athlete\n# Answer:\nm.0sgpgnt"], "ground_truth": ["2001 Major League Baseball Season"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1119", "prediction": ["# Reasoning Path:\nColorado Technical University -> education.educational_institution.phone_number -> m.0hprv5g\n# Answer:\nm.0hprv5g", "# Reasoning Path:\nColorado Technical University -> education.university.number_of_undergraduates -> m.0h783sd\n# Answer:\nm.0h783sd", "# Reasoning Path:\nColorado Technical University -> education.educational_institution.phone_number -> m.0nbc0rl\n# Answer:\nm.0nbc0rl", "# Reasoning Path:\nColorado Technical University -> education.university.number_of_undergraduates -> m.0hpxh03\n# Answer:\nm.0hpxh03"], "ground_truth": ["College/University"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.notable_types -> Profession\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.article -> m.0knj9\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Bard\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Poet\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.profession -> Writer\n# Answer:\nAnne Bront\u00eb", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.image -> Hope-coventina01a\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Author -> tv.tv_subject.tv_episodes -> Author David Kerr - The Ol' Turkey Hunter\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Cardiovascular disease\n# Answer:\nMale"], "ground_truth": ["Writer", "Author", "Bard", "Poet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.8, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1120", "prediction": ["# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> Andorra\n# Answer:\nFrench franc", "# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> French Polynesia\n# Answer:\nFrench franc", "# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> French Polynesia\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.11b60tv_69\n# Answer:\ng.11b60tv_69", "# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> Monaco\n# Answer:\nFrench franc", "# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> New Caledonia\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6gn9g\n# Answer:\ng.12tb6gn9g", "# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> Wallis and Futuna\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Andorra\n# Answer:\nEuro", "# Reasoning Path:\nFrance -> location.country.currency_used -> Euro -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nEuro"], "ground_truth": ["French franc", "Assignat"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.37499999999999994, "ans_precission": 0.3, "ans_recall": 0.5, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.37499999999999994, "path_ans_precision": 0.3, "path_ans_recall": 0.5}
{"id": "WebQTest-1121", "prediction": ["# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.0hz9xtw -> location.location_symbol_relationship.symbol -> Cardinal\n# Answer:\nm.0hz9xtw", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.0hz9xtw -> location.location_symbol_relationship.Kind_of_symbol -> State bird\n# Answer:\nm.0hz9xtw", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0d65 -> location.location_symbol_relationship.symbol -> Brachiopod\n# Answer:\nm.04l0d65", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0nqp -> location.location_symbol_relationship.symbol -> Liriodendron\n# Answer:\nm.04l0nqp", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0d65 -> location.location_symbol_relationship.Kind_of_symbol -> State fossil\n# Answer:\nm.04l0d65", "# Reasoning Path:\nKentucky -> military.military_unit_place_of_origin.military_units -> 10th Kentucky Cavalry Regiment (Diamond's) -> common.topic.notable_types -> Military unit\n# Answer:\n10th Kentucky Cavalry Regiment (Diamond's)", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0nqp -> location.location_symbol_relationship.Kind_of_symbol -> State tree\n# Answer:\nm.04l0nqp", "# Reasoning Path:\nKentucky -> common.topic.webpage -> m.09wljkf -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09wljkf", "# Reasoning Path:\nKentucky -> military.military_unit_place_of_origin.military_units -> 10th Kentucky Cavalry Regiment (Diamond's) -> military.military_unit.armed_force -> Confederate States Army\n# Answer:\n10th Kentucky Cavalry Regiment (Diamond's)", "# Reasoning Path:\nKentucky -> common.topic.webpage -> m.09xmm58 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09xmm58"], "ground_truth": ["United we stand, divided we fall"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1124", "prediction": ["# Reasoning Path:\nBewitched -> tv.tv_program.regular_cast -> m.0kdhbl1 -> tv.regular_tv_appearance.actor -> Maurice Evans\n# Answer:\nm.0kdhbl1", "# Reasoning Path:\nBewitched -> tv.tv_program.regular_cast -> m.02nrqs4 -> tv.regular_tv_appearance.actor -> Dick York\n# Answer:\nm.02nrqs4", "# Reasoning Path:\nBewitched -> tv.tv_program.regular_cast -> m.02nrqt1 -> tv.regular_tv_appearance.actor -> Agnes Moorehead\n# Answer:\nm.02nrqt1", "# Reasoning Path:\nBewitched -> tv.tv_program.regular_cast -> m.02nrqs4 -> tv.regular_tv_appearance.character -> Darrin\n# Answer:\nm.02nrqs4", "# Reasoning Path:\nBewitched -> tv.tv_program.regular_cast -> m.02nrqt1 -> tv.regular_tv_appearance.character -> Endora\n# Answer:\nm.02nrqt1", "# Reasoning Path:\nBewitched -> common.topic.webpage -> m.0941_05 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.0941_05", "# Reasoning Path:\nBewitched -> common.topic.webpage -> m.0941vyc -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.0941vyc"], "ground_truth": ["Elizabeth Montgomery"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1125", "prediction": ["# Reasoning Path:\nIsaac Newton -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.0101ddrk\n# Answer:\nPhysicist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Physicist -> people.profession.part_of_professional_field -> Physics\n# Answer:\nPhysicist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Philosopher -> people.profession.specializations -> Logician\n# Answer:\nPhilosopher", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.01066smw\n# Answer:\nPhysicist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Daniel Faraday\n# Answer:\nPhysicist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Chemist -> base.descriptive_names.names.descriptive_name -> m.01066g6n\n# Answer:\nChemist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Chemist -> base.schemastaging.context_name.pronunciation -> g.125_m09_2\n# Answer:\nChemist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.01066sxg\n# Answer:\nPhysicist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Chemist -> common.topic.notable_types -> Profession\n# Answer:\nChemist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Philosopher -> fictional_universe.character_rank.characters_of_this_rank -> Heraclitus\n# Answer:\nPhilosopher"], "ground_truth": ["Philosopher", "Physicist", "Astronomer", "Scientist", "Mathematician", "Chemist"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-1126", "prediction": ["# Reasoning Path:\nSan Diego International Airport -> aviation.airport.serves -> San Diego -> location.location.containedby -> San Diego County\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego -> location.location.containedby -> San Diego County -> location.location.containedby -> California\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego International Airport -> aviation.airport.serves -> San Diego -> location.location.containedby -> 92101\n# Answer:\n92101", "# Reasoning Path:\nSan Diego -> location.location.containedby -> San Diego County -> location.location.containedby -> United States of America\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego International Airport -> location.location.containedby -> San Diego County -> location.location.containedby -> California\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego International Airport -> aviation.airport.serves -> Ashford University -> location.location.containedby -> San Diego\n# Answer:\nAshford University", "# Reasoning Path:\nSan Diego -> location.location.containedby -> 92101 -> location.location.containedby -> San Diego County\n# Answer:\n92101", "# Reasoning Path:\nSan Diego International Airport -> aviation.airport.serves -> San Diego -> location.location.containedby -> Area code 858\n# Answer:\nArea code 858", "# Reasoning Path:\nSan Diego International Airport -> location.location.containedby -> San Diego County -> location.us_county.hud_county_place -> San Diego\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego -> location.hud_county_place.county -> San Diego County -> location.location.containedby -> California\n# Answer:\nSan Diego County"], "ground_truth": ["San Diego County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1127", "prediction": ["# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> Antigua and Barbuda -> location.country.languages_spoken -> English Language\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> Antigua and Barbuda -> location.country.form_of_government -> Parliamentary system\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> Bahamas -> location.country.languages_spoken -> English Language\n# Answer:\nBahamas", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> Bahamas -> location.location.containedby -> Americas\n# Answer:\nBahamas", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> Antigua and Barbuda -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> Bahamas -> base.uncommon.topic.exceptions -> City/Town/Village should pertain to Ghana, since City/Town/Village subdivides Ghanaian Municipal District.\n# Answer:\nBahamas", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> Canada -> location.location.containedby -> Americas\n# Answer:\nCanada", "# Reasoning Path:\nNorth America -> cvg.computer_game_region.versions_released_in_this_region -> 2Moons -> cvg.game_version.publisher -> Acclaim Games\n# Answer:\n2Moons", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> Canada -> base.uncommon.topic.exceptions -> City/Town/Village should pertain to Ghana, since City/Town/Village subdivides Ghanaian Municipal District.\n# Answer:\nCanada", "# Reasoning Path:\nNorth America -> location.location.partiallycontains -> m.0_gjrm_ -> location.partial_containment_relationship.partially_contains -> Nordic countries\n# Answer:\nm.0_gjrm_"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1130", "prediction": ["# Reasoning Path:\nSanta Claus -> common.topic.webpage -> m.09w47wj -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w47wj", "# Reasoning Path:\nSanta Claus -> film.film_character.portrayed_in_films -> m.010rvdmk -> film.performance.actor -> Googy Gress\n# Answer:\nm.010rvdmk", "# Reasoning Path:\nSanta Claus -> film.film.genre -> Cult film\n# Answer:\nCult film", "# Reasoning Path:\nSanta Claus -> common.topic.webpage -> m.09w85m2 -> common.webpage.resource -> Uma Thurman, Jesse L. Martin to appear in NBC's Muppets holiday special\n# Answer:\nm.09w85m2", "# Reasoning Path:\nSanta Claus -> film.film_character.portrayed_in_films -> m.0111f_4n -> film.performance.actor -> Jim Broadbent\n# Answer:\nm.0111f_4n", "# Reasoning Path:\nSanta Claus -> common.topic.webpage -> m.09w47wj -> common.webpage.resource -> Exclusive: Look what Santa Claus brought me!\n# Answer:\nm.09w47wj", "# Reasoning Path:\nSanta Claus -> common.topic.webpage -> m.09w9dng -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w9dng", "# Reasoning Path:\nSanta Claus -> common.topic.webpage -> m.09w85m2 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w85m2", "# Reasoning Path:\nSanta Claus -> film.film_character.portrayed_in_films -> m.010rvdmk -> film.performance.film -> Naughty or Nice\n# Answer:\nm.010rvdmk", "# Reasoning Path:\nSanta Claus -> common.topic.webpage -> m.09w9dng -> common.webpage.resource -> Judge In 'Watchmen' Case Rules For Fox; Fans Brace For Fallout\n# Answer:\nm.09w9dng"], "ground_truth": ["Saint Nicholas"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1131", "prediction": ["# Reasoning Path:\nKelly Slater -> people.person.profession -> Surfer -> fictional_universe.character_occupation.characters_with_this_occupation -> Merliah Summers\n# Answer:\nSurfer", "# Reasoning Path:\nKelly Slater -> people.person.profession -> Surfer -> people.profession.specialization_of -> Athlete\n# Answer:\nSurfer", "# Reasoning Path:\nKelly Slater -> people.person.profession -> Surfer -> common.topic.notable_types -> Profession\n# Answer:\nSurfer", "# Reasoning Path:\nKelly Slater -> people.person.profession -> Actor -> people.profession.specializations -> Voice Actor\n# Answer:\nActor", "# Reasoning Path:\nKelly Slater -> tv.tv_actor.guest_roles -> m.09nw1h8 -> tv.tv_guest_role.episodes_appeared_in -> Tentacles (1)\n# Answer:\nm.09nw1h8", "# Reasoning Path:\nKelly Slater -> people.person.profession -> Actor -> common.topic.subjects -> Michael Palance\n# Answer:\nActor", "# Reasoning Path:\nKelly Slater -> people.person.profession -> Film Producer -> base.descriptive_names.names.descriptive_name -> m.0106b5xy\n# Answer:\nFilm Producer", "# Reasoning Path:\nKelly Slater -> award.award_nominee.award_nominations -> m.0yw3ngv -> award.award_nomination.ceremony -> 2012 Laureus World Sports Awards\n# Answer:\nm.0yw3ngv", "# Reasoning Path:\nKelly Slater -> people.person.profession -> Actor -> common.topic.subjects -> Chris Free\n# Answer:\nActor", "# Reasoning Path:\nKelly Slater -> people.person.profession -> Actor -> common.topic.subjects -> Jeremy Yablan\n# Answer:\nActor"], "ground_truth": ["1990"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1132", "prediction": ["# Reasoning Path:\nCoretta Scott King -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.includes_causes_of_death -> Choking\n# Answer:\nRespiratory failure", "# Reasoning Path:\nCoretta Scott King -> people.deceased_person.cause_of_death -> Respiratory failure -> common.topic.notable_types -> Symptom\n# Answer:\nRespiratory failure", "# Reasoning Path:\nCoretta Scott King -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nRespiratory failure", "# Reasoning Path:\nCoretta Scott King -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.includes_causes_of_death -> Erotic asphyxiation\n# Answer:\nRespiratory failure", "# Reasoning Path:\nCoretta Scott King -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.includes_causes_of_death -> Strangling\n# Answer:\nRespiratory failure", "# Reasoning Path:\nCoretta Scott King -> people.person.ethnicity -> African American -> common.topic.webpage -> m.09w10n8\n# Answer:\nAfrican American", "# Reasoning Path:\nCoretta Scott King -> people.person.ethnicity -> African American -> common.topic.webpage -> m.09w163n\n# Answer:\nAfrican American", "# Reasoning Path:\nCoretta Scott King -> people.person.ethnicity -> African American -> visual_art.art_subject.artwork_on_the_subject -> African-American Monument\n# Answer:\nAfrican American", "# Reasoning Path:\nCoretta Scott King -> people.person.ethnicity -> African American -> common.topic.webpage -> m.09w7g5_\n# Answer:\nAfrican American", "# Reasoning Path:\nCoretta Scott King -> people.person.ethnicity -> African American -> book.school_or_movement.associated_works -> 10 Great Moments in African-American History\n# Answer:\nAfrican American"], "ground_truth": ["Respiratory failure"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1133", "prediction": ["# Reasoning Path:\nWilliam Henry Smith -> common.topic.notable_types -> Politician -> common.topic.article -> m.02xlj10\n# Answer:\nPolitician", "# Reasoning Path:\nWilliam Henry Smith -> people.person.spouse_s -> m.0j4mrzc -> people.marriage.spouse -> Emily Smith, 1st Viscountess Hambleden\n# Answer:\nm.0j4mrzc", "# Reasoning Path:\nWilliam Henry Smith -> people.person.place_of_birth -> London -> location.location.partially_contains -> River Lea\n# Answer:\nLondon", "# Reasoning Path:\nWilliam Henry Smith -> common.topic.notable_types -> Politician -> type.type.expected_by -> politician\n# Answer:\nPolitician", "# Reasoning Path:\nWilliam Henry Smith -> people.person.place_of_birth -> London -> periodicals.newspaper_circulation_area.newspapers -> Al-Hayat\n# Answer:\nLondon", "# Reasoning Path:\nWilliam Henry Smith -> people.person.spouse_s -> m.0j4mrzc -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0j4mrzc", "# Reasoning Path:\nWilliam Henry Smith -> people.person.place_of_birth -> London -> location.location.partially_contains -> River Thames\n# Answer:\nLondon", "# Reasoning Path:\nWilliam Henry Smith -> people.person.place_of_birth -> London -> organization.organization_scope.organizations_with_this_scope -> The TEFL Academy Ashford\n# Answer:\nLondon", "# Reasoning Path:\nWilliam Henry Smith -> common.topic.notable_types -> Politician -> type.type.expected_by -> Member\n# Answer:\nPolitician", "# Reasoning Path:\nWilliam Henry Smith -> people.person.place_of_birth -> London -> organization.organization_scope.organizations_with_this_scope -> Box UK\n# Answer:\nLondon"], "ground_truth": ["WHSmith"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1135", "prediction": ["# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Gerald Ford -> base.americancomedy.impersonated_celebrity.impersonated_by -> Rich Little\n# Answer:\nGerald Ford", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Spiro Agnew -> people.person.quotations -> A spirit of national masochism prevails, encouraged by an effete corps of impudent snobs who characterize themselves as intellectuals.\n# Answer:\nSpiro Agnew", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Spiro Agnew -> people.person.quotations -> Freedom of speech is useless without freedom of thought. And I fear that the politics of protest is shutting out the process of thought, so necessary to rational discussion. We are faced with the Ten Commandments of Protest:Thou Shalt Not Allow Thy Opponent to Speak. Thou Shalt Not Set Forth a Program of Thine Own. Thou Shalt Not Trust Anybody Over Thirty. Thou Shalt Not Honor Thy Father or Thy Mother. Thou Shalt Not Heed the Lessons of History. Thou Shalt Not Write Anything Longer than a Slogan. Thou Shalt Not Present a Negotiable Demand. Thou Shalt Not Accept Any Establishment Idea. Thou Shalt Not Revere Any but Totalitarian Heroes. Thou Shalt Not Ask Forgiveness for Thy Transgressions, Rather Thou Shalt Demand Amnesty for Them.\n# Answer:\nSpiro Agnew", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Spiro Agnew -> film.person_or_entity_appearing_in_film.films -> m.0114dk_n\n# Answer:\nSpiro Agnew", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Spiro Agnew -> people.person.quotations -> In the United States today, we have more than our share of the nattering nabobs of negativism. They have formed their own 4H Clubthe hopeless, hysterical hypochondriacs of history.\n# Answer:\nSpiro Agnew", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Spiro Agnew -> people.person.education -> m.02wpd1f\n# Answer:\nSpiro Agnew", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.employment_history -> m.0k0dcyp\n# Answer:\nJulie Nixon Eisenhower", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Spiro Agnew -> film.person_or_entity_appearing_in_film.films -> m.0cv_zwx\n# Answer:\nSpiro Agnew", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Gerald Ford -> freebase.valuenotation.is_reviewed -> Children\n# Answer:\nGerald Ford", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Spiro Agnew -> film.person_or_entity_appearing_in_film.films -> m.0w1mtml\n# Answer:\nSpiro Agnew"], "ground_truth": ["Gerald Ford", "Spiro Agnew"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1136", "prediction": ["# Reasoning Path:\nAdolf Hitler -> people.person.religion -> Catholicism -> religion.religion.is_part_of -> Christianity\n# Answer:\nCatholicism", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ng.1254zbtvv", "# Reasoning Path:\nAdolf Hitler -> people.person.religion -> Catholicism -> religion.religion.branched_from -> Judaism\n# Answer:\nCatholicism", "# Reasoning Path:\nAdolf Hitler -> people.person.religion -> Christianity -> religion.religion.branched_from -> Judaism\n# Answer:\nChristianity", "# Reasoning Path:\nAdolf Hitler -> people.person.religion -> Christianity -> religion.religion.includes -> Catholicism\n# Answer:\nChristianity", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09wf140", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.resource -> Another Megan Fox letter -- and this one sings her praises!\n# Answer:\nm.09wf140", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wgn1j -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09wgn1j", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wgn1j -> common.webpage.resource -> Charles Manson: 40 years later, the movie about him you have to see\n# Answer:\nm.09wgn1j", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wqfb3 -> common.webpage.resource -> Erykah Badu teases 'New Amerykah Part Two' with trippy, funky clip\n# Answer:\nm.09wqfb3"], "ground_truth": ["Catholicism", "Christianity"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1137", "prediction": ["# Reasoning Path:\nIllinois River -> location.location.containedby -> Illinois -> location.location.containedby -> Contiguous United States\n# Answer:\nIllinois", "# Reasoning Path:\nIllinois River -> geography.river.mouth -> Mississippi River -> location.location.containedby -> United States of America\n# Answer:\nMississippi River", "# Reasoning Path:\nIllinois River -> location.location.containedby -> Illinois -> location.location.containedby -> East North Central States\n# Answer:\nIllinois", "# Reasoning Path:\nIllinois River -> location.location.containedby -> Illinois -> location.location.containedby -> Midwestern United States\n# Answer:\nIllinois", "# Reasoning Path:\nIllinois River -> geography.river.mouth -> Mississippi River -> location.location.containedby -> North America\n# Answer:\nMississippi River", "# Reasoning Path:\nIllinois River -> location.location.containedby -> Illinois -> location.statistical_region.religions -> m.04403jk\n# Answer:\nIllinois", "# Reasoning Path:\nIllinois River -> location.location.containedby -> Illinois -> common.topic.webpage -> m.04lstq9\n# Answer:\nIllinois", "# Reasoning Path:\nIllinois River -> location.location.containedby -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nIllinois River -> location.location.containedby -> United States of America -> location.location.containedby -> Americas\n# Answer:\nUnited States of America", "# Reasoning Path:\nIllinois River -> geography.river.mouth -> Mississippi River -> location.location.partially_containedby -> Illinois\n# Answer:\nMississippi River"], "ground_truth": ["North America", "United States of America", "Illinois"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1138", "prediction": ["# Reasoning Path:\nHonduras -> base.locations.countries.continent -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nHonduras -> location.location.containedby -> Central America -> location.location.containedby -> Americas\n# Answer:\nCentral America", "# Reasoning Path:\nHonduras -> location.location.containedby -> Latin America -> location.location.containedby -> Americas\n# Answer:\nLatin America", "# Reasoning Path:\nHonduras -> base.locations.countries.continent -> North America -> location.location.contains_major_portion_of -> United States of America\n# Answer:\nNorth America", "# Reasoning Path:\nHonduras -> base.locations.countries.continent -> North America -> base.locations.continents.countries_within -> Bahamas\n# Answer:\nNorth America", "# Reasoning Path:\nHonduras -> location.location.containedby -> Americas -> location.location.contains -> Latin America\n# Answer:\nAmericas", "# Reasoning Path:\nHonduras -> base.locations.countries.continent -> North America -> base.locations.continents.countries_within -> Belize\n# Answer:\nNorth America", "# Reasoning Path:\nHonduras -> location.location.containedby -> Central America -> location.location.contains -> Belize\n# Answer:\nCentral America", "# Reasoning Path:\nHonduras -> base.locations.countries.continent -> North America -> base.locations.continents.countries_within -> Cuba\n# Answer:\nNorth America", "# Reasoning Path:\nHonduras -> location.location.containedby -> Central America -> meteorology.cyclone_affected_area.cyclones -> Hurricane Abby\n# Answer:\nCentral America"], "ground_truth": ["Latin America", "Central America", "North America", "Americas"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.actor -> Hayden Christensen\n# Answer:\nm.02sg5ry", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.actor -> Hayden Christensen\n# Answer:\nm.02sg5s6", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.film -> Star Wars Episode III: Revenge of the Sith\n# Answer:\nm.02sg5ry", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.film -> Star Wars Episode II: Attack of the Clones\n# Answer:\nm.02sg5s6", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.part_of_series -> Jedi Quest\n# Answer:\nPath to Truth", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.author -> Judy Blundell\n# Answer:\nPath to Truth"], "ground_truth": ["Hayden Christensen"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1142", "prediction": ["# Reasoning Path:\nRonaldinho -> sports.pro_athlete.teams -> m.0j3_kkz -> sports.sports_team_roster.team -> Brazil national football team\n# Answer:\nm.0j3_kkz", "# Reasoning Path:\nRonaldinho -> sports.pro_athlete.teams -> m.0j3_kkz -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0j3_kkz", "# Reasoning Path:\nRonaldinho -> sports.pro_athlete.teams -> m.011qx677 -> sports.sports_team_roster.team -> Quer\u00e9taro F.C.\n# Answer:\nm.011qx677", "# Reasoning Path:\nRonaldinho -> sports.pro_athlete.teams -> m.0j3_kkz -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0j3_kkz", "# Reasoning Path:\nRonaldinho -> people.person.nationality -> Brazil -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\nBrazil", "# Reasoning Path:\nRonaldinho -> soccer.football_player.statistics -> m.0w9db76 -> soccer.football_player_stats.team -> Brazil national football team\n# Answer:\nm.0w9db76", "# Reasoning Path:\nRonaldinho -> sports.pro_athlete.teams -> m.0123wrqk -> sports.sports_team_roster.position -> Forward\n# Answer:\nm.0123wrqk", "# Reasoning Path:\nRonaldinho -> people.person.nationality -> Brazil -> location.country.currency_used -> Brazilian real\n# Answer:\nBrazil", "# Reasoning Path:\nRonaldinho -> soccer.football_player.statistics -> m.0w9b586 -> soccer.football_player_stats.team -> Clube de Regatas do Flamengo\n# Answer:\nm.0w9b586", "# Reasoning Path:\nRonaldinho -> sports.pro_athlete.teams -> m.011qx677 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.011qx677"], "ground_truth": ["Brazil national football team", "Clube de Regatas do Flamengo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1143", "prediction": ["# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Astronomer -> people.profession.specialization_of -> Scientist\n# Answer:\nAstronomer", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Daniel Faraday\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Emmett Brown\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.0101ddrk\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Mathematician -> base.descriptive_names.names.descriptive_name -> m.01066gkh\n# Answer:\nMathematician", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> J. Robert Oppenheimer\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> common.topic.image -> Albert Einstein 1947\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Astronomer -> base.ontologies.ontology_instance.equivalent_instances -> m.07ndjrd\n# Answer:\nAstronomer", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.01066smw\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Mathematician -> base.descriptive_names.names.descriptive_name -> m.010f3g_r\n# Answer:\nMathematician"], "ground_truth": ["Heliocentrism", "Solar System", "Kinematics"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1144", "prediction": ["# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\ng.11b60v9zy7", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Austria\n# Answer:\nFederal republic", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_38m_\n# Answer:\ng.1245_38m_", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Austria\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fdl8\n# Answer:\ng.12tb6fdl8", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc38c_4\n# Answer:\ng.1hhc38c_4", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_67jr\n# Answer:\ng.1245_67jr", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_jlz4\n# Answer:\ng.1245_jlz4", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Czech Republic\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Poland\n# Answer:\nParliamentary republic"], "ground_truth": ["Tuvalu", "Scotland", "Tanzania", "Afghanistan", "Albania", "Nepal", "Algeria", "Madagascar", "Antigua and Barbuda", "United States of America"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1145", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.institution -> Harvard Law School\n# Answer:\nm.02kvkg9", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkf4 -> education.education.institution -> Stanford University\n# Answer:\nm.02kvkf4", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.major_field_of_study -> Law\n# Answer:\nm.02kvkg9", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkf4 -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nm.02kvkf4", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.0123vxrw -> education.educational_institution.students_graduates -> m.0123vxqw\n# Answer:\nm.0123vxrw", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0_gvz4k -> film.personal_film_appearance.film -> Mitt\n# Answer:\nm.0_gvz4k", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0_gvz4k -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.0_gvz4k", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0nhmmbv -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.0nhmmbv", "# Reasoning Path:\nMitt Romney -> tv.tv_actor.guest_roles -> m.0bvvym_ -> tv.tv_guest_role.episodes_appeared_in -> The Comeback of President Bush\n# Answer:\nm.0bvvym_", "# Reasoning Path:\nMitt Romney -> tv.tv_actor.guest_roles -> m.0bvw9my -> tv.tv_guest_role.episodes_appeared_in -> Mitt Romney, Ryan Sheckler, Jonas Brothers\n# Answer:\nm.0bvw9my"], "ground_truth": ["Brigham Young University"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1146", "prediction": ["# Reasoning Path:\nJohnny Depp -> film.actor.film -> m.0jwq1q -> film.performance.character -> Victor Van Dort\n# Answer:\nm.0jwq1q", "# Reasoning Path:\nCorpse Bride -> film.film.starring -> m.0jwq1q -> film.performance.character -> Victor Van Dort\n# Answer:\nm.0jwq1q", "# Reasoning Path:\nJohnny Depp -> film.actor.film -> g.11b6tp52w8\n# Answer:\ng.11b6tp52w8", "# Reasoning Path:\nJohnny Depp -> film.actor.film -> m.0jwq1q -> film.performance.film -> Corpse Bride\n# Answer:\nm.0jwq1q", "# Reasoning Path:\nCorpse Bride -> film.film.starring -> m.03jsyd7 -> film.performance.character -> Elder Gutknecht\n# Answer:\nm.03jsyd7", "# Reasoning Path:\nCorpse Bride -> film.film.starring -> m.0jwq1q -> film.performance.actor -> Johnny Depp\n# Answer:\nm.0jwq1q", "# Reasoning Path:\nJohnny Depp -> film.actor.film -> m.0107gr6b -> film.performance.character -> Guy LaPointe\n# Answer:\nm.0107gr6b", "# Reasoning Path:\nCorpse Bride -> film.film.starring -> m.03l4b_9 -> film.performance.character -> Nell Van Dort\n# Answer:\nm.03l4b_9", "# Reasoning Path:\nCorpse Bride -> film.film.starring -> m.03jsyd7 -> film.performance.special_performance_type -> Voice\n# Answer:\nm.03jsyd7", "# Reasoning Path:\nCorpse Bride -> film.film.other_crew -> m.0gyph9s -> film.film_crew_gig.film_crew_role -> Audio Engineer\n# Answer:\nm.0gyph9s"], "ground_truth": ["Victor Van Dort"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1147", "prediction": ["# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72401 -> location.location.contains -> 1211 Stone\n# Answer:\n72401", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72401 -> location.postal_code.country -> United States of America\n# Answer:\n72401", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72403 -> common.topic.notable_for -> g.1259zdl3v\n# Answer:\n72403", "# Reasoning Path:\nJonesboro -> base.wikipedia_infobox.settlement.area_code -> Area code 870 -> location.location.containedby -> Arkansas\n# Answer:\nArea code 870", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72401 -> location.location.containedby -> Craighead County\n# Answer:\n72401", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72402 -> common.topic.notable_for -> g.125drjskq\n# Answer:\n72402", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72401 -> location.location.contains -> 1222 W Monroe @ West Elementary\n# Answer:\n72401", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72403 -> location.postal_code.country -> United States of America\n# Answer:\n72403", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72401 -> location.location.contains -> 1st Care Occupational Clinic on Highland\n# Answer:\n72401", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72403 -> common.topic.notable_types -> Postal Code\n# Answer:\n72403"], "ground_truth": ["72403", "72404", "72401", "72402"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.8181818181818182, "path_precision": 0.9, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-1148", "prediction": ["# Reasoning Path:\nS. E. Hinton -> people.person.education -> m.04hd46b -> education.education.institution -> University of Tulsa\n# Answer:\nm.04hd46b", "# Reasoning Path:\nS. E. Hinton -> people.person.education -> m.0n1jntg -> education.education.institution -> Will Rogers High School\n# Answer:\nm.0n1jntg", "# Reasoning Path:\nS. E. Hinton -> book.author.book_editions_published -> Hawkes Harbor -> book.book_edition.publisher -> Tor Books\n# Answer:\nHawkes Harbor", "# Reasoning Path:\nS. E. Hinton -> film.actor.film -> m.0t_lwd8 -> film.performance.character -> Hooker on Strip\n# Answer:\nm.0t_lwd8", "# Reasoning Path:\nS. E. Hinton -> book.author.book_editions_published -> Outsiders -> common.topic.notable_types -> Book Edition\n# Answer:\nOutsiders", "# Reasoning Path:\nS. E. Hinton -> film.actor.film -> m.0vn2n_r -> film.performance.character -> Mrs. Barnes\n# Answer:\nm.0vn2n_r", "# Reasoning Path:\nS. E. Hinton -> book.author.book_editions_published -> Hawkes Harbor (Brilliance Audio on Compact Disc) -> common.topic.notable_for -> g.12571zn1r\n# Answer:\nHawkes Harbor (Brilliance Audio on Compact Disc)", "# Reasoning Path:\nS. E. Hinton -> book.author.book_editions_published -> Hawkes Harbor (Brilliance Audio on Compact Disc) -> common.topic.notable_types -> Book Edition\n# Answer:\nHawkes Harbor (Brilliance Audio on Compact Disc)", "# Reasoning Path:\nS. E. Hinton -> film.actor.film -> m.0t_lwd8 -> film.performance.film -> Rumble Fish\n# Answer:\nm.0t_lwd8", "# Reasoning Path:\nS. E. Hinton -> book.author.book_editions_published -> Hawkes Harbor -> book.book.genre -> Fantasy\n# Answer:\nHawkes Harbor"], "ground_truth": ["University of Tulsa"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1149", "prediction": ["# Reasoning Path:\nFearless Tour -> music.concert_tour.artist -> Taylor Swift -> people.person.place_of_birth -> Reading\n# Answer:\nTaylor Swift", "# Reasoning Path:\nFearless Tour -> music.concert_tour.artist -> Taylor Swift -> base.popstra.celebrity.infidelity_perpetrator -> m.06530lq\n# Answer:\nTaylor Swift", "# Reasoning Path:\nFearless Tour -> music.concert_tour.artist -> Taylor Swift -> celebrities.celebrity.sexual_relationships -> m.0pcnxtx\n# Answer:\nTaylor Swift", "# Reasoning Path:\nFearless Tour -> music.concert_tour.artist -> Taylor Swift -> celebrities.celebrity.sexual_relationships -> m.0t54pz9\n# Answer:\nTaylor Swift", "# Reasoning Path:\nFearless Tour -> common.topic.notable_types -> Concert tour -> type.type.expected_by -> Concert Tour\n# Answer:\nConcert tour", "# Reasoning Path:\nFearless Tour -> common.topic.notable_types -> Concert tour -> type.type.properties -> Artist\n# Answer:\nConcert tour", "# Reasoning Path:\nFearless Tour -> common.topic.notable_types -> Concert tour -> common.topic.article -> m.01z0n4w\n# Answer:\nConcert tour", "# Reasoning Path:\nFearless Tour -> common.topic.webpage -> m.09y9kf8 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09y9kf8", "# Reasoning Path:\nFearless Tour -> common.topic.notable_types -> Concert tour -> type.type.expected_by -> Concert Tours\n# Answer:\nConcert tour", "# Reasoning Path:\nFearless Tour -> common.topic.webpage -> m.09y9kf8 -> common.webpage.resource -> Taylor Swift to headline Fearless Tour: Awesome? Scary?\n# Answer:\nm.09y9kf8"], "ground_truth": ["2009-04-23"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.season -> 2006 NFL season\n# Answer:\nm.04kcv8b", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_8 -> sports.sports_award.season -> 2003 NFL season\n# Answer:\nm.04kg9_8", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_j -> sports.sports_award.season -> 2004 NFL season\n# Answer:\nm.04kg9_j", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.award -> Super Bowl Most Valuable Player Award\n# Answer:\nm.04kcv8b", "# Reasoning Path:\nPeyton Manning -> people.person.spouse_s -> m.0hpd4nj -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nm.0hpd4nj", "# Reasoning Path:\nPeyton Manning -> people.person.spouse_s -> m.0hpd4nj -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0hpd4nj", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nPeyton Manning -> people.person.spouse_s -> m.0hpd4nj -> people.marriage.spouse -> Ashley Thompson\n# Answer:\nm.0hpd4nj"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1151", "prediction": ["# Reasoning Path:\nCharles Darwin -> influence.influence_node.influenced -> Karl Marx -> people.deceased_person.place_of_death -> London\n# Answer:\nKarl Marx", "# Reasoning Path:\nCharles Darwin -> influence.influence_node.influenced -> Sigmund Freud -> people.deceased_person.place_of_death -> London\n# Answer:\nSigmund Freud", "# Reasoning Path:\nCharles Darwin -> influence.influence_node.influenced -> John Bowlby -> influence.influence_node.influenced -> Jenny Aubry\n# Answer:\nJohn Bowlby", "# Reasoning Path:\nCharles Darwin -> influence.influence_node.influenced -> Karl Marx -> people.person.profession -> Economist\n# Answer:\nKarl Marx", "# Reasoning Path:\nCharles Darwin -> influence.influence_node.influenced -> John Bowlby -> influence.influence_node.influenced -> Mary Ainsworth\n# Answer:\nJohn Bowlby", "# Reasoning Path:\nCharles Darwin -> influence.influence_node.influenced -> John Bowlby -> people.person.profession -> Psychologist\n# Answer:\nJohn Bowlby", "# Reasoning Path:\nCharles Darwin -> influence.influence_node.influenced -> Karl Marx -> influence.influence_node.influenced_by -> Adam Smith\n# Answer:\nKarl Marx", "# Reasoning Path:\nCharles Darwin -> influence.influence_node.influenced -> John Bowlby -> book.author.works_written -> A Secure Base\n# Answer:\nJohn Bowlby", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> influence.influence_node.influenced -> Karl Marx -> people.person.profession -> Journalist\n# Answer:\nKarl Marx"], "ground_truth": ["London", "Shrewsbury", "England", "Downe, Kent"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.14285714285714288, "ans_precission": 0.1, "ans_recall": 0.25, "path_f1": 0.22222222222222224, "path_precision": 0.3, "path_recall": 0.17647058823529413, "path_ans_f1": 0.37499999999999994, "path_ans_precision": 0.3, "path_ans_recall": 0.5}
{"id": "WebQTest-1152", "prediction": ["# Reasoning Path:\nKITT -> tv.tv_character.appeared_in_tv_program -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKITT -> film.film_character.portrayed_in_films -> m.0j7ph8m -> film.performance.actor -> William Daniels\n# Answer:\nm.0j7ph8m", "# Reasoning Path:\nKITT -> tv.tv_character.appeared_in_tv_program -> m.07spfgl -> tv.regular_tv_appearance.actor -> Val Kilmer\n# Answer:\nm.07spfgl", "# Reasoning Path:\nKITT -> tv.tv_character.appeared_in_tv_episodes -> m.09nfwd3 -> tv.tv_guest_role.actor -> William Daniels\n# Answer:\nm.09nfwd3", "# Reasoning Path:\nKITT -> tv.tv_character.appeared_in_tv_program -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 3\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKITT -> film.film_character.portrayed_in_films -> m.0j7ph8m -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0j7ph8m", "# Reasoning Path:\nKITT -> tv.tv_character.appeared_in_tv_program -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 4\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKITT -> tv.tv_character.appeared_in_tv_program -> m.07spfgl -> tv.regular_tv_appearance.series -> Knight Rider\n# Answer:\nm.07spfgl"], "ground_truth": ["William Daniels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1153", "prediction": ["# Reasoning Path:\nEminem -> people.person.ethnicity -> Scotch-Irish American -> people.ethnicity.people -> Miley Cyrus\n# Answer:\nScotch-Irish American", "# Reasoning Path:\nEminem -> people.person.ethnicity -> Scotch-Irish American -> people.ethnicity.people -> George W. Bush\n# Answer:\nScotch-Irish American", "# Reasoning Path:\nEminem -> music.artist.origin -> Detroit -> location.location.containedby -> United States of America\n# Answer:\nDetroit", "# Reasoning Path:\nEminem -> people.person.ethnicity -> Scotch-Irish American -> people.ethnicity.people -> John McCain\n# Answer:\nScotch-Irish American", "# Reasoning Path:\nEminem -> music.artist.origin -> Detroit -> travel.travel_destination.tourist_attractions -> Comerica Park\n# Answer:\nDetroit", "# Reasoning Path:\nEminem -> music.artist.origin -> Warren -> location.location.containedby -> United States of America\n# Answer:\nWarren", "# Reasoning Path:\nEminem -> music.producer.releases_produced -> All or Nothing -> common.topic.notable_for -> g.1258d_62k\n# Answer:\nAll or Nothing", "# Reasoning Path:\nEminem -> music.producer.releases_produced -> All or Nothing -> common.topic.notable_types -> Musical Release\n# Answer:\nAll or Nothing", "# Reasoning Path:\nEminem -> music.producer.releases_produced -> B.o.B. Presents The Adventures of Bobby Ray -> common.topic.notable_for -> g.125761_vk\n# Answer:\nB.o.B. Presents The Adventures of Bobby Ray", "# Reasoning Path:\nEminem -> music.producer.releases_produced -> Before Hell Freezes Over -> common.topic.notable_for -> g.12586g1_z\n# Answer:\nBefore Hell Freezes Over"], "ground_truth": ["Marshall Bruce Mathers III"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1154", "prediction": ["# Reasoning Path:\nAmy Grant -> people.person.spouse_s -> m.0h4q7j_ -> people.marriage.spouse -> Vince Gill\n# Answer:\nm.0h4q7j_", "# Reasoning Path:\nAmy Grant -> people.person.spouse_s -> m.0w2ygvd -> people.marriage.spouse -> Gary Chapman\n# Answer:\nm.0w2ygvd", "# Reasoning Path:\nAmy Grant -> people.person.spouse_s -> m.0h4q7j_ -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.0h4q7j_", "# Reasoning Path:\nAmy Grant -> people.person.spouse_s -> m.0h4q7j_ -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0h4q7j_", "# Reasoning Path:\nAmy Grant -> music.artist.album -> 20th Century Masters: The Best of Amy Grant: The Christmas Collection -> common.topic.notable_for -> g.12h33zs8t\n# Answer:\n20th Century Masters: The Best of Amy Grant: The Christmas Collection", "# Reasoning Path:\nAmy Grant -> people.person.spouse_s -> m.0w2ygvd -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.0w2ygvd", "# Reasoning Path:\nAmy Grant -> people.person.spouse_s -> m.0w2ygvd -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0w2ygvd", "# Reasoning Path:\nAmy Grant -> music.artist.album -> A Christmas Album -> music.album.release_type -> Album\n# Answer:\nA Christmas Album", "# Reasoning Path:\nAmy Grant -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAmy Grant -> music.artist.album -> 20th Century Masters: The Best of Amy Grant: The Christmas Collection -> music.album.album_content_type -> Compilation album\n# Answer:\n20th Century Masters: The Best of Amy Grant: The Christmas Collection"], "ground_truth": ["Gary Chapman"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1155", "prediction": ["# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.location.contains -> Belize District\n# Answer:\nBelize", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.location.contains -> Cayo District\n# Answer:\nBelize", "# Reasoning Path:\nCentral America -> location.location.contains -> Panama -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nPanama", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.location.contains -> Corozal District\n# Answer:\nBelize", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.location.containedby -> Americas\n# Answer:\nBelize", "# Reasoning Path:\nCentral America -> location.location.contains -> Panama -> location.location.partiallycontains -> m.0wg9hnr\n# Answer:\nPanama", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.country.administrative_divisions -> Belize District\n# Answer:\nBelize", "# Reasoning Path:\nCentral America -> location.location.contains -> Honduras -> location.location.containedby -> Americas\n# Answer:\nHonduras", "# Reasoning Path:\nCentral America -> location.location.contains -> Honduras -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nHonduras", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.location.containedby -> Latin America\n# Answer:\nBelize"], "ground_truth": ["Guatemala", "Gran Colombia", "Belize", "Honduras", "Panama", "Costa Rica", "El Salvador"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 1.0, "ans_recall": 0.42857142857142855, "path_f1": 0.6, "path_precision": 1.0, "path_recall": 0.42857142857142855, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1156", "prediction": ["# Reasoning Path:\nJamie Lynn Spears -> people.person.children -> Maddie Briann Aldridge -> people.person.parents -> Casey Aldridge\n# Answer:\nMaddie Briann Aldridge", "# Reasoning Path:\nJamie Lynn Spears -> people.person.children -> Maddie Briann Aldridge -> common.topic.notable_for -> g.125g6_bhr\n# Answer:\nMaddie Briann Aldridge", "# Reasoning Path:\nJamie Lynn Spears -> people.person.children -> Maddie Briann Aldridge -> people.person.nationality -> United States of America\n# Answer:\nMaddie Briann Aldridge", "# Reasoning Path:\nJamie Lynn Spears -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#range -> Person\n# Answer:\nChildren", "# Reasoning Path:\nJamie Lynn Spears -> people.person.place_of_birth -> McComb -> location.location.people_born_here -> Maddie Briann Aldridge\n# Answer:\nMcComb", "# Reasoning Path:\nJamie Lynn Spears -> freebase.valuenotation.is_reviewed -> Children -> type.property.schema -> Person\n# Answer:\nChildren", "# Reasoning Path:\nJamie Lynn Spears -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#domain -> Person\n# Answer:\nChildren", "# Reasoning Path:\nJamie Lynn Spears -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nJamie Lynn Spears -> people.person.place_of_birth -> McComb -> common.topic.notable_for -> g.12590pzfj\n# Answer:\nMcComb", "# Reasoning Path:\nJamie Lynn Spears -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth"], "ground_truth": ["Maddie Briann Aldridge"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Canada\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> base.wikipedia_infobox.settlement.area_code -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Canada\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Ontario\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> common.topic.notable_types -> City/Town/Village\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Ontario\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.people_born_here -> Jeremy Bieber\n# Answer:\nStratford"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1160", "prediction": ["# Reasoning Path:\nMichael Jackson -> people.person.parents -> Katherine Jackson -> people.person.children -> La Toya Jackson\n# Answer:\nKatherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Katherine Jackson -> people.person.nationality -> United States of America\n# Answer:\nKatherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Katherine Jackson -> people.person.children -> Brandon Jackson\n# Answer:\nKatherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Katherine Jackson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nKatherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Joe Jackson -> people.person.children -> Brandon Jackson\n# Answer:\nJoe Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Katherine Jackson -> people.person.children -> Jackie Jackson\n# Answer:\nKatherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Joe Jackson -> people.family_member.family -> Jackson family\n# Answer:\nJoe Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Katherine Jackson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nKatherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Joe Jackson -> people.person.children -> Jackie Jackson\n# Answer:\nJoe Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Joe Jackson -> people.person.profession -> Actor\n# Answer:\nJoe Jackson"], "ground_truth": ["Katherine Jackson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1161", "prediction": ["# Reasoning Path:\nClemson University -> education.educational_institution.mascot -> The Tiger and the Cub -> common.topic.notable_types -> School mascot\n# Answer:\nThe Tiger and the Cub", "# Reasoning Path:\nClemson University -> education.educational_institution.mascot -> The Tiger and the Cub -> common.topic.notable_for -> g.1258wsn3q\n# Answer:\nThe Tiger and the Cub", "# Reasoning Path:\nClemson University -> education.educational_institution.mascot -> Clemson University The Tiger -> common.topic.notable_for -> g.12568xvfh\n# Answer:\nClemson University The Tiger", "# Reasoning Path:\nClemson University -> education.educational_institution.mascot -> Clemson University The Tiger -> common.topic.notable_types -> School mascot\n# Answer:\nClemson University The Tiger", "# Reasoning Path:\nClemson University -> business.employer.employees -> m.010h0sdv -> business.employment_tenure.person -> James P. Clements\n# Answer:\nm.010h0sdv", "# Reasoning Path:\nClemson University -> award.ranked_item.appears_in_ranked_lists -> m.0_6sgsn -> award.ranking.list -> US News & World Report: National University Rankings\n# Answer:\nm.0_6sgsn", "# Reasoning Path:\nClemson University -> business.employer.employees -> m.05bqvj_ -> business.employment_tenure.person -> Joe L. Kincheloe\n# Answer:\nm.05bqvj_", "# Reasoning Path:\nClemson University -> business.employer.employees -> m.05nf2mp -> business.employment_tenure.person -> Shaka Smart\n# Answer:\nm.05nf2mp", "# Reasoning Path:\nClemson University -> award.ranked_item.appears_in_ranked_lists -> m.0h6f984 -> award.ranking.list -> US News & World Report: National University Rankings\n# Answer:\nm.0h6f984", "# Reasoning Path:\nClemson University -> business.employer.employees -> m.05nf2mp -> business.employment_tenure.title -> Director of Basketball Operations\n# Answer:\nm.05nf2mp"], "ground_truth": ["Clemson University The Tiger", "The Tiger and the Cub"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1163", "prediction": ["# Reasoning Path:\nSouth America -> location.location.containedby -> Americas -> location.location.time_zones -> Amazon Time Zone\n# Answer:\nAmericas", "# Reasoning Path:\nSouth America -> location.location.containedby -> Americas -> location.location.contains -> Guyana\n# Answer:\nAmericas", "# Reasoning Path:\nSouth America -> location.location.time_zones -> Amazon Time Zone -> time.time_zone.locations_in_this_time_zone -> Americas\n# Answer:\nAmazon Time Zone", "# Reasoning Path:\nSouth America -> location.location.time_zones -> Amazon Time Zone -> time.time_zone.day_dst_begins -> Third Sunday in October\n# Answer:\nAmazon Time Zone", "# Reasoning Path:\nSouth America -> location.location.containedby -> Americas -> location.location.time_zones -> Alaska Time Zone\n# Answer:\nAmericas", "# Reasoning Path:\nSouth America -> location.location.containedby -> Americas -> location.location.contains -> Paraguay\n# Answer:\nAmericas", "# Reasoning Path:\nSouth America -> location.location.containedby -> DVD Region 4 -> location.location.contains -> Oceania\n# Answer:\nDVD Region 4", "# Reasoning Path:\nSouth America -> book.book_subject.works -> Amazon -> book.written_work.subjects -> Brazil\n# Answer:\nAmazon", "# Reasoning Path:\nSouth America -> book.book_subject.works -> Bogota 360 -> book.written_work.subjects -> Colombia\n# Answer:\nBogota 360", "# Reasoning Path:\nSouth America -> location.location.time_zones -> Amazon Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nAmazon Time Zone"], "ground_truth": ["Amazon rainforest"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1164", "prediction": ["# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Eyewitness: Reports from an Art World in Crisis\n# Answer:\nModern art", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nModern art", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Impressionism -> book.book_subject.works -> Monet\n# Answer:\nImpressionism", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Origins, Imitation, Conventions\n# Answer:\nModern art", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Impressionism -> book.written_work.subjects -> Art\n# Answer:\nImpressionism", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Oscar Bluemner\n# Answer:\nModern art", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> visual_art.art_period_movement.associated_artworks -> Elemental: The Power of Illuminated Love\n# Answer:\nModern art", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Impressionism -> base.schemastaging.context_name.pronunciation -> g.125_m6dv2\n# Answer:\nImpressionism", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.art_forms -> Painting -> base.descriptive_names.names.descriptive_name -> m.0109p390\n# Answer:\nPainting", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Impressionism -> book.book_subject.works -> Ce\u0301zanne\n# Answer:\nImpressionism"], "ground_truth": ["Impressionism", "Modern art"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1166", "prediction": ["# Reasoning Path:\nIraq War -> base.argumentmaps.motivation_s.motivation -> m.05z179k -> base.argumentmaps.motivation.intention -> Withdrawal of U.S. troops from Iraq\n# Answer:\nm.05z179k", "# Reasoning Path:\nIraq War -> book.book_subject.works -> Iraq -> military.military_combatant.military_conflicts -> m.043wpnt\n# Answer:\nIraq", "# Reasoning Path:\nIraq War -> base.fight.subject_of_protest.protests_about_this -> 20 March 2003 anti-war protest -> common.topic.article -> m.082shh\n# Answer:\n20 March 2003 anti-war protest", "# Reasoning Path:\nIraq War -> base.argumentmaps.motivation_s.motivation -> m.05z179k -> base.argumentmaps.motivation.type_of_motivation -> Resistance movement\n# Answer:\nm.05z179k", "# Reasoning Path:\nIraq War -> book.book_subject.works -> Iraq -> military.military_combatant.military_conflicts -> m.04ftyvf\n# Answer:\nIraq", "# Reasoning Path:\nIraq War -> book.book_subject.works -> Allies -> book.written_work.subjects -> Iraq\n# Answer:\nAllies", "# Reasoning Path:\nIraq War -> base.fight.subject_of_protest.protests_about_this -> Berkeley Marine Corps Recruiting Center protests -> common.topic.notable_for -> g.1255gcll3\n# Answer:\nBerkeley Marine Corps Recruiting Center protests", "# Reasoning Path:\nIraq War -> book.book_subject.works -> Iraq -> common.topic.notable_types -> Book\n# Answer:\nIraq", "# Reasoning Path:\nIraq War -> base.fight.subject_of_protest.protests_about_this -> 20 March 2003 anti-war protest -> common.topic.notable_for -> g.1259y6sqr\n# Answer:\n20 March 2003 anti-war protest", "# Reasoning Path:\nIraq War -> base.fight.subject_of_protest.protests_about_this -> Bring Them Home Now Tour -> common.topic.notable_types -> Event\n# Answer:\nBring Them Home Now Tour"], "ground_truth": ["2003-03-20"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1168", "prediction": ["# Reasoning Path:\nKurt Cobain -> music.guitarist.guitars_played -> Fender Jag-Stang -> common.topic.article -> m.047q4q\n# Answer:\nFender Jag-Stang", "# Reasoning Path:\nKurt Cobain -> music.group_member.instruments_played -> Guitar -> music.instrument.variation -> Bass guitar\n# Answer:\nGuitar", "# Reasoning Path:\nKurt Cobain -> music.guitarist.guitars_played -> Fender Jag-Stang -> music.guitar.brand -> Fender Musical Instruments Corporation\n# Answer:\nFender Jag-Stang", "# Reasoning Path:\nKurt Cobain -> music.group_member.instruments_played -> Guitar -> music.instrument.variation -> Fretless guitar\n# Answer:\nGuitar", "# Reasoning Path:\nKurt Cobain -> music.group_member.instruments_played -> Guitar -> common.topic.subject_of -> Luke Marchie\n# Answer:\nGuitar", "# Reasoning Path:\nKurt Cobain -> music.group_member.instruments_played -> Electric guitar -> music.instrument.family -> Guitar\n# Answer:\nElectric guitar", "# Reasoning Path:\nKurt Cobain -> music.group_member.instruments_played -> Guitar -> music.instrument.variation -> Acoustic bass guitar\n# Answer:\nGuitar", "# Reasoning Path:\nKurt Cobain -> music.group_member.instruments_played -> Guitar -> book.book_subject.works -> Clapton's Guitar: Watching Wayne Henderson Build the Perfect Instrument\n# Answer:\nGuitar", "# Reasoning Path:\nKurt Cobain -> music.group_member.instruments_played -> Guitar -> common.topic.subject_of -> Music Lessons\n# Answer:\nGuitar", "# Reasoning Path:\nKurt Cobain -> music.group_member.instruments_played -> Electric guitar -> interests.hobby.people_with_this_hobby -> Tony Edward\n# Answer:\nElectric guitar"], "ground_truth": ["Fender Jag-Stang"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1169", "prediction": ["# Reasoning Path:\nJennifer Lawrence -> film.actor.film -> m.0s912lb -> film.performance.character -> Raven / Mystique\n# Answer:\nm.0s912lb", "# Reasoning Path:\nJennifer Lawrence -> film.actor.film -> m.010sqn6t -> film.performance.character -> Raven / Mystique\n# Answer:\nm.010sqn6t", "# Reasoning Path:\nJennifer Lawrence -> film.actor.film -> m.0cgr66b -> film.performance.character -> Raven / Mystique\n# Answer:\nm.0cgr66b", "# Reasoning Path:\nJennifer Lawrence -> film.actor.film -> m.0s912lb -> film.performance.film -> X-Men: Days of Future Past\n# Answer:\nm.0s912lb", "# Reasoning Path:\nJennifer Lawrence -> film.actor.film -> m.010sqn6t -> film.performance.film -> X-Men: Apocalypse\n# Answer:\nm.010sqn6t", "# Reasoning Path:\nJennifer Lawrence -> film.actor.film -> m.0cgr66b -> film.performance.film -> X-Men: First Class\n# Answer:\nm.0cgr66b", "# Reasoning Path:\nJennifer Lawrence -> award.award_winner.awards_won -> m.0101mk_y -> award.award_honor.ceremony -> 2014 Kids' Choice Awards\n# Answer:\nm.0101mk_y", "# Reasoning Path:\nJennifer Lawrence -> award.award_winner.awards_won -> m.0101mk_y -> award.award_honor.honored_for -> The Hunger Games: Catching Fire\n# Answer:\nm.0101mk_y", "# Reasoning Path:\nJennifer Lawrence -> award.award_winner.awards_won -> m.0101mld6 -> award.award_honor.honored_for -> The Hunger Games: Catching Fire\n# Answer:\nm.0101mld6", "# Reasoning Path:\nJennifer Lawrence -> award.award_winner.awards_won -> m.0101mk_y -> award.award_honor.award -> Kids' Choice Award for Favorite Movie Actress\n# Answer:\nm.0101mk_y"], "ground_truth": ["Raven / Mystique"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1170", "prediction": ["# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> government.government_position_held.office_holder -> Tim Kaine\n# Answer:\nm.0r99pcd", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0r99pcd", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.04g7cb8 -> government.government_position_held.office_holder -> Gerald L. Baliles\n# Answer:\nm.04g7cb8", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nm.0r99pcd", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.04g7cmm -> government.government_position_held.office_holder -> Gerald L. Baliles\n# Answer:\nm.04g7cmm", "# Reasoning Path:\nVirginia -> location.location.containedby -> Charles Irving Thornton Tombstone -> location.location.geolocation -> m.0zjvs72\n# Answer:\nCharles Irving Thornton Tombstone", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.04g7cb8 -> government.government_position_held.basic_title -> Governor\n# Answer:\nm.04g7cb8", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.04g7cmm -> government.government_position_held.basic_title -> Assistant Attorney General\n# Answer:\nm.04g7cmm", "# Reasoning Path:\nVirginia -> location.location.containedby -> Charles Irving Thornton Tombstone -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.0zjw6db\n# Answer:\nCharles Irving Thornton Tombstone", "# Reasoning Path:\nVirginia -> location.location.partiallycontains -> m.0w_v1r2 -> location.partial_containment_relationship.partially_contains -> Skeet Rock Knob\n# Answer:\nm.0w_v1r2"], "ground_truth": ["Bob McDonnell"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1171", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.parents -> Shmi Skywalker -> fictional_universe.fictional_character.gender -> Female\n# Answer:\nShmi Skywalker", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.parents -> Shmi Skywalker -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe\n# Answer:\nShmi Skywalker", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.actor -> James Earl Jones\n# Answer:\nm.02nv74t", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.children -> Leia Organa -> fictional_universe.fictional_character.parents -> Padm\u00e9 Amidala\n# Answer:\nLeia Organa", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zsqt -> film.performance.actor -> David Prowse\n# Answer:\nm.0j7zsqt", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.children -> Luke Skywalker -> fictional_universe.fictional_character.parents -> Padm\u00e9 Amidala\n# Answer:\nLuke Skywalker", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.children -> Leia Organa -> film.film_character.portrayed_in_films -> m.010wvf3v\n# Answer:\nLeia Organa", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.film -> Star Wars\n# Answer:\nm.02nv74t"], "ground_truth": ["Abraham Benrubi", "Matt Lanter", "Zac Efron", "Dr. Smoov", "James Earl Jones"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.1904761904761905, "path_precision": 0.2, "path_recall": 0.18181818181818182, "path_ans_f1": 0.20000000000000004, "path_ans_precision": 0.2, "path_ans_recall": 0.2}
{"id": "WebQTest-1173", "prediction": ["# Reasoning Path:\nWalton County -> location.location.containedby -> Georgia -> location.location.containedby -> United States of America\n# Answer:\nGeorgia", "# Reasoning Path:\nWalton County -> location.location.containedby -> Atlanta-Sandy Springs-Marietta, GA Metropolitan Statistical Area -> location.location.containedby -> Georgia\n# Answer:\nAtlanta-Sandy Springs-Marietta, GA Metropolitan Statistical Area", "# Reasoning Path:\nWalton County -> location.location.containedby -> Georgia -> location.location.containedby -> United States, with Territories\n# Answer:\nGeorgia", "# Reasoning Path:\nWalton County -> location.location.containedby -> Georgia -> base.locations.states_and_provences.country -> United States of America\n# Answer:\nGeorgia", "# Reasoning Path:\nWalton County -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nWalton County -> location.us_county.hud_county_place -> Between -> location.location.containedby -> Georgia\n# Answer:\nBetween", "# Reasoning Path:\nWalton County -> location.location.containedby -> Georgia -> common.topic.subject_of -> Harlem Renaissance Way Down South\n# Answer:\nGeorgia", "# Reasoning Path:\nWalton County -> location.statistical_region.population -> g.11b674j1z2\n# Answer:\ng.11b674j1z2", "# Reasoning Path:\nWalton County -> location.us_county.hud_county_place -> Good Hope -> location.location.containedby -> Georgia\n# Answer:\nGood Hope", "# Reasoning Path:\nWalton County -> location.location.containedby -> Georgia -> common.topic.subject_of -> Photography Exhibit Commemorates 150th Anniversary of American Civil War (part 1)\n# Answer:\nGeorgia"], "ground_truth": ["Atlanta-Sandy Springs-Marietta, GA Metropolitan Statistical Area", "Georgia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1174", "prediction": ["# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Lymphoma -> medicine.disease.symptoms -> Abdominal mass\n# Answer:\nLymphoma", "# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Lymphoma -> common.topic.notable_types -> Cause Of Death\n# Answer:\nLymphoma", "# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Lymphoma -> base.bioventurist.bv_medical_condition.therapeutics -> m.052wqkp\n# Answer:\nLymphoma", "# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Lymphoma -> medicine.disease.symptoms -> Anemia\n# Answer:\nLymphoma", "# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.medical_specialties -> Oncology\n# Answer:\nCancer", "# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Lymphoma -> medicine.disease.symptoms -> Anorexia\n# Answer:\nLymphoma", "# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Cancer -> medicine.risk_factor.diseases -> Anemia\n# Answer:\nCancer", "# Reasoning Path:\nJacqueline Kennedy Onassis -> opera.opera_character.operas_appears_in -> m.0dm2d1m -> opera.opera_character_voice.opera -> Jackie O\n# Answer:\nm.0dm2d1m", "# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease_cause.diseases -> Erythema nodosum\n# Answer:\nCancer", "# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease_cause.diseases -> Kyphosis\n# Answer:\nCancer"], "ground_truth": ["Lymphoma"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1175", "prediction": ["# Reasoning Path:\nLauren Conrad -> people.person.education -> m.040zfb0 -> education.education.major_field_of_study -> Fashion design\n# Answer:\nm.040zfb0", "# Reasoning Path:\nLauren Conrad -> people.person.education -> m.0gx3gdb -> education.education.institution -> Laguna Beach High School\n# Answer:\nm.0gx3gdb", "# Reasoning Path:\nLauren Conrad -> people.person.education -> m.04hyzwc -> education.education.major_field_of_study -> Fashion design\n# Answer:\nm.04hyzwc", "# Reasoning Path:\nLauren Conrad -> base.popstra.celebrity.friendship -> m.063g5hk -> base.popstra.friendship.participant -> Brody Jenner\n# Answer:\nm.063g5hk", "# Reasoning Path:\nLauren Conrad -> people.person.education -> m.040zfb0 -> education.education.institution -> Fashion Institute of Design & Merchandising\n# Answer:\nm.040zfb0", "# Reasoning Path:\nLauren Conrad -> base.popstra.celebrity.dated -> m.063t50c -> base.popstra.dated.participant -> Cisco Adler\n# Answer:\nm.063t50c", "# Reasoning Path:\nLauren Conrad -> base.popstra.celebrity.friendship -> m.063tm27 -> base.popstra.friendship.participant -> Audrina Patridge\n# Answer:\nm.063tm27", "# Reasoning Path:\nLauren Conrad -> people.person.education -> m.04hyzwc -> education.education.institution -> Academy of Art University\n# Answer:\nm.04hyzwc", "# Reasoning Path:\nLauren Conrad -> base.popstra.celebrity.friendship -> m.064v2hh -> base.popstra.friendship.participant -> Heidi Montag\n# Answer:\nm.064v2hh", "# Reasoning Path:\nLauren Conrad -> base.popstra.celebrity.dated -> m.064v2gv -> base.popstra.dated.participant -> Kyle Howard\n# Answer:\nm.064v2gv"], "ground_truth": ["Fashion design"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1176", "prediction": ["# Reasoning Path:\nPennsylvania -> location.location.time_zones -> Eastern Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nPennsylvania -> location.location.time_zones -> UTC\u221205:00 -> common.topic.notable_types -> Time Zone\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nPennsylvania -> location.location.time_zones -> UTC\u221205:00 -> common.topic.notable_for -> g.125fnb_r1\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.title -> Director, Office of Health Care Reform\n# Answer:\nm.04ds08c", "# Reasoning Path:\nPennsylvania -> location.location.time_zones -> UTC\u221205:00 -> common.topic.image -> Timezones2008 UTC-5\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.person -> Rosemarie B. Greco\n# Answer:\nm.04ds08c", "# Reasoning Path:\nPennsylvania -> common.topic.webpage -> m.04lsvtk -> common.webpage.resource -> m.0blz_lk\n# Answer:\nm.04lsvtk", "# Reasoning Path:\nPennsylvania -> common.topic.webpage -> m.09wcxfl -> common.webpage.resource -> Exclusive: Jon Gosselin tells EW 'I am not guilty of cheating on my wife'\n# Answer:\nm.09wcxfl", "# Reasoning Path:\nPennsylvania -> common.topic.webpage -> m.04lsvtk -> common.webpage.category -> Official Website\n# Answer:\nm.04lsvtk", "# Reasoning Path:\nPennsylvania -> common.topic.webpage -> m.09wcyd_ -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09wcyd_"], "ground_truth": ["UTC\u221205:00", "Eastern Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1177", "prediction": ["# Reasoning Path:\nMario Lopez -> people.person.education -> m.0g_9bvg -> education.education.institution -> Chula Vista High School\n# Answer:\nm.0g_9bvg", "# Reasoning Path:\nMario Lopez -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nMario Lopez -> film.actor.film -> m.03jq9tb -> film.performance.film -> Colors\n# Answer:\nm.03jq9tb", "# Reasoning Path:\nMario Lopez -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nMario Lopez -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nMale", "# Reasoning Path:\nMario Lopez -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Acute myeloid leukemia\n# Answer:\nMale", "# Reasoning Path:\nMario Lopez -> people.person.gender -> Male -> biology.hybrid_parent_gender.hybrids -> m.0blp580\n# Answer:\nMale", "# Reasoning Path:\nMario Lopez -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_98hd\n# Answer:\nMale", "# Reasoning Path:\nMario Lopez -> film.actor.film -> m.03jq9tb -> film.performance.character -> Felipe's Friend\n# Answer:\nm.03jq9tb", "# Reasoning Path:\nMario Lopez -> film.actor.film -> m.03l8jfc -> film.performance.film -> A Crack in the Floor\n# Answer:\nm.03l8jfc"], "ground_truth": ["Chula Vista High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1178", "prediction": ["# Reasoning Path:\nTupac Shakur -> people.person.place_of_birth -> East Harlem -> location.location.people_born_here -> Lumidee\n# Answer:\nEast Harlem", "# Reasoning Path:\nTupac Shakur -> people.person.ethnicity -> African American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nAfrican American", "# Reasoning Path:\nTupac Shakur -> people.person.place_of_birth -> East Harlem -> location.location.containedby -> Manhattan\n# Answer:\nEast Harlem", "# Reasoning Path:\nTupac Shakur -> people.person.place_of_birth -> East Harlem -> location.location.containedby -> New York City\n# Answer:\nEast Harlem", "# Reasoning Path:\nTupac Shakur -> people.person.place_of_birth -> East Harlem -> location.neighborhood.neighborhood_of -> Manhattan\n# Answer:\nEast Harlem", "# Reasoning Path:\nTupac Shakur -> people.person.ethnicity -> African American -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nAfrican American", "# Reasoning Path:\nTupac Shakur -> people.person.place_of_birth -> East Harlem -> location.neighborhood.neighborhood_of -> New York City\n# Answer:\nEast Harlem", "# Reasoning Path:\nTupac Shakur -> people.person.place_of_birth -> East Harlem -> location.location.containedby -> New York metropolitan area\n# Answer:\nEast Harlem", "# Reasoning Path:\nTupac Shakur -> people.person.place_of_birth -> East Harlem -> location.neighborhood.neighborhood_of -> New York metropolitan area\n# Answer:\nEast Harlem", "# Reasoning Path:\nTupac Shakur -> people.person.education -> m.0h35g85 -> freebase.valuenotation.has_value -> End Date\n# Answer:\nm.0h35g85"], "ground_truth": ["East Harlem"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1179", "prediction": ["# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Darth Vader -> film.film_character.portrayed_in_films -> m.02nv74t\n# Answer:\nDarth Vader", "# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Darth Vader -> film.film_character.portrayed_in_films -> m.01xpnt2\n# Answer:\nDarth Vader", "# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Darth Vader -> fictional_universe.fictional_character.occupation -> Jedi\n# Answer:\nDarth Vader", "# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Darth Vader -> film.film_character.portrayed_in_films -> m.0235q84\n# Answer:\nDarth Vader", "# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Darth Vader -> fictional_universe.fictional_character.occupation -> Martial Artist\n# Answer:\nDarth Vader", "# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Darth Vader -> fictional_universe.fictional_character.powers_or_abilities -> Force\n# Answer:\nDarth Vader", "# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ansion -> common.topic.image -> Ansion_02.JPG\n# Answer:\nAnsion", "# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Anoat -> common.topic.notable_for -> g.1yl5yk50h\n# Answer:\nAnoat", "# Reasoning Path:\nGeorge Lucas -> award.award_winner.awards_won -> m.010x01cz -> award.award_honor.award -> Daytime Emmy Award for Outstanding Special Class Animated Program\n# Answer:\nm.010x01cz", "# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Darth Vader -> fictional_universe.fictional_character.powers_or_abilities -> Levitation\n# Answer:\nDarth Vader"], "ground_truth": ["Baron Papanoida"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.06vz4t9 -> military.military_combatant_group.combatants -> Argentina\n# Answer:\nm.06vz4t9", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> military.military_conflict.combatants -> m.05ckldy\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z973l -> military.military_combatant_group.combatants -> Iraq\n# Answer:\nm.03z973l", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> time.event.locations -> Iraqi Kurdistan\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> Australia\n# Answer:\nm.03z96c5", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> military.military_conflict.combatants -> m.05cklf2\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> military.military_conflict.combatants -> m.05cklf7\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> common.topic.notable_for -> g.125cxty6s\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> time.event.includes_event -> 1991 uprisings in Iraq -> military.military_conflict.combatants -> m.04y_t_s\n# Answer:\n1991 uprisings in Iraq", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wphl -> military.casualties.combatant -> United States of America\n# Answer:\nm.043wphl"], "ground_truth": ["United Kingdom", "Saudi Arabia", "United States of America", "Argentina", "France", "Iraq", "Australia"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.23076923076923073, "ans_precission": 0.6, "ans_recall": 0.14285714285714285, "path_f1": 0.08955223880597013, "path_precision": 0.3, "path_recall": 0.05263157894736842, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 1.0, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-1180", "prediction": ["# Reasoning Path:\nSarah Dessen -> film.film_story_contributor.film_story_credits -> How to Deal -> film.film.genre -> Comedy\n# Answer:\nHow to Deal", "# Reasoning Path:\nSarah Dessen -> film.film_story_contributor.film_story_credits -> How to Deal -> common.topic.notable_types -> Film\n# Answer:\nHow to Deal", "# Reasoning Path:\nSarah Dessen -> film.film_story_contributor.film_story_credits -> How to Deal -> film.film.genre -> Drama\n# Answer:\nHow to Deal", "# Reasoning Path:\nSarah Dessen -> film.film_story_contributor.film_story_credits -> How to Deal -> film.film.runtime -> m.0jzmqx\n# Answer:\nHow to Deal", "# Reasoning Path:\nSarah Dessen -> film.film_story_contributor.film_story_credits -> How to Deal -> film.film.genre -> Comedy-drama\n# Answer:\nHow to Deal", "# Reasoning Path:\nSarah Dessen -> book.author.works_written -> Dreamland -> book.written_work.subjects -> Identity\n# Answer:\nDreamland", "# Reasoning Path:\nSarah Dessen -> book.author.works_written -> How to Deal movie tie-in -> book.book_edition.isbn -> 9780142501030\n# Answer:\nHow to Deal movie tie-in", "# Reasoning Path:\nSarah Dessen -> book.author.book_editions_published -> Crazy Moon. -> book.book_edition.publisher -> Dtv\n# Answer:\nCrazy Moon.", "# Reasoning Path:\nSarah Dessen -> book.author.works_written -> Along for the Ride -> book.written_work.original_language -> English Language\n# Answer:\nAlong for the Ride", "# Reasoning Path:\nSarah Dessen -> book.author.works_written -> Along for the Ride -> book.book.characters -> Adam\n# Answer:\nAlong for the Ride"], "ground_truth": ["How to Deal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1181", "prediction": ["# Reasoning Path:\nHawaii -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nKauai County -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nKauai -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nHawaiian Islands -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nHawaiian Islands -> location.location.contains -> Hawaii -> location.location.time_zones -> Hawaii-Aleutian Time Zone\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nKauai County -> location.us_county.hud_county_place -> Anahola -> location.location.time_zones -> Hawaii-Aleutian Time Zone\n# Answer:\nAnahola", "# Reasoning Path:\nKauai -> common.topic.image -> Kauai from space oriented -> common.image.size -> m.02brp1k\n# Answer:\nKauai from space oriented", "# Reasoning Path:\nHawaii -> book.book_subject.works -> Anatomy of Paradise -> book.written_work.subjects -> Oceania\n# Answer:\nAnatomy of Paradise", "# Reasoning Path:\nKauai -> common.topic.image -> Kauai from space oriented -> common.image.appears_in_topic_gallery -> Sch\u00e4ffer affair\n# Answer:\nKauai from space oriented"], "ground_truth": ["Hawaii-Aleutian Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-1183", "prediction": ["# Reasoning Path:\nFarrah Fawcett -> common.topic.notable_for -> g.125f8rws1\n# Answer:\ng.125f8rws1", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.place_of_burial -> Westwood Village Memorial Park Cemetery -> location.location.geolocation -> m.0cqx58y\n# Answer:\nWestwood Village Memorial Park Cemetery", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.place_of_burial -> Westwood Village Memorial Park Cemetery -> location.location.containedby -> 90024\n# Answer:\nWestwood Village Memorial Park Cemetery", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.place_of_burial -> Westwood Village Memorial Park Cemetery -> common.topic.notable_for -> g.1256km_vm\n# Answer:\nWestwood Village Memorial Park Cemetery", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.place_of_burial -> Westwood Village Memorial Park Cemetery -> location.location.containedby -> Los Angeles County\n# Answer:\nWestwood Village Memorial Park Cemetery", "# Reasoning Path:\nFarrah Fawcett -> common.topic.webpage -> m.0773vxj -> common.webpage.category -> Topic Webpage\n# Answer:\nm.0773vxj", "# Reasoning Path:\nFarrah Fawcett -> common.topic.webpage -> m.08wwplb -> common.webpage.resource -> Farrah Fawcett: 16 highlights\n# Answer:\nm.08wwplb", "# Reasoning Path:\nFarrah Fawcett -> common.topic.webpage -> m.0773vxj -> common.webpage.resource -> TV and Movie Trivia Tribute\n# Answer:\nm.0773vxj", "# Reasoning Path:\nFarrah Fawcett -> common.topic.webpage -> m.094039t -> common.webpage.resource -> '70s Comebacks of the Week\n# Answer:\nm.094039t", "# Reasoning Path:\nFarrah Fawcett -> common.topic.webpage -> m.08wwplb -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.08wwplb"], "ground_truth": ["Westwood Village Memorial Park Cemetery"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1185", "prediction": ["# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced -> Christopher Moore -> influence.influence_node.influenced_by -> Kurt Vonnegut\n# Answer:\nChristopher Moore", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced -> Colum McCann -> influence.influence_node.influenced_by -> William Faulkner\n# Answer:\nColum McCann", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced -> Christopher Moore -> influence.influence_node.influenced_by -> Paul Auster\n# Answer:\nChristopher Moore", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced -> Christopher Moore -> book.author.book_editions_published -> A Dirty Job\n# Answer:\nChristopher Moore", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced -> Christopher Moore -> book.author.works_written -> A Dirty Job\n# Answer:\nChristopher Moore", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced -> Cork Graham -> influence.influence_node.influenced_by -> Ernest Hemingway\n# Answer:\nCork Graham", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced -> Colum McCann -> influence.influence_node.influenced_by -> Don DeLillo\n# Answer:\nColum McCann", "# Reasoning Path:\nJohn Steinbeck -> book.author.works_written -> A Russian Journal -> common.topic.notable_for -> g.1256s0x_d\n# Answer:\nA Russian Journal", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced -> Colum McCann -> influence.influence_node.influenced_by -> Edna O'Brien\n# Answer:\nColum McCann", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced -> Colum McCann -> freebase.valuenotation.has_value -> Parents\n# Answer:\nColum McCann"], "ground_truth": ["Thomas Malory", "William Faulkner", "Sherwood Anderson", "Robert Burns"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.14285714285714288, "path_ans_precision": 0.1, "path_ans_recall": 0.25}
{"id": "WebQTest-1186", "prediction": ["# Reasoning Path:\nCuba -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Soviet Union\n# Answer:\nSocialist state", "# Reasoning Path:\nCuba -> location.country.form_of_government -> Socialist state -> common.topic.notable_types -> Form of Government\n# Answer:\nSocialist state", "# Reasoning Path:\nCuba -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Venezuela\n# Answer:\nSocialist state", "# Reasoning Path:\nCuba -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Haiti\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nCuba -> location.country.form_of_government -> Semi-presidential system -> common.topic.notable_types -> Form of Government\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nCuba -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc38hmp\n# Answer:\ng.1hhc38hmp", "# Reasoning Path:\nCuba -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Bahamas\n# Answer:\nUnitary state", "# Reasoning Path:\nCuba -> location.country.form_of_government -> Unitary state -> common.topic.notable_types -> Form of Government\n# Answer:\nUnitary state", "# Reasoning Path:\nCuba -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> France\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nCuba -> book.book_subject.works -> A simple Habana melody -> book.written_work.subjects -> France\n# Answer:\nA simple Habana melody"], "ground_truth": ["Socialist state", "Republic", "Semi-presidential system", "Unitary state"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.8, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.8, "path_ans_recall": 0.75}
{"id": "WebQTest-1187", "prediction": ["# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Republic of Ireland -> base.locations.countries.continent -> Europe\n# Answer:\nRepublic of Ireland", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Republic of Ireland -> base.aareas.schema.administrative_area.administrative_area_type -> Sovereign state\n# Answer:\nRepublic of Ireland", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Australia -> sports.sport_country.multi_event_tournaments_participated_in -> 2010 Commonwealth Games\n# Answer:\nAustralia", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Kingdom of Great Britain -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nKingdom of Great Britain", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Kingdom of Great Britain -> location.country.capital -> London\n# Answer:\nKingdom of Great Britain", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.location.containedby -> Americas\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Republic of Ireland -> location.country.languages_spoken -> Forth and Bargy dialect\n# Answer:\nRepublic of Ireland", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Kingdom of Great Britain -> location.location.containedby -> British Isles\n# Answer:\nKingdom of Great Britain", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Australia -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\nAustralia"], "ground_truth": ["Japan", "Qatar", "Kiribati", "Rwanda", "Samoa", "Zimbabwe", "Australia", "Jordan", "Bangladesh", "Turks and Caicos Islands", "State of Palestine", "Saint Vincent and the Grenadines", "United Kingdom", "Sudan", "Philippines", "Kingdom of Great Britain", "Malta", "Mandatory Palestine", "India", "Lesotho", "Wales", "South Africa", "Bermuda", "Kenya", "Ghana", "England", "Territory of New Guinea", "Malaysia", "South Yemen", "Barbados", "Cura\u00e7ao", "Bahamas", "Transkei", "Singapore", "Saint Kitts and Nevis", "Liberia", "Israel", "Puerto Rico", "Antigua and Barbuda", "Grenada", "Republic of Ireland", "Bonaire", "Swaziland", "Sri Lanka", "Timor-Leste", "Cook Islands", "Vanuatu", "Nauru", "Cyprus", "Marshall Islands", "China", "Montserrat", "Territory of Papua and New Guinea", "Uganda", "Gazankulu", "Indonesia", "Guam", "Dominica", "Pakistan", "Ethiopia", "Saint Lucia", "Tuvalu", "New Zealand", "Tokelau", "Honduras", "Papua New Guinea", "Gambia", "Canada", "Cayman Islands", "United States of America", "Tanzania", "Zambia", "Botswana", "Cameroon", "Hong Kong", "Nigeria", "Isle of Man", "Belize", "Gibraltar", "Namibia", "Laos", "Guyana", "Jersey", "Fiji", "Brunei", "Vatican City", "Sierra Leone"], "ans_acc": 0.04597701149425287, "ans_hit": 1, "ans_f1": 0.08791208791208792, "ans_precission": 1.0, "ans_recall": 0.04597701149425287, "path_f1": 0.06611570247933884, "path_precision": 0.8, "path_recall": 0.034482758620689655, "path_ans_f1": 0.08791208791208792, "path_ans_precision": 1.0, "path_ans_recall": 0.04597701149425287}
{"id": "WebQTest-1189", "prediction": ["# Reasoning Path:\nItaly -> location.country.capital -> Rome -> travel.travel_destination.tourist_attractions -> Vatican City\n# Answer:\nRome", "# Reasoning Path:\nItaly -> location.country.capital -> Rome -> location.location.containedby -> Lazio\n# Answer:\nRome", "# Reasoning Path:\nItaly -> location.country.capital -> Rome -> location.location.time_zones -> Central European Time Zone\n# Answer:\nRome", "# Reasoning Path:\nItaly -> location.location.containedby -> Eurasia -> base.locations.continents.countries_within -> Turkey\n# Answer:\nEurasia", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\ng.11b71mdn0q", "# Reasoning Path:\nItaly -> location.location.containedby -> Eurasia -> base.locations.continents.countries_within -> France\n# Answer:\nEurasia", "# Reasoning Path:\nItaly -> location.location.containedby -> Eurasia -> location.location.events -> World War II\n# Answer:\nEurasia", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\ng.12tb6gbb1", "# Reasoning Path:\nItaly -> location.location.containedby -> Eurasia -> common.topic.notable_types -> Location\n# Answer:\nEurasia", "# Reasoning Path:\nItaly -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Albania\n# Answer:\nEurope"], "ground_truth": ["Rome"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.character -> London Tipton\n# Answer:\nm.040p0ym", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 1\n# Answer:\nm.040p0ym", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.05v3ngr -> tv.regular_tv_appearance.actor -> Erin Cardillo\n# Answer:\nm.05v3ngr", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.character -> Mr. Moseby\n# Answer:\nm.040p0ys", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Show\n# Answer:\nm.0sgkxsb", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> award.award_nomination.ceremony -> 2011 Kids' Choice Awards\n# Answer:\nm.0sgkxsb", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkyvs -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Actor\n# Answer:\nm.0sgkyvs", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.05v3ngr -> tv.regular_tv_appearance.character -> Emma Tutweiller\n# Answer:\nm.05v3ngr", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 2\n# Answer:\nm.040p0ym", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 2\n# Answer:\nm.040p0ys"], "ground_truth": ["Brenda Song"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1191", "prediction": ["# Reasoning Path:\nBahamas -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Barbados\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.country.languages_spoken -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Belize\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> Canada\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.country.languages_spoken -> Bahamas Creole English Language -> common.topic.notable_types -> Human Language\n# Answer:\nBahamas Creole English Language", "# Reasoning Path:\nBahamas -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Barbados\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.statistical_region.co2_emissions_per_capita -> g.1245_22wy\n# Answer:\ng.1245_22wy", "# Reasoning Path:\nBahamas -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Bermuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.country.official_language -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language"], "ground_truth": ["Bahamas Creole English Language", "English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1192", "prediction": ["# Reasoning Path:\nGeorge Michael -> people.person.education -> m.0h3d9q4 -> education.education.institution -> Bushey Meads School\n# Answer:\nm.0h3d9q4", "# Reasoning Path:\nGeorge Michael -> music.lyricist.lyrics_written -> A Different Corner -> music.recording.artist -> WHAM!\n# Answer:\nA Different Corner", "# Reasoning Path:\nGeorge Michael -> music.artist.track_contributions -> m.010__qx2 -> music.track_contribution.track -> The First Time Ever I Saw Your Face\n# Answer:\nm.010__qx2", "# Reasoning Path:\nGeorge Michael -> music.lyricist.lyrics_written -> A Last Request (I Want Your Sex, Part III) -> common.topic.notable_for -> g.1z2sq43ys\n# Answer:\nA Last Request (I Want Your Sex, Part III)", "# Reasoning Path:\nGeorge Michael -> music.artist.track_contributions -> m.010__qx2 -> music.track_contribution.role -> Vocals\n# Answer:\nm.010__qx2", "# Reasoning Path:\nGeorge Michael -> music.artist.track_contributions -> m.010_v1d8 -> music.track_contribution.role -> Vocals\n# Answer:\nm.010_v1d8", "# Reasoning Path:\nGeorge Michael -> music.lyricist.lyrics_written -> A Last Request (I Want Your Sex, Part III) -> common.topic.notable_types -> Composition\n# Answer:\nA Last Request (I Want Your Sex, Part III)", "# Reasoning Path:\nGeorge Michael -> music.artist.track_contributions -> m.010_t6xt -> music.track_contribution.role -> Vocals\n# Answer:\nm.010_t6xt", "# Reasoning Path:\nGeorge Michael -> music.lyricist.lyrics_written -> A Ray of Sunshine -> music.composition.language -> English Language\n# Answer:\nA Ray of Sunshine", "# Reasoning Path:\nGeorge Michael -> music.artist.track_contributions -> m.010_t6xt -> music.track_contribution.track -> Roxanne\n# Answer:\nm.010_t6xt"], "ground_truth": ["Bushey Meads School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1194", "prediction": ["# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nm.010dzr92", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzrlj -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nm.010dzrlj", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.match -> 2014 Real Madrid CF vs. CA Osasuna football match\n# Answer:\nm.010dzr92", "# Reasoning Path:\nCristiano Ronaldo -> common.topic.article -> m.02xt70\n# Answer:\nm.02xt70", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010p4jhc -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nm.010p4jhc", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzrlj -> soccer.football_goal.match -> 2014 Real Madrid CF vs. CA Osasuna football match\n# Answer:\nm.010dzrlj", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010p4jhc -> soccer.football_goal.match -> 2014 UEFA Champions League Final\n# Answer:\nm.010p4jhc", "# Reasoning Path:\nCristiano Ronaldo -> award.award_winner.awards_won -> m.010nx4bl -> award.award_honor.award -> UEFA Champions League Top Goalscorer\n# Answer:\nm.010nx4bl", "# Reasoning Path:\nCristiano Ronaldo -> award.award_winner.awards_won -> m.010rnpty -> award.award_honor.award -> Pichichi Trophy\n# Answer:\nm.010rnpty", "# Reasoning Path:\nCristiano Ronaldo -> award.award_winner.awards_won -> m.0114z017 -> award.award_honor.ceremony -> 2014 ESPY Awards\n# Answer:\nm.0114z017"], "ground_truth": ["Real Madrid C.F.", "Portugal national football team"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2, "path_precision": 0.3, "path_recall": 0.15, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-1195", "prediction": ["# Reasoning Path:\nBahamas -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nBahamas -> location.location.time_zones -> UTC\u221205:00\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nBahamas -> location.statistical_region.co2_emissions_per_capita -> g.1245_22wy\n# Answer:\ng.1245_22wy", "# Reasoning Path:\nBahamas -> location.statistical_region.co2_emissions_per_capita -> g.1245_7s_2\n# Answer:\ng.1245_7s_2", "# Reasoning Path:\nBahamas -> location.country.languages_spoken -> Bahamas Creole English Language -> common.topic.notable_types -> Human Language\n# Answer:\nBahamas Creole English Language", "# Reasoning Path:\nBahamas -> location.statistical_region.co2_emissions_per_capita -> g.1245_b1hf\n# Answer:\ng.1245_b1hf", "# Reasoning Path:\nBahamas -> location.country.languages_spoken -> Bahamas Creole English Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nBahamas Creole English Language", "# Reasoning Path:\nBahamas -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Barbados\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.country.languages_spoken -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Belize\n# Answer:\nEnglish Language"], "ground_truth": ["UTC\u221205:00", "Eastern Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1197", "prediction": ["# Reasoning Path:\nBrazil -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> Sovereign state\n# Answer:\nEarth", "# Reasoning Path:\nBrazil -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> South America\n# Answer:\nEarth", "# Reasoning Path:\nBrazil -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.uncommon.topic.exceptions -> City/Town/Village should pertain to Ghana, since City/Town/Village subdivides Ghanaian Municipal District.\n# Answer:\nEarth", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\ng.12cp_jcjy", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\ng.11b71vdgdz", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\ng.1hhc390hy", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc39sjp\n# Answer:\ng.1hhc39sjp", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fm01\n# Answer:\ng.12tb6fm01", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc385hl\n# Answer:\ng.1hhc385hl"], "ground_truth": ["Brazilian Portuguese", "Portuguese Language", "Italian Language"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1198", "prediction": ["# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced -> Ben Okri -> influence.influence_node.influenced -> Aberjhani\n# Answer:\nBen Okri", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced -> Ben Okri -> influence.influence_node.influenced -> David Anthony Durham\n# Answer:\nBen Okri", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced -> David Alexander -> influence.influence_node.influenced_by -> Edgar Allan Poe\n# Answer:\nDavid Alexander", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced -> Ben Okri -> influence.influence_node.influenced -> Nnedi Okorafor\n# Answer:\nBen Okri", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced -> Hall Caine -> influence.influence_node.influenced_by -> Dante Gabriel Rossetti\n# Answer:\nHall Caine", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced -> Ben Okri -> people.person.profession -> Author\n# Answer:\nBen Okri", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced -> David Alexander -> influence.influence_node.influenced_by -> Ernest Hemingway\n# Answer:\nDavid Alexander", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced -> David Alexander -> people.person.nationality -> United States of America\n# Answer:\nDavid Alexander", "# Reasoning Path:\nSamuel Taylor Coleridge -> people.person.quotations -> A religion, that is, a true religion, must consist of ideas and facts both; not of ideas alone without facts, for then it would be mere Philosophy; -- nor of facts alone without ideas, of which those facts are symbols, or out of which they arise, or upon which they are grounded: for then it would be mere History. -> media_common.quotation.subjects -> Religion\n# Answer:\nA religion, that is, a true religion, must consist of ideas and facts both; not of ideas alone without facts, for then it would be mere Philosophy; -- nor of facts alone without ideas, of which those facts are symbols, or out of which they arise, or upon which they are grounded: for then it would be mere History.", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced -> Ben Okri -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nBen Okri"], "ground_truth": ["Algernon Charles Swinburne", "Thomas Browne", "Baruch Spinoza", "Immanuel Kant", "William Shakespeare", "Friedrich Wilhelm Joseph Schelling", "Emanuel Swedenborg", "John Milton", "Giambattista Vico", "Robert Burns"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1199", "prediction": ["# Reasoning Path:\nKim Kardashian -> people.person.profession -> Socialite -> base.descriptive_names.names.descriptive_name -> m.010fn4s1\n# Answer:\nSocialite", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> TV Personality -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nTV Personality", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> Socialite -> common.topic.notable_for -> g.1256dwvkv\n# Answer:\nSocialite", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> Socialite -> base.descriptive_names.names.descriptive_name -> m.010fn4vt\n# Answer:\nSocialite", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> Socialite -> fictional_universe.character_occupation.characters_with_this_occupation -> Alexandra Spaulding\n# Answer:\nSocialite", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> Socialite -> base.descriptive_names.names.descriptive_name -> m.010h6jx6\n# Answer:\nSocialite", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> Model -> fictional_universe.character_occupation.characters_with_this_occupation -> Misa Amane\n# Answer:\nModel", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> Socialite -> fictional_universe.character_occupation.characters_with_this_occupation -> Alexis Colby\n# Answer:\nSocialite", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> TV Personality -> base.descriptive_names.names.descriptive_name -> m.0106hk42\n# Answer:\nTV Personality", "# Reasoning Path:\nKim Kardashian -> base.popstra.celebrity.dated -> m.065q3r2 -> base.popstra.dated.participant -> Nick Cannon\n# Answer:\nm.065q3r2"], "ground_truth": ["Model", "Actor", "Fashion designer", "TV Personality", "Businessperson", "Socialite", "Television producer", "Entrepreneur"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.5294117647058825, "ans_precission": 0.9, "ans_recall": 0.375, "path_f1": 0.5294117647058825, "path_precision": 0.9, "path_recall": 0.375, "path_ans_f1": 0.5294117647058825, "path_ans_precision": 0.9, "path_ans_recall": 0.375}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.office_holder -> Rob Portman\n# Answer:\nm.0gm5f32", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nm.0gm5f32", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.office_holder -> George Voinovich\n# Answer:\nm.05kfccr", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nm.0gm5f32", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_holder -> Sherrod Brown\n# Answer:\nm.05kg_6s", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.image -> The Rhodes State Office Tower adjacent to the Statehouse\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.notable_for -> g.125h3hwcp\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> meteorology.tropical_cyclone.affected_areas -> Alabama\n# Answer:\nHurricane Bob", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nm.05kg_6s", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> common.topic.notable_for -> g.1259xs2jv\n# Answer:\nHurricane Bob"], "ground_truth": ["Ted Strickland", "Return J. Meigs, Jr.", "John Kasich"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1200", "prediction": ["# Reasoning Path:\nMississippi River -> geography.river.mouth -> Gulf of Mexico -> base.aubreymaturin.place.mentions -> m.05pwg4d\n# Answer:\nGulf of Mexico", "# Reasoning Path:\nMississippi River -> geography.river.mouth -> Gulf of Mexico -> common.topic.notable_for -> g.12559b43k\n# Answer:\nGulf of Mexico", "# Reasoning Path:\nMississippi River -> geography.river.mouth -> Gulf of Mexico -> location.statistical_region.population -> m.0hyq8n7\n# Answer:\nGulf of Mexico", "# Reasoning Path:\nMississippi River -> location.location.partially_containedby -> Mississippi -> location.location.containedby -> Gulf Coast of the United States\n# Answer:\nMississippi", "# Reasoning Path:\nMississippi River -> book.book_subject.works -> Adventures of Huckleberry Finn -> book.written_work.subjects -> Adventure\n# Answer:\nAdventures of Huckleberry Finn", "# Reasoning Path:\nMississippi River -> geography.river.mouth -> Gulf of Mexico -> location.statistical_region.population -> m.0t4sg4g\n# Answer:\nGulf of Mexico", "# Reasoning Path:\nMississippi River -> location.location.partially_containedby -> Mississippi -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMississippi", "# Reasoning Path:\nMississippi River -> location.location.partially_containedby -> Mississippi -> book.book_subject.works -> Come love a stranger\n# Answer:\nMississippi", "# Reasoning Path:\nMississippi River -> location.location.partially_containedby -> Mississippi -> location.location.containedby -> Contiguous United States\n# Answer:\nMississippi", "# Reasoning Path:\nMississippi River -> book.book_subject.works -> Adventures of Huckleberry Finn -> book.written_work.subjects -> 19th century\n# Answer:\nAdventures of Huckleberry Finn"], "ground_truth": ["Gulf of Mexico"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1201", "prediction": ["# Reasoning Path:\nPsych -> tv.tv_program.regular_cast -> m.02wsgt5 -> tv.regular_tv_appearance.actor -> Timothy Omundson\n# Answer:\nm.02wsgt5", "# Reasoning Path:\nPsych -> tv.tv_program.regular_cast -> m.02wsgt5 -> tv.regular_tv_appearance.character -> Carlton Lassiter\n# Answer:\nm.02wsgt5", "# Reasoning Path:\nPsych -> award.award_nominated_work.award_nominations -> m.0_ln542 -> award.award_nomination.award_nominee -> James Roday\n# Answer:\nm.0_ln542", "# Reasoning Path:\nPsych -> tv.tv_program.regular_cast -> m.02wsgl5 -> tv.regular_tv_appearance.actor -> James Roday\n# Answer:\nm.02wsgl5", "# Reasoning Path:\nPsych -> tv.tv_program.regular_cast -> m.011j_6wj -> tv.regular_tv_appearance.character -> Beautiful Woman\n# Answer:\nm.011j_6wj", "# Reasoning Path:\nPsych -> award.award_nominated_work.award_nominations -> m.0_lq_93 -> award.award_nomination.award_nominee -> James Roday\n# Answer:\nm.0_lq_93", "# Reasoning Path:\nPsych -> award.award_nominated_work.award_nominations -> m.0_ln542 -> award.award_nomination.ceremony -> 13th ALMA Awards\n# Answer:\nm.0_ln542", "# Reasoning Path:\nPsych -> award.award_nominated_work.award_nominations -> m.0_ls2dd -> award.award_nomination.award_nominee -> James Roday\n# Answer:\nm.0_ls2dd", "# Reasoning Path:\nPsych -> common.topic.notable_types -> Award-Winning Work -> freebase.documented_object.documentation -> m.02nqvch\n# Answer:\nAward-Winning Work", "# Reasoning Path:\nPsych -> tv.tv_program.regular_cast -> m.02wsgl5 -> tv.regular_tv_appearance.character -> Shawn Spencer\n# Answer:\nm.02wsgl5"], "ground_truth": ["Timothy Omundson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1203", "prediction": ["# Reasoning Path:\nIsrael -> government.governmental_jurisdiction.governing_officials -> m.0114vpvq -> government.government_position_held.appointed_by -> Benjamin Netanyahu\n# Answer:\nm.0114vpvq", "# Reasoning Path:\nIsrael -> government.governmental_jurisdiction.governing_officials -> m.0j5wkf1 -> government.government_position_held.office_holder -> Shimon Peres\n# Answer:\nm.0j5wkf1", "# Reasoning Path:\nIsrael -> government.governmental_jurisdiction.governing_officials -> m.0114vpvq -> government.government_position_held.governmental_body -> Israeli Ministry of Defense\n# Answer:\nm.0114vpvq", "# Reasoning Path:\nIsrael -> government.governmental_jurisdiction.governing_officials -> m.010gg9lp -> government.government_position_held.office_holder -> Ehud Olmert\n# Answer:\nm.010gg9lp", "# Reasoning Path:\nIsrael -> location.statistical_region.health_expenditure_as_percent_of_gdp -> g.12cp_j7nj\n# Answer:\ng.12cp_j7nj", "# Reasoning Path:\nIsrael -> government.governmental_jurisdiction.governing_officials -> m.0j5wkf1 -> government.government_position_held.office_position_or_title -> President of Israel\n# Answer:\nm.0j5wkf1", "# Reasoning Path:\nIsrael -> location.statistical_region.cpi_inflation_rate -> g.11b60st8pp\n# Answer:\ng.11b60st8pp", "# Reasoning Path:\nIsrael -> location.statistical_region.health_expenditure_as_percent_of_gdp -> g.1hhc38hll\n# Answer:\ng.1hhc38hll", "# Reasoning Path:\nIsrael -> location.statistical_region.health_expenditure_as_percent_of_gdp -> g.1hhc3_l0d\n# Answer:\ng.1hhc3_l0d", "# Reasoning Path:\nIsrael -> location.statistical_region.cpi_inflation_rate -> g.12tb6fss0\n# Answer:\ng.12tb6fss0"], "ground_truth": ["Reuven Rivlin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1204", "prediction": ["# Reasoning Path:\nAncestral Puebloans -> people.ethnicity.geographic_distribution -> Four Corners -> common.topic.article -> m.027v6q\n# Answer:\nFour Corners", "# Reasoning Path:\nAncestral Puebloans -> people.ethnicity.geographic_distribution -> Four Corners -> common.topic.image -> The Four Corners region is in the red area on this map\n# Answer:\nFour Corners", "# Reasoning Path:\nAncestral Puebloans -> common.topic.article -> m.0ly_\n# Answer:\nm.0ly_", "# Reasoning Path:\nAncestral Puebloans -> people.ethnicity.geographic_distribution -> Four Corners -> common.topic.notable_for -> g.1257_prs_\n# Answer:\nFour Corners", "# Reasoning Path:\nAncestral Puebloans -> common.topic.notable_types -> Ethnicity -> type.type.expected_by -> Applies to\n# Answer:\nEthnicity", "# Reasoning Path:\nAncestral Puebloans -> common.topic.notable_types -> Ethnicity -> freebase.type_hints.included_types -> Topic\n# Answer:\nEthnicity", "# Reasoning Path:\nAncestral Puebloans -> common.topic.notable_types -> Ethnicity -> freebase.type_profile.equivalent_topic -> Ethnic group\n# Answer:\nEthnicity", "# Reasoning Path:\nAncestral Puebloans -> common.topic.notable_types -> Ethnicity -> type.type.expected_by -> Ethnic population\n# Answer:\nEthnicity", "# Reasoning Path:\nAncestral Puebloans -> common.topic.notable_types -> Ethnicity -> type.type.expected_by -> Included in group(s)\n# Answer:\nEthnicity"], "ground_truth": ["Four Corners"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-1205", "prediction": ["# Reasoning Path:\nHarley Quinn -> tv.tv_character.appeared_in_tv_program -> m.02wm17r -> tv.regular_tv_appearance.actor -> Hynden Walch\n# Answer:\nm.02wm17r", "# Reasoning Path:\nHarley Quinn -> tv.tv_character.appeared_in_tv_program -> m.0wz39vs -> tv.regular_tv_appearance.actor -> Arleen Sorkin\n# Answer:\nm.0wz39vs", "# Reasoning Path:\nHarley Quinn -> tv.tv_character.appeared_in_tv_program -> m.02wm18b -> tv.regular_tv_appearance.actor -> Mia Sara\n# Answer:\nm.02wm18b", "# Reasoning Path:\nHarley Quinn -> tv.tv_character.appeared_in_tv_program -> m.0wz39vs -> tv.regular_tv_appearance.seasons -> Batman: The Animated Series - Season 3\n# Answer:\nm.0wz39vs", "# Reasoning Path:\nHarley Quinn -> cvg.game_character.games -> m.09dycc_ -> cvg.game_performance.game -> Batman: Arkham Asylum\n# Answer:\nm.09dycc_", "# Reasoning Path:\nHarley Quinn -> fictional_universe.fictional_character.romantically_involved_with -> m.030rldt -> fictional_universe.romantic_involvement.partner -> Joker\n# Answer:\nm.030rldt", "# Reasoning Path:\nHarley Quinn -> tv.tv_character.appeared_in_tv_program -> m.02wm18b -> tv.regular_tv_appearance.series -> Birds of Prey\n# Answer:\nm.02wm18b", "# Reasoning Path:\nHarley Quinn -> cvg.game_character.games -> m.09dycc_ -> cvg.game_performance.voice_actor -> Arleen Sorkin\n# Answer:\nm.09dycc_", "# Reasoning Path:\nHarley Quinn -> cvg.game_character.games -> m.0ygmchz -> cvg.game_performance.voice_actor -> Tara Strong\n# Answer:\nm.0ygmchz", "# Reasoning Path:\nHarley Quinn -> cvg.game_character.games -> m.0ygmchz -> cvg.game_performance.game -> Batman: Arkham City\n# Answer:\nm.0ygmchz"], "ground_truth": ["Mia Sara", "Hynden Walch", "Arleen Sorkin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1206", "prediction": ["# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> common.topic.subject_of -> Autobiography\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Statesman -> common.topic.article -> m.016fc9\n# Answer:\nStatesman", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> tv.tv_subject.tv_programs -> The Secret Life of Scientists & Engineers\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Philosopher -> fictional_universe.character_rank.characters_of_this_rank -> Heraclitus\n# Answer:\nPhilosopher", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> common.topic.subject_of -> Biography\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> base.descriptive_names.names.descriptive_name -> m.01053kvm\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> common.topic.subject_of -> Book\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Philosopher -> common.topic.notable_types -> Profession\n# Answer:\nPhilosopher", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> base.descriptive_names.names.descriptive_name -> m.01053kyr\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Statesman -> base.descriptive_names.names.descriptive_name -> m.0101fnw4\n# Answer:\nStatesman"], "ground_truth": ["Philosopher", "Statesman", "Author", "Scientist", "Spy"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 1.0, "ans_recall": 0.6, "path_f1": 0.8571428571428571, "path_precision": 1.0, "path_recall": 0.75, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 1.0, "path_ans_recall": 0.6}
{"id": "WebQTest-1209", "prediction": ["# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 0 -> tv.tv_series_season.episodes -> 10 Items or Less: 10 Random Looks at NCIS\n# Answer:\nNCIS - Season 0", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 0 -> tv.tv_series_season.episodes -> 10 Years Aft\n# Answer:\nNCIS - Season 0", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 0 -> common.topic.notable_for -> g.125505_2h\n# Answer:\nNCIS - Season 0", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 0 -> tv.tv_series_season.episodes -> A Death in the Family\n# Answer:\nNCIS - Season 0", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 1 -> tv.tv_series_season.episodes -> A Weak Link\n# Answer:\nNCIS - Season 1", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 10 -> tv.tv_series_season.episodes -> Berlin\n# Answer:\nNCIS - Season 10", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 1 -> tv.tv_series_season.episodes -> B\u00eate Noire\n# Answer:\nNCIS - Season 1", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 1 -> tv.tv_series_season.episodes -> Dead Man Talking\n# Answer:\nNCIS - Season 1", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 1 -> common.topic.image -> NCISSeason01.jpg\n# Answer:\nNCIS - Season 1", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 10 -> tv.tv_series_season.episodes -> Canary\n# Answer:\nNCIS - Season 10"], "ground_truth": ["NCIS - Season 3"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nm.0w9plqp", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m -> soccer.football_player_stats.team -> LA Galaxy\n# Answer:\nm.0w8w79m", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nm.0j2zzj_", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9021c -> soccer.football_player_stats.team -> A.C. Milan\n# Answer:\nm.0w9021c", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0k4ytw5", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0j2zzj_", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.0qd3y3c -> sports.sports_team_roster.team -> Paris Saint-Germain F.C.\n# Answer:\nm.0qd3y3c", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nm.0k4ytw5", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.0qd3y3c -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nm.0qd3y3c", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.02nr829 -> sports.sports_team_roster.team -> LA Galaxy\n# Answer:\nm.02nr829"], "ground_truth": ["LA Galaxy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.47058823529411764, "path_precision": 0.4, "path_recall": 0.5714285714285714, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1210", "prediction": ["# Reasoning Path:\nCanada -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> City/Town/Village\n# Answer:\nEarth", "# Reasoning Path:\nCanada -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> New Zealand\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.subdividing_type -> Sovereign state\n# Answer:\nEarth", "# Reasoning Path:\nCanada -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> Sovereign state\n# Answer:\nEarth", "# Reasoning Path:\nCanada -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> France\n# Answer:\nEarth", "# Reasoning Path:\nCanada -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.12cp_k6ss\n# Answer:\ng.12cp_k6ss", "# Reasoning Path:\nCanada -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Bermuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.country.official_language -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> United States of America\n# Answer:\nEarth"], "ground_truth": ["O Canada"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1212", "prediction": ["# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> education.education.major_field_of_study -> Computer programming\n# Answer:\nm.0j_gm2q", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.02n93cn -> education.education.major_field_of_study -> Computer Science\n# Answer:\nm.02n93cn", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nm.0j_gm2q", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.02n93cn -> education.education.major_field_of_study -> Psychology\n# Answer:\nm.02n93cn", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_value -> End Date\n# Answer:\nm.0j_gm2q", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.02n93cn -> education.education.institution -> Harvard University\n# Answer:\nm.02n93cn", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nm.0j_gm2q", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.02n93cn -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nm.02n93cn", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.04hc7zn -> education.education.institution -> Phillips Exeter Academy\n# Answer:\nm.04hc7zn", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Specialization\n# Answer:\nm.0j_gm2q"], "ground_truth": ["Computer Science", "Computer programming", "Psychology"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1213", "prediction": ["# Reasoning Path:\nMichael Jackson -> people.person.place_of_birth -> Gary -> location.location.containedby -> United States of America\n# Answer:\nGary", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1Club.FM: 70s (Pop) -> broadcast.content.location -> Chicago\n# Answer:\n1Club.FM: 70s (Pop)", "# Reasoning Path:\nMichael Jackson -> music.artist.origin -> Gary -> location.location.containedby -> United States of America\n# Answer:\nGary", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1Club.FM: 70s (Pop) -> common.topic.image -> 1clubfm.jpg\n# Answer:\n1Club.FM: 70s (Pop)", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1Club.FM: 70s (Pop) -> common.topic.notable_types -> Broadcast Content\n# Answer:\n1Club.FM: 70s (Pop)", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1Club.FM: 80s (Pop) -> broadcast.content.genre -> 80's\n# Answer:\n1Club.FM: 80s (Pop)", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1Club.FM: 80s (Pop) -> broadcast.content.producer -> 1Club.FM\n# Answer:\n1Club.FM: 80s (Pop)", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1Club.FM: 80s (Pop) -> common.topic.image -> 1clubfm.jpg\n# Answer:\n1Club.FM: 80s (Pop)", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1Club.FM: 80s (Pop) -> broadcast.content.genre -> Classic hits\n# Answer:\n1Club.FM: 80s (Pop)", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1Club.FM: 80s (Pop) -> broadcast.content.genre -> Contemporary R&B\n# Answer:\n1Club.FM: 80s (Pop)"], "ground_truth": ["United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1215", "prediction": ["# Reasoning Path:\nStephen Covey -> people.person.profession -> Consultant -> business.product_line.brand -> Resilient Solution\n# Answer:\nConsultant", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Consultant -> common.topic.subjects -> Ben Galley\n# Answer:\nConsultant", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Motivational speaker -> common.topic.notable_for -> g.1254yfch4\n# Answer:\nMotivational speaker", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Consultant -> business.industry.companies -> Corporate Citizenship\n# Answer:\nConsultant", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Motivational speaker -> base.descriptive_names.names.descriptive_name -> m.0_z7q_5\n# Answer:\nMotivational speaker", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Motivational speaker -> people.profession.specialization_of -> Public speaker\n# Answer:\nMotivational speaker", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Author -> tv.tv_subject.tv_episodes -> Author David Kerr - The Ol' Turkey Hunter\n# Answer:\nAuthor", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor", "# Reasoning Path:\nStephen Covey -> book.author.works_written -> 360 Words I Know -> book.book.editions -> 360 Words I Know (Grades K - 2) (Step Ahead Golden Books Workbook)\n# Answer:\n360 Words I Know", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Author -> tv.tv_subject.tv_episodes -> J. J. Woods \u2013 Author - The Forgotten Covenant\n# Answer:\nAuthor"], "ground_truth": ["Professor", "Motivational speaker", "Manager", "Writer", "Author", "Consultant"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.6428571428571429, "ans_precission": 0.9, "ans_recall": 0.5, "path_f1": 0.42857142857142855, "path_precision": 0.6, "path_recall": 0.3333333333333333, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1216", "prediction": ["# Reasoning Path:\nJoseph Stalin -> people.person.nationality -> Soviet Union -> location.country.capital -> Moscow\n# Answer:\nSoviet Union", "# Reasoning Path:\nJoseph Stalin -> people.person.nationality -> Soviet Union -> government.governmental_jurisdiction.government_positions -> General Secretary of the Communist Party of the Soviet Union\n# Answer:\nSoviet Union", "# Reasoning Path:\nJoseph Stalin -> government.politician.government_positions_held -> m.049x6_k -> government.government_position_held.jurisdiction_of_office -> Soviet Union\n# Answer:\nm.049x6_k", "# Reasoning Path:\nJoseph Stalin -> people.person.nationality -> Soviet Union -> base.rivalries.rival.rivals -> Space Race\n# Answer:\nSoviet Union", "# Reasoning Path:\nJoseph Stalin -> people.person.nationality -> Russian Empire -> location.country.official_language -> Russian Language\n# Answer:\nRussian Empire", "# Reasoning Path:\nJoseph Stalin -> government.politician.government_positions_held -> m.049x6_k -> government.government_position_held.office_position_or_title -> General Secretary of the Communist Party of the Soviet Union\n# Answer:\nm.049x6_k", "# Reasoning Path:\nJoseph Stalin -> government.politician.government_positions_held -> m.049x6_k -> government.government_position_held.governmental_body -> Central Committee of the Communist Party of the Soviet Union\n# Answer:\nm.049x6_k", "# Reasoning Path:\nJoseph Stalin -> symbols.name_source.namesakes -> Country Joe McDonald -> people.person.nationality -> United States of America\n# Answer:\nCountry Joe McDonald", "# Reasoning Path:\nJoseph Stalin -> government.politician.government_positions_held -> m.0c9_j12 -> government.government_position_held.jurisdiction_of_office -> Soviet Union\n# Answer:\nm.0c9_j12", "# Reasoning Path:\nJoseph Stalin -> people.person.nationality -> Russian Empire -> location.location.events -> Allied intervention in the Russian Civil War\n# Answer:\nRussian Empire"], "ground_truth": ["Soviet Union"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1217", "prediction": ["# Reasoning Path:\nMonta Ellis -> basketball.basketball_player.player_statistics -> m.04qds38 -> basketball.basketball_player_stats.season -> 2005\u201306 NBA season\n# Answer:\nm.04qds38", "# Reasoning Path:\nMonta Ellis -> sports.pro_athlete.sports_played_professionally -> m.0c54277 -> sports.pro_sports_played.sport -> Basketball\n# Answer:\nm.0c54277", "# Reasoning Path:\nMonta Ellis -> basketball.basketball_player.player_statistics -> m.04qhmvs -> basketball.basketball_player_stats.season -> 2007\u201308 NBA season\n# Answer:\nm.04qhmvs", "# Reasoning Path:\nMonta Ellis -> common.topic.article -> m.06rsnr\n# Answer:\nm.06rsnr", "# Reasoning Path:\nMonta Ellis -> basketball.basketball_player.player_statistics -> m.04qq_57 -> basketball.basketball_player_stats.season -> 2006\u201307 NBA season\n# Answer:\nm.04qq_57", "# Reasoning Path:\nMonta Ellis -> basketball.basketball_player.player_statistics -> m.04qds38 -> basketball.basketball_player_stats.team -> Golden State Warriors\n# Answer:\nm.04qds38", "# Reasoning Path:\nMonta Ellis -> basketball.basketball_player.player_statistics -> m.04qhmvs -> basketball.basketball_player_stats.team -> Golden State Warriors\n# Answer:\nm.04qhmvs", "# Reasoning Path:\nMonta Ellis -> basketball.basketball_player.player_statistics -> m.04qq_57 -> basketball.basketball_player_stats.team -> Golden State Warriors\n# Answer:\nm.04qq_57"], "ground_truth": ["NBA Most Improved Player Award"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1219", "prediction": ["# Reasoning Path:\nRihanna -> people.person.nationality -> Barbados -> location.location.contains -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> people.person.nationality -> Barbados -> location.country.administrative_divisions -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.location.contains -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> people.person.nationality -> Barbados -> common.topic.notable_types -> Country\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.country.administrative_divisions -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\ng.11bv1mttmr", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> common.topic.notable_types -> Country\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\ng.11bv383dbd", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish"], "ground_truth": ["Barbados"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> F\u00e1tima Bl\u00e1zquez\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> common.topic.image -> Die Plaza Mayor am Abend\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Hidalgo\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7t8559g\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Zacuto\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7tbpl3m\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.nationality -> Spain -> location.country.administrative_divisions -> Andalusia\n# Answer:\nSpain", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7vzj2hj\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> common.topic.image -> Salamanca 2008\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.nationality -> Spain -> location.country.second_level_divisions -> Asturias\n# Answer:\nSpain"], "ground_truth": ["Salamanca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1220", "prediction": ["# Reasoning Path:\nShannen Doherty -> tv.tv_actor.starring_roles -> m.0220tfg -> tv.regular_tv_appearance.series -> Beverly Hills, 90210\n# Answer:\nm.0220tfg", "# Reasoning Path:\nShannen Doherty -> tv.tv_actor.starring_roles -> m.03ltlbv -> tv.regular_tv_appearance.series -> North Shore\n# Answer:\nm.03ltlbv", "# Reasoning Path:\nShannen Doherty -> tv.tv_actor.starring_roles -> m.0220tfg -> tv.regular_tv_appearance.character -> Brenda Walsh\n# Answer:\nm.0220tfg", "# Reasoning Path:\nShannen Doherty -> tv.tv_actor.starring_roles -> m.0220tfg -> tv.regular_tv_appearance.seasons -> Beverly Hills, 90210 - Season 1\n# Answer:\nm.0220tfg", "# Reasoning Path:\nShannen Doherty -> tv.tv_director.episodes_directed -> All Hell Breaks Loose -> common.topic.notable_for -> g.125dwbgbl\n# Answer:\nAll Hell Breaks Loose", "# Reasoning Path:\nShannen Doherty -> tv.tv_actor.starring_roles -> m.03l8dl9 -> tv.regular_tv_appearance.character -> Prue Halliwell\n# Answer:\nm.03l8dl9", "# Reasoning Path:\nShannen Doherty -> tv.tv_actor.guest_roles -> m.05ntycx -> tv.tv_guest_role.character -> Brenda Walsh\n# Answer:\nm.05ntycx", "# Reasoning Path:\nShannen Doherty -> tv.tv_actor.starring_roles -> m.0220tfg -> tv.regular_tv_appearance.seasons -> Beverly Hills, 90210 - Season 2\n# Answer:\nm.0220tfg", "# Reasoning Path:\nShannen Doherty -> tv.tv_actor.starring_roles -> m.03l8dl9 -> tv.regular_tv_appearance.seasons -> Charmed - Season 2\n# Answer:\nm.03l8dl9", "# Reasoning Path:\nShannen Doherty -> tv.tv_director.episodes_directed -> All Hell Breaks Loose -> common.topic.article -> m.0kh01l4\n# Answer:\nAll Hell Breaks Loose"], "ground_truth": ["Nowhere", "Freeze Frame", "Nightlight", "Girls Just Want to Have Fun", "Striking Poses", "Hell on Heels: The Battle of Mary Kay", "Category 7: The End of the World", "Growing the Big One", "The Lost Treasure of the Grand Canyon", "The Other Lover", "Burning Palms", "Gone in the Night", "Another Day", "Bukowski", "Night Shift", "Almost Dead", "Blindfold: Acts of Obsession", "Heathers", "Bless All the Dear Children", "Christmas Caper", "Friends 'Til the End", "Mallrats", "Sleeping with the Devil", "Obsessed", "The Rendering", "Jailbreakers", "A Burning Passion: The Margaret Mitchell Story", "Jay and Silent Bob Strike Back", "The Delphi Effect", "The Ticket", "The Secret of NIMH", "Satan's School for Girls", "Witchslayer Gretl"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1221", "prediction": ["# Reasoning Path:\nJoakim Noah -> people.person.education -> m.03p7z1w -> education.education.institution -> University of Florida\n# Answer:\nm.03p7z1w", "# Reasoning Path:\nJoakim Noah -> sports.drafted_athlete.drafted -> m.02nqxbv -> sports.sports_league_draft_pick.team -> Chicago Bulls\n# Answer:\nm.02nqxbv", "# Reasoning Path:\nJoakim Noah -> people.person.education -> m.0b6vrkl -> education.education.institution -> Poly Prep Country Day School\n# Answer:\nm.0b6vrkl", "# Reasoning Path:\nJoakim Noah -> people.person.education -> m.040rzsq -> education.education.institution -> Lawrenceville School\n# Answer:\nm.040rzsq", "# Reasoning Path:\nJoakim Noah -> sports.drafted_athlete.drafted -> m.02nqxbv -> sports.sports_league_draft_pick.draft -> 2007 NBA draft\n# Answer:\nm.02nqxbv", "# Reasoning Path:\nJoakim Noah -> sports.pro_athlete.teams -> m.02397ys -> sports.sports_team_roster.team -> Chicago Bulls\n# Answer:\nm.02397ys", "# Reasoning Path:\nJoakim Noah -> sports.pro_athlete.teams -> m.02397ys -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.02397ys", "# Reasoning Path:\nJoakim Noah -> sports.pro_athlete.teams -> m.0n819kn -> sports.sports_team_roster.team -> Florida Gators men's basketball\n# Answer:\nm.0n819kn", "# Reasoning Path:\nJoakim Noah -> sports.pro_athlete.teams -> m.0n819kn -> sports.sports_team_roster.position -> Center\n# Answer:\nm.0n819kn", "# Reasoning Path:\nJoakim Noah -> sports.pro_athlete.teams -> m.0n819kn -> sports.sports_team_roster.position -> Forward\n# Answer:\nm.0n819kn"], "ground_truth": ["University of Florida"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1223", "prediction": ["# Reasoning Path:\nBilly Elliot -> film.film.starring -> m.0y81fs9 -> film.performance.actor -> Adam Cooper\n# Answer:\nm.0y81fs9", "# Reasoning Path:\nBilly Elliot -> film.film.starring -> m.0jvcy7 -> film.performance.actor -> Jamie Bell\n# Answer:\nm.0jvcy7", "# Reasoning Path:\nBilly Elliot -> film.film_character.portrayed_in_films -> m.0y81fs9 -> film.performance.actor -> Adam Cooper\n# Answer:\nm.0y81fs9", "# Reasoning Path:\nBilly Elliot -> film.film.starring -> m.02tbgcw -> film.performance.actor -> Gary Lewis\n# Answer:\nm.02tbgcw", "# Reasoning Path:\nBilly Elliot -> film.film_character.portrayed_in_films -> m.0jvcy7 -> film.performance.actor -> Jamie Bell\n# Answer:\nm.0jvcy7", "# Reasoning Path:\nBilly Elliot -> film.film.starring -> m.02tbgcw -> film.performance.character -> Jackie Elliot\n# Answer:\nm.02tbgcw", "# Reasoning Path:\nBilly Elliot -> common.topic.webpage -> m.03l6gmg -> common.webpage.resource -> m.0bkz0lq\n# Answer:\nm.03l6gmg", "# Reasoning Path:\nBilly Elliot -> common.topic.webpage -> m.03l6gmg -> common.webpage.category -> Official Website\n# Answer:\nm.03l6gmg", "# Reasoning Path:\nBilly Elliot -> common.topic.webpage -> m.0940b0f -> common.webpage.resource -> Fall film catchphrases\n# Answer:\nm.0940b0f", "# Reasoning Path:\nBilly Elliot -> common.topic.webpage -> m.0940b0f -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.0940b0f"], "ground_truth": ["Jamie Bell", "Adam Cooper"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.17391304347826086, "path_precision": 0.2, "path_recall": 0.15384615384615385, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1225", "prediction": ["# Reasoning Path:\nKourtney Kardashian -> base.popstra.celebrity.dated -> m.065px1n -> base.popstra.dated.participant -> Scott Disick\n# Answer:\nm.065px1n", "# Reasoning Path:\nKourtney Kardashian -> common.topic.image -> Kourtney Kardashian 2 2009 -> common.image.size -> m.0632364\n# Answer:\nKourtney Kardashian 2 2009", "# Reasoning Path:\nKourtney Kardashian -> award.award_winner.awards_won -> m.0wj9z_x -> award.award_honor.award_winner -> Kendall\n# Answer:\nm.0wj9z_x", "# Reasoning Path:\nKourtney Kardashian -> award.award_winner.awards_won -> m.0wj9z_x -> award.award_honor.ceremony -> 2013 Teen Choice Awards\n# Answer:\nm.0wj9z_x", "# Reasoning Path:\nKourtney Kardashian -> award.award_winner.awards_won -> m.0y66_42 -> award.award_honor.honored_for -> Keeping Up with the Kardashians\n# Answer:\nm.0y66_42", "# Reasoning Path:\nKourtney Kardashian -> award.award_winner.awards_won -> m.0wj9z_x -> award.award_honor.award -> Teen Choice Award for Choice TV Female Reality Star\n# Answer:\nm.0wj9z_x", "# Reasoning Path:\nKourtney Kardashian -> award.award_winner.awards_won -> m.0wj9z_x -> award.award_honor.award_winner -> Khlo\u00e9 Kardashian\n# Answer:\nm.0wj9z_x", "# Reasoning Path:\nKourtney Kardashian -> award.award_winner.awards_won -> m.0y66_42 -> award.award_honor.award -> Teen Choice Award for Choice TV Female Reality Star\n# Answer:\nm.0y66_42", "# Reasoning Path:\nKourtney Kardashian -> award.award_winner.awards_won -> m.0wj9z_x -> award.award_honor.award_winner -> Kim Kardashian\n# Answer:\nm.0wj9z_x", "# Reasoning Path:\nKourtney Kardashian -> award.award_winner.awards_won -> m.0y76dmh -> award.award_honor.award_winner -> Khlo\u00e9 Kardashian\n# Answer:\nm.0y76dmh"], "ground_truth": ["Scott Disick"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1226", "prediction": ["# Reasoning Path:\nAustralia -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\ng.11b60qdnlp", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Canada\n# Answer:\nParliamentary system", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\ng.11b60vfr15", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Federal monarchy -> government.form_of_government.countries -> Canada\n# Answer:\nFederal monarchy", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\ng.1245_2hrg", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Japan\n# Answer:\nParliamentary system", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_55mr\n# Answer:\ng.1245_55mr", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_2hqk\n# Answer:\ng.1245_2hqk", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> New Caledonia\n# Answer:\nParliamentary system"], "ground_truth": ["Constitutional monarchy", "Federal monarchy", "Parliamentary system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1227", "prediction": ["# Reasoning Path:\nHowie Long -> sports.pro_athlete.teams -> m.0hqqf18 -> sports.sports_team_roster.team -> Oakland Raiders\n# Answer:\nm.0hqqf18", "# Reasoning Path:\nHowie Long -> sports.pro_athlete.teams -> m.0hqqf0t -> sports.sports_team_roster.team -> Villanova Wildcats football\n# Answer:\nm.0hqqf0t", "# Reasoning Path:\nHowie Long -> sports.pro_athlete.teams -> m.0hqqf18 -> sports.sports_team_roster.position -> Defensive end\n# Answer:\nm.0hqqf18", "# Reasoning Path:\nHowie Long -> sports.pro_athlete.teams -> m.0hqqf0t -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hqqf0t", "# Reasoning Path:\nHowie Long -> common.topic.image -> Howie Long - American Football Player TV host -> common.image.size -> m.02d_35f\n# Answer:\nHowie Long - American Football Player TV host", "# Reasoning Path:\nHowie Long -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nHowie Long -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nHowie Long -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nHowie Long -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#domain -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nHowie Long -> freebase.valuenotation.is_reviewed -> Gender -> type.property.schema -> Person\n# Answer:\nGender"], "ground_truth": ["Villanova Wildcats football", "Oakland Raiders"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1228", "prediction": ["# Reasoning Path:\nGreek alphabet -> language.language_writing_system.parent_writing_systems -> Phoenician alphabet -> language.language_writing_system.parent_writing_systems -> Proto-Canaanite alphabet\n# Answer:\nPhoenician alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.parent_writing_systems -> Phoenician alphabet -> language.language_writing_system.languages -> Phoenician\n# Answer:\nPhoenician alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.parent_writing_systems -> Phoenician alphabet -> base.schemastaging.context_name.pronunciation -> g.125_pyj6_\n# Answer:\nPhoenician alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.child_writing_systems -> Gothic alphabet -> language.language_writing_system.parent_writing_systems -> Latin alphabet\n# Answer:\nGothic alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.parent_writing_systems -> Phoenician alphabet -> language.language_writing_system.languages -> Araona Language\n# Answer:\nPhoenician alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.parent_writing_systems -> Phoenician alphabet -> language.language_writing_system.languages -> Biblical Hebrew\n# Answer:\nPhoenician alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.child_writing_systems -> Gothic alphabet -> base.schemastaging.context_name.pronunciation -> m.01314m6_\n# Answer:\nGothic alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.child_writing_systems -> Gothic alphabet -> language.language_writing_system.languages -> Crimean Gothic\n# Answer:\nGothic alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.child_writing_systems -> Lycian alphabet -> common.topic.notable_types -> Language Writing System\n# Answer:\nLycian alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.child_writing_systems -> Lycian alphabet -> common.topic.notable_for -> g.1256qjtn2\n# Answer:\nLycian alphabet"], "ground_truth": ["Phoenician alphabet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1229", "prediction": ["# Reasoning Path:\nElizabethtown -> location.location.time_zones -> Eastern Time Zone -> common.topic.notable_for -> g.125bn2wcf\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nElizabethtown -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07c9g7m\n# Answer:\nm.07c9g7m", "# Reasoning Path:\nElizabethtown -> location.location.time_zones -> Eastern Time Zone -> common.topic.webpage -> m.03hc_rb\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nElizabethtown -> location.statistical_region.unemployment_rate -> g.11b60rwz93\n# Answer:\ng.11b60rwz93", "# Reasoning Path:\nElizabethtown -> location.location.time_zones -> Eastern Time Zone -> common.topic.article -> m.02hcvg\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nElizabethtown -> location.statistical_region.unemployment_rate -> g.11b60thpzf\n# Answer:\ng.11b60thpzf", "# Reasoning Path:\nElizabethtown -> location.location.time_zones -> Eastern Time Zone -> common.topic.article -> m.0j44rkj\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nElizabethtown -> location.statistical_region.unemployment_rate -> g.11b66dqlpc\n# Answer:\ng.11b66dqlpc", "# Reasoning Path:\nElizabethtown -> location.location.time_zones -> Eastern Time Zone -> common.topic.article -> m.0j62c65\n# Answer:\nEastern Time Zone"], "ground_truth": ["Eastern Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-123", "prediction": ["# Reasoning Path:\nGoogle -> organization.organization_partnership.members -> Acxiom Corporation -> business.customer.supplier -> IBM\n# Answer:\nAcxiom Corporation", "# Reasoning Path:\nGoogle -> common.topic.subjects -> Google Apps for Work -> organization.organization_partnership.members -> Redcorus\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> organization.organization_partnership.members -> Acxiom Corporation -> organization.organization.partnerships -> Hewlett-Packard\n# Answer:\nAcxiom Corporation", "# Reasoning Path:\nGoogle -> common.topic.subjects -> Google Apps for Work -> common.topic.subjects -> Gmail\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> organization.organization_partnership.members -> Cleverly Engaged Marketing -> organization.organization.sectors -> Digital marketing\n# Answer:\nCleverly Engaged Marketing", "# Reasoning Path:\nGoogle -> organization.organization_partnership.members -> Acxiom Corporation -> common.topic.notable_types -> Organization founder\n# Answer:\nAcxiom Corporation", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Ganeti -> common.topic.webpage -> m.0h40m62\n# Answer:\nGaneti", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> common.topic.subjects -> Gmail\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> common.topic.subjects -> Google Cloud Platform -> common.topic.subjects -> Google Developers\n# Answer:\nGoogle Cloud Platform", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Ganeti -> computer.software.languages_used -> Haskell\n# Answer:\nGaneti"], "ground_truth": ["Nexus S", "Nexus Q", "Google Maps", "Google Earth", "Google Classroom", "Google Buzz", "Apache Wave", "Nexus 7", "Google Docs, Sheets, and Slides", "Google Glass", "Google Drive", "Chromecast", "Nexus 10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1230", "prediction": ["# Reasoning Path:\nChristchurch -> travel.travel_destination.tourist_attractions -> Canterbury Museum, Christchurch -> common.topic.notable_types -> Museum\n# Answer:\nCanterbury Museum, Christchurch", "# Reasoning Path:\nChristchurch -> travel.travel_destination.tourist_attractions -> Canterbury Museum, Christchurch -> architecture.structure.architectural_style -> Gothic Revival architecture\n# Answer:\nCanterbury Museum, Christchurch", "# Reasoning Path:\nChristchurch -> travel.travel_destination.tourist_attractions -> Canterbury Museum, Christchurch -> common.topic.image -> Canterbury Museum1\n# Answer:\nCanterbury Museum, Christchurch", "# Reasoning Path:\nChristchurch -> travel.travel_destination.tourist_attractions -> Air Force Museum of New Zealand -> location.location.geolocation -> m.0wgdwgn\n# Answer:\nAir Force Museum of New Zealand", "# Reasoning Path:\nChristchurch -> location.location.contains -> Al-Huda Islamic Charitable Trust - HICT -> common.topic.webpage -> m.03ltr7y\n# Answer:\nAl-Huda Islamic Charitable Trust - HICT", "# Reasoning Path:\nChristchurch -> location.statistical_region.population -> g.11bcdlk0kf\n# Answer:\ng.11bcdlk0kf", "# Reasoning Path:\nChristchurch -> travel.travel_destination.tourist_attractions -> Cathedral Square, Christchurch -> location.location.containedby -> New Zealand\n# Answer:\nCathedral Square, Christchurch", "# Reasoning Path:\nChristchurch -> travel.travel_destination.tourist_attractions -> Air Force Museum of New Zealand -> common.topic.article -> m.06nnw3\n# Answer:\nAir Force Museum of New Zealand", "# Reasoning Path:\nChristchurch -> location.statistical_region.population -> g.11btt59cj5\n# Answer:\ng.11btt59cj5", "# Reasoning Path:\nChristchurch -> travel.travel_destination.tourist_attractions -> Canterbury Museum, Christchurch -> common.topic.image -> CanterburyMuseum gobeirne\n# Answer:\nCanterbury Museum, Christchurch"], "ground_truth": ["Cathedral Square, Christchurch", "Christchurch Botanic Gardens", "International Antarctic Centre", "Ferrymead Heritage Park", "TranzAlpine", "Willowbank Wildlife Reserve, Christchurch", "Air Force Museum of New Zealand", "Canterbury Museum, Christchurch", "Lyttelton Timeball Station", "Christchurch Arts Centre"], "ans_acc": 0.3, "ans_hit": 1, "ans_f1": 0.42, "ans_precission": 0.7, "ans_recall": 0.3, "path_f1": 0.3, "path_precision": 0.6, "path_recall": 0.2, "path_ans_f1": 0.42, "path_ans_precision": 0.7, "path_ans_recall": 0.3}
{"id": "WebQTest-1231", "prediction": ["# Reasoning Path:\nChris Paul -> people.person.place_of_birth -> Winston-Salem -> base.biblioness.bibs_location.state -> North Carolina\n# Answer:\nWinston-Salem", "# Reasoning Path:\nChris Paul -> common.topic.article -> m.0cymlx\n# Answer:\nm.0cymlx", "# Reasoning Path:\nChris Paul -> people.person.place_of_birth -> Winston-Salem -> common.topic.notable_types -> City/Town/Village\n# Answer:\nWinston-Salem", "# Reasoning Path:\nChris Paul -> people.person.place_of_birth -> Winston-Salem -> location.citytown.postal_codes -> 27101\n# Answer:\nWinston-Salem", "# Reasoning Path:\nChris Paul -> sports.pro_athlete.sports_played_professionally -> m.0c54blq -> sports.pro_sports_played.sport -> Basketball\n# Answer:\nm.0c54blq", "# Reasoning Path:\nChris Paul -> people.person.place_of_birth -> Winston-Salem -> location.citytown.postal_codes -> 27101-2755\n# Answer:\nWinston-Salem", "# Reasoning Path:\nChris Paul -> people.person.place_of_birth -> Winston-Salem -> location.citytown.postal_codes -> 27101-5025\n# Answer:\nWinston-Salem"], "ground_truth": ["United States of America"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1232", "prediction": ["# Reasoning Path:\nAlbert Pujols -> sports.sports_award_winner.awards -> m.04ynyvj -> sports.sports_award.award -> National League Rookie of the Year\n# Answer:\nm.04ynyvj", "# Reasoning Path:\nAlbert Pujols -> sports.sports_award_winner.awards -> m.04ynyvj -> sports.sports_award.season -> 2001 Major League Baseball Season\n# Answer:\nm.04ynyvj", "# Reasoning Path:\nAlbert Pujols -> sports.sports_award_winner.awards -> m.04xwlnb -> sports.sports_award.season -> 2008 Major League Baseball season\n# Answer:\nm.04xwlnb", "# Reasoning Path:\nAlbert Pujols -> sports.drafted_athlete.drafted -> m.0461q4y -> sports.sports_league_draft_pick.team -> St. Louis Cardinals\n# Answer:\nm.0461q4y", "# Reasoning Path:\nAlbert Pujols -> sports.sports_award_winner.awards -> m.04xwlnk -> sports.sports_award.season -> 2005 Major League Baseball season\n# Answer:\nm.04xwlnk", "# Reasoning Path:\nAlbert Pujols -> sports.drafted_athlete.drafted -> m.0461q4y -> sports.sports_league_draft_pick.school -> Maple Woods Community College\n# Answer:\nm.0461q4y", "# Reasoning Path:\nAlbert Pujols -> sports.sports_award_winner.awards -> m.04xwlnb -> sports.sports_award.award -> National League Most Valuable Player Award\n# Answer:\nm.04xwlnb", "# Reasoning Path:\nAlbert Pujols -> sports.sports_award_winner.awards -> m.04xwlnk -> sports.sports_award.award -> National League Most Valuable Player Award\n# Answer:\nm.04xwlnk", "# Reasoning Path:\nAlbert Pujols -> baseball.baseball_player.batting_stats -> m.06s6m0p -> baseball.batting_statistics.season -> 2001 Major League Baseball Season\n# Answer:\nm.06s6m0p", "# Reasoning Path:\nAlbert Pujols -> baseball.baseball_player.batting_stats -> m.06s6m8k -> baseball.batting_statistics.season -> 2006 Major League Baseball season\n# Answer:\nm.06s6m8k"], "ground_truth": ["2001 Major League Baseball Season"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1233", "prediction": ["# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> Blue -> book.book_subject.works -> Blue: The History of a Color\n# Answer:\nBlue", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> Blue -> common.topic.notable_for -> g.125gl7_n4\n# Answer:\nBlue", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> Blue -> base.schemastaging.visual_color_extra.hsv -> m.010nmgqr\n# Answer:\nBlue", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> Grey -> business.product_category.products -> Pewter Faux Leather (D8564)\n# Answer:\nGrey", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> Grey -> base.fashionmodels.eye_color.fashion_models_with_this_eye_color -> Karen Mulder\n# Answer:\nGrey", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> White -> base.roses.rose_color.roses_of_this_color -> Rosa laevigata\n# Answer:\nWhite", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> Grey -> common.topic.article -> m.036k5t\n# Answer:\nGrey", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> White -> base.roses.rose_color.roses_of_this_color -> Rosa 'Ambiente'\n# Answer:\nWhite", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> White -> base.schemastaging.visual_color_extra.srgb -> m.010q1h63\n# Answer:\nWhite", "# Reasoning Path:\nNew York Giants -> sports.sports_team.season_record -> m.075fpjh -> sports.sports_team_season_record.season -> 2000 NFL season\n# Answer:\nm.075fpjh"], "ground_truth": ["Red", "Blue", "White", "Grey"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.8181818181818182, "path_precision": 0.9, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-1234", "prediction": ["# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Business magnate -> base.descriptive_names.names.descriptive_name -> m.0106b2k8\n# Answer:\nBusiness magnate", "# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Business magnate -> common.topic.article -> m.025sq3\n# Answer:\nBusiness magnate", "# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Businessperson -> common.topic.subjects -> Krishna Mali\n# Answer:\nBusinessperson", "# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Business magnate -> book.book_subject.works -> Masters of Enterprise: Giants of American Business from John Jacob Astor and J.P. Morgan to Bill Gates and Oprah Winfrey\n# Answer:\nBusiness magnate", "# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Business magnate -> base.descriptive_names.names.descriptive_name -> m.0106b2kr\n# Answer:\nBusiness magnate", "# Reasoning Path:\nAndrew Carnegie -> symbols.name_source.namesakes -> Carnegie -> location.location.time_zones -> Central Time Zone\n# Answer:\nCarnegie", "# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Business magnate -> base.descriptive_names.names.descriptive_name -> m.010fn7xm\n# Answer:\nBusiness magnate", "# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Businessperson -> common.topic.notable_types -> Profession\n# Answer:\nBusinessperson", "# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Entrepreneur -> common.topic.subject_of -> Krishna Mali\n# Answer:\nEntrepreneur", "# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Businessperson -> common.topic.subject_of -> Krishna Mali\n# Answer:\nBusinessperson"], "ground_truth": ["financier", "steel"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1235", "prediction": ["# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Pet door -> common.topic.notable_types -> Invention\n# Answer:\nPet door", "# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Pet door -> common.topic.article -> m.079vqs\n# Answer:\nPet door", "# Reasoning Path:\nIsaac Newton -> base.argumentmaps.innovator.original_ideas -> Newton's law of universal gravitation -> common.topic.article -> m.01kffv\n# Answer:\nNewton's law of universal gravitation", "# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Reflecting telescope -> base.onephylogeny.type_of_thing.includes -> Eyepiece\n# Answer:\nReflecting telescope", "# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Reflecting telescope -> astronomy.telescope_principle_type.telescopic_classifications_of_this_technique -> Optical telescope\n# Answer:\nReflecting telescope", "# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Reflecting telescope -> base.onephylogeny.type_of_thing.includes -> Objective\n# Answer:\nReflecting telescope", "# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Reflecting telescope -> astronomy.telescope_type.telescopes_of_this_type -> 60 inch telescope\n# Answer:\nReflecting telescope", "# Reasoning Path:\nIsaac Newton -> base.argumentmaps.innovator.original_ideas -> Newton's law of universal gravitation -> common.topic.image -> Isaacnewton\n# Answer:\nNewton's law of universal gravitation", "# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Reflecting telescope -> astronomy.telescope_type.telescopes_of_this_type -> Astrophysical Research Consortium Telescope\n# Answer:\nReflecting telescope", "# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Reflecting telescope -> astronomy.telescope_type.telescopes_of_this_type -> Gran Telescopio Canarias\n# Answer:\nReflecting telescope"], "ground_truth": ["Reflecting telescope", "Pet door"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1237", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.team -> Orlando Magic\n# Answer:\nm.02kbc3w", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nm.04qjyzm", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nm.04q9532", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qcbdr -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nm.04qcbdr", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.season -> 1992\u201393 NBA season\n# Answer:\nm.04qjyzm", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.season -> 1992\u201393 NBA season\n# Answer:\nm.02kbc3w", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player -> common.topic.notable_for -> g.125ctmqvt\n# Answer:\nPlayer", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.season -> 2003\u201304 NBA season\n# Answer:\nm.04q9532", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc58 -> sports.sports_award.season -> 1999\u20132000 NBA season\n# Answer:\nm.02kbc58", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qcbdr -> basketball.basketball_player_stats.season -> 1995\u201396 NBA season\n# Answer:\nm.04qcbdr"], "ground_truth": ["Orlando Magic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3529411764705882, "path_precision": 0.3, "path_recall": 0.42857142857142855, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1238", "prediction": ["# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.character_created_by -> Walt Disney -> award.award_winner.awards_won -> m.010qccn6\n# Answer:\nWalt Disney", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.character_created_by -> Walt Disney -> award.award_winner.awards_won -> m.09dznhz\n# Answer:\nWalt Disney", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.character_created_by -> Walt Disney -> award.award_nominee.award_nominations -> m.05bs963\n# Answer:\nWalt Disney", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Mouseton -> fictional_universe.fictional_universe.characters -> Ferdie Fieldmouse\n# Answer:\nMouseton", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Mouseton -> common.topic.notable_for -> g.1255cgp_j\n# Answer:\nMouseton", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.character_created_by -> Walt Disney -> award.award_winner.awards_won -> m.0_gjwtk\n# Answer:\nWalt Disney", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.character_created_by -> Walt Disney -> film.director.film -> Alice Gets Stung\n# Answer:\nWalt Disney", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Mouseton -> common.topic.notable_types -> Fictional Universe\n# Answer:\nMouseton", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.character_created_by -> Walt Disney -> award.award_nominee.award_nominations -> m.097k9k3\n# Answer:\nWalt Disney", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.character_created_by -> Ub Iwerks -> film.director.film -> Arctic Antics\n# Answer:\nUb Iwerks"], "ground_truth": ["Ub Iwerks", "Walt Disney"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1239", "prediction": ["# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94518 -> location.location.geometry -> m.056d_lq\n# Answer:\n94518", "# Reasoning Path:\nConcord -> common.topic.article -> m.0qyn0\n# Answer:\nm.0qyn0", "# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94520 -> location.location.geolocation -> m.03dnk1k\n# Answer:\n94520", "# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94518 -> location.location.containedby -> Contra Costa County\n# Answer:\n94518", "# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94527 -> location.postal_code.country -> United States of America\n# Answer:\n94527", "# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94520 -> location.location.geometry -> m.058s25t\n# Answer:\n94520", "# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94520 -> common.topic.notable_types -> Postal Code\n# Answer:\n94520", "# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94527 -> common.topic.notable_types -> Postal Code\n# Answer:\n94527", "# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94527 -> common.topic.notable_for -> g.1257xjg2l\n# Answer:\n94527", "# Reasoning Path:\nConcord -> location.location.people_born_here -> Alex Sanchez -> people.person.education -> m.0sw1m0b\n# Answer:\nAlex Sanchez"], "ground_truth": ["94521", "94522", "94529", "94524", "94519", "94527", "94518", "94520"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.5106382978723405, "ans_precission": 0.8, "ans_recall": 0.375, "path_f1": 0.5106382978723405, "path_precision": 0.8, "path_recall": 0.375, "path_ans_f1": 0.5106382978723405, "path_ans_precision": 0.8, "path_ans_recall": 0.375}
{"id": "WebQTest-124", "prediction": ["# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.region -> Africa\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> base.rosetta.languoid.parent -> Arabic Group\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Nobiin Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nNobiin Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.writing_system -> Arabic alphabet\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Bedawi Arabic -> language.human_language.countries_spoken_in -> Jordan\n# Answer:\nBedawi Arabic", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Nobiin Language -> common.topic.notable_types -> Human Language\n# Answer:\nNobiin Language", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\ng.12cp_k6sh", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Bedawi Arabic -> language.human_language.countries_spoken_in -> Syria\n# Answer:\nBedawi Arabic", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Bedawi Arabic -> language.language_dialect.language -> Arabic Language\n# Answer:\nBedawi Arabic", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Bedawi Arabic -> language.human_language.language_family -> Afroasiatic languages\n# Answer:\nBedawi Arabic"], "ground_truth": ["Modern Standard Arabic"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1240", "prediction": ["# Reasoning Path:\nMichael Vick -> american_football.football_player.rushing -> m.07tdd11 -> american_football.player_rushing_statistics.team -> Philadelphia Eagles\n# Answer:\nm.07tdd11", "# Reasoning Path:\nMichael Vick -> american_football.football_player.rushing -> m.07sh2b2 -> american_football.player_rushing_statistics.team -> Philadelphia Eagles\n# Answer:\nm.07sh2b2", "# Reasoning Path:\nMichael Vick -> american_football.football_player.rushing -> m.07vzkbw -> american_football.player_rushing_statistics.team -> Philadelphia Eagles\n# Answer:\nm.07vzkbw", "# Reasoning Path:\nMichael Vick -> american_football.football_player.rushing -> m.07sh2b2 -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\nm.07sh2b2", "# Reasoning Path:\nMichael Vick -> american_football.football_player.rushing -> m.07tdd11 -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\nm.07tdd11", "# Reasoning Path:\nMichael Vick -> american_football.football_player.rushing -> m.07vzkbw -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\nm.07vzkbw", "# Reasoning Path:\nMichael Vick -> people.person.education -> m.0hpcsnj -> education.education.institution -> Virginia Tech\n# Answer:\nm.0hpcsnj", "# Reasoning Path:\nMichael Vick -> common.topic.image -> VICKpb crop -> common.image.size -> m.03sv987\n# Answer:\nVICKpb crop", "# Reasoning Path:\nMichael Vick -> people.person.education -> m.0n1k_2y -> education.education.institution -> Warwick High School\n# Answer:\nm.0n1k_2y", "# Reasoning Path:\nMichael Vick -> common.topic.image -> Vick 6 -> common.image.size -> m.0bct9py\n# Answer:\nVick 6"], "ground_truth": ["New York Jets"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1241", "prediction": ["# Reasoning Path:\nColumbia River -> geography.river.origin -> Columbia Lake -> common.topic.notable_types -> Lake\n# Answer:\nColumbia Lake", "# Reasoning Path:\nColumbia River -> geography.river.origin -> Columbia Lake -> location.location.containedby -> British Columbia\n# Answer:\nColumbia Lake", "# Reasoning Path:\nColumbia River -> location.location.events -> Robert Gray's Columbia River expedition -> time.event.locations -> North America\n# Answer:\nRobert Gray's Columbia River expedition", "# Reasoning Path:\nColumbia River -> geography.river.origin -> Columbia Lake -> location.location.containedby -> Canada\n# Answer:\nColumbia Lake", "# Reasoning Path:\nColumbia River -> location.location.events -> Robert Gray's Columbia River expedition -> common.topic.image -> Capt Robert Gray\n# Answer:\nRobert Gray's Columbia River expedition", "# Reasoning Path:\nColumbia River -> location.location.events -> Robert Gray's Columbia River expedition -> common.topic.notable_for -> g.1256p97hd\n# Answer:\nRobert Gray's Columbia River expedition", "# Reasoning Path:\nColumbia River -> geography.body_of_water.bridges -> Astoria\u2013Megler Bridge -> common.topic.article -> m.03wf3p\n# Answer:\nAstoria\u2013Megler Bridge", "# Reasoning Path:\nColumbia River -> geography.body_of_water.bridges -> Beebe Bridge -> transportation.bridge.locale -> Chelan\n# Answer:\nBeebe Bridge", "# Reasoning Path:\nColumbia River -> geography.body_of_water.bridges -> Astoria\u2013Megler Bridge -> transportation.bridge.locale -> Astoria\n# Answer:\nAstoria\u2013Megler Bridge", "# Reasoning Path:\nColumbia River -> geography.body_of_water.bridges -> Beebe Bridge -> common.topic.notable_types -> Bridge\n# Answer:\nBeebe Bridge"], "ground_truth": ["Columbia Lake"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1242", "prediction": ["# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Deathly Hallows -> book.book_edition.author_editor -> J. K. Rowling\n# Answer:\nHarry Potter and the Deathly Hallows", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.locations -> Azkaban\n# Answer:\nAzkaban", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Chamber of Secrets -> book.written_work.previous_in_series -> Harry Potter and the Philosopher's Stone\n# Answer:\nHarry Potter and the Chamber of Secrets", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Deathly Hallows -> common.topic.webpage -> m.09w7pv0\n# Answer:\nHarry Potter and the Deathly Hallows", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Prisoner of Azkaban -> award.award_nominated_work.award_nominations -> m.08f5br7\n# Answer:\nHarry Potter and the Prisoner of Azkaban", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_character.has_possessed -> Deathly Hallows -> common.topic.image -> HP books\n# Answer:\nDeathly Hallows", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.locations -> 4 Privet Drive\n# Answer:\n4 Privet Drive", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Deathly Hallows -> common.topic.webpage -> m.09xtg5f\n# Answer:\nHarry Potter and the Deathly Hallows", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Chamber of Secrets -> book.written_work.subjects -> England\n# Answer:\nHarry Potter and the Chamber of Secrets", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Prisoner of Azkaban -> common.topic.notable_for -> g.125dz6zlg\n# Answer:\nHarry Potter and the Prisoner of Azkaban"], "ground_truth": ["Harry Potter and the Chamber of Secrets", "Harry Potter and the Half-Blood Prince", "Harry Potter and the Prisoner of Azkaban", "Harry Potter and the Philosopher's Stone", "Harry Potter and the Deathly Hallows", "Harry Potter and the Goblet of Fire", "Harry Potter and the Order of the Phoenix"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.5316455696202531, "ans_precission": 0.7, "ans_recall": 0.42857142857142855, "path_f1": 0.16666666666666666, "path_precision": 0.2, "path_recall": 0.14285714285714285, "path_ans_f1": 0.6292134831460674, "path_ans_precision": 0.7, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-1244", "prediction": ["# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Sailor -> fictional_universe.character_occupation.characters_with_this_occupation -> Jack Aubrey\n# Answer:\nSailor", "# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Sailor -> base.schemastaging.context_name.pronunciation -> g.125_nnp67\n# Answer:\nSailor", "# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Courtier -> base.descriptive_names.names.descriptive_name -> m.010b1tg9\n# Answer:\nCourtier", "# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Sailor -> base.descriptive_names.names.descriptive_name -> m.0105cyc4\n# Answer:\nSailor", "# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Poet -> media_common.quotation_subject.quotations_about_this_subject -> As imagination bodies forth The forms of things unknown, the poet's pen Turns them to shapes\n# Answer:\nPoet", "# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Sailor -> fictional_universe.character_occupation.characters_with_this_occupation -> Alfred King\n# Answer:\nSailor", "# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Sailor -> base.descriptive_names.names.descriptive_name -> m.0105cyz5\n# Answer:\nSailor", "# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Poet -> media_common.quotation_subject.quotations_about_this_subject -> \u201cA poet is a verb that blossoms light in gardens of dawn, or sometimes midnight.\u201d\n# Answer:\nPoet", "# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Poet -> fictional_universe.character_occupation.characters_with_this_occupation -> Arthur Rimbaud\n# Answer:\nPoet", "# Reasoning Path:\nWalter Raleigh -> common.topic.notable_types -> Person -> freebase.type_profile.strict_included_types -> Agent\n# Answer:\nPerson"], "ground_truth": ["Poet", "Soldier", "Writer", "Sailor", "Courtier"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7200000000000001, "ans_precission": 0.9, "ans_recall": 0.6, "path_f1": 0.7200000000000001, "path_precision": 0.9, "path_recall": 0.6, "path_ans_f1": 0.7200000000000001, "path_ans_precision": 0.9, "path_ans_recall": 0.6}
{"id": "WebQTest-1245", "prediction": ["# Reasoning Path:\nEuro -> finance.currency.countries_used -> \u00c5land Islands -> location.administrative_division.country -> Finland\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nEuro -> finance.currency.countries_used -> \u00c5land Islands -> location.location.containedby -> Eurasia\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nEuro -> finance.currency.countries_used -> Vatican City -> location.statistical_region.population -> g.11b7tfzdvm\n# Answer:\nVatican City", "# Reasoning Path:\nEuro -> finance.currency.countries_used -> \u00c5land Islands -> location.location.containedby -> Finland\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nEuro -> finance.currency.countries_used -> \u00c5land Islands -> location.location.people_born_here -> Gabriel Fliflet\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nEuro -> finance.currency.countries_used -> Austria -> location.country.administrative_divisions -> Vienna\n# Answer:\nAustria", "# Reasoning Path:\nEuro -> finance.currency.countries_used -> Vatican City -> location.statistical_region.population -> g.11bc8810fn\n# Answer:\nVatican City", "# Reasoning Path:\nEuro -> finance.currency.countries_used -> Vatican City -> travel.travel_destination.tour_operators ->   Adventures by Disney - 5-Night Mediterranean Magic Cruise\n# Answer:\nVatican City", "# Reasoning Path:\nEuro -> common.topic.webpage -> m.04lst2j -> common.webpage.category -> Topic Webpage\n# Answer:\nm.04lst2j", "# Reasoning Path:\nEuro -> finance.currency.countries_used -> \u00c5land Islands -> location.location.containedby -> Nordic countries\n# Answer:\n\u00c5land Islands"], "ground_truth": ["Collectivity of Saint Martin", "Kingdom of the Netherlands", "Monaco", "Belgium", "Mayotte", "Zimbabwe", "Greece", "Varese", "Republic of Kosovo", "Slovakia", "Lithuania", "Saint Barth\u00e9lemy", "France", "Cyprus", "Italy", "Estonia", "Martinique", "Caribbean special municipalities of the Netherlands", "Netherlands", "Germany", "Finland", "Province of Varese", "San Marino", "Andorra", "Malta", "Latvia", "Spain", "\u00c5land Islands", "Austria", "Guadeloupe", "Vatican City", "Luxembourg", "Saint Pierre and Miquelon", "Slovenia", "Republic of Ireland", "Portugal", "Montenegro"], "ans_acc": 0.10810810810810811, "ans_hit": 1, "ans_f1": 0.1487603305785124, "ans_precission": 0.9, "ans_recall": 0.08108108108108109, "path_f1": 0.1487603305785124, "path_precision": 0.9, "path_recall": 0.08108108108108109, "path_ans_f1": 0.19302949061662197, "path_ans_precision": 0.9, "path_ans_recall": 0.10810810810810811}
{"id": "WebQTest-1246", "prediction": ["# Reasoning Path:\nBarbados -> location.country.currency_used -> Barbadian dollar -> common.topic.notable_for -> g.125b468qw\n# Answer:\nBarbadian dollar", "# Reasoning Path:\nBarbados -> location.country.currency_used -> Barbadian dollar -> common.topic.article -> m.05hy7y\n# Answer:\nBarbadian dollar", "# Reasoning Path:\nBarbados -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nBarbados -> location.location.contains -> 3Ws Oval -> common.topic.article -> m.02656zh\n# Answer:\n3Ws Oval", "# Reasoning Path:\nBarbados -> location.country.official_language -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nBarbados -> location.location.contains -> 3Ws Oval -> location.location.geolocation -> m.02_vlf8\n# Answer:\n3Ws Oval", "# Reasoning Path:\nBarbados -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nBarbados -> location.country.official_language -> English Language -> language.human_language.main_country -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nBarbados -> location.location.contains -> Barbados Community College, main campus -> common.topic.notable_for -> g.12599d6nv\n# Answer:\nBarbados Community College, main campus", "# Reasoning Path:\nBarbados -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Bahamas\n# Answer:\nEnglish Language"], "ground_truth": ["Barbadian dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1247", "prediction": ["# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Model -> base.lightweight.profession.similar_professions -> Actor\n# Answer:\nModel", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Model -> common.topic.notable_types -> Profession\n# Answer:\nModel", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Film director -> freebase.type_profile.published -> Published\n# Answer:\nFilm director", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Film director -> freebase.type_hints.included_types -> Person\n# Answer:\nFilm director", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Author -> common.topic.subject_of -> C-SPAN's LCV 2011 U.S. Cities Tour Wraps up Savannah Shoot, Now in Charleston\n# Answer:\nAuthor", "# Reasoning Path:\nAngelina Jolie -> award.award_nominee.award_nominations -> m.010wr37v -> award.award_nomination.nominated_for -> Maleficent\n# Answer:\nm.010wr37v", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Author -> common.topic.subjects -> Jeremy Yablan\n# Answer:\nAuthor", "# Reasoning Path:\nAngelina Jolie -> book.author.book_editions_published -> Notes from My Travels -> common.topic.notable_types -> Book\n# Answer:\nNotes from My Travels", "# Reasoning Path:\nAngelina Jolie -> book.author.book_editions_published -> Notes from My Travels -> book.book_edition.number_of_pages -> m.04tp5bt\n# Answer:\nNotes from My Travels"], "ground_truth": ["Model", "Actor", "Screenwriter", "Voice Actor", "Film director", "Writer", "Author", "Film Producer"], "ans_acc": 0.625, "ans_hit": 1, "ans_f1": 0.48837209302325574, "ans_precission": 0.7, "ans_recall": 0.375, "path_f1": 0.48837209302325574, "path_precision": 0.7, "path_recall": 0.375, "path_ans_f1": 0.7377049180327869, "path_ans_precision": 0.9, "path_ans_recall": 0.625}
{"id": "WebQTest-1248", "prediction": ["# Reasoning Path:\nNineveh -> location.location.containedby -> Nineveh Governorate -> location.location.containedby -> Iraq\n# Answer:\nNineveh Governorate", "# Reasoning Path:\nNineveh -> location.location.containedby -> Iraq -> location.location.containedby -> Arab world\n# Answer:\nIraq", "# Reasoning Path:\nNineveh -> location.location.containedby -> Nineveh Governorate -> location.administrative_division.capital -> Mosul\n# Answer:\nNineveh Governorate", "# Reasoning Path:\nNineveh -> location.location.containedby -> Nineveh Governorate -> location.location.contains -> Baiji\n# Answer:\nNineveh Governorate", "# Reasoning Path:\nNineveh -> location.location.containedby -> Iraq -> location.location.containedby -> Asia\n# Answer:\nIraq", "# Reasoning Path:\nNineveh -> location.location.containedby -> Iraq -> location.country.administrative_divisions -> Nineveh Governorate\n# Answer:\nIraq", "# Reasoning Path:\nNineveh -> location.location.containedby -> Iraq -> location.location.containedby -> Eurasia\n# Answer:\nIraq", "# Reasoning Path:\nNineveh -> common.topic.notable_types -> City/Town/Village -> base.descriptive_names.names.descriptive_name -> m.0105y_05\n# Answer:\nCity/Town/Village", "# Reasoning Path:\nNineveh -> common.topic.notable_types -> City/Town/Village -> freebase.type_profile.strict_included_types -> Location\n# Answer:\nCity/Town/Village", "# Reasoning Path:\nNineveh -> location.location.containedby -> Nineveh Governorate -> location.location.contains -> Bakhdida\n# Answer:\nNineveh Governorate"], "ground_truth": ["Nineveh Governorate", "Iraq"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1249", "prediction": ["# Reasoning Path:\nNew Mexico -> government.governmental_jurisdiction.official_symbols -> m.0hzbf17 -> location.location_symbol_relationship.symbol -> Yucca flower\n# Answer:\nm.0hzbf17", "# Reasoning Path:\nNew Mexico -> government.governmental_jurisdiction.official_symbols -> m.04st6n2 -> location.location_symbol_relationship.symbol -> New Mexico spadefoot toad\n# Answer:\nm.04st6n2", "# Reasoning Path:\nNew Mexico -> government.governmental_jurisdiction.official_symbols -> m.0hzbf17 -> location.location_symbol_relationship.Kind_of_symbol -> State flower\n# Answer:\nm.0hzbf17", "# Reasoning Path:\nNew Mexico -> government.governmental_jurisdiction.official_symbols -> m.04st870 -> location.location_symbol_relationship.symbol -> Greater roadrunner\n# Answer:\nm.04st870", "# Reasoning Path:\nNew Mexico -> government.governmental_jurisdiction.official_symbols -> m.04st6n2 -> location.location_symbol_relationship.Kind_of_symbol -> State Amphibian\n# Answer:\nm.04st6n2", "# Reasoning Path:\nNew Mexico -> book.book_subject.works -> ...And Now Miguel -> book.written_work.subjects -> Men\n# Answer:\n...And Now Miguel", "# Reasoning Path:\nNew Mexico -> government.governmental_jurisdiction.official_symbols -> m.04st870 -> location.location_symbol_relationship.Kind_of_symbol -> State bird\n# Answer:\nm.04st870", "# Reasoning Path:\nNew Mexico -> book.book_subject.works -> A first clay gathering -> book.written_work.author -> Nora Naranjo-Morse\n# Answer:\nA first clay gathering", "# Reasoning Path:\nNew Mexico -> book.book_subject.works -> A first clay gathering -> common.topic.notable_for -> g.1259f0_9n\n# Answer:\nA first clay gathering", "# Reasoning Path:\nNew Mexico -> book.book_subject.works -> Anasazi legends -> book.written_work.subjects -> Americas\n# Answer:\nAnasazi legends"], "ground_truth": ["Yucca flower"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-125", "prediction": ["# Reasoning Path:\nAbraham -> people.person.children -> Ishmael -> people.person.parents -> Hagar\n# Answer:\nIshmael", "# Reasoning Path:\nAbraham -> people.person.children -> Isaac -> people.person.children -> Esau\n# Answer:\nIsaac", "# Reasoning Path:\nAbraham -> people.person.children -> Jokshan -> people.person.sibling_s -> m.048b8rb\n# Answer:\nJokshan", "# Reasoning Path:\nAbraham -> people.person.children -> Ishmael -> common.topic.webpage -> m.04m0j0k\n# Answer:\nIshmael", "# Reasoning Path:\nAbraham -> people.person.children -> Isaac -> people.person.children -> Jacob\n# Answer:\nIsaac", "# Reasoning Path:\nAbraham -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#range -> Person\n# Answer:\nChildren", "# Reasoning Path:\nAbraham -> people.person.children -> Ishmael -> common.topic.image -> Expulsion of Ishmael and His Mother\n# Answer:\nIshmael", "# Reasoning Path:\nAbraham -> people.person.children -> Jokshan -> common.topic.notable_for -> g.125bxhcfr\n# Answer:\nJokshan", "# Reasoning Path:\nAbraham -> people.person.children -> Ishmael -> common.topic.webpage -> m.09ytykz\n# Answer:\nIshmael", "# Reasoning Path:\nAbraham -> people.person.children -> Isaac -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nIsaac"], "ground_truth": ["Zimran", "Jokshan", "Isaac", "Ishbak", "Midian", "Medan", "Ishmael", "Shuah"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.5294117647058825, "ans_precission": 0.9, "ans_recall": 0.375, "path_f1": 0.5294117647058825, "path_precision": 0.9, "path_recall": 0.375, "path_ans_f1": 0.5294117647058825, "path_ans_precision": 0.9, "path_ans_recall": 0.375}
{"id": "WebQTest-1250", "prediction": ["# Reasoning Path:\nNational FFA Organization -> organization.organization.headquarters -> m.0wkkztl -> location.mailing_address.citytown -> Alexandria\n# Answer:\nm.0wkkztl", "# Reasoning Path:\nNational FFA Organization -> organization.organization.headquarters -> m.0wkkztl -> location.mailing_address.country -> United States of America\n# Answer:\nm.0wkkztl", "# Reasoning Path:\nNational FFA Organization -> organization.organization.headquarters -> m.0wkkztl -> location.mailing_address.postal_code -> 22314\n# Answer:\nm.0wkkztl", "# Reasoning Path:\nNational FFA Organization -> organization.non_profit_organization.registered_with -> m.0pqg9g3 -> organization.non_profit_registration.registering_agency -> Internal Revenue Service\n# Answer:\nm.0pqg9g3", "# Reasoning Path:\nNational FFA Organization -> organization.organization.sectors -> Agriculture -> education.field_of_study.students_majoring -> m.01047wk4\n# Answer:\nAgriculture", "# Reasoning Path:\nNational FFA Organization -> organization.non_profit_organization.registered_with -> m.0pqg9g3 -> organization.non_profit_registration.registered_as -> 501(c)(3)\n# Answer:\nm.0pqg9g3", "# Reasoning Path:\nNational FFA Organization -> organization.organization.sectors -> Agriculture -> education.field_of_study.students_majoring -> m.0105sgdw\n# Answer:\nAgriculture", "# Reasoning Path:\nNational FFA Organization -> organization.organization.sectors -> Agriculture -> education.field_of_study.students_majoring -> m.010l0s2j\n# Answer:\nAgriculture", "# Reasoning Path:\nNational FFA Organization -> organization.organization.sectors -> Agriculture -> common.topic.article -> m.0hkr\n# Answer:\nAgriculture", "# Reasoning Path:\nNational FFA Organization -> organization.organization.sectors -> Agriculture -> business.industry.companies -> Illovo Sugar\n# Answer:\nAgriculture"], "ground_truth": ["Alexandria"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1251", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Eschen\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.country.first_level_divisions -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Andreas Bielau\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Gamprin\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.location.partially_contains -> Alps\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Germany -> location.country.languages_spoken -> Bavarian Language\n# Answer:\nGermany", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Liechtenstein\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.country.first_level_divisions -> Eschen\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Andreas Diebitz\n# Answer:\nEast Germany"], "ground_truth": ["Belgium", "Germany", "Luxembourg", "East Germany", "Austria", "Switzerland", "Liechtenstein"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5806451612903225, "ans_precission": 0.9, "ans_recall": 0.42857142857142855, "path_f1": 0.4210526315789473, "path_precision": 0.8, "path_recall": 0.2857142857142857, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1252", "prediction": ["# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_nominated_work.award_nominations -> m.09k3q0p -> award.award_nomination.award_nominee -> Miranda Otto\n# Answer:\nm.09k3q0p", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_nominated_work.award_nominations -> m.0b4d5rz -> award.award_nomination.award_nominee -> Miranda Otto\n# Answer:\nm.0b4d5rz", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_nominated_work.award_nominations -> m.09k3q0p -> award.award_nomination.award_nominee -> Andy Serkis\n# Answer:\nm.09k3q0p", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_nominated_work.award_nominations -> m.09k3q0p -> award.award_nomination.ceremony -> 9th Critics' Choice Awards\n# Answer:\nm.09k3q0p", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_winning_work.awards_won -> m.09k3pgy -> award.award_honor.award_winner -> Miranda Otto\n# Answer:\nm.09k3pgy", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_nominated_work.award_nominations -> m.09k3q0p -> award.award_nomination.award_nominee -> Bernard Hill\n# Answer:\nm.09k3q0p", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_nominated_work.award_nominations -> m.09k3q0p -> award.award_nomination.award -> Critics' Choice Movie Award for Best Acting Ensemble\n# Answer:\nm.09k3q0p", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_nominated_work.award_nominations -> m.05bkygq -> award.award_nomination.award -> Academy Award for Best Director\n# Answer:\nm.05bkygq", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_nominated_work.award_nominations -> m.0b4d5rz -> award.award_nomination.award_nominee -> Andy Serkis\n# Answer:\nm.0b4d5rz", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_nominated_work.award_nominations -> m.0b4d5rz -> award.award_nomination.ceremony -> 10th Screen Actors Guild Awards\n# Answer:\nm.0b4d5rz"], "ground_truth": ["Miranda Otto"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4, "path_precision": 0.3, "path_recall": 0.6, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1254", "prediction": ["# Reasoning Path:\n2012 Masters Tournament -> time.event.locations -> Augusta National Golf Course -> location.location.events -> 2007 Masters Tournament\n# Answer:\nAugusta National Golf Course", "# Reasoning Path:\n2012 Masters Tournament -> time.event.locations -> Augusta National Golf Course -> common.topic.notable_for -> g.12556r5p0\n# Answer:\nAugusta National Golf Course", "# Reasoning Path:\n2012 Masters Tournament -> time.event.locations -> Augusta National Golf Course -> location.location.events -> 2008 Masters Tournament\n# Answer:\nAugusta National Golf Course", "# Reasoning Path:\n2012 Masters Tournament -> time.event.locations -> Augusta National Golf Course -> award.ranked_item.appears_in_ranked_lists -> m.04tk4kl\n# Answer:\nAugusta National Golf Course", "# Reasoning Path:\n2012 Masters Tournament -> time.event.locations -> Augusta National Golf Course -> location.location.events -> 2009 Masters Tournament\n# Answer:\nAugusta National Golf Course", "# Reasoning Path:\n2012 Masters Tournament -> time.event.locations -> Augusta National Golf Course -> award.ranked_item.appears_in_ranked_lists -> m.04v6l04\n# Answer:\nAugusta National Golf Course", "# Reasoning Path:\n2012 Masters Tournament -> freebase.valuenotation.is_reviewed -> End date -> type.property.schema -> Event\n# Answer:\nEnd date", "# Reasoning Path:\n2012 Masters Tournament -> freebase.valuenotation.is_reviewed -> End date -> rdf-schema#range -> Date/Time\n# Answer:\nEnd date", "# Reasoning Path:\n2012 Masters Tournament -> freebase.valuenotation.is_reviewed -> Location(s) -> owl#inverseOf -> Events\n# Answer:\nLocation(s)", "# Reasoning Path:\n2012 Masters Tournament -> common.topic.notable_types -> Competition -> freebase.type_hints.included_types -> Event\n# Answer:\nCompetition"], "ground_truth": ["Augusta National Golf Course"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1255", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04d7nvm\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A Time to Mend\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjf_c\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjfp1\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Anthony Casso\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.person.gender -> Female -> medicine.risk_factor.diseases -> Ovarian cancer\n# Answer:\nFemale", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A season in hell\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> common.topic.webpage -> m.04tnn4f\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> Anticancer\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Bryan Murray\n# Answer:\nCancer"], "ground_truth": ["Cancer", "Cervical cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1256", "prediction": ["# Reasoning Path:\nTerrell Owens -> american_football.football_player.rushing -> m.07t77sv -> american_football.player_rushing_statistics.team -> Buffalo Bills\n# Answer:\nm.07t77sv", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.receiving -> m.09rltd7 -> american_football.player_receiving_statistics.team -> Buffalo Bills\n# Answer:\nm.09rltd7", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.rushing -> m.0791bhh -> american_football.player_rushing_statistics.team -> Dallas Cowboys\n# Answer:\nm.0791bhh", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.receiving -> m.07vz1kc -> american_football.player_receiving_statistics.team -> Buffalo Bills\n# Answer:\nm.07vz1kc", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.rushing -> m.07t77sv -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\nm.07t77sv", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.rushing -> m.07mmd6t -> american_football.player_rushing_statistics.team -> Buffalo Bills\n# Answer:\nm.07mmd6t", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.receiving -> m.09gr3sg -> american_football.player_receiving_statistics.team -> Buffalo Bills\n# Answer:\nm.09gr3sg", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.receiving -> m.09rltd7 -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\nm.09rltd7", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.rushing -> m.0791bhh -> american_football.player_rushing_statistics.season -> 2008 NFL season\n# Answer:\nm.0791bhh", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.receiving -> m.07vz1kc -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\nm.07vz1kc"], "ground_truth": ["Texas Revolution", "Cincinnati Bengals", "Buffalo Bills"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15873015873015875, "path_precision": 0.5, "path_recall": 0.09433962264150944, "path_ans_f1": 0.4, "path_ans_precision": 0.5, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1257", "prediction": ["# Reasoning Path:\nDenmark -> location.location.containedby -> South Carolina -> location.location.containedby -> Southeastern United States\n# Answer:\nSouth Carolina", "# Reasoning Path:\nDenmark -> location.location.containedby -> Bamberg County -> location.location.containedby -> South Carolina\n# Answer:\nBamberg County", "# Reasoning Path:\nDenmark -> location.location.containedby -> South Carolina -> location.location.containedby -> Contiguous United States\n# Answer:\nSouth Carolina", "# Reasoning Path:\nDenmark -> location.location.containedby -> South Carolina -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nSouth Carolina", "# Reasoning Path:\nDenmark -> location.hud_county_place.county -> Bamberg County -> location.location.containedby -> South Carolina\n# Answer:\nBamberg County", "# Reasoning Path:\nDenmark -> location.location.containedby -> South Carolina -> base.aareas.schema.administrative_area.administrative_children -> Orangeburg County\n# Answer:\nSouth Carolina", "# Reasoning Path:\nDenmark -> location.location.containedby -> South Carolina -> location.location.containedby -> South Atlantic States\n# Answer:\nSouth Carolina", "# Reasoning Path:\nDenmark -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nDenmark -> location.citytown.postal_codes -> 29042 -> location.location.containedby -> Bamberg County\n# Answer:\n29042", "# Reasoning Path:\nDenmark -> location.location.containedby -> South Carolina -> base.aareas.schema.administrative_area.administrative_children -> Abbeville County\n# Answer:\nSouth Carolina"], "ground_truth": ["South Carolina", "Bamberg County", "United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1259", "prediction": ["# Reasoning Path:\nErnest Hemingway -> influence.influence_node.influenced -> Andre Dubus -> influence.influence_node.influenced_by -> Anton Chekhov\n# Answer:\nAndre Dubus", "# Reasoning Path:\nErnest Hemingway -> influence.influence_node.influenced -> Andre Dubus -> people.person.gender -> Male\n# Answer:\nAndre Dubus", "# Reasoning Path:\nErnest Hemingway -> influence.influence_node.influenced -> Ann Beattie -> influence.influence_node.influenced -> Tao Lin\n# Answer:\nAnn Beattie", "# Reasoning Path:\nErnest Hemingway -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Cardiovascular disease\n# Answer:\nMale", "# Reasoning Path:\nErnest Hemingway -> influence.influence_node.influenced -> Andre Dubus -> people.person.profession -> Author\n# Answer:\nAndre Dubus", "# Reasoning Path:\nErnest Hemingway -> common.topic.webpage -> m.09wc371 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09wc371", "# Reasoning Path:\nErnest Hemingway -> influence.influence_node.influenced -> Andre Dubus -> influence.influence_node.influenced_by -> John Cheever\n# Answer:\nAndre Dubus", "# Reasoning Path:\nErnest Hemingway -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Emphysema\n# Answer:\nMale", "# Reasoning Path:\nErnest Hemingway -> influence.influence_node.influenced -> Andre Dubus -> influence.influence_node.influenced_by -> Richard Yates\n# Answer:\nAndre Dubus", "# Reasoning Path:\nErnest Hemingway -> influence.influence_node.influenced -> Anne Rice -> influence.influence_node.influenced_by -> Albert Camus\n# Answer:\nAnne Rice"], "ground_truth": ["Three Stories and Ten Poems", "Fable", "Fifth Column and Four Stories of the Spanish Civil War (Fifth Column & 4 Stories Hre)", "A Farewell To Arms", "Reportagen 1920 - 1924", "On Writing", "The Cambridge Edition of the Letters of Ernest Hemingway", "The sun also rises", "The Dangerous Summer", "Valentine", "Bi xatire\u0302 s\u0131\u0302lehan", "The essential Hemingway", "Three novels: The sun also rises; A farewell to arms; The old man and the sea", "La quinta columna", "Los Asesinos", "Collected poems", "The Gambler, the Nun, and the Radio", "The torrents of spring", "On being shot again", "The only thing that counts", "The Undefeated", "Der alte Mann und das Meer, und andere Meisterwerke", "The Hemingway reader", "After the Storm", "88 Poems", "The Old Man and the Sea (Old Man & the Sea Illus Gift Ed C)", "Best Of Bad Hemingway: Vol 1", "Sobranie sochineni\u012d v chetyrekh tomakh", "The sun also rises.", "Death in the afternoon", "Der al\u1e6der un der yam", "The Complete Short Stories of Ernest Hemingway: The Finca Vig\u00eda Edition", "Voor wien de klok luidt", "Hayat Francis Macumber", "A Day's Wait", "The Spanish War", "The president vanquishes", "The Old Man and the Sea (MacMillan Literature Series, Signature Edition)", "Defense of dirty words", "Shootism versus sport", "50000 dollars", "A Farewell to Arms", "A farewell to arms", "On the blue water", "Complete poems", "Three stories", "The enduring Hemingway", "At have og ikke have", "Lao ren yu hai", "In Our Time", "The denunciation", "In Harry's Bar in Venice and More", "The old man and the sea =", "The fifth column and four stories of the Spanish Civil War", "Notes on dangerous game", "Old Man and the Sea (New Windmill)", "A Moveable Feast (Moveable Feast Srs)", "Genio after Josie", "The malady of power", "A Clean, Well-Lighted Place", "Relatos Ineditos", "Old Man and the Sea (Special Student)", "By-Line", "In Another Country", "Under Kilimanjaro", "Bullfighting, sport & industry", "The garden of Eden.", "Green Hills of Africa", "Vier Stories aus dem spanischen B\u00fcrgerkrieg", "Winner take nothing.", "Ernest Hemingway's Apprenticeship", "Hemingway on Fishing", "Green Hills of Africa (Hudson River editions)", "For Whom the Bell Tolls (Vintage Classics)", "De vye zanmi", "The faithful bull", "Across the river and into the trees", "Across the River and into the Trees (Arrow Classic)", "Ernest Hemingway: The Collected Stories", "The fifth column, and four stories of the Spanish Civil War.", "Hemingway on war", "Bullfighting", "The garden of Eden", "Green Hills of Africa (Vintage Classics)", "For Whom the Bell Tolls", "For Whom the Bell Tolls (Audio Library Classics)", "Across The River And Into The Trees", "Across the river and into the trees.", "El Buen Leon", "To Have and Have Not (To Have & Have Not Hre)", "Un corresponsal llamado Hemingway", "Le chaud et le froid", "The colected poems of Ernest Hemingway", "The old man and the sea. (Lernmaterialien)", "The Old Man and the Sea (Old Man & the Sea Tr)", "A divine gesture", "By-line, Ernest Hemingway", "Men Without Women (Arrow Classic)", "The Snows of Kilimanjaro", "Hemingway on Hunting (On)", "Men Without Women", "A moveable feast", "A Farewell to Arms (Vintage Classics)", "Soldier's Home", "Men without women", "Now I Lay Me", "The collected poems of Ernest Hemingway", "g.1ym_l5zt2", "A Moveable Feast (Scribner Classic)", "Oeuvres Romanesques Vol. 1", "Die Wahrheit im Morgenlicht. Eine afrikanische Safari.", "The Old Man and the Sea (Vintage Classics)", "Give us a prescription, Doctor", "E.H, apprenti reporter", "In Our Time (In Our Time Hre)", "By-Line Ernest Hemingway", "Nieves del Kilimanjaro, Las", "The circus", "The Old Man and the Sea. (Lernmaterialien)", "Winner Take Nothing (Scribner Classic)", "A Simple Enquiry", "Old Man and the Sea", "Islands in the Stream", "The Sun Also Rises", "Ernest Hemingway Selected Letters 1917\u20131961", "The Killers", "Death in the Afternoon", "Million dollar fright", "The snows of Kilimanjaro", "The Revolutionist", "To have and have not.", "The Old Man and the Sea (A Scribner Classic)", "Zhan di zhong sheng", "Storie della guerra de Spagna", "Night before battle", "On Paris", "Winner take nothing", "Conversations with Ernest Hemingway", "The good lion", "Proshchai\u0306 oruzhie!", "Major Works of Ernest Hemingway", "The Letters of Ernest Hemingway: Volume 1, 1907-1922", "Die sch\u00f6nsten Geschichten Afrikas", "Across the River and Into the Trees", "Sun Also Rises", "For Whom the Bell Tolls (War Promo)", "A Farewell to Arms (Scribner Classics)", "Sun Also Rises (Sun Also Rises Tr)", "Wada'an Lilseelah", "The Capital of the World", "Die Stories", "Hemingway", "The Torrents of Spring", "True at first light", "He who gets slap happy", "a.d. southern style", "The snows of Kilimanjaro and other stories", "g.11b6s8f7gb", "The Old Man And The Sea", "Complete Poems", "Dateline: Toronto", "A Farewell to Arms (Farewell to Arms Tr)", "Winner Take Nothing", "a.d. in Africa", "The dangerous summer", "True At First Light", "To Have and Have Not (To Have & Have Not Srs)", "Across the River and into the Trees", "The Garden of Eden", "The sights of Whitehead Street", "Sun Also Rises (Library Reprint Editions)", "The old man and the sea", "Death in the afternoon.", "Ernest Hemingway: Cub Reporter", "To have and have not", "There she breaches!", "A folyo n at, a fa k koze", "Old Man and the Sea/ (Cassette)", "Farewell to Arms (A Scribner Classic)", "Hemingway and the mechanism of fame", "Ernest Hemingway, selected letters, 1917-1961", "The soul of Spain with Mc. Almon and Bird the publishers", "The Old Man and the Sea (York Notes)", "The tradesman's return", "Green hills of Africa", "Up in Michigan", "The Fifth Column and Four Stories of the Spanish Civil War", "The short stories of Ernest Hemingway", "Dear Papa, dear Hotch", "Wings always over Africa", "The TORRENTS OF SPRING", "Monologue to the maestro", "L'Etrange contr\u00e9e", "The Butterfly and the Tank...fiction", "Green hills of Africa.", "Las Nieves del Kilimanjaro", "Marlin off the Morro", "Obras Completas:Por qui\u00e9n doblan las campanas, El viejo y el mar", "The Christmas gift", "ACROSS THE RIVER AND INTO THE TREES", "True at First Light", "The friend of Spain", "The wild years", "Che Ti Dice La Patria?", "The Short Happy Life of Francis Macomber", "En ligne", "The Sun Also Rises (Archive of Literary Documents)", "Hemingway on hunting", "For whom the bell tolls", "The great blue river", "Men at war", "Big Two-Hearted River", "Indian Camp", "To Have and Have Not", "The Fifth Column and the First Forty-Nine Stories", "The End of Something", "Klokkene ringer for deg", "The Old Man and the Sea", "Death in the Afternoon (Death in the Afternoon Hre)", "Ernest Hemingway in high school", "El Toro Fiel / the Faithful Bull", "The old man and the sea.", "A Paris letter", "A Moveable Feast", "Neues vom Festland. Stories", "Green Hills of Africa (Scribner Classic)", "The Spanish earth", "The Sun Also Rises (A Scribner classic)", "Hills Like White Elephants", "Sailfish off Mombasa", "The Nick Adams Stories", "quarantanove racconti", "Cat in the Rain", "Marlin off Cuba", "The Old Man and The Sea (Annual Review of the Institute for Information Studies)", "The Battler", "In our time"], "ans_acc": 0.004219409282700422, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.008403361344537815, "path_ans_precision": 1.0, "path_ans_recall": 0.004219409282700422}
{"id": "WebQTest-126", "prediction": ["# Reasoning Path:\nJana Gana Mana -> music.composition.composer -> Rabindranath Tagore -> people.person.profession -> Composer\n# Answer:\nRabindranath Tagore", "# Reasoning Path:\nJana Gana Mana -> music.composition.lyricist -> Rabindranath Tagore -> people.person.profession -> Lyricist\n# Answer:\nRabindranath Tagore", "# Reasoning Path:\nJana Gana Mana -> music.composition.composer -> Rabindranath Tagore -> people.person.profession -> Lyricist\n# Answer:\nRabindranath Tagore", "# Reasoning Path:\nJana Gana Mana -> music.composition.lyricist -> Rabindranath Tagore -> people.person.profession -> Composer\n# Answer:\nRabindranath Tagore", "# Reasoning Path:\nJana Gana Mana -> music.composition.composer -> Rabindranath Tagore -> people.person.profession -> Artist\n# Answer:\nRabindranath Tagore", "# Reasoning Path:\nJana Gana Mana -> music.composition.composer -> Rabindranath Tagore -> film.film_story_contributor.film_story_credits -> An Obsolete Altar\n# Answer:\nRabindranath Tagore", "# Reasoning Path:\nJana Gana Mana -> music.composition.composer -> Ram Singh Thakur -> common.topic.article -> m.02vx725\n# Answer:\nRam Singh Thakur", "# Reasoning Path:\nJana Gana Mana -> music.composition.composer -> Rabindranath Tagore -> people.person.quotations -> Clouds come floating into my life, no longer to carry rain or usher storm, but to add color to my sunset sky.\n# Answer:\nRabindranath Tagore", "# Reasoning Path:\nJana Gana Mana -> music.composition.lyricist -> Rabindranath Tagore -> people.person.profession -> Artist\n# Answer:\nRabindranath Tagore", "# Reasoning Path:\nJana Gana Mana -> music.composition.composer -> Rabindranath Tagore -> film.film_story_contributor.film_story_credits -> Char Adhyay\n# Answer:\nRabindranath Tagore"], "ground_truth": ["Ram Singh Thakur", "Rabindranath Tagore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1260", "prediction": ["# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.article -> m.0642vqz\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> sports.sports_championship_event.season -> 2000 NFL season\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> sports.sports_championship_event.season -> 2012 NFL season\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> 2013 AFC Championship Game -> time.event.instance_of_recurring_event -> AFC Championship Game\n# Answer:\n2013 AFC Championship Game", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> common.topic.notable_types -> Super bowl\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> film.film_subject.films -> The Band That Wouldn't Die -> film.film.music -> Marcello Zavros\n# Answer:\nThe Band That Wouldn't Die", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> time.event.locations -> Raymond James Stadium\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> 2013 AFC Championship Game -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n2013 AFC Championship Game", "# Reasoning Path:\nBaltimore Ravens -> film.film_subject.films -> The Band That Wouldn't Die -> common.topic.image -> Baltimore Ravens Marching Band\n# Answer:\nThe Band That Wouldn't Die"], "ground_truth": ["Super Bowl XXXV"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1261", "prediction": ["# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Flanders -> location.location.containedby -> Belgium\n# Answer:\nFlanders", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Passendale -> location.location.geolocation -> m.02_nfnm\n# Answer:\nPassendale", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Flanders -> military.military_post.people_posted_here -> m.011nsjvz\n# Answer:\nFlanders", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Flanders -> location.location.contains -> Aalst\n# Answer:\nFlanders", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Flanders -> location.location.contains -> Berlare\n# Answer:\nFlanders", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Passendale -> location.location.events -> First Battle of Passchendaele\n# Answer:\nPassendale", "# Reasoning Path:\nBattle of Passchendaele -> military.military_conflict.commanders -> m.04kxm8s -> military.military_command.military_commander -> Herbert Plumer, 1st Viscount Plumer\n# Answer:\nm.04kxm8s", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Flanders -> location.location.contains -> Burst\n# Answer:\nFlanders", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Passendale -> common.topic.image -> Passendale - Sint-Audomaruskerk 1\n# Answer:\nPassendale", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Passendale -> location.location.events -> Second Battle of Passchendaele\n# Answer:\nPassendale"], "ground_truth": ["Flanders", "Passendale"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1262", "prediction": ["# Reasoning Path:\nOhio -> location.location.containedby -> Midwestern United States -> location.location.containedby -> United States of America\n# Answer:\nMidwestern United States", "# Reasoning Path:\nOhio -> location.location.containedby -> East North Central States -> location.location.containedby -> Midwestern United States\n# Answer:\nEast North Central States", "# Reasoning Path:\nOhio -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nOhio -> location.location.containedby -> Midwestern United States -> book.book_subject.works -> Caught in the Middle: America's Heartland in the Age of Globalism\n# Answer:\nMidwestern United States", "# Reasoning Path:\nOhio -> location.location.containedby -> Contiguous United States -> location.location.containedby -> Continental United States\n# Answer:\nContiguous United States", "# Reasoning Path:\nOhio -> location.location.containedby -> East North Central States -> location.location.containedby -> Contiguous United States\n# Answer:\nEast North Central States", "# Reasoning Path:\nOhio -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.location.containedby -> Americas\n# Answer:\nUnited States of America", "# Reasoning Path:\nOhio -> location.location.containedby -> Midwestern United States -> location.location.contains -> East North Central States\n# Answer:\nMidwestern United States", "# Reasoning Path:\nOhio -> location.location.containedby -> East North Central States -> common.topic.notable_for -> g.1257k6kxr\n# Answer:\nEast North Central States", "# Reasoning Path:\nOhio -> location.location.containedby -> Contiguous United States -> location.location.containedby -> North America\n# Answer:\nContiguous United States"], "ground_truth": ["East North Central States", "Midwestern United States"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1263", "prediction": ["# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nm.010dzr92", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzrlj -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nm.010dzrlj", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010p4jhc -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nm.010p4jhc", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.match -> 2014 Real Madrid CF vs. CA Osasuna football match\n# Answer:\nm.010dzr92", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w8w78v -> soccer.football_player_stats.team -> Real Madrid C.F.\n# Answer:\nm.0w8w78v", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzrlj -> soccer.football_goal.match -> 2014 Real Madrid CF vs. CA Osasuna football match\n# Answer:\nm.010dzrlj", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010p4jhc -> soccer.football_goal.match -> 2014 UEFA Champions League Final\n# Answer:\nm.010p4jhc", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w8w78v -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0w8w78v", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w9gdc1 -> soccer.football_player_stats.team -> Manchester United F.C.\n# Answer:\nm.0w9gdc1", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w8w7f0 -> soccer.football_player_stats.team -> Portugal national football team\n# Answer:\nm.0w8w7f0"], "ground_truth": ["Real Madrid C.F.", "Portugal national football team"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3333333333333333, "path_precision": 0.5, "path_recall": 0.25, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1264", "prediction": ["# Reasoning Path:\nMichigan -> location.location.time_zones -> UTC\u221205:00 -> common.topic.notable_types -> Time Zone\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Eastern Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Eastern Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nMichigan -> government.governmental_jurisdiction.governing_officials -> m.010f1h5h -> government.government_position_held.basic_title -> Attorney general\n# Answer:\nm.010f1h5h", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Eastern Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nMichigan -> military.military_unit_place_of_origin.military_units -> 102nd Regiment United States Colored Troops -> common.topic.notable_types -> Military unit\n# Answer:\n102nd Regiment United States Colored Troops"], "ground_truth": ["UTC\u221205:00", "Eastern Time Zone", "Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1265", "prediction": ["# Reasoning Path:\nJohn Tyler -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Representative\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Tyler -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> Ambassador\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Tyler -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> Diplomat\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Tyler -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdn_\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Tyler -> people.person.profession -> Lawyer -> base.descriptive_names.names.descriptive_name -> m.0101pjk9\n# Answer:\nLawyer", "# Reasoning Path:\nJohn Tyler -> people.person.profession -> Politician -> people.profession.specializations -> AIDS Policy Specialist\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Tyler -> people.person.profession -> Lawyer -> common.topic.subject_of -> Lawyer SEO Services\n# Answer:\nLawyer", "# Reasoning Path:\nJohn Tyler -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdq3\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Tyler -> people.person.profession -> Lawyer -> base.descriptive_names.names.descriptive_name -> m.0101pk4l\n# Answer:\nLawyer", "# Reasoning Path:\nJohn Tyler -> people.person.profession -> Lawyer -> people.profession.specializations -> Attorney at law\n# Answer:\nLawyer"], "ground_truth": ["US President"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1266", "prediction": ["# Reasoning Path:\nSergio Romo -> people.person.profession -> Baseball player -> common.topic.notable_types -> Profession\n# Answer:\nBaseball player", "# Reasoning Path:\nSergio Romo -> people.person.profession -> Baseball player -> base.ontologies.ontology_instance.equivalent_instances -> m.0gy7hfc\n# Answer:\nBaseball player", "# Reasoning Path:\nSergio Romo -> people.person.profession -> Baseball player -> book.book_subject.works -> Highpockets\n# Answer:\nBaseball player", "# Reasoning Path:\nSergio Romo -> common.topic.notable_types -> Baseball Player -> type.type.properties -> Baseball Almanac ID\n# Answer:\nBaseball Player", "# Reasoning Path:\nSergio Romo -> common.topic.notable_types -> Baseball Player -> common.topic.article -> m.0d9v52v\n# Answer:\nBaseball Player", "# Reasoning Path:\nSergio Romo -> people.person.profession -> Baseball player -> base.ontologies.ontology_instance.equivalent_instances -> m.0gy7jbg\n# Answer:\nBaseball player", "# Reasoning Path:\nSergio Romo -> common.topic.notable_types -> Baseball Player -> freebase.type_profile.published -> Published\n# Answer:\nBaseball Player", "# Reasoning Path:\nSergio Romo -> people.person.profession -> Baseball player -> book.book_subject.works -> The Glory of Their Times: The Story of the Early Days of Baseball Told by the Men Who Played It\n# Answer:\nBaseball player", "# Reasoning Path:\nSergio Romo -> common.topic.notable_types -> Baseball Player -> type.type.properties -> Bats\n# Answer:\nBaseball Player", "# Reasoning Path:\nSergio Romo -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Alabama\n# Answer:\nUnited States of America"], "ground_truth": ["Baseball player"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1267", "prediction": ["# Reasoning Path:\nJamaica -> location.country.currency_used -> Jamaican dollar -> common.topic.notable_types -> Currency\n# Answer:\nJamaican dollar", "# Reasoning Path:\nJamaica -> location.location.containedby -> Americas -> location.location.contains -> Bahamas\n# Answer:\nAmericas", "# Reasoning Path:\nJamaica -> location.location.containedby -> Americas -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nAmericas", "# Reasoning Path:\nJamaica -> location.location.containedby -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nJamaica -> location.location.containedby -> Americas -> location.location.contains -> Belize\n# Answer:\nAmericas", "# Reasoning Path:\nJamaica -> location.location.containedby -> Greater Antilles -> location.location.contains -> Cayman Islands\n# Answer:\nGreater Antilles", "# Reasoning Path:\nJamaica -> location.location.containedby -> Americas -> location.location.contains -> Bermuda\n# Answer:\nAmericas", "# Reasoning Path:\nJamaica -> location.location.containedby -> Greater Antilles -> location.location.time_zones -> UTC\u221205:00\n# Answer:\nGreater Antilles", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\n1812 Louisiana hurricane"], "ground_truth": ["Jamaican dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1268", "prediction": ["# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.location.containedby -> Asia\n# Answer:\nMaldives", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.location.containedby -> Eurasia\n# Answer:\nMaldives", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.statistical_region.agriculture_as_percent_of_gdp -> g.12tb6flp5\n# Answer:\nMaldives", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_burial -> Pleasure Point, Santa Cruz, California -> location.location.containedby -> Santa Cruz\n# Answer:\nPleasure Point, Santa Cruz, California", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.location.containedby -> Indian subcontinent\n# Answer:\nMaldives", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.statistical_region.gni_in_ppp_dollars -> g.11b60vv5wn\n# Answer:\nMaldives", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.statistical_region.agriculture_as_percent_of_gdp -> g.1hhc37phc\n# Answer:\nMaldives", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.statistical_region.agriculture_as_percent_of_gdp -> g.1hhc3__tq\n# Answer:\nMaldives", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.statistical_region.gni_in_ppp_dollars -> g.1245__b0q\n# Answer:\nMaldives", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_burial -> Pleasure Point, Santa Cruz, California -> common.topic.image -> Rancho2\n# Answer:\nPleasure Point, Santa Cruz, California"], "ground_truth": ["Maldives"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1269", "prediction": ["# Reasoning Path:\nBoston Tea Party -> time.event.locations -> Boston Harbor -> location.location.containedby -> Massachusetts\n# Answer:\nBoston Harbor", "# Reasoning Path:\nBoston Tea Party -> common.topic.notable_for -> g.12574v___\n# Answer:\ng.12574v___", "# Reasoning Path:\nBoston Tea Party -> time.event.locations -> Boston Harbor -> location.location.events -> Capture of USS Chesapeake\n# Answer:\nBoston Harbor", "# Reasoning Path:\nBoston Tea Party -> time.event.locations -> Boston Harbor -> geography.body_of_water.islands -> Bumpkin Island\n# Answer:\nBoston Harbor", "# Reasoning Path:\nBoston Tea Party -> time.event.locations -> Boston Harbor -> geography.body_of_water.islands -> Button Island\n# Answer:\nBoston Harbor", "# Reasoning Path:\nBoston Tea Party -> time.event.locations -> Boston Harbor -> geography.body_of_water.islands -> Calf Island\n# Answer:\nBoston Harbor", "# Reasoning Path:\nBoston Tea Party -> common.topic.webpage -> m.09y2fn0 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09y2fn0", "# Reasoning Path:\nBoston Tea Party -> common.topic.webpage -> m.09y2fn0 -> common.webpage.resource -> What I learned about the '70s from 'Swingtown'\n# Answer:\nm.09y2fn0"], "ground_truth": ["Boston Harbor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-127", "prediction": ["# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.parent_disease -> Cancer\n# Answer:\nOvarian cancer", "# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.parent_disease -> Endocrine disease\n# Answer:\nOvarian cancer", "# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.survival_rates -> m.04nvdb_\n# Answer:\nOvarian cancer", "# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.parent_disease -> Female urologic disease\n# Answer:\nOvarian cancer", "# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nOvarian cancer", "# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.survival_rates -> m.04nvdc7\n# Answer:\nOvarian cancer", "# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.survival_rates -> m.04nvdc_\n# Answer:\nOvarian cancer", "# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.treatments -> Liposomal doxorubicin\n# Answer:\nOvarian cancer", "# Reasoning Path:\nGilda Radner -> music.artist.album -> Live From New York -> common.topic.notable_for -> g.12571zhk0\n# Answer:\nLive From New York", "# Reasoning Path:\nGilda Radner -> film.actor.film -> m.02vbgr3 -> film.performance.film -> Gilda Live\n# Answer:\nm.02vbgr3"], "ground_truth": ["Ovarian cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1270", "prediction": ["# Reasoning Path:\nLindsay Lohan -> base.popstra.celebrity.substance_abuse -> m.064wlqm -> base.popstra.substance_abuse.substance -> Cocaine\n# Answer:\nm.064wlqm", "# Reasoning Path:\nLindsay Lohan -> base.popstra.celebrity.substance_abuse -> m.063fx3q -> base.popstra.substance_abuse.substance -> Alcoholic beverage\n# Answer:\nm.063fx3q", "# Reasoning Path:\nLindsay Lohan -> celebrities.celebrity.substance_abuse_problems -> m.04hykz6 -> celebrities.substance_abuse_problem.substance -> Alcoholic beverage\n# Answer:\nm.04hykz6", "# Reasoning Path:\nLindsay Lohan -> base.popstra.celebrity.friendship -> m.063gnw4 -> base.popstra.friendship.participant -> Britney Spears\n# Answer:\nm.063gnw4", "# Reasoning Path:\nLindsay Lohan -> base.popstra.celebrity.friendship -> m.063hns8 -> base.popstra.friendship.participant -> Ashlee Simpson\n# Answer:\nm.063hns8", "# Reasoning Path:\nLindsay Lohan -> base.popstra.celebrity.friendship -> m.064wk92 -> base.popstra.friendship.participant -> Jonathan Bennett\n# Answer:\nm.064wk92"], "ground_truth": ["Alcoholic beverage", "Cocaine"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.6, "path_precision": 0.5, "path_recall": 0.75, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1271", "prediction": ["# Reasoning Path:\nCarolina Panthers -> sports.sports_team.championships -> 2004 NFC Championship Game -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n2004 NFC Championship Game", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.championships -> 2004 NFC Championship Game -> sports.sports_championship_event.championship -> The NFC Championship Game\n# Answer:\n2004 NFC Championship Game", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.away_games -> Carolina Panthers at Arizona Cardinals, 2009-11-01 -> american_football.football_game.receiving -> m.07z2llv\n# Answer:\nCarolina Panthers at Arizona Cardinals, 2009-11-01", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.season_record -> m.075fhm9 -> sports.sports_team_season_record.season -> 2008 NFL season\n# Answer:\nm.075fhm9", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.season_record -> m.075fhmk -> sports.sports_team_season_record.season -> 2007 NFL season\n# Answer:\nm.075fhmk", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.away_games -> Carolina Panthers at Arizona Cardinals, 2009-11-01 -> american_football.football_game.receiving -> m.07z2lm5\n# Answer:\nCarolina Panthers at Arizona Cardinals, 2009-11-01", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.away_games -> Carolina Panthers at Arizona Cardinals, 2009-11-01 -> american_football.football_game.rushing -> m.07z2lkt\n# Answer:\nCarolina Panthers at Arizona Cardinals, 2009-11-01", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.away_games -> Carolina Panthers at Atlanta Falcons, 2008-11-23 -> american_football.football_game.receiving -> m.079454s\n# Answer:\nCarolina Panthers at Atlanta Falcons, 2008-11-23", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.season_record -> m.075fhmt -> sports.sports_team_season_record.season -> 2006 NFL season\n# Answer:\nm.075fhmt", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.away_games -> Carolina Panthers at Arizona Cardinals, 2009-11-01 -> american_football.football_game.receiving -> m.07z2lmj\n# Answer:\nCarolina Panthers at Arizona Cardinals, 2009-11-01"], "ground_truth": ["Super Bowl XXXVIII"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1272", "prediction": ["# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Myanmar -> location.country.official_language -> Burmese Language\n# Answer:\nMyanmar", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Myanmar -> location.location.events -> 2009 Kokang incident\n# Answer:\nMyanmar", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Ayeyarwady Region -> location.location.containedby -> Myanmar\n# Answer:\nAyeyarwady Region", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Myanmar -> location.location.events -> Battle of Bilin River\n# Answer:\nMyanmar", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Myanmar -> location.location.events -> Battle of Meiktila and Mandalay\n# Answer:\nMyanmar", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Myanmar -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_mygc\n# Answer:\nMyanmar", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Ayeyarwady Region -> location.administrative_division.country -> Myanmar\n# Answer:\nAyeyarwady Region", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.category -> Category 4 Severe Tropical Cyclone (BOM) -> meteorology.tropical_cyclone_category.Beaufort_scale -> Beaufort force 12\n# Answer:\nCategory 4 Severe Tropical Cyclone (BOM)", "# Reasoning Path:\nCyclone Nargis -> common.topic.article -> m.0479wmw\n# Answer:\nm.0479wmw", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Myanmar -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_ryq0\n# Answer:\nMyanmar"], "ground_truth": ["2008-04-27"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1274", "prediction": ["# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0gy -> government.government_position_held.office_holder -> Charles Allen Culberson\n# Answer:\nm.04ks0gy", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0k1 -> government.government_position_held.office_holder -> Price Daniel\n# Answer:\nm.04ks0k1", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0f5 -> government.government_position_held.office_holder -> Sam Houston\n# Answer:\nm.04ks0f5", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0gy -> government.government_position_held.office_position_or_title -> Governor of Texas\n# Answer:\nm.04ks0gy", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0k1 -> government.government_position_held.office_position_or_title -> Governor of Texas\n# Answer:\nm.04ks0k1", "# Reasoning Path:\nTexas -> government.political_district.representatives -> m.04j5vd_ -> government.government_position_held.office_holder -> Lyndon B. Johnson\n# Answer:\nm.04j5vd_", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0k1 -> government.government_position_held.basic_title -> Governor\n# Answer:\nm.04ks0k1", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0f5 -> government.government_position_held.basic_title -> Governor\n# Answer:\nm.04ks0f5", "# Reasoning Path:\nTexas -> organization.organization_scope.organizations_with_this_scope -> B'nai B'rith District Grand Lodge 7 -> organization.organization.geographic_scope -> Alabama\n# Answer:\nB'nai B'rith District Grand Lodge 7", "# Reasoning Path:\nTexas -> government.political_district.representatives -> m.052p616 -> government.government_position_held.office_holder -> William Neff Patman\n# Answer:\nm.052p616"], "ground_truth": ["Sam Houston", "George Tyler Wood", "Pat Morris Neff", "John Ireland", "James E. Ferguson", "Fletcher Stockdale", "J. W. Henderson", "William P. Hobby", "Dan Moody", "Coke R. Stevenson", "James Pinckney Henderson", "Francis Lubbock", "Ann Richards", "James Allred", "Edward Clark", "Peter Hansborough Bell", "Beauford H. Jester", "Oran Milo Roberts", "W. Lee O'Daniel", "Charles Allen Culberson", "Miriam A. Ferguson", "Richard B. Hubbard", "Hardin Richard Runnels", "Allan Shivers", "Edmund J. Davis", "Price Daniel", "Pendleton Murrah", "Ross S. Sterling", "Elisha M. Pease", "Lawrence Sullivan Ross", "Andrew Jackson Hamilton", "Thomas Mitchell Campbell", "Dolph Briscoe", "James W. Throckmorton", "John Connally", "Rick Perry", "Jim Hogg", "Joseph D. Sayers", "Preston Smith", "S. W. T. Lanham", "Bill Clements", "Mark White", "Oscar Branch Colquitt", "George W. Bush", "Richard Coke"], "ans_acc": 0.06666666666666667, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.08, "path_precision": 0.3, "path_recall": 0.046153846153846156, "path_ans_f1": 0.1090909090909091, "path_ans_precision": 0.3, "path_ans_recall": 0.06666666666666667}
{"id": "WebQTest-1275", "prediction": ["# Reasoning Path:\nHelen Flanagan -> people.person.profession -> Actor -> people.profession.specializations -> Audio Dramatist\n# Answer:\nActor", "# Reasoning Path:\nHelen Flanagan -> people.person.profession -> Actor -> type.property.expected_type -> TV Actor\n# Answer:\nActor", "# Reasoning Path:\nHelen Flanagan -> people.person.profession -> Actor -> people.profession.specializations -> Camgirl\n# Answer:\nActor", "# Reasoning Path:\nHelen Flanagan -> people.person.profession -> Actor -> music.special_music_video_performance_type.special_music_video_performances -> m.010wvs8m\n# Answer:\nActor", "# Reasoning Path:\nHelen Flanagan -> people.person.profession -> Actor -> people.profession.specializations -> Character Actor\n# Answer:\nActor", "# Reasoning Path:\nHelen Flanagan -> common.topic.notable_types -> TV Actor -> type.type.domain -> TV\n# Answer:\nTV Actor", "# Reasoning Path:\nHelen Flanagan -> people.person.profession -> Actor -> music.special_music_video_performance_type.special_music_video_performances -> m.011qs93_\n# Answer:\nActor", "# Reasoning Path:\nHelen Flanagan -> people.person.profession -> Actor -> music.special_music_video_performance_type.special_music_video_performances -> m.0gw6dxs\n# Answer:\nActor", "# Reasoning Path:\nHelen Flanagan -> common.topic.notable_types -> TV Actor -> type.type.expected_by -> Actor\n# Answer:\nTV Actor", "# Reasoning Path:\nHelen Flanagan -> tv.tv_actor.guest_roles -> m.09nhvxt -> tv.tv_guest_role.episodes_appeared_in -> Ant & Dec's Saturday Night Takeaway Season 9 Episode 5\n# Answer:\nm.09nhvxt"], "ground_truth": ["Actor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1276", "prediction": ["# Reasoning Path:\nOlympic Games -> common.topic.notable_types -> Website Category -> common.topic.article -> m.02ht13p\n# Answer:\nWebsite Category", "# Reasoning Path:\nOlympic Games -> common.topic.notable_types -> Website Category -> freebase.type_profile.kind -> Classification\n# Answer:\nWebsite Category", "# Reasoning Path:\nOlympic Games -> common.topic.notable_types -> Website Category -> type.type.properties -> Websites of this genre\n# Answer:\nWebsite Category", "# Reasoning Path:\nOlympic Games -> time.recurring_event.current_frequency -> Quadrennial -> common.topic.notable_types -> Unit of frequency\n# Answer:\nQuadrennial", "# Reasoning Path:\nOlympic Games -> book.book_subject.works -> All That Glitters Is Not Gold: The Olympic Game -> book.written_work.author -> William O. Johnson Jr.\n# Answer:\nAll That Glitters Is Not Gold: The Olympic Game", "# Reasoning Path:\nOlympic Games -> time.recurring_event.current_frequency -> Quadrennial -> common.topic.notable_for -> g.125d3_wnj\n# Answer:\nQuadrennial", "# Reasoning Path:\nOlympic Games -> book.book_subject.works -> Berlin Games -> book.written_work.subjects -> Europe\n# Answer:\nBerlin Games", "# Reasoning Path:\nOlympic Games -> time.recurring_event.current_frequency -> Quadrennial -> freebase.unit_profile.dimension -> Time\n# Answer:\nQuadrennial", "# Reasoning Path:\nOlympic Games -> book.book_subject.works -> Berlin Games -> book.written_work.subjects -> Germany\n# Answer:\nBerlin Games", "# Reasoning Path:\nOlympic Games -> book.book_subject.works -> Berlin Games -> common.topic.notable_types -> Book\n# Answer:\nBerlin Games"], "ground_truth": ["Quadrennial"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1277", "prediction": ["# Reasoning Path:\nTennessee River -> location.location.partially_containedby -> Kentucky -> location.location.adjoin_s -> m.02thc30\n# Answer:\nKentucky", "# Reasoning Path:\nTennessee River -> location.location.partially_containedby -> Kentucky -> location.location.partially_contains -> Appalachian Mountains\n# Answer:\nKentucky", "# Reasoning Path:\nTennessee River -> location.location.partially_containedby -> Alabama -> location.location.partially_contains -> Appalachian Mountains\n# Answer:\nAlabama", "# Reasoning Path:\nTennessee River -> location.location.partially_containedby -> Kentucky -> location.location.events -> Cherokee\u2013American wars\n# Answer:\nKentucky", "# Reasoning Path:\nTennessee River -> location.location.partially_containedby -> Alabama -> meteorology.cyclone_affected_area.cyclones -> 1837 Racer's Storm\n# Answer:\nAlabama", "# Reasoning Path:\nTennessee River -> location.location.partially_containedby -> Alabama -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nAlabama", "# Reasoning Path:\nTennessee River -> location.location.partially_containedby -> Tennessee -> location.location.partially_contains -> Appalachian Mountains\n# Answer:\nTennessee", "# Reasoning Path:\nTennessee River -> location.location.partially_containedby -> Kentucky -> location.location.partially_contains -> Big South Fork of the Cumberland River\n# Answer:\nKentucky", "# Reasoning Path:\nTennessee River -> location.location.partially_containedby -> Alabama -> location.location.partially_contains -> Buttahatchee River\n# Answer:\nAlabama", "# Reasoning Path:\nTennessee River -> location.location.partially_containedby -> Tennessee -> base.aareas.schema.administrative_area.administrative_children -> Humphreys County\n# Answer:\nTennessee"], "ground_truth": ["Kentucky", "Alabama", "Tennessee", "Mississippi"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.8571428571428571, "path_precision": 1.0, "path_recall": 0.75, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-1278", "prediction": ["# Reasoning Path:\nVenezuela -> location.location.events -> 2013 Venezuela Transaereo 5074 Britten-Norman Islander crash -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n2013 Venezuela Transaereo 5074 Britten-Norman Islander crash", "# Reasoning Path:\nVenezuela -> location.location.events -> Battle of La Victoria -> common.topic.notable_types -> Military Conflict\n# Answer:\nBattle of La Victoria", "# Reasoning Path:\nVenezuela -> location.location.events -> 2013 Venezuela Transaereo 5074 Britten-Norman Islander crash -> freebase.valuenotation.is_reviewed -> Start date\n# Answer:\n2013 Venezuela Transaereo 5074 Britten-Norman Islander crash", "# Reasoning Path:\nVenezuela -> location.location.events -> Battle of La Victoria -> time.event.included_in_event -> Venezuelan War of Independence\n# Answer:\nBattle of La Victoria", "# Reasoning Path:\nVenezuela -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71v1t_3\n# Answer:\ng.11b71v1t_3", "# Reasoning Path:\nVenezuela -> location.location.events -> 2007 RCTV protests -> base.newsevents.news_reported_event.news_report_s -> m.07wx5cm\n# Answer:\n2007 RCTV protests", "# Reasoning Path:\nVenezuela -> location.location.events -> 2007 RCTV protests -> common.topic.notable_for -> g.125773yxl\n# Answer:\n2007 RCTV protests", "# Reasoning Path:\nVenezuela -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> Argentina\n# Answer:\nSpanish Language", "# Reasoning Path:\nVenezuela -> location.location.events -> 2007 RCTV protests -> common.topic.article -> m.02rf6lg\n# Answer:\n2007 RCTV protests", "# Reasoning Path:\nVenezuela -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fdz2\n# Answer:\ng.12tb6fdz2"], "ground_truth": ["2007 RCTV protests", "Venezuelan crisis of 1902\u201303", "2014 Festival Internacional de Cine Infantil y Juvenil", "g.122dshgz", "2014 Caribbean Earthquake", "Machurucuto Incident", "12th G-15 summit", "2014 NACAM Rally Championship", "Venezuelan War of Independence", "Dutch\u2013Venezuelan crisis of 1908", "2014\u201315 Venezuelan protests", "Battle of La Victoria", "Dutch\u2013Venezuela War", "Los Maniceros massacre", "Bombardment of Fort San Carlos", "Battle of Santa In\u00e9s", "2011 ALBA Games", "2002 Venezuelan coup d'\u00e9tat attempt", "2013 Venezuela Transaereo 5074 Britten-Norman Islander crash"], "ans_acc": 0.21052631578947367, "ans_hit": 1, "ans_f1": 0.2576687116564417, "ans_precission": 0.7, "ans_recall": 0.15789473684210525, "path_f1": 0.2576687116564417, "path_precision": 0.7, "path_recall": 0.15789473684210525, "path_ans_f1": 0.3236994219653179, "path_ans_precision": 0.7, "path_ans_recall": 0.21052631578947367}
{"id": "WebQTest-1279", "prediction": ["# Reasoning Path:\nCorey Taylor -> music.group_member.instruments_played -> Bass guitar -> music.instrument.variation -> Kalindula\n# Answer:\nBass guitar", "# Reasoning Path:\nCorey Taylor -> music.group_member.instruments_played -> Bass guitar -> common.topic.webpage -> m.09wnh17\n# Answer:\nBass guitar", "# Reasoning Path:\nCorey Taylor -> music.group_member.instruments_played -> Bass guitar -> common.topic.subject_of -> Reverb\n# Answer:\nBass guitar", "# Reasoning Path:\nCorey Taylor -> music.group_member.instruments_played -> Guitar -> music.instrument.variation -> Bass guitar\n# Answer:\nGuitar", "# Reasoning Path:\nCorey Taylor -> music.group_member.instruments_played -> Drums -> music.instrument.variation -> Ashiko\n# Answer:\nDrums", "# Reasoning Path:\nCorey Taylor -> music.group_member.instruments_played -> Drums -> common.topic.notable_for -> g.125brs154\n# Answer:\nDrums", "# Reasoning Path:\nCorey Taylor -> music.group_member.instruments_played -> Guitar -> music.instrument.variation -> Fretless guitar\n# Answer:\nGuitar", "# Reasoning Path:\nCorey Taylor -> music.group_member.instruments_played -> Drums -> music.instrument.variation -> Basler drum\n# Answer:\nDrums", "# Reasoning Path:\nCorey Taylor -> music.artist.track -> Death to All But Metal (Radio Edit) -> music.recording.artist -> Steel Panther\n# Answer:\nDeath to All But Metal (Radio Edit)", "# Reasoning Path:\nCorey Taylor -> music.composer.compositions -> Before I Forget -> music.recording.artist -> Slipknot\n# Answer:\nBefore I Forget"], "ground_truth": ["Bass guitar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-128", "prediction": ["# Reasoning Path:\nJuni Cortez -> film.film_character.portrayed_in_films -> m.0g51m5_ -> film.performance.actor -> Daryl Sabara\n# Answer:\nm.0g51m5_", "# Reasoning Path:\nJuni Cortez -> film.film_character.portrayed_in_films -> m.02s9tvb -> film.performance.actor -> Daryl Sabara\n# Answer:\nm.02s9tvb", "# Reasoning Path:\nJuni Cortez -> film.film_character.portrayed_in_films -> m.02s9trw -> film.performance.actor -> Daryl Sabara\n# Answer:\nm.02s9trw", "# Reasoning Path:\nJuni Cortez -> common.topic.notable_for -> g.1254x65b_\n# Answer:\ng.1254x65b_", "# Reasoning Path:\nJuni Cortez -> film.film_character.portrayed_in_films -> m.0g51m5_ -> film.performance.film -> Spy Kids: All the Time in the World\n# Answer:\nm.0g51m5_", "# Reasoning Path:\nJuni Cortez -> common.topic.article -> m.02ql78\n# Answer:\nm.02ql78", "# Reasoning Path:\nJuni Cortez -> film.film_character.portrayed_in_films -> m.02s9tvb -> film.performance.film -> Spy Kids 2: The Island of Lost Dreams\n# Answer:\nm.02s9tvb", "# Reasoning Path:\nJuni Cortez -> film.film_character.portrayed_in_films -> m.02s9trw -> film.performance.film -> Spy Kids\n# Answer:\nm.02s9trw"], "ground_truth": ["Daryl Sabara"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.375, "path_recall": 0.75, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1280", "prediction": ["# Reasoning Path:\nFranklin D. Roosevelt -> government.us_president.vice_president -> Harry S. Truman -> organization.organization_founder.organizations_founded -> Central Intelligence Agency (CIA)\n# Answer:\nHarry S. Truman", "# Reasoning Path:\nFranklin D. Roosevelt -> government.us_president.vice_president -> Henry A. Wallace -> people.person.nationality -> United States of America\n# Answer:\nHenry A. Wallace", "# Reasoning Path:\nFranklin D. Roosevelt -> people.person.children -> Franklin D. Roosevelt, Jr. -> freebase.valuenotation.has_no_value -> Children\n# Answer:\nFranklin D. Roosevelt, Jr.", "# Reasoning Path:\nFranklin D. Roosevelt -> government.us_president.vice_president -> Henry A. Wallace -> people.person.profession -> Politician\n# Answer:\nHenry A. Wallace", "# Reasoning Path:\nFranklin D. Roosevelt -> people.person.children -> Franklin D. Roosevelt, Jr. -> common.topic.notable_types -> Deceased Person\n# Answer:\nFranklin D. Roosevelt, Jr.", "# Reasoning Path:\nFranklin D. Roosevelt -> government.us_president.vice_president -> Harry S. Truman -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nHarry S. Truman", "# Reasoning Path:\nFranklin D. Roosevelt -> government.us_president.vice_president -> John Nance Garner -> people.person.place_of_birth -> Detroit\n# Answer:\nJohn Nance Garner", "# Reasoning Path:\nFranklin D. Roosevelt -> people.person.children -> Franklin D. Roosevelt, Jr. -> freebase.valuenotation.has_no_value -> Profession\n# Answer:\nFranklin D. Roosevelt, Jr.", "# Reasoning Path:\nFranklin D. Roosevelt -> government.us_president.vice_president -> Harry S. Truman -> freebase.valuenotation.is_reviewed -> Date of death\n# Answer:\nHarry S. Truman", "# Reasoning Path:\nFranklin D. Roosevelt -> government.us_president.vice_president -> Henry A. Wallace -> people.person.religion -> Anglicanism\n# Answer:\nHenry A. Wallace"], "ground_truth": ["Harry S. Truman"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1281", "prediction": ["# Reasoning Path:\nKate Gosselin -> people.person.place_of_birth -> Philadelphia -> base.biblioness.bibs_location.state -> Pennsylvania\n# Answer:\nPhiladelphia", "# Reasoning Path:\nKate Gosselin -> people.person.place_of_birth -> Philadelphia -> location.location.containedby -> Pennsylvania\n# Answer:\nPhiladelphia", "# Reasoning Path:\nKate Gosselin -> people.person.children -> Cara Nicole Gosselin -> people.person.place_of_birth -> Penn State Milton S. Hershey Medical Center\n# Answer:\nCara Nicole Gosselin", "# Reasoning Path:\nKate Gosselin -> people.person.place_of_birth -> Philadelphia -> location.location.containedby -> United States of America\n# Answer:\nPhiladelphia", "# Reasoning Path:\nKate Gosselin -> common.topic.webpage -> m.046s2cx -> common.webpage.category -> Topic Webpage\n# Answer:\nm.046s2cx", "# Reasoning Path:\nKate Gosselin -> people.person.children -> Aaden Jonathan Gosselin -> people.person.place_of_birth -> Penn State Milton S. Hershey Medical Center\n# Answer:\nAaden Jonathan Gosselin", "# Reasoning Path:\nKate Gosselin -> people.person.children -> Cara Nicole Gosselin -> people.person.parents -> Jon Gosselin\n# Answer:\nCara Nicole Gosselin", "# Reasoning Path:\nKate Gosselin -> people.person.children -> Alexis Faith Gosselin -> people.person.place_of_birth -> Penn State Milton S. Hershey Medical Center\n# Answer:\nAlexis Faith Gosselin", "# Reasoning Path:\nKate Gosselin -> common.topic.webpage -> m.046s2cx -> common.webpage.resource -> The Gosselin 10\n# Answer:\nm.046s2cx", "# Reasoning Path:\nKate Gosselin -> common.topic.webpage -> m.09wcrf4 -> common.webpage.resource -> 'Jon & Kate Plus 8' star attracts hundreds of fans to California book signing\n# Answer:\nm.09wcrf4"], "ground_truth": ["Philadelphia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1282", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> American Samoa\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\ng.125_r6z32", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guam\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Germany\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Germany\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nm.010r7f0z", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Marshall Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Mexico\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Mexico\n# Answer:\nFederal republic"], "ground_truth": ["Constitutional republic", "Presidential system", "Federal republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1284", "prediction": ["# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> common.topic.notable_for -> g.1256sch6w\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> architecture.structure.architect -> Domenico da Cortona\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> common.topic.image -> Chateau de Chambord\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Arc de Triomphe -> architecture.structure.architectural_style -> Neoclassicism\n# Answer:\nArc de Triomphe", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> architecture.structure.architect -> Pierre Nepveu\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> common.topic.webpage -> m.02k8dbw -> common.webpage.category -> Topic Webpage\n# Answer:\nm.02k8dbw", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> common.topic.image -> France Loir-et-Cher Chambord Chateau 03\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Bois de Boulogne -> location.location.containedby -> France\n# Answer:\nBois de Boulogne", "# Reasoning Path:\nParis -> common.topic.webpage -> m.02k8dbw -> common.webpage.resource -> m.0bkwkw7\n# Answer:\nm.02k8dbw", "# Reasoning Path:\nParis -> common.topic.webpage -> m.051qt2p -> common.webpage.category -> Topic Webpage\n# Answer:\nm.051qt2p"], "ground_truth": ["Petit Palais", "Mus\u00e9e Maillol", "\u00cele de la Cit\u00e9", "Eiffel Tower", "Place de la Concorde", "Mus\u00e9e d'Orsay", "Ch\u00e2teau de Chambord", "Centre Georges Pompidou", "Mus\u00e9e des Arts et M\u00e9tiers", "Bois de Boulogne", "Mus\u00e9e du quai Branly", "Galerie nationale du Jeu de Paume", "Parc Ast\u00e9rix", "Gare d'Orsay", "Notre Dame de Paris", "Grande Arche", "Galerie Claude Bernard", "Disneyland Park", "Caf\u00e9 Volpini", "Arc de Triomphe", "Sacr\u00e9-C\u0153ur, Paris", "Folies Berg\u00e8re", "Les Invalides", "Champs-\u00c9lys\u00e9es", "Jardin du Luxembourg", "The Louvre", "Disneyland Paris", "La Maison Rouge", "Panth\u00e9on", "Mus\u00e9e de l'Orangerie", "Verdon Gorge"], "ans_acc": 0.0967741935483871, "ans_hit": 1, "ans_f1": 0.1700404858299595, "ans_precission": 0.7, "ans_recall": 0.0967741935483871, "path_f1": 0.1700404858299595, "path_precision": 0.7, "path_recall": 0.0967741935483871, "path_ans_f1": 0.1700404858299595, "path_ans_precision": 0.7, "path_ans_recall": 0.0967741935483871}
{"id": "WebQTest-1285", "prediction": ["# Reasoning Path:\nLassen Peak -> location.location.containedby -> Shasta County -> location.location.containedby -> California\n# Answer:\nShasta County", "# Reasoning Path:\nLassen Peak -> location.location.containedby -> Lassen Volcanic National Park -> location.location.containedby -> Shasta County\n# Answer:\nLassen Volcanic National Park", "# Reasoning Path:\nLassen Peak -> common.topic.notable_for -> g.125cb11p0\n# Answer:\ng.125cb11p0", "# Reasoning Path:\nLassen Peak -> location.location.containedby -> Shasta County -> location.us_county.hud_county_place -> Shasta Lake\n# Answer:\nShasta County", "# Reasoning Path:\nLassen Peak -> location.location.containedby -> Shasta County -> location.us_county.hud_county_place -> Anderson\n# Answer:\nShasta County", "# Reasoning Path:\nLassen Peak -> location.location.containedby -> Lassen Volcanic National Park -> common.topic.notable_for -> g.1255yn2p5\n# Answer:\nLassen Volcanic National Park", "# Reasoning Path:\nLassen Peak -> location.location.containedby -> Shasta County -> location.us_county.hud_county_place -> Big Bend\n# Answer:\nShasta County", "# Reasoning Path:\nLassen Peak -> location.location.containedby -> Shasta County -> location.statistical_region.population -> g.11b674hm30\n# Answer:\nShasta County", "# Reasoning Path:\nLassen Peak -> location.location.containedby -> Lassen Volcanic National Park -> symbols.namesake.named_after -> Peter Lassen\n# Answer:\nLassen Volcanic National Park", "# Reasoning Path:\nLassen Peak -> location.location.containedby -> California -> location.location.containedby -> Contiguous United States\n# Answer:\nCalifornia"], "ground_truth": ["California", "Shasta County", "Cascade Range", "Lassen Volcanic National Park", "North America", "United States of America"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6428571428571429, "ans_precission": 0.9, "ans_recall": 0.5, "path_f1": 0.6428571428571429, "path_precision": 0.9, "path_recall": 0.5, "path_ans_f1": 0.6428571428571429, "path_ans_precision": 0.9, "path_ans_recall": 0.5}
{"id": "WebQTest-1287", "prediction": ["# Reasoning Path:\nOhio -> government.governmental_jurisdiction.official_symbols -> m.04st75g -> location.location_symbol_relationship.symbol -> Tomato juice\n# Answer:\nm.04st75g", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.official_symbols -> m.04st75g -> location.location_symbol_relationship.Kind_of_symbol -> State beverage\n# Answer:\nm.04st75g", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.official_symbols -> m.04st87j -> location.location_symbol_relationship.symbol -> Cardinal\n# Answer:\nm.04st87j", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.official_symbols -> m.04stkcq -> location.location_symbol_relationship.symbol -> Walleye\n# Answer:\nm.04stkcq", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.official_symbols -> m.04st87j -> location.location_symbol_relationship.Kind_of_symbol -> State bird\n# Answer:\nm.04st87j", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.legislative_sessions -> 106th United States Congress\n# Answer:\nm.05kfccr", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.office_holder -> George Voinovich\n# Answer:\nm.05kfccr", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nm.05kg_6s", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.official_symbols -> m.04stkcq -> location.location_symbol_relationship.Kind_of_symbol -> State Fish\n# Answer:\nm.04stkcq", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> meteorology.tropical_cyclone.affected_areas -> Alabama\n# Answer:\nHurricane Bob"], "ground_truth": ["With God, all things are possible"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1289", "prediction": ["# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Art song -> music.compositional_form.superforms -> Song\n# Answer:\nArt song", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Opera -> common.topic.notable_types -> Composition type\n# Answer:\nOpera", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Art song -> music.genre.albums -> Acts of Beauty/Exit no Exit\n# Answer:\nArt song", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Chamber music -> music.genre.albums -> Acts of Beauty/Exit no Exit\n# Answer:\nChamber music", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Chamber music -> music.genre.parent_genre -> Classical music\n# Answer:\nChamber music", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Art song -> music.genre.artists -> Arnold Schoenberg\n# Answer:\nArt song", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Art song -> music.genre.albums -> String Quartets 2, 3 & 4/If & Why\n# Answer:\nArt song", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Chamber music -> music.genre.albums -> String Quartets 2, 3 & 4/If & Why\n# Answer:\nChamber music", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Art song -> music.genre.albums -> Time Will Pronounce\n# Answer:\nArt song", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Art song -> music.genre.artists -> Claude Debussy\n# Answer:\nArt song"], "ground_truth": ["Ballet", "Chamber music", "Opera", "Classical music", "Art song"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 1.0, "ans_recall": 0.6, "path_f1": 0.7499999999999999, "path_precision": 1.0, "path_recall": 0.6, "path_ans_f1": 0.888888888888889, "path_ans_precision": 1.0, "path_ans_recall": 0.8}
{"id": "WebQTest-129", "prediction": ["# Reasoning Path:\nLucille Ball -> people.person.profession -> Model -> base.lightweight.profession.similar_professions -> Actor\n# Answer:\nModel", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Model -> common.topic.notable_types -> Profession\n# Answer:\nModel", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Actor -> owl#inverseOf -> Film performances\n# Answer:\nActor", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Television producer -> common.topic.subjects -> Billy Sorrentino\n# Answer:\nTelevision producer", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Television producer -> common.topic.notable_types -> Profession\n# Answer:\nTelevision producer", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Actor -> freebase.equivalent_topic.equivalent_type -> Film actor\n# Answer:\nActor", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Television producer -> common.topic.subjects -> Michael Palance\n# Answer:\nTelevision producer", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Television producer -> common.topic.subject_of -> Billy Sorrentino\n# Answer:\nTelevision producer", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Actor -> base.lightweight.profession.professions_similar -> Model\n# Answer:\nActor", "# Reasoning Path:\nLucille Ball -> tv.tv_actor.starring_roles -> m.02t91wg -> tv.regular_tv_appearance.character -> Lucy Ricardo\n# Answer:\nm.02t91wg"], "ground_truth": ["Model", "Actor", "Singer", "Television producer", "Comedian"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7200000000000001, "ans_precission": 0.9, "ans_recall": 0.6, "path_f1": 0.7200000000000001, "path_precision": 0.9, "path_recall": 0.6, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 1.0, "path_ans_recall": 0.6}
{"id": "WebQTest-1291", "prediction": ["# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Kathleen Cavendish -> people.person.place_of_birth -> Brookline\n# Answer:\nKathleen Cavendish", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Ted Kennedy -> people.person.children -> Patrick J. Kennedy\n# Answer:\nTed Kennedy", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Kathleen Cavendish -> people.person.places_lived -> m.076lk9c\n# Answer:\nKathleen Cavendish", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Ted Kennedy -> people.person.children -> Edward M. Kennedy, Jr.\n# Answer:\nTed Kennedy", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Rosemary Kennedy -> common.topic.notable_types -> Family member\n# Answer:\nRosemary Kennedy", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Kathleen Cavendish -> people.family_member.family -> House of Cavendish\n# Answer:\nKathleen Cavendish", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Ted Kennedy -> people.person.children -> Kara Kennedy\n# Answer:\nTed Kennedy", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Kathleen Cavendish -> people.family_member.family -> Kennedy family\n# Answer:\nKathleen Cavendish", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Ted Kennedy -> people.person.sibling_s -> m.02_wjwf\n# Answer:\nTed Kennedy", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Rosemary Kennedy -> common.topic.article -> m.01kyp0\n# Answer:\nRosemary Kennedy"], "ground_truth": ["Joseph P. Kennedy, Jr.", "Kathleen Cavendish", "Ted Kennedy", "John F. Kennedy", "Rosemary Kennedy", "Patricia Kennedy Lawford", "Jean Kennedy Smith", "Eunice Kennedy Shriver", "Robert F. Kennedy"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 1.0, "ans_recall": 0.3333333333333333, "path_f1": 0.5, "path_precision": 1.0, "path_recall": 0.3333333333333333, "path_ans_f1": 0.5, "path_ans_precision": 1.0, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1292", "prediction": ["# Reasoning Path:\nBarcelona -> location.location.nearby_airports -> Barcelona\u2013El Prat Airport -> location.location.geolocation -> m.02_h61n\n# Answer:\nBarcelona\u2013El Prat Airport", "# Reasoning Path:\nBarcelona -> location.location.nearby_airports -> Barcelona\u2013El Prat Airport -> aviation.airport.operator -> ENAIRE\n# Answer:\nBarcelona\u2013El Prat Airport", "# Reasoning Path:\nBarcelona -> location.location.nearby_airports -> Barcelona\u2013El Prat Airport -> aviation.airport.hub_for -> Air Nostrum\n# Answer:\nBarcelona\u2013El Prat Airport", "# Reasoning Path:\nBarcelona -> travel.travel_destination.how_to_get_here -> m.051r6vy -> travel.transportation.transport_terminus -> Carrer d'Al\u00ed Bei, 80\n# Answer:\nm.051r6vy", "# Reasoning Path:\nBarcelona -> travel.travel_destination.how_to_get_here -> m.051r6vn -> travel.transportation.transport_terminus -> Barcelona\u2013El Prat Airport\n# Answer:\nm.051r6vn", "# Reasoning Path:\nBarcelona -> location.location.nearby_airports -> Barcelona\u2013El Prat Airport -> aviation.airport.hub_for -> Clickair\n# Answer:\nBarcelona\u2013El Prat Airport", "# Reasoning Path:\nBarcelona -> travel.travel_destination.how_to_get_here -> m.051r6vy -> travel.transportation.mode_of_transportation -> Bus\n# Answer:\nm.051r6vy", "# Reasoning Path:\nBarcelona -> location.location.nearby_airports -> Barcelona\u2013El Prat Airport -> aviation.airport.hub_for -> Girjet\n# Answer:\nBarcelona\u2013El Prat Airport", "# Reasoning Path:\nBarcelona -> travel.travel_destination.how_to_get_here -> m.051r6vn -> travel.transportation.mode_of_transportation -> Air travel\n# Answer:\nm.051r6vn", "# Reasoning Path:\nBarcelona -> travel.travel_destination.how_to_get_here -> m.051r6wb -> travel.transportation.transport_terminus -> Barcelona Fran\u00e7a railway station\n# Answer:\nm.051r6wb"], "ground_truth": ["Barcelona\u2013El Prat Airport"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1293", "prediction": ["# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> Anglo-Frisian languages -> language.language_family.languages -> Frisian languages\n# Answer:\nAnglo-Frisian languages", "# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> Anglo-Frisian languages -> common.topic.article -> m.082j1_\n# Answer:\nAnglo-Frisian languages", "# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> Indo-European languages -> language.language_family.sub_families -> Germanic languages\n# Answer:\nIndo-European languages", "# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> Anglo-Frisian languages -> common.topic.notable_types -> Language Family\n# Answer:\nAnglo-Frisian languages", "# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> Anglo-Frisian languages -> language.language_family.languages -> Forth and Bargy dialect\n# Answer:\nAnglo-Frisian languages", "# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> Indo-European languages -> common.topic.notable_types -> Language Family\n# Answer:\nIndo-European languages", "# Reasoning Path:\nEnglish Language -> education.school_category.schools_of_this_kind -> Piers Midwinter -> people.person.quotations -> Learn with a NEST. Please be my GUEST. I will give you my BEST,\n# Answer:\nPiers Midwinter", "# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> Anglo-Frisian languages -> language.language_family.languages -> Frisian, Northern Language\n# Answer:\nAnglo-Frisian languages", "# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> West Germanic languages -> common.topic.image -> Europe germanic languages\n# Answer:\nWest Germanic languages", "# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> West Germanic languages -> common.topic.notable_types -> Language Family\n# Answer:\nWest Germanic languages"], "ground_truth": ["Anglo-Frisian languages", "West Germanic languages", "Indo-European languages"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1294", "prediction": ["# Reasoning Path:\nWorld War II -> time.event.locations -> Europe -> base.locations.continents.countries_within -> Poland\n# Answer:\nEurope", "# Reasoning Path:\nWorld War II -> time.event.locations -> Europe -> location.location.time_zones -> Eastern European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nWorld War II -> time.event.locations -> Europe -> base.locations.continents.countries_within -> Germany\n# Answer:\nEurope", "# Reasoning Path:\nWorld War II -> time.event.locations -> Europe -> location.location.partially_contains -> Soviet Union\n# Answer:\nEurope", "# Reasoning Path:\nWorld War II -> time.event.locations -> Europe -> base.locations.continents.countries_within -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nWorld War II -> time.event.locations -> Asia -> location.location.partially_contains -> Soviet Union\n# Answer:\nAsia", "# Reasoning Path:\nWorld War II -> time.event.locations -> Africa -> base.locations.continents.countries_within -> South Africa\n# Answer:\nAfrica", "# Reasoning Path:\nWorld War II -> time.event.locations -> Africa -> location.location.events -> World War I\n# Answer:\nAfrica", "# Reasoning Path:\nWorld War II -> military.military_conflict.combatants -> m.04g9ysp -> military.military_combatant_group.combatants -> Poland\n# Answer:\nm.04g9ysp", "# Reasoning Path:\nWorld War II -> military.military_conflict.combatants -> m.04g9ywq -> military.military_combatant_group.combatants -> Nazi Germany\n# Answer:\nm.04g9ywq"], "ground_truth": ["Battle of Falmouth"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1295", "prediction": ["# Reasoning Path:\nMatt Damon -> film.actor.film -> m.0k2pxw -> film.performance.film -> Rounders\n# Answer:\nm.0k2pxw", "# Reasoning Path:\nMatt Damon -> award.award_nominee.award_nominations -> m.04dn0vx -> award.award_nomination.nominated_for -> The Bourne Ultimatum\n# Answer:\nm.04dn0vx", "# Reasoning Path:\nMatt Damon -> film.actor.film -> g.11b6v52kzy\n# Answer:\ng.11b6v52kzy", "# Reasoning Path:\nMatt Damon -> film.actor.film -> m.0k2pxw -> film.performance.character -> Mike McDermott\n# Answer:\nm.0k2pxw", "# Reasoning Path:\nMatt Damon -> award.award_nominee.award_nominations -> m.04dn0w4 -> award.award_nomination.nominated_for -> Saving Private Ryan\n# Answer:\nm.04dn0w4", "# Reasoning Path:\nMatt Damon -> film.actor.film -> m.0114bqjl -> film.performance.film -> Judge Not: In Defense of Dogma\n# Answer:\nm.0114bqjl", "# Reasoning Path:\nMatt Damon -> award.award_nominee.award_nominations -> m.04dn0w4 -> award.award_nomination.nominated_for -> Good Will Hunting\n# Answer:\nm.04dn0w4", "# Reasoning Path:\nMatt Damon -> award.award_nominee.award_nominations -> m.04dn0w4 -> award.award_nomination.award -> London Film Critics' Circle Award for Actor of the Year\n# Answer:\nm.04dn0w4", "# Reasoning Path:\nMatt Damon -> film.actor.film -> m.0114bqjl -> film.performance.special_performance_type -> Him/Herself\n# Answer:\nm.0114bqjl", "# Reasoning Path:\nMatt Damon -> award.award_nominee.award_nominations -> m.04dn0w4 -> award.award_nomination.nominated_for -> The Rainmaker\n# Answer:\nm.04dn0w4"], "ground_truth": ["The Third Wheel", "The Departed", "Interstellar", "Ocean's Thirteen", "Happy Feet Two", "Ocean's Twelve", "The Majestic", "Chasing Amy", "The Great Wall", "Confessions of a Dangerous Mind", "True Grit", "Titan A.E.", "Glory Daze", "Inside Job", "The Monuments Men", "The Legend of Bagger Vance", "The Zero Theorem", "The People Speak", "Promised Land", "Good Will Hunting", "Spirit: Stallion of the Cimarron", "Gerry", "The Bourne Identity", "The Talented Mr. Ripley", "Hereafter", "All the Pretty Horses", "Stuck on You", "Field of Dreams", "Elysium", "Contagion", "Behind the Candelabra", "Saving Private Ryan", "Invictus", "The Informant!", "Ocean's Eleven", "Geronimo: An American Legend", "Behind the Screens", "Dogma", "The Adjustment Bureau", "Magnificent Desolation: Walking On The Moon 3D", "Courage Under Fire", "The Bourne Ultimatum", "The Brothers Grimm", "Che: Part Two", "We Bought a Zoo", "Unauthorized: The Harvey Weinstein Project", "Rising Son", "The Good Mother", "Extreme Realities", "School Ties", "Youth Without Youth", "The Rainmaker", "Finding Forrester", "The Bourne Supremacy", "Mystic Pizza", "Rounders 2", "Green Zone", "Howard Zinn: You Can\u00b4t Be Neutral on a Moving Train", "Rounders", "Jay and Silent Bob Strike Back", "EuroTrip", "The Good Shepherd", "The Martian", "The Good Old Boys", "Jersey Girl", "Oh, What a Lovely Tea Party", "Syriana", "Margaret", "Push, Nevada", "Judge Not: In Defense of Dogma"], "ans_acc": 0.1, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.07352941176470588, "path_precision": 0.5, "path_recall": 0.03968253968253968, "path_ans_f1": 0.17142857142857143, "path_ans_precision": 0.6, "path_ans_recall": 0.1}
{"id": "WebQTest-1296", "prediction": ["# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> common.topic.notable_types -> Profession\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> base.descriptive_names.names.descriptive_name -> m.0105y97v\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> fictional_universe.character_occupation.characters_with_this_occupation -> Cam Calloway\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> base.descriptive_names.names.descriptive_name -> m.01069wj6\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Actor -> people.profession.specializations -> Voice Actor\n# Answer:\nActor", "# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.player_statistics -> m.04qg18x -> basketball.basketball_player_stats.team -> Houston Rockets\n# Answer:\nm.04qg18x", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> base.descriptive_names.names.descriptive_name -> m.01069wkk\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> fictional_universe.character_occupation.characters_with_this_occupation -> Derek Roman\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.player_statistics -> m.04q9rcg -> basketball.basketball_player_stats.season -> 2003\u201304 NBA season\n# Answer:\nm.04q9rcg", "# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.player_statistics -> m.04qb_fd -> basketball.basketball_player_stats.team -> Chicago Bulls\n# Answer:\nm.04qb_fd"], "ground_truth": ["Basketball player", "Actor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1297", "prediction": ["# Reasoning Path:\nCanada -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> North America\n# Answer:\nEarth", "# Reasoning Path:\nCanada -> location.location.containedby -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nCanada -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> City/Town/Village\n# Answer:\nEarth", "# Reasoning Path:\nCanada -> location.location.containedby -> North America -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nNorth America", "# Reasoning Path:\nCanada -> location.location.containedby -> DVD Region 1 -> location.location.contains -> Bermuda\n# Answer:\nDVD Region 1", "# Reasoning Path:\nCanada -> base.locations.countries.continent -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nCanada -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> France\n# Answer:\nEarth", "# Reasoning Path:\nCanada -> location.location.containedby -> North America -> location.location.contains_major_portion_of -> United States of America\n# Answer:\nNorth America", "# Reasoning Path:\nCanada -> base.locations.countries.continent -> North America -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nNorth America", "# Reasoning Path:\nCanada -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> Sovereign state\n# Answer:\nEarth"], "ground_truth": ["North America", "Americas"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1298", "prediction": ["# Reasoning Path:\nGeorge Wilson -> people.person.profession -> American football player -> common.topic.notable_types -> Profession\n# Answer:\nAmerican football player", "# Reasoning Path:\nGeorge Wilson -> common.topic.notable_types -> American football player -> common.topic.notable_types -> Profession\n# Answer:\nAmerican football player", "# Reasoning Path:\nGeorge Wilson -> people.person.profession -> American football player -> type.type.properties -> 40-yard dash time\n# Answer:\nAmerican football player", "# Reasoning Path:\nGeorge Wilson -> people.person.profession -> American football player -> people.profession.specialization_of -> Football player\n# Answer:\nAmerican football player", "# Reasoning Path:\nGeorge Wilson -> american_football.football_player.position_s -> Safety -> common.topic.notable_types -> Sports position\n# Answer:\nSafety", "# Reasoning Path:\nGeorge Wilson -> common.topic.notable_types -> American football player -> people.profession.specialization_of -> Football player\n# Answer:\nAmerican football player", "# Reasoning Path:\nGeorge Wilson -> people.person.profession -> American football player -> type.type.properties -> Football Almanac ID\n# Answer:\nAmerican football player", "# Reasoning Path:\nGeorge Wilson -> american_football.football_player.position_s -> Safety -> common.topic.article -> m.05b3tz\n# Answer:\nSafety", "# Reasoning Path:\nGeorge Wilson -> people.person.profession -> American football player -> type.type.properties -> Games\n# Answer:\nAmerican football player", "# Reasoning Path:\nGeorge Wilson -> american_football.football_player.position_s -> Safety -> organization.role.governors -> m.0_gm5jn\n# Answer:\nSafety"], "ground_truth": ["American football player"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> base.americancomedy.impersonated_celebrity.impersonated_by -> Rich Little\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of death\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> Mortal Error: The Shot That Killed JFK\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.nationality -> United States of America\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Caroline Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nCaroline Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> freebase.valuenotation.has_no_value -> Children\n# Answer:\nArabella Kennedy"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1300", "prediction": ["# Reasoning Path:\nFulham F.C. -> sports.sports_team.location -> Fulham -> location.location.containedby -> London\n# Answer:\nFulham", "# Reasoning Path:\nFulham F.C. -> sports.sports_team.location -> Fulham -> location.location.containedby -> England\n# Answer:\nFulham", "# Reasoning Path:\nFulham F.C. -> sports.sports_team.location -> Fulham -> location.location.contains -> Aragon House\n# Answer:\nFulham", "# Reasoning Path:\nFulham F.C. -> sports.sports_team.location -> Fulham -> common.topic.image -> Putney Bridge\n# Answer:\nFulham", "# Reasoning Path:\nFulham F.C. -> sports.sports_team.location -> Fulham -> location.location.contains -> Duke of Cumberland, Fulham\n# Answer:\nFulham", "# Reasoning Path:\nFulham F.C. -> sports.sports_team.location -> Fulham -> location.location.contains -> Stamford Bridge\n# Answer:\nFulham", "# Reasoning Path:\nFulham F.C. -> sports.sports_team.location -> Fulham -> common.topic.image -> Sheraton Skyline Hotel at London Heathrow is located in Greater London\n# Answer:\nFulham", "# Reasoning Path:\nFulham F.C. -> soccer.football_team.player_statistics -> m.0w8_1dt -> soccer.football_player_stats.player -> Adam Watts\n# Answer:\nm.0w8_1dt", "# Reasoning Path:\nFulham F.C. -> soccer.football_team.player_statistics -> m.0w8_sxc -> soccer.football_player_stats.player -> Sean Davis\n# Answer:\nm.0w8_sxc", "# Reasoning Path:\nFulham F.C. -> sports.professional_sports_team.owner_s -> Mohamed Al-Fayed -> people.person.sibling_s -> m.0h2nc1y\n# Answer:\nMohamed Al-Fayed"], "ground_truth": ["Fulham"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1301", "prediction": ["# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> B Street Theatre -> architecture.building.building_function -> Theatre\n# Answer:\nB Street Theatre", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> B Street Theatre -> location.location.geolocation -> m.0131pvp5\n# Answer:\nB Street Theatre", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> B Street Theatre -> freebase.valuenotation.has_value -> Architect\n# Answer:\nB Street Theatre", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Sacramento Zoo -> common.topic.image -> A uniquely colored lion statue greets visitors to teh Sacramento Zoo\n# Answer:\nSacramento Zoo", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Sacramento Zoo -> common.topic.webpage -> m.03lh9qj\n# Answer:\nSacramento Zoo", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Sacramento Zoo -> common.topic.notable_types -> Zoo\n# Answer:\nSacramento Zoo", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> California Automobile Museum -> common.topic.article -> m.065yn2j\n# Answer:\nCalifornia Automobile Museum", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> B Street Theatre -> freebase.valuenotation.has_value -> Architectural Style\n# Answer:\nB Street Theatre", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Sacramento Zoo -> common.topic.image -> LWBSacramentoZoo\n# Answer:\nSacramento Zoo", "# Reasoning Path:\nSacramento -> location.location.events -> 1991 Sacramento hostage crisis -> common.topic.notable_types -> Event\n# Answer:\n1991 Sacramento hostage crisis"], "ground_truth": ["California State Indian Museum", "Folsom Lake", "California State Railroad Museum", "Raging Waters Sacramento", "California State Capitol Museum", "Crocker Art Museum", "B Street Theatre", "Sutter's Fort", "Sacramento History Museum", "Sacramento Zoo", "California Automobile Museum"], "ans_acc": 0.2727272727272727, "ans_hit": 1, "ans_f1": 0.41860465116279066, "ans_precission": 0.9, "ans_recall": 0.2727272727272727, "path_f1": 0.41860465116279066, "path_precision": 0.9, "path_recall": 0.2727272727272727, "path_ans_f1": 0.41860465116279066, "path_ans_precision": 0.9, "path_ans_recall": 0.2727272727272727}
{"id": "WebQTest-1303", "prediction": ["# Reasoning Path:\nJosh Hutchersonm -> award.award_winner.awards_won -> m.0107j71z -> award.award_honor.honored_for -> The Hunger Games: Catching Fire\n# Answer:\nm.0107j71z", "# Reasoning Path:\nJosh Hutchersonm -> award.award_nominee.award_nominations -> m.0z83x3f -> award.award_nomination.nominated_for -> The Hunger Games\n# Answer:\nm.0z83x3f", "# Reasoning Path:\nJosh Hutchersonm -> award.award_winner.awards_won -> m.011jzbn6 -> award.award_honor.honored_for -> The Hunger Games: Catching Fire\n# Answer:\nm.011jzbn6", "# Reasoning Path:\nJosh Hutchersonm -> award.award_winner.awards_won -> m.0jxldhh -> award.award_honor.honored_for -> The Hunger Games\n# Answer:\nm.0jxldhh", "# Reasoning Path:\nJosh Hutchersonm -> award.award_nominee.award_nominations -> m.0jdc80p -> award.award_nomination.nominated_for -> The Hunger Games\n# Answer:\nm.0jdc80p", "# Reasoning Path:\nJosh Hutchersonm -> award.award_nominee.award_nominations -> m.0z83x3f -> award.award_nomination.ceremony -> 2012 Teen Choice Awards\n# Answer:\nm.0z83x3f", "# Reasoning Path:\nJosh Hutchersonm -> award.award_nominee.award_nominations -> m.0z83x3f -> award.award_nomination.award_nominee -> Jennifer Lawrence\n# Answer:\nm.0z83x3f", "# Reasoning Path:\nJosh Hutchersonm -> award.award_winner.awards_won -> m.0107j71z -> freebase.valuenotation.is_reviewed -> Award category\n# Answer:\nm.0107j71z", "# Reasoning Path:\nJosh Hutchersonm -> award.award_winner.awards_won -> m.011jzbn6 -> award.award_honor.award -> Teen Choice Award for Choice Movie Actor - Sci-Fi/Fantasy\n# Answer:\nm.011jzbn6", "# Reasoning Path:\nJosh Hutchersonm -> award.award_nominee.award_nominations -> m.0jdc80p -> award.award_nomination.award -> MTV Movie Award for Best Kiss\n# Answer:\nm.0jdc80p"], "ground_truth": ["The Polar Express", "In Dubious Battle", "Zathura", "RV", "Bridge to Terabithia", "Kicking & Screaming", "Little Manhattan", "Journey to the Center of the Earth", "Winged Creatures", "Escobar: Paradise Lost", "American Splendor", "Journey 2: The Mysterious Island", "Detention", "Epic", "The Hunger Games", "House Blend", "The Third Rule", "The Hunger Games: Catching Fire", "The Kids Are All Right", "One Last Ride", "Wilder Days", "Motocross Kids", "Red Dawn", "Cirque du Freak: The Vampire's Assistant", "The Forger", "Miracle Dogs", "Party Wagon", "The Hunger Games: Mockingjay, Part 2", "7 Days in Havana", "The Hunger Games: Mockingjay, Part 1", "The Long Home", "Firehouse Dog"], "ans_acc": 0.0625, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14925373134328357, "path_precision": 0.5, "path_recall": 0.08771929824561403, "path_ans_f1": 0.1111111111111111, "path_ans_precision": 0.5, "path_ans_recall": 0.0625}
{"id": "WebQTest-1304", "prediction": ["# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.place_founded -> Nazi Germany\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.founders -> Heinrich Himmler\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> base.schemastaging.context_name.pronunciation -> g.125_pt37m\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ng.1254zbtvv", "# Reasoning Path:\nAdolf Hitler -> government.politician.party -> m.0btmmq1 -> government.political_party_tenure.party -> German Workers' Party\n# Answer:\nm.0btmmq1", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> book.book_subject.works -> Waffen SS\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Schutzstaffel -> book.book_subject.works -> Heinrich Himmler\n# Answer:\nSchutzstaffel", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> book.book_subject.works -> Anne Frank and the Children of the Holocaust\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> government.politician.party -> m.075rkrk -> government.political_party_tenure.party -> Nazi Party\n# Answer:\nm.075rkrk", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Schutzstaffel -> organization.organization.child -> m.0w1p81d\n# Answer:\nSchutzstaffel"], "ground_truth": ["Nazi Party", "1st SS Panzer Division Leibstandarte SS Adolf Hitler", "Gestapo", "Hitler Youth", "Sturmabteilung", "Waffen-SS", "Schutzstaffel", "Wehrmacht"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.48837209302325574, "ans_precission": 0.7, "ans_recall": 0.375, "path_f1": 0.48837209302325574, "path_precision": 0.7, "path_recall": 0.375, "path_ans_f1": 0.5106382978723405, "path_ans_precision": 0.8, "path_ans_recall": 0.375}
{"id": "WebQTest-1305", "prediction": ["# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.0909kmj -> award.award_nomination.award -> Golden Globe Award for Best Actress \u2013 Motion Picture \u2013 Musical or Comedy\n# Answer:\nm.0909kmj", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.0909kq0 -> award.award_nomination.award -> Golden Globe Award for Best Actress \u2013 Motion Picture \u2013 Musical or Comedy\n# Answer:\nm.0909kq0", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.0909kmj -> award.award_nomination.nominated_for -> Bus Stop\n# Answer:\nm.0909kmj", "# Reasoning Path:\nMarilyn Monroe -> award.award_winner.awards_won -> m.07ynmx5 -> award.award_honor.award -> Golden Globe Award for Best Actress \u2013 Motion Picture \u2013 Musical or Comedy\n# Answer:\nm.07ynmx5", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.07zn09y -> award.award_nomination.award -> BAFTA Award for Best Foreign Actress\n# Answer:\nm.07zn09y", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.0909kq0 -> award.award_nomination.nominated_for -> Some Like It Hot\n# Answer:\nm.0909kq0", "# Reasoning Path:\nMarilyn Monroe -> award.award_winner.awards_won -> m.0m20gt1 -> award.award_honor.award -> Golden Globe Henrietta Award for World Film Favorites\n# Answer:\nm.0m20gt1", "# Reasoning Path:\nMarilyn Monroe -> award.award_winner.awards_won -> m.07ynmx5 -> award.award_honor.ceremony -> 17th Golden Globe Awards\n# Answer:\nm.07ynmx5", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.0909kq0 -> award.award_nomination.ceremony -> 17th Golden Globe Awards\n# Answer:\n17th Golden Globe Awards", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.07zn09y -> award.award_nomination.ceremony -> 9th British Academy Film Awards\n# Answer:\nm.07zn09y"], "ground_truth": ["Playboy Playmate", "Golden Globe Henrietta Award for World Film Favorites", "Golden Globe Award for Best Actress \u2013 Motion Picture \u2013 Musical or Comedy", "David di Donatello Golden Plate Award"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4000000000000001, "path_precision": 0.4, "path_recall": 0.4, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.4, "path_ans_recall": 0.5}
{"id": "WebQTest-1306", "prediction": ["# Reasoning Path:\nIran -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Iraq\n# Answer:\nParliamentary system", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal_per_capita -> g.11b60rrlgr\n# Answer:\ng.11b60rrlgr", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal -> g.11bbwyx0z8\n# Answer:\ng.11bbwyx0z8", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6fdrh\n# Answer:\ng.12tb6fdrh", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6fls_\n# Answer:\ng.12tb6fls_", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal -> g.11bbwz0bn1\n# Answer:\ng.11bbwz0bn1", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal -> g.11bbwz7d2q\n# Answer:\ng.11bbwz7d2q"], "ground_truth": ["Presidential system", "Parliamentary system", "Unitary state", "Theocracy", "Islamic republic"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.16666666666666666, "ans_precission": 0.14285714285714285, "ans_recall": 0.2, "path_f1": 0.16666666666666666, "path_precision": 0.14285714285714285, "path_recall": 0.2, "path_ans_f1": 0.16666666666666666, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 0.2}
{"id": "WebQTest-1307", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nm.0lwxmy1", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> University of Alabama School of Law\n# Answer:\nm.0n1l46h", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.place_of_birth -> Los Angeles\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138 -> education.education.institution -> University of Alabama\n# Answer:\nm.04hx138", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.gender -> Female\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_types -> Book Edition\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_for -> g.125920htw\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Matar Un Ruisenor -> book.book_edition.book -> To Kill a Mockingbird\n# Answer:\nMatar Un Ruisenor", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nHarper Seven Beckham"], "ground_truth": ["Monroe County High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1308", "prediction": ["# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0n1m7cd -> education.education.institution -> University of Oxford\n# Answer:\nm.0n1m7cd", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.02wmyw7 -> education.education.institution -> Exeter College, Oxford\n# Answer:\nm.02wmyw7", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0w48bvz -> education.education.institution -> King Edward's School, Birmingham\n# Answer:\nm.0w48bvz", "# Reasoning Path:\nJ. R. R. Tolkien -> book.author.works_written -> A Middle English Reader and Vocabulary -> book.written_work.author -> Kenneth Sisam\n# Answer:\nA Middle English Reader and Vocabulary", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0n1m7cd -> freebase.valuenotation.has_value -> End Date\n# Answer:\nm.0n1m7cd", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.02wmyw7 -> education.education.degree -> First Class Honours\n# Answer:\nm.02wmyw7", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.employment_history -> m.02ht_8b -> business.employment_tenure.company -> University of Oxford\n# Answer:\nm.02ht_8b", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0n1m7cd -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nm.0n1m7cd", "# Reasoning Path:\nJ. R. R. Tolkien -> book.author.works_written -> A Middle English Reader and Vocabulary -> common.topic.notable_for -> g.12550hfvd\n# Answer:\nA Middle English Reader and Vocabulary", "# Reasoning Path:\nJ. R. R. Tolkien -> book.author.works_written -> A Middle English Reader and Vocabulary -> common.topic.notable_types -> Book\n# Answer:\nA Middle English Reader and Vocabulary"], "ground_truth": ["University of Oxford", "Exeter College, Oxford"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1309", "prediction": ["# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.art_forms -> Painting -> base.descriptive_names.names.descriptive_name -> m.0109p390\n# Answer:\nPainting", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.art_forms -> Painting -> base.descriptive_names.names.descriptive_name -> m.0109p3lg\n# Answer:\nPainting", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.art_forms -> Painting -> interests.collection_category.collectors -> m.09vb4f2\n# Answer:\nPainting", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Eyewitness: Reports from an Art World in Crisis\n# Answer:\nModern art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nModern art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.art_forms -> Painting -> base.descriptive_names.names.descriptive_name -> m.0_z4x_g\n# Answer:\nPainting", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.art_forms -> Painting -> base.skosbase.vocabulary_equivalent_topic.narrower_concept -> Acrylic painting\n# Answer:\nPainting", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.art_forms -> Painting -> interests.collection_category.collectors -> m.09vb4m1\n# Answer:\nPainting", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Origins, Imitation, Conventions\n# Answer:\nModern art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.art_forms -> Painting -> interests.collection_category.collectors -> m.09vb57n\n# Answer:\nPainting"], "ground_truth": ["Painting"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-131", "prediction": ["# Reasoning Path:\nBarack Obama -> influence.influence_node.influenced_by -> Nipsey Russell -> influence.influence_node.influenced -> Michael Jackson\n# Answer:\nNipsey Russell", "# Reasoning Path:\nBarack Obama -> influence.influence_node.influenced_by -> Reinhold Niebuhr -> influence.influence_node.influenced -> Martin Luther King, Jr.\n# Answer:\nReinhold Niebuhr", "# Reasoning Path:\nBarack Obama -> influence.influence_node.influenced_by -> Nipsey Russell -> people.deceased_person.place_of_death -> New York City\n# Answer:\nNipsey Russell", "# Reasoning Path:\nBarack Obama -> influence.influence_node.influenced_by -> Nipsey Russell -> people.person.ethnicity -> African American\n# Answer:\nNipsey Russell", "# Reasoning Path:\nBarack Obama -> influence.influence_node.influenced_by -> Saul Alinsky -> common.topic.notable_types -> Author\n# Answer:\nSaul Alinsky", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.subjects -> Mitt Romney presidential campaign, 2012\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares", "# Reasoning Path:\nBarack Obama -> influence.influence_node.influenced_by -> Reinhold Niebuhr -> common.topic.notable_types -> Author\n# Answer:\nReinhold Niebuhr", "# Reasoning Path:\nBarack Obama -> people.person.ethnicity -> English American -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Michael Scott\n# Answer:\nEnglish American", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Bound Man -> book.written_work.author -> Shelby Steele\n# Answer:\nA Bound Man", "# Reasoning Path:\nBarack Obama -> influence.influence_node.influenced_by -> Reinhold Niebuhr -> people.person.gender -> Male\n# Answer:\nReinhold Niebuhr"], "ground_truth": ["Nipsey Russell", "Saul Alinsky", "Reinhold Niebuhr"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1310", "prediction": ["# Reasoning Path:\nFootball League Cup -> sports.sports_championship.events -> 2012 Football League Cup Final -> sports.sports_championship_event.champion -> Liverpool F.C.\n# Answer:\n2012 Football League Cup Final", "# Reasoning Path:\nFootball League Cup -> sports.sports_championship.events -> 2012 Football League Cup Final -> soccer.football_match.held_at -> Wembley Stadium\n# Answer:\n2012 Football League Cup Final", "# Reasoning Path:\nFootball League Cup -> sports.sports_championship.events -> 2012 Football League Cup Final -> freebase.valuenotation.is_reviewed -> Championship\n# Answer:\n2012 Football League Cup Final", "# Reasoning Path:\nFootball League Cup -> common.topic.notable_types -> Football Competition -> type.type.domain -> Soccer\n# Answer:\nFootball Competition", "# Reasoning Path:\nFootball League Cup -> sports.sports_championship.events -> 1965 Football League Cup Final -> common.topic.notable_for -> g.1255gcj1_\n# Answer:\n1965 Football League Cup Final", "# Reasoning Path:\nFootball League Cup -> sports.sports_championship.events -> 1964 Football League Cup Final -> sports.sports_championship_event.season -> 1963\u201364 Football League Cup\n# Answer:\n1964 Football League Cup Final", "# Reasoning Path:\nFootball League Cup -> common.topic.notable_types -> Football Competition -> freebase.type_profile.kind -> Definition\n# Answer:\nFootball Competition", "# Reasoning Path:\nFootball League Cup -> sports.sports_championship.events -> 2012 Football League Cup Final -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n2012 Football League Cup Final", "# Reasoning Path:\nFootball League Cup -> sports.sports_championship.events -> 1964 Football League Cup Final -> common.topic.notable_types -> Football Match\n# Answer:\n1964 Football League Cup Final", "# Reasoning Path:\nFootball League Cup -> sports.sports_championship.events -> 1965 Football League Cup Final -> common.topic.notable_types -> Football Match\n# Answer:\n1965 Football League Cup Final"], "ground_truth": ["Liverpool F.C."], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.13333333333333333, "path_precision": 0.1, "path_recall": 0.2, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1311", "prediction": ["# Reasoning Path:\nAntoni Gaud\u00ed -> influence.influence_node.influenced_by -> Eug\u00e8ne Viollet-le-Duc -> influence.influence_node.influenced -> Benjamin Bucknall\n# Answer:\nEug\u00e8ne Viollet-le-Duc", "# Reasoning Path:\nAntoni Gaud\u00ed -> influence.influence_node.influenced_by -> Eug\u00e8ne Viollet-le-Duc -> influence.influence_node.influenced -> Hector Guimard\n# Answer:\nEug\u00e8ne Viollet-le-Duc", "# Reasoning Path:\nAntoni Gaud\u00ed -> influence.influence_node.influenced_by -> William Morris -> influence.influence_node.influenced_by -> John Ruskin\n# Answer:\nWilliam Morris", "# Reasoning Path:\nAntoni Gaud\u00ed -> influence.influence_node.influenced_by -> Eug\u00e8ne Viollet-le-Duc -> influence.influence_node.influenced -> William Burges\n# Answer:\nEug\u00e8ne Viollet-le-Duc", "# Reasoning Path:\nAntoni Gaud\u00ed -> influence.influence_node.influenced_by -> Eug\u00e8ne Viollet-le-Duc -> base.kwebbase.kwtopic.connections_to -> gothic revival style used by eugene-emmanuel viollet-le-duc\n# Answer:\nEug\u00e8ne Viollet-le-Duc", "# Reasoning Path:\nAntoni Gaud\u00ed -> influence.influence_node.influenced_by -> Eug\u00e8ne Viollet-le-Duc -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nEug\u00e8ne Viollet-le-Duc", "# Reasoning Path:\nAntoni Gaud\u00ed -> influence.influence_node.influenced_by -> Eug\u00e8ne Viollet-le-Duc -> base.kwebbase.kwtopic.connections_to -> prosper merimee assisted by eugene-emmanuel viollet-le-duc\n# Answer:\nEug\u00e8ne Viollet-le-Duc", "# Reasoning Path:\nAntoni Gaud\u00ed -> influence.influence_node.influenced_by -> Eug\u00e8ne Viollet-le-Duc -> freebase.valuenotation.is_reviewed -> Date of death\n# Answer:\nEug\u00e8ne Viollet-le-Duc", "# Reasoning Path:\nAntoni Gaud\u00ed -> influence.influence_node.influenced_by -> William Morris -> base.kwebbase.kwtopic.connections_from -> william morris a pal of alphonse mucha\n# Answer:\nWilliam Morris", "# Reasoning Path:\nAntoni Gaud\u00ed -> architecture.architect.structures_designed -> Artigas Gardens -> location.location.containedby -> Barcelona\n# Answer:\nArtigas Gardens"], "ground_truth": ["Eug\u00e8ne Viollet-le-Duc", "William Morris"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1312", "prediction": ["# Reasoning Path:\nDanish Language -> language.human_language.countries_spoken_in -> Greenland -> location.administrative_division.country -> Denmark\n# Answer:\nGreenland", "# Reasoning Path:\nDanish Language -> language.human_language.countries_spoken_in -> Denmark -> location.country.administrative_divisions -> Greenland\n# Answer:\nDenmark", "# Reasoning Path:\nDanish Language -> language.human_language.countries_spoken_in -> Denmark -> location.location.containedby -> Europe\n# Answer:\nDenmark", "# Reasoning Path:\nDanish Language -> language.human_language.main_country -> Denmark -> location.location.containedby -> Europe\n# Answer:\nDenmark", "# Reasoning Path:\nDanish Language -> language.human_language.countries_spoken_in -> Denmark -> location.location.containedby -> Northern Europe\n# Answer:\nDenmark", "# Reasoning Path:\nDanish Language -> language.human_language.countries_spoken_in -> Denmark -> location.location.containedby -> Scandinavia\n# Answer:\nDenmark", "# Reasoning Path:\nDanish Language -> language.human_language.main_country -> Denmark -> location.location.containedby -> Northern Europe\n# Answer:\nDenmark", "# Reasoning Path:\nDanish Language -> language.human_language.main_country -> Denmark -> location.location.containedby -> Scandinavia\n# Answer:\nDenmark", "# Reasoning Path:\nDanish Language -> language.human_language.main_country -> Denmark -> location.country.languages_spoken -> Faroese\n# Answer:\nDenmark", "# Reasoning Path:\nDanish Language -> media_common.netflix_genre.titles -> R -> media_common.netflix_title.netflix_genres -> Drama\n# Answer:\nR"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1313", "prediction": ["# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.children -> Persephone -> fictional_universe.fictional_character.parents -> Zeus\n# Answer:\nPersephone", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.children -> Persephone -> common.topic.article -> m.08nkrwy\n# Answer:\nPersephone", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.children -> Persephone -> film.film_character.portrayed_in_films -> m.08nknvb\n# Answer:\nPersephone", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.siblings -> m.0j85m5t -> fictional_universe.sibling_relationship_of_fictional_characters.siblings -> Zeus\n# Answer:\nm.0j85m5t", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.children -> Persephone -> film.film_character.portrayed_in_films -> m.0bvrrhh\n# Answer:\nPersephone", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.siblings -> m.0gwhv5j -> fictional_universe.sibling_relationship_of_fictional_characters.siblings -> Zeus\n# Answer:\nm.0gwhv5j", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.gender -> Female -> medicine.risk_factor.diseases -> Acne\n# Answer:\nFemale", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.children -> Persephone -> film.film_character.portrayed_in_films -> m.0cs1kzn\n# Answer:\nPersephone", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.gender -> Female -> medicine.risk_factor.diseases -> Acrodermatitis chronica atrophicans\n# Answer:\nFemale", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.gender -> Female -> base.gender.gender_identity.people -> m.02_98hp\n# Answer:\nFemale"], "ground_truth": ["Zeus"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1314", "prediction": ["# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel Ritz Paris -> common.topic.image -> Hotel Ritz Paris\n# Answer:\nH\u00f4tel Ritz Paris", "# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel de Crillon -> common.topic.webpage -> m.0g5c9ry\n# Answer:\nH\u00f4tel de Crillon", "# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel Ritz Paris -> common.topic.image -> The Ritz garden caf\u00e9 by the Swiss artist, Pierre-Georges Jeanniot (1848-1934)\n# Answer:\nH\u00f4tel Ritz Paris", "# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel Ritz Paris -> architecture.structure.architect -> Bernard Gaucherel\n# Answer:\nH\u00f4tel Ritz Paris", "# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel de Crillon -> common.topic.image -> H\u00c3\u00b4tel de Crillon 25 08 2007\n# Answer:\nH\u00f4tel de Crillon", "# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel de Crillon -> common.topic.notable_for -> g.1259xm9y4\n# Answer:\nH\u00f4tel de Crillon", "# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel Ritz Paris -> architecture.structure.architect -> Charles Mew\u00e8s\n# Answer:\nH\u00f4tel Ritz Paris", "# Reasoning Path:\nParis -> common.topic.webpage -> m.02k8dbw -> common.webpage.category -> Topic Webpage\n# Answer:\nm.02k8dbw", "# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel Ritz Paris -> architecture.structure.architect -> Jules Hardouin-Mansart\n# Answer:\nH\u00f4tel Ritz Paris", "# Reasoning Path:\nParis -> common.topic.webpage -> m.02k8dbw -> common.webpage.resource -> m.0bkwkw7\n# Answer:\nm.02k8dbw"], "ground_truth": ["H\u00f4tel de Crillon", "H\u00f4tel Ritz Paris"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1316", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Anne Harris\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger -> people.deceased_person.place_of_burial -> Granary Burying Ground\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> people.marriage.type_of_union -> Common-law marriage\n# Answer:\nm.0j4kb46", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Ebenezer Franklin\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.place_of_birth -> Ecton\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Elizabeth Douse\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.spouse_s -> m.0j4k0mt\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> people.marriage.spouse -> Deborah Read\n# Answer:\nm.0j4kb46", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger -> people.person.parents -> Mary Morrill\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> freebase.valuenotation.is_reviewed -> Spouse\n# Answer:\nm.0j4kb46"], "ground_truth": ["Deborah Read"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1317", "prediction": ["# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> periodicals.newspaper_circulation_area.newspapers -> P\u00fablico\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> base.aareas.schema.administrative_area.administrative_parent -> Community of Madrid\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\ng.1hhc37x4q", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> periodicals.newspaper_circulation_area.newspapers -> ADN\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60xs9d2\n# Answer:\ng.11b60xs9d2", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> periodicals.newspaper_circulation_area.newspapers -> Diario AS\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc38hhk\n# Answer:\ng.1hhc38hhk", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc3dvnk\n# Answer:\ng.1hhc3dvnk", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.1hhc39shg\n# Answer:\ng.1hhc39shg", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.1hhc3h715\n# Answer:\ng.1hhc3h715"], "ground_truth": ["Catalonia"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1319", "prediction": ["# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> baseball.baseball_player.batting_stats -> m.06sbxkk -> baseball.batting_statistics.team -> Detroit Tigers\n# Answer:\nm.06sbxkk", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> sports.pro_athlete.teams -> m.0z9w091 -> sports.sports_team_roster.team -> Detroit Tigers\n# Answer:\nm.0z9w091", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> baseball.baseball_player.batting_stats -> m.06sbxlb -> baseball.batting_statistics.team -> Detroit Tigers\n# Answer:\nm.06sbxlb", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> sports.pro_athlete.teams -> m.0ywy2gy -> sports.sports_team_roster.team -> Texas Rangers\n# Answer:\nm.0ywy2gy", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> sports.pro_athlete.teams -> m.0z9w091 -> sports.sports_team_roster.position -> Catcher\n# Answer:\nm.0z9w091", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> sports.pro_athlete.teams -> m.0z9w064 -> sports.sports_team_roster.team -> Miami Marlins\n# Answer:\nm.0z9w064", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> baseball.baseball_player.batting_stats -> m.06sbxkk -> baseball.batting_statistics.season -> 2004 Major League Baseball season\n# Answer:\nm.06sbxkk", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> baseball.baseball_player.batting_stats -> m.06sbx7d -> baseball.batting_statistics.team -> Texas Rangers\n# Answer:\nm.06sbx7d", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> sports.pro_athlete.teams -> m.0ywy2gy -> sports.sports_team_roster.position -> Catcher\n# Answer:\nm.0ywy2gy", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> baseball.baseball_player.batting_stats -> m.06sbxlb -> baseball.batting_statistics.season -> 2005 Major League Baseball season\n# Answer:\nm.06sbxlb"], "ground_truth": ["Detroit Tigers", "Miami Marlins", "Texas Rangers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3870967741935483, "path_precision": 0.6, "path_recall": 0.2857142857142857, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-132", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> common.topic.notable_types -> Person\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.nationality -> United States of America\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Esm\u00e9 Annabelle Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nEsm\u00e9 Annabelle Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.spouse -> Tracy Pollan\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.location_of_ceremony -> Arlington\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.profession -> Actor\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.sibling_s -> m.0j217jw\n# Answer:\nSam Michael Fox"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1320", "prediction": ["# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.arena_stadium -> Miller Park -> base.playball.baseball_stadium.naming_rights -> m.05lm6s0\n# Answer:\nMiller Park", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.arena_stadium -> Miller Park -> sports.sports_facility.home_venue_for -> m.0wz1z23\n# Answer:\nMiller Park", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.arena_stadium -> Miller Park -> architecture.structure.architecture_firm -> Eppstein Uhen Architects\n# Answer:\nMiller Park", "# Reasoning Path:\nMilwaukee Brewers -> base.schemastaging.organization_extra.contact_webpages -> m.010dwkgw -> internet.localized_uri.location -> United States of America\n# Answer:\nm.010dwkgw", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.arena_stadium -> Miller Park -> architecture.structure.architecture_firm -> HKS, Inc.\n# Answer:\nMiller Park", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.arena_stadium -> Miller Park -> architecture.structure.architecture_firm -> NBBJ\n# Answer:\nMiller Park", "# Reasoning Path:\nMilwaukee Brewers -> base.schemastaging.organization_extra.contact_webpages -> m.010dwkgw -> internet.localized_uri.language -> English\n# Answer:\nm.010dwkgw", "# Reasoning Path:\nMilwaukee Brewers -> baseball.baseball_team.team_stats -> m.05n61_w -> baseball.baseball_team_stats.season -> 1977 Major League Baseball Season\n# Answer:\nm.05n61_w", "# Reasoning Path:\nMilwaukee Brewers -> baseball.baseball_team.team_stats -> m.05n61hh -> baseball.baseball_team_stats.season -> 1970 Major League Baseball Season\n# Answer:\nm.05n61hh", "# Reasoning Path:\nMilwaukee Brewers -> baseball.baseball_team.team_stats -> m.05n61lk -> baseball.baseball_team_stats.season -> 1971 Major League Baseball Season\n# Answer:\nm.05n61lk"], "ground_truth": ["Miller Park"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1321", "prediction": ["# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> film.film.country -> Australia\n# Answer:\nGallipoli", "# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> film.film.country -> Turkey\n# Answer:\nGallipoli", "# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> film.film.release_date_s -> m.04mvn_0\n# Answer:\nGallipoli", "# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> location.location.events -> Battle for No.3 Post\n# Answer:\nGallipoli", "# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> film.film.release_date_s -> m.04mvnyd\n# Answer:\nGallipoli", "# Reasoning Path:\nGallipoli Campaign -> military.military_conflict.military_personnel_involved -> Alec Campbell -> military.military_person.participated_in_conflicts -> World War I\n# Answer:\nAlec Campbell", "# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> film.film.release_date_s -> m.04mvnyq\n# Answer:\nGallipoli", "# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> location.location.events -> Battle of Chunuk Bair\n# Answer:\nGallipoli", "# Reasoning Path:\nGallipoli Campaign -> military.military_conflict.force_strengths -> m.04kc3p1 -> military.force_strength.combatant -> Ottoman Empire\n# Answer:\nm.04kc3p1", "# Reasoning Path:\nGallipoli Campaign -> military.military_conflict.military_personnel_involved -> Albert Jacka -> people.person.nationality -> Australia\n# Answer:\nAlbert Jacka"], "ground_truth": ["Gallipoli"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1322", "prediction": ["# Reasoning Path:\nTom Hardy -> film.actor.film -> g.11b77pmp46\n# Answer:\ng.11b77pmp46", "# Reasoning Path:\nTom Hardy -> film.actor.film -> m.010s_rgp -> film.performance.film -> Legend\n# Answer:\nm.010s_rgp", "# Reasoning Path:\nTom Hardy -> film.actor.film -> m.010s_szd -> film.performance.film -> Legend\n# Answer:\nm.010s_szd", "# Reasoning Path:\nTom Hardy -> freebase.valuenotation.has_no_value -> Siblings -> rdf-schema#domain -> Person\n# Answer:\nSiblings", "# Reasoning Path:\nTom Hardy -> award.award_nominee.award_nominations -> m.0dlskzm -> award.award_nomination.nominated_for -> Inception\n# Answer:\nm.0dlskzm", "# Reasoning Path:\nTom Hardy -> freebase.valuenotation.has_no_value -> Siblings -> type.property.master_property -> Sibling\n# Answer:\nSiblings", "# Reasoning Path:\nTom Hardy -> film.actor.film -> m.010s_rgp -> film.performance.character -> Ronald Kray\n# Answer:\nm.010s_rgp", "# Reasoning Path:\nTom Hardy -> film.actor.film -> m.010s_szd -> film.performance.character -> Reginald Kray\n# Answer:\nm.010s_szd", "# Reasoning Path:\nTom Hardy -> freebase.valuenotation.has_no_value -> Siblings -> type.property.schema -> Person\n# Answer:\nSiblings", "# Reasoning Path:\nTom Hardy -> award.award_nominee.award_nominations -> m.0hjbfqm -> award.award_nomination.nominated_for -> Warrior\n# Answer:\nm.0hjbfqm"], "ground_truth": ["Warrior", "Deserter", "Layer Cake", "Child 44", "The Revenant", "Sucker Punch", "Flood", "London Road", "Everest", "Star Trek Nemesis", "RocknRolla", "Black Hawk Down", "Inception", "Scenes of a Sexual Nature", "Lethal Dose", "Thick as Thieves", "This Means War", "Minotaur", "EMR", "The Drop", "Marie Antoinette", "The Outsider", "Legend", "Perfect", "Locke", "Dot the I", "Gideon's Daughter", "The Inheritance", "W\u0394Z", "The Reckoning", "Mad Max: Fury Road", "Splinter Cell", "Colditz", "Sergeant Slaughter, My Big Brother", "The Dark Knight Rises", "Lawless", "Bronson", "The Virgin Queen", "Sweeney Todd", "Tinker Tailor Soldier Spy"], "ans_acc": 0.075, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.11940298507462686, "path_precision": 0.4, "path_recall": 0.07017543859649122, "path_ans_f1": 0.1263157894736842, "path_ans_precision": 0.4, "path_ans_recall": 0.075}
{"id": "WebQTest-1323", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> common.topic.notable_types -> Zoo\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Turner Field -> sports.sports_facility.teams -> Atlanta Braves\n# Answer:\nTurner Field", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Men's rings\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.geolocation -> m.0clwfck\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Turner Field -> common.topic.article -> m.020fp8\n# Answer:\nTurner Field", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> travel.transport_terminus.travel_destinations_served -> m.052zwlt\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Women's artistic team all-around\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Turner Field -> location.location.containedby -> Georgia\n# Answer:\nTurner Field"], "ground_truth": ["Omni Coliseum", "Cobb Energy Performing Arts Centre", "Georgia Dome", "Atlanta Symphony Orchestra", "Center for Puppetry Arts", "Georgia State Capitol", "World of Coca-Cola", "Six Flags Over Georgia", "Jimmy Carter Library and Museum", "Six Flags White Water", "Centennial Olympic Park", "Margaret Mitchell House & Museum", "Masquerade", "Georgia Aquarium", "Atlanta Ballet", "Woodruff Arts Center", "Zoo Atlanta", "Atlanta Cyclorama & Civil War Museum", "CNN Center", "Variety Playhouse", "Atlanta History Center", "Fernbank Museum of Natural History", "Fox Theatre", "Peachtree Road Race", "Fernbank Science Center", "Atlanta Jewish Film Festival", "Hyatt Regency Atlanta", "Georgia World Congress Center", "Underground Atlanta", "Martin Luther King, Jr. National Historic Site", "Arbor Place Mall", "Turner Field", "Philips Arena", "The Tabernacle", "Atlanta Marriott Marquis", "Four Seasons Hotel Atlanta"], "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0.15254237288135591, "ans_precission": 0.9, "ans_recall": 0.08333333333333333, "path_f1": 0.15254237288135591, "path_precision": 0.9, "path_recall": 0.08333333333333333, "path_ans_f1": 0.1978021978021978, "path_ans_precision": 0.9, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-1324", "prediction": ["# Reasoning Path:\nJohn Forbes Nash, Jr. -> people.person.education -> m.03kwg72 -> education.education.institution -> Carnegie Mellon College of Engineering\n# Answer:\nm.03kwg72", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> people.person.education -> m.02kq1bj -> education.education.institution -> Princeton University\n# Answer:\nm.02kq1bj", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> people.person.education -> m.03kwg72 -> education.education.major_field_of_study -> Mathematics\n# Answer:\nm.03kwg72", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> people.person.education -> m.03kwg72 -> education.education.degree -> Master of Science\n# Answer:\nm.03kwg72", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> people.person.education -> m.0n0rj5w -> education.education.institution -> Bluefield High School\n# Answer:\nm.0n0rj5w", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> people.person.education -> m.02kq1bj -> education.education.degree -> Doctorate\n# Answer:\nm.02kq1bj", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> education.academic.advisors -> Albert W. Tucker -> people.person.education -> m.02kq0xy\n# Answer:\nAlbert W. Tucker", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> book.author.works_written -> Non-cooperative Games -> education.dissertation.institution -> Princeton University\n# Answer:\nNon-cooperative Games", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> book.author.works_written -> Non-cooperative Games -> education.dissertation.degree -> Doctorate\n# Answer:\nNon-cooperative Games", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> education.academic.advisors -> Albert W. Tucker -> people.person.education -> m.05k7m3x\n# Answer:\nAlbert W. Tucker"], "ground_truth": ["Carnegie Mellon College of Engineering", "Princeton University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4, "path_precision": 0.3, "path_recall": 0.6, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1325", "prediction": ["# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Botswana\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Cyprus\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.main_country -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> Maltese Language -> language.human_language.region -> Europe\n# Answer:\nMaltese Language", "# Reasoning Path:\nMalta -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Botswana\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.statistical_region.deposit_interest_rate -> g.1hhc37h6m\n# Answer:\ng.1hhc37h6m", "# Reasoning Path:\nMalta -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Cyprus\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.main_country -> Kingdom of Great Britain\n# Answer:\nEnglish Language"], "ground_truth": ["Maltese Language", "English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1326", "prediction": ["# Reasoning Path:\nThomas Edison -> base.argumentmaps.innovator.original_ideas -> Phonograph -> common.topic.article -> m.063mq\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Phonograph -> common.topic.article -> m.063mq\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> base.argumentmaps.innovator.original_ideas -> Phonograph -> base.argumentmaps.original_idea.innovator -> Eldridge R. Johnson\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> base.argumentmaps.innovator.original_ideas -> Phonograph -> law.invention.inventor -> Eldridge R. Johnson\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Carbon microphone -> common.topic.article -> m.0d9fl1\n# Answer:\nCarbon microphone", "# Reasoning Path:\nThomas Edison -> common.image.size -> m.01xjmzn\n# Answer:\nm.01xjmzn", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Phonograph -> base.argumentmaps.original_idea.innovator -> Eldridge R. Johnson\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Movie camera -> common.topic.notable_types -> Invention\n# Answer:\nMovie camera", "# Reasoning Path:\nThomas Edison -> base.argumentmaps.innovator.original_ideas -> Phonograph -> law.invention.inventor -> Emile Berliner\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Phonograph -> law.invention.inventor -> Eldridge R. Johnson\n# Answer:\nPhonograph"], "ground_truth": ["Mimeograph", "Carbon microphone", "Movie camera", "Phonograph cylinder", "Electric Power Distribution", "Quadruplex telegraph", "Phonograph", "Incandescent light bulb"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.5294117647058825, "ans_precission": 0.9, "ans_recall": 0.375, "path_f1": 0.42857142857142855, "path_precision": 0.5, "path_recall": 0.375, "path_ans_f1": 0.5294117647058825, "path_ans_precision": 0.9, "path_ans_recall": 0.375}
{"id": "WebQTest-1327", "prediction": ["# Reasoning Path:\nShakira -> people.person.place_of_birth -> Barranquilla -> location.location.containedby -> Colombia\n# Answer:\nBarranquilla", "# Reasoning Path:\nShakira -> people.person.nationality -> Colombia -> location.country.languages_spoken -> Spanish Language\n# Answer:\nColombia", "# Reasoning Path:\nShakira -> people.person.nationality -> Colombia -> common.topic.notable_types -> Country\n# Answer:\nColombia", "# Reasoning Path:\nShakira -> people.person.nationality -> Colombia -> location.country.official_language -> Spanish Language\n# Answer:\nColombia", "# Reasoning Path:\nShakira -> tv.tv_actor.guest_roles -> m.09nb6_1 -> tv.tv_guest_role.episodes_appeared_in -> and then there were 9...\n# Answer:\nm.09nb6_1", "# Reasoning Path:\nShakira -> tv.tv_actor.guest_roles -> m.09nb6_6 -> tv.tv_guest_role.episodes_appeared_in -> Episode 904A\n# Answer:\nm.09nb6_6"], "ground_truth": ["Colombia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-1328", "prediction": ["# Reasoning Path:\nRutherford B. Hayes -> people.person.profession -> Politician -> common.topic.notable_types -> Profession\n# Answer:\nPolitician", "# Reasoning Path:\nRutherford B. Hayes -> people.person.profession -> Lawyer -> common.topic.notable_types -> Profession\n# Answer:\nLawyer", "# Reasoning Path:\nRutherford B. Hayes -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdn_\n# Answer:\nPolitician", "# Reasoning Path:\nRutherford B. Hayes -> people.person.profession -> Politician -> people.profession.specializations -> AIDS Policy Specialist\n# Answer:\nPolitician", "# Reasoning Path:\nRutherford B. Hayes -> common.topic.notable_types -> US President -> freebase.type_profile.kind -> Significant\n# Answer:\nUS President", "# Reasoning Path:\nRutherford B. Hayes -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdq3\n# Answer:\nPolitician", "# Reasoning Path:\nRutherford B. Hayes -> people.person.profession -> Lawyer -> base.descriptive_names.names.descriptive_name -> m.0101pjk9\n# Answer:\nLawyer", "# Reasoning Path:\nRutherford B. Hayes -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101h_l2\n# Answer:\nPolitician", "# Reasoning Path:\nRutherford B. Hayes -> people.person.profession -> Politician -> people.profession.specializations -> Advocacy\n# Answer:\nPolitician", "# Reasoning Path:\nRutherford B. Hayes -> common.topic.notable_types -> US President -> freebase.type_profile.kind -> Title\n# Answer:\nUS President"], "ground_truth": ["US President"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1329", "prediction": ["# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.02vc8t6 -> film.performance.actor -> Ian Holm\n# Answer:\nm.02vc8t6", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> film.film.starring -> m.02vc8t6 -> film.performance.actor -> Ian Holm\n# Answer:\nm.02vc8t6", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.03l6qv_ -> film.performance.actor -> Ian Holm\n# Answer:\nm.03l6qv_", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.0gwn5hf -> film.performance.actor -> Ian Holm\n# Answer:\nm.0gwn5hf", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.02vc8t6 -> film.performance.film -> The Lord of the Rings: The Fellowship of the Ring\n# Answer:\nm.02vc8t6", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> film.film.starring -> m.02vc6p4 -> film.performance.actor -> Dominic Monaghan\n# Answer:\nm.02vc6p4", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> film.film.starring -> m.02vc8t6 -> film.performance.character -> Old Bilbo\n# Answer:\nm.02vc8t6", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> film.film.starring -> m.02vc8d8 -> film.performance.actor -> Billy Boyd\n# Answer:\nm.02vc8d8", "# Reasoning Path:\nOld Bilbo -> fictional_universe.fictional_character.powers_or_abilities -> Invisibility -> book.book_subject.works -> Memoirs of an Invisible Man\n# Answer:\nInvisibility", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.03l6qv_ -> film.performance.film -> The Lord of the Rings: The Return of the King\n# Answer:\nm.03l6qv_"], "ground_truth": ["Ian Holm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-133", "prediction": ["# Reasoning Path:\nDolly Parton -> music.artist.origin -> Sevierville -> location.location.people_born_here -> Avie Lee Owens\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> music.artist.origin -> Sevierville -> location.location.people_born_here -> Randy Parton\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> music.artist.origin -> Sevierville -> location.location.containedby -> Tennessee\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> music.artist.origin -> Sevierville -> location.location.people_born_here -> Stella Parton\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> people.person.place_of_birth -> Sevierville -> location.location.people_born_here -> Avie Lee Owens\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> music.artist.origin -> Sevierville -> location.location.containedby -> Sevier County\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> people.person.place_of_birth -> Sevierville -> location.location.people_born_here -> Randy Parton\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> people.person.place_of_birth -> Sevierville -> location.location.containedby -> Tennessee\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> people.person.place_of_birth -> Sevierville -> location.location.people_born_here -> Stella Parton\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> people.person.place_of_birth -> Sevierville -> location.location.containedby -> Sevier County\n# Answer:\nSevierville"], "ground_truth": ["Sevierville"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1330", "prediction": ["# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories -> location.country.first_level_divisions -> American Samoa\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories -> location.country.first_level_divisions -> Guam\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories -> base.aareas.schema.administrative_area.administrative_children -> American Samoa\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories -> location.country.first_level_divisions -> Northern Mariana Islands\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories -> base.aareas.schema.administrative_area.administrative_children -> Guam\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nUnited States of America -> location.location.containedby -> DVD Region 1 -> location.location.contains -> Canada\n# Answer:\nDVD Region 1", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\ng.125_r6z32", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories -> base.aareas.schema.administrative_area.administrative_children -> Northern Mariana Islands\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nUnited States of America -> location.location.containedby -> Americas -> location.location.time_zones -> Alaska Time Zone\n# Answer:\nAmericas", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nm.010r7f0z"], "ground_truth": ["New York"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1331", "prediction": ["# Reasoning Path:\nArizona -> government.political_district.representatives -> m.09w0zkh -> government.government_position_held.office_holder -> Barry Goldwater\n# Answer:\nm.09w0zkh", "# Reasoning Path:\nArizona -> government.political_district.representatives -> m.09w120p -> government.government_position_held.office_holder -> Henry F. Ashurst\n# Answer:\nm.09w120p", "# Reasoning Path:\nArizona -> government.political_district.representatives -> m.09w0zkh -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nm.09w0zkh", "# Reasoning Path:\nArizona -> government.political_district.representatives -> m.09w11bv -> government.government_position_held.office_holder -> Barry Goldwater\n# Answer:\nm.09w11bv", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y42 -> government.government_position_held.office_holder -> Ernest McFarland\n# Answer:\nm.04j8y42", "# Reasoning Path:\nArizona -> government.political_district.representatives -> m.09w120p -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nm.09w120p", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.0108gfng -> government.government_position_held.office_holder -> Keith Brown\n# Answer:\nm.0108gfng", "# Reasoning Path:\nArizona -> government.political_district.representatives -> m.09w11bv -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nm.09w11bv", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.010dylrt -> government.government_position_held.office_holder -> Terry Goddard\n# Answer:\nm.010dylrt", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y42 -> government.government_position_held.office_position_or_title -> Governor of Arizona\n# Answer:\nm.04j8y42"], "ground_truth": ["Ralph H. Cameron", "Ernest McFarland", "Paul Fannin", "Marcus A. Smith", "Barry Goldwater", "Carl Hayden", "Henry F. Ashurst", "John McCain", "Jeff Flake", "Jon Kyl", "Dennis DeConcini"], "ans_acc": 0.2727272727272727, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3333333333333333, "path_precision": 0.4, "path_recall": 0.2857142857142857, "path_ans_f1": 0.3243243243243243, "path_ans_precision": 0.4, "path_ans_recall": 0.2727272727272727}
{"id": "WebQTest-1332", "prediction": ["# Reasoning Path:\nVietnam War -> time.event.locations -> Cambodia -> location.location.events -> Cambodian\u2013Vietnamese War\n# Answer:\nCambodia", "# Reasoning Path:\nVietnam War -> time.event.locations -> Cambodia -> location.location.events -> Cambodian Campaign\n# Answer:\nCambodia", "# Reasoning Path:\nVietnam War -> time.event.locations -> Cambodia -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nCambodia", "# Reasoning Path:\nVietnam War -> time.event.locations -> Laos -> location.location.events -> Joint warfare in South Vietnam, 1963\u201369\n# Answer:\nLaos", "# Reasoning Path:\nVietnam War -> time.event.locations -> Cambodia -> location.location.events -> Cambodian Civil War\n# Answer:\nCambodia", "# Reasoning Path:\nVietnam War -> time.event.locations -> Cambodia -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\nCambodia", "# Reasoning Path:\nVietnam War -> time.event.locations -> Laos -> location.location.events -> Operation Pony Express\n# Answer:\nLaos", "# Reasoning Path:\nVietnam War -> time.event.locations -> Laos -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\nLaos", "# Reasoning Path:\nVietnam War -> time.event.included_in_event -> 1960s -> time.event.includes_event -> 1960\n# Answer:\n1960s", "# Reasoning Path:\nVietnam War -> time.event.locations -> Laos -> location.location.containedby -> Asia\n# Answer:\nLaos"], "ground_truth": ["1975-04-30"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1333", "prediction": ["# Reasoning Path:\nTennessee River -> geography.river.mouth -> Ohio River -> geography.river.mouth -> Cairo\n# Answer:\nOhio River", "# Reasoning Path:\nTennessee River -> geography.river.mouth -> Ohio River -> location.location.partially_containedby -> Ohio\n# Answer:\nOhio River", "# Reasoning Path:\nTennessee River -> geography.river.mouth -> Ohio River -> geography.river.mouth -> Mississippi River\n# Answer:\nOhio River", "# Reasoning Path:\nTennessee River -> geography.river.mouth -> Ohio River -> location.location.containedby -> North America\n# Answer:\nOhio River", "# Reasoning Path:\nTennessee River -> geography.river.mouth -> Ohio River -> location.location.partially_containedby -> Kentucky\n# Answer:\nOhio River", "# Reasoning Path:\nTennessee River -> geography.river.basin_countries -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nTennessee River -> geography.river.mouth -> Ohio River -> location.location.containedby -> United States of America\n# Answer:\nOhio River", "# Reasoning Path:\nTennessee River -> geography.river.basin_countries -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Kentucky\n# Answer:\nUnited States of America", "# Reasoning Path:\nTennessee River -> location.location.containedby -> North America -> location.location.time_zones -> Central Time Zone\n# Answer:\nNorth America", "# Reasoning Path:\nTennessee River -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America"], "ground_truth": ["Ohio River"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1334", "prediction": ["# Reasoning Path:\nApollo 13 -> film.film.story_by -> Jim Lovell -> freebase.valuenotation.has_value -> Parents\n# Answer:\nJim Lovell", "# Reasoning Path:\nTom Hanks -> film.actor.film -> g.11b70lbbvb\n# Answer:\ng.11b70lbbvb", "# Reasoning Path:\nApollo 13 -> film.film.story_by -> Jim Lovell -> people.person.profession -> Actor\n# Answer:\nJim Lovell", "# Reasoning Path:\nTom Hanks -> award.award_winner.awards_won -> m.03mlq_x -> award.award_honor.honored_for -> Philadelphia\n# Answer:\nm.03mlq_x", "# Reasoning Path:\nApollo 13 -> film.film.story_by -> Jim Lovell -> freebase.valuenotation.is_reviewed -> Children\n# Answer:\nJim Lovell", "# Reasoning Path:\nApollo 13 -> film.film.starring -> m.02tbgwr -> film.performance.character -> Marilyn Lovell\n# Answer:\nm.02tbgwr", "# Reasoning Path:\nApollo 13 -> film.film.story_by -> Jim Lovell -> people.person.profession -> Writer\n# Answer:\nJim Lovell", "# Reasoning Path:\nTom Hanks -> award.award_winner.awards_won -> m.03mlq_x -> award.award_honor.award -> Academy Award for Best Actor\n# Answer:\nm.03mlq_x", "# Reasoning Path:\nApollo 13 -> film.film.story_by -> Jeffrey Kluger -> book.author.works_written -> Journey beyond Sele\u0304ne\u0304\n# Answer:\nJeffrey Kluger", "# Reasoning Path:\nApollo 13 -> film.film.starring -> m.02tbgwr -> film.performance.actor -> Kathleen Quinlan\n# Answer:\nm.02tbgwr"], "ground_truth": ["Jim Lovell"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1335", "prediction": ["# Reasoning Path:\nGhana -> location.country.languages_spoken -> Ga Language -> base.rosetta.languoid.local_name -> Ga\n# Answer:\nGa Language", "# Reasoning Path:\nGhana -> location.country.languages_spoken -> Ga Language -> common.topic.article -> m.01kphr\n# Answer:\nGa Language", "# Reasoning Path:\nGhana -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> India\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.languages_spoken -> Ga Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nGa Language", "# Reasoning Path:\nGhana -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Kenya\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.languages_spoken -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> India\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60ywwvy\n# Answer:\ng.11b60ywwvy", "# Reasoning Path:\nGhana -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Nigeria\n# Answer:\nEnglish Language"], "ground_truth": ["Dagbani Language", "Dangme Language", "Gonja Language", "Akan Language", "Asante dialect", "Fula language", "Dagaare language", "Ga Language", "Kasem Language", "English Language", "Nzema Language", "\u00c9w\u00e9 Language"], "ans_acc": 0.16666666666666666, "ans_hit": 1, "ans_f1": 0.28125, "ans_precission": 0.9, "ans_recall": 0.16666666666666666, "path_f1": 0.2692307692307692, "path_precision": 0.7, "path_recall": 0.16666666666666666, "path_ans_f1": 0.28125, "path_ans_precision": 0.9, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTest-1336", "prediction": ["# Reasoning Path:\nUtah State Capitol -> location.location.geolocation -> m.0cnlcc_\n# Answer:\nm.0cnlcc_", "# Reasoning Path:\nUtah -> base.aareas.schema.administrative_area.capital -> Salt Lake City -> location.statistical_region.population -> g.11b66h2b_k\n# Answer:\nSalt Lake City", "# Reasoning Path:\nUtah State Capitol -> location.location.containedby -> Salt Lake City -> location.location.containedby -> Utah\n# Answer:\nSalt Lake City", "# Reasoning Path:\nUtah -> base.aareas.schema.administrative_area.capital -> Salt Lake City -> location.statistical_region.population -> g.11b7tm7k1_\n# Answer:\nSalt Lake City", "# Reasoning Path:\nUtah -> base.aareas.schema.administrative_area.capital -> Salt Lake City -> tv.tv_location.tv_shows_filmed_here -> Proper Manors\n# Answer:\nSalt Lake City", "# Reasoning Path:\nUtah -> base.aareas.schema.administrative_area.capital -> Salt Lake City -> location.statistical_region.population -> g.11btt563rr\n# Answer:\nSalt Lake City", "# Reasoning Path:\nUtah -> base.aareas.schema.administrative_area.capital -> Salt Lake City -> location.citytown.postal_codes -> 84101\n# Answer:\nSalt Lake City", "# Reasoning Path:\nUtah -> base.aareas.schema.administrative_area.capital -> Salt Lake City -> tv.tv_location.tv_shows_filmed_here -> The Stand\n# Answer:\nSalt Lake City", "# Reasoning Path:\nUtah State Capitol -> location.location.containedby -> Salt Lake City -> location.statistical_region.population -> g.11b66h2b_k\n# Answer:\nSalt Lake City", "# Reasoning Path:\nUtah -> book.book_subject.works -> Blossoms of faith -> common.topic.notable_for -> g.125fjpyr7\n# Answer:\nBlossoms of faith"], "ground_truth": ["Salt Lake City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1337", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_trwk\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> Glass\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove -> common.topic.article -> m.029mn3\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_ty__\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> An Armonica\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> medicine.medical_treatment.used_to_treat -> Presbyopia\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_wj2d\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.instrument.family -> Crystallophone\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.image -> A bifocal corrective eyeglasses lens\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nm.012zbkk5"], "ground_truth": ["Lightning rod", "Glass harmonica", "Bifocals", "Franklin stove"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.8181818181818182, "path_precision": 0.9, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-1339", "prediction": ["# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0j2mvjf -> base.schemastaging.athlete_salary.team -> New York Knicks\n# Answer:\nm.0j2mvjf", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0_qrbd1 -> base.schemastaging.athlete_salary.team -> Houston Rockets\n# Answer:\nm.0_qrbd1", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0j2mvjf -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0j2mvjf", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0ng9xr6 -> base.schemastaging.athlete_salary.team -> Houston Rockets\n# Answer:\nm.0ng9xr6", "# Reasoning Path:\nJeremy Lin -> sports.pro_athlete.teams -> m.0k6s01p -> sports.sports_team_roster.team -> New York Knicks\n# Answer:\nm.0k6s01p", "# Reasoning Path:\nJeremy Lin -> sports.pro_athlete.teams -> m.01145k7g -> sports.sports_team_roster.team -> Los Angeles Lakers\n# Answer:\nm.01145k7g", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0_qrbd1 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0_qrbd1", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0ng9xr6 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0ng9xr6", "# Reasoning Path:\nJeremy Lin -> sports.pro_athlete.teams -> m.0k6s01p -> sports.sports_team_roster.position -> Point guard\n# Answer:\nm.0k6s01p", "# Reasoning Path:\nJeremy Lin -> sports.pro_athlete.teams -> m.0j2dfqc -> sports.sports_team_roster.team -> Houston Rockets\n# Answer:\nm.0j2dfqc"], "ground_truth": ["Houston Rockets"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-134", "prediction": ["# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Gumnaam -> film.film.story_by -> Charandas Shokh\n# Answer:\nGumnaam", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Evil Under the Sun -> film.film.genre -> Suspense\n# Answer:\nEvil Under the Sun", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Gumnaam -> film.film.edited_by -> D.N. Pai\n# Answer:\nGumnaam", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Evil Under the Sun -> film.film.genre -> Drama\n# Answer:\nEvil Under the Sun", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Gumnaam -> film.film.cinematography -> K.H.Kapadia\n# Answer:\nGumnaam", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Evil Under the Sun -> book.written_work.subjects -> England\n# Answer:\nEvil Under the Sun", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> A Caribbean Mystery -> film.film.genre -> Mystery\n# Answer:\nA Caribbean Mystery", "# Reasoning Path:\nAgatha Christie -> common.topic.webpage -> m.03lzz8x -> common.webpage.resource -> m.0bl41lc\n# Answer:\nm.03lzz8x", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Evil Under the Sun -> film.film.genre -> Crime Fiction\n# Answer:\nEvil Under the Sun", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> A Caribbean Mystery -> film.film.genre -> Drama\n# Answer:\nA Caribbean Mystery"], "ground_truth": ["Playwright", "Poet", "Screenwriter", "Writer", "Novelist"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1340", "prediction": ["# Reasoning Path:\nSteve Nash -> people.person.nationality -> Canada -> location.country.languages_spoken -> English Language\n# Answer:\nCanada", "# Reasoning Path:\nSteve Nash -> people.person.nationality -> Canada -> common.topic.notable_types -> Country\n# Answer:\nCanada", "# Reasoning Path:\nSteve Nash -> people.person.nationality -> United Kingdom -> location.country.languages_spoken -> English Language\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nSteve Nash -> people.person.place_of_birth -> Johannesburg -> location.location.contains -> 120 End Street\n# Answer:\nJohannesburg", "# Reasoning Path:\nSteve Nash -> people.person.nationality -> Canada -> location.country.official_language -> English Language\n# Answer:\nCanada", "# Reasoning Path:\nSteve Nash -> people.person.nationality -> United Kingdom -> location.country.official_language -> English Language\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nSteve Nash -> people.person.place_of_birth -> Johannesburg -> location.location.contains -> 2055\n# Answer:\nJohannesburg", "# Reasoning Path:\nSteve Nash -> people.person.place_of_birth -> Johannesburg -> location.location.events -> 10th African Movie Academy Awards Nomination Announcement\n# Answer:\nJohannesburg", "# Reasoning Path:\nSteve Nash -> people.person.place_of_birth -> Johannesburg -> location.location.contains -> 2104\n# Answer:\nJohannesburg", "# Reasoning Path:\nSteve Nash -> people.person.place_of_birth -> Johannesburg -> common.topic.image -> Johannesburg skyline from Gold Reef City\n# Answer:\nJohannesburg"], "ground_truth": ["United Kingdom", "Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1341", "prediction": ["# Reasoning Path:\nPennsylvania -> government.political_district.representatives -> m.0481629 -> government.government_position_held.office_holder -> Arlen Specter\n# Answer:\nm.0481629", "# Reasoning Path:\nPennsylvania -> government.political_district.representatives -> m.04dxp_0 -> government.government_position_held.office_holder -> Bob Casey, Jr.\n# Answer:\nm.04dxp_0", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.010f42tf -> government.government_position_held.office_holder -> Jim Cawley\n# Answer:\nm.010f42tf", "# Reasoning Path:\nPennsylvania -> government.political_district.representatives -> m.04flnp7 -> government.government_position_held.office_holder -> Michael J. Holston\n# Answer:\nm.04flnp7", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.011crdxd -> government.government_position_held.office_holder -> Rosemary Brown\n# Answer:\nm.011crdxd", "# Reasoning Path:\nPennsylvania -> government.political_district.representatives -> m.0481629 -> government.government_position_held.basic_title -> Senator\n# Answer:\nm.0481629", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.048157l -> government.government_position_held.office_holder -> Catherine Baker Knoll\n# Answer:\nm.048157l", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.title -> Director, Office of Health Care Reform\n# Answer:\nm.04ds08c", "# Reasoning Path:\nPennsylvania -> government.political_district.representatives -> m.0481629 -> government.government_position_held.legislative_sessions -> 100th United States Congress\n# Answer:\nm.0481629", "# Reasoning Path:\nPennsylvania -> government.political_district.representatives -> m.04dxp_0 -> government.government_position_held.basic_title -> Senator\n# Answer:\nm.04dxp_0"], "ground_truth": ["Arlen Specter", "James J. Davis", "Joseph F. Guffey", "Matthew Quay", "Pat Toomey", "Michael Leib", "James Buchanan", "Albert Gallatin", "George T. Oliver", "George W. Pepper", "Simon Cameron", "William Wilkins", "Edward Martin", "William Marks", "Walter Lowrie", "Joseph R. Grundy", "Richard Brodhead", "Philander C. Knox", "Robert Morris", "Samuel Maclay", "James Ross", "Harris Wofford", "George M. Dallas", "Boies Penrose", "Richard Schweiker", "William Bingham", "Bob Casey, Jr.", "Peter Muhlenberg", "Jonathan Roberts", "Andrew Gregg", "Isaac D. Barnard", "William Maclay", "Daniel Sturgeon", "William Scott Vare", "Abner Lacock", "Francis J. Myers", "William E. Crow", "Rick Santorum", "H. John Heinz III", "George Logan", "Samuel McKean", "William Findlay", "David A. Reed", "James H. Duff", "William Bigler", "Joseph S. Clark, Jr.", "Hugh Scott"], "ans_acc": 0.0425531914893617, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.0625, "path_precision": 0.2, "path_recall": 0.037037037037037035, "path_ans_f1": 0.07017543859649122, "path_ans_precision": 0.2, "path_ans_recall": 0.0425531914893617}
{"id": "WebQTest-1342", "prediction": ["# Reasoning Path:\nFrance -> location.country.official_language -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nFrance -> location.country.official_language -> French -> language.human_language.language_family -> Romance languages\n# Answer:\nFrench", "# Reasoning Path:\nFrance -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> Algeria\n# Answer:\nFrench", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.11b60tv_69\n# Answer:\ng.11b60tv_69", "# Reasoning Path:\nFrance -> location.country.languages_spoken -> Alsatian dialect -> language.human_language.language_family -> Indo-European languages\n# Answer:\nAlsatian dialect", "# Reasoning Path:\nFrance -> location.country.languages_spoken -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nFrance -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> Belgium\n# Answer:\nFrench", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6gn9g\n# Answer:\ng.12tb6gn9g", "# Reasoning Path:\nFrance -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> Luxembourg\n# Answer:\nFrench", "# Reasoning Path:\nFrance -> location.country.official_language -> French -> language.human_language.language_family -> Indo-European languages\n# Answer:\nFrench"], "ground_truth": ["French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1343", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.02wp75f -> education.education.degree -> PhD\n# Answer:\nm.02wp75f", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> education.education.degree -> Bachelor of Divinity\n# Answer:\nm.0dh6jzz", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.04hddst -> education.education.degree -> Bachelor of Arts\n# Answer:\nm.04hddst", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.02wp75f -> education.education.institution -> Boston University\n# Answer:\nm.02wp75f", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> education.education.institution -> Crozer Theological Seminary\n# Answer:\nm.0dh6jzz", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nm.0dh6jzz", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.04hddst -> education.education.major_field_of_study -> Sociology\n# Answer:\nm.04hddst", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nm.011j_4sh", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> freebase.valuenotation.has_value -> Minor\n# Answer:\nm.0dh6jzz", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> common.topic.article -> m.05nnh0\n# Answer:\nDexter Avenue Baptist Church"], "ground_truth": ["PhD", "Bachelor of Divinity", "Secondary education", "Bachelor of Arts"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4285714285714285, "path_ans_precision": 0.3, "path_ans_recall": 0.75}
{"id": "WebQTest-1344", "prediction": ["# Reasoning Path:\nSt. Louis -> location.location.containedby -> Missouri -> location.location.containedby -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nSt. Louis -> location.location.containedby -> Area code 314 -> location.location.containedby -> Missouri\n# Answer:\nArea code 314", "# Reasoning Path:\nSt. Louis -> base.biblioness.bibs_location.state -> Missouri -> location.location.containedby -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nSt. Louis -> location.location.containedby -> Missouri -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nSt. Louis -> location.location.containedby -> Missouri -> base.aareas.schema.administrative_area.administrative_children -> Franklin County\n# Answer:\nMissouri", "# Reasoning Path:\nSt. Louis -> location.location.containedby -> Area code 557 -> location.location.containedby -> Missouri\n# Answer:\nArea code 557", "# Reasoning Path:\nSt. Louis -> location.location.containedby -> Area code 314 -> common.topic.notable_for -> g.125bvnkp4\n# Answer:\nArea code 314", "# Reasoning Path:\nSt. Louis -> base.biblioness.bibs_location.state -> Missouri -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nSt. Louis -> base.biblioness.bibs_location.state -> Missouri -> base.aareas.schema.administrative_area.administrative_children -> Franklin County\n# Answer:\nMissouri", "# Reasoning Path:\nSt. Louis -> location.location.containedby -> Area code 557 -> common.topic.notable_types -> Area code\n# Answer:\nArea code 557"], "ground_truth": ["Missouri"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1348", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.drafted_athlete.drafted -> m.04vb3pp -> sports.sports_league_draft_pick.team -> Indianapolis Colts\n# Answer:\nm.04vb3pp", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nm.0j5d2kv", "# Reasoning Path:\nPeyton Manning -> sports.drafted_athlete.drafted -> m.04vb3pp -> sports.sports_league_draft_pick.draft -> 1998 NFL draft\n# Answer:\nm.04vb3pp", "# Reasoning Path:\nPeyton Manning -> sports.drafted_athlete.drafted -> m.04vb3pp -> sports.sports_league_draft_pick.school -> University of Tennessee\n# Answer:\nm.04vb3pp", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.0j5d2kv", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.team -> Indianapolis Colts\n# Answer:\nm.0j4z5bh", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nm.0j5d2kv", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nm.0j5d2kv", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.0j4z5bh"], "ground_truth": ["Houston Oilers", "Ole Miss Rebels football", "New Orleans Saints", "Minnesota Vikings"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1350", "prediction": ["# Reasoning Path:\nJesse Owens -> olympics.olympic_athlete.medals_won -> m.04hdrqm -> olympics.olympic_medal_honor.event -> Athletics at the 1936 Summer Olympics \u2013 Men's long jump\n# Answer:\nm.04hdrqm", "# Reasoning Path:\nJesse Owens -> olympics.olympic_athlete.medals_won -> m.04hdrqm -> olympics.olympic_medal_honor.country -> United States of America\n# Answer:\nm.04hdrqm", "# Reasoning Path:\nJesse Owens -> olympics.olympic_athlete.medals_won -> m.04hdrr2 -> olympics.olympic_medal_honor.event -> Athletics at the 1936 Summer Olympics \u2013 Men's 200 metres\n# Answer:\nm.04hdrr2", "# Reasoning Path:\nJesse Owens -> olympics.olympic_athlete.medals_won -> m.04hdrrk -> olympics.olympic_medal_honor.event -> Athletics at the 1936 Summer Olympics \u2013 Men's 4 \u00d7 100 metres relay\n# Answer:\nm.04hdrrk", "# Reasoning Path:\nJesse Owens -> people.person.places_lived -> m.03pfl8z -> people.place_lived.location -> Cleveland\n# Answer:\nm.03pfl8z", "# Reasoning Path:\nJesse Owens -> olympics.olympic_athlete.medals_won -> m.04hdrr2 -> olympics.olympic_medal_honor.medal -> Gold medal\n# Answer:\nm.04hdrr2", "# Reasoning Path:\nJesse Owens -> film.person_or_entity_appearing_in_film.films -> m.046qw5v -> film.personal_film_appearance.type_of_appearance -> Subject of film\n# Answer:\nm.046qw5v", "# Reasoning Path:\nJesse Owens -> people.person.places_lived -> m.0wk2xk3 -> people.place_lived.location -> Oakville, Alabama\n# Answer:\nm.0wk2xk3", "# Reasoning Path:\nJesse Owens -> olympics.olympic_athlete.medals_won -> m.04hdrrk -> olympics.olympic_medal_honor.country -> United States of America\n# Answer:\nm.04hdrrk", "# Reasoning Path:\nJesse Owens -> olympics.olympic_athlete.medals_won -> m.04hdrrk -> olympics.olympic_medal_honor.medalist -> Foy Draper\n# Answer:\nm.04hdrrk"], "ground_truth": ["Associated Press Male Athlete of the Year"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1351", "prediction": ["# Reasoning Path:\nPeru -> location.country.currency_used -> Peruvian nuevo sol -> common.topic.notable_types -> Currency\n# Answer:\nPeruvian nuevo sol", "# Reasoning Path:\nPeru -> location.country.currency_used -> Peruvian nuevo sol -> common.topic.notable_for -> g.125cdv3sb\n# Answer:\nPeruvian nuevo sol", "# Reasoning Path:\nPeru -> location.country.currency_used -> Peruvian nuevo sol -> common.topic.article -> m.0b4243\n# Answer:\nPeruvian nuevo sol", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.11b71n53cl\n# Answer:\ng.11b71n53cl", "# Reasoning Path:\nPeru -> location.statistical_region.energy_use_per_capita -> g.1245_8rt3\n# Answer:\ng.1245_8rt3", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.1hhc37ctc\n# Answer:\ng.1hhc37ctc", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.1hhc38qlv\n# Answer:\ng.1hhc38qlv", "# Reasoning Path:\nPeru -> location.statistical_region.energy_use_per_capita -> g.1245_9gm0\n# Answer:\ng.1245_9gm0", "# Reasoning Path:\nPeru -> location.statistical_region.energy_use_per_capita -> g.1245__jr1\n# Answer:\ng.1245__jr1"], "ground_truth": ["Peruvian nuevo sol"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-1353", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> base.aareas.schema.administrative_area.administrative_children -> Anglesey\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Down\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> base.aareas.schema.administrative_area.administrative_children -> Blaenau Gwent\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> location.country.first_level_divisions -> Anglesey\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> base.aareas.schema.administrative_area.administrative_children -> East Midlands\n# Answer:\nEngland"], "ground_truth": ["England", "Wales", "Scotland", "Northern Ireland"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.6, "path_precision": 0.5, "path_recall": 0.75, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-1354", "prediction": ["# Reasoning Path:\nJudy Garland -> people.person.parents -> Francis Avent Gumm -> people.person.children -> Dorothy Virginia Gumm\n# Answer:\nFrancis Avent Gumm", "# Reasoning Path:\nJudy Garland -> people.person.parents -> Francis Avent Gumm -> people.person.place_of_birth -> Tennessee\n# Answer:\nFrancis Avent Gumm", "# Reasoning Path:\nJudy Garland -> people.person.parents -> Francis Avent Gumm -> common.topic.notable_for -> g.1255nt3hr\n# Answer:\nFrancis Avent Gumm", "# Reasoning Path:\nJudy Garland -> people.person.parents -> Francis Avent Gumm -> people.person.children -> Mary Jane Gumm\n# Answer:\nFrancis Avent Gumm", "# Reasoning Path:\nJudy Garland -> people.person.parents -> Ethel Marion Milne -> common.topic.notable_types -> Deceased Person\n# Answer:\nEthel Marion Milne", "# Reasoning Path:\nJudy Garland -> music.artist.album -> Judy -> music.album.releases -> g.11bbmg7ycv\n# Answer:\nJudy", "# Reasoning Path:\nJudy Garland -> people.person.parents -> Ethel Marion Milne -> people.deceased_person.place_of_death -> Los Angeles\n# Answer:\nEthel Marion Milne", "# Reasoning Path:\nJudy Garland -> music.artist.album -> Judy -> common.topic.article -> m.0gw_1lr\n# Answer:\nJudy", "# Reasoning Path:\nJudy Garland -> music.artist.album -> (Dear Mr. Gable) You Made Me Love You / You Can't Have Everything -> common.topic.notable_types -> Musical Album\n# Answer:\n(Dear Mr. Gable) You Made Me Love You / You Can't Have Everything", "# Reasoning Path:\nJudy Garland -> film.actor.film -> m.012zh74p -> film.performance.film -> Bubbles\n# Answer:\nm.012zh74p"], "ground_truth": ["Francis Avent Gumm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1355", "prediction": ["# Reasoning Path:\nRobert F. Kennedy -> people.person.education -> m.04yst0l -> education.education.institution -> University of Virginia\n# Answer:\nm.04yst0l", "# Reasoning Path:\nRobert F. Kennedy -> people.person.education -> m.02wp24k -> education.education.institution -> Harvard University\n# Answer:\nm.02wp24k", "# Reasoning Path:\nRobert F. Kennedy -> people.person.education -> m.04hddhr -> education.education.institution -> Bates College\n# Answer:\nm.04hddhr", "# Reasoning Path:\nRobert F. Kennedy -> people.family_member.family -> Kennedy family -> people.family.members -> Anthony Shriver\n# Answer:\nKennedy family", "# Reasoning Path:\nRobert F. Kennedy -> people.family_member.family -> Kennedy family -> common.topic.notable_for -> g.12573hgz4\n# Answer:\nKennedy family", "# Reasoning Path:\nRobert F. Kennedy -> people.family_member.family -> Kennedy family -> people.family.members -> Bobby Shriver\n# Answer:\nKennedy family", "# Reasoning Path:\nRobert F. Kennedy -> people.family_member.family -> Kennedy family -> common.topic.image -> Kennedy Arms\n# Answer:\nKennedy family", "# Reasoning Path:\nRobert F. Kennedy -> people.family_member.family -> Kennedy family -> people.family.members -> Caroline Kennedy\n# Answer:\nKennedy family", "# Reasoning Path:\nRobert F. Kennedy -> film.film_character.portrayed_in_films -> m.03jr73t -> film.performance.film -> Thirteen Days\n# Answer:\nm.03jr73t", "# Reasoning Path:\nRobert F. Kennedy -> people.family_member.family -> Kennedy family -> common.topic.image -> Kennedy bros\n# Answer:\nKennedy family"], "ground_truth": ["Harvard University", "Milton Academy", "Bates College", "University of Virginia School of Law", "Harvard College", "University of Virginia"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.37499999999999994, "path_ans_precision": 0.3, "path_ans_recall": 0.5}
{"id": "WebQTest-1356", "prediction": ["# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.cause_of_death -> Barbiturate overdose -> common.topic.article -> m.0bwj2gk\n# Answer:\nBarbiturate overdose", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.cause_of_death -> Barbiturate overdose -> people.cause_of_death.people -> Chester Morris\n# Answer:\nBarbiturate overdose", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.cause_of_death -> Barbiturate overdose -> common.topic.notable_for -> g.125cvd366\n# Answer:\nBarbiturate overdose", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.cause_of_death -> Barbiturate overdose -> people.cause_of_death.people -> Cindy James\n# Answer:\nBarbiturate overdose", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.cause_of_death -> Barbiturate overdose -> people.cause_of_death.people -> Edie Sedgwick\n# Answer:\nBarbiturate overdose", "# Reasoning Path:\nMarilyn Monroe -> people.person.parents -> Gladys Pearl Baker -> people.person.spouse_s -> m.0kpgnt0\n# Answer:\nGladys Pearl Baker", "# Reasoning Path:\nMarilyn Monroe -> people.person.parents -> Martin Edward Mortensen -> people.person.spouse_s -> m.0kpgnt0\n# Answer:\nMartin Edward Mortensen", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.07zn09y -> award.award_nomination.ceremony -> 9th British Academy Film Awards\n# Answer:\nm.07zn09y", "# Reasoning Path:\nMarilyn Monroe -> people.person.parents -> Gladys Pearl Baker -> film.person_or_entity_appearing_in_film.films -> m.0rpz9hz\n# Answer:\nGladys Pearl Baker", "# Reasoning Path:\nMarilyn Monroe -> people.person.parents -> Gladys Pearl Baker -> people.person.spouse_s -> m.0qf21l5\n# Answer:\nGladys Pearl Baker"], "ground_truth": ["Barbiturate overdose"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1357", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.location.containedby -> Asia\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nm.0clv1h_", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> Fukushima Prefecture\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Aichi Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> Japan\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> common.topic.image -> Okuma town office\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.country.administrative_divisions -> Aichi Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> T\u014dhoku region\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Akita Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_for -> g.125btmfy0\n# Answer:\nFukushima I \u2013 1"], "ground_truth": ["Japan", "Okuma"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1358", "prediction": ["# Reasoning Path:\nLos Angeles -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nPacific Time Zone", "# Reasoning Path:\nLos Angeles -> base.popstra.location.vacationers -> m.064sw_0 -> base.popstra.vacation_choice.vacationer -> Madonna\n# Answer:\nm.064sw_0", "# Reasoning Path:\nLos Angeles -> periodicals.newspaper_circulation_area.newspapers -> Hoy -> book.newspaper.circulation_areas -> Chicago\n# Answer:\nHoy", "# Reasoning Path:\nLos Angeles -> periodicals.newspaper_circulation_area.newspapers -> LA CityBeat -> book.periodical.first_issue_date -> m.02npnzp\n# Answer:\nLA CityBeat", "# Reasoning Path:\nLos Angeles -> periodicals.newspaper_circulation_area.newspapers -> Hoy -> book.periodical.language -> Spanish Language\n# Answer:\nHoy", "# Reasoning Path:\nLos Angeles -> base.popstra.location.vacationers -> m.064c8fd -> base.popstra.vacation_choice.vacationer -> David Schwimmer\n# Answer:\nm.064c8fd", "# Reasoning Path:\nLos Angeles -> periodicals.newspaper_circulation_area.newspapers -> LA CityBeat -> book.periodical.final_issue_date -> m.0j057sg\n# Answer:\nLA CityBeat", "# Reasoning Path:\nLos Angeles -> periodicals.newspaper_circulation_area.newspapers -> Hoy -> book.newspaper.circulation_areas -> New York City\n# Answer:\nHoy", "# Reasoning Path:\nLos Angeles -> base.popstra.location.vacationers -> m.064sn_4 -> base.popstra.vacation_choice.vacationer -> Hilary Duff\n# Answer:\nm.064sn_4", "# Reasoning Path:\nLos Angeles -> periodicals.newspaper_circulation_area.newspapers -> Angeleno -> common.topic.notable_for -> g.1ym_c_n37\n# Answer:\nAngeleno"], "ground_truth": ["Pacific Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1359", "prediction": ["# Reasoning Path:\nNicolas Cage -> people.person.children -> Kal-El Coppola Cage -> people.person.place_of_birth -> New York City\n# Answer:\nKal-El Coppola Cage", "# Reasoning Path:\nNicolas Cage -> people.person.children -> Kal-El Coppola Cage -> people.person.sibling_s -> m.0pcd733\n# Answer:\nKal-El Coppola Cage", "# Reasoning Path:\nNicolas Cage -> people.person.children -> Weston Coppola Cage -> people.person.children -> Lucian Augustus Coppola Cage\n# Answer:\nWeston Coppola Cage", "# Reasoning Path:\nNicolas Cage -> people.person.children -> Kal-El Coppola Cage -> common.topic.notable_types -> Person\n# Answer:\nKal-El Coppola Cage", "# Reasoning Path:\nNicolas Cage -> base.saturdaynightlive.snl_host.episodes_hosted -> SNL - 18.1 -> base.saturdaynightlive.snl_episode.season -> Saturday Night Live - Season 18\n# Answer:\nSNL - 18.1", "# Reasoning Path:\nNicolas Cage -> celebrities.celebrity.sexual_orientation -> m.05n7hz_ -> celebrities.sexual_orientation_phase.sexual_orientation -> Heterosexuality\n# Answer:\nm.05n7hz_", "# Reasoning Path:\nNicolas Cage -> base.saturdaynightlive.snl_host.episodes_hosted -> SNL - 18.1 -> common.topic.notable_for -> g.1259fc15z\n# Answer:\nSNL - 18.1", "# Reasoning Path:\nNicolas Cage -> people.person.children -> Weston Coppola Cage -> film.actor.film -> m.011dvr25\n# Answer:\nWeston Coppola Cage", "# Reasoning Path:\nNicolas Cage -> base.saturdaynightlive.snl_host.episodes_hosted -> SNL - 18.1 -> base.saturdaynightlive.snl_episode.musical_guest -> Bobby Brown\n# Answer:\nSNL - 18.1", "# Reasoning Path:\nNicolas Cage -> people.person.children -> Weston Coppola Cage -> people.person.profession -> Actor\n# Answer:\nWeston Coppola Cage"], "ground_truth": ["Weston Coppola Cage", "Kal-El Coppola Cage"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1361", "prediction": ["# Reasoning Path:\nMexico -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> North America\n# Answer:\nEarth", "# Reasoning Path:\nMexico -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> United States of America\n# Answer:\nEarth", "# Reasoning Path:\nMexico -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> Sovereign state\n# Answer:\nEarth", "# Reasoning Path:\nMexico -> location.location.containedby -> Latin America -> location.location.containedby -> Americas\n# Answer:\nLatin America", "# Reasoning Path:\nMexico -> location.location.containedby -> Latin America -> location.location.contains -> Cuauht\u00e9moc, D.F.\n# Answer:\nLatin America", "# Reasoning Path:\nMexico -> location.location.containedby -> Americas -> location.location.contains -> Latin America\n# Answer:\nAmericas", "# Reasoning Path:\nMexico -> location.location.containedby -> North America -> base.locations.continents.countries_within -> Guatemala\n# Answer:\nNorth America", "# Reasoning Path:\nMexico -> location.location.containedby -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nMexico -> location.location.containedby -> Americas -> location.location.contains -> Guatemala\n# Answer:\nAmericas", "# Reasoning Path:\nMexico -> location.location.containedby -> Latin America -> organization.organization_scope.organizations_with_this_scope -> AMREP Mexico\n# Answer:\nLatin America"], "ground_truth": ["Mexican peso"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1362", "prediction": ["# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0hz6vzk -> government.government_position_held.appointed_by -> Rick Scott\n# Answer:\nm.0hz6vzk", "# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0hz6vzk -> government.government_position_held.office_holder -> Kurt S. Browning\n# Answer:\nm.0hz6vzk", "# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0115c969 -> government.government_position_held.office_holder -> Jan Fortune\n# Answer:\nm.0115c969", "# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0hz6vzk -> government.government_position_held.basic_title -> Secretary of state\n# Answer:\nm.0hz6vzk", "# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0115gjyq -> government.government_position_held.office_holder -> Joe Davis\n# Answer:\nm.0115gjyq", "# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0115c969 -> government.government_position_held.basic_title -> State Representative\n# Answer:\nm.0115c969", "# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0115c969 -> freebase.valuenotation.has_no_value -> Appointed By (if Position is Appointed)\n# Answer:\nm.0115c969", "# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0115gjyq -> government.government_position_held.governmental_body -> Florida House of Representatives\n# Answer:\nm.0115gjyq", "# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0115gjyq -> government.government_position_held.district_represented -> Seminole County\n# Answer:\nm.0115gjyq", "# Reasoning Path:\nFlorida -> location.location.partiallycontains -> m.0wg9m99 -> location.partial_containment_relationship.partially_contains -> St. Marys River\n# Answer:\nm.0wg9m99"], "ground_truth": ["Rick Scott"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1363", "prediction": ["# Reasoning Path:\nJessica Simpson -> people.person.spouse_s -> m.010gz8v_ -> people.marriage.spouse -> Eric Johnson\n# Answer:\nm.010gz8v_", "# Reasoning Path:\nJessica Simpson -> people.person.spouse_s -> m.07sl20z -> people.marriage.spouse -> Nick Lachey\n# Answer:\nm.07sl20z", "# Reasoning Path:\nJessica Simpson -> celebrities.celebrity.sexual_relationships -> m.010gz96f -> celebrities.romantic_relationship.celebrity -> Eric Johnson\n# Answer:\nm.010gz96f", "# Reasoning Path:\nJessica Simpson -> people.person.spouse_s -> m.07sl20z -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.07sl20z", "# Reasoning Path:\nJessica Simpson -> celebrities.celebrity.sexual_relationships -> m.010gz96f -> freebase.valuenotation.has_no_value -> End date\n# Answer:\nm.010gz96f", "# Reasoning Path:\nJessica Simpson -> celebrities.celebrity.sexual_relationships -> m.010gz93y -> celebrities.romantic_relationship.celebrity -> Tony Romo\n# Answer:\nm.010gz93y", "# Reasoning Path:\nJessica Simpson -> people.person.spouse_s -> m.07sl20z -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.07sl20z", "# Reasoning Path:\nJessica Simpson -> people.person.spouse_s -> m.010gz8v_ -> freebase.valuenotation.has_no_value -> Location of ceremony\n# Answer:\nm.010gz8v_", "# Reasoning Path:\nJessica Simpson -> people.person.spouse_s -> m.0113nnph -> people.marriage.spouse -> Eric Johnson\n# Answer:\nm.0113nnph", "# Reasoning Path:\nJessica Simpson -> celebrities.celebrity.sexual_relationships -> m.010gz96f -> celebrities.romantic_relationship.relationship_type -> Engaged to\n# Answer:\nm.010gz96f"], "ground_truth": ["Nick Lachey", "Eric Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.380952380952381, "path_precision": 0.4, "path_recall": 0.36363636363636365, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1364", "prediction": ["# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.countries_spoken_in -> Kingdom of Great Britain\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.main_country -> United Kingdom\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Old English -> language.human_language.countries_spoken_in -> England\n# Answer:\nOld English", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scots Language -> common.topic.notable_types -> Human Language\n# Answer:\nScots Language", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Old English -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nOld English", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Old English -> language.human_language.main_country -> England\n# Answer:\nOld English", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scots Language -> language.human_language.countries_spoken_in -> Kingdom of Great Britain\n# Answer:\nScots Language", "# Reasoning Path:\nScotland -> location.country.first_level_divisions -> Aberdeenshire -> location.administrative_division.second_level_division_of -> United Kingdom\n# Answer:\nAberdeenshire"], "ground_truth": ["English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1365", "prediction": ["# Reasoning Path:\nJohn Dalton -> people.person.profession -> Chemist -> base.descriptive_names.names.descriptive_name -> m.01066g6n\n# Answer:\nChemist", "# Reasoning Path:\nJohn Dalton -> people.person.profession -> Chemist -> common.topic.article -> m.01pxq\n# Answer:\nChemist", "# Reasoning Path:\nJohn Dalton -> people.person.profession -> Chemist -> base.schemastaging.context_name.pronunciation -> g.125_m09_2\n# Answer:\nChemist", "# Reasoning Path:\nJohn Dalton -> people.person.profession -> Chemist -> base.descriptive_names.names.descriptive_name -> m.0106_ymb\n# Answer:\nChemist", "# Reasoning Path:\nJohn Dalton -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Daniel Faraday\n# Answer:\nPhysicist", "# Reasoning Path:\nJohn Dalton -> base.kwebbase.kwtopic.has_sentences -> After 1794, Dalton read the results of all his research to the Philosophical Society. -> base.kwebbase.kwsentence.dates -> m.0c1622q\n# Answer:\nAfter 1794, Dalton read the results of all his research to the Philosophical Society.", "# Reasoning Path:\nJohn Dalton -> people.person.profession -> Chemist -> base.descriptive_names.names.descriptive_name -> m.010f2qwv\n# Answer:\nChemist", "# Reasoning Path:\nJohn Dalton -> base.kwebbase.kwtopic.connections_to -> abraham darby benefactor of john dalton -> base.kwebbase.kwconnection.relation -> benefactor of\n# Answer:\nabraham darby benefactor of john dalton", "# Reasoning Path:\nJohn Dalton -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Emmett Brown\n# Answer:\nPhysicist", "# Reasoning Path:\nJohn Dalton -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.0101ddrk\n# Answer:\nPhysicist"], "ground_truth": ["Physicist", "Chemist"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1366", "prediction": ["# Reasoning Path:\nZac Brown Band -> music.artist.album -> As She's Walking Away -> music.recording.featured_artists -> Alan Jackson\n# Answer:\nAs She's Walking Away", "# Reasoning Path:\nZac Brown Band -> music.artist.album -> As She's Walking Away -> music.album.album_content_type -> Studio album\n# Answer:\nAs She's Walking Away", "# Reasoning Path:\nZac Brown Band -> music.artist.album -> Chicken Fried -> music.recording.releases -> The Foundation\n# Answer:\nChicken Fried", "# Reasoning Path:\nZac Brown Band -> music.artist.album -> As She's Walking Away -> music.composition.composer -> Zac Brown\n# Answer:\nAs She's Walking Away", "# Reasoning Path:\nZac Brown Band -> common.topic.webpage -> m.0460qbn -> common.webpage.resource -> m.0bjwcfr\n# Answer:\nm.0460qbn", "# Reasoning Path:\nZac Brown Band -> music.artist.album -> Chicken Fried -> common.topic.notable_for -> g.12yxh5hrx\n# Answer:\nChicken Fried", "# Reasoning Path:\nZac Brown Band -> music.artist.album -> Colder Weather -> common.topic.notable_for -> g.126srqk7h\n# Answer:\nColder Weather", "# Reasoning Path:\nZac Brown Band -> common.topic.webpage -> m.0460qbn -> common.webpage.category -> Official Website\n# Answer:\nm.0460qbn", "# Reasoning Path:\nZac Brown Band -> music.artist.album -> Colder Weather -> common.topic.notable_types -> Musical Album\n# Answer:\nColder Weather", "# Reasoning Path:\nZac Brown Band -> music.artist.album -> As She's Walking Away -> music.composition.composer -> Wyatt Durrette\n# Answer:\nAs She's Walking Away"], "ground_truth": ["I Shall Be Released (live)", "Castaway", "Bittersweet", "Beautiful Drug", "Day for the Dead", "Oh My Sweet Carolina (live)", "Let It Go", "Harmony", "Trying to Drive", "Make This Day", "Every Little Bit", "Where the Boat Leaves From / One Love", "DJ", "Day That I Die", "Goodbye In Her Eyes", "Mary", "Natural Disaster", "I Lost It (live)", "Where the Boat Leaves From", "Better Day", "Whatever It Is", "Wildfire", "Let It Rain", "Overnight (Feat. Trombone Shorty)", "Tomorrow Never Comes (acoustic version)", "Heavy Is the Head", "Let It Go (Live In Atlanta)", "Toes", "Free / Into the Mystic", "Homegrown", "Valentines", "Colder Weather", "I Shall Be Released", "A Different Kind of Fine", "Quiet Your Mind", "Martin (Live In Atlanta)", "Smoke Rise", "Junkyard", "Loving You Easy", "Last But Not Least", "As She's Walking Away", "Cold Hearted", "Violin Intro to Free", "Every Little Bit (live)", "Mango Tree", "America the Beautiful", "Uncaged", "It's Not Okay", "Bad Moon Rising", "One Day", "Forever and Ever, Amen", "Intro", "Not OK", "Keep Me In Mind", "It's Not OK", "The Muse", "Whiskey's Gone", "These Days", "Curse Me", "Free", "Remedy", "Jolene", "Day That I Die (Feat. Amos Lee)", "Settle Me Down", "Trouble", "Young and Wild", "Bar", "Chicken Fried (Full Version)", "On This Train", "Tomorrow Never Comes", "Tax Man Shoes", "Can't You See", "Black Water", "Knee Deep (Feat. Jimmy Buffett)", "Martin", "Different Kind of Fine", "The Devil Went Down to Georgia", "Sweet Annie", "Lance's Song", "Heather", "No Hurry", "Island Song", "Nothing", "I'll Be Your Man (Song For a Daughter)", "Alabama Jubilee", "The Wind", "Sic 'em on a Chicken", "Chicken Fried", "The Night They Drove Old Dixie Down", "All Alright", "Dress Blues", "I Play the Road", "The Night They Drove Old Dixie Down (live)", "Highway 20 Ride", "Keep Me in Mind", "We're Gonna Make This Day", "Who Knows", "Jump Right In", "Human", "Blackbird", "Sic 'em on the Chicken"], "ans_acc": 0.0297029702970297, "ans_hit": 1, "ans_f1": 0.05727923627684964, "ans_precission": 0.8, "ans_recall": 0.0297029702970297, "path_f1": 0.37305699481865284, "path_precision": 0.8, "path_recall": 0.24324324324324326, "path_ans_f1": 0.05727923627684964, "path_ans_precision": 0.8, "path_ans_recall": 0.0297029702970297}
{"id": "WebQTest-1367", "prediction": ["# Reasoning Path:\nGlastonbury -> location.location.containedby -> Mendip -> location.location.containedby -> England\n# Answer:\nMendip", "# Reasoning Path:\nGlastonbury -> location.location.containedby -> Mendip -> location.location.containedby -> Somerset\n# Answer:\nMendip", "# Reasoning Path:\nGlastonbury -> location.location.containedby -> United Kingdom -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nGlastonbury -> location.location.containedby -> Mendip -> location.location.containedby -> South West England\n# Answer:\nMendip", "# Reasoning Path:\nGlastonbury -> travel.travel_destination.tourist_attractions -> The Tribunal, Glastonbury -> location.location.containedby -> United Kingdom\n# Answer:\nThe Tribunal, Glastonbury", "# Reasoning Path:\nGlastonbury -> location.location.containedby -> Mendip -> location.location.contains -> Ashwick\n# Answer:\nMendip", "# Reasoning Path:\nGlastonbury -> location.location.containedby -> Mendip -> base.aareas.schema.administrative_area.administrative_children -> Mells\n# Answer:\nMendip", "# Reasoning Path:\nGlastonbury -> location.location.containedby -> Mendip -> location.location.contains -> Baltonsborough\n# Answer:\nMendip", "# Reasoning Path:\nGlastonbury -> location.location.containedby -> United Kingdom -> location.country.first_level_divisions -> England\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nGlastonbury -> location.location.containedby -> Mendip -> location.location.contains -> Batcombe\n# Answer:\nMendip"], "ground_truth": ["United Kingdom", "Mendip"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1368", "prediction": ["# Reasoning Path:\nAmerican Federation of Labor -> organization.organization.founders -> Samuel Gompers -> people.person.profession -> Labor Union Leader\n# Answer:\nSamuel Gompers", "# Reasoning Path:\nAmerican Federation of Labor -> common.topic.article -> m.01z4fw\n# Answer:\nm.01z4fw", "# Reasoning Path:\nAmerican Federation of Labor -> organization.organization.founders -> Samuel Gompers -> base.activism.activist.area_of_activism -> Labor rights\n# Answer:\nSamuel Gompers", "# Reasoning Path:\nAmerican Federation of Labor -> common.topic.notable_types -> Organization -> type.type.properties -> Founders\n# Answer:\nOrganization", "# Reasoning Path:\nAmerican Federation of Labor -> organization.organization.founders -> Samuel Gompers -> book.author.works_written -> 70 Years of Life and Labor\n# Answer:\nSamuel Gompers", "# Reasoning Path:\nAmerican Federation of Labor -> common.topic.notable_types -> Organization -> common.topic.article -> m.03bwfg2\n# Answer:\nOrganization", "# Reasoning Path:\nAmerican Federation of Labor -> organization.organization.founders -> Samuel Gompers -> book.author.works_written -> America's fight for the preservation of democracy\n# Answer:\nSamuel Gompers", "# Reasoning Path:\nAmerican Federation of Labor -> common.topic.notable_types -> Organization -> freebase.type_profile.published -> Published\n# Answer:\nOrganization", "# Reasoning Path:\nAmerican Federation of Labor -> organization.organization.founders -> Samuel Gompers -> book.author.works_written -> Editorial in the American Federationist\n# Answer:\nSamuel Gompers", "# Reasoning Path:\nAmerican Federation of Labor -> common.topic.notable_types -> Organization -> type.type.properties -> Acquired by\n# Answer:\nOrganization"], "ground_truth": ["Samuel Gompers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1369", "prediction": ["# Reasoning Path:\nDenver -> travel.travel_destination.tourist_attractions -> Forney Transportation Museum -> location.location.containedby -> 80216\n# Answer:\nForney Transportation Museum", "# Reasoning Path:\nDenver -> travel.travel_destination.tourist_attractions -> Forney Transportation Museum -> common.topic.notable_for -> g.125dp7k7r\n# Answer:\nForney Transportation Museum", "# Reasoning Path:\nDenver -> travel.travel_destination.tourist_attractions -> Westin Westminster -> common.topic.image -> The Westin Westminster\n# Answer:\nWestin Westminster", "# Reasoning Path:\nDenver -> travel.travel_destination.tourist_attractions -> Forney Transportation Museum -> common.topic.article -> m.0cvztx\n# Answer:\nForney Transportation Museum", "# Reasoning Path:\nDenver -> location.statistical_region.population -> g.11b66hmvpg\n# Answer:\ng.11b66hmvpg", "# Reasoning Path:\nDenver -> travel.travel_destination.tourist_attractions -> Westin Westminster -> location.location.geolocation -> m.0wmnr3b\n# Answer:\nWestin Westminster", "# Reasoning Path:\nDenver -> travel.travel_destination.tourist_attractions -> Westin Westminster -> travel.hotel.operating_brand -> Starwood\n# Answer:\nWestin Westminster", "# Reasoning Path:\nDenver -> travel.travel_destination.tourist_attractions -> Denver Mint -> common.topic.image -> The Denver Mint\n# Answer:\nDenver Mint", "# Reasoning Path:\nDenver -> location.statistical_region.population -> g.11b7tnnw1n\n# Answer:\ng.11b7tnnw1n", "# Reasoning Path:\nDenver -> travel.travel_destination.tourist_attractions -> Denver Mint -> common.topic.notable_types -> Tourist attraction\n# Answer:\nDenver Mint"], "ground_truth": ["Denver Museum of Nature and Science", "Forney Transportation Museum", "Elitch Gardens", "Frederic C. Hamilton Building", "Children's Museum of Denver", "Festivus Film Festival", "Black American West Museum & Heritage Center", "North Building", "Molly Brown House", "Denver Botanic Gardens", "Westin Westminster", "Denver Mint", "Chamberlin Observatory", "Museum of Contemporary Art Denver", "Colorado State Capitol", "Denver Firefighters Museum"], "ans_acc": 0.1875, "ans_hit": 1, "ans_f1": 0.3037974683544304, "ans_precission": 0.8, "ans_recall": 0.1875, "path_f1": 0.3037974683544304, "path_precision": 0.8, "path_recall": 0.1875, "path_ans_f1": 0.3037974683544304, "path_ans_precision": 0.8, "path_ans_recall": 0.1875}
{"id": "WebQTest-1370", "prediction": ["# Reasoning Path:\nThomas Hobbes -> people.person.place_of_birth -> Westport, Wiltshire -> common.topic.notable_types -> Location\n# Answer:\nWestport, Wiltshire", "# Reasoning Path:\nThomas Hobbes -> people.person.place_of_birth -> Westport, Wiltshire -> common.topic.notable_for -> g.1256l2xh5\n# Answer:\nWestport, Wiltshire", "# Reasoning Path:\nThomas Hobbes -> people.deceased_person.place_of_death -> Derbyshire -> location.location.containedby -> East Midlands\n# Answer:\nDerbyshire", "# Reasoning Path:\nThomas Hobbes -> people.deceased_person.place_of_death -> Derbyshire -> location.administrative_division.country -> United Kingdom\n# Answer:\nDerbyshire", "# Reasoning Path:\nThomas Hobbes -> people.deceased_person.place_of_death -> Derbyshire -> location.administrative_division.second_level_division_of -> England\n# Answer:\nDerbyshire", "# Reasoning Path:\nThomas Hobbes -> people.person.places_lived -> m.0jvv0vn -> people.place_lived.location -> Paris\n# Answer:\nm.0jvv0vn", "# Reasoning Path:\nThomas Hobbes -> people.deceased_person.place_of_death -> Derbyshire -> location.location.containedby -> United Kingdom\n# Answer:\nDerbyshire"], "ground_truth": ["Paris"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.14285714285714285, "path_recall": 0.25, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-1373", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_death -> Dallas -> location.location.containedby -> United States of America\n# Answer:\nDallas", "# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_death -> Dallas -> travel.travel_destination.tourist_attractions -> Dealey Plaza\n# Answer:\nDallas", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr. -> people.deceased_person.place_of_death -> Atlantic Ocean\n# Answer:\nJohn F. Kennedy Jr.", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr. -> symbols.name_source.namesakes -> John F. Kennedy Jr. plane crash\n# Answer:\nJohn F. Kennedy Jr.", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr. -> people.person.place_of_birth -> Washington, D.C.\n# Answer:\nJohn F. Kennedy Jr.", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> time.event.locations -> Dallas\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nIrish American", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> time.event.locations -> Dealey Plaza\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School -> common.topic.notable_for -> g.125583hnb\n# Answer:\nBloomington Kennedy High School", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nIrish American"], "ground_truth": ["Dallas"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1374", "prediction": ["# Reasoning Path:\nItaly -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Brazil\n# Answer:\nConstitutional republic", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\ng.11b71mdn0q", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Germany\n# Answer:\nConstitutional republic", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Albania\n# Answer:\nParliamentary republic", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\ng.12tb6gbb1", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Austria\n# Answer:\nParliamentary republic", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37cxr\n# Answer:\ng.1hhc37cxr", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Czech Republic\n# Answer:\nParliamentary republic"], "ground_truth": ["Constitutional republic", "Parliamentary republic", "Unitary state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1376", "prediction": ["# Reasoning Path:\nMontreal -> location.location.containedby -> Qu\u00e9bec -> location.location.containedby -> Canada\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nMontreal -> base.biblioness.bibs_location.state -> Qu\u00e9bec -> location.location.containedby -> Canada\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nMontreal -> location.location.containedby -> Qu\u00e9bec -> location.administrative_division.country -> Canada\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nMontreal -> location.location.containedby -> Canada -> location.location.containedby -> North America\n# Answer:\nCanada", "# Reasoning Path:\nMontreal -> location.location.containedby -> Qu\u00e9bec -> sports.sports_team_location.teams -> Montreal Canadiens\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nMontreal -> location.location.containedby -> Urban agglomeration of Montreal -> location.location.containedby -> Qu\u00e9bec\n# Answer:\nUrban agglomeration of Montreal", "# Reasoning Path:\nMontreal -> base.biblioness.bibs_location.state -> Qu\u00e9bec -> location.administrative_division.country -> Canada\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nMontreal -> base.biblioness.bibs_location.state -> Qu\u00e9bec -> sports.sports_team_location.teams -> Montreal Canadiens\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nMontreal -> location.location.containedby -> Canada -> base.aareas.schema.administrative_area.administrative_children -> Qu\u00e9bec\n# Answer:\nCanada", "# Reasoning Path:\nMontreal -> location.location.containedby -> Urban agglomeration of Montreal -> location.location.containedby -> Canada\n# Answer:\nUrban agglomeration of Montreal"], "ground_truth": ["Qu\u00e9bec"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1378", "prediction": ["# Reasoning Path:\nKellan Lutz -> film.actor.film -> g.11b7qz5rxs\n# Answer:\ng.11b7qz5rxs", "# Reasoning Path:\nKellan Lutz -> film.actor.film -> g.11b7xkpw1f\n# Answer:\ng.11b7xkpw1f", "# Reasoning Path:\nKellan Lutz -> film.actor.film -> g.11bws79p_d\n# Answer:\ng.11bws79p_d", "# Reasoning Path:\nKellan Lutz -> common.topic.webpage -> m.05cl_2n -> common.webpage.category -> Official Website\n# Answer:\nm.05cl_2n", "# Reasoning Path:\nKellan Lutz -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nKellan Lutz -> common.topic.webpage -> m.05cl_2n -> common.webpage.resource -> m.0bkkh14\n# Answer:\nm.05cl_2n", "# Reasoning Path:\nKellan Lutz -> common.topic.webpage -> m.09wb24h -> common.webpage.resource -> 'Twilight' stars, Dakota Fanning talk 'New Moon'\n# Answer:\nm.09wb24h", "# Reasoning Path:\nKellan Lutz -> common.topic.webpage -> m.09wb51n -> common.webpage.resource -> 'Twilight' stars, Dakota Fanning talk 'New Moon'\n# Answer:\nm.09wb51n", "# Reasoning Path:\nKellan Lutz -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nKellan Lutz -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nMale"], "ground_truth": ["Rick Leland"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1379", "prediction": ["# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Composer -> type.type.expected_by -> composer\n# Answer:\nComposer", "# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Pianist -> music.musician_profession.instrument -> Piano\n# Answer:\nPianist", "# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Composer -> type.type.expected_by -> Music\n# Answer:\nComposer", "# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Composer -> people.profession.specialization_of -> Musician\n# Answer:\nComposer", "# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Priest -> common.topic.notable_types -> Profession\n# Answer:\nPriest", "# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Composer -> base.descriptive_names.names.descriptive_name -> m.01260py_\n# Answer:\nComposer", "# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Composer -> type.type.expected_by -> Arranger\n# Answer:\nComposer", "# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Composer -> base.descriptive_names.names.descriptive_name -> m.012ggp0l\n# Answer:\nComposer", "# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Pianist -> base.descriptive_names.names.descriptive_name -> m.0106bbms\n# Answer:\nPianist", "# Reasoning Path:\nLife of Franz Liszt -> base.kwebbase.kwtopic.connections_to -> achille-claude debussy played for franz liszt -> base.kwebbase.kwconnection.relation -> played for\n# Answer:\nachille-claude debussy played for franz liszt"], "ground_truth": ["Priest", "Virtuoso", "Pianist", "Composer", "Film Score Composer"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7200000000000001, "ans_precission": 0.9, "ans_recall": 0.6, "path_f1": 0.34285714285714286, "path_precision": 0.3, "path_recall": 0.4, "path_ans_f1": 0.7200000000000001, "path_ans_precision": 0.9, "path_ans_recall": 0.6}
{"id": "WebQTest-138", "prediction": ["# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Basque Language -> language.human_language.main_country -> France\n# Answer:\nBasque Language", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\ng.1hhc37x4q", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Andorra\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.region -> Europe\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60xs9d2\n# Answer:\ng.11b60xs9d2", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> language.human_language.countries_spoken_in -> Andorra\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Basque Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nBasque Language", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc38hhk\n# Answer:\ng.1hhc38hhk", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Gibraltar\n# Answer:\nSpanish Language"], "ground_truth": ["Catalan language", "Galician Language", "Spanish Language", "Basque Language", "Occitan language"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6461538461538462, "ans_precission": 0.7, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.6461538461538462, "path_ans_precision": 0.7, "path_ans_recall": 0.6}
{"id": "WebQTest-1380", "prediction": ["# Reasoning Path:\nNikola Tesla -> people.person.profession -> Futurist -> common.topic.notable_types -> Profession\n# Answer:\nFuturist", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Futurist -> common.topic.article -> m.04q6b4\n# Answer:\nFuturist", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Futurist -> people.profession.people_with_this_profession -> Alfred Webre\n# Answer:\nFuturist", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Scientist -> common.topic.subject_of -> Autobiography\n# Answer:\nScientist", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Scientist -> tv.tv_subject.tv_programs -> The Secret Life of Scientists & Engineers\n# Answer:\nScientist", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Inventor -> base.descriptive_names.names.descriptive_name -> m.0101hljz\n# Answer:\nInventor", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Futurist -> people.profession.people_with_this_profession -> Allen M. Tough\n# Answer:\nFuturist", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Scientist -> common.topic.subject_of -> Biography\n# Answer:\nScientist", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Scientist -> base.descriptive_names.names.descriptive_name -> m.01053kvm\n# Answer:\nScientist", "# Reasoning Path:\nNikola Tesla -> book.author.works_written -> 'Death Ray' for Planes -> book.written_work.original_language -> English Language\n# Answer:\n'Death Ray' for Planes"], "ground_truth": ["Physicist", "Futurist", "Inventor", "Scientist", "Mechanical Engineer", "Electrical engineer"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6428571428571429, "ans_precission": 0.9, "ans_recall": 0.5, "path_f1": 0.6428571428571429, "path_precision": 0.9, "path_recall": 0.5, "path_ans_f1": 0.6428571428571429, "path_ans_precision": 0.9, "path_ans_recall": 0.5}
{"id": "WebQTest-1381", "prediction": ["# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08540 -> location.location.contains -> 08540-6449\n# Answer:\n08540", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08540 -> location.location.geolocation -> m.03dtqfp\n# Answer:\n08540", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08540 -> location.postal_code.country -> United States of America\n# Answer:\n08540", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08541 -> common.topic.notable_types -> Postal Code\n# Answer:\n08541", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08540 -> location.location.contains -> Colross\n# Answer:\n08540", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08540-6449 -> location.location.containedby -> 08540\n# Answer:\n08540-6449", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08540-6449 -> common.topic.notable_types -> Postal Code\n# Answer:\n08540-6449", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08540 -> location.location.contains -> Forrestal Village\n# Answer:\n08540", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08540-6449 -> common.topic.notable_for -> g.1ypm_h1bc\n# Answer:\n08540-6449", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08541 -> common.topic.notable_for -> g.12551k_bx\n# Answer:\n08541"], "ground_truth": ["08542", "08540", "08540-6449", "08542-4511", "08544", "08543", "08541"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 1.0, "ans_recall": 0.42857142857142855, "path_f1": 0.36363636363636365, "path_precision": 0.5, "path_recall": 0.2857142857142857, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1382", "prediction": ["# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.sports -> Basketball\n# Answer:\n1952 Summer Olympics", "# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> Uruguay\n# Answer:\n1952 Summer Olympics", "# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 1900 Summer Olympics -> olympics.olympic_games.participating_countries -> Mexico\n# Answer:\n1900 Summer Olympics", "# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> freebase.valuenotation.is_reviewed -> Start date\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Brazil\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Asiatic Boy -> base.thoroughbredracing.thoroughbred_racehorse.color -> Chestnut\n# Answer:\nAsiatic Boy", "# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> Brazil\n# Answer:\n1952 Summer Olympics", "# Reasoning Path:\nArgentina -> location.location.partially_contains -> Alto San Juan -> location.location.partially_contained_by -> m.0wg8lvc\n# Answer:\nAlto San Juan", "# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Chile\n# Answer:\n2014 Winter Olympics"], "ground_truth": ["Uruguay", "Paraguay", "Bolivia", "Brazil", "Chile"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.05442176870748299, "path_precision": 0.4, "path_recall": 0.029197080291970802, "path_ans_f1": 0.48, "path_ans_precision": 0.4, "path_ans_recall": 0.6}
{"id": "WebQTest-1383", "prediction": ["# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> language.human_language.region -> Europe\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.official_language -> Icelandic Language -> language.human_language.region -> Europe\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> base.rosetta.languoid.local_name -> Icelandic\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> common.topic.article -> m.01gmks\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.statistical_region.gni_in_ppp_dollars -> g.11b61jk1t5\n# Answer:\ng.11b61jk1t5", "# Reasoning Path:\nIceland -> location.country.official_language -> Icelandic Language -> base.rosetta.languoid.local_name -> Icelandic\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> common.topic.article -> m.02nt727\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.official_language -> Icelandic Language -> common.topic.article -> m.01gmks\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.statistical_region.gni_in_ppp_dollars -> g.1245_7d01\n# Answer:\ng.1245_7d01", "# Reasoning Path:\nIceland -> location.country.official_language -> Icelandic Language -> common.topic.article -> m.02nt727\n# Answer:\nIcelandic Language"], "ground_truth": ["Icelandic Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1384", "prediction": ["# Reasoning Path:\nIsrael -> location.statistical_region.religions -> m.0468s5y -> location.religion_percentage.religion -> Judaism\n# Answer:\nm.0468s5y", "# Reasoning Path:\nIsrael -> media_common.quotation.author -> Aaron Friedman -> people.person.religion -> Judaism\n# Answer:\nAaron Friedman", "# Reasoning Path:\nIsrael -> symbols.namesake.named_after -> Jacob -> people.person.religion -> Judaism\n# Answer:\nJacob", "# Reasoning Path:\nIsrael -> symbols.namesake.named_after -> Jacob -> people.deceased_person.place_of_death -> Egypt\n# Answer:\nJacob", "# Reasoning Path:\nIsrael -> media_common.quotation.author -> Aaron Friedman -> people.person.nationality -> United States of America\n# Answer:\nAaron Friedman"], "ground_truth": ["Judaism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.6666666666666665, "path_precision": 0.6, "path_recall": 0.75, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1385", "prediction": ["# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.04xg_yc -> american_football.football_historical_coach_position.coach -> Dom Capers\n# Answer:\nm.04xg_yc", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.0j81th4 -> american_football.football_historical_coach_position.coach -> John Fox\n# Answer:\nm.0j81th4", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.04xg_yl -> american_football.football_historical_coach_position.coach -> George Seifert\n# Answer:\nm.04xg_yl", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> award.award_winner.awards_won -> m.0_qw472\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.0j81th4 -> american_football.football_historical_coach_position.position -> Head coach\n# Answer:\nm.0j81th4", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> people.person.profession -> Coach\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> people.person.education -> m.0g5vwbs\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> people.person.education -> m.0n0rkd4\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.away_games -> Carolina Panthers at Arizona Cardinals, 2009-11-01 -> american_football.football_game.receiving -> m.07z2llv\n# Answer:\nCarolina Panthers at Arizona Cardinals, 2009-11-01", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> people.person.profession -> American Football coach\n# Answer:\nRon Rivera"], "ground_truth": ["George Seifert", "Dom Capers", "John Fox"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4, "path_precision": 0.3, "path_recall": 0.6, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1387", "prediction": ["# Reasoning Path:\nToronto -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nToronto -> common.topic.webpage -> m.02nc8t6 -> common.webpage.resource -> City of Toronto Web site\n# Answer:\nm.02nc8t6", "# Reasoning Path:\nToronto -> travel.travel_destination.local_transportation -> GO Transit -> common.topic.notable_types -> Mass Transportation System\n# Answer:\nGO Transit", "# Reasoning Path:\nToronto -> common.topic.webpage -> m.059h8f0 -> common.webpage.resource -> Visitor Information\n# Answer:\nm.059h8f0", "# Reasoning Path:\nToronto -> common.topic.webpage -> m.02nc8t6 -> common.webpage.category -> Topic Webpage\n# Answer:\nm.02nc8t6", "# Reasoning Path:\nToronto -> common.topic.webpage -> m.059h8f0 -> common.webpage.category -> Topic Webpage\n# Answer:\nm.059h8f0", "# Reasoning Path:\nToronto -> common.topic.webpage -> m.09w_q2h -> common.webpage.resource -> David Cronenberg journeys to 'Cosmopolis'\n# Answer:\nm.09w_q2h", "# Reasoning Path:\nToronto -> travel.travel_destination.local_transportation -> Line 3 Scarborough -> metropolitan_transit.transit_line.stops -> Ellesmere\n# Answer:\nLine 3 Scarborough", "# Reasoning Path:\nToronto -> travel.travel_destination.local_transportation -> Line 3 Scarborough -> common.topic.webpage -> m.059_864\n# Answer:\nLine 3 Scarborough", "# Reasoning Path:\nToronto -> common.topic.webpage -> m.09w_q2h -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w_q2h"], "ground_truth": ["Eastern Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1388", "prediction": ["# Reasoning Path:\nToussaint Louverture -> people.person.nationality -> France -> location.location.containedby -> Europe\n# Answer:\nFrance", "# Reasoning Path:\nToussaint Louverture -> people.person.nationality -> France -> location.location.containedby -> Western Europe\n# Answer:\nFrance", "# Reasoning Path:\nToussaint Louverture -> people.person.nationality -> France -> location.location.primarily_containedby -> Europe\n# Answer:\nFrance", "# Reasoning Path:\nToussaint Louverture -> people.person.nationality -> France -> location.location.partially_containedby -> Europe\n# Answer:\nFrance", "# Reasoning Path:\nToussaint Louverture -> people.person.nationality -> France -> location.location.primarily_containedby -> Western Europe\n# Answer:\nFrance", "# Reasoning Path:\nToussaint Louverture -> people.person.children -> Isaac Louverture -> common.topic.notable_types -> Person\n# Answer:\nIsaac Louverture", "# Reasoning Path:\nToussaint Louverture -> people.person.nationality -> France -> location.location.partially_containedby -> Western Europe\n# Answer:\nFrance", "# Reasoning Path:\nToussaint Louverture -> military.military_person.service -> m.0123t_s0 -> military.military_service.military_force -> French Army\n# Answer:\nm.0123t_s0", "# Reasoning Path:\nToussaint Louverture -> people.person.children -> Placide Louverture -> people.person.parents -> Suzanne Simone Baptiste Louverture\n# Answer:\nPlacide Louverture", "# Reasoning Path:\nToussaint Louverture -> people.person.children -> Isaac Louverture -> people.person.gender -> Male\n# Answer:\nIsaac Louverture"], "ground_truth": ["Haitian Rebellion of 1891", "French Revolutionary Army", "French Army"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.15384615384615383, "path_ans_precision": 0.1, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1389", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.notable_types -> Profession\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.article -> m.0knj9\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Bard\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.profession -> Writer\n# Answer:\nAnne Bront\u00eb", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.image -> Hope-coventina01a\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Poet\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Author -> tv.tv_subject.tv_episodes -> Author David Kerr - The Ol' Turkey Hunter\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Cardiovascular disease\n# Answer:\nMale"], "ground_truth": ["Writer", "Author", "Bard", "Poet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.8, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-139", "prediction": ["# Reasoning Path:\nNational Rifle Association -> common.topic.webpage -> m.03lsdwh -> common.webpage.category -> Official Website\n# Answer:\nm.03lsdwh", "# Reasoning Path:\nNational Rifle Association -> common.topic.webpage -> m.03lsdwh -> common.webpage.resource -> m.0bkhqtx\n# Answer:\nm.03lsdwh", "# Reasoning Path:\nNational Rifle Association -> common.topic.notable_types -> Membership organization -> freebase.type_profile.strict_included_types -> Organization\n# Answer:\nMembership organization", "# Reasoning Path:\nNational Rifle Association -> common.topic.webpage -> m.042276w -> common.webpage.resource -> New York Times\n# Answer:\nm.042276w", "# Reasoning Path:\nNational Rifle Association -> common.topic.notable_types -> Membership organization -> type.type.expected_by -> Organization\n# Answer:\nMembership organization", "# Reasoning Path:\nNational Rifle Association -> common.topic.notable_types -> Membership organization -> freebase.type_hints.included_types -> Organization\n# Answer:\nMembership organization", "# Reasoning Path:\nNational Rifle Association -> organization.membership_organization.members -> m.03yt7ns -> organization.organization_membership.member -> Charlton Heston\n# Answer:\nm.03yt7ns", "# Reasoning Path:\nNational Rifle Association -> common.topic.webpage -> m.042276w -> common.webpage.category -> Topic Webpage\n# Answer:\nm.042276w", "# Reasoning Path:\nNational Rifle Association -> common.topic.notable_types -> Membership organization -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nMembership organization", "# Reasoning Path:\nNational Rifle Association -> common.topic.notable_types -> Membership organization -> freebase.type_hints.included_types -> Topic\n# Answer:\nMembership organization"], "ground_truth": ["Fairfax"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1390", "prediction": ["# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0hzbyqy -> government.government_position_held.office_holder -> Mark Shurtleff\n# Answer:\nm.0hzbyqy", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0zxk98b -> government.government_position_held.office_holder -> Sean Reyes\n# Answer:\nm.0zxk98b", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0hzbyqy -> government.government_position_held.basic_title -> Attorney general\n# Answer:\nm.0hzbyqy", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0nbwncq -> government.government_position_held.office_holder -> John Swallow\n# Answer:\nm.0nbwncq", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0hzbyqy -> government.government_position_held.office_position_or_title -> Utah Attorney General\n# Answer:\nm.0hzbyqy", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0nbwncq -> government.government_position_held.office_position_or_title -> Utah Attorney General\n# Answer:\nm.0nbwncq", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0zxk98b -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0zxk98b", "# Reasoning Path:\nUtah -> book.book_subject.works -> Blossoms of faith -> common.topic.notable_for -> g.125fjpyr7\n# Answer:\nBlossoms of faith", "# Reasoning Path:\nUtah -> travel.travel_destination.tour_operators -> Adventures by Disney -> travel.tour_operator.travel_destinations -> Abu Simbel temples\n# Answer:\nAdventures by Disney", "# Reasoning Path:\nUtah -> book.book_subject.works -> Blossoms of faith -> common.topic.notable_types -> Book\n# Answer:\nBlossoms of faith"], "ground_truth": ["Sean Reyes", "Mark Shurtleff", "John Swallow"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1392", "prediction": ["# Reasoning Path:\nIndonesia -> location.country.languages_spoken -> Tobelo Language -> common.topic.notable_types -> Human Language\n# Answer:\nTobelo Language", "# Reasoning Path:\nIndonesia -> location.country.languages_spoken -> Tobelo Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nTobelo Language", "# Reasoning Path:\nIndonesia -> location.country.languages_spoken -> Indonesian Language -> language.human_language.dialects -> Bali Language\n# Answer:\nIndonesian Language", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60ptk2z\n# Answer:\ng.11b60ptk2z", "# Reasoning Path:\nIndonesia -> location.country.languages_spoken -> Bali Language -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nBali Language", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.12cp_j7n1\n# Answer:\ng.12cp_j7n1", "# Reasoning Path:\nIndonesia -> location.country.languages_spoken -> Bali Language -> language.human_language.region -> Asia\n# Answer:\nBali Language", "# Reasoning Path:\nIndonesia -> location.country.languages_spoken -> Indonesian Language -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nIndonesian Language", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_4m6h\n# Answer:\ng.1245_4m6h", "# Reasoning Path:\nIndonesia -> location.country.languages_spoken -> Indonesian Language -> language.human_language.countries_spoken_in -> Timor-Leste\n# Answer:\nIndonesian Language"], "ground_truth": ["Bali Language", "Malay Language", "Sunda Language", "Madura Language", "Indonesian Language", "English Language", "Javanese Language", "Dutch Language", "Batak Language", "Tobelo Language"], "ans_acc": 0.3, "ans_hit": 1, "ans_f1": 0.42, "ans_precission": 0.7, "ans_recall": 0.3, "path_f1": 0.42, "path_precision": 0.7, "path_recall": 0.3, "path_ans_f1": 0.42, "path_ans_precision": 0.7, "path_ans_recall": 0.3}
{"id": "WebQTest-1394", "prediction": ["# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> award.award_winner.awards_won -> m.0_qw472\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.0j81th4 -> american_football.football_historical_coach_position.coach -> John Fox\n# Answer:\nm.0j81th4", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> people.person.profession -> Coach\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> people.person.education -> m.0g5vwbs\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.0j81th4 -> american_football.football_historical_coach_position.position -> Head coach\n# Answer:\nm.0j81th4", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.04xg_yc -> american_football.football_historical_coach_position.coach -> Dom Capers\n# Answer:\nm.04xg_yc", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.coaches -> m.0_3r22s -> sports.sports_team_coach_tenure.coach -> John Fox\n# Answer:\nm.0_3r22s", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> people.person.education -> m.0n0rkd4\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.coaches -> m.0_3r22s -> sports.sports_team_coach_tenure.position -> Head coach\n# Answer:\nm.0_3r22s", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> people.person.profession -> American Football coach\n# Answer:\nRon Rivera"], "ground_truth": ["Ron Rivera"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1395", "prediction": ["# Reasoning Path:\nSouth Africa -> location.location.partially_contains -> Njesuthi -> location.location.partially_containedby -> Lesotho\n# Answer:\nNjesuthi", "# Reasoning Path:\nSouth Africa -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Botswana\n# Answer:\nEnglish Language", "# Reasoning Path:\nSouth Africa -> location.location.partially_contains -> Caledon River -> geography.river.basin_countries -> Lesotho\n# Answer:\nCaledon River", "# Reasoning Path:\nSouth Africa -> location.statistical_region.electricity_consumption_per_capita -> g.1245_4m5f\n# Answer:\ng.1245_4m5f", "# Reasoning Path:\nSouth Africa -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Namibia\n# Answer:\nEnglish Language", "# Reasoning Path:\nSouth Africa -> location.country.languages_spoken -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nSouth Africa -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Tanzania\n# Answer:\nArabic Language", "# Reasoning Path:\nSouth Africa -> location.country.languages_spoken -> Dutch Language -> language.human_language.countries_spoken_in -> Namibia\n# Answer:\nDutch Language", "# Reasoning Path:\nSouth Africa -> location.location.partially_contains -> Cleft Peak -> location.location.partially_containedby -> Lesotho\n# Answer:\nCleft Peak", "# Reasoning Path:\nSouth Africa -> location.country.languages_spoken -> Arabic Language -> common.topic.notable_types -> Human Language\n# Answer:\nArabic Language"], "ground_truth": ["Swaziland", "Mozambique", "Namibia", "Botswana", "Zimbabwe", "Lesotho"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.12820512820512822, "path_precision": 0.5, "path_recall": 0.07352941176470588, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.6, "path_ans_recall": 0.5}
{"id": "WebQTest-1396", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.actor -> James Earl Jones\n# Answer:\nm.03jr0x9", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.actor -> James Earl Jones\n# Answer:\nm.02nwtmz", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.special_performance_type -> Voice\n# Answer:\nm.03jr0x9", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.film -> The Benchwarmers\n# Answer:\nm.03jr0x9", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.film -> Return of the Jedi\n# Answer:\nm.02nwtmz", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.part_of_series -> Jedi Quest\n# Answer:\nPath to Truth"], "ground_truth": ["David Prowse", "Sebastian Shaw", "James Earl Jones"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.28571428571428564, "path_precision": 0.3, "path_recall": 0.2727272727272727, "path_ans_f1": 0.3157894736842105, "path_ans_precision": 0.3, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1398", "prediction": ["# Reasoning Path:\nJoan Crawford -> award.award_nominee.award_nominations -> m.03mlww8 -> award.award_nomination.nominated_for -> Possessed\n# Answer:\nm.03mlww8", "# Reasoning Path:\nJoan Crawford -> award.award_nominee.award_nominations -> m.03mlwz0 -> award.award_nomination.nominated_for -> Sudden Fear\n# Answer:\nm.03mlwz0", "# Reasoning Path:\nJoan Crawford -> award.award_nominee.award_nominations -> m.03mlww8 -> award.award_nomination.award -> Academy Award for Best Actress\n# Answer:\nm.03mlww8", "# Reasoning Path:\nJoan Crawford -> award.award_nominee.award_nominations -> m.03mlwz0 -> award.award_nomination.ceremony -> 25th Academy Awards\n# Answer:\nm.03mlwz0", "# Reasoning Path:\nJoan Crawford -> award.award_nominee.award_nominations -> m.03mlwz0 -> award.award_nomination.award -> Academy Award for Best Actress\n# Answer:\nm.03mlwz0", "# Reasoning Path:\nJoan Crawford -> award.award_nominee.award_nominations -> m.05bz0fg -> award.award_nomination.award -> Academy Award for Best Actress\n# Answer:\nm.05bz0fg", "# Reasoning Path:\nJoan Crawford -> film.actor.film -> m.0115tf9q -> film.performance.character -> Della Chappell\n# Answer:\nm.0115tf9q", "# Reasoning Path:\nJoan Crawford -> tv.tv_actor.guest_roles -> m.0bv_0kc -> tv.tv_guest_role.episodes_appeared_in -> Strange Witness\n# Answer:\nm.0bv_0kc", "# Reasoning Path:\nJoan Crawford -> award.award_nominee.award_nominations -> m.05bz0fg -> award.award_nomination.nominated_for -> Mildred Pierce\n# Answer:\nm.05bz0fg", "# Reasoning Path:\nJoan Crawford -> film.actor.film -> m.0115tf9q -> film.performance.film -> Della\n# Answer:\nm.0115tf9q"], "ground_truth": ["Paris", "Chained", "I Saw What You Did", "Our Blushing Brides", "Laughing Sinners", "The Caretakers", "Berserk!", "Flamingo Road", "Female on the Beach", "Untamed", "Dancing Lady", "The Merry Widow", "Paid", "Winners of the Wilderness", "Daisy Kenyon", "West Point", "The Circle", "The Law of the Range", "The Shining Hour", "Rain", "Possessed", "Della", "When Ladies Meet", "The Taxi Dancer", "The Understanding Heart", "The Women", "The Karate Killers", "Across to Singapore", "Autumn Leaves", "Montana Moon", "They All Kissed the Bride", "The Boob", "It's a Great Feeling", "A Woman's Face", "Goodbye, My Fancy", "Twelve Miles Out", "The Gorgeous Hussy", "The Last of Mrs. Cheyney", "Four Walls", "Harry Langdon: The Forgotten Clown", "Reunion in France", "The Best of Everything", "Pretty Ladies", "Dream of Love", "Today We Live", "No More Ladies", "Old Clothes", "The Duke Steps Out", "Queen Bee", "Strange Cargo", "Johnny Guitar", "Dance, Fools, Dance", "Mannequin", "The Damned Don't Cry!", "Forsaking All Others", "The Bride Wore Red", "Above Suspicion", "A Slave of Fashion", "Mildred Pierce", "Harriet Craig", "I Live My Life", "Trog", "The Ice Follies of 1939", "Our Modern Maidens", "Rose-Marie", "The Story of Esther Costello", "Love on the Run", "Sally, Irene and Mary", "Letty Lynton", "Our Dancing Daughters", "The Unknown", "What Ever Happened to Baby Jane?", "Tide of Empire", "Strait-Jacket", "The Hollywood Revue of 1929", "The Stolen Jools", "Hush\u2026 Hush, Sweet Charlotte", "Spring Fever", "Great Day", "Susan and God", "This Woman Is Dangerous", "Sudden Fear", "Torch Song", "Humoresque", "Tramp, Tramp, Tramp", "This Modern Age", "Sadie McKee", "Grand Hotel"], "ans_acc": 0.045454545454545456, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.06936416184971098, "path_precision": 0.3, "path_recall": 0.0392156862745098, "path_ans_f1": 0.08333333333333334, "path_ans_precision": 0.5, "path_ans_recall": 0.045454545454545456}
{"id": "WebQTest-1399", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nLojban", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nSpanish Language", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nLojban", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Lojban -> language.human_language.region -> Americas\n# Answer:\nLojban", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_children -> Mississippi -> freebase.valuenotation.has_no_value -> Minimum wage\n# Answer:\nMississippi", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nSpanish Language", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\ng.125_r6z32", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Northern Mariana Islands\n# Answer:\nSpanish Language", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Abenaki language -> language.human_language.region -> Americas\n# Answer:\nAbenaki language", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_children -> Mississippi -> freebase.valuenotation.is_reviewed -> Minimum wage\n# Answer:\nMississippi"], "ground_truth": ["Pharmaceutical Preparation", "Food Manufacturing", "Automotive industry", "Industrial Organic Chemicals, NEC"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.location.containedby -> Asia\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nm.0clv1h_", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> common.topic.image -> Okuma town office\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Aichi Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.country.administrative_divisions -> Aichi Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> common.topic.notable_types -> City/Town/Village\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_for -> g.125btmfy0\n# Answer:\nFukushima I \u2013 1", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Akita Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Aomori Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.country.administrative_divisions -> Akita Prefecture\n# Answer:\nJapan"], "ground_truth": ["Japan", "Okuma"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1402", "prediction": ["# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.0hz9xtw -> location.location_symbol_relationship.symbol -> Cardinal\n# Answer:\nm.0hz9xtw", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.0hz9xtw -> location.location_symbol_relationship.Kind_of_symbol -> State bird\n# Answer:\nm.0hz9xtw", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0d65 -> location.location_symbol_relationship.symbol -> Brachiopod\n# Answer:\nm.04l0d65", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0nqp -> location.location_symbol_relationship.symbol -> Liriodendron\n# Answer:\nm.04l0nqp", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0d65 -> location.location_symbol_relationship.Kind_of_symbol -> State fossil\n# Answer:\nm.04l0d65", "# Reasoning Path:\nKentucky -> military.military_unit_place_of_origin.military_units -> 10th Kentucky Cavalry Regiment (Diamond's) -> common.topic.notable_types -> Military unit\n# Answer:\n10th Kentucky Cavalry Regiment (Diamond's)", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0nqp -> location.location_symbol_relationship.Kind_of_symbol -> State tree\n# Answer:\nm.04l0nqp", "# Reasoning Path:\nKentucky -> common.topic.webpage -> m.09wljkf -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09wljkf", "# Reasoning Path:\nKentucky -> military.military_unit_place_of_origin.military_units -> 10th Kentucky Cavalry Regiment (Diamond's) -> military.military_unit.armed_force -> Confederate States Army\n# Answer:\n10th Kentucky Cavalry Regiment (Diamond's)", "# Reasoning Path:\nKentucky -> military.military_unit_place_of_origin.military_units -> 10th Regiment Kentucky Volunteer Cavalry -> common.topic.notable_for -> g.125bm53w8\n# Answer:\n10th Regiment Kentucky Volunteer Cavalry"], "ground_truth": ["Cardinal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1403", "prediction": ["# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.04kc_7_ -> government.government_position_held.office_holder -> Jean Chr\u00e9tien\n# Answer:\nm.04kc_7_", "# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.04kk22r -> government.government_position_held.office_holder -> Brian Mulroney\n# Answer:\nm.04kk22r", "# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.04kmh11 -> government.government_position_held.office_holder -> Alexander Mackenzie\n# Answer:\nm.04kmh11", "# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.04kc_7_ -> government.government_position_held.jurisdiction_of_office -> Canada\n# Answer:\nm.04kc_7_", "# Reasoning Path:\nPrime Minister of Canada -> base.schemastaging.context_name.pronunciation -> g.125_rtp7r\n# Answer:\ng.125_rtp7r", "# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.04kc_7_ -> government.government_position_held.governmental_body -> Parliament of Canada\n# Answer:\nm.04kc_7_", "# Reasoning Path:\nPrime Minister of Canada -> base.firsts.achievement.firsts -> m.04wvp27 -> base.firsts.first_achievement.first -> Kim Campbell\n# Answer:\nm.04wvp27", "# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.04kk22r -> government.government_position_held.basic_title -> Prime minister\n# Answer:\nm.04kk22r", "# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.04kk22r -> government.government_position_held.governmental_body -> Parliament of Canada\n# Answer:\nm.04kk22r", "# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.04kmh11 -> government.government_position_held.basic_title -> Prime minister\n# Answer:\nm.04kmh11"], "ground_truth": ["Brian Mulroney", "Kim Campbell", "Alexander Mackenzie", "Jean Chr\u00e9tien"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1405", "prediction": ["# Reasoning Path:\nSalt Lake City -> travel.travel_destination.tourist_attractions -> Temple Square -> location.location.containedby -> Utah\n# Answer:\nTemple Square", "# Reasoning Path:\nSalt Lake City -> location.statistical_region.population -> g.11b66h2b_k\n# Answer:\ng.11b66h2b_k", "# Reasoning Path:\nSalt Lake City -> location.statistical_region.population -> g.11b7tm7k1_\n# Answer:\ng.11b7tm7k1_", "# Reasoning Path:\nSalt Lake City -> travel.travel_destination.tourist_attractions -> Antelope Island State Park -> location.location.geolocation -> m.0wmpg71\n# Answer:\nAntelope Island State Park", "# Reasoning Path:\nSalt Lake City -> location.statistical_region.population -> g.11btt563rr\n# Answer:\ng.11btt563rr", "# Reasoning Path:\nSalt Lake City -> travel.travel_destination.tourist_attractions -> Antelope Island State Park -> common.topic.notable_types -> Tourist attraction\n# Answer:\nAntelope Island State Park", "# Reasoning Path:\nSalt Lake City -> travel.travel_destination.tourist_attractions -> Bryce Canyon National Park -> location.location.contains -> Aquarius Plateau\n# Answer:\nBryce Canyon National Park", "# Reasoning Path:\nSalt Lake City -> travel.travel_destination.tourist_attractions -> Bryce Canyon National Park -> protected_sites.protected_site.iucn_category -> National park\n# Answer:\nBryce Canyon National Park", "# Reasoning Path:\nSalt Lake City -> travel.travel_destination.tourist_attractions -> Bryce Canyon National Park -> location.location.contains -> Bryce Canyon Lodge\n# Answer:\nBryce Canyon National Park", "# Reasoning Path:\nSalt Lake City -> tv.tv_location.tv_shows_filmed_here -> Proper Manors -> common.topic.webpage -> m.0jzvwvq\n# Answer:\nProper Manors"], "ground_truth": ["Discovery Gateway", "Salt Lake City Public Library", "Hogle Zoo", "Salt Lake Temple", "Trolley Square", "Red Butte Garden and Arboretum", "Temple Square", "Days of '47 Parade", "Sugar House Park", "Antelope Island State Park", "Bryce Canyon National Park", "Church History Museum", "Clark Planetarium", "Zion National Park", "g.121xqqc4"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.3, "ans_precission": 0.6, "ans_recall": 0.2, "path_f1": 0.3, "path_precision": 0.6, "path_recall": 0.2, "path_ans_f1": 0.3, "path_ans_precision": 0.6, "path_ans_recall": 0.2}
{"id": "WebQTest-1408", "prediction": ["# Reasoning Path:\nUnited States Senate -> government.governmental_body.jurisdiction -> United States of America -> government.governmental_jurisdiction.government_bodies -> United States House of Representatives\n# Answer:\nUnited States of America", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.jurisdiction -> United States of America -> government.governmental_jurisdiction.government_bodies -> United States Congress\n# Answer:\nUnited States of America", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.jurisdiction -> United States of America -> location.country.first_level_divisions -> Kansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> United States Senator -> government.government_office_or_title.jurisdiction -> United States of America\n# Answer:\nUnited States Senator", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.jurisdiction -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Kansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.jurisdiction -> United States of America -> location.country.first_level_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> United States Senator -> government.government_office_or_title.governmental_body_if_any -> United States Congress\n# Answer:\nUnited States Senator", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.jurisdiction -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nUnited States Senate -> common.topic.image -> 600px-Senate_Seal.svg.png -> common.image.appears_in_topic_gallery -> United States Senator\n# Answer:\n600px-Senate_Seal.svg.png", "# Reasoning Path:\nUnited States Senate -> common.topic.image -> 600px-Senate_Seal.svg.png -> common.image.size -> m.01x3k95\n# Answer:\n600px-Senate_Seal.svg.png"], "ground_truth": ["Sherrod Brown", "Rob Portman"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1409", "prediction": ["# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> British Sign Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nBritish Sign Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> British Sign Language -> common.topic.notable_types -> Human Language\n# Answer:\nBritish Sign Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> British Sign Language -> language.human_language.region -> Europe\n# Answer:\nBritish Sign Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Scots Language -> common.topic.notable_types -> Human Language\n# Answer:\nScots Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.countries_spoken_in -> Kingdom of Great Britain\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nUnited Kingdom -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.language_family -> Indo-European languages\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nUnited Kingdom -> location.country.official_language -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nm.0ghc35h", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.countries_spoken_in -> Scotland\n# Answer:\nScottish Gaelic"], "ground_truth": ["English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-141", "prediction": ["# Reasoning Path:\nGimli -> fictional_universe.fictional_character.parents -> Gloin -> film.film_character.portrayed_in_films -> m.0gwn5ny\n# Answer:\nGloin", "# Reasoning Path:\nGimli -> fictional_universe.fictional_character.parents -> Gloin -> fictional_universe.fictional_character.organizations -> Thorin and Company\n# Answer:\nGloin", "# Reasoning Path:\nGimli -> fictional_universe.fictional_character.parents -> Gloin -> film.film_character.portrayed_in_films -> m.0y623dj\n# Answer:\nGloin", "# Reasoning Path:\nGimli -> fictional_universe.fictional_character.species -> Dwarf -> fictional_universe.character_species.characters_of_this_species -> Balin\n# Answer:\nDwarf", "# Reasoning Path:\nGimli -> fictional_universe.fictional_character.species -> Dwarf -> common.topic.article -> m.0h16z\n# Answer:\nDwarf", "# Reasoning Path:\nGimli -> fictional_universe.fictional_character.species -> Dwarf -> common.topic.image -> Cirth\n# Answer:\nDwarf", "# Reasoning Path:\nGimli -> fictional_universe.fictional_character.species -> Dwarf -> fictional_universe.character_species.characters_of_this_species -> D\u00e1in II Ironfoot\n# Answer:\nDwarf", "# Reasoning Path:\nGimli -> fictional_universe.fictional_character.species -> Dwarf -> fictional_universe.character_species.characters_of_this_species -> Fili\n# Answer:\nDwarf", "# Reasoning Path:\nGimli -> film.film_character.portrayed_in_films -> m.03l6qbh -> film.performance.actor -> John Rhys-Davies\n# Answer:\nm.03l6qbh", "# Reasoning Path:\nGimli -> film.film_character.portrayed_in_films -> m.03l6qbh -> film.performance.film -> The Lord of the Rings: The Two Towers\n# Answer:\nm.03l6qbh"], "ground_truth": ["Gloin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1410", "prediction": ["# Reasoning Path:\nSaint Nicholas -> book.book_subject.works -> St. Nicholas of Myra, Bari and Manhattan: Biography of a Legend -> book.book.editions -> Saint Nicholas of Myra, Bari, and Manhattan : biography of a legend\n# Answer:\nSt. Nicholas of Myra, Bari and Manhattan: Biography of a Legend", "# Reasoning Path:\nSaint Nicholas -> book.book_subject.works -> St. Nicholas of Myra, Bari and Manhattan: Biography of a Legend -> common.topic.notable_for -> g.1256xmt38\n# Answer:\nSt. Nicholas of Myra, Bari and Manhattan: Biography of a Legend", "# Reasoning Path:\nSaint Nicholas -> people.person.place_of_birth -> Patara, Lycia -> location.location.people_born_here -> Aedesius of Alexandria\n# Answer:\nPatara, Lycia", "# Reasoning Path:\nSaint Nicholas -> people.person.place_of_birth -> Patara, Lycia -> travel.travel_destination.tourist_attractions -> Patara Beach\n# Answer:\nPatara, Lycia", "# Reasoning Path:\nSaint Nicholas -> symbols.name_source.namesakes -> Basilica di San Nicola -> common.topic.article -> m.0dw4h0\n# Answer:\nBasilica di San Nicola", "# Reasoning Path:\nSaint Nicholas -> people.person.place_of_birth -> Patara, Lycia -> common.topic.notable_for -> g.125g6zs00\n# Answer:\nPatara, Lycia", "# Reasoning Path:\nSaint Nicholas -> symbols.name_source.namesakes -> Agios Nikolaos -> location.location.containedby -> Crete\n# Answer:\nAgios Nikolaos", "# Reasoning Path:\nSaint Nicholas -> symbols.name_source.namesakes -> Agios Nikolaos -> location.location.geolocation -> m.0d84dct\n# Answer:\nAgios Nikolaos", "# Reasoning Path:\nSaint Nicholas -> symbols.name_source.namesakes -> Basilica di San Nicola -> location.location.geolocation -> m.0wm95d9\n# Answer:\nBasilica di San Nicola", "# Reasoning Path:\nSaint Nicholas -> symbols.name_source.namesakes -> Agios Nikolaos -> location.location.containedby -> Greece\n# Answer:\nAgios Nikolaos"], "ground_truth": ["Bishop"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1414", "prediction": ["# Reasoning Path:\nPope John Paul II -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Male\n# Answer:\nheart attack", "# Reasoning Path:\nPope John Paul II -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.parent_disease -> Cardiovascular disease\n# Answer:\nheart attack", "# Reasoning Path:\nPope John Paul II -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Old age\n# Answer:\nheart attack", "# Reasoning Path:\nPope John Paul II -> people.deceased_person.cause_of_death -> heart attack -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nheart attack", "# Reasoning Path:\nPope John Paul II -> people.deceased_person.cause_of_death -> heart attack -> medicine.risk_factor.diseases -> Major depression\n# Answer:\nheart attack", "# Reasoning Path:\nPope John Paul II -> people.deceased_person.cause_of_death -> Parkinson's disease -> medicine.disease.treatments -> Amantadine\n# Answer:\nParkinson's disease", "# Reasoning Path:\nPope John Paul II -> common.topic.webpage -> m.09xfw5c -> common.webpage.resource -> One pope to rule them all...\n# Answer:\nm.09xfw5c", "# Reasoning Path:\nPope John Paul II -> people.deceased_person.cause_of_death -> Parkinson's disease -> medicine.disease.treatments -> Anticholinergic\n# Answer:\nParkinson's disease", "# Reasoning Path:\nPope John Paul II -> people.deceased_person.cause_of_death -> Parkinson's disease -> medicine.disease.notable_people_with_this_condition -> Adolf Hitler\n# Answer:\nParkinson's disease", "# Reasoning Path:\nPope John Paul II -> symbols.name_source.namesakes -> Blessed Pope John Paul II Catholic Secondary School -> education.educational_institution.school_type -> Catholic school\n# Answer:\nBlessed Pope John Paul II Catholic Secondary School"], "ground_truth": ["heart attack", "Parkinson's disease"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1416", "prediction": ["# Reasoning Path:\nFranklin Pierce -> people.person.profession -> Politician -> common.topic.notable_types -> Profession\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin Pierce -> people.person.profession -> Lawyer -> common.topic.notable_types -> Profession\n# Answer:\nLawyer", "# Reasoning Path:\nFranklin Pierce -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdn_\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin Pierce -> people.person.profession -> Politician -> people.profession.specializations -> Mayor of Auckland\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin Pierce -> common.topic.notable_types -> US President -> type.type.expected_by -> President\n# Answer:\nUS President", "# Reasoning Path:\nFranklin Pierce -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdq3\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin Pierce -> people.person.profession -> Lawyer -> common.topic.subjects -> Accident Advice Helpline\n# Answer:\nLawyer", "# Reasoning Path:\nFranklin Pierce -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101h_l2\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin Pierce -> people.person.profession -> Politician -> people.profession.specializations -> AIDS Policy Specialist\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin Pierce -> people.person.profession -> Lawyer -> common.topic.subject_of -> Accident Advice Helpline\n# Answer:\nLawyer"], "ground_truth": ["US President"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1417", "prediction": ["# Reasoning Path:\nBoston Bruins -> sports.sports_team.championships -> 1929 Stanley Cup Finals -> common.topic.article -> m.02rnkk5\n# Answer:\n1929 Stanley Cup Finals", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.arena_stadium -> TD Garden -> sports.sports_facility.teams -> Boston Celtics\n# Answer:\nTD Garden", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.championships -> 2011 Stanley Cup Finals -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n2011 Stanley Cup Finals", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.championships -> 2011 Stanley Cup Finals -> common.topic.article -> m.0glrlkh\n# Answer:\n2011 Stanley Cup Finals", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.championships -> 1939 Stanley Cup Finals -> common.topic.article -> m.03c6y4r\n# Answer:\n1939 Stanley Cup Finals", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.championships -> 1929 Stanley Cup Finals -> common.topic.notable_for -> g.125dsx3g8\n# Answer:\n1929 Stanley Cup Finals", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.championships -> 2011 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n2011 Stanley Cup Finals", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.championships -> 2011 Stanley Cup Finals -> freebase.valuenotation.is_reviewed -> Location(s)\n# Answer:\n2011 Stanley Cup Finals", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.championships -> 1939 Stanley Cup Finals -> common.topic.notable_for -> g.125br8jr8\n# Answer:\n1939 Stanley Cup Finals", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.arena_stadium -> TD Garden -> common.topic.notable_types -> Sports Facility\n# Answer:\nTD Garden"], "ground_truth": ["2013 Stanley Cup Finals"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1418", "prediction": ["# Reasoning Path:\nMeredith Grey -> fictional_universe.fictional_character.married_to -> m.0gx44zk -> fictional_universe.marriage_of_fictional_characters.spouses -> Derek Shepherd\n# Answer:\nm.0gx44zk", "# Reasoning Path:\nMeredith Grey -> base.italiantv.adapted_tv_character.dubbing_performances -> m.09tms_9 -> base.italiantv.tv_dubbing_performance.actor -> Giuppy Izzo\n# Answer:\nm.09tms_9", "# Reasoning Path:\nMeredith Grey -> fictional_universe.fictional_character.employers -> m.0k53dtw -> fictional_universe.fictional_employment_tenure.title -> Surgeon\n# Answer:\nm.0k53dtw", "# Reasoning Path:\nMeredith Grey -> base.italiantv.adapted_tv_character.dubbing_performances -> m.09tms_9 -> base.italiantv.tv_dubbing_performance.tv_program -> Italian adaptation of Grey's Anatomy\n# Answer:\nm.09tms_9", "# Reasoning Path:\nMeredith Grey -> fictional_universe.fictional_character.employers -> m.0k53dtw -> fictional_universe.fictional_employment_tenure.employer -> Seattle Grace Mercy West Hospital\n# Answer:\nm.0k53dtw"], "ground_truth": ["Chris Ivery"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1419", "prediction": ["# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced -> Auguste Rodin -> influence.influence_node.influenced -> Alberto Giacometti\n# Answer:\nAuguste Rodin", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced -> Auguste Rodin -> influence.influence_node.influenced -> Antoine Bourdelle\n# Answer:\nAuguste Rodin", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced -> Auguste Rodin -> influence.influence_node.influenced -> Bertram Mackennal\n# Answer:\nAuguste Rodin", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced -> Auguste Rodin -> people.deceased_person.place_of_death -> Meudon\n# Answer:\nAuguste Rodin", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced -> El Greco -> influence.influence_node.influenced -> Jack Levine\n# Answer:\nEl Greco", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced -> Auguste Rodin -> visual_art.visual_artist.artworks -> Balzac in a Frock Coat\n# Answer:\nAuguste Rodin", "# Reasoning Path:\nMichelangelo -> base.arthist.muveszek.muvek -> m.07mg2ng -> base.arthist.attribuciok.mutargy -> Bacchus\n# Answer:\nm.07mg2ng", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced -> Daniele da Volterra -> common.topic.article -> m.08qhds\n# Answer:\nDaniele da Volterra", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced -> El Greco -> influence.influence_node.influenced -> John T. Biggers\n# Answer:\nEl Greco", "# Reasoning Path:\nMichelangelo -> music.lyricist.lyrics_written -> Madrigal, op. 15 no. 1 -> music.composition.composer -> Richard Strauss\n# Answer:\nMadrigal, op. 15 no. 1"], "ground_truth": ["Giovanni Pico della Mirandola", "Donatello", "Lorenzo de' Medici", "Girolamo Savonarola", "Melozzo da Forl\u00ec", "Domenico Ghirlandaio"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-142", "prediction": ["# Reasoning Path:\nTransmission Control Protocol -> base.schemastaging.context_name.pronunciation -> g.125_qdtmg\n# Answer:\ng.125_qdtmg", "# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Robert  E. Kahn -> people.person.employment_history -> m.04_j1q_\n# Answer:\nRobert  E. Kahn", "# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Robert  E. Kahn -> people.person.gender -> Male\n# Answer:\nRobert  E. Kahn", "# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Robert  E. Kahn -> people.person.employment_history -> m.04_j1qm\n# Answer:\nRobert  E. Kahn", "# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Vint Cerf -> law.inventor.inventions -> The Internet\n# Answer:\nVint Cerf", "# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Robert  E. Kahn -> people.person.employment_history -> m.04_j1sq\n# Answer:\nRobert  E. Kahn", "# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Robert  E. Kahn -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nRobert  E. Kahn", "# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Robert  E. Kahn -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nRobert  E. Kahn", "# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Vint Cerf -> education.academic.advisors -> Gerald Estrin\n# Answer:\nVint Cerf", "# Reasoning Path:\nTransmission Control Protocol -> base.argumentmaps.original_idea.innovator -> Robert  E. Kahn -> people.person.employment_history -> m.04_j1q_\n# Answer:\nRobert  E. Kahn"], "ground_truth": ["Robert  E. Kahn", "Vint Cerf"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.6, "path_recall": 0.5, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1420", "prediction": ["# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agriculture -> business.industry.companies -> AMERICAN CRYSTAL SUGAR CO /MN/\n# Answer:\nAgriculture", "# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agriculture -> common.topic.article -> m.0hkr\n# Answer:\nAgriculture", "# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agriculture -> freebase.valuenotation.has_value -> NAICS 2007 code\n# Answer:\nAgriculture", "# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agriculture -> business.industry.companies -> AVEBE\n# Answer:\nAgriculture", "# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agrochemical -> business.industry.companies -> Aarti Industries\n# Answer:\nAgrochemical", "# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agrochemical -> base.nasagcmd.gcmd_keyword1.includes_keyword_s -> Pesticide\n# Answer:\nAgrochemical", "# Reasoning Path:\nMonsanto -> organization.organization.child -> m.09pdql9 -> organization.organization_relationship.child -> Mahyco Monsanto Biotech (I) Pvt. Ltd.\n# Answer:\nm.09pdql9", "# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agriculture -> business.industry.companies -> Adler Seeds\n# Answer:\nAgriculture", "# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agrochemical -> business.industry.companies -> Agrium\n# Answer:\nAgrochemical", "# Reasoning Path:\nMonsanto -> organization.organization.child -> m.09pdqmn -> organization.organization_relationship.child -> Semillas y Agroproductos Monsanto S.A. de C.V.\n# Answer:\nm.09pdqmn"], "ground_truth": ["Agriculture", "Seed", "Chemical industry", "Agrochemical"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.6153846153846154, "ans_precission": 0.8, "ans_recall": 0.5, "path_f1": 0.6153846153846154, "path_precision": 0.8, "path_recall": 0.5, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.8, "path_ans_recall": 0.75}
{"id": "WebQTest-1421", "prediction": ["# Reasoning Path:\nCheers -> tv.tv_program.regular_cast -> m.02hsl63 -> tv.regular_tv_appearance.character -> Norm Peterson\n# Answer:\nm.02hsl63", "# Reasoning Path:\nCheers -> tv.tv_program.regular_cast -> m.01y0qy_ -> tv.regular_tv_appearance.character -> Frasier Crane\n# Answer:\nm.01y0qy_", "# Reasoning Path:\nWoody Harrelson -> tv.tv_actor.starring_roles -> m.02t8y53 -> tv.regular_tv_appearance.character -> Woody Boyd\n# Answer:\nm.02t8y53", "# Reasoning Path:\nCheers -> tv.tv_program.regular_cast -> m.02hsl63 -> tv.regular_tv_appearance.actor -> George Wendt\n# Answer:\nm.02hsl63", "# Reasoning Path:\nCheers -> tv.tv_program.regular_cast -> m.02h7tb1 -> tv.regular_tv_appearance.character -> Rebecca Howe\n# Answer:\nm.02h7tb1", "# Reasoning Path:\nWoody Harrelson -> film.actor.film -> g.11b6g7srhd\n# Answer:\ng.11b6g7srhd", "# Reasoning Path:\nWoody Harrelson -> tv.tv_actor.starring_roles -> m.0_0mgc5 -> tv.regular_tv_appearance.character -> Martin Hart\n# Answer:\nm.0_0mgc5", "# Reasoning Path:\nWoody Harrelson -> tv.tv_actor.starring_roles -> m.0h0vpxx -> tv.regular_tv_appearance.character -> Richard\n# Answer:\nm.0h0vpxx", "# Reasoning Path:\nWoody Harrelson -> tv.tv_actor.starring_roles -> m.02t8y53 -> tv.regular_tv_appearance.seasons -> Cheers - Season 10\n# Answer:\nm.02t8y53", "# Reasoning Path:\nWoody Harrelson -> tv.tv_actor.starring_roles -> m.02t8y53 -> tv.regular_tv_appearance.series -> Cheers\n# Answer:\nm.02t8y53"], "ground_truth": ["Woody Boyd"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1422", "prediction": ["# Reasoning Path:\nSweden -> location.country.official_language -> Swedish Language -> language.human_language.countries_spoken_in -> Finland\n# Answer:\nSwedish Language", "# Reasoning Path:\nSweden -> location.country.official_language -> Swedish Language -> language.human_language.region -> Europe\n# Answer:\nSwedish Language", "# Reasoning Path:\nSweden -> location.country.official_language -> Swedish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSwedish Language", "# Reasoning Path:\nSweden -> location.country.languages_spoken -> Swedish Sign Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nSwedish Sign Language", "# Reasoning Path:\nSweden -> location.country.languages_spoken -> Swedish Language -> language.human_language.countries_spoken_in -> Finland\n# Answer:\nSwedish Language", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\ng.11b60n70x2", "# Reasoning Path:\nSweden -> location.country.languages_spoken -> Swedish Sign Language -> common.topic.notable_types -> Human Language\n# Answer:\nSwedish Sign Language", "# Reasoning Path:\nSweden -> location.country.languages_spoken -> Me\u00e4nkieli -> common.topic.notable_types -> Human Language\n# Answer:\nMe\u00e4nkieli", "# Reasoning Path:\nSweden -> location.country.languages_spoken -> Swedish Language -> language.human_language.region -> Europe\n# Answer:\nSwedish Language", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ll\n# Answer:\ng.1245_22ll"], "ground_truth": ["Swedish Sign Language", "Me\u00e4nkieli", "Turoyo Language", "Yiddish Language", "Finnish Language", "Romani language", "Swedish Language"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5581395348837209, "ans_precission": 0.8, "ans_recall": 0.42857142857142855, "path_f1": 0.5, "path_precision": 0.6, "path_recall": 0.42857142857142855, "path_ans_f1": 0.5581395348837209, "path_ans_precision": 0.8, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1423", "prediction": ["# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> India\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Kenya\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Nigeria\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.languages_spoken -> \u00c9w\u00e9 Language -> language.human_language.countries_spoken_in -> Togo\n# Answer:\n\u00c9w\u00e9 Language", "# Reasoning Path:\nGhana -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> India\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60ywwvy\n# Answer:\ng.11b60ywwvy", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.main_country -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.languages_spoken -> \u00c9w\u00e9 Language -> language.human_language.writing_system -> Latin alphabet\n# Answer:\n\u00c9w\u00e9 Language", "# Reasoning Path:\nGhana -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Kenya\n# Answer:\nEnglish Language"], "ground_truth": ["Dagbani Language", "Dangme Language", "Gonja Language", "Akan Language", "Asante dialect", "Fula language", "Dagaare language", "Ga Language", "Kasem Language", "English Language", "Nzema Language", "\u00c9w\u00e9 Language"], "ans_acc": 0.16666666666666666, "ans_hit": 1, "ans_f1": 0.28125, "ans_precission": 0.9, "ans_recall": 0.16666666666666666, "path_f1": 0.23529411764705882, "path_precision": 0.4, "path_recall": 0.16666666666666666, "path_ans_f1": 0.28125, "path_ans_precision": 0.9, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTest-1424", "prediction": ["# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Indonesia\n# Answer:\nConstitutional republic", "# Reasoning Path:\nPhilippines -> location.statistical_region.external_debt_stock -> g.11b71qs22v\n# Answer:\ng.11b71qs22v", "# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Russia\n# Answer:\nConstitutional republic", "# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Japan\n# Answer:\nUnitary state", "# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Singapore\n# Answer:\nConstitutional republic", "# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guam\n# Answer:\nPresidential system", "# Reasoning Path:\nPhilippines -> location.statistical_region.external_debt_stock -> g.1hhc37prj\n# Answer:\ng.1hhc37prj", "# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Marshall Islands\n# Answer:\nUnitary state", "# Reasoning Path:\nPhilippines -> location.statistical_region.external_debt_stock -> g.1hhc3_49g\n# Answer:\ng.1hhc3_49g", "# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Singapore\n# Answer:\nUnitary state"], "ground_truth": ["Constitutional republic", "Presidential system", "Republic", "Unitary state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.7241379310344827, "path_precision": 0.7, "path_recall": 0.75, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1425", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.institution -> Harvard Law School\n# Answer:\nm.02kvkg9", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkf4 -> education.education.institution -> Stanford University\n# Answer:\nm.02kvkf4", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.major_field_of_study -> Law\n# Answer:\nm.02kvkg9", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkf4 -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nm.02kvkf4", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.0123vxrw -> education.educational_institution.students_graduates -> m.0123vxqw\n# Answer:\nm.0123vxrw", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0_gvz4k -> film.personal_film_appearance.film -> Mitt\n# Answer:\nm.0_gvz4k", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0_gvz4k -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.0_gvz4k", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0nhmmbv -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.0nhmmbv", "# Reasoning Path:\nMitt Romney -> tv.tv_actor.guest_roles -> m.0bvvym_ -> tv.tv_guest_role.episodes_appeared_in -> The Comeback of President Bush\n# Answer:\nm.0bvvym_", "# Reasoning Path:\nMitt Romney -> tv.tv_actor.guest_roles -> m.0bvw9my -> tv.tv_guest_role.episodes_appeared_in -> Mitt Romney, Ryan Sheckler, Jonas Brothers\n# Answer:\nm.0bvw9my"], "ground_truth": ["Harvard Business School", "Stanford University", "Brigham Young University", "Harvard University", "Harvard Law School"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.2, "path_recall": 0.3333333333333333, "path_ans_f1": 0.26666666666666666, "path_ans_precision": 0.2, "path_ans_recall": 0.4}
{"id": "WebQTest-1426", "prediction": ["# Reasoning Path:\nIbrahim Afellay -> people.person.place_of_birth -> Utrecht -> location.administrative_division.second_level_division_of -> Netherlands\n# Answer:\nUtrecht", "# Reasoning Path:\nIbrahim Afellay -> people.person.place_of_birth -> Utrecht -> location.location.containedby -> Kingdom of the Netherlands\n# Answer:\nUtrecht", "# Reasoning Path:\nIbrahim Afellay -> people.person.place_of_birth -> Utrecht -> location.location.containedby -> Netherlands\n# Answer:\nUtrecht", "# Reasoning Path:\nIbrahim Afellay -> people.person.place_of_birth -> Utrecht -> location.statistical_region.population -> g.1jmcbdzl0\n# Answer:\nUtrecht", "# Reasoning Path:\nIbrahim Afellay -> people.person.profession -> Football player -> common.topic.article -> m.01lkv6\n# Answer:\nFootball player", "# Reasoning Path:\nIbrahim Afellay -> people.person.place_of_birth -> Utrecht -> location.statistical_region.population -> g.1jmcbfmb_\n# Answer:\nUtrecht", "# Reasoning Path:\nIbrahim Afellay -> people.person.profession -> Football player -> type.type.expected_by -> On\n# Answer:\nFootball player", "# Reasoning Path:\nIbrahim Afellay -> soccer.football_player.statistics -> m.0w8x1wl -> soccer.football_player_stats.team -> PSV Eindhoven\n# Answer:\nm.0w8x1wl", "# Reasoning Path:\nIbrahim Afellay -> people.person.profession -> Football player -> common.topic.article -> m.02ktm6x\n# Answer:\nFootball player", "# Reasoning Path:\nIbrahim Afellay -> people.person.profession -> Football player -> type.type.properties -> Disciplinary action\n# Answer:\nFootball player"], "ground_truth": ["Utrecht"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1427", "prediction": ["# Reasoning Path:\nAndrew Jackson -> government.political_appointer.appointees -> m.0bws0wh -> government.government_position_held.office_holder -> Edward Livingston\n# Answer:\nm.0bws0wh", "# Reasoning Path:\nAndrew Jackson -> government.political_appointer.appointees -> m.04kq1q9 -> government.government_position_held.office_holder -> Louis McLane\n# Answer:\nm.04kq1q9", "# Reasoning Path:\nAndrew Jackson -> government.us_president.vice_president -> Martin Van Buren -> government.us_president.vice_president -> Richard Mentor Johnson\n# Answer:\nMartin Van Buren", "# Reasoning Path:\nAndrew Jackson -> government.political_appointer.appointees -> m.0bws0wh -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nm.0bws0wh", "# Reasoning Path:\nAndrew Jackson -> government.political_appointer.appointees -> m.04kq1q5 -> government.government_position_held.office_holder -> Samuel D. Ingham\n# Answer:\nm.04kq1q5", "# Reasoning Path:\nAndrew Jackson -> government.political_appointer.appointees -> m.04kq1q9 -> government.government_position_held.office_position_or_title -> United States Secretary of the Treasury\n# Answer:\nm.04kq1q9", "# Reasoning Path:\nAndrew Jackson -> government.political_appointer.appointees -> m.04kq1q9 -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nm.04kq1q9", "# Reasoning Path:\nAndrew Jackson -> people.deceased_person.place_of_death -> Nashville -> common.topic.notable_types -> City/Town/Village\n# Answer:\nNashville", "# Reasoning Path:\nAndrew Jackson -> government.us_president.vice_president -> Martin Van Buren -> government.politician.party -> m.03gjg5m\n# Answer:\nMartin Van Buren", "# Reasoning Path:\nAndrew Jackson -> government.political_appointer.appointees -> m.04kq1q5 -> government.government_position_held.governmental_body -> Cabinet of the United States\n# Answer:\nm.04kq1q5"], "ground_truth": ["James Alexander Hamilton", "Martin Van Buren", "Edward Livingston", "Louis McLane"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.22222222222222224, "ans_precission": 0.2, "ans_recall": 0.25, "path_f1": 0.48, "path_precision": 0.4, "path_recall": 0.6, "path_ans_f1": 0.5217391304347827, "path_ans_precision": 0.4, "path_ans_recall": 0.75}
{"id": "WebQTest-1428", "prediction": ["# Reasoning Path:\nRon Stoppable -> tv.tv_character.appeared_in_tv_program -> m.03jspv_ -> tv.regular_tv_appearance.actor -> Will Friedle\n# Answer:\nm.03jspv_", "# Reasoning Path:\nRon Stoppable -> fictional_universe.fictional_character.character_created_by -> Bob Schooley -> fictional_universe.fictional_character_creator.fictional_characters_created -> Kim Possible\n# Answer:\nBob Schooley", "# Reasoning Path:\nRon Stoppable -> common.topic.article -> m.09w3my\n# Answer:\nm.09w3my", "# Reasoning Path:\nRon Stoppable -> tv.tv_character.appeared_in_tv_program -> m.03jspv_ -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nm.03jspv_", "# Reasoning Path:\nRon Stoppable -> fictional_universe.fictional_character.character_created_by -> Bob Schooley -> fictional_universe.fictional_character_creator.fictional_characters_created -> Shego\n# Answer:\nBob Schooley", "# Reasoning Path:\nRon Stoppable -> fictional_universe.fictional_character.character_created_by -> Mark McCorkle -> fictional_universe.fictional_character_creator.fictional_characters_created -> Shego\n# Answer:\nMark McCorkle", "# Reasoning Path:\nRon Stoppable -> fictional_universe.fictional_character.character_created_by -> Bob Schooley -> fictional_universe.fictional_character_creator.fictional_characters_created -> DNAmy\n# Answer:\nBob Schooley", "# Reasoning Path:\nRon Stoppable -> fictional_universe.fictional_character.character_created_by -> Bob Schooley -> common.topic.notable_types -> TV Writer\n# Answer:\nBob Schooley", "# Reasoning Path:\nRon Stoppable -> fictional_universe.fictional_character.character_created_by -> Mark McCorkle -> fictional_universe.fictional_character_creator.fictional_characters_created -> Kim Possible\n# Answer:\nMark McCorkle", "# Reasoning Path:\nRon Stoppable -> fictional_universe.fictional_character.character_created_by -> Mark McCorkle -> people.person.gender -> Male\n# Answer:\nMark McCorkle"], "ground_truth": ["Will Friedle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.13333333333333333, "path_precision": 0.1, "path_recall": 0.2, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1429", "prediction": ["# Reasoning Path:\nNew Zealand -> location.country.currency_used -> New Zealand dollar -> finance.currency.countries_used -> Cook Islands\n# Answer:\nNew Zealand dollar", "# Reasoning Path:\nNew Zealand -> location.country.currency_used -> New Zealand dollar -> finance.currency.countries_used -> Niue\n# Answer:\nNew Zealand dollar", "# Reasoning Path:\nNew Zealand -> location.country.currency_used -> New Zealand dollar -> finance.currency.countries_used -> Tokelau\n# Answer:\nNew Zealand dollar", "# Reasoning Path:\nNew Zealand -> olympics.olympic_participating_country.medals_won -> m.04gdvs5 -> olympics.olympic_medal_honor.medal -> Gold medal\n# Answer:\nm.04gdvs5", "# Reasoning Path:\nNew Zealand -> film.film_location.featured_in_films -> Black Sheep -> film.film.country -> Australia\n# Answer:\nBlack Sheep", "# Reasoning Path:\nNew Zealand -> film.film_location.featured_in_films -> Black Sheep -> common.topic.notable_types -> Film\n# Answer:\nBlack Sheep", "# Reasoning Path:\nNew Zealand -> film.film_location.featured_in_films -> Bridge to Terabithia -> film.film.country -> United States of America\n# Answer:\nBridge to Terabithia", "# Reasoning Path:\nNew Zealand -> film.film_location.featured_in_films -> King Kong -> film.film.country -> United States of America\n# Answer:\nKing Kong", "# Reasoning Path:\nNew Zealand -> olympics.olympic_participating_country.medals_won -> m.04gdvs5 -> olympics.olympic_medal_honor.olympics -> 2008 Summer Olympics\n# Answer:\nm.04gdvs5", "# Reasoning Path:\nNew Zealand -> olympics.olympic_participating_country.medals_won -> m.04gdzbh -> olympics.olympic_medal_honor.medal -> Gold medal\n# Answer:\nm.04gdzbh"], "ground_truth": ["New Zealand dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1432", "prediction": ["# Reasoning Path:\nWest Midlands -> base.aareas.schema.administrative_area.administrative_children -> Birmingham -> travel.travel_destination.tourist_attractions -> Sarehole Mill\n# Answer:\nBirmingham", "# Reasoning Path:\nWest Midlands -> base.aareas.schema.administrative_area.administrative_children -> Birmingham -> book.book_subject.works -> The Rotters' Club\n# Answer:\nBirmingham", "# Reasoning Path:\nWest Midlands -> location.administrative_division.country -> England -> base.aareas.schema.administrative_area.administrative_children -> North East England\n# Answer:\nEngland", "# Reasoning Path:\nWest Midlands -> base.aareas.schema.administrative_area.administrative_children -> Birmingham -> location.location.containedby -> England\n# Answer:\nBirmingham", "# Reasoning Path:\nWest Midlands -> base.aareas.schema.administrative_area.administrative_children -> Coventry -> location.administrative_division.country -> United Kingdom\n# Answer:\nCoventry", "# Reasoning Path:\nWest Midlands -> location.administrative_division.country -> England -> base.aareas.schema.administrative_area.administrative_children -> Cambridgeshire\n# Answer:\nEngland"], "ground_truth": ["Hodge Hill", "Edgbaston", "Stechford and Yardley North", "Bradley, West Midlands", "University of Wolverhampton, City Campus", "Walsall College, Wisemore campus", "Shropshire", "Soho, West Midlands", "Handsworth Wood", "Kings Heath", "Sandwell College, Smethwick", "The Public, West Bromwich", "Metropolitan Borough of Walsall", "Brandwood End", "National School of Blacksmithing, Holme Lacy campus", "Darlaston", "Wollaston", "Brades Village", "Selly Park", "Brandwood", "City College, Birmingham", "Bromford", "Acocks Green", "Chad Valley, Birmingham", "Billesley, West Midlands", "Low Hill", "Weoley Castle", "Moseley and Kings Heath", "Sedgley", "Oakham, West Midlands", "Great Barr", "West Bromwich", "Hampton-in-Arden", "Kingshurst", "Stowheath", "Castle Vale", "California, Birmingham", "Coseley", "The Sixth Form College, Solihull", "Willenhall", "Sutton New Hall", "Warwickshire", "Horseley Fields", "Turners Hill, West Midlands", "Netherton", "Oscott", "Blackheath", "Great Bridge, West Midlands", "Perry Barr", "Oldbury", "Walmley", "Kings Norton", "Metropolitan Borough of Solihull", "Coventry University", "Pelsall", "Nechells", "Hall Green", "Walsall Wood", "University of Wolverhampton, Walsall Campus", "Shard End", "Woodcross", "Halesowen", "South Yardley", "Redditch", "Chelmsley Wood", "Cheylesmore", "Birmingham City University", "Alumwell Business and Enterprise College", "Lozells and East Handsworth", "Smethwick", "Solihull", "Bilston", "Catherine-de-Barnes", "Staffordshire", "Clayhanger, West Midlands", "Cradley Heath", "Tettenhall", "Saltley", "Wolverhampton", "Tudor Hill", "Merridale", "Walsall College, Green Lane campus", "Harborne", "Herefordshire", "Moor Pool", "Lode Heath School and Specialist College", "Brownhills West", "Worcestershire", "Coventry", "Woodsetton, Dudley", "Scotlands Estate", "Bearwood, West Midlands", "Chadwick End", "Tyburn, West Midlands", "Sutton Trinity", "Sparkbrook", "Lye", "Rednal", "Yardley Wood", "West Heath, West Midlands", "Jewellery Quarter", "Selly Oak", "Wordsley", "Gornal, West Midlands", "Stourbridge", "Springfield, Birmingham", "Sutton Vesey", "Dudley", "Dudley College", "Balsall Common", "University of Wolverhampton, Compton Park Campus", "Wednesfield", "Sandwell College, Oldbury", "Stone Cross, West Midlands", "Wednesbury", "Hockley", "Streetly", "Castlecroft", "Compton, Wolverhampton", "Ladywood", "Stoke-on-Trent", "Chapelfields", "Park Village", "Aston", "Ashted", "Brownhills", "Redhill School, Stourbridge", "Birmingham", "Castle Bromwich", "Sheldon, West Midlands", "Barr Beacon", "Lyndon School, Solihull", "Stourton", "Bickenhill", "Aston Business School", "Brierley Hill", "Sutton Four Oaks", "Metropolitan Borough of Dudley", "Pensnett", "Barston", "City College Coventry", "Stockland Green", "West Midlands", "City of Wolverhampton College", "Dorridge", "Radford, Coventry", "Bournville", "Hockley Heath", "Hamstead, West Midlands", "Whitmore Reans", "Fordhouses", "Middleton", "Oxley, Wolverhampton", "Telford and Wrekin", "Bournbrook", "Quarry Bank", "Quinton, Birmingham", "Wall Heath", "Kingstanding", "Norton, Dudley", "Amblecote", "Boldmere", "Longbridge", "Erdington", "Tividale", "Druids Heath", "Walsall", "Mary Arden's Farm", "Brandhall", "Aston Ramsden", "Patton", "Blossomfield", "Fordbridge", "Bishopgate Green", "Olton", "Bordesley Green", "Allesley", "Dovecotes", "Dunstall Hill", "Marston Green", "Bushbury", "New Frankley", "Shirley, West Midlands", "Rushall, West Midlands", "Kingswinford", "Meriden", "Sandwell", "Sandwell College, West Bromwich", "Tipton", "Gorsebrook", "St Francis of Assisi Catholic Technology College", "Knowle, West Midlands", "Washwood Heath", "Berkswell", "Wylde Green", "Northfield, Birmingham", "Ashmore Park", "Bartley Green", "Pendeford", "Blakenhall", "Blakenall Heath", "Aldridge", "Bloxwich", "Handsworth, West Midlands", "Allesley Green", "Smith's Wood", "King Edward VI College, Stourbridge", "Earlsdon", "Rowley Regis", "Aston University, Birmingham"], "ans_acc": 0.014285714285714285, "ans_hit": 1, "ans_f1": 0.018779342723004695, "ans_precission": 0.6666666666666666, "ans_recall": 0.009523809523809525, "path_f1": 0.12, "path_precision": 1.0, "path_recall": 0.06382978723404255, "path_ans_f1": 0.028169014084507043, "path_ans_precision": 1.0, "path_ans_recall": 0.014285714285714285}
{"id": "WebQTest-1433", "prediction": ["# Reasoning Path:\nAsheville -> travel.travel_destination.tourist_attractions -> Biltmore Estate -> business.customer.supplier -> Joyce Farms\n# Answer:\nBiltmore Estate", "# Reasoning Path:\nAsheville -> travel.travel_destination.tourist_attractions -> Blue Ridge Parkway -> transportation.road.end2 -> m.0123xlpk\n# Answer:\nBlue Ridge Parkway", "# Reasoning Path:\nAsheville -> travel.travel_destination.tourist_attractions -> Biltmore Estate -> location.location.geolocation -> m.0w6bk_9\n# Answer:\nBiltmore Estate", "# Reasoning Path:\nAsheville -> travel.travel_destination.tourist_attractions -> Thomas Wolfe House -> location.location.containedby -> North Carolina\n# Answer:\nThomas Wolfe House", "# Reasoning Path:\nAsheville -> location.statistical_region.population -> g.11b66ff3kh\n# Answer:\ng.11b66ff3kh", "# Reasoning Path:\nAsheville -> travel.travel_destination.tourist_attractions -> Biltmore Estate -> film.film_location.featured_in_films -> Hannibal\n# Answer:\nBiltmore Estate", "# Reasoning Path:\nAsheville -> travel.travel_destination.tourist_attractions -> Blue Ridge Parkway -> common.topic.notable_types -> Tourist attraction\n# Answer:\nBlue Ridge Parkway", "# Reasoning Path:\nAsheville -> travel.travel_destination.tourist_attractions -> Thomas Wolfe House -> base.usnris.nris_listing.significant_person -> Thomas Wolfe\n# Answer:\nThomas Wolfe House", "# Reasoning Path:\nAsheville -> location.statistical_region.population -> g.11bc85ny0m\n# Answer:\ng.11bc85ny0m", "# Reasoning Path:\nAsheville -> travel.travel_destination.tourist_attractions -> Biltmore Estate -> film.film_location.featured_in_films -> Being There\n# Answer:\nBiltmore Estate"], "ground_truth": ["Thomas Wolfe House", "Biltmore Estate", "Folk Art Center", "Smith-McDowell House", "Carl Sandburg Home National Historic Site", "Bele Chere", "Pisgah National Forest", "Blue Ridge Parkway", "Asheville Art Museum", "The Omni Grove Park Inn"], "ans_acc": 0.3, "ans_hit": 1, "ans_f1": 0.4363636363636363, "ans_precission": 0.8, "ans_recall": 0.3, "path_f1": 0.4363636363636363, "path_precision": 0.8, "path_recall": 0.3, "path_ans_f1": 0.4363636363636363, "path_ans_precision": 0.8, "path_ans_recall": 0.3}
{"id": "WebQTest-1434", "prediction": ["# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Kansas City Royals -> organization.organization.leadership -> m.0z45pkh\n# Answer:\nKansas City Royals", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Detroit Tigers -> common.topic.notable_for -> g.125b2p9mf\n# Answer:\nDetroit Tigers", "# Reasoning Path:\nAmerican League -> business.employer.employees -> g.11b88cjhcb\n# Answer:\ng.11b88cjhcb", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Kansas City Royals -> sports.professional_sports_team.draft_picks -> m.010q80sj\n# Answer:\nKansas City Royals", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Kansas City Royals -> baseball.baseball_team.current_coaches -> m.0527g5f\n# Answer:\nKansas City Royals", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Minnesota Twins -> sports.professional_sports_team.draft_picks -> m.04vw_lf\n# Answer:\nMinnesota Twins", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Kansas City Royals -> sports.professional_sports_team.draft_picks -> m.010q8104\n# Answer:\nKansas City Royals", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Detroit Tigers -> baseball.baseball_team.current_coaches -> m.0527dqt\n# Answer:\nDetroit Tigers", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Kansas City Royals -> sports.professional_sports_team.draft_picks -> m.04vw__9\n# Answer:\nKansas City Royals", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Kansas City Royals -> baseball.baseball_team.current_coaches -> m.0527gf5\n# Answer:\nKansas City Royals"], "ground_truth": ["Cleveland Naps", "Boston Red Sox", "Oakland Athletics", "Seattle Pilots", "Detroit Tigers", "Kansas City Royals", "Milwaukee Brewers", "Washington Senators", "Los Angeles Angels of Anaheim", "Anaheim Angels", "Chicago White Sox", "Houston Astros", "Cleveland Indians", "Toronto Blue Jays", "Philadelphia Athletics", "Texas Rangers", "Tampa Bay Devil Rays", "California Angels", "Chicago White Stockings", "Baltimore Orioles", "New York Yankees", "New York Highlanders", "Seattle Mariners", "Kansas City Athletics", "Minnesota Twins", "Tampa Bay Rays", "St. Louis Browns"], "ans_acc": 0.1111111111111111, "ans_hit": 1, "ans_f1": 0.1978021978021978, "ans_precission": 0.9, "ans_recall": 0.1111111111111111, "path_f1": 0.12413793103448276, "path_precision": 0.9, "path_recall": 0.06666666666666667, "path_ans_f1": 0.1978021978021978, "path_ans_precision": 0.9, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-1435", "prediction": ["# Reasoning Path:\nKurt Warner -> sports.pro_athlete.teams -> m.045wt3q -> sports.sports_team_roster.team -> St. Louis Rams\n# Answer:\nm.045wt3q", "# Reasoning Path:\nKurt Warner -> sports.pro_athlete.teams -> m.05bd999 -> sports.sports_team_roster.team -> New York Giants\n# Answer:\nm.05bd999", "# Reasoning Path:\nKurt Warner -> sports.pro_athlete.teams -> m.0hprztb -> sports.sports_team_roster.team -> Arizona Cardinals\n# Answer:\nm.0hprztb", "# Reasoning Path:\nKurt Warner -> sports.pro_athlete.teams -> m.045wt3q -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.045wt3q", "# Reasoning Path:\nKurt Warner -> american_football.football_player.rushing -> m.0791hhh -> american_football.player_rushing_statistics.team -> Arizona Cardinals\n# Answer:\nm.0791hhh", "# Reasoning Path:\nKurt Warner -> sports.pro_athlete.teams -> m.05bd999 -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.05bd999", "# Reasoning Path:\nKurt Warner -> sports.pro_athlete.teams -> m.0hprztb -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.0hprztb", "# Reasoning Path:\nKurt Warner -> american_football.football_player.rushing -> m.0791hhh -> american_football.player_rushing_statistics.season -> 2008 NFL season\n# Answer:\nm.0791hhh", "# Reasoning Path:\nKurt Warner -> american_football.football_player.rushing -> m.07mmqb9 -> american_football.player_rushing_statistics.team -> Arizona Cardinals\n# Answer:\nm.07mmqb9", "# Reasoning Path:\nKurt Warner -> american_football.football_player.passing -> m.0791h8d -> american_football.player_passing_statistics.team -> Arizona Cardinals\n# Answer:\nm.0791h8d"], "ground_truth": ["Northern Iowa Panthers football", "Amsterdam Admirals", "Arizona Cardinals", "Iowa Barnstormers", "St. Louis Rams", "New York Giants"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.17391304347826086, "path_precision": 0.6, "path_recall": 0.1016949152542373, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.6, "path_ans_recall": 0.5}
{"id": "WebQTest-1436", "prediction": ["# Reasoning Path:\nDirk Nowitzki -> people.person.parents -> Helga Nowitzki -> people.person.children -> Silke Nowitzki\n# Answer:\nHelga Nowitzki", "# Reasoning Path:\nDirk Nowitzki -> people.person.parents -> Helga Nowitzki -> people.person.spouse_s -> m.0vnrtcx\n# Answer:\nHelga Nowitzki", "# Reasoning Path:\nDirk Nowitzki -> people.person.spouse_s -> m.0r8dkzz -> people.marriage.spouse -> Jessica Olsson\n# Answer:\nm.0r8dkzz", "# Reasoning Path:\nDirk Nowitzki -> people.person.parents -> Jorg Werner -> people.person.spouse_s -> m.0vnrtcx\n# Answer:\nJorg Werner", "# Reasoning Path:\nDirk Nowitzki -> people.person.spouse_s -> m.0r8dkzz -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0r8dkzz", "# Reasoning Path:\nDirk Nowitzki -> people.person.parents -> Helga Nowitzki -> people.person.profession -> Basketball player\n# Answer:\nHelga Nowitzki", "# Reasoning Path:\nDirk Nowitzki -> people.person.spouse_s -> m.0r8dkzz -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0r8dkzz", "# Reasoning Path:\nDirk Nowitzki -> people.person.parents -> Jorg Werner -> people.person.gender -> Male\n# Answer:\nJorg Werner", "# Reasoning Path:\nDirk Nowitzki -> award.award_winner.awards_won -> m.010f1rwg -> freebase.valuenotation.has_no_value -> Ceremony\n# Answer:\nm.010f1rwg", "# Reasoning Path:\nDirk Nowitzki -> award.award_winner.awards_won -> m.0_qrbz6 -> award.award_honor.award -> Bill Russell NBA Finals Most Valuable Player Award\n# Answer:\nm.0_qrbz6"], "ground_truth": ["Jessica Olsson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1441", "prediction": ["# Reasoning Path:\nPablo Picasso -> people.person.place_of_birth -> M\u00e1laga -> location.location.time_zones -> Central European Time Zone\n# Answer:\nM\u00e1laga", "# Reasoning Path:\nPablo Picasso -> people.person.place_of_birth -> M\u00e1laga -> common.topic.notable_types -> City/Town/Village\n# Answer:\nM\u00e1laga", "# Reasoning Path:\nPablo Picasso -> people.person.place_of_birth -> M\u00e1laga -> travel.travel_destination.tourist_attractions -> Museo Picasso M\u00e1laga\n# Answer:\nM\u00e1laga", "# Reasoning Path:\nPablo Picasso -> people.person.places_lived -> m.04yt_9c -> people.place_lived.location -> M\u00e1laga\n# Answer:\nm.04yt_9c", "# Reasoning Path:\nPablo Picasso -> people.person.places_lived -> m.0wfdzk8 -> people.place_lived.location -> Madrid\n# Answer:\nm.0wfdzk8", "# Reasoning Path:\nPablo Picasso -> book.author.works_written -> L'\u0153uvre de Picasso a\u0300 Antibes -> common.topic.notable_for -> g.125b58h_3\n# Answer:\nL'\u0153uvre de Picasso a\u0300 Antibes", "# Reasoning Path:\nPablo Picasso -> people.person.places_lived -> m.0jw3bpj -> people.place_lived.location -> France\n# Answer:\nm.0jw3bpj", "# Reasoning Path:\nPablo Picasso -> book.author.works_written -> L'\u0153uvre de Picasso a\u0300 Antibes -> common.topic.notable_types -> Book\n# Answer:\nL'\u0153uvre de Picasso a\u0300 Antibes", "# Reasoning Path:\nPablo Picasso -> book.author.works_written -> 156 darrers gravats originals -> common.topic.notable_for -> g.125cdp5_3\n# Answer:\n156 darrers gravats originals", "# Reasoning Path:\nPablo Picasso -> book.author.works_written -> 156 darrers gravats originals -> common.topic.notable_types -> Book\n# Answer:\n156 darrers gravats originals"], "ground_truth": ["France", "Madrid", "M\u00e1laga"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3157894736842105, "ans_precission": 0.3, "ans_recall": 0.3333333333333333, "path_f1": 0.1935483870967742, "path_precision": 0.5, "path_recall": 0.12, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1442", "prediction": ["# Reasoning Path:\nCroatia -> location.country.currency_used -> Croatian kuna -> common.topic.article -> m.02z8k3\n# Answer:\nCroatian kuna", "# Reasoning Path:\nCroatia -> common.topic.notable_for -> g.1256bxl73\n# Answer:\ng.1256bxl73", "# Reasoning Path:\nCroatia -> location.country.currency_used -> Croatian kuna -> common.topic.notable_types -> Currency\n# Answer:\nCroatian kuna", "# Reasoning Path:\nCroatia -> location.country.currency_used -> Croatian kuna -> common.topic.webpage -> m.04m1ls_\n# Answer:\nCroatian kuna", "# Reasoning Path:\nCroatia -> location.statistical_region.gdp_growth_rate -> g.11b60t8rf2\n# Answer:\ng.11b60t8rf2", "# Reasoning Path:\nCroatia -> location.country.currency_used -> Croatian kuna -> common.topic.webpage -> m.04m1lsv\n# Answer:\nCroatian kuna", "# Reasoning Path:\nCroatia -> location.statistical_region.gdp_growth_rate -> g.1245_1jbw\n# Answer:\ng.1245_1jbw", "# Reasoning Path:\nCroatia -> location.country.currency_used -> Croatian kuna -> common.topic.webpage -> m.05nnkw1\n# Answer:\nCroatian kuna", "# Reasoning Path:\nCroatia -> location.statistical_region.gdp_growth_rate -> g.1245z0pzv\n# Answer:\ng.1245z0pzv"], "ground_truth": ["Croatian kuna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-1443", "prediction": ["# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> baseball.baseball_player.batting_stats -> m.06qyjc3 -> baseball.batting_statistics.team -> Seattle Mariners\n# Answer:\nm.06qyjc3", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> sports.pro_athlete.teams -> m.01118sn3 -> sports.sports_team_roster.team -> Kansas City Royals\n# Answer:\nm.01118sn3", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> baseball.baseball_player.batting_stats -> m.06qyjc3 -> baseball.batting_statistics.season -> 1996 Major League Baseball Season\n# Answer:\nm.06qyjc3", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> baseball.baseball_player.batting_stats -> m.06qyjdq -> baseball.batting_statistics.team -> Seattle Mariners\n# Answer:\nm.06qyjdq", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> baseball.baseball_player.batting_stats -> m.06qyjg3 -> baseball.batting_statistics.team -> Seattle Mariners\n# Answer:\nm.06qyjg3", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> baseball.baseball_player.batting_stats -> m.06qyjdq -> baseball.batting_statistics.season -> 1997 Major League Baseball Season\n# Answer:\nm.06qyjdq", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> sports.pro_athlete.teams -> m.0j2gtzk -> sports.sports_team_roster.team -> Philadelphia Phillies\n# Answer:\nm.0j2gtzk", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> sports.pro_athlete.teams -> m.0nbm395 -> sports.sports_team_roster.team -> New York Yankees\n# Answer:\nm.0nbm395", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> sports.pro_athlete.teams -> m.01118sn3 -> sports.sports_team_roster.position -> Outfielder\n# Answer:\nm.01118sn3"], "ground_truth": ["Kansas City Royals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.13333333333333333, "path_precision": 0.1, "path_recall": 0.2, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1445", "prediction": ["# Reasoning Path:\nMark Harmon -> award.award_nominee.award_nominations -> m.0vz6kq1 -> award.award_nomination.nominated_for -> Eleanor and Franklin: The White House Years\n# Answer:\nm.0vz6kq1", "# Reasoning Path:\nMark Harmon -> award.award_nominee.award_nominations -> m.0vz6kq1 -> award.award_nomination.ceremony -> 29th Primetime Emmy Awards\n# Answer:\nm.0vz6kq1", "# Reasoning Path:\nMark Harmon -> award.award_nominee.award_nominations -> m.07t74mx -> award.award_nomination.nominated_for -> The West Wing\n# Answer:\nm.07t74mx", "# Reasoning Path:\nMark Harmon -> award.award_nominee.award_nominations -> m.090f7q8 -> award.award_nomination.nominated_for -> Reasonable Doubts\n# Answer:\nm.090f7q8", "# Reasoning Path:\nMark Harmon -> tv.tv_actor.starring_roles -> m.0bgtlxt -> tv.regular_tv_appearance.series -> From the Earth to the Moon\n# Answer:\nm.0bgtlxt", "# Reasoning Path:\nMark Harmon -> award.award_nominee.award_nominations -> m.07t74mx -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Guest Actor in a Drama Series\n# Answer:\nm.07t74mx", "# Reasoning Path:\nMark Harmon -> award.award_nominee.award_nominations -> m.07t74mx -> award.award_nomination.ceremony -> 54th Primetime Emmy Awards\n# Answer:\nm.07t74mx", "# Reasoning Path:\nMark Harmon -> tv.tv_actor.starring_roles -> m.0bmqqd2 -> tv.regular_tv_appearance.series -> Centennial\n# Answer:\nm.0bmqqd2", "# Reasoning Path:\nMark Harmon -> tv.tv_actor.starring_roles -> m.02t8rt7 -> tv.regular_tv_appearance.series -> Flamingo Road\n# Answer:\nm.02t8rt7", "# Reasoning Path:\nMark Harmon -> people.person.parents -> Tom Harmon -> award.award_nominee.award_nominations -> m.0wdddy3\n# Answer:\nTom Harmon"], "ground_truth": ["Sam", "Centennial", "From the Earth to the Moon", "The Dream Merchants", "Reasonable Doubts", "Charlie Grace", "240-Robert", "NCIS", "Chicago Hope", "St. Elsewhere", "Eleanor and Franklin: The White House Years", "Flamingo Road"], "ans_acc": 0.4166666666666667, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3448275862068966, "path_precision": 0.5, "path_recall": 0.2631578947368421, "path_ans_f1": 0.45454545454545453, "path_ans_precision": 0.5, "path_ans_recall": 0.4166666666666667}
{"id": "WebQTest-1446", "prediction": ["# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> location.location.containedby -> Orange County\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> location.location.containedby -> Area code 657\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> location.location.containedby -> Area code 714\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> location.location.contains -> Huntington Beach Pier\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> common.topic.image -> Huntington Beach Pier\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> location.location.contains -> Apollos University\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> location.location.contains -> Baja Sharkeez\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> common.topic.image -> Location of Huntington Beach within Orange County, California\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> common.topic.webpage -> m.03lc2g9 -> common.webpage.resource -> m.0bk4f79\n# Answer:\nm.03lc2g9"], "ground_truth": ["2009-12-28"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1447", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5rn -> film.performance.actor -> Jake Lloyd\n# Answer:\nm.02sg5rn", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.actor -> James Earl Jones\n# Answer:\nm.02nv74t", "# Reasoning Path:\nStar Wars Episode I: The Phantom Menace -> music.album.release_type -> Album\n# Answer:\nAlbum", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5rn -> film.performance.film -> Star Wars Episode I: The Phantom Menace\n# Answer:\nm.02sg5rn", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.010wvb1d -> film.performance.actor -> Abraham Benrubi\n# Answer:\nm.010wvb1d", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_universe.locations -> Alderaan\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nStar Wars Episode I: The Phantom Menace -> film.film.other_crew -> m.0btcvr_ -> film.film_crew_gig.film_crew_role -> Lead Matte Painter\n# Answer:\nm.0btcvr_", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.film -> Star Wars\n# Answer:\nm.02nv74t", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_universe.locations -> Coruscant\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_universe.literary_series_set_here -> Choose Your Own Star Wars Adventures\n# Answer:\nStar Wars Expanded Universe"], "ground_truth": ["Jake Lloyd"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1448", "prediction": ["# Reasoning Path:\nMichelle Pfeiffer -> people.person.places_lived -> m.04hdch1 -> people.place_lived.location -> Orange County\n# Answer:\nm.04hdch1", "# Reasoning Path:\nMichelle Pfeiffer -> people.person.place_of_birth -> Santa Ana -> location.location.containedby -> Orange County\n# Answer:\nSanta Ana", "# Reasoning Path:\nMichelle Pfeiffer -> people.person.place_of_birth -> Santa Ana -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nSanta Ana", "# Reasoning Path:\nMichelle Pfeiffer -> people.person.place_of_birth -> Santa Ana -> location.location.containedby -> California\n# Answer:\nSanta Ana", "# Reasoning Path:\nMichelle Pfeiffer -> people.person.place_of_birth -> Santa Ana -> location.location.containedby -> United States of America\n# Answer:\nSanta Ana", "# Reasoning Path:\nMichelle Pfeiffer -> award.award_nominee.award_nominations -> m.021yhcs -> award.award_nomination.award -> Academy Award for Best Actress\n# Answer:\nm.021yhcs", "# Reasoning Path:\nMichelle Pfeiffer -> award.award_nominee.award_nominations -> m.05brbfm -> award.award_nomination.award -> Academy Award for Best Actress\n# Answer:\nm.05brbfm", "# Reasoning Path:\nMichelle Pfeiffer -> award.award_nominee.award_nominations -> m.021yhcs -> award.award_nomination.ceremony -> 62nd Academy Awards\n# Answer:\nm.021yhcs", "# Reasoning Path:\nMichelle Pfeiffer -> award.award_nominee.award_nominations -> m.05c7j74 -> award.award_nomination.nominated_for -> Dangerous Liaisons\n# Answer:\nm.05c7j74", "# Reasoning Path:\nMichelle Pfeiffer -> award.award_nominee.award_nominations -> m.05brbfm -> award.award_nomination.nominated_for -> Love Field\n# Answer:\nm.05brbfm"], "ground_truth": ["Orange County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1449", "prediction": ["# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Writer -> people.profession.specializations -> Novelist\n# Answer:\nWriter", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Journalist -> people.profession.corresponding_type -> Author\n# Answer:\nJournalist", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Writer -> people.profession.specializations -> Author\n# Answer:\nWriter", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Writer -> common.topic.article -> m.0cbdb\n# Answer:\nWriter", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Writer -> comic_strips.comic_strip_creator_role.creators -> m.0102ldp_\n# Answer:\nWriter", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Journalist -> common.topic.subjects -> Ciro Pellegrino\n# Answer:\nJournalist", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Writer -> people.profession.specializations -> Bard\n# Answer:\nWriter", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Journalist -> common.topic.subject_of -> Ciro Pellegrino\n# Answer:\nJournalist", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Author -> tv.tv_subject.tv_episodes -> Author David Kerr - The Ol' Turkey Hunter\n# Answer:\nAuthor", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor"], "ground_truth": ["Teacher", "Journalist", "Author", "Writer"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.6153846153846154, "path_precision": 0.8, "path_recall": 0.5, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-145", "prediction": ["# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k05ps4 -> education.education.institution -> University of Zurich\n# Answer:\nm.0k05ps4", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k03_k5 -> education.education.institution -> ETH Zurich\n# Answer:\nm.0k03_k5", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0j_z4g5 -> education.education.institution -> Luitpold Gymnasium\n# Answer:\nm.0j_z4g5", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k05ps4 -> freebase.valuenotation.has_value -> Minor\n# Answer:\nm.0k05ps4", "# Reasoning Path:\nAlbert Einstein -> people.person.employment_history -> m.02kq3y4 -> business.employment_tenure.company -> University of Zurich\n# Answer:\nm.02kq3y4", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k05ps4 -> freebase.valuenotation.is_reviewed -> Minor\n# Answer:\nm.0k05ps4", "# Reasoning Path:\nAlbert Einstein -> people.person.employment_history -> m.02kq4ch -> business.employment_tenure.company -> Leiden University\n# Answer:\nm.02kq4ch", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k05ps4 -> freebase.valuenotation.has_value -> Specialization\n# Answer:\nm.0k05ps4", "# Reasoning Path:\nAlbert Einstein -> people.person.employment_history -> m.02kq3y4 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.02kq3y4", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k03_k5 -> education.education.degree -> Bachelor of Science\n# Answer:\nm.0k03_k5"], "ground_truth": ["ETH Zurich", "University of Zurich"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4, "path_precision": 0.3, "path_recall": 0.6, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1452", "prediction": ["# Reasoning Path:\nDwight Howard -> people.person.place_of_birth -> Atlanta -> base.biblioness.bibs_location.state -> Georgia\n# Answer:\nAtlanta", "# Reasoning Path:\nDwight Howard -> basketball.basketball_player.player_statistics -> m.04qh0tc -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nm.04qh0tc", "# Reasoning Path:\nDwight Howard -> sports.pro_athlete.teams -> m.0kq8z4b -> sports.sports_team_roster.team -> Orlando Magic\n# Answer:\nm.0kq8z4b", "# Reasoning Path:\nDwight Howard -> people.person.place_of_birth -> Atlanta -> location.location.containedby -> Georgia\n# Answer:\nAtlanta", "# Reasoning Path:\nDwight Howard -> people.person.place_of_birth -> Atlanta -> location.hud_county_place.county -> Fulton County\n# Answer:\nAtlanta", "# Reasoning Path:\nDwight Howard -> sports.pro_athlete.teams -> m.0kq8zyx -> sports.sports_team_roster.team -> Los Angeles Lakers\n# Answer:\nm.0kq8zyx", "# Reasoning Path:\nDwight Howard -> people.person.place_of_birth -> Atlanta -> location.location.containedby -> Area code 404\n# Answer:\nAtlanta", "# Reasoning Path:\nDwight Howard -> sports.pro_athlete.teams -> m.0w7fk1d -> sports.sports_team_roster.team -> Houston Rockets\n# Answer:\nm.0w7fk1d", "# Reasoning Path:\nDwight Howard -> people.person.place_of_birth -> Atlanta -> location.location.containedby -> Area code 678\n# Answer:\nAtlanta", "# Reasoning Path:\nDwight Howard -> basketball.basketball_player.player_statistics -> m.04qh0tc -> basketball.basketball_player_stats.season -> 2007\u201308 NBA season\n# Answer:\nm.04qh0tc"], "ground_truth": ["Houston Rockets"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1453", "prediction": ["# Reasoning Path:\nNick Lachey -> people.person.children -> Camden John Lachey -> people.person.parents -> Vanessa Lachey\n# Answer:\nCamden John Lachey", "# Reasoning Path:\nNick Lachey -> people.person.children -> Camden John Lachey -> common.topic.notable_for -> g.1259plkqz\n# Answer:\nCamden John Lachey", "# Reasoning Path:\nNick Lachey -> people.person.children -> Camden John Lachey -> common.topic.notable_types -> Person\n# Answer:\nCamden John Lachey", "# Reasoning Path:\nNick Lachey -> music.artist.track_contributions -> m.01389mpb -> music.track_contribution.role -> Vocals\n# Answer:\nm.01389mpb", "# Reasoning Path:\nNick Lachey -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nNick Lachey -> music.artist.track_contributions -> m.01389mpb -> music.track_contribution.track -> True To Your Heart (Mulan)\n# Answer:\nm.01389mpb", "# Reasoning Path:\nNick Lachey -> music.artist.track_contributions -> m.0q5cc0f -> music.track_contribution.track -> The Way You Want Me To\n# Answer:\nm.0q5cc0f", "# Reasoning Path:\nNick Lachey -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nNick Lachey -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nNick Lachey -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#domain -> Person\n# Answer:\nCountry of nationality"], "ground_truth": ["Vanessa Lachey"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1454", "prediction": ["# Reasoning Path:\nMadonna -> people.person.place_of_birth -> Bay City -> location.location.containedby -> United States of America\n# Answer:\nBay City", "# Reasoning Path:\nMadonna -> people.person.place_of_birth -> Bay City -> location.location.people_born_here -> Christopher Ciccone\n# Answer:\nBay City", "# Reasoning Path:\nMadonna -> music.composer.compositions -> Die Another Day -> music.composition.composer -> Mirwais Ahmadza\u00ef\n# Answer:\nDie Another Day", "# Reasoning Path:\nMadonna -> people.person.place_of_birth -> Bay City -> location.location.people_born_here -> Madonna Louise Ciccone\n# Answer:\nBay City", "# Reasoning Path:\nMadonna -> music.composer.compositions -> Music -> music.album.genre -> Country\n# Answer:\nMusic", "# Reasoning Path:\nMadonna -> music.composer.compositions -> Music -> music.composition.language -> English Language\n# Answer:\nMusic", "# Reasoning Path:\nMadonna -> music.composer.compositions -> 4 Minutes -> music.composition.composer -> Justin Timberlake\n# Answer:\n4 Minutes", "# Reasoning Path:\nMadonna -> base.popstra.celebrity.dated -> m.064_jxy -> base.popstra.dated.participant -> Chris Rock\n# Answer:\nm.064_jxy", "# Reasoning Path:\nMadonna -> music.composer.compositions -> Die Another Day -> music.composition.recordings -> Die Another Day (Deepsky edit)\n# Answer:\nDie Another Day", "# Reasoning Path:\nMadonna -> music.composer.compositions -> Die Another Day -> common.topic.notable_types -> Composition\n# Answer:\nDie Another Day"], "ground_truth": ["Bay City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1455", "prediction": ["# Reasoning Path:\nBrazil -> location.country.capital -> Bras\u00edlia -> base.biblioness.bibs_location.city -> Federal District\n# Answer:\nBras\u00edlia", "# Reasoning Path:\nBrazil -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> South America\n# Answer:\nEarth", "# Reasoning Path:\nBrazil -> location.country.capital -> Bras\u00edlia -> location.location.containedby -> Federal District\n# Answer:\nBras\u00edlia", "# Reasoning Path:\nBrazil -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> Sovereign state\n# Answer:\nEarth", "# Reasoning Path:\nBrazil -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.uncommon.topic.exceptions -> City/Town/Village should pertain to Ghana, since City/Town/Village subdivides Ghanaian Municipal District.\n# Answer:\nEarth", "# Reasoning Path:\nBrazil -> location.country.administrative_divisions -> Almino Afonso -> location.location.containedby -> Rio Grande do Norte\n# Answer:\nAlmino Afonso"], "ground_truth": ["Bras\u00edlia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-1456", "prediction": ["# Reasoning Path:\nJohn Stockton -> people.person.education -> m.04hd6_9 -> education.education.institution -> Gonzaga University\n# Answer:\nm.04hd6_9", "# Reasoning Path:\nJohn Stockton -> people.person.education -> m.0n1l1ft -> education.education.institution -> Gonzaga Preparatory School\n# Answer:\nm.0n1l1ft", "# Reasoning Path:\nJohn Stockton -> award.hall_of_fame_inductee.hall_of_fame_inductions -> m.0bjck8b -> award.hall_of_fame_induction.category -> Basketball player\n# Answer:\nm.0bjck8b", "# Reasoning Path:\nJohn Stockton -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nJohn Stockton -> people.person.education -> m.0n1l1ft -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nm.0n1l1ft", "# Reasoning Path:\nJohn Stockton -> award.hall_of_fame_inductee.hall_of_fame_inductions -> m.0bjck8b -> award.hall_of_fame_induction.hall_of_fame -> Naismith Memorial Basketball Hall of Fame\n# Answer:\nm.0bjck8b", "# Reasoning Path:\nJohn Stockton -> freebase.valuenotation.is_reviewed -> Date of birth -> type.property.schema -> Person\n# Answer:\nDate of birth", "# Reasoning Path:\nJohn Stockton -> freebase.valuenotation.is_reviewed -> Gender -> type.property.schema -> Person\n# Answer:\nGender", "# Reasoning Path:\nJohn Stockton -> freebase.valuenotation.is_reviewed -> Date of birth -> type.property.expected_type -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nJohn Stockton -> freebase.valuenotation.is_reviewed -> Place of birth -> type.property.schema -> Person\n# Answer:\nPlace of birth"], "ground_truth": ["Gonzaga University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1457", "prediction": ["# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_0s31\n# Answer:\ng.1245_0s31", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.11b60v4rn6\n# Answer:\ng.11b60v4rn6", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_38p7\n# Answer:\ng.1245_38p7", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_569j\n# Answer:\ng.1245_569j", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.12tb6hbx9\n# Answer:\ng.12tb6hbx9", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.1hhc378k5\n# Answer:\ng.1hhc378k5"], "ground_truth": ["Mexican peso"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1458", "prediction": ["# Reasoning Path:\nChrysler Group LLC -> organization.organization.parent -> m.01322yt1 -> organization.organization_relationship.parent -> Fiat Chrysler Automobiles\n# Answer:\nm.01322yt1", "# Reasoning Path:\nChrysler Group LLC -> organization.organization.parent -> m.0gb6_f8 -> organization.organization_relationship.parent -> Government of Canada\n# Answer:\nm.0gb6_f8", "# Reasoning Path:\nChrysler Group LLC -> organization.organization.parent -> m.05ckhgd -> organization.organization_relationship.parent -> Fiat S.p.A.\n# Answer:\nm.05ckhgd", "# Reasoning Path:\nChrysler Group LLC -> organization.organization.parent -> m.05ckhgd -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.05ckhgd", "# Reasoning Path:\nChrysler Group LLC -> automotive.company.make_s -> Chrysler -> automotive.make.model_s -> AMC Eagle\n# Answer:\nChrysler", "# Reasoning Path:\nChrysler Group LLC -> automotive.company.make_s -> Chrysler -> base.schemastaging.entity_hint.corresponds_to -> m.0y4l_9z\n# Answer:\nChrysler", "# Reasoning Path:\nChrysler Group LLC -> automotive.company.make_s -> Chrysler -> business.endorsed_product.endorsements -> m.05nys01\n# Answer:\nChrysler", "# Reasoning Path:\nChrysler Group LLC -> award.award_winner.awards_won -> m.04_j175 -> award.award_honor.award -> Motor Trend Car of the Year\n# Answer:\nm.04_j175", "# Reasoning Path:\nChrysler Group LLC -> automotive.company.make_s -> Chrysler -> automotive.make.model_s -> Chrysler 180\n# Answer:\nChrysler", "# Reasoning Path:\nChrysler Group LLC -> automotive.company.make_s -> Chrysler -> automotive.make.model_s -> Chrysler 200\n# Answer:\nChrysler"], "ground_truth": ["Government of Canada", "Fiat Chrysler Automobiles", "Fiat S.p.A.", "Voluntary Employee Beneficiary Association", "Federal government of the United States"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4, "path_precision": 0.3, "path_recall": 0.6, "path_ans_f1": 0.4, "path_ans_precision": 0.3, "path_ans_recall": 0.6}
{"id": "WebQTest-1459", "prediction": ["# Reasoning Path:\nMark Twain -> film.film_story_contributor.film_story_credits -> The Prince and the Pauper -> book.written_work.next_in_series -> Life on the Mississippi\n# Answer:\nThe Prince and the Pauper", "# Reasoning Path:\nMark Twain -> film.film_story_contributor.film_story_credits -> A Connecticut Yankee -> film.film.written_by -> Jack Moffitt\n# Answer:\nA Connecticut Yankee", "# Reasoning Path:\nMark Twain -> people.person.children -> Clara Clemens -> book.author.works_written -> My father\n# Answer:\nClara Clemens", "# Reasoning Path:\nMark Twain -> film.film_story_contributor.film_story_credits -> A Connecticut Yankee -> film.film.starring -> m.0dls0x3\n# Answer:\nA Connecticut Yankee", "# Reasoning Path:\nMark Twain -> film.film_story_contributor.film_story_credits -> A Connecticut Yankee -> film.film.written_by -> Owen Davis\n# Answer:\nA Connecticut Yankee", "# Reasoning Path:\nMark Twain -> film.film_story_contributor.film_story_credits -> The Prince and the Pauper -> media_common.adapted_work.adaptations -> Barbie as the Princess and the Pauper\n# Answer:\nThe Prince and the Pauper", "# Reasoning Path:\nMark Twain -> people.person.children -> Clara Clemens -> book.author.works_written -> My husband, Gabrilowitsch\n# Answer:\nClara Clemens", "# Reasoning Path:\nMark Twain -> people.person.children -> Clara Clemens -> people.person.children -> Nina  Gabrilowitsc\n# Answer:\nClara Clemens", "# Reasoning Path:\nMark Twain -> people.person.children -> Jean Clemens -> people.deceased_person.place_of_death -> Stormfield\n# Answer:\nJean Clemens", "# Reasoning Path:\nMark Twain -> film.film_story_contributor.film_story_credits -> A Connecticut Yankee -> film.film.written_by -> William Conselman\n# Answer:\nA Connecticut Yankee"], "ground_truth": ["Tom Sawyer Abroad", "Adventures of Tom Sawyer (New Windmill)", "The Adventures of Huckleberry Finn", "American Claimant", "Adventures of Huckleberry Finn With Reader's Guide (Amsco Literature Program Series Grade 7-12)", "The Adventures of Tom Sawyer and The Adventures of Huckleberry Finn (Signet Classical Books)", "The Prince and the Pauper (Webster's Italian Thesaurus Edition)", "Adventures of Tom Sawyer (Progress English)", "The Adventures Of Tom Sawyer", "Adventures of Tom Sawyer (Deluxe Watermill Classics)", "The Adventures of Tom Sawyer Adventure Classic (Adventure Classics)", "Letters from the Earth", "Christian Science", "Personal Recollections Of Joan Of Arc", "Adventures of Tom Sawyer, The", "Personal Recollections of Joan of Arc Volume 2", "The Adventures of Tom Sawyer (The Classic Collection)", "Sketches, New and Old (1875) (The Oxford Mark Twain)", "Adventures of Tom Sawyer - Huckleberry Finn (Classic Compendium)", "Christian Science (1907) (The Oxford Mark Twain)", "Adventures of Tom Sawyer (Penguin Classics)", "The Mysterious Stranger (Signet Classics)", "Personal Recollections of Joan of Arc (Complete)", "Life on the Mississippi", "Adventures of Huckleberry Finn With Reader's Guide (Amsco Literature Program Series Grade 7 12, R 120 ALP)", "The wit and wisdom of Mark Twain", "Personal Recollections of Joan of Arc Volume 1", "The innocents abroad", "Adventures of Tom Sawyer (Children's Classics)", "Roughing It (Centre for Editions of American Authors)", "Adventures of Tom Sawyer & Huck Finn Collector's Library Volume I (Collector's Library of Classics, Volume 1)", "The Adventures of Tom Sawyer - Literary Touchstone Edition", "Tom Sawyer, Detective (Dover Evergreen Classics)", "Tom Sawyer Abroad (1894) (The Oxford Mark Twain)", "Tom Sawyer Detective (Austral Juvenil)", "A Connecticut Yankee in King Arthur's Court, Set", "Old Times on the Mississippi", "Roughing it", "Roughing It (Konemann Classics)", "Following the equator", "Adventures of Tom Sawyer (Streamline Books)", "Old times on the Mississippi", "The mysterious stranger", "The Adventures of Tom Sawyer With Audio CD (Hear It Read It)", "Adventures of Tom Sawyer (Modern Library Classics (Sagebrush))", "Adventures of Tom Sawyer (Episodes from/Cdl 51205)", "Adventures Of Tom Sawyer (Whole Story)", "A Tramp Abroad", "Adventures of Tom Sawyer (Classics)", "The American Claimant (1896)", "Sketches New And Old (The Works Of Mark Twain - 25 Volumes - Author's National Edition)", "A Tramp Abroad (1880) (The Oxford Mark Twain)", "Adventures of Tom Sawyer (Family Classics Library)", "The Prince and the Pauper (Webster's Korean Thesaurus Edition)", "Personal Recollections of Joan of Arc Volume 1 (Large Print Edition)", "Tom Sawyer, Detective (Tor Classics)", "Tom Sawyer, detective", "The Prince And the Pauper", "The Mysterious Stranger (Large Print Edition)", "Sketches New and Old", "A Connecticut Yankee in King Arthur's Court", "Wild Nights!", "Adventures of Tom Sawyer (Webster's Korean Thesaurus Edition)", "Tom Sawyer Abroad (Penguin Classics)", "The American claimant", "Adventures of Tom Sawyer (Fiction)", "The prince and the pauper", "Adventures of Tom Sawyer and Huckleberry Finn", "Adventures of Tom Sawyer, The (Classic Collection)", "The Innocents Abroad (Classic Books on Cassettes Collection)", "The Innocents Abroad (Signet Classics)", "The Adventures of Tom Sawyer and the Adventures of Huckleberry Finn (Signet Classics)", "Roughing It (Classics of the Old West)", "The adventures of Huckleberry Finn (Tom Sawyer's comrade)", "The Adventures of Tom Sawyer (Puffin Books)", "Adventures of Tom Sawyer (Cassette Sac 967)", "Adventures of Tom Sawyer. Adventure stories typically. Prince and pauper. Stories", "The Prince and the Pauper (Webster's Chinese-Simplified Thesaurus Edition)", "Adventures of Tom Sawyer (Saddleback Classics)", "Personal Recollections of Joan of Arc", "1601", "Roughing It (Works of Mark Twain, Volume One)", "The Innocents Abroad, vol. 1: The Authorized Uniform Edition", "Old Times on the Mississippi.", "Ignorance, Confidence, and Filthy Rich Friends: The Business Adventures of Mark Twain, Chronic Speculator and Entrepreneur", "Adventures of Tom Sawyer (Oxford Progressive English Readers)", "Autobiography of Mark Twain", "Personal recollections of Joan of Arc", "Adventures of Tom Sawyer (08454) (Deans Childrens Classics)", "Adventures of Tom Sawyer Promo (Action Packs)", "Adventures of Tom Sawyer (Children's Illustrated Classics)", "Personal Recollections of Joan of Arc (1896) (The Oxford Mark Twain)", "The adventures of Huckleberry Finn", "Following the Equator", "Christian Science (Large Print Edition)", "Tom Sawyer abroad", "How to tell a story, and other essays", "A Tramp Abroad (Konemann Classics)", "The Adventures of Tom Sawyer", "A Connecticut Yankee in King Arthur's Court (Enriched Classics Series)", "The adventures of Tom Sawyer ; The adventures of Huckleberry Finn ; The prince and the pauper ; Pudd'nhead Wilson ; Short stories ; A Connecticut Yankee at King Arthur's court", "Grant and Twain: The Story of a Friendship That Changed America", "Personal Recollections of Joan of Arc Volume 2 (Large Print Edition)", "Adventures of Huckleberry Finn", "Roughing It (1872) (The Oxford Mark Twain)", "Tom Sawyer Abroad (Large Print Edition)", "The Adventures of Tom Sawyer, (Classic Books on CDs) [UNABRIDGED] (Classic Books on CD)", "The Prince and the Pauper (Dover Children's Thrift Classics)", "Tom Sawyer Detective (Watermill Classic)", "Who Is Mark Twain?", "Adventures of Huckleberry Finn (1885) (The Oxford Mark Twain)", "A Connecticut Yankee in King Arthur's Court Readalong", "The adventures of Tom Sawyer ; The adventures of Huckleberry Finn ; The prince and the pauper", "Sketches New And Old", "Tom Sawyer, Detective", "The Innocents Abroad", "The American Claimant (Large Print Edition)", "The Adventures of Tom Sawyer and The Adventures of Huckleberry Finn", "The Adventures of Tom Sawyer, 1876 (IN RUSSIAN LANGUAGE) / (Las aventuras de Tom Sawyer / les Aventures de Tom Sawyer / Die Abenteuer des Tom Sawyer)", "Roughing It", "A Connecticut Yankee in King Arthur's Court (Penguin Classics)", "A Connecticut Yankee in King Arthur's Court (Tor Classics)", "Roughing It (Signet Classics)", "A Connecticut Yankee in King Arthur's Court (Classics Read By Celebrities Series)", "Christian Science (Large Print)", "Tom Sawyer - Detective", "The Adventures of Huckleberry Finn, Complete (Large Print)", "The Adventures of Tom Sawyer and The Adventures of Huckleberry Finn (Signet Classics)", "The Adventures of Tom Sawyer and The Adventures of Huckleberry Finn (Everyman's Library)", "Roughing it.", "The Adventures of Huckleberry Finn (Signet Classics)", "Tom Sawyer Abroad (The Works Of Mark Twain - 25 Volumes - Author's National Edition)", "Adventures of Huckleberry Finn (Tom Sawyer's comrade)", "The Trouble Begins at 8: A Life of Mark Twain in the Wild, Wild West", "Tom Sawyer, Detective (Hesperus Classics)", "A Tramp Abroad (Large Print Edition)", "The American Claimant (1892) (The Oxford Mark Twain)", "Sketches, new and old", "The Prince and the Pauper (New Method Supplementary Readers)", "Letters from the earth", "The Prince and the Pauper", "Old Times On The Mississippi", "Adventures of Tom Sawyer (Dramascripts)", "The American Claimant", "Adventures of Tom Sawyer GB", "The Adventures of Huckleberry Finn CD set (Cambridge Literature)", "Adventures of Huckleberry Finn (Illustrated Edition) (Dodo Press)", "Tom Sawyer Abroad (Watermill Classic)", "How to Tell a Story and Other Essays (1897) (The Oxford Mark Twain)", "The American Claimant (Large Print)", "Personal Recollections of Joan of Arc, V2", "Tom Sawyer Detective", "The adventures of Tom Sawyer", "Adventures of Huckleberry Finn/Tom Sawyer (Junior Classics)", "Adventures of Tom Sawyer (Ec04)", "The Mysterious Stranger", "The Prince and the Pauper (Webster's Chinese-Traditional Thesaurus Edition)", "Roughing It (Mark Twain Library)", "The adventures of Tom Sawyer ; Tom Sawyer abroad", "A Connecticut Yankee in King Arthur's Court, 1889 (novel) (IN RUSSIAN LANGUAGE) / (Ein Yankee am Hofe des K\u00f6nig Artus / Janki iz Konnektikuta pri dvore korolja Artura)", "Adventures of Tom Sawyer (Everyman's Classics S.)", "Adventures of Tom Sawyer (Classic Library)", "American Claimant (Writings of Mark Twain)", "Tom Sawyer Abroad (Tor Classics)", "The Innocents Abroad (1869) (The Oxford Mark Twain)", "The Adventures of Tom Sawyer and The Adventures of Huckleberry Finn (Everyman Paperbacks)", "Tom Sawyer Abroad (Watermill Classics Library)", "Adventures of Tom Sawyer (Longman Simplified English Series)", "Sketches New and Old (Complete)"], "ans_acc": 0.023668639053254437, "ans_hit": 1, "ans_f1": 0.03260869565217391, "ans_precission": 0.2, "ans_recall": 0.01775147928994083, "path_f1": 0.1473684210526316, "path_precision": 0.2, "path_recall": 0.11666666666666667, "path_ans_f1": 0.042328042328042326, "path_ans_precision": 0.2, "path_ans_recall": 0.023668639053254437}
{"id": "WebQTest-146", "prediction": ["# Reasoning Path:\nJordan -> sports.sports_team_location.teams -> Al-Wehdat SC -> sports.sports_team.location -> Amman\n# Answer:\nAl-Wehdat SC", "# Reasoning Path:\nJordan -> royalty.kingdom.rulers -> Hussein of Jordan -> people.person.place_of_birth -> Amman\n# Answer:\nHussein of Jordan", "# Reasoning Path:\nJordan -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71xk1k6\n# Answer:\ng.11b71xk1k6", "# Reasoning Path:\nJordan -> royalty.kingdom.rulers -> Hussein of Jordan -> royalty.monarch.royal_line -> Hashemites\n# Answer:\nHussein of Jordan", "# Reasoning Path:\nJordan -> royalty.kingdom.rulers -> Abdullah I of Jordan -> government.politician.government_positions_held -> m.0k_ktrb\n# Answer:\nAbdullah I of Jordan", "# Reasoning Path:\nJordan -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6hc5l\n# Answer:\ng.12tb6hc5l", "# Reasoning Path:\nJordan -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37x9d\n# Answer:\ng.1hhc37x9d", "# Reasoning Path:\nJordan -> royalty.kingdom.rulers -> Abdullah II of Jordan -> people.person.parents -> Hussein of Jordan\n# Answer:\nAbdullah II of Jordan", "# Reasoning Path:\nJordan -> royalty.kingdom.rulers -> Abdullah I of Jordan -> people.person.nationality -> Ottoman Empire\n# Answer:\nAbdullah I of Jordan", "# Reasoning Path:\nJordan -> royalty.kingdom.rulers -> Abdullah II of Jordan -> government.political_appointer.appointees -> m.0k27q6l\n# Answer:\nAbdullah II of Jordan"], "ground_truth": ["Jordan national football team", "Al-Wehdat SC"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.16666666666666669, "ans_precission": 0.1, "ans_recall": 0.5, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 0.1, "path_ans_recall": 0.5}
{"id": "WebQTest-1460", "prediction": ["# Reasoning Path:\nGregor Mendel -> people.person.profession -> Monk -> people.profession.people_with_this_profession -> Abhishiktananda\n# Answer:\nMonk", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Monk -> people.profession.people_with_this_profession -> Adrian Urmanov\n# Answer:\nMonk", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Monk -> common.topic.image -> Germany M\u00c3\u00bcnchen Monks\n# Answer:\nMonk", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Botanist -> base.descriptive_names.names.descriptive_name -> m.0102h0md\n# Answer:\nBotanist", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Botanist -> common.topic.notable_for -> g.12568993_\n# Answer:\nBotanist", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Monk -> people.profession.people_with_this_profession -> Aniruddha Mahathera\n# Answer:\nMonk", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Monk -> base.descriptive_names.names.descriptive_name -> m.0100v_8n\n# Answer:\nMonk", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Botanist -> common.topic.notable_types -> Profession\n# Answer:\nBotanist", "# Reasoning Path:\nGregor Mendel -> common.topic.image -> Mendel -> astronomy.extraterrestrial_location.on_celestial_object -> Mars\n# Answer:\nMendel", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Monk -> common.topic.image -> StAnthony\n# Answer:\nMonk"], "ground_truth": ["Botanist", "Scientist", "Monk"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7659574468085106, "ans_precission": 0.9, "ans_recall": 0.6666666666666666, "path_f1": 0.7659574468085106, "path_precision": 0.9, "path_recall": 0.6666666666666666, "path_ans_f1": 0.7659574468085106, "path_ans_precision": 0.9, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1461", "prediction": ["# Reasoning Path:\nRay Allen -> people.person.spouse_s -> m.0cs25kk -> people.marriage.spouse -> Shannon Walker Williams\n# Answer:\nm.0cs25kk", "# Reasoning Path:\nRay Allen -> people.person.spouse_s -> m.0cs25kk -> people.marriage.location_of_ceremony -> Martha's Vineyard\n# Answer:\nm.0cs25kk", "# Reasoning Path:\nRay Allen -> people.person.spouse_s -> m.0cs25kk -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0cs25kk", "# Reasoning Path:\nRay Allen -> people.person.sibling_s -> m.0j2h3vr -> people.sibling_relationship.sibling -> Kristie Raye\n# Answer:\nm.0j2h3vr", "# Reasoning Path:\nRay Allen -> people.person.sibling_s -> m.0ztqw9x -> people.sibling_relationship.sibling -> John Allen\n# Answer:\nm.0ztqw9x", "# Reasoning Path:\nRay Allen -> sports.pro_athlete.teams -> m.0k2_4y3 -> sports.sports_team_roster.team -> Boston Celtics\n# Answer:\nm.0k2_4y3", "# Reasoning Path:\nRay Allen -> people.person.sibling_s -> m.0ztqwb8 -> people.sibling_relationship.sibling -> Kim Allen\n# Answer:\nm.0ztqwb8", "# Reasoning Path:\nRay Allen -> sports.pro_athlete.teams -> m.05cf4bc -> sports.sports_team_roster.team -> Connecticut Huskies men's basketball\n# Answer:\nm.05cf4bc", "# Reasoning Path:\nRay Allen -> sports.pro_athlete.teams -> m.0k2_4y3 -> sports.sports_team_roster.position -> Shooting guard\n# Answer:\nm.0k2_4y3", "# Reasoning Path:\nRay Allen -> sports.pro_athlete.teams -> m.08s4cgk -> sports.sports_team_roster.position -> Shooting guard\n# Answer:\nm.08s4cgk"], "ground_truth": ["Shannon Walker Williams"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.13333333333333333, "path_precision": 0.1, "path_recall": 0.2, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1462", "prediction": ["# Reasoning Path:\nBarack Obama -> government.us_president.vice_president -> Joe Biden -> people.person.profession -> Lawyer\n# Answer:\nJoe Biden", "# Reasoning Path:\nBarack Obama -> government.us_president.vice_president -> Joe Biden -> base.politicalconventions.vice_presidential_nominee.convention_nominated_at -> 2008 Democratic National Convention\n# Answer:\nJoe Biden", "# Reasoning Path:\nBarack Obama -> government.us_president.vice_president -> Joe Biden -> common.topic.subject_of -> President Barack Obama and the Message Beyond the Photograph\n# Answer:\nJoe Biden", "# Reasoning Path:\nBarack Obama -> government.us_president.vice_president -> Joe Biden -> people.person.profession -> Politician\n# Answer:\nJoe Biden", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.author -> Aberjhani\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.subjects -> Mitt Romney presidential campaign, 2012\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Bound Man -> book.written_work.subjects -> United States of America\n# Answer:\nA Bound Man", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.subjects -> 47 Percent\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> common.topic.notable_types -> Short Non-fiction\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Writer's Journey to Selma, Alabama -> book.written_work.subjects -> Selma\n# Answer:\nA Writer's Journey to Selma, Alabama"], "ground_truth": ["Joe Biden"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1463", "prediction": ["# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> British Sign Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nBritish Sign Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> British Sign Language -> common.topic.notable_types -> Human Language\n# Answer:\nBritish Sign Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> British Sign Language -> language.human_language.region -> Europe\n# Answer:\nBritish Sign Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Scots Language -> common.topic.notable_types -> Human Language\n# Answer:\nScots Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.countries_spoken_in -> Kingdom of Great Britain\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nm.0ghc35h", "# Reasoning Path:\nUnited Kingdom -> location.statistical_region.long_term_unemployment_rate -> g.12cp_jvpx\n# Answer:\ng.12cp_jvpx", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.language_family -> Indo-European languages\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.countries_spoken_in -> Scotland\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.arrested_person -> Gary Glitter\n# Answer:\nm.0ghc35h"], "ground_truth": ["English Language"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1464", "prediction": ["# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Centre for the Arts -> architecture.structure.architect -> Eberhard Zeidler\n# Answer:\nToronto Centre for the Arts", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Centre for the Arts -> travel.tourist_attraction.near_travel_destination -> Newmarket\n# Answer:\nToronto Centre for the Arts", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Centre for the Arts -> common.topic.article -> m.05398q\n# Answer:\nToronto Centre for the Arts", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Islands -> geography.island.body_of_water -> Lake Ontario\n# Answer:\nToronto Islands", "# Reasoning Path:\nToronto -> common.topic.webpage -> m.02nc8t6 -> common.webpage.resource -> City of Toronto Web site\n# Answer:\nm.02nc8t6", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Islands -> common.topic.notable_for -> g.1257xhscc\n# Answer:\nToronto Islands", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Islands -> location.location.containedby -> Ontario\n# Answer:\nToronto Islands", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Air Canada Centre -> location.location.events -> 1st Canadian Women's Hockey League All-Star Game\n# Answer:\nAir Canada Centre", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Air Canada Centre -> common.topic.notable_for -> g.125fsz2rg\n# Answer:\nAir Canada Centre", "# Reasoning Path:\nToronto -> travel.travel_destination.local_transportation -> GO Transit -> common.topic.notable_types -> Mass Transportation System\n# Answer:\nGO Transit"], "ground_truth": ["McMichael Canadian Art Collection", "Sugar Beach", "Queen Street West", "Air Canada Centre", "Sony Centre for the Performing Arts", "Hanlan's Point Beach", "Corktown Common", "Rogers Centre", "First Toronto Post Office", "Cabbagetown, Toronto", "Bare Oaks Family Naturist Park", "Queens Quay", "Black Creek Pioneer Village", "CN Tower", "The Beaches", "Gardiner Museum", "Yorkville, Toronto", "Luminato", "Toronto Waterfront Marathon", "Textile Museum of Canada", "Royal Alexandra Theatre", "Spadina House", "Edwards Gardens", "Canada's Wonderland", "BMO Field", "Planet in Focus", "Toronto Islands", "Toronto Centre for the Arts", "Chinatown, Toronto", "Bayview Village Shopping Centre", "Hockey Hall of Fame", "High Park", "Kensington Market", "Roy Thomson Hall", "Heartland Town Centre", "Princess of Wales Theatre", "Royal Ontario Museum", "Bata Shoe Museum", "Distillery District", "Toronto Eaton Centre", "Danforth Avenue", "Art Gallery of Ontario", "Nathan Phillips Square", "Ontario Place", "Casa Loma", "Toronto Zoo", "St. Lawrence Market", "Ontario Science Centre", "Museum of Contemporary Canadian Art"], "ans_acc": 0.061224489795918366, "ans_hit": 1, "ans_f1": 0.1137440758293839, "ans_precission": 0.8, "ans_recall": 0.061224489795918366, "path_f1": 0.1137440758293839, "path_precision": 0.8, "path_recall": 0.061224489795918366, "path_ans_f1": 0.1137440758293839, "path_ans_precision": 0.8, "path_ans_recall": 0.061224489795918366}
{"id": "WebQTest-1465", "prediction": ["# Reasoning Path:\nSouth Korea -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Argentina\n# Answer:\nConstitutional republic", "# Reasoning Path:\nSouth Korea -> location.statistical_region.gdp_nominal_per_capita -> g.11b60sjxs7\n# Answer:\ng.11b60sjxs7", "# Reasoning Path:\nSouth Korea -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Chile\n# Answer:\nConstitutional republic", "# Reasoning Path:\nSouth Korea -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Angola\n# Answer:\nUnitary state", "# Reasoning Path:\nSouth Korea -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nSouth Korea -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Angola\n# Answer:\nPresidential system", "# Reasoning Path:\nSouth Korea -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6g_c4\n# Answer:\ng.12tb6g_c4", "# Reasoning Path:\nSouth Korea -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Chile\n# Answer:\nUnitary state", "# Reasoning Path:\nSouth Korea -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc37pmk\n# Answer:\ng.1hhc37pmk", "# Reasoning Path:\nSouth Korea -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state"], "ground_truth": ["Constitutional republic", "Presidential system", "Unitary state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1467", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Anne Harris\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Ebenezer Franklin\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.place_of_birth -> Ecton\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Elizabeth Douse\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.spouse_s -> m.0j4k0mt\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger -> people.person.parents -> Mary Morrill\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger -> people.person.parents -> Peter Folger\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger -> people.deceased_person.place_of_burial -> Granary Burying Ground\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nm.012zbkk5", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.spouse_s -> m.0j4k0ll\n# Answer:\nJosiah Franklin"], "ground_truth": ["Deborah Read"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1468", "prediction": ["# Reasoning Path:\nSri Lanka -> location.country.capital -> Colombo -> location.location.events -> Battle of Mulleriyawa\n# Answer:\nColombo", "# Reasoning Path:\nSri Lanka -> location.country.capital -> Sri Jayawardenepura Kotte -> location.location.events -> 1987 grenade attack in the Sri Lankan Parliament\n# Answer:\nSri Jayawardenepura Kotte", "# Reasoning Path:\nSri Lanka -> location.statistical_region.electricity_consumption_per_capita -> g.1245_0s28\n# Answer:\ng.1245_0s28", "# Reasoning Path:\nSri Lanka -> location.country.capital -> Sri Jayawardenepura Kotte -> common.topic.notable_types -> City/Town/Village\n# Answer:\nSri Jayawardenepura Kotte", "# Reasoning Path:\nSri Lanka -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vqmm4\n# Answer:\ng.11b71vqmm4", "# Reasoning Path:\nSri Lanka -> location.country.capital -> Sri Jayawardenepura Kotte -> common.topic.image -> The Parliament of Sri Lanka\n# Answer:\nSri Jayawardenepura Kotte", "# Reasoning Path:\nSri Lanka -> location.statistical_region.electricity_consumption_per_capita -> g.1245_4m56\n# Answer:\ng.1245_4m56", "# Reasoning Path:\nSri Lanka -> location.statistical_region.electricity_consumption_per_capita -> g.1245_67k2\n# Answer:\ng.1245_67k2", "# Reasoning Path:\nSri Lanka -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gh7w\n# Answer:\ng.12tb6gh7w", "# Reasoning Path:\nSri Lanka -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37hcc\n# Answer:\ng.1hhc37hcc"], "ground_truth": ["Sri Jayawardenepura Kotte", "Colombo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-147", "prediction": ["# Reasoning Path:\nTennessee -> government.governmental_jurisdiction.governing_officials -> m.04j5sk4 -> government.government_position_held.office_holder -> Andrew Johnson\n# Answer:\nm.04j5sk4", "# Reasoning Path:\nTennessee -> government.governmental_jurisdiction.governing_officials -> m.04j5sk4 -> government.government_position_held.basic_title -> Governor\n# Answer:\nm.04j5sk4", "# Reasoning Path:\nTennessee -> government.governmental_jurisdiction.governing_officials -> m.010178yn -> government.government_position_held.office_holder -> Andrew Farmer\n# Answer:\nm.010178yn", "# Reasoning Path:\nTennessee -> government.governmental_jurisdiction.governing_officials -> m.04j5sk4 -> government.government_position_held.office_position_or_title -> Governor of Tennessee\n# Answer:\nm.04j5sk4", "# Reasoning Path:\nTennessee -> government.governmental_jurisdiction.governing_officials -> m.01017wwj -> government.government_position_held.office_holder -> Timothy Hill\n# Answer:\nm.01017wwj", "# Reasoning Path:\nTennessee -> government.governmental_jurisdiction.governing_officials -> m.010178yn -> government.government_position_held.basic_title -> State Representative\n# Answer:\nm.010178yn", "# Reasoning Path:\nTennessee -> government.governmental_jurisdiction.governing_officials -> m.01017wwj -> government.government_position_held.office_position_or_title -> Tennessee State Representative\n# Answer:\nm.01017wwj", "# Reasoning Path:\nTennessee -> book.book_subject.works -> After the war -> book.written_work.author -> Richard Marius\n# Answer:\nAfter the war", "# Reasoning Path:\nTennessee -> book.book_subject.works -> At Risk -> book.written_work.subjects -> Massachusetts\n# Answer:\nAt Risk", "# Reasoning Path:\nTennessee -> common.topic.webpage -> m.04lswkp -> common.webpage.category -> Official Website\n# Answer:\nm.04lswkp"], "ground_truth": ["William Haslam"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1470", "prediction": ["# Reasoning Path:\nPoland -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Austria\n# Answer:\nParliamentary republic", "# Reasoning Path:\nPoland -> location.statistical_region.size_of_armed_forces -> g.12cp_j3lx\n# Answer:\ng.12cp_j3lx", "# Reasoning Path:\nPoland -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Czech Republic\n# Answer:\nParliamentary republic", "# Reasoning Path:\nPoland -> location.country.form_of_government -> Democracy -> organization.organization_sector.organizations_in_this_sector -> Council of Europe\n# Answer:\nDemocracy", "# Reasoning Path:\nPoland -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Finland\n# Answer:\nParliamentary republic", "# Reasoning Path:\nPoland -> location.statistical_region.size_of_armed_forces -> g.1hhc385gk\n# Answer:\ng.1hhc385gk", "# Reasoning Path:\nPoland -> location.statistical_region.size_of_armed_forces -> g.1hhc39krm\n# Answer:\ng.1hhc39krm", "# Reasoning Path:\nPoland -> book.book_subject.works -> A Holocaust Controversy -> book.written_work.subjects -> 20th century\n# Answer:\nA Holocaust Controversy", "# Reasoning Path:\nPoland -> book.book_subject.works -> A Holocaust Controversy -> book.written_work.subjects -> France\n# Answer:\nA Holocaust Controversy", "# Reasoning Path:\nPoland -> book.book_subject.works -> A Holocaust Controversy -> common.topic.notable_types -> Book\n# Answer:\nA Holocaust Controversy"], "ground_truth": ["Democracy", "Parliamentary republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1471", "prediction": ["# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.010hbq1t -> government.government_position_held.office_holder -> John Carney\n# Answer:\nm.010hbq1t", "# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.010hbwj4 -> government.government_position_held.office_holder -> Harriet Smith Windsor\n# Answer:\nm.010hbwj4", "# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.010hbq1t -> government.government_position_held.office_position_or_title -> Lieutenant Governor of Delaware\n# Answer:\nm.010hbq1t", "# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.010hbqbl -> government.government_position_held.office_holder -> Carl Danberg\n# Answer:\nm.010hbqbl", "# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.010hbq1t -> government.government_position_held.basic_title -> Lieutenant Governor\n# Answer:\nm.010hbq1t", "# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.010hbqbl -> government.government_position_held.basic_title -> Attorney general\n# Answer:\nm.010hbqbl", "# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.010hbqbl -> government.government_position_held.office_position_or_title -> Attorney General of Delaware\n# Answer:\nm.010hbqbl", "# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.010hbwj4 -> government.government_position_held.appointed_by -> Ruth Ann Minner\n# Answer:\nm.010hbwj4", "# Reasoning Path:\nDelaware -> military.military_unit_place_of_origin.military_units -> 1st Delaware Infantry Regiment -> military.military_unit.armed_force -> Union Army\n# Answer:\n1st Delaware Infantry Regiment", "# Reasoning Path:\nDelaware -> military.military_unit_place_of_origin.military_units -> 1st Regiment Delaware Volunteer Cavalry -> military.military_unit.armed_force -> Union Army\n# Answer:\n1st Regiment Delaware Volunteer Cavalry"], "ground_truth": ["Patrick Kearney", "R. R. M. Carpenter", "Huck Betts", "Alfred I. du Pont", "Tully Satre", "William Grassie", "Francine Fournier", "Matt Stawicki", "Jeremy Conway", "Steve Ressel", "Jacqueline Jones", "Bill Indursky", "Chris Dapkins", "Solomon Bayley", "Rebecca Lee Crumpler", "Collins J. Seitz", "John Sedwick", "Ed Haskett", "Andrew Cebulka", "Chris Gutierrez", "Norman Hutchins", "Fred Lonberg-Holm", "David McElwee", "William King", "Edward Groesbeck Voss", "Nathaniel Harrington Bannister", "James B. Clark, Jr.", "Jim Wilson", "A.C. Golden", "Tom Peszek", "Eddie Paskey", "Katharine Pyle", "Herbert Bennett Fenn", "Reuben James", "Rex Carlton", "Outerbridge Horsey IV", "Griffin Seward", "Hampton Del Ruth", "Billy Ficca", "Jeffrey W. Bullock", "Joe Garcio", "Hugh T. Broomall", "James Tilton", "Ann Marie Borghese", "Gimel \\\"Young Guru\\\" Keaton"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1472", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> government.politician.government_positions_held -> m.04stpgl -> government.government_position_held.office_position_or_title -> Governor of Massachusetts\n# Answer:\nm.04stpgl", "# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Michigan\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Michigan\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> government.politician.government_positions_held -> m.04stpgl -> government.government_position_held.jurisdiction_of_office -> Massachusetts\n# Answer:\nm.04stpgl", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0_gvz4k -> film.personal_film_appearance.film -> Mitt\n# Answer:\nm.0_gvz4k", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0_gvz4k -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.0_gvz4k", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0nhmmbv -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.0nhmmbv", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0nhlrm9 -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.0nhlrm9"], "ground_truth": ["2003-01-02"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1473", "prediction": ["# Reasoning Path:\nCyprus -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nCyprus -> location.country.currency_used -> Euro -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nEuro", "# Reasoning Path:\nCyprus -> location.country.currency_used -> Euro -> common.topic.notable_types -> Currency\n# Answer:\nEuro", "# Reasoning Path:\nCyprus -> location.country.currency_used -> Euro -> finance.currency.countries_used -> France\n# Answer:\nEuro", "# Reasoning Path:\nCyprus -> location.statistical_region.gender_balance_members_of_parliament -> g.11b71y1l3w\n# Answer:\ng.11b71y1l3w", "# Reasoning Path:\nCyprus -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Germany\n# Answer:\nEuro", "# Reasoning Path:\nCyprus -> location.statistical_region.gender_balance_members_of_parliament -> g.1hhc3_4dv\n# Answer:\ng.1hhc3_4dv", "# Reasoning Path:\nCyprus -> location.statistical_region.gender_balance_members_of_parliament -> g.1hhc3f_q5\n# Answer:\ng.1hhc3f_q5", "# Reasoning Path:\nCyprus -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Greece\n# Answer:\nConstitutional republic", "# Reasoning Path:\nCyprus -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Azerbaijan\n# Answer:\nConstitutional republic"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1475", "prediction": ["# Reasoning Path:\nJohnny Depp -> music.group_member.instruments_played -> Guitar -> music.instrument.variation -> Bass guitar\n# Answer:\nGuitar", "# Reasoning Path:\nJohnny Depp -> music.group_member.instruments_played -> Guitar -> music.instrument.family -> Reverb\n# Answer:\nGuitar", "# Reasoning Path:\nJohnny Depp -> music.group_member.instruments_played -> Guitar -> common.topic.subject_of -> Reverb\n# Answer:\nGuitar", "# Reasoning Path:\nJohnny Depp -> music.group_member.instruments_played -> Guitar -> music.instrument.variation -> Electric guitar\n# Answer:\nGuitar", "# Reasoning Path:\nJohnny Depp -> music.group_member.instruments_played -> Slide guitar -> music.performance_role.guest_performances -> m.04flflz\n# Answer:\nSlide guitar", "# Reasoning Path:\nJohnny Depp -> music.group_member.instruments_played -> Slide guitar -> music.performance_role.guest_performances -> m.04flfm5\n# Answer:\nSlide guitar", "# Reasoning Path:\nJohnny Depp -> award.award_nominee.award_nominations -> m.03mlr5q -> award.award_nomination.nominated_for -> Pirates of the Caribbean: The Curse of the Black Pearl\n# Answer:\nm.03mlr5q", "# Reasoning Path:\nJohnny Depp -> award.award_nominee.award_nominations -> m.03mlr72 -> award.award_nomination.ceremony -> 77th Academy Awards\n# Answer:\nm.03mlr72", "# Reasoning Path:\nJohnny Depp -> award.award_nominee.award_nominations -> m.03mlr5q -> award.award_nomination.ceremony -> 76th Academy Awards\n# Answer:\nm.03mlr5q", "# Reasoning Path:\nJohnny Depp -> film.person_or_entity_appearing_in_film.films -> m.049ykr5 -> film.personal_film_appearance.type_of_appearance -> Narrator\n# Answer:\nm.049ykr5"], "ground_truth": ["Guitar", "Slide guitar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1477", "prediction": ["# Reasoning Path:\nCarrie Underwood -> broadcast.artist.content -> 181-kickincountry -> common.topic.notable_types -> Broadcast Content\n# Answer:\n181-kickincountry", "# Reasoning Path:\nCarrie Underwood -> broadcast.artist.content -> 1Club.FM: Channel One -> broadcast.content.artist -> Aaliyah\n# Answer:\n1Club.FM: Channel One", "# Reasoning Path:\nCarrie Underwood -> broadcast.artist.content -> 1Club.FM: Channel One -> broadcast.content.location -> Chicago\n# Answer:\n1Club.FM: Channel One", "# Reasoning Path:\nCarrie Underwood -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nCarrie Underwood -> celebrities.celebrity.sexual_relationships -> m.04fnq50 -> celebrities.romantic_relationship.celebrity -> Chace Crawford\n# Answer:\nm.04fnq50", "# Reasoning Path:\nCarrie Underwood -> broadcast.artist.content -> 1Club.FM: Channel One -> broadcast.content.artist -> Alicia Keys\n# Answer:\n1Club.FM: Channel One", "# Reasoning Path:\nCarrie Underwood -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nCarrie Underwood -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nCarrie Underwood -> broadcast.artist.content -> 1Club.FM: Channel One -> broadcast.content.artist -> Anna Nalick\n# Answer:\n1Club.FM: Channel One", "# Reasoning Path:\nCarrie Underwood -> broadcast.artist.content -> 1Club.FM: Channel One -> broadcast.content.genre -> Classic hits\n# Answer:\n1Club.FM: Channel One"], "ground_truth": ["Hooked"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1478", "prediction": ["# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Making Our Democracy Work -> book.written_work.author -> Stephen Breyer\n# Answer:\nMaking Our Democracy Work", "# Reasoning Path:\nSupreme Court of the United States -> law.court.inferior_courts -> Arkansas Supreme Court -> law.court.judges -> m.0wzgtvm\n# Answer:\nArkansas Supreme Court", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Savannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal -> book.written_work.author -> Aberjhani\n# Answer:\nSavannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> law.judicial_title.judges -> m.046x3bs\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Making Our Democracy Work -> book.written_work.subjects -> United States Constitution\n# Answer:\nMaking Our Democracy Work", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Making Our Democracy Work -> common.topic.notable_for -> g.125728tkq\n# Answer:\nMaking Our Democracy Work", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Courtiers of the Marble Palace: The Rise and Influence of the Supreme Court Law Clerk -> book.written_work.subjects -> Law clerk\n# Answer:\nCourtiers of the Marble Palace: The Rise and Influence of the Supreme Court Law Clerk", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> people.appointed_role.appointment -> m.0hn35mm\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> law.court.inferior_courts -> Arkansas Supreme Court -> law.court.legal_cases -> Howard v. Arkansas\n# Answer:\nArkansas Supreme Court", "# Reasoning Path:\nSupreme Court of the United States -> law.court.inferior_courts -> Arkansas Supreme Court -> common.topic.notable_for -> g.12597wdlt\n# Answer:\nArkansas Supreme Court"], "ground_truth": ["Antonin Scalia", "John Roberts", "Ruth Bader Ginsburg", "Anthony Kennedy", "Samuel Alito", "Stephen Breyer", "Sonia Sotomayor", "Elena Kagan", "Clarence Thomas"], "ans_acc": 0.1111111111111111, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.10526315789473685, "path_ans_precision": 0.1, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-1479", "prediction": ["# Reasoning Path:\nDustin Johnson -> people.person.profession -> Golfer -> common.topic.notable_types -> Profession\n# Answer:\nGolfer", "# Reasoning Path:\nDustin Johnson -> people.person.profession -> Golfer -> people.profession.specializations -> Professional golfer\n# Answer:\nGolfer", "# Reasoning Path:\nDustin Johnson -> celebrities.celebrity.sexual_relationships -> m.0wpd79b -> celebrities.romantic_relationship.celebrity -> Paulina Gretzky\n# Answer:\nm.0wpd79b", "# Reasoning Path:\nDustin Johnson -> people.person.profession -> Golfer -> base.descriptive_names.names.descriptive_name -> m.0106cygr\n# Answer:\nGolfer", "# Reasoning Path:\nDustin Johnson -> common.topic.notable_types -> Celebrity -> type.type.properties -> Celebrity friends\n# Answer:\nCelebrity", "# Reasoning Path:\nDustin Johnson -> common.topic.notable_types -> Celebrity -> type.type.domain -> Celebrities\n# Answer:\nCelebrity", "# Reasoning Path:\nDustin Johnson -> celebrities.celebrity.sexual_relationships -> m.0wpd79b -> celebrities.romantic_relationship.relationship_type -> Engaged to\n# Answer:\nm.0wpd79b", "# Reasoning Path:\nDustin Johnson -> people.person.profession -> Golfer -> base.descriptive_names.names.descriptive_name -> m.0106cyt1\n# Answer:\nGolfer", "# Reasoning Path:\nDustin Johnson -> common.topic.notable_types -> Celebrity -> type.type.properties -> Celebrity rivals\n# Answer:\nCelebrity", "# Reasoning Path:\nDustin Johnson -> people.person.profession -> Golfer -> base.descriptive_names.names.descriptive_name -> m.0106db86\n# Answer:\nGolfer"], "ground_truth": ["Golfer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1480", "prediction": ["# Reasoning Path:\nCher -> people.person.spouse_s -> m.023b86m -> people.marriage.location_of_ceremony -> Tijuana\n# Answer:\nm.023b86m", "# Reasoning Path:\nCher -> people.person.spouse_s -> m.023b86m -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.023b86m", "# Reasoning Path:\nCher -> people.person.spouse_s -> m.04yw182 -> people.marriage.spouse -> Gregg Allman\n# Answer:\nm.04yw182", "# Reasoning Path:\nCher -> award.award_nominee.award_nominations -> m.0w4glj0 -> award.award_nomination.ceremony -> 24th Primetime Emmy Awards\n# Answer:\nm.0w4glj0", "# Reasoning Path:\nCher -> people.person.spouse_s -> m.023b86m -> people.marriage.spouse -> Sonny Bono\n# Answer:\nm.023b86m", "# Reasoning Path:\nCher -> award.award_nominee.award_nominations -> m.0z8_9qp -> award.award_nomination.award_nominee -> Sonny Bono\n# Answer:\nm.0z8_9qp", "# Reasoning Path:\nCher -> people.person.spouse_s -> m.04yw182 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.04yw182", "# Reasoning Path:\nCher -> award.award_nominee.award_nominations -> m.0w4glj0 -> award.award_nomination.nominated_for -> The Sonny & Cher Comedy Hour\n# Answer:\nm.0w4glj0", "# Reasoning Path:\nCher -> people.person.spouse_s -> m.04yw182 -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.04yw182", "# Reasoning Path:\nCher -> award.award_nominee.award_nominations -> m.0w4glj0 -> award.award_nomination.award_nominee -> Sonny Bono\n# Answer:\nm.0w4glj0"], "ground_truth": ["Sonny Bono", "Gregg Allman"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3076923076923077, "path_precision": 0.4, "path_recall": 0.25, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1481", "prediction": ["# Reasoning Path:\nUniversity of Oregon -> location.location.containedby -> Eugene -> location.location.containedby -> Oregon\n# Answer:\nEugene", "# Reasoning Path:\nUniversity of Oregon -> location.location.containedby -> Oregon -> location.location.containedby -> United States of America\n# Answer:\nOregon", "# Reasoning Path:\nUniversity of Oregon -> location.location.containedby -> Eugene -> location.location.containedby -> Area code 458\n# Answer:\nEugene", "# Reasoning Path:\nUniversity of Oregon -> location.location.containedby -> Eugene -> location.location.containedby -> Area code 541\n# Answer:\nEugene", "# Reasoning Path:\nUniversity of Oregon -> education.university.number_of_undergraduates -> m.0h77tx1\n# Answer:\nm.0h77tx1", "# Reasoning Path:\nUniversity of Oregon -> location.location.containedby -> Eugene -> location.location.contains -> A. V. Peters House\n# Answer:\nEugene", "# Reasoning Path:\nUniversity of Oregon -> location.location.containedby -> Oregon -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nOregon", "# Reasoning Path:\nUniversity of Oregon -> location.location.containedby -> Eugene -> location.location.events -> 1963 NCAA Men's Division I Basketball Tournament\n# Answer:\nEugene", "# Reasoning Path:\nUniversity of Oregon -> location.location.containedby -> Oregon -> location.us_state.capital -> Salem\n# Answer:\nOregon", "# Reasoning Path:\nUniversity of Oregon -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Oregon\n# Answer:\nUnited States of America"], "ground_truth": ["Eugene"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1482", "prediction": ["# Reasoning Path:\nSpy Kids -> film.film.starring -> m.02s9tsc -> film.performance.actor -> Alexa Vega\n# Answer:\nm.02s9tsc", "# Reasoning Path:\nSpy Kids -> film.film.starring -> m.02s9trw -> film.performance.actor -> Daryl Sabara\n# Answer:\nm.02s9trw", "# Reasoning Path:\nSpy Kids -> film.film.starring -> m.02s9tsc -> film.performance.character -> Baby Carmen\n# Answer:\nm.02s9tsc", "# Reasoning Path:\nSpy Kids -> film.film.starring -> m.01z0pft -> film.performance.actor -> Carla Gugino\n# Answer:\nm.01z0pft", "# Reasoning Path:\nSpy Kids -> common.topic.webpage -> http://elfman.filmmusic.com/spykids/\n# Answer:\nhttp://elfman.filmmusic.com/spykids/", "# Reasoning Path:\nSpy Kids -> film.film.starring -> m.02s9trw -> film.performance.character -> Juni Cortez\n# Answer:\nm.02s9trw", "# Reasoning Path:\nSpy Kids -> common.topic.webpage -> http://www.soundtrack.net/soundtracks/database/?id=2839\n# Answer:\nhttp://www.soundtrack.net/soundtracks/database/?id=2839", "# Reasoning Path:\nSpy Kids -> film.film.sequel -> Spy Kids 2: The Island of Lost Dreams -> film.film.starring -> m.02s9tvb\n# Answer:\nSpy Kids 2: The Island of Lost Dreams", "# Reasoning Path:\nSpy Kids -> film.film.starring -> m.01z0pft -> film.performance.character -> Ingrid Cortez\n# Answer:\nm.01z0pft", "# Reasoning Path:\nSpy Kids -> film.film.sequel -> Spy Kids 2: The Island of Lost Dreams -> film.film.starring -> m.02s9tx3\n# Answer:\nSpy Kids 2: The Island of Lost Dreams"], "ground_truth": ["Alexa Vega", "Addisyn Fair"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 0.1, "path_ans_recall": 0.5}
{"id": "WebQTest-1483", "prediction": ["# Reasoning Path:\nGermany -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Austria\n# Answer:\nFederal republic", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\ng.11b60v9zy7", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Austria\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_38m_\n# Answer:\ng.1245_38m_", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fdl8\n# Answer:\ng.12tb6fdl8", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Czech Republic\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc38c_4\n# Answer:\ng.1hhc38c_4", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_67jr\n# Answer:\ng.1245_67jr", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Poland\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_jlz4\n# Answer:\ng.1245_jlz4"], "ground_truth": ["Representative democracy", "Constitutional republic", "Federal republic", "Parliamentary republic"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.4, "ans_recall": 0.5, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.4, "path_ans_recall": 0.5}
{"id": "WebQTest-1484", "prediction": ["# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Geoffrey Chaucer -> influence.influence_node.influenced_by -> Ovid\n# Answer:\nGeoffrey Chaucer", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Lucian -> influence.influence_node.influenced -> Baltasar Graci\u00e1n\n# Answer:\nLucian", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Geoffrey Chaucer -> common.topic.notable_types -> Author\n# Answer:\nGeoffrey Chaucer", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Geoffrey Chaucer -> people.person.ethnicity -> English people\n# Answer:\nGeoffrey Chaucer", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Lucian -> common.topic.notable_types -> Author\n# Answer:\nLucian", "# Reasoning Path:\nWilliam Shakespeare -> freebase.valuenotation.is_reviewed -> Art Form\n# Answer:\nArt Form", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Lucian -> people.person.gender -> Male\n# Answer:\nLucian", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Thomas More -> people.deceased_person.place_of_death -> London\n# Answer:\nThomas More", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Thomas More -> people.person.nationality -> United Kingdom\n# Answer:\nThomas More", "# Reasoning Path:\nWilliam Shakespeare -> freebase.valuenotation.is_reviewed -> Art Subject\n# Answer:\nArt Subject"], "ground_truth": ["John Pory", "Terence", "Thomas More", "Ovid", "Virgil", "Plautus", "Thomas Kyd", "Plutarch", "Edmund Spenser", "Geoffrey Chaucer", "Seneca the Younger", "Christopher Marlowe", "Michel de Montaigne", "Lucian"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.3380281690140845, "ans_precission": 0.8, "ans_recall": 0.21428571428571427, "path_f1": 0.3380281690140845, "path_precision": 0.8, "path_recall": 0.21428571428571427, "path_ans_f1": 0.4210526315789473, "path_ans_precision": 0.8, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-1485", "prediction": ["# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Revaluation of the Turkish Lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nRevaluation of the Turkish Lira", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.065sxsl -> military.military_combatant_group.conflict -> Korean War\n# Answer:\nm.065sxsl", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.066402y -> military.military_combatant_group.conflict -> Iraqi no-fly zones\n# Answer:\nm.066402y"], "ground_truth": ["Turkish lira"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-1486", "prediction": ["# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> education.education.institution -> Mercy College\n# Answer:\nm.0j_gm2q", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.04hc7zn -> education.education.institution -> Phillips Exeter Academy\n# Answer:\nm.04hc7zn", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.02n93cn -> education.education.institution -> Harvard University\n# Answer:\nm.02n93cn", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nm.0j_gm2q", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_value -> End Date\n# Answer:\nm.0j_gm2q", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nm.0j_gm2q", "# Reasoning Path:\nMark Zuckerberg -> influence.influence_node.influenced -> Austin Cameron -> influence.influence_node.influenced_by -> Bill Gates\n# Answer:\nAustin Cameron", "# Reasoning Path:\nMark Zuckerberg -> influence.influence_node.influenced -> Paul Scolnick -> people.person.education -> m.01314533\n# Answer:\nPaul Scolnick", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.02n93cn -> freebase.valuenotation.has_value -> End Date\n# Answer:\nm.02n93cn", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Specialization\n# Answer:\nm.0j_gm2q"], "ground_truth": ["Mercy College", "Harvard University", "Ardsley High School", "Phillips Exeter Academy"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4285714285714285, "path_ans_precision": 0.3, "path_ans_recall": 0.75}
{"id": "WebQTest-1487", "prediction": ["# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.places_lived -> m.03plx6x -> people.place_lived.location -> Reading\n# Answer:\nm.03plx6x", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.places_lived -> m.03pv20m -> people.place_lived.location -> Bucklebury\n# Answer:\nm.03pv20m", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.place_of_birth -> Royal Berkshire Hospital -> common.topic.webpage -> m.03l7bmh\n# Answer:\nRoyal Berkshire Hospital", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.place_of_birth -> Royal Berkshire Hospital -> location.location.geolocation -> m.02_scs7\n# Answer:\nRoyal Berkshire Hospital", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.place_of_birth -> Royal Berkshire Hospital -> common.topic.notable_types -> Hospital\n# Answer:\nRoyal Berkshire Hospital", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.nationality -> England -> government.governmental_jurisdiction.government_bodies -> NHS England\n# Answer:\nEngland", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.place_of_birth -> Royal Berkshire Hospital -> common.topic.webpage -> m.0fq430w\n# Answer:\nRoyal Berkshire Hospital", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.nationality -> England -> government.governmental_jurisdiction.agencies -> NHS England\n# Answer:\nEngland", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.nationality -> England -> government.governmental_jurisdiction.government_bodies -> National Industrial Relations Court\n# Answer:\nEngland", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.nationality -> England -> base.aareas.schema.administrative_area.pertinent_type -> English administrative county\n# Answer:\nEngland"], "ground_truth": ["Reading", "Bucklebury"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1488", "prediction": ["# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Surrealism -> influence.influence_node.influenced -> Aberjhani\n# Answer:\nSurrealism", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Surrealism -> business.product_theme.products -> Salvador Dali Registry\n# Answer:\nSurrealism", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Surrealism -> book.school_or_movement.associated_authors -> Aberjhani\n# Answer:\nSurrealism", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Eyewitness: Reports from an Art World in Crisis\n# Answer:\nModern art", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nModern art", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Cubism -> visual_art.art_period_movement.associated_artworks -> Abstraction\n# Answer:\nCubism", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Cubism -> common.topic.notable_for -> g.1257zncnt\n# Answer:\nCubism", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Surrealism -> book.school_or_movement.associated_authors -> August Strindberg\n# Answer:\nSurrealism", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Origins, Imitation, Conventions\n# Answer:\nModern art", "# Reasoning Path:\nMarc Chagall -> base.kwebbase.kwtopic.has_sentences -> After he and Bella were married, Chagall was surprised when the outbreak of World War I prevented him from leaving the country,  and he was conscripted into the army. -> base.kwebbase.kwsentence.previous_sentence -> He intended to make an appearance at the opening before continuing on to Vitebsk, where he would marry his sweetheart, Bella Rosenberg, and return with her to Paris.\n# Answer:\nAfter he and Bella were married, Chagall was surprised when the outbreak of World War I prevented him from leaving the country,  and he was conscripted into the army."], "ground_truth": ["Expressionism", "Cubism", "Modern art", "Fauvism", "Surrealism"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7200000000000001, "ans_precission": 0.9, "ans_recall": 0.6, "path_f1": 0.7200000000000001, "path_precision": 0.9, "path_recall": 0.6, "path_ans_f1": 0.7200000000000001, "path_ans_precision": 0.9, "path_ans_recall": 0.6}
{"id": "WebQTest-1489", "prediction": ["# Reasoning Path:\nBilly Dee Williams -> film.actor.film -> m.0h2jz23 -> film.performance.character -> Lando Calrissian\n# Answer:\nm.0h2jz23", "# Reasoning Path:\nBilly Dee Williams -> film.actor.film -> m.0h2jz23 -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0h2jz23", "# Reasoning Path:\nBilly Dee Williams -> film.actor.film -> m.0h2jz23 -> film.performance.film -> Robot Chicken: Star Wars Episode III\n# Answer:\nm.0h2jz23", "# Reasoning Path:\nBilly Dee Williams -> film.actor.film -> m.011rh2vj -> film.performance.character -> Bartender (voice)\n# Answer:\nm.011rh2vj", "# Reasoning Path:\nBilly Dee Williams -> film.actor.film -> m.010g_8m5 -> film.performance.film -> The Right of the People\n# Answer:\nm.010g_8m5", "# Reasoning Path:\nBilly Dee Williams -> people.person.place_of_birth -> New York City -> common.topic.subject_of -> The Approaching 100th Anniversary of the Harlem Renaissance (part 2)\n# Answer:\nNew York City", "# Reasoning Path:\nBilly Dee Williams -> film.actor.film -> m.011rh2vj -> film.performance.film -> Oedipus\n# Answer:\nm.011rh2vj", "# Reasoning Path:\nBilly Dee Williams -> people.person.place_of_birth -> New York City -> base.militaryinfiction.location_in_fiction.contained_by -> New York\n# Answer:\nNew York City", "# Reasoning Path:\nBilly Dee Williams -> common.topic.webpage -> m.09433bs -> common.webpage.resource -> WILLIAMS TELLS\n# Answer:\nm.09433bs", "# Reasoning Path:\nBilly Dee Williams -> common.topic.webpage -> m.09433bs -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09433bs"], "ground_truth": ["Lando Calrissian"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-149", "prediction": ["# Reasoning Path:\nPortugal -> location.country.official_language -> Portuguese Language -> language.human_language.countries_spoken_in -> Mozambique\n# Answer:\nPortuguese Language", "# Reasoning Path:\nPortugal -> location.country.languages_spoken -> Portuguese Language -> language.human_language.countries_spoken_in -> Mozambique\n# Answer:\nPortuguese Language", "# Reasoning Path:\nPortugal -> location.country.official_language -> Portuguese Language -> language.human_language.region -> Europe\n# Answer:\nPortuguese Language", "# Reasoning Path:\nPortugal -> location.country.official_language -> Portuguese Language -> language.human_language.countries_spoken_in -> Angola\n# Answer:\nPortuguese Language", "# Reasoning Path:\nPortugal -> location.statistical_region.cpi_inflation_rate -> g.11b60zrgdp\n# Answer:\ng.11b60zrgdp", "# Reasoning Path:\nPortugal -> location.country.official_language -> Portuguese Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nPortuguese Language", "# Reasoning Path:\nPortugal -> location.country.languages_spoken -> Portuguese Language -> language.human_language.region -> Europe\n# Answer:\nPortuguese Language", "# Reasoning Path:\nPortugal -> location.country.languages_spoken -> Portuguese Language -> language.human_language.countries_spoken_in -> Angola\n# Answer:\nPortuguese Language", "# Reasoning Path:\nPortugal -> location.statistical_region.cpi_inflation_rate -> g.12tb6ggzs\n# Answer:\ng.12tb6ggzs", "# Reasoning Path:\nPortugal -> location.country.languages_spoken -> Portuguese Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nPortuguese Language"], "ground_truth": ["Portuguese Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1490", "prediction": ["# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Guaran\u00ed language -> common.topic.notable_types -> Human Language\n# Answer:\nGuaran\u00ed language", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Kayak II -> biology.organism.organism_type -> Horse\n# Answer:\nKayak II", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nItalian Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Italian Language -> common.topic.notable_types -> Human Language\n# Answer:\nItalian Language", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Kayak II -> base.thoroughbredracing.thoroughbred_racehorse.sex -> Stallion\n# Answer:\nKayak II", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> language.language_family.geographic_distribution -> Bolivia\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.location.partially_contains -> Alto San Juan -> location.location.partially_contained_by -> m.0wg8lvc\n# Answer:\nAlto San Juan", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nItalian Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> common.topic.notable_types -> Human Language\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> language.language_family.geographic_distribution -> Peru\n# Answer:\nQuechuan languages"], "ground_truth": ["Italian Language", "Yiddish Language", "Quechuan languages", "Guaran\u00ed language", "Spanish Language"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6461538461538462, "ans_precission": 0.7, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.6461538461538462, "path_ans_precision": 0.7, "path_ans_recall": 0.6}
{"id": "WebQTest-1493", "prediction": ["# Reasoning Path:\nWilfred Owen -> people.person.education -> m.0n18h9_ -> education.education.institution -> University of Reading\n# Answer:\nm.0n18h9_", "# Reasoning Path:\nWilfred Owen -> people.person.education -> m.010hdpvg -> education.education.institution -> Wakeman School\n# Answer:\nm.010hdpvg", "# Reasoning Path:\nWilfred Owen -> people.person.education -> m.0n18h9_ -> freebase.valuenotation.has_value -> Degree\n# Answer:\nm.0n18h9_", "# Reasoning Path:\nWilfred Owen -> people.person.education -> m.010hdpvg -> freebase.valuenotation.has_value -> End Date\n# Answer:\nm.010hdpvg", "# Reasoning Path:\nWilfred Owen -> common.topic.notable_types -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor", "# Reasoning Path:\nWilfred Owen -> people.person.education -> m.0n18h9_ -> freebase.valuenotation.has_value -> End Date\n# Answer:\nm.0n18h9_", "# Reasoning Path:\nWilfred Owen -> people.person.education -> m.010hdpvg -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nm.010hdpvg", "# Reasoning Path:\nWilfred Owen -> common.topic.notable_types -> Fictional Character -> freebase.type_profile.published -> Published\n# Answer:\nFictional Character", "# Reasoning Path:\nWilfred Owen -> people.person.education -> m.0n18h9_ -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nm.0n18h9_", "# Reasoning Path:\nWilfred Owen -> book.author.works_written -> A mid-century look at resources -> common.topic.notable_types -> Book\n# Answer:\nA mid-century look at resources"], "ground_truth": ["University of Reading", "Wakeman School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1494", "prediction": ["# Reasoning Path:\nKevin Jonas -> people.person.spouse_s -> m.0hyfpcv -> people.marriage.spouse -> Danielle Deleasa\n# Answer:\nm.0hyfpcv", "# Reasoning Path:\nKevin Jonas -> people.person.spouse_s -> m.0hyfpcv -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0hyfpcv", "# Reasoning Path:\nKevin Jonas -> music.artist.contribution -> m.0cqz7_2 -> music.recording_contribution.album -> JONAS\n# Answer:\nm.0cqz7_2", "# Reasoning Path:\nKevin Jonas -> people.person.spouse_s -> m.0hyfpcv -> people.marriage.location_of_ceremony -> Oheka Castle\n# Answer:\nm.0hyfpcv", "# Reasoning Path:\nKevin Jonas -> tv.tv_actor.starring_roles -> m.09j06y6 -> tv.regular_tv_appearance.series -> Jonas L.A.\n# Answer:\nm.09j06y6", "# Reasoning Path:\nKevin Jonas -> music.artist.contribution -> m.0cqz7_j -> music.recording_contribution.album -> Jonas L.A.\n# Answer:\nm.0cqz7_j", "# Reasoning Path:\nKevin Jonas -> tv.tv_actor.starring_roles -> m.09j06y6 -> tv.regular_tv_appearance.character -> Kevin Lucas\n# Answer:\nm.09j06y6", "# Reasoning Path:\nKevin Jonas -> tv.tv_actor.starring_roles -> m.09j10zv -> tv.regular_tv_appearance.series -> Jonas Brothers: Living the Dream\n# Answer:\nm.09j10zv"], "ground_truth": ["Danielle Deleasa"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1495", "prediction": ["# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> Hartsfield\u2013Jackson Atlanta International Airport -> location.location.containedby -> Georgia\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> Hartsfield\u2013Jackson Atlanta International Airport -> aviation.airport.number_of_passengers -> m.02k9dqr\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> Hartsfield\u2013Jackson Atlanta International Airport -> location.location.containedby -> United States of America\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> Hartsfield\u2013Jackson Atlanta International Airport -> aviation.airport.hub_for -> Air Atlanta\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.focus_cities -> Akron\u2013Canton Airport -> aviation.airport.hub_for -> Castle Aviation\n# Answer:\nAkron\u2013Canton Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.focus_cities -> Fort Lauderdale\u2013Hollywood International Airport -> aviation.airport.serves -> Fort Lauderdale\n# Answer:\nFort Lauderdale\u2013Hollywood International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> Hartsfield\u2013Jackson Atlanta International Airport -> aviation.airport.hub_for -> AirTran JetConnect\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> Baltimore\u2013Washington International Airport -> aviation.airport.hub_for -> AirTran JetConnect\n# Answer:\nBaltimore\u2013Washington International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> Baltimore\u2013Washington International Airport -> aviation.airport.serves -> Washington, D.C.\n# Answer:\nBaltimore\u2013Washington International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.airports_served -> m.03hbkzh -> aviation.airline_airport_presence.airport -> Raleigh\u2013Durham International Airport\n# Answer:\nm.03hbkzh"], "ground_truth": ["Philadelphia International Airport", "LaGuardia Airport", "Denver International Airport", "Raleigh\u2013Durham International Airport", "Logan International Airport", "Baltimore\u2013Washington International Airport"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.1818181818181818, "ans_precission": 0.2, "ans_recall": 0.16666666666666666, "path_f1": 0.3157894736842105, "path_precision": 0.3, "path_recall": 0.3333333333333333, "path_ans_f1": 0.3157894736842105, "path_ans_precision": 0.3, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1496", "prediction": ["# Reasoning Path:\nParis -> symbols.namesake.named_after -> Parisii -> common.topic.image -> ParisiiCoins\n# Answer:\nParisii", "# Reasoning Path:\nParis -> periodicals.newspaper_circulation_area.newspapers -> Le Moniteur Universel -> book.periodical.final_issue_date -> m.0h_jjhd\n# Answer:\nLe Moniteur Universel", "# Reasoning Path:\nParis -> symbols.namesake.named_after -> Parisii -> common.topic.article -> m.0283_dp\n# Answer:\nParisii", "# Reasoning Path:\nParis -> symbols.name_source.namesakes -> 21 Lutetia -> symbols.namesake.named_after -> Lutetia\n# Answer:\n21 Lutetia", "# Reasoning Path:\nParis -> symbols.namesake.named_after -> Parisii -> common.topic.image -> The Vellavi sited south of the Arverni\u2014 whose name is embodied in Auvergne\n# Answer:\nParisii", "# Reasoning Path:\nParis -> periodicals.newspaper_circulation_area.newspapers -> Le Moniteur Universel -> book.periodical.first_issue_date -> m.0n1_7rl\n# Answer:\nLe Moniteur Universel", "# Reasoning Path:\nParis -> periodicals.newspaper_circulation_area.newspapers -> Le Moniteur Universel -> common.topic.notable_types -> Newspaper\n# Answer:\nLe Moniteur Universel", "# Reasoning Path:\nParis -> periodicals.newspaper_circulation_area.newspapers -> Al-Hayat -> book.newspaper.circulation_areas -> London\n# Answer:\nAl-Hayat", "# Reasoning Path:\nParis -> periodicals.newspaper_circulation_area.newspapers -> 20 minutes -> book.newspaper.headquarters -> m.0j1xxhb\n# Answer:\n20 minutes", "# Reasoning Path:\nParis -> periodicals.newspaper_circulation_area.newspapers -> Al-Hayat -> base.meedan.arabic_language_media_source.city -> London\n# Answer:\nAl-Hayat"], "ground_truth": ["Parisii"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1499", "prediction": ["# Reasoning Path:\nSandra Bullock -> base.popstra.celebrity.dated -> m.065pshj -> base.popstra.dated.participant -> Ryan Gosling\n# Answer:\nRyan Gosling", "# Reasoning Path:\nSandra Bullock -> base.popstra.celebrity.dated -> m.065p_f4 -> base.popstra.dated.participant -> Michael Mailer\n# Answer:\nm.065p_f4", "# Reasoning Path:\nSandra Bullock -> base.popstra.celebrity.dated -> m.065pps3 -> base.popstra.dated.participant -> Matthew McConaughey\n# Answer:\nm.065pps3", "# Reasoning Path:\nSandra Bullock -> base.popstra.celebrity.infidelity_participant -> m.0650s2k -> base.popstra.infidelity.victim -> Janine Lindemulder\n# Answer:\nm.0650s2k", "# Reasoning Path:\nSandra Bullock -> base.popstra.celebrity.infidelity_participant -> m.0650s2k -> base.popstra.infidelity.perpetrator -> Jesse James\n# Answer:\nm.0650s2k", "# Reasoning Path:\nSandra Bullock -> award.award_nominee.award_nominations -> m.010wrd_8 -> award.award_nomination.nominated_for -> Gravity\n# Answer:\nm.010wrd_8", "# Reasoning Path:\nSandra Bullock -> award.award_nominee.award_nominations -> m.010b2qz4 -> award.award_nomination.ceremony -> 3rd AACTA International Awards\n# Answer:\nm.010b2qz4", "# Reasoning Path:\nSandra Bullock -> award.award_nominee.award_nominations -> m.08hhgmc -> award.award_nomination.award -> Razzie Award for Worst Actress\n# Answer:\nm.08hhgmc", "# Reasoning Path:\nSandra Bullock -> award.award_nominee.award_nominations -> m.010b2qz4 -> freebase.valuenotation.is_reviewed -> Award\n# Answer:\nm.010b2qz4", "# Reasoning Path:\nSandra Bullock -> award.award_nominee.award_nominations -> m.010wrd_8 -> award.award_nomination.award -> Teen Choice Award for Choice Movie Actress\n# Answer:\nm.010wrd_8"], "ground_truth": ["Jesse James"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-150", "prediction": ["# Reasoning Path:\nCuba -> location.country.languages_spoken -> Haitian Creole -> language.human_language.main_country -> Haiti\n# Answer:\nHaitian Creole", "# Reasoning Path:\nCuba -> location.country.languages_spoken -> Haitian Creole -> common.topic.notable_types -> Human Language\n# Answer:\nHaitian Creole", "# Reasoning Path:\nCuba -> location.country.languages_spoken -> Haitian Creole -> language.human_language.countries_spoken_in -> Haiti\n# Answer:\nHaitian Creole", "# Reasoning Path:\nCuba -> location.country.languages_spoken -> Haitian Creole -> language.human_language.main_country -> Puerto Rico\n# Answer:\nHaitian Creole", "# Reasoning Path:\nCuba -> location.country.languages_spoken -> Lucumi Language -> base.rosetta.languoid.parent -> Edekiri Group\n# Answer:\nLucumi Language", "# Reasoning Path:\nCuba -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc38hmp\n# Answer:\ng.1hhc38hmp", "# Reasoning Path:\nCuba -> location.country.languages_spoken -> Lucumi Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nLucumi Language", "# Reasoning Path:\nCuba -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Argentina\n# Answer:\nSpanish Language", "# Reasoning Path:\nCuba -> location.country.languages_spoken -> Spanish Language -> language.human_language.region -> Europe\n# Answer:\nSpanish Language", "# Reasoning Path:\nCuba -> location.country.languages_spoken -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language"], "ground_truth": ["Haitian Creole", "Lucumi Language", "Spanish Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.631578947368421, "path_precision": 0.6, "path_recall": 0.6666666666666666, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1500", "prediction": ["# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Jefferson County -> location.location.containedby -> Washington\n# Answer:\nJefferson County", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Washington -> location.location.containedby -> Contiguous United States\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> base.usnationalparks.us_national_park.state -> Washington -> location.location.containedby -> Contiguous United States\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> common.topic.notable_for -> g.1257hvh8r\n# Answer:\ng.1257hvh8r", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Jefferson County -> location.statistical_region.population -> g.11b66j25ww\n# Answer:\nJefferson County", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Washington -> location.location.containedby -> Northwestern United States\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> base.usnationalparks.us_national_park.state -> Washington -> location.location.containedby -> Northwestern United States\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Jefferson County -> location.location.adjoin_s -> m.03jq636\n# Answer:\nJefferson County", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Washington -> location.location.containedby -> Pacific Northwest\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> base.usnationalparks.us_national_park.state -> Washington -> location.location.containedby -> Pacific Northwest\n# Answer:\nWashington"], "ground_truth": ["Washington"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1501", "prediction": ["# Reasoning Path:\nUnited Nations -> organization.organization.headquarters -> m.04300z7 -> location.mailing_address.citytown -> New York City\n# Answer:\nm.04300z7", "# Reasoning Path:\nUnited Nations -> organization.organization.headquarters -> m.04300z7 -> common.topic.notable_for -> g.1254yl5sx\n# Answer:\nm.04300z7", "# Reasoning Path:\nUnited Nations -> book.book_subject.works -> Report on 2011 International Year Part 7: Photography of The African Continuum -> book.written_work.subjects -> Africa\n# Answer:\nReport on 2011 International Year Part 7: Photography of The African Continuum", "# Reasoning Path:\nUnited Nations -> organization.organization.headquarters -> m.04300z7 -> freebase.valuenotation.has_no_value -> Street Address 2\n# Answer:\nm.04300z7", "# Reasoning Path:\nUnited Nations -> book.book_subject.works -> Report on 2011 International Year Part 7: Photography of The African Continuum -> book.written_work.subjects -> Art\n# Answer:\nReport on 2011 International Year Part 7: Photography of The African Continuum", "# Reasoning Path:\nUnited Nations -> book.book_subject.works -> Report on 2011 International Year Part 7: Photography of The African Continuum -> base.reviews.review.mentions -> New York City\n# Answer:\nReport on 2011 International Year Part 7: Photography of The African Continuum", "# Reasoning Path:\nUnited Nations -> organization.organization.headquarters -> m.04300z7 -> freebase.valuenotation.has_no_value -> Street Address 3\n# Answer:\nm.04300z7", "# Reasoning Path:\nUnited Nations -> book.book_subject.works -> Black History Month Enhanced by International Year for People of African Descent -> book.written_work.part_of_series -> Report on 2011 International Year by Aberjhani\n# Answer:\nBlack History Month Enhanced by International Year for People of African Descent", "# Reasoning Path:\nUnited Nations -> organization.organization.headquarters -> m.04300z7 -> freebase.valuenotation.has_no_value -> Street Address 4\n# Answer:\nm.04300z7", "# Reasoning Path:\nUnited Nations -> book.book_subject.works -> Report on 2011 International Year Part 7: Photography of The African Continuum -> book.written_work.subjects -> 2011 International Year for People of African Descent\n# Answer:\nReport on 2011 International Year Part 7: Photography of The African Continuum"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1502", "prediction": ["# Reasoning Path:\nUniversity Yale -> location.location.containedby -> New Haven -> location.location.containedby -> Connecticut\n# Answer:\nNew Haven", "# Reasoning Path:\nUniversity Yale -> location.location.containedby -> New Haven -> location.location.containedby -> United States of America\n# Answer:\nNew Haven", "# Reasoning Path:\nUniversity Yale -> location.location.containedby -> Connecticut -> location.location.containedby -> United States of America\n# Answer:\nConnecticut", "# Reasoning Path:\nUniversity Yale -> location.location.containedby -> New Haven -> location.location.containedby -> Area code 203\n# Answer:\nNew Haven", "# Reasoning Path:\nUniversity Yale -> location.location.containedby -> New Haven -> base.biblioness.bibs_location.state -> Connecticut\n# Answer:\nNew Haven", "# Reasoning Path:\nUniversity Yale -> location.location.containedby -> Connecticut -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nConnecticut", "# Reasoning Path:\nUniversity Yale -> organization.organization.headquarters -> m.04dsstq -> location.mailing_address.citytown -> New Haven\n# Answer:\nm.04dsstq", "# Reasoning Path:\nUniversity Yale -> location.location.containedby -> Connecticut -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nConnecticut", "# Reasoning Path:\nUniversity Yale -> organization.organization.headquarters -> m.04dsstq -> location.mailing_address.country -> United States of America\n# Answer:\nm.04dsstq", "# Reasoning Path:\nUniversity Yale -> organization.organization.headquarters -> m.09s01fm -> location.mailing_address.citytown -> New Haven\n# Answer:\nm.09s01fm"], "ground_truth": ["New Haven"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1503", "prediction": ["# Reasoning Path:\nThomas Paine -> people.deceased_person.place_of_death -> New York City -> common.topic.subject_of -> The Approaching 100th Anniversary of the Harlem Renaissance (part 2)\n# Answer:\nNew York City", "# Reasoning Path:\nThomas Paine -> people.deceased_person.place_of_death -> New York City -> location.administrative_division.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nThomas Paine -> base.kwebbase.kwtopic.has_sentences -> After his death in New York City in 1809, only six mourners attended his funeral. -> base.kwebbase.kwsentence.dates -> m.0c12k2g\n# Answer:\nAfter his death in New York City in 1809, only six mourners attended his funeral.", "# Reasoning Path:\nThomas Paine -> base.kwebbase.kwtopic.has_sentences -> After his death in New York City in 1809, only six mourners attended his funeral. -> base.kwebbase.kwsentence.previous_sentence -> In 1806, he was refused the right to vote because he was not an American citizen.\n# Answer:\nAfter his death in New York City in 1809, only six mourners attended his funeral.", "# Reasoning Path:\nThomas Paine -> base.kwebbase.kwtopic.has_sentences -> After working as corset-maker for a number of years, Paine became an excise officer. -> base.kwebbase.kwsentence.next_sentence -> The 1760s were a decade of rising prices, falling wages, and sporadic food riots.\n# Answer:\nAfter working as corset-maker for a number of years, Paine became an excise officer.", "# Reasoning Path:\nThomas Paine -> base.kwebbase.kwtopic.has_sentences -> As well as analyzing the reasons for discontent in European society, it outlined a plan for a rudimentary welfare state, financed by progressive taxation on property. -> base.kwebbase.kwsentence.next_sentence -> The tract became a central text of the British radical tradition.\n# Answer:\nAs well as analyzing the reasons for discontent in European society, it outlined a plan for a rudimentary welfare state, financed by progressive taxation on property.", "# Reasoning Path:\nThomas Paine -> people.person.quotations -> A bad cause will never be supported by bad means and bad men. -> media_common.quotation.subjects -> Causes\n# Answer:\nA bad cause will never be supported by bad means and bad men.", "# Reasoning Path:\nThomas Paine -> base.kwebbase.kwtopic.has_sentences -> As well as analyzing the reasons for discontent in European society, it outlined a plan for a rudimentary welfare state, financed by progressive taxation on property. -> base.kwebbase.kwsentence.previous_sentence -> At least eight editions were published in one year and it was quickly reprinted in the U.S.  When Burke replied, Paine wrote \\\"The Rights of Man, Part the Second,\\\" published in 1792.\n# Answer:\nAs well as analyzing the reasons for discontent in European society, it outlined a plan for a rudimentary welfare state, financed by progressive taxation on property.", "# Reasoning Path:\nThomas Paine -> people.person.quotations -> A long habit of not thinking a thing wrong, gives it a superficial appearance of being right, and raises at first a formidable outcry in defense of custom. But the tumult soon subsides. Time makes more converts than reason. -> common.topic.notable_for -> g.125f13wcy\n# Answer:\nA long habit of not thinking a thing wrong, gives it a superficial appearance of being right, and raises at first a formidable outcry in defense of custom. But the tumult soon subsides. Time makes more converts than reason.", "# Reasoning Path:\nThomas Paine -> people.person.quotations -> A long habit of not thinking a thing wrong, gives it a superficial appearance of being right, and raises at first a formidable outcry in defense of custom. But the tumult soon subsides. Time makes more converts than reason. -> common.topic.notable_types -> Quotation\n# Answer:\nA long habit of not thinking a thing wrong, gives it a superficial appearance of being right, and raises at first a formidable outcry in defense of custom. But the tumult soon subsides. Time makes more converts than reason."], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1504", "prediction": ["# Reasoning Path:\nBarack Obama, Sr. -> people.person.place_of_birth -> Nyang'oma Kogelo -> location.location.containedby -> Nyanza Province\n# Answer:\nNyang'oma Kogelo", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.place_of_birth -> Nyang'oma Kogelo -> location.location.geolocation -> m.04nvlsq\n# Answer:\nNyang'oma Kogelo", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.place_of_birth -> Nyang'oma Kogelo -> common.topic.article -> m.04f2ymv\n# Answer:\nNyang'oma Kogelo", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.place_of_birth -> Nyang'oma Kogelo -> location.location.containedby -> Siaya County\n# Answer:\nNyang'oma Kogelo", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.spouse_s -> m.040fvqp -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.040fvqp", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.religion -> Atheism -> base.activism.activism_issue.activists -> Karl Marx\n# Answer:\nAtheism", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.religion -> Atheism -> book.book_subject.works -> Atheism: The Case Against God\n# Answer:\nAtheism", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.spouse_s -> m.040fvqp -> people.marriage.spouse -> Ann Dunham\n# Answer:\nm.040fvqp", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.spouse_s -> m.0j4l4y8 -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0j4l4y8", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.spouse_s -> m.040fvqp -> people.marriage.location_of_ceremony -> Maui\n# Answer:\nm.040fvqp"], "ground_truth": ["Nyang'oma Kogelo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1506", "prediction": ["# Reasoning Path:\nNew England Patriots -> sports.sports_team.arena_stadium -> Gillette Stadium -> soccer.football_pitch.matches -> 2014 Pre-World Cup Friendly Mexico vs Portugal football match\n# Answer:\nGillette Stadium", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.arena_stadium -> Gillette Stadium -> common.topic.image -> Gillette stadium\n# Answer:\nGillette Stadium", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.arena_stadium -> Gillette Stadium -> location.location.events -> 2003 FIFA Women's World Cup\n# Answer:\nGillette Stadium", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.fight_song -> I'm Shipping Up to Boston -> music.composition.composer -> Woody Guthrie\n# Answer:\nI'm Shipping Up to Boston", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.fight_song -> I'm Shipping Up to Boston -> music.composition.subjects -> Boston\n# Answer:\nI'm Shipping Up to Boston", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.arena_stadium -> Gillette Stadium -> common.topic.image -> Gillette Stadium2\n# Answer:\nGillette Stadium", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.colors -> Nautical Blue -> common.topic.notable_for -> g.125dysf2k\n# Answer:\nNautical Blue", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.arena_stadium -> Gillette Stadium -> location.location.events -> 2004 AFC Championship Game\n# Answer:\nGillette Stadium", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.fight_song -> I'm Shipping Up to Boston -> music.composition.composer -> Dropkick Murphys\n# Answer:\nI'm Shipping Up to Boston", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.colors -> Nautical Blue -> common.topic.notable_types -> Color\n# Answer:\nNautical Blue"], "ground_truth": ["Gillette Stadium"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1507", "prediction": ["# Reasoning Path:\nKate Chopin -> people.person.profession -> Novelist -> common.topic.notable_types -> Profession\n# Answer:\nNovelist", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Novelist -> base.descriptive_names.names.descriptive_name -> m.01064zd_\n# Answer:\nNovelist", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Businessperson -> base.descriptive_names.names.descriptive_name -> m.0100z128\n# Answer:\nBusinessperson", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Writer -> common.topic.notable_types -> Profession\n# Answer:\nWriter", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Novelist -> fictional_universe.character_occupation.characters_with_this_occupation -> Bruce Butler\n# Answer:\nNovelist", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Writer -> people.profession.specializations -> Novelist\n# Answer:\nWriter", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Novelist -> base.descriptive_names.names.descriptive_name -> m.01064zf_\n# Answer:\nNovelist", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Businessperson -> base.descriptive_names.names.descriptive_name -> m.0105_bmw\n# Answer:\nBusinessperson", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Businessperson -> people.profession.specializations -> Actuary\n# Answer:\nBusinessperson", "# Reasoning Path:\nKate Chopin -> book.author.works_written -> A Pair of Silk Stockings -> book.written_work.original_language -> English Language\n# Answer:\nA Pair of Silk Stockings"], "ground_truth": ["Novelist", "Author", "Businessperson", "Writer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.8181818181818182, "path_precision": 0.9, "path_recall": 0.75, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1508", "prediction": ["# Reasoning Path:\nBill Clinton -> people.person.education -> m.0h1khf8 -> education.education.institution -> Edmund A. Walsh School of Foreign Service\n# Answer:\nm.0h1khf8", "# Reasoning Path:\nBill Clinton -> people.person.education -> m.02hvncd -> education.education.institution -> University College, Oxford\n# Answer:\nm.02hvncd", "# Reasoning Path:\nBill Clinton -> people.person.education -> m.0h1khf8 -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nm.0h1khf8", "# Reasoning Path:\nBill Clinton -> people.person.education -> m.0125cddf -> education.education.institution -> University of Oxford\n# Answer:\nm.0125cddf", "# Reasoning Path:\nBill Clinton -> people.person.education -> m.02hvncd -> freebase.valuenotation.has_value -> End Date\n# Answer:\nm.02hvncd", "# Reasoning Path:\nBill Clinton -> film.person_or_entity_appearing_in_film.films -> m.010l29t4 -> film.personal_film_appearance.film -> All About Ann: Governor Richards of the Lone Star State\n# Answer:\nm.010l29t4", "# Reasoning Path:\nBill Clinton -> award.award_nominee.award_nominations -> m.0k08gry -> award.award_nomination.ceremony -> 47th Annual Grammy Awards\n# Answer:\nm.0k08gry", "# Reasoning Path:\nBill Clinton -> people.person.education -> m.02hvncd -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nm.02hvncd", "# Reasoning Path:\nBill Clinton -> award.award_nominee.award_nominations -> m.0k08gry -> award.award_nomination.award -> Grammy Award for Best Spoken Word Album\n# Answer:\nm.0k08gry", "# Reasoning Path:\nBill Clinton -> award.award_nominee.award_nominations -> m.0xnp1nm -> award.award_nomination.nominated_for -> Back To Work: Why We Need Smart Government For A Strong Economy\n# Answer:\nm.0xnp1nm"], "ground_truth": ["Georgetown University", "Yale Law School", "University of Oxford", "University Yale", "University College, Oxford", "Edmund A. Walsh School of Foreign Service"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.37499999999999994, "path_ans_precision": 0.3, "path_ans_recall": 0.5}
{"id": "WebQTest-1510", "prediction": ["# Reasoning Path:\nStaffordshire Bull Terrier -> biology.animal_breed.breed_of -> Dog -> base.popstra.product.sold_to -> m.063p_sk\n# Answer:\nDog", "# Reasoning Path:\nStaffordshire Bull Terrier -> biology.animal_breed.place_of_origin -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nStaffordshire Bull Terrier -> biology.animal_breed.breed_of -> Dog -> base.petsupplyandaccessory.pet_apparel_user.pet_apparel_company -> The Woofer\n# Answer:\nDog", "# Reasoning Path:\nStaffordshire Bull Terrier -> biology.animal_breed.breed_of -> Dog -> base.pethealth.pet_with_medical_condition.diseases_and_other_conditions_of_this_pet -> Allergic Dermatitis in animals\n# Answer:\nDog", "# Reasoning Path:\nStaffordshire Bull Terrier -> biology.animal_breed.place_of_origin -> England -> location.location.containedby -> Great Britain\n# Answer:\nEngland", "# Reasoning Path:\nStaffordshire Bull Terrier -> common.topic.notable_types -> Animal breed -> freebase.type_profile.strict_included_types -> Animal\n# Answer:\nAnimal breed", "# Reasoning Path:\nStaffordshire Bull Terrier -> biology.animal_breed.place_of_origin -> England -> location.location.events -> First Barons' War\n# Answer:\nEngland", "# Reasoning Path:\nStaffordshire Bull Terrier -> biology.animal_breed.breed_of -> Dog -> base.pethealth.pet_with_medical_condition.diseases_and_other_conditions_of_this_pet -> Anal sac disease\n# Answer:\nDog", "# Reasoning Path:\nStaffordshire Bull Terrier -> biology.animal_breed.place_of_origin -> England -> location.location.containedby -> United Kingdom\n# Answer:\nEngland", "# Reasoning Path:\nStaffordshire Bull Terrier -> common.topic.notable_types -> Animal breed -> type.type.expected_by -> Breed\n# Answer:\nAnimal breed"], "ground_truth": ["Dog"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1513", "prediction": ["# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Sacramento Zoo -> common.topic.image -> A uniquely colored lion statue greets visitors to teh Sacramento Zoo\n# Answer:\nSacramento Zoo", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Sacramento Zoo -> common.topic.webpage -> m.03lh9qj\n# Answer:\nSacramento Zoo", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Sacramento Zoo -> common.topic.notable_types -> Zoo\n# Answer:\nSacramento Zoo", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Sacramento Zoo -> common.topic.image -> LWBSacramentoZoo\n# Answer:\nSacramento Zoo", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> B Street Theatre -> architecture.building.building_function -> Theatre\n# Answer:\nB Street Theatre", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> California Automobile Museum -> common.topic.article -> m.065yn2j\n# Answer:\nCalifornia Automobile Museum", "# Reasoning Path:\nSacramento -> location.location.events -> 1991 Sacramento hostage crisis -> common.topic.notable_types -> Event\n# Answer:\n1991 Sacramento hostage crisis", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> B Street Theatre -> location.location.geolocation -> m.0131pvp5\n# Answer:\nB Street Theatre", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> B Street Theatre -> freebase.valuenotation.has_value -> Architect\n# Answer:\nB Street Theatre", "# Reasoning Path:\nSacramento -> location.location.events -> 1992 Sacramento International Gay and Lesbian Film Festival -> time.event.instance_of_recurring_event -> Sacramento International Gay and Lesbian Film Festival\n# Answer:\n1992 Sacramento International Gay and Lesbian Film Festival"], "ground_truth": ["California State Indian Museum", "Folsom Lake", "California State Railroad Museum", "Raging Waters Sacramento", "California State Capitol Museum", "Crocker Art Museum", "B Street Theatre", "Sutter's Fort", "Sacramento History Museum", "Sacramento Zoo", "California Automobile Museum"], "ans_acc": 0.2727272727272727, "ans_hit": 1, "ans_f1": 0.4067796610169491, "ans_precission": 0.8, "ans_recall": 0.2727272727272727, "path_f1": 0.4067796610169491, "path_precision": 0.8, "path_recall": 0.2727272727272727, "path_ans_f1": 0.4067796610169491, "path_ans_precision": 0.8, "path_ans_recall": 0.2727272727272727}
{"id": "WebQTest-1515", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.parents -> Phyllis Piper -> common.topic.notable_for -> g.125c8dhdm\n# Answer:\nPhyllis Piper", "# Reasoning Path:\nMichael J. Fox -> people.person.parents -> William Fox -> freebase.valuenotation.has_value -> Parents\n# Answer:\nWilliam Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.parents -> Phyllis Piper -> people.person.employment_history -> m.0jb95tm\n# Answer:\nPhyllis Piper", "# Reasoning Path:\nMichael J. Fox -> people.person.parents -> Phyllis Piper -> people.person.children -> Jacki Fox\n# Answer:\nPhyllis Piper", "# Reasoning Path:\nMichael J. Fox -> people.person.parents -> William Fox -> people.person.nationality -> Canada\n# Answer:\nWilliam Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.parents -> William Fox -> people.person.spouse_s -> m.0s940f6\n# Answer:\nWilliam Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.parents -> Phyllis Piper -> people.person.children -> Karen Fox\n# Answer:\nPhyllis Piper", "# Reasoning Path:\nMichael J. Fox -> people.person.parents -> William Fox -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nWilliam Fox", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvttg -> award.award_nomination.nominated_for -> Spin City\n# Answer:\nm.07nvttg", "# Reasoning Path:\nMichael J. Fox -> people.person.parents -> Phyllis Piper -> people.person.children -> Kelli Fox\n# Answer:\nPhyllis Piper"], "ground_truth": ["William Fox", "Phyllis Piper"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1516", "prediction": ["# Reasoning Path:\nSanjay Gupta -> people.person.education -> m.02wnv1q -> education.education.institution -> University of Michigan\n# Answer:\nm.02wnv1q", "# Reasoning Path:\nSanjay Gupta -> education.honorary_degree_recipient.honorary_degrees -> m.0xn83q1 -> education.honorary_degree.institution -> University of Michigan\n# Answer:\nm.0xn83q1", "# Reasoning Path:\nSanjay Gupta -> people.person.education -> m.02wnv1q -> education.education.degree -> Bachelor's degree\n# Answer:\nm.02wnv1q", "# Reasoning Path:\nSanjay Gupta -> people.person.education -> m.05b8sml -> education.education.institution -> University of Michigan Health System\n# Answer:\nm.05b8sml", "# Reasoning Path:\nSanjay Gupta -> people.person.education -> m.0n1j65x -> education.education.institution -> Novi High School\n# Answer:\nm.0n1j65x", "# Reasoning Path:\nSanjay Gupta -> people.person.education -> m.05b8sml -> education.education.degree -> Doctor of Medicine\n# Answer:\nm.05b8sml", "# Reasoning Path:\nSanjay Gupta -> award.award_nominee.award_nominations -> m.010g1lf8 -> award.award_nomination.award_nominee -> Ashley Bateman\n# Answer:\nm.010g1lf8", "# Reasoning Path:\nSanjay Gupta -> award.award_nominee.award_nominations -> m.010g1lf8 -> award.award_nomination.nominated_for -> Sanjay Gupta MD\n# Answer:\nm.010g1lf8", "# Reasoning Path:\nSanjay Gupta -> award.award_nominee.award_nominations -> m.010g1lf8 -> award.award_nomination.ceremony -> 41st Daytime Creative Arts Emmy Awards\n# Answer:\nm.010g1lf8", "# Reasoning Path:\nSanjay Gupta -> award.award_nominee.award_nominations -> m.010g1lf8 -> award.award_nomination.award_nominee -> Ben Tinker\n# Answer:\nm.010g1lf8"], "ground_truth": ["University of Michigan", "University of Michigan Health System", "Novi High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1518", "prediction": ["# Reasoning Path:\nNorth American Free Trade Agreement -> organization.membership_organization.members -> m.03w60c3 -> organization.organization_membership.member -> Mexico\n# Answer:\nm.03w60c3", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.membership_organization.members -> m.03w60c8 -> organization.organization_membership.member -> Canada\n# Answer:\nm.03w60c8", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.membership_organization.members -> m.03w60cf -> organization.organization_membership.member -> United States of America\n# Answer:\nm.03w60cf", "# Reasoning Path:\nNorth American Free Trade Agreement -> freebase.valuenotation.has_value -> Place founded -> rdf-schema#domain -> Organization\n# Answer:\nPlace founded", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.organization.founders -> George H. W. Bush -> people.person.profession -> Businessperson\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nNorth American Free Trade Agreement -> freebase.valuenotation.has_value -> Place founded -> type.property.expected_type -> Location\n# Answer:\nPlace founded", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.organization.founders -> George H. W. Bush -> people.person.profession -> Entrepreneur\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.organization.founders -> George H. W. Bush -> government.political_appointer.appointees -> m.048zk8w\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nNorth American Free Trade Agreement -> freebase.valuenotation.has_value -> Place founded -> type.property.schema -> Organization\n# Answer:\nPlace founded", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.organization.founders -> George H. W. Bush -> people.person.profession -> Military aviator\n# Answer:\nGeorge H. W. Bush"], "ground_truth": ["Canada", "Mexico", "United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1519", "prediction": ["# Reasoning Path:\nCayman Islands -> location.location.containedby -> Americas -> location.location.containedby -> Western Hemisphere\n# Answer:\nAmericas", "# Reasoning Path:\nCayman Islands -> location.location.containedby -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nCayman Islands -> location.location.containedby -> Americas -> location.location.contains -> Haiti\n# Answer:\nAmericas", "# Reasoning Path:\nCayman Islands -> location.location.containedby -> Americas -> location.location.contains -> Honduras\n# Answer:\nAmericas", "# Reasoning Path:\nCayman Islands -> location.location.containedby -> Greater Antilles -> location.location.containedby -> Antilles\n# Answer:\nGreater Antilles", "# Reasoning Path:\nCayman Islands -> location.location.containedby -> North America -> location.location.containedby -> Western Hemisphere\n# Answer:\nNorth America", "# Reasoning Path:\nCayman Islands -> location.statistical_region.internet_users_percent_population -> g.11b60v9ztp\n# Answer:\ng.11b60v9ztp", "# Reasoning Path:\nCayman Islands -> location.location.containedby -> Americas -> location.location.time_zones -> Alaska Time Zone\n# Answer:\nAmericas", "# Reasoning Path:\nCayman Islands -> location.location.containedby -> North America -> base.locations.continents.countries_within -> Haiti\n# Answer:\nNorth America", "# Reasoning Path:\nCayman Islands -> location.location.containedby -> Americas -> location.location.contains -> Anguilla\n# Answer:\nAmericas"], "ground_truth": ["North America", "Americas"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1520", "prediction": ["# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Brazilian Portuguese -> language.language_dialect.language -> Portuguese Language\n# Answer:\nBrazilian Portuguese", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\ng.12cp_jcjy", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\ng.11b71vdgdz", "# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Portuguese Language -> language.human_language.dialects -> Brazilian Portuguese\n# Answer:\nPortuguese Language", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\ng.1hhc390hy", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc39sjp\n# Answer:\ng.1hhc39sjp", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fm01\n# Answer:\ng.12tb6fm01", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc385hl\n# Answer:\ng.1hhc385hl"], "ground_truth": ["Brazilian Portuguese", "Portuguese Language", "Italian Language"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.36363636363636365, "ans_precission": 0.25, "ans_recall": 0.6666666666666666, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 0.36363636363636365, "path_ans_precision": 0.25, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1522", "prediction": ["# Reasoning Path:\nKroy Biermann -> sports.pro_athlete.teams -> m.07djz52 -> sports.sports_team_roster.team -> Atlanta Falcons\n# Answer:\nm.07djz52", "# Reasoning Path:\nKroy Biermann -> sports.pro_athlete.teams -> m.0hqf_3w -> sports.sports_team_roster.team -> Montana Grizzlies football\n# Answer:\nm.0hqf_3w", "# Reasoning Path:\nKroy Biermann -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#range -> Person\n# Answer:\nChildren", "# Reasoning Path:\nKroy Biermann -> sports.pro_athlete.teams -> m.07djz52 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.07djz52", "# Reasoning Path:\nKroy Biermann -> sports.pro_athlete.teams -> m.0hqf_3w -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hqf_3w", "# Reasoning Path:\nKroy Biermann -> freebase.valuenotation.is_reviewed -> Children -> type.property.schema -> Person\n# Answer:\nChildren", "# Reasoning Path:\nKroy Biermann -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#domain -> Person\n# Answer:\nChildren", "# Reasoning Path:\nKroy Biermann -> american_football.football_player.games -> m.07nvpcg -> american_football.player_game_statistics.team -> Atlanta Falcons\n# Answer:\nm.07nvpcg", "# Reasoning Path:\nKroy Biermann -> american_football.football_player.games -> m.07tds9c -> american_football.player_game_statistics.team -> Atlanta Falcons\n# Answer:\nm.07tds9c", "# Reasoning Path:\nKroy Biermann -> american_football.football_player.games -> m.07nvpcg -> american_football.player_game_statistics.season -> 2008 NFL season\n# Answer:\nm.07nvpcg"], "ground_truth": ["Atlanta Falcons"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.19354838709677416, "path_precision": 0.3, "path_recall": 0.14285714285714285, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1523", "prediction": ["# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> Super Bowl X -> time.event.locations -> Miami Orange Bowl\n# Answer:\nSuper Bowl X", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> Super Bowl X -> time.event.instance_of_recurring_event -> Super Bowl\n# Answer:\nSuper Bowl X", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> 1996 AFC Championship Game -> common.topic.article -> m.0hzps_m\n# Answer:\n1996 AFC Championship Game", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> 2006 AFC Championship Game -> sports.sports_championship_event.runner_up -> Denver Broncos\n# Answer:\n2006 AFC Championship Game", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> 1996 AFC Championship Game -> common.topic.notable_for -> g.1q3scnq3r\n# Answer:\n1996 AFC Championship Game", "# Reasoning Path:\nPittsburgh Steelers -> american_football.football_team.current_head_coach -> Mike Tomlin -> american_football.football_coach.coaching_history -> m.05cvsxx\n# Answer:\nMike Tomlin", "# Reasoning Path:\nPittsburgh Steelers -> american_football.football_team.current_head_coach -> Mike Tomlin -> people.person.place_of_birth -> Hampton\n# Answer:\nMike Tomlin", "# Reasoning Path:\nPittsburgh Steelers -> common.topic.webpage -> m.03nbxbp -> common.webpage.category -> Topic Webpage\n# Answer:\nm.03nbxbp", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> 2006 AFC Championship Game -> sports.sports_championship_event.season -> 2005 NFL season\n# Answer:\n2006 AFC Championship Game", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> 1996 AFC Championship Game -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n1996 AFC Championship Game"], "ground_truth": ["Super Bowl XLIII"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1524", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.notable_figures -> Muhammad in Islam -> book.author.works_written -> Sayings of Mohammed\n# Answer:\nMuhammad in Islam", "# Reasoning Path:\nIslam -> religion.religion.founding_figures -> Muhammad in Islam -> book.author.works_written -> Sayings of Mohammed\n# Answer:\nMuhammad in Islam", "# Reasoning Path:\nIslam -> religion.religion.notable_figures -> Muhammad in Islam -> influence.influence_node.influenced -> Abd Allah ibn Abbas\n# Answer:\nMuhammad in Islam", "# Reasoning Path:\nIslam -> religion.religion.notable_figures -> Muhammad in Islam -> people.person.spouse_s -> m.0j4jg5y\n# Answer:\nMuhammad in Islam", "# Reasoning Path:\nIslam -> religion.religion.notable_figures -> Abu Bakr -> people.person.parents -> Salma Umm-ul-Khair\n# Answer:\nAbu Bakr", "# Reasoning Path:\nIslam -> religion.religion.notable_figures -> Muhammad in Islam -> influence.influence_node.influenced -> Abdussalam Puthige\n# Answer:\nMuhammad in Islam", "# Reasoning Path:\nIslam -> religion.religion.founding_figures -> Muhammad in Islam -> influence.influence_node.influenced -> Abd Allah ibn Abbas\n# Answer:\nMuhammad in Islam", "# Reasoning Path:\nIslam -> religion.religion.notable_figures -> Abu Bakr -> people.person.children -> Abdul-Rahman ibn Abi Bakr\n# Answer:\nAbu Bakr", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> common.topic.article -> m.0jfq\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.notable_figures -> Muhammad in Islam -> influence.influence_node.influenced -> Abu Bakr\n# Answer:\nMuhammad in Islam"], "ground_truth": ["Muhammad in Islam"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1525", "prediction": ["# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.06vz4t9 -> military.military_combatant_group.combatants -> Argentina\n# Answer:\nm.06vz4t9", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z973l -> military.military_combatant_group.combatants -> Iraq\n# Answer:\nm.03z973l", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> Australia\n# Answer:\nm.03z96c5", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wphl -> military.casualties.combatant -> United States of America\n# Answer:\nm.043wphl", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> United States of America\n# Answer:\nm.03z96c5", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wphv -> military.casualties.combatant -> United States of America\n# Answer:\nm.043wphv", "# Reasoning Path:\nGulf War -> time.event.includes_event -> 1991 uprisings in Iraq -> military.military_conflict.combatants -> m.04y_t_s\n# Answer:\n1991 uprisings in Iraq", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wphl -> military.casualties.type_of_casualties -> Military - MIA\n# Answer:\nm.043wphl", "# Reasoning Path:\nGulf War -> time.event.includes_event -> 1991 uprisings in Iraq -> time.event.locations -> Iraq\n# Answer:\n1991 uprisings in Iraq", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Battle for Jalibah Airfield -> military.military_conflict.combatants -> m.0c5nrh4\n# Answer:\nBattle for Jalibah Airfield"], "ground_truth": ["United Kingdom", "Saudi Arabia", "United States of America", "Argentina", "France", "Iraq", "Australia"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.16666666666666666, "ans_precission": 0.2, "ans_recall": 0.14285714285714285, "path_f1": 0.14925373134328357, "path_precision": 0.5, "path_recall": 0.08771929824561403, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.8, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-1526", "prediction": ["# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> German Language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nGerman Language", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> German Language -> language.human_language.region -> Europe\n# Answer:\nGerman Language", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> German Language -> language.human_language.countries_spoken_in -> Belgium\n# Answer:\nGerman Language", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Italy\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> Italian Language -> language.human_language.main_country -> Italy\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> German Language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nGerman Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> Italian Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> Italian Language -> language.human_language.main_country -> Italy\n# Answer:\nItalian Language"], "ground_truth": ["German Language", "Italian Language", "Romansh language", "French"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-1527", "prediction": ["# Reasoning Path:\nBarack Obama -> government.politician.party -> m.03gjhww -> government.political_party_tenure.party -> Democratic Party\n# Answer:\nm.03gjhww", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.party -> Democratic Party\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> government.politician.party -> m.03gjhww -> freebase.valuenotation.is_reviewed -> Party\n# Answer:\nm.03gjhww", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.venues -> Pepsi Center\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> government.politician.party -> m.03gjhww -> freebase.valuenotation.is_reviewed -> Politician\n# Answer:\nm.03gjhww", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.speeches -> m.04j00hr\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.subjects -> Mitt Romney presidential campaign, 2012\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.venues -> Sports Authority Field at Mile High\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Writer's Journey to Selma, Alabama -> book.written_work.subjects -> Selma\n# Answer:\nA Writer's Journey to Selma, Alabama", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.speeches -> m.04j00j0\n# Answer:\n2008 Democratic National Convention"], "ground_truth": ["Democratic Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.2, "path_recall": 0.3333333333333333, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1528", "prediction": ["# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.03jq21w -> film.performance.film -> The Butterfly Effect\n# Answer:\nm.03jq21w", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.03jq2p4 -> film.performance.film -> 3:10 to Yuma\n# Answer:\nm.03jq2p4", "# Reasoning Path:\nLogan Lerman -> common.topic.article -> m.04f1sc\n# Answer:\nm.04f1sc", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.04ls5pn -> film.performance.film -> My One and Only\n# Answer:\nm.04ls5pn", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.03jq21w -> film.performance.character -> Evan Treborn\n# Answer:\nm.03jq21w", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.03jq2p4 -> film.performance.character -> William Evans\n# Answer:\nm.03jq2p4", "# Reasoning Path:\nLogan Lerman -> people.person.profession -> Actor -> people.profession.specializations -> Audio Dramatist\n# Answer:\nActor", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.04ls5pn -> film.performance.character -> George Devereaux\n# Answer:\nm.04ls5pn", "# Reasoning Path:\nLogan Lerman -> people.person.profession -> Actor -> people.profession.specializations -> Camgirl\n# Answer:\nActor", "# Reasoning Path:\nLogan Lerman -> people.person.profession -> Actor -> music.special_music_video_performance_type.special_music_video_performances -> m.010wvs8m\n# Answer:\nActor"], "ground_truth": ["The Number 23", "Hoot", "Riding in Cars with Boys", "The Butterfly Effect", "Gamer", "Noah", "3:10 to Yuma", "A Painted House", "Stuck in Love", "The Scribe", "Percy Jackson & the Olympians: The Lightning Thief", "The Only Living Boy in New York", "My One and Only", "Fury", "The Patriot", "The Flannerys", "Meet Bill", "What Women Want", "Percy Jackson: Sea of Monsters", "The Perks of Being a Wallflower", "The Three Musketeers"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.13953488372093023, "path_precision": 0.3, "path_recall": 0.09090909090909091, "path_ans_f1": 0.19354838709677416, "path_ans_precision": 0.3, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-1529", "prediction": ["# Reasoning Path:\nConstantine the Great -> people.deceased_person.place_of_death -> Nicomedia -> common.topic.notable_for -> g.1255y5dbj\n# Answer:\nNicomedia", "# Reasoning Path:\nConstantine the Great -> people.deceased_person.place_of_death -> Nicomedia -> common.topic.notable_types -> City/Town/Village\n# Answer:\nNicomedia", "# Reasoning Path:\nConstantine the Great -> people.deceased_person.place_of_death -> Nicomedia -> location.location.people_born_here -> Arrian\n# Answer:\nNicomedia", "# Reasoning Path:\nConstantine the Great -> people.deceased_person.place_of_death -> Nicomedia -> location.location.people_born_here -> Aaron ben Elijah\n# Answer:\nNicomedia", "# Reasoning Path:\nConstantine the Great -> symbols.name_source.namesakes -> Constantine -> location.location.people_born_here -> Abdelhak Benhamouda\n# Answer:\nConstantine", "# Reasoning Path:\nConstantine the Great -> people.deceased_person.place_of_death -> Nicomedia -> location.location.people_born_here -> Donatus of Arezzo\n# Answer:\nNicomedia", "# Reasoning Path:\nConstantine the Great -> people.person.sibling_s -> m.0k6lgkh -> people.sibling_relationship.sibling -> Flavia Julia Constantia\n# Answer:\nm.0k6lgkh", "# Reasoning Path:\nConstantine the Great -> symbols.name_source.namesakes -> Constantine -> location.location.people_born_here -> Abdelhamid Ben Badis\n# Answer:\nConstantine", "# Reasoning Path:\nConstantine the Great -> symbols.name_source.namesakes -> Constantine -> location.location.events -> Miss Europe 1937\n# Answer:\nConstantine", "# Reasoning Path:\nConstantine the Great -> symbols.name_source.namesakes -> Arch of Constantine -> common.topic.article -> m.02p9gq\n# Answer:\nArch of Constantine"], "ground_truth": ["Nicomedia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-153", "prediction": ["# Reasoning Path:\nMadrid -> base.arthist.helynevek.intezmeny -> Museo Nacional Del Prado -> exhibitions.exhibition_venue.exhibitions_at_this_venue -> m.04sg179\n# Answer:\nMuseo Nacional Del Prado", "# Reasoning Path:\nMadrid -> base.arthist.helynevek.intezmeny -> Museo Nacional Del Prado -> base.schemastaging.organization_extra.contact_webpages -> m.010hbjzz\n# Answer:\nMuseo Nacional Del Prado", "# Reasoning Path:\nMadrid -> base.arthist.helynevek.intezmeny -> Museo Nacional Del Prado -> exhibitions.exhibition_venue.exhibitions_at_this_venue -> m.0c0b719\n# Answer:\nMuseo Nacional Del Prado", "# Reasoning Path:\nMadrid -> base.arthist.helynevek.intezmeny -> Museo Nacional Del Prado -> visual_art.art_owner.artworks_owned -> m.043zgzc\n# Answer:\nMuseo Nacional Del Prado", "# Reasoning Path:\nMadrid -> base.arthist.helynevek.intezmeny -> Thyssen-Bornemisza Museum -> location.location.geolocation -> m.02_sq5m\n# Answer:\nThyssen-Bornemisza Museum", "# Reasoning Path:\nMadrid -> travel.travel_destination.tourist_attractions -> Buen Retiro Park -> common.topic.notable_for -> g.125gs6jwx\n# Answer:\nBuen Retiro Park", "# Reasoning Path:\nMadrid -> location.statistical_region.population -> g.11b7vbxnhf\n# Answer:\ng.11b7vbxnhf", "# Reasoning Path:\nMadrid -> base.arthist.helynevek.intezmeny -> Museo Nacional Del Prado -> exhibitions.exhibition_venue.exhibitions_at_this_venue -> m.0c0b71f\n# Answer:\nMuseo Nacional Del Prado", "# Reasoning Path:\nMadrid -> base.arthist.helynevek.intezmeny -> Collecci\u00f3n Fern\u00e1ndez Araoz -> common.topic.notable_types -> Art owner\n# Answer:\nCollecci\u00f3n Fern\u00e1ndez Araoz", "# Reasoning Path:\nMadrid -> travel.travel_destination.tourist_attractions -> Buen Retiro Park -> location.location.containedby -> Spain\n# Answer:\nBuen Retiro Park"], "ground_truth": ["Almudena Cathedral", "El Escorial", "Madrid Arena", "Thyssen-Bornemisza Museum", "Paseo del Prado", "Parque Warner Madrid", "Plaza de Cibeles", "Madrid Marathon", "Plaza Mayor, Madrid", "Gran V\u00eda", "Festimad", "Temple of Debod", "Museo Nacional Del Prado", "Museo Nacional Centro de Arte Reina Sof\u00eda", "Summercase", "Buen Retiro Park", "Sorolla Museum", "Royal Palace of Madrid", "Museum of L\u00e1zaro Galdiano", "Palace of la Bolsa de Madrid", "Puerta del Sol", "La Vaguada"], "ans_acc": 0.13636363636363635, "ans_hit": 1, "ans_f1": 0.23300970873786406, "ans_precission": 0.8, "ans_recall": 0.13636363636363635, "path_f1": 0.23300970873786406, "path_precision": 0.8, "path_recall": 0.13636363636363635, "path_ans_f1": 0.23300970873786406, "path_ans_precision": 0.8, "path_ans_recall": 0.13636363636363635}
{"id": "WebQTest-1531", "prediction": ["# Reasoning Path:\nMali -> base.locations.countries.continent -> Africa -> base.locations.continents.countries_within -> Algeria\n# Answer:\nAfrica", "# Reasoning Path:\nMali -> base.locations.countries.continent -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica", "# Reasoning Path:\nMali -> base.locations.countries.continent -> Africa -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nAfrica", "# Reasoning Path:\nMali -> base.locations.countries.continent -> Africa -> base.locations.continents.countries_within -> Benin\n# Answer:\nAfrica", "# Reasoning Path:\nMali -> location.location.containedby -> Africa -> base.locations.continents.countries_within -> Algeria\n# Answer:\nAfrica", "# Reasoning Path:\nMali -> location.location.containedby -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica", "# Reasoning Path:\nMali -> location.location.containedby -> West Africa -> location.location.containedby -> Africa\n# Answer:\nWest Africa", "# Reasoning Path:\nMali -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d0tc16\n# Answer:\ng.11b6d0tc16", "# Reasoning Path:\nMali -> base.locations.countries.continent -> Africa -> base.locations.continents.countries_within -> Botswana\n# Answer:\nAfrica", "# Reasoning Path:\nMali -> location.location.containedby -> Africa -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nAfrica"], "ground_truth": ["Africa", "West Africa"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1533", "prediction": ["# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.place_of_birth -> g.120mc0gk\n# Answer:\ng.120mc0gk", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.places_lived -> Tatooine -> common.topic.notable_for -> g.1256pb32r\n# Answer:\nTatooine", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.places_lived -> Tatooine -> fictional_universe.fictional_setting.works_set_here -> Return of the Jedi\n# Answer:\nTatooine", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.places_lived -> Tatooine -> fictional_universe.fictional_setting.works_set_here -> Star Wars\n# Answer:\nTatooine", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.places_lived -> Tatooine -> fictional_universe.fictional_setting.works_set_here -> Star Wars Episode I: The Phantom Menace\n# Answer:\nTatooine", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.places_lived -> Tatooine -> fictional_universe.fictional_setting.characters_that_have_lived_here -> Beru Lars\n# Answer:\nTatooine", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.places_lived -> Tatooine -> fictional_universe.fictional_setting.characters_that_have_lived_here -> C-3PO\n# Answer:\nTatooine", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.places_lived -> Tatooine -> fictional_universe.fictional_setting.characters_that_have_lived_here -> Darth Vader\n# Answer:\nTatooine", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.powers_or_abilities -> Force -> fictional_universe.character_powers.characters_with_this_ability -> Darth Maul\n# Answer:\nForce", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.powers_or_abilities -> Force -> common.topic.notable_for -> g.125f342hs\n# Answer:\nForce"], "ground_truth": ["Polis Massa"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1534", "prediction": ["# Reasoning Path:\nOakland Raiders -> american_football.football_team.current_head_coach -> Tony Sparano -> sports.pro_athlete.sports_played_professionally -> m.0d6m4sl\n# Answer:\nTony Sparano", "# Reasoning Path:\nOakland Raiders -> american_football.football_team.current_head_coach -> Tony Sparano -> people.person.profession -> Coach\n# Answer:\nTony Sparano", "# Reasoning Path:\nOakland Raiders -> sports.sports_team.coaches -> m.0_msn30 -> sports.sports_team_coach_tenure.coach -> Joe Woods\n# Answer:\nm.0_msn30", "# Reasoning Path:\nOakland Raiders -> american_football.football_team.historical_coaching_staff -> m.04ykh5s -> american_football.football_historical_coach_position.coach -> Lane Kiffin\n# Answer:\nm.04ykh5s", "# Reasoning Path:\nOakland Raiders -> sports.sports_team.coaches -> m.0_msn30 -> sports.sports_team_coach_tenure.position -> Defensive Backs Coach\n# Answer:\nm.0_msn30", "# Reasoning Path:\nOakland Raiders -> american_football.football_team.current_head_coach -> Tony Sparano -> people.person.profession -> American football player\n# Answer:\nTony Sparano", "# Reasoning Path:\nOakland Raiders -> american_football.football_team.historical_coaching_staff -> m.04ykh_2 -> american_football.football_historical_coach_position.coach -> Mike Shanahan\n# Answer:\nm.04ykh_2", "# Reasoning Path:\nOakland Raiders -> american_football.football_team.historical_coaching_staff -> m.04ykh5s -> american_football.football_historical_coach_position.position -> Head coach\n# Answer:\nm.04ykh5s", "# Reasoning Path:\nOakland Raiders -> american_football.football_team.historical_coaching_staff -> m.04ykh_2 -> american_football.football_historical_coach_position.position -> Head coach\n# Answer:\nm.04ykh_2", "# Reasoning Path:\nOakland Raiders -> american_football.football_team.historical_coaching_staff -> m.011xd62y -> american_football.football_historical_coach_position.coach -> Dennis Allen\n# Answer:\nm.011xd62y"], "ground_truth": ["Tony Sparano"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1535", "prediction": ["# Reasoning Path:\nMicrosoft Corporation -> organization.organization.place_founded -> Albuquerque -> location.location.containedby -> United States of America\n# Answer:\nAlbuquerque", "# Reasoning Path:\nMicrosoft Corporation -> organization.organization.place_founded -> Albuquerque -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nAlbuquerque", "# Reasoning Path:\nMicrosoft Corporation -> organization.organization_sector.organizations_in_this_sector -> Exclaimer -> organization.organization.place_founded -> United Kingdom\n# Answer:\nExclaimer", "# Reasoning Path:\nMicrosoft Corporation -> organization.organization.headquarters -> m.02shzm7 -> location.mailing_address.citytown -> Redmond\n# Answer:\nm.02shzm7", "# Reasoning Path:\nMicrosoft Corporation -> organization.organization.headquarters -> m.02shzm7 -> common.topic.notable_types -> Location\n# Answer:\nm.02shzm7", "# Reasoning Path:\nMicrosoft Corporation -> organization.organization.headquarters -> m.02shzm7 -> freebase.valuenotation.has_no_value -> Street Address 2\n# Answer:\nm.02shzm7", "# Reasoning Path:\nMicrosoft Corporation -> organization.organization_sector.organizations_in_this_sector -> Exclaimer -> freebase.valuenotation.is_reviewed -> Image\n# Answer:\nExclaimer", "# Reasoning Path:\nMicrosoft Corporation -> organization.organization.headquarters -> m.02shzm7 -> freebase.valuenotation.has_no_value -> Street Address 3\n# Answer:\nm.02shzm7", "# Reasoning Path:\nMicrosoft Corporation -> organization.organization_sector.organizations_in_this_sector -> Exclaimer -> business.business_operation.industry -> Computer\n# Answer:\nExclaimer", "# Reasoning Path:\nMicrosoft Corporation -> organization.organization.headquarters -> m.02shzm7 -> freebase.valuenotation.has_no_value -> Street Address 4\n# Answer:\nm.02shzm7"], "ground_truth": ["Redmond"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1536", "prediction": ["# Reasoning Path:\nJohn Steinbeck -> people.person.education -> m.0n1dbm0 -> education.education.institution -> Salinas High School\n# Answer:\nm.0n1dbm0", "# Reasoning Path:\nJohn Steinbeck -> people.person.education -> m.0n1dbm0 -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nm.0n1dbm0", "# Reasoning Path:\nJohn Steinbeck -> people.person.education -> m.02wpt95 -> education.education.institution -> Stanford University\n# Answer:\nm.02wpt95", "# Reasoning Path:\nJohn Steinbeck -> people.person.education -> m.02wpt95 -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nm.02wpt95", "# Reasoning Path:\nJohn Steinbeck -> award.award_nominee.award_nominations -> m.0b9zmns -> award.award_nomination.ceremony -> 1955 National Book Awards\n# Answer:\nm.0b9zmns", "# Reasoning Path:\nJohn Steinbeck -> book.author.works_written -> A Russian Journal -> common.topic.notable_for -> g.1256s0x_d\n# Answer:\nA Russian Journal", "# Reasoning Path:\nJohn Steinbeck -> book.author.works_written -> A Russian Journal -> book.book.editions -> A Russian journal\n# Answer:\nA Russian Journal", "# Reasoning Path:\nJohn Steinbeck -> book.author.works_written -> A letter -> common.topic.notable_for -> g.125fb994m\n# Answer:\nA letter", "# Reasoning Path:\nJohn Steinbeck -> award.award_nominee.award_nominations -> m.0b9zmns -> award.award_nomination.nominated_for -> Sweet Thursday\n# Answer:\nm.0b9zmns", "# Reasoning Path:\nJohn Steinbeck -> book.author.works_written -> A primer on the thirties -> common.topic.notable_for -> g.1254zp4_p\n# Answer:\nA primer on the thirties"], "ground_truth": ["Stanford University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1537", "prediction": ["# Reasoning Path:\nNiels Henrik David Bohr -> base.nobelprizes.nobel_prize_winner.nobel_honor -> m.064wn6r -> base.nobelprizes.nobel_honor.subject_area -> Quantum mechanics\n# Answer:\nm.064wn6r", "# Reasoning Path:\nNiels Henrik David Bohr -> symbols.name_source.namesakes -> Bohr -> common.topic.notable_types -> Extraterrestrial location\n# Answer:\nBohr", "# Reasoning Path:\nNiels Henrik David Bohr -> symbols.name_source.namesakes -> Bohr -> common.topic.article -> m.0469fk\n# Answer:\nBohr", "# Reasoning Path:\nNiels Henrik David Bohr -> symbols.name_source.namesakes -> 3948 Bohr -> astronomy.star_system_body.star_system -> Solar System\n# Answer:\n3948 Bohr", "# Reasoning Path:\nNiels Henrik David Bohr -> symbols.name_source.namesakes -> 3948 Bohr -> astronomy.orbital_relationship.orbits -> Sun\n# Answer:\n3948 Bohr", "# Reasoning Path:\nNiels Henrik David Bohr -> base.kwebbase.kwtopic.connections_from -> niels henrik david bohr activist with julius robert oppenheimer -> base.kwebbase.kwconnection.other -> J. Robert Oppenheimer\n# Answer:\nniels henrik david bohr activist with julius robert oppenheimer", "# Reasoning Path:\nNiels Henrik David Bohr -> base.kwebbase.kwtopic.connections_from -> niels henrik david bohr appealed to winston churchill -> base.kwebbase.kwconnection.relation -> appealed to\n# Answer:\nniels henrik david bohr appealed to winston churchill", "# Reasoning Path:\nNiels Henrik David Bohr -> symbols.name_source.namesakes -> 3948 Bohr -> common.topic.notable_for -> g.125c3yszl\n# Answer:\n3948 Bohr", "# Reasoning Path:\nNiels Henrik David Bohr -> symbols.name_source.namesakes -> Bohr compactification -> common.topic.article -> m.02z045\n# Answer:\nBohr compactification", "# Reasoning Path:\nNiels Henrik David Bohr -> base.kwebbase.kwtopic.connections_from -> niels henrik david bohr activist with julius robert oppenheimer -> base.kwebbase.kwconnection.relation -> activist with\n# Answer:\nniels henrik david bohr activist with julius robert oppenheimer"], "ground_truth": ["Quantum mechanics"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1538", "prediction": ["# Reasoning Path:\nLouis Sachar -> book.author.works_written -> The Cardturner -> book.book.editions -> Cardturner\n# Answer:\nThe Cardturner", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> The Cardturner -> award.award_nominated_work.award_nominations -> m.0g2978l\n# Answer:\nThe Cardturner", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> The Cardturner -> common.topic.article -> m.0fq08x4\n# Answer:\nThe Cardturner", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> Sixth Grade Secrets -> book.book.editions -> Sixth Grade Secrets (An Apple Paperback)\n# Answer:\nSixth Grade Secrets", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> Sixth Grade Secrets -> common.topic.article -> m.0h0jhr\n# Answer:\nSixth Grade Secrets", "# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> common.topic.article -> m.03tgtz\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> Sixth Grade Secrets -> book.written_work.subjects -> Schools\n# Answer:\nSixth Grade Secrets", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> Sixth Grade Secrets -> book.book.editions -> Sixth Grade Secrets (Apple Paperbacks)\n# Answer:\nSixth Grade Secrets", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> A Flying Birthday Cake? -> common.topic.notable_for -> g.11bbqmptyt\n# Answer:\nA Flying Birthday Cake?", "# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> common.topic.article -> m.0dq62d\n# Answer:\nHoles"], "ground_truth": ["Small Steps", "Il y a un gar\u00e7on dans les toilettes des filles", "Someday Angeline", "Wayside School is Falling Down", "Dogs Don't Tell Jokes", "There's a boy in the girls bathroom", "Johnny's in the Basement", "Wayside School Gets A Little Stranger", "Sixth Grade Secrets", "Boy Who Lost His Face", "Hay Un Chico En El Bano De Las Chicas", "Stanley Yelnats Survival Guide to Camp Green Lake", "Wayside School is falling down", "Wayside School Is Falling Down", "Kidnapped at Birth?", "Class President (A Stepping Stone Book(TM))", "Super Fast, Out of Control! (A Stepping Stone Book(TM))", "Wayside School Boxed Set", "Johnny's in the basement", "Holes (Listening Library)", "Why Pick on Me? (A Stepping Stone Book(TM))", "Small Steps (Readers Circle)", "Louis Sacher Collection", "Holes (World Book Day 2001)", "Pequenos Pasos/ Small Steps", "Sideways Arithmetic from Wayside School", "A Flying Birthday Cake?", "Sideways Stories from Wayside School", "The Boy Who Lost His Face", "Sixth Grade Secrets (Apple Paperbacks)", "More Sideways Arithmetic from Wayside School", "Holes", "Sideways stories from Wayside School", "g.1218f5g0", "Stanley Yelnats' Survival Guide to Camp Green Lake", "Sixth Grade Secrets (An Apple Paperback)", "Holes (with \\\"Connections\\\") HRW Library (HRW library)", "Class President", "Why Pick on Me?", "Small steps", "Holes. (Lernmaterialien)", "Wayside School is falling down (Celebrate reading, Scott Foresman)", "Wayside School gets a little stranger", "Holes (Cascades)", "Super Fast, Out of Control!", "Hoyos/Holes", "Kidnapped at Birth? (A Stepping Stone Book(TM))", "Wayside School Gets a Little Stranger (rack) (Wayside School)", "More Sideways Arithmetic From Wayside School", "The boy who lost his face", "Wayside School Gets a Little Stranger", "Someday Angeline (Avon/Camelot Book)", "The Cardturner", "Marvin Redpost.", "Holes (Yearling Books)", "Sixth grade secrets", "Holes Activity Pack", "There's a Boy in the Girls' Bathroom", "Sideways Arithmetic From Wayside School", "Holes (Readers Circle)", "Wayside School Collection", "A Flying Birthday Cake? (A Stepping Stone Book(TM))", "Alone in His Teacher's House", "Marvin Redpost", "L\u00f6cher", "A magic crystal?", "Monkey soup", "Der Fluch des David Ballinger. ( Ab 11 J.)."], "ans_acc": 0.10294117647058823, "ans_hit": 1, "ans_f1": 0.13698630136986303, "ans_precission": 1.0, "ans_recall": 0.07352941176470588, "path_f1": 0.10389610389610389, "path_precision": 0.8, "path_recall": 0.05555555555555555, "path_ans_f1": 0.18666666666666665, "path_ans_precision": 1.0, "path_ans_recall": 0.10294117647058823}
{"id": "WebQTest-1539", "prediction": ["# Reasoning Path:\nRyan Reynolds -> people.person.parents -> Tammy Reynolds -> people.person.spouse_s -> m.0pbmp4r\n# Answer:\nTammy Reynolds", "# Reasoning Path:\nRyan Reynolds -> people.person.parents -> Tammy Reynolds -> people.person.children -> Jeff Reynolds\n# Answer:\nTammy Reynolds", "# Reasoning Path:\nRyan Reynolds -> people.person.parents -> Jim Reynolds -> people.person.children -> Jeff Reynolds\n# Answer:\nJim Reynolds", "# Reasoning Path:\nRyan Reynolds -> people.person.parents -> Tammy Reynolds -> people.person.gender -> Female\n# Answer:\nTammy Reynolds", "# Reasoning Path:\nRyan Reynolds -> people.person.parents -> Tammy Reynolds -> people.person.children -> Patrick Reynolds\n# Answer:\nTammy Reynolds", "# Reasoning Path:\nRyan Reynolds -> people.person.parents -> Jim Reynolds -> people.person.children -> Patrick Reynolds\n# Answer:\nJim Reynolds", "# Reasoning Path:\nRyan Reynolds -> people.person.parents -> Jim Reynolds -> common.topic.notable_for -> g.125ftkw0d\n# Answer:\nJim Reynolds", "# Reasoning Path:\nRyan Reynolds -> people.person.spouse_s -> m.0ygrd3d -> people.marriage.spouse -> Scarlett Johansson\n# Answer:\nScarlett Johansson", "# Reasoning Path:\nRyan Reynolds -> people.person.parents -> Tammy Reynolds -> people.person.children -> Terry Reynolds\n# Answer:\nTammy Reynolds", "# Reasoning Path:\nRyan Reynolds -> people.person.parents -> Jim Reynolds -> people.person.children -> Terry Reynolds\n# Answer:\nJim Reynolds"], "ground_truth": ["Blake Lively"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-154", "prediction": ["# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.instrumentalists -> Bix Beiderbecke\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> common.topic.notable_types -> Musical instrument\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> music.instrument.family -> Brass instrument\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.family -> Brass instrument\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.instrumentalists -> James Morrison\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.instrumentalists -> King Oliver\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> book.written_work.subjects -> Trumpeter\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> common.topic.notable_types -> Book\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.featured_artist.recordings -> Boog It -> music.recording.artist -> Mills Brothers\n# Answer:\nBoog It", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> common.topic.notable_types -> Musical instrument\n# Answer:\nTrumpet"], "ground_truth": ["Trumpet", "Cornet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1540", "prediction": ["# Reasoning Path:\nNewt Gingrich -> people.person.parents -> Kathleen Daughterty -> people.person.spouse_s -> m.0vn1cy8\n# Answer:\nKathleen Daughterty", "# Reasoning Path:\nNewt Gingrich -> people.person.parents -> Kathleen Daughterty -> people.person.children -> Roberta Brown Gingrich\n# Answer:\nKathleen Daughterty", "# Reasoning Path:\nNewt Gingrich -> people.person.spouse_s -> m.0hybx80 -> people.marriage.spouse -> Callista Gingrich\n# Answer:\nm.0hybx80", "# Reasoning Path:\nNewt Gingrich -> people.person.spouse_s -> m.0hybx80 -> people.marriage.location_of_ceremony -> Alexandria\n# Answer:\nm.0hybx80", "# Reasoning Path:\nNewt Gingrich -> people.person.spouse_s -> m.0j1782r -> people.marriage.spouse -> Jackie Battley\n# Answer:\nm.0j1782r", "# Reasoning Path:\nNewt Gingrich -> people.person.parents -> Kathleen Daughterty -> people.person.spouse_s -> m.0vn1dgk\n# Answer:\nKathleen Daughterty", "# Reasoning Path:\nNewt Gingrich -> people.person.parents -> Newton McPherson -> people.person.spouse_s -> m.0vn1dgk\n# Answer:\nNewton McPherson", "# Reasoning Path:\nNewt Gingrich -> people.person.spouse_s -> m.0hybx80 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0hybx80", "# Reasoning Path:\nNewt Gingrich -> people.person.spouse_s -> m.0j17836 -> people.marriage.spouse -> Marianne Ginther\n# Answer:\nm.0j17836", "# Reasoning Path:\nNewt Gingrich -> people.person.parents -> Robert Gingrich -> people.person.spouse_s -> m.0vn1cy8\n# Answer:\nRobert Gingrich"], "ground_truth": ["Callista Gingrich"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1541", "prediction": ["# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Baroque music -> music.genre.albums -> A Bit o' This & That\n# Answer:\nBaroque music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Baroque music -> common.topic.image -> Baschenis - Musical Instruments\n# Answer:\nBaroque music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Baroque music -> music.genre.albums -> Baroque Masterpieces\n# Answer:\nBaroque music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> broadcast.genre.content -> Classica\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Baroque music -> music.genre.albums -> Baroque Rocks!\n# Answer:\nBaroque music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Baroque music -> influence.influence_node.influenced -> Les Fradkin\n# Answer:\nBaroque music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> broadcast.genre.content -> 1.FM Otto's  Baroque music\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> radio.radio_subject.programs_with_this_subject -> Adventures in Good Music\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> base.ontologies.ontology_instance.equivalent_instances -> m.07ndktx -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nm.07ndktx", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Baroque music -> influence.influence_node.influenced -> The Left Banke\n# Answer:\nBaroque music"], "ground_truth": ["Baroque music", "Classical music"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1542", "prediction": ["# Reasoning Path:\nJames Franco -> people.person.education -> m.0cjcrvl -> education.education.institution -> Tisch School of the Arts\n# Answer:\nm.0cjcrvl", "# Reasoning Path:\nJames Franco -> people.person.education -> m.0n1n3d5 -> education.education.institution -> Rhode Island School of Design\n# Answer:\nm.0n1n3d5", "# Reasoning Path:\nJames Franco -> people.person.education -> m.0gxbcv6 -> education.education.institution -> Brooklyn College\n# Answer:\nm.0gxbcv6", "# Reasoning Path:\nJames Franco -> people.person.education -> m.0cjcrvl -> education.education.major_field_of_study -> Filmmaking\n# Answer:\nm.0cjcrvl", "# Reasoning Path:\nJames Franco -> people.person.education -> m.0gxbcv6 -> education.education.degree -> Master of Fine Arts\n# Answer:\nm.0gxbcv6", "# Reasoning Path:\nJames Franco -> common.topic.webpage -> m.098dnd6 -> common.webpage.resource -> Where are the ''Freaks and Geeks'' now?\n# Answer:\nm.098dnd6", "# Reasoning Path:\nJames Franco -> people.person.education -> m.0gxbcv6 -> education.education.major_field_of_study -> Fiction writing\n# Answer:\nm.0gxbcv6", "# Reasoning Path:\nJames Franco -> award.award_nominee.award_nominations -> m.0110_sg8 -> award.award_nomination.award_nominee -> Seth Rogen\n# Answer:\nm.0110_sg8", "# Reasoning Path:\nJames Franco -> common.topic.webpage -> m.098dx2c -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.098dx2c", "# Reasoning Path:\nJames Franco -> common.topic.webpage -> m.098dnd6 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.098dnd6"], "ground_truth": ["Tisch School of the Arts", "Rhode Island School of Design", "Warren Wilson College", "Palo Alto High School", "University of California, Los Angeles", "Columbia University School of the Arts", "University Yale", "Brooklyn College"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333326, "path_precision": 0.3, "path_recall": 0.375, "path_ans_f1": 0.33333333333333326, "path_ans_precision": 0.3, "path_ans_recall": 0.375}
{"id": "WebQTest-1543", "prediction": ["# Reasoning Path:\nMark Twain -> people.person.profession -> Humorist -> people.profession.specializations -> Raconteur\n# Answer:\nHumorist", "# Reasoning Path:\nMark Twain -> people.person.profession -> Humorist -> people.profession.specialization_of -> Writer\n# Answer:\nHumorist", "# Reasoning Path:\nMark Twain -> people.person.profession -> Journalist -> common.topic.notable_types -> Profession\n# Answer:\nJournalist", "# Reasoning Path:\nMark Twain -> people.person.profession -> Humorist -> common.topic.notable_for -> g.1256kn4dk\n# Answer:\nHumorist", "# Reasoning Path:\nMark Twain -> people.person.profession -> Writer -> people.profession.specializations -> Humorist\n# Answer:\nWriter", "# Reasoning Path:\nMark Twain -> people.person.profession -> Journalist -> people.profession.corresponding_type -> Author\n# Answer:\nJournalist", "# Reasoning Path:\nMark Twain -> people.person.profession -> Journalist -> people.profession.specialization_of -> Writer\n# Answer:\nJournalist", "# Reasoning Path:\nMark Twain -> people.person.profession -> Writer -> people.profession.specializations -> Journalist\n# Answer:\nWriter", "# Reasoning Path:\nMark Twain -> people.person.profession -> Writer -> book.book_subject.works -> The Passion-Driven Writer and the Digital-Age Literary Marketplace\n# Answer:\nWriter", "# Reasoning Path:\nMark Twain -> people.person.profession -> Writer -> common.topic.notable_types -> Profession\n# Answer:\nWriter"], "ground_truth": ["Journalist", "Humorist", "Teacher", "Writer", "Author", "Novelist"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1544", "prediction": ["# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx817 -> government.government_position_held.office_holder -> Harry S. Truman\n# Answer:\nm.03fx817", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03q2h_9 -> government.government_position_held.office_holder -> Franklin D. Roosevelt\n# Answer:\nm.03q2h_9", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx8cf -> government.government_position_held.office_holder -> Richard Nixon\n# Answer:\nm.03fx8cf", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx817 -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nm.03fx817", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03q2h_9 -> government.government_position_held.basic_title -> President\n# Answer:\nm.03q2h_9", "# Reasoning Path:\nPresident of the United States -> freebase.equivalent_topic.equivalent_type -> US President -> freebase.type_profile.kind -> Title\n# Answer:\nUS President", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx8cf -> government.government_position_held.basic_title -> President\n# Answer:\nm.03fx8cf", "# Reasoning Path:\nPresident of the United States -> freebase.equivalent_topic.equivalent_type -> US President -> type.type.expected_by -> President\n# Answer:\nUS President", "# Reasoning Path:\nPresident of the United States -> freebase.equivalent_topic.equivalent_type -> US President -> freebase.type_profile.kind -> Significant\n# Answer:\nUS President", "# Reasoning Path:\nPresident of the United States -> freebase.equivalent_topic.equivalent_type -> US President -> type.type.properties -> President number\n# Answer:\nUS President"], "ground_truth": ["John Tyler"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1546", "prediction": ["# Reasoning Path:\nRick Santorum -> people.person.education -> m.012gg3yy -> education.education.institution -> Joseph M. Katz Graduate School of Business\n# Answer:\nm.012gg3yy", "# Reasoning Path:\nRick Santorum -> people.person.education -> m.02wn695 -> education.education.institution -> Pennsylvania State University\n# Answer:\nm.02wn695", "# Reasoning Path:\nRick Santorum -> people.person.education -> m.04hx35s -> education.education.institution -> University of Pittsburgh\n# Answer:\nm.04hx35s", "# Reasoning Path:\nRick Santorum -> business.board_member.organization_board_memberships -> m.09tymds -> organization.organization_board_membership.organization -> Universal Health Services\n# Answer:\nm.09tymds", "# Reasoning Path:\nRick Santorum -> people.person.education -> m.02wn695 -> education.education.degree -> Bachelor of Arts\n# Answer:\nm.02wn695", "# Reasoning Path:\nRick Santorum -> government.politician.party -> m.03gjgj7 -> government.political_party_tenure.party -> Republican Party\n# Answer:\nm.03gjgj7", "# Reasoning Path:\nRick Santorum -> people.person.education -> m.02wn695 -> education.education.major_field_of_study -> Political Science\n# Answer:\nm.02wn695", "# Reasoning Path:\nRick Santorum -> people.person.education -> m.04hx35s -> education.education.major_field_of_study -> Business Administration\n# Answer:\nm.04hx35s"], "ground_truth": ["Carmel High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1547", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> book.author.works_written -> 41: A Portrait of My Father -> book.book.genre -> Biography\n# Answer:\n41: A Portrait of My Father", "# Reasoning Path:\nGeorge W. Bush -> book.author.works_written -> 41: A Portrait of My Father -> common.topic.notable_for -> g.11b7c727bj\n# Answer:\n41: A Portrait of My Father", "# Reasoning Path:\nGeorge W. Bush -> book.author.works_written -> The George W. Bush foreign policy reader -> common.topic.notable_types -> Book\n# Answer:\nThe George W. Bush foreign policy reader", "# Reasoning Path:\nGeorge W. Bush -> book.author.works_written -> A Charge to Keep -> book.written_work.author -> Michael Herskowitz\n# Answer:\nA Charge to Keep", "# Reasoning Path:\nGeorge W. Bush -> book.author.works_written -> The George W. Bush foreign policy reader -> common.topic.notable_for -> g.125b58jd4\n# Answer:\nThe George W. Bush foreign policy reader", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointed_role -> Director of the Bureau of Counterterrorism\n# Answer:\nm.010p95d2", "# Reasoning Path:\nGeorge W. Bush -> book.author.works_written -> A Charge to Keep -> book.written_work.next_in_series -> Decision Points\n# Answer:\nA Charge to Keep", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pk -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.010l29pk", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointee -> Henry A. Crumpton\n# Answer:\nm.010p95d2", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1 -> people.appointment.appointed_role -> United States Ambassador to Angola\n# Answer:\nm.079pxt1"], "ground_truth": ["In My Time: A Personal and Political Memoir"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1548", "prediction": ["# Reasoning Path:\nAntoine Lavoisier -> people.deceased_person.place_of_death -> Place de la Concorde -> location.location.contains -> Luxor Obelisk\n# Answer:\nPlace de la Concorde", "# Reasoning Path:\nAntoine Lavoisier -> people.deceased_person.place_of_death -> Place de la Concorde -> common.topic.article -> m.0kx9q\n# Answer:\nPlace de la Concorde", "# Reasoning Path:\nAntoine Lavoisier -> people.deceased_person.place_of_death -> Place de la Concorde -> base.schemastaging.context_name.pronunciation -> g.11b7zfzkkp\n# Answer:\nPlace de la Concorde", "# Reasoning Path:\nAntoine Lavoisier -> influence.influence_node.influenced_by -> Guillaume-Fran\u00e7ois Rouelle -> people.deceased_person.place_of_death -> Paris\n# Answer:\nGuillaume-Fran\u00e7ois Rouelle", "# Reasoning Path:\nAntoine Lavoisier -> freebase.valuenotation.is_reviewed -> Place of death -> type.property.schema -> Deceased Person\n# Answer:\nPlace of death", "# Reasoning Path:\nAntoine Lavoisier -> influence.influence_node.influenced_by -> John Dalton -> people.deceased_person.place_of_death -> Manchester\n# Answer:\nJohn Dalton", "# Reasoning Path:\nAntoine Lavoisier -> freebase.valuenotation.is_reviewed -> Place of death -> rdf-schema#range -> Location\n# Answer:\nPlace of death", "# Reasoning Path:\nAntoine Lavoisier -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nAntoine Lavoisier -> influence.influence_node.influenced_by -> Joseph Priestley -> people.deceased_person.place_of_death -> Pennsylvania\n# Answer:\nJoseph Priestley", "# Reasoning Path:\nAntoine Lavoisier -> freebase.valuenotation.is_reviewed -> Place of death -> rdf-schema#domain -> Deceased Person\n# Answer:\nPlace of death"], "ground_truth": ["Place de la Concorde"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-155", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> base.inaugurations.inauguration_speaker.inauguration -> George W. Bush 2001 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nGeorge W. Bush 2001 presidential inauguration", "# Reasoning Path:\nGeorge W. Bush -> base.inaugurations.inauguration_speaker.inauguration -> George W. Bush 2001 presidential inauguration -> common.topic.image -> GWBush1.jpg\n# Answer:\nGeorge W. Bush 2001 presidential inauguration", "# Reasoning Path:\nGeorge W. Bush -> base.inaugurations.inauguration_speaker.inauguration -> George W. Bush 2005 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nGeorge W. Bush 2005 presidential inauguration", "# Reasoning Path:\nGeorge W. Bush -> base.inaugurations.inauguration_speaker.inauguration -> George W. Bush 2001 presidential inauguration -> time.event.instance_of_recurring_event -> United States presidential inauguration\n# Answer:\nGeorge W. Bush 2001 presidential inauguration", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2004 -> government.election_campaign.election -> United States presidential election, 2004\n# Answer:\nGeorge W. Bush presidential campaign, 2004", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2000 -> government.election_campaign.election -> United States presidential election, 2000\n# Answer:\nGeorge W. Bush presidential campaign, 2000", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2000 -> government.election_campaign.party -> Republican Party\n# Answer:\nGeorge W. Bush presidential campaign, 2000", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2000 -> common.topic.notable_types -> Election campaign\n# Answer:\nGeorge W. Bush presidential campaign, 2000", "# Reasoning Path:\nGeorge W. Bush -> base.inaugurations.inauguration_speaker.inauguration -> George W. Bush 2005 presidential inauguration -> common.topic.notable_for -> g.1255hgrk4\n# Answer:\nGeorge W. Bush 2005 presidential inauguration", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2004 -> common.topic.notable_for -> g.1257w3www\n# Answer:\nGeorge W. Bush presidential campaign, 2004"], "ground_truth": ["2000-11-07", "2004-11-02"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1550", "prediction": ["# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> people.person.parents -> Alexandra of Denmark\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> people.person.spouse_s -> m.02kp9gn\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> people.person.parents -> Edward VII\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> Mary of Teck -> people.person.children -> Edward VIII of the United Kingdom\n# Answer:\nMary of Teck", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> freebase.valuenotation.is_reviewed -> Children\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> Mary of Teck -> people.person.children -> Prince John of the United Kingdom\n# Answer:\nMary of Teck", "# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.person.children -> Anne, Princess Royal\n# Answer:\nElizabeth II", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.person.spouse_s -> m.028zms8\n# Answer:\nElizabeth II", "# Reasoning Path:\nGeorge VI -> people.person.parents -> Mary of Teck -> people.person.parents -> Francis, Duke of Teck\n# Answer:\nMary of Teck"], "ground_truth": ["Queen Elizabeth The Queen Mother"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1551", "prediction": ["# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> time.event.instance_of_recurring_event -> Super Bowl\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> freebase.valuenotation.is_reviewed -> Champion\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> freebase.valuenotation.has_no_value -> Comment\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> 2006 NFC Championship Game -> common.topic.notable_for -> g.1z2spvm2w\n# Answer:\n2006 NFC Championship Game", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> freebase.valuenotation.is_reviewed -> Season\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> 2006 NFC Championship Game -> time.event.locations -> CenturyLink Field\n# Answer:\n2006 NFC Championship Game", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.season_record -> m.075fsdt -> sports.sports_team_season_record.season -> 2005 NFL season\n# Answer:\nm.075fsdt", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.season_record -> m.075fsd1 -> sports.sports_team_season_record.season -> 2008 NFL season\n# Answer:\nm.075fsd1", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> 2006 NFC Championship Game -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n2006 NFC Championship Game", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> freebase.valuenotation.is_reviewed -> Championship\n# Answer:\nSuper Bowl XLVIII"], "ground_truth": ["Super Bowl XLVIII", "Super Bowl XL"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1553", "prediction": ["# Reasoning Path:\nSami Yusuf -> people.person.place_of_birth -> Tehran -> location.location.events -> 1974 Tehran International Short Film Festival\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> people.person.places_lived -> m.03pn2p8 -> people.place_lived.location -> Tehran\n# Answer:\nm.03pn2p8", "# Reasoning Path:\nSami Yusuf -> people.person.place_of_birth -> Tehran -> location.location.events -> 1983 Asian Wrestling Championships\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> people.person.place_of_birth -> Tehran -> sports.sports_team_location.teams -> Esteghlal Tehran Football Club\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> music.artist.origin -> Tehran -> location.location.events -> 1974 Tehran International Short Film Festival\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> people.person.place_of_birth -> Tehran -> location.location.events -> 1983 Fajr International Film Festival\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> people.person.place_of_birth -> Tehran -> periodicals.newspaper_circulation_area.newspapers -> Asrar\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> people.person.place_of_birth -> Tehran -> sports.sports_team_location.teams -> Paykan Tehran VC\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> music.artist.origin -> Tehran -> location.location.events -> 1983 Asian Wrestling Championships\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> music.artist.origin -> Tehran -> sports.sports_team_location.teams -> Esteghlal Tehran Football Club\n# Answer:\nTehran"], "ground_truth": ["Tehran"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1554", "prediction": ["# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Portuguese Language -> language.human_language.dialects -> Brazilian Portuguese\n# Answer:\nPortuguese Language", "# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Brazilian Portuguese -> language.language_dialect.language -> Portuguese Language\n# Answer:\nBrazilian Portuguese", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\ng.12cp_jcjy", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\ng.11b71vdgdz", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\ng.1hhc390hy", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc39sjp\n# Answer:\ng.1hhc39sjp", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fm01\n# Answer:\ng.12tb6fm01", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc385hl\n# Answer:\ng.1hhc385hl"], "ground_truth": ["Portuguese Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1555", "prediction": ["# Reasoning Path:\nParliament of Nepal -> government.governmental_body.jurisdiction -> Nepal -> government.governmental_jurisdiction.government_bodies -> 1st Nepalese Constituent Assembly\n# Answer:\nNepal", "# Reasoning Path:\nParliament of Nepal -> government.governmental_body.jurisdiction -> Nepal -> government.governmental_jurisdiction.government_bodies -> House of Representatives\n# Answer:\nNepal", "# Reasoning Path:\nParliament of Nepal -> government.governmental_body.jurisdiction -> Nepal -> location.country.official_language -> Nepali Language\n# Answer:\nNepal", "# Reasoning Path:\nParliament of Nepal -> government.governmental_body.jurisdiction -> Nepal -> government.governmental_jurisdiction.government_bodies -> 2nd Nepalese Constituent Assembly\n# Answer:\nNepal", "# Reasoning Path:\nParliament of Nepal -> government.governmental_body.jurisdiction -> Nepal -> location.statistical_region.electricity_consumption_per_capita -> g.1245_0stl\n# Answer:\nNepal", "# Reasoning Path:\nParliament of Nepal -> common.topic.notable_types -> Governmental Body -> type.type.expected_by -> Parliament\n# Answer:\nGovernmental Body", "# Reasoning Path:\nParliament of Nepal -> common.topic.image -> Nepal Coat of Arms -> common.image.appears_in_topic_gallery -> Emblem of Nepal\n# Answer:\nNepal Coat of Arms", "# Reasoning Path:\nParliament of Nepal -> common.topic.image -> Nepal Coat of Arms -> common.image.size -> m.0bn01yk\n# Answer:\nNepal Coat of Arms", "# Reasoning Path:\nParliament of Nepal -> government.governmental_body.jurisdiction -> Nepal -> location.statistical_region.electricity_consumption_per_capita -> g.1245_4_mx\n# Answer:\nNepal", "# Reasoning Path:\nParliament of Nepal -> common.topic.notable_types -> Governmental Body -> freebase.type_profile.kind -> Definition\n# Answer:\nGovernmental Body"], "ground_truth": ["Parliament of Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1556", "prediction": ["# Reasoning Path:\nNew York Mets -> sports.sports_team.championships -> 1986 World Series -> time.event.locations -> Fenway Park\n# Answer:\n1986 World Series", "# Reasoning Path:\nNew York Mets -> sports.sports_team.championships -> 1986 World Series -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1986 World Series", "# Reasoning Path:\nNew York Mets -> sports.sports_team.championships -> 1986 World Series -> common.topic.article -> m.01q9lj\n# Answer:\n1986 World Series", "# Reasoning Path:\nNew York Mets -> sports.sports_team.championships -> 1986 World Series -> time.event.locations -> Shea Stadium\n# Answer:\n1986 World Series", "# Reasoning Path:\nNew York Mets -> sports.sports_team.championships -> 1969 World Series -> film.film_subject.films -> 1969 World Series Highlight Film\n# Answer:\n1969 World Series", "# Reasoning Path:\nNew York Mets -> sports.sports_team.championships -> 1969 World Series -> time.event.locations -> Memorial Stadium\n# Answer:\n1969 World Series", "# Reasoning Path:\nNew York Mets -> sports.sports_team.championships -> 1969 World Series -> common.topic.notable_for -> g.12553vqj6\n# Answer:\n1969 World Series", "# Reasoning Path:\nNew York Mets -> sports.sports_team.championships -> 1969 World Series -> time.event.locations -> Shea Stadium\n# Answer:\n1969 World Series", "# Reasoning Path:\nNew York Mets -> baseball.baseball_team.team_stats -> m.05n69_2 -> baseball.baseball_team_stats.season -> 1964 Major League Baseball Season\n# Answer:\nm.05n69_2", "# Reasoning Path:\nNew York Mets -> sports.sports_team.arena_stadium -> Citi Field -> common.topic.webpage -> m.09w_bd2\n# Answer:\nCiti Field"], "ground_truth": ["1986 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1557", "prediction": ["# Reasoning Path:\nFlorida Panthers -> sports.sports_team.arena_stadium -> BB&T Center -> sports.sports_facility.home_venue_for -> m.0wz2lgr\n# Answer:\nBB&T Center", "# Reasoning Path:\nFlorida Panthers -> sports.sports_team.arena_stadium -> Miami Arena -> sports.sports_facility.teams -> Miami Matadors\n# Answer:\nMiami Arena", "# Reasoning Path:\nFlorida Panthers -> sports.sports_team.arena_stadium -> BB&T Center -> location.location.geolocation -> m.02_vl7c\n# Answer:\nBB&T Center", "# Reasoning Path:\nFlorida Panthers -> sports.sports_team.arena_stadium -> Miami Arena -> location.location.containedby -> Miami\n# Answer:\nMiami Arena", "# Reasoning Path:\nFlorida Panthers -> sports.sports_team.arena_stadium -> Miami Arena -> common.topic.article -> m.039xm3\n# Answer:\nMiami Arena", "# Reasoning Path:\nFlorida Panthers -> sports.sports_team.location -> Sunrise -> location.statistical_region.population -> g.11b66c6kz3\n# Answer:\nSunrise", "# Reasoning Path:\nFlorida Panthers -> sports.sports_team.location -> Sunrise -> common.topic.webpage -> m.03ky2zw\n# Answer:\nSunrise", "# Reasoning Path:\nFlorida Panthers -> sports.sports_team.location -> Sunrise -> location.location.contains -> BB&T Center\n# Answer:\nSunrise", "# Reasoning Path:\nFlorida Panthers -> sports.sports_team.arena_stadium -> Miami Arena -> location.location.containedby -> 33136\n# Answer:\nMiami Arena", "# Reasoning Path:\nFlorida Panthers -> sports.sports_team.location -> Sunrise -> location.statistical_region.population -> g.11bykxlp3b\n# Answer:\nSunrise"], "ground_truth": ["Miami Arena", "BB&T Center"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1558", "prediction": ["# Reasoning Path:\nJohn Cena -> people.person.spouse_s -> m.07n79hl -> people.marriage.spouse -> Elizabeth Huberdeau\n# Answer:\nm.07n79hl", "# Reasoning Path:\nJohn Cena -> people.person.spouse_s -> m.07n79hl -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.07n79hl", "# Reasoning Path:\nJohn Cena -> people.person.spouse_s -> m.07n79hl -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.07n79hl", "# Reasoning Path:\nJohn Cena -> music.artist.track -> Bad, Bad Man -> music.recording.releases -> Hit Mania Dance 2006\n# Answer:\nBad, Bad Man", "# Reasoning Path:\nJohn Cena -> base.schemastaging.person_extra.net_worth -> m.0j8mg01 -> measurement_unit.dated_money_value.currency -> United States Dollar\n# Answer:\nm.0j8mg01", "# Reasoning Path:\nJohn Cena -> music.artist.track -> Bad, Bad Man -> music.recording.artist -> Tha Trademarc\n# Answer:\nBad, Bad Man", "# Reasoning Path:\nJohn Cena -> music.artist.track -> Bad, Bad Man (feat. Bumpy Knuckles) -> music.recording.artist -> Tha Trademarc\n# Answer:\nBad, Bad Man (feat. Bumpy Knuckles)", "# Reasoning Path:\nJohn Cena -> music.artist.track -> Bad, Bad Man -> music.recording.lyrics_website -> m.05_4lv7\n# Answer:\nBad, Bad Man", "# Reasoning Path:\nJohn Cena -> music.artist.track -> Basic Thugonomics -> music.recording.tracks -> g.11b81sbnsv\n# Answer:\nBasic Thugonomics", "# Reasoning Path:\nJohn Cena -> base.schemastaging.person_extra.net_worth -> m.0j8mg01 -> measurement_unit.dated_money_value.source -> celebritynetworth.com\n# Answer:\nm.0j8mg01"], "ground_truth": ["Elizabeth Huberdeau"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1559", "prediction": ["# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> Santana Row -> common.topic.article -> m.072p6d\n# Answer:\nSantana Row", "# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> San Jose Museum of Art -> common.topic.image -> The San Jose Museum of Art\n# Answer:\nSan Jose Museum of Art", "# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> Santana Row -> location.location.containedby -> 95128\n# Answer:\nSantana Row", "# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> San Jose Museum of Art -> common.topic.notable_types -> Museum\n# Answer:\nSan Jose Museum of Art", "# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> Kelley Park -> common.topic.notable_types -> Location\n# Answer:\nKelley Park", "# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> Santana Row -> location.location.containedby -> California\n# Answer:\nSantana Row", "# Reasoning Path:\nSan Jose -> location.location.containedby -> Area code 408 -> location.location.contains -> Santa Clara\n# Answer:\nArea code 408", "# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> Santana Row -> location.location.containedby -> Santa Clara County\n# Answer:\nSantana Row", "# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> Kelley Park -> location.location.containedby -> 95112\n# Answer:\nKelley Park", "# Reasoning Path:\nSan Jose -> location.place_with_neighborhoods.neighborhoods -> Alum Rock -> location.location.containedby -> California\n# Answer:\nAlum Rock"], "ground_truth": ["Children's Discovery Museum of San Jose", "Winchester Mystery House", "Cinequest Film Festival", "Alum Rock Park", "Rosicrucian Egyptian Museum", "San Jose Municipal Rose Garden", "Rock \u2018n\u2019 Roll San Jose Half Marathon", "Japantown", "San Jose Museum of Quilts & Textiles", "Santana Row", "Kelley Park", "The Tech Museum of Innovation", "Happy Hollow Park & Zoo", "Downtown San Jose", "San Jose Museum of Art"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.32000000000000006, "ans_precission": 0.8, "ans_recall": 0.2, "path_f1": 0.32000000000000006, "path_precision": 0.8, "path_recall": 0.2, "path_ans_f1": 0.32000000000000006, "path_ans_precision": 0.8, "path_ans_recall": 0.2}
{"id": "WebQTest-156", "prediction": ["# Reasoning Path:\nCalifornia -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nPacific Time Zone", "# Reasoning Path:\nCalifornia -> film.film_location.featured_in_films -> Mumford -> film.film.featured_film_locations -> Guerneville\n# Answer:\nMumford", "# Reasoning Path:\nCalifornia -> film.film_location.featured_in_films -> Mumford -> film.film.country -> United States of America\n# Answer:\nMumford", "# Reasoning Path:\nCalifornia -> film.film_location.featured_in_films -> Mumford -> film.film.featured_film_locations -> Healdsburg\n# Answer:\nMumford", "# Reasoning Path:\nCalifornia -> location.location.events -> 1987 Wine Country Film Festival -> freebase.valuenotation.has_value -> Closing date\n# Answer:\n1987 Wine Country Film Festival", "# Reasoning Path:\nCalifornia -> location.location.events -> 1987 Wine Country Film Festival -> time.event.locations -> Sonoma Valley\n# Answer:\n1987 Wine Country Film Festival", "# Reasoning Path:\nCalifornia -> film.film_location.featured_in_films -> Mumford -> film.film.featured_film_locations -> Napa County\n# Answer:\nMumford", "# Reasoning Path:\nCalifornia -> film.film_location.featured_in_films -> Mumford -> film.film.language -> English Language\n# Answer:\nMumford", "# Reasoning Path:\nCalifornia -> location.location.events -> 1987 Wine Country Film Festival -> time.event.instance_of_recurring_event -> Wine Country Film Festival\n# Answer:\n1987 Wine Country Film Festival", "# Reasoning Path:\nCalifornia -> location.location.events -> 1989 Wine Country Film Festival -> time.event.instance_of_recurring_event -> Wine Country Film Festival\n# Answer:\n1989 Wine Country Film Festival"], "ground_truth": ["Pacific Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1560", "prediction": ["# Reasoning Path:\nNew York University -> organization.organization.founders -> Albert Gallatin -> organization.organization_founder.organizations_founded -> American Ethnological Society\n# Answer:\nAlbert Gallatin", "# Reasoning Path:\nNew York University -> organization.organization.founders -> Albert Gallatin -> people.deceased_person.place_of_death -> Astoria\n# Answer:\nAlbert Gallatin", "# Reasoning Path:\nNew York University -> organization.organization.founders -> Mordecai Manuel Noah -> book.author.works_written -> Correspondence and documents relative to the attempt to negotiate for the release of the American captives at Algiers\n# Answer:\nMordecai Manuel Noah", "# Reasoning Path:\nNew York University -> law.litigant.party_to_cases -> m.0dggt0t -> law.legal_case_party_relationship.case -> New York University v. Ariel Fund Ltd\n# Answer:\nm.0dggt0t", "# Reasoning Path:\nNew York University -> organization.organization.founders -> Albert Gallatin -> government.politician.government_positions_held -> m.04kq1pk\n# Answer:\nAlbert Gallatin", "# Reasoning Path:\nNew York University -> organization.organization.founders -> Mordecai Manuel Noah -> common.topic.notable_types -> Organization founder\n# Answer:\nMordecai Manuel Noah", "# Reasoning Path:\nNew York University -> organization.organization.founders -> Mordecai Manuel Noah -> book.author.works_written -> Discourse delivered at the consecration of the synagogue of [K.K. She\u02bberit Yisra\u02bbel] in the city of New-York\n# Answer:\nMordecai Manuel Noah", "# Reasoning Path:\nNew York University -> organization.organization.founders -> Mordecai Manuel Noah -> people.person.profession -> Diplomat\n# Answer:\nMordecai Manuel Noah", "# Reasoning Path:\nNew York University -> law.litigant.party_to_cases -> m.0dggt0t -> law.legal_case_party_relationship.role -> Plaintiff\n# Answer:\nm.0dggt0t", "# Reasoning Path:\nNew York University -> organization.organization.founders -> Albert Gallatin -> government.politician.government_positions_held -> m.09rfklv\n# Answer:\nAlbert Gallatin"], "ground_truth": ["Albert Gallatin", "Mordecai Manuel Noah"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1561", "prediction": ["# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> Cookie Jar Group -> common.topic.notable_types -> Production company\n# Answer:\nCookie Jar Group", "# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> WGBH -> broadcast.broadcast.distributor -> Non-commercial educational\n# Answer:\nWGBH", "# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> Cookie Jar Group -> film.production_company.films -> Caillou's Holiday Movie\n# Answer:\nCookie Jar Group", "# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> Cookie Jar Group -> common.topic.webpage -> m.04m3pcs\n# Answer:\nCookie Jar Group", "# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> Marc Brown -> award.award_nominee.award_nominations -> m.0m2vcqk\n# Answer:\nMarc Brown", "# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> Marc Brown -> people.person.place_of_birth -> Erie\n# Answer:\nMarc Brown", "# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> Cookie Jar Group -> film.production_company.films -> The Real Story of Happy Birthday to You\n# Answer:\nCookie Jar Group", "# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> WGBH -> tv.tv_producer.programs_produced -> m.06f52v8\n# Answer:\nWGBH", "# Reasoning Path:\nArthur -> award.award_nominated_work.award_nominations -> m.010g23zh -> award.award_nomination.award_nominee -> Alan Cumming\n# Answer:\nm.010g23zh", "# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> Cookie Jar Group -> film.production_company.films -> Voulez-vous coucher avec God?\n# Answer:\nCookie Jar Group"], "ground_truth": ["Cookie Jar Group", "Marc Brown", "WGBH"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1562", "prediction": ["# Reasoning Path:\nEuropean Union -> organization.organization.geographic_scope -> Europe -> base.locations.continents.countries_within -> Hungary\n# Answer:\nEurope", "# Reasoning Path:\nEuropean Union -> organization.organization.geographic_scope -> Europe -> base.locations.continents.countries_within -> Lithuania\n# Answer:\nEurope", "# Reasoning Path:\nEuropean Union -> organization.organization.geographic_scope -> Europe -> base.locations.continents.countries_within -> Latvia\n# Answer:\nEurope", "# Reasoning Path:\nEuropean Union -> organization.organization.geographic_scope -> Europe -> government.governmental_jurisdiction.governing_officials -> m.010pyz2k\n# Answer:\nEurope", "# Reasoning Path:\nEuropean Union -> organization.organization.geographic_scope -> Europe -> organization.organization_scope.organizations_with_this_scope -> Council of Europe\n# Answer:\nEurope", "# Reasoning Path:\nEuropean Union -> location.statistical_region.broadband_penetration_rate -> g.1245_6ndt\n# Answer:\ng.1245_6ndt", "# Reasoning Path:\nEuropean Union -> location.statistical_region.broadband_penetration_rate -> g.1245yvl64\n# Answer:\ng.1245yvl64", "# Reasoning Path:\nEuropean Union -> location.statistical_region.broadband_penetration_rate -> g.1245ywrjj\n# Answer:\ng.1245ywrjj", "# Reasoning Path:\nEuropean Union -> internet.website_owner.websites_owned -> European Space Weather portal -> common.topic.article -> m.0gvrybr\n# Answer:\nEuropean Space Weather portal", "# Reasoning Path:\nEuropean Union -> internet.website_owner.websites_owned -> Health-EU portal -> common.topic.article -> m.065yqz9\n# Answer:\nHealth-EU portal"], "ground_truth": ["Czech Republic", "Belgium", "Hungary", "Poland", "Greece", "Romania", "Slovakia", "Lithuania", "Bulgaria", "Croatia", "France", "Cyprus", "Italy", "Estonia", "Sweden", "Netherlands", "Germany", "Finland", "United Kingdom", "Malta", "Latvia", "Spain", "Austria", "Luxembourg", "Denmark", "Slovenia", "Republic of Ireland", "Portugal"], "ans_acc": 0.10714285714285714, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.08450704225352113, "path_precision": 0.3, "path_recall": 0.04918032786885246, "path_ans_f1": 0.15789473684210525, "path_ans_precision": 0.3, "path_ans_recall": 0.10714285714285714}
{"id": "WebQTest-1563", "prediction": ["# Reasoning Path:\nNorth Africa -> location.location.contains -> Sudan -> location.location.containedby -> Africa\n# Answer:\nSudan", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Sudan -> location.location.time_zones -> East Africa Time Zone\n# Answer:\nSudan", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Sudan -> location.location.partiallycontains -> m.0wg8_26\n# Answer:\nSudan", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Alexandria Governorate -> location.administrative_division.country -> Egypt\n# Answer:\nAlexandria Governorate", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Sudan -> location.location.containedby -> Arab world\n# Answer:\nSudan", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Al Sharqia Governorate -> location.location.contains -> Faqous\n# Answer:\nAl Sharqia Governorate", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Al Sharqia Governorate -> location.location.contains -> Kafr Saqr\n# Answer:\nAl Sharqia Governorate", "# Reasoning Path:\nNorth Africa -> location.location.partiallycontains -> m.0wg920z -> location.partial_containment_relationship.partially_contains -> Arab world\n# Answer:\nm.0wg920z", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Al Sharqia Governorate -> location.location.contains -> Tanis Royal Necropolis\n# Answer:\nAl Sharqia Governorate", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Al Sharqia Governorate -> location.location.people_born_here -> Abdallah Ghayth\n# Answer:\nAl Sharqia Governorate"], "ground_truth": ["Western Roman Empire", "Ptolemaic Kingdom", "Sudan", "United Arab Republic", "Rashidun Caliphate", "Caliphate of C\u00f3rdoba", "Roman Republic"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0.21052631578947364, "ans_precission": 0.4, "ans_recall": 0.14285714285714285, "path_f1": 0.21052631578947364, "path_precision": 0.4, "path_recall": 0.14285714285714285, "path_ans_f1": 0.21052631578947364, "path_ans_precision": 0.4, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-1564", "prediction": ["# Reasoning Path:\nBobby Darin -> people.deceased_person.cause_of_death -> Surgical complications -> people.cause_of_death.parent_cause_of_death -> Assassination by 'Accident'\n# Answer:\nSurgical complications", "# Reasoning Path:\nBobby Darin -> people.deceased_person.cause_of_death -> Surgical complications -> common.topic.notable_types -> Cause Of Death\n# Answer:\nSurgical complications", "# Reasoning Path:\nBobby Darin -> people.deceased_person.cause_of_death -> Surgical complications -> common.topic.notable_for -> g.12565x30z\n# Answer:\nSurgical complications", "# Reasoning Path:\nBobby Darin -> music.composer.compositions -> Boss Barracuda -> music.composition.composer -> Terry Melcher\n# Answer:\nBoss Barracuda", "# Reasoning Path:\nBobby Darin -> music.composer.compositions -> As Long as I'm Singing -> common.topic.notable_types -> Composition\n# Answer:\nAs Long as I'm Singing", "# Reasoning Path:\nBobby Darin -> music.composer.compositions -> Boss Barracuda -> music.composition.recordings -> Boss Baracuda\n# Answer:\nBoss Barracuda", "# Reasoning Path:\nBobby Darin -> music.composer.compositions -> Boss Barracuda -> common.topic.notable_for -> g.1yp3ddqfs\n# Answer:\nBoss Barracuda", "# Reasoning Path:\nBobby Darin -> music.composer.compositions -> As Long as I'm Singing -> common.topic.notable_types -> Musical Recording\n# Answer:\nAs Long as I'm Singing", "# Reasoning Path:\nBobby Darin -> music.composer.compositions -> Bill Bailey -> music.recording.releases -> Darin at the Copa\n# Answer:\nBill Bailey", "# Reasoning Path:\nBobby Darin -> music.composer.compositions -> Bill Bailey -> music.recording.releases -> The Bobby Darin Story\n# Answer:\nBill Bailey"], "ground_truth": ["Surgical complications"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1565", "prediction": ["# Reasoning Path:\nHouston Dynamo -> sports.sports_team.arena_stadium -> BBVA Compass Stadium -> common.topic.notable_types -> Sports Facility\n# Answer:\nBBVA Compass Stadium", "# Reasoning Path:\nHouston Dynamo -> sports.sports_team.arena_stadium -> BBVA Compass Stadium -> common.topic.article -> m.0413pb7\n# Answer:\nBBVA Compass Stadium", "# Reasoning Path:\nHouston Dynamo -> sports.sports_team.arena_stadium -> BBVA Compass Stadium -> location.location.containedby -> Houston\n# Answer:\nBBVA Compass Stadium", "# Reasoning Path:\nHouston Dynamo -> sports.sports_team.location -> Houston -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nHouston", "# Reasoning Path:\nHouston Dynamo -> sports.sports_team.location -> Houston -> base.wikipedia_infobox.settlement.area_code -> Area codes 281, 346, 713, and 832\n# Answer:\nHouston", "# Reasoning Path:\nHouston Dynamo -> sports.sports_team.arena_stadium -> BBVA Compass Stadium -> location.location.containedby -> East Downtown Houston\n# Answer:\nBBVA Compass Stadium", "# Reasoning Path:\nHouston Dynamo -> sports.sports_team.arena_stadium -> BBVA Compass Stadium -> location.location.containedby -> East End\n# Answer:\nBBVA Compass Stadium", "# Reasoning Path:\nHouston Dynamo -> sports.sports_team.location -> Houston -> location.location.containedby -> Area codes 281, 346, 713, and 832\n# Answer:\nHouston", "# Reasoning Path:\nHouston Dynamo -> soccer.football_team.player_statistics -> m.0w8__hc -> soccer.football_player_stats.player -> Julius James\n# Answer:\nm.0w8__hc", "# Reasoning Path:\nHouston Dynamo -> sports.sports_team.location -> Houston -> location.location.containedby -> Texas\n# Answer:\nHouston"], "ground_truth": ["Houston"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1566", "prediction": ["# Reasoning Path:\nJohn Mayer -> music.guitarist.guitars_played -> Fender Stratocaster -> music.guitar.guitarists -> Eric Clapton\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nJohn Mayer -> common.topic.webpage -> http://www.discogs.com/artist/John+Mayer -> common.webpage.category -> Topic Webpage\n# Answer:\nhttp://www.discogs.com/artist/John+Mayer", "# Reasoning Path:\nJohn Mayer -> tv.tv_actor.guest_roles -> m.0bv_4q4 -> tv.tv_guest_role.episodes_appeared_in -> Show #2662\n# Answer:\nm.0bv_4q4", "# Reasoning Path:\nJohn Mayer -> common.topic.webpage -> m.09w5t6j -> common.webpage.resource -> Celebrity date night at John Mayer's 'revue'\n# Answer:\nm.09w5t6j", "# Reasoning Path:\nJohn Mayer -> common.topic.webpage -> http://www.discogs.com/artist/John+Mayer -> common.webpage.resource -> m.0bm4m94\n# Answer:\nhttp://www.discogs.com/artist/John+Mayer", "# Reasoning Path:\nJohn Mayer -> common.topic.webpage -> m.09w5t6j -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w5t6j", "# Reasoning Path:\nJohn Mayer -> common.topic.webpage -> m.09w6d63 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w6d63", "# Reasoning Path:\nJohn Mayer -> tv.tv_actor.guest_roles -> m.0bv_shf -> tv.tv_guest_role.episodes_appeared_in -> Dave Chapelle, John Mayer\n# Answer:\nm.0bv_shf", "# Reasoning Path:\nJohn Mayer -> common.topic.webpage -> m.09w6d63 -> common.webpage.resource -> John Mayer taps newcomers Colbie Caillat and Brett Dennen to open his summer tour\n# Answer:\nm.09w6d63", "# Reasoning Path:\nJohn Mayer -> tv.tv_actor.guest_roles -> m.0bvt_7s -> tv.tv_guest_role.episodes_appeared_in -> John Mayer / Buddy Guy and Double Trouble\n# Answer:\nm.0bvt_7s"], "ground_truth": ["Fender Stratocaster"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1567", "prediction": ["# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9g\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.risk_factors -> African American\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9q\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.risk_factors -> Chronic pancreatitis\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9g\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> African American\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.risk_factors -> Diabetes mellitus\n# Answer:\nPancreatic cancer"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1568", "prediction": ["# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgmw -> education.education.institution -> Columbia University\n# Answer:\nm.02nqgmw", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgp6 -> education.education.institution -> Punahou School\n# Answer:\nm.02nqgp6", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> education.education.institution -> Occidental College\n# Answer:\nm.02nqgpk", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgmw -> education.education.degree -> Bachelor of Arts\n# Answer:\nm.02nqgmw", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.subjects -> United States elections, 2012\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgp6 -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nm.02nqgp6", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> freebase.valuenotation.has_value -> Degree\n# Answer:\nm.02nqgpk", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Writer's Journey to Selma, Alabama -> book.written_work.subjects -> Selma\n# Answer:\nA Writer's Journey to Selma, Alabama", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.subjects -> 2012 Presidential debate\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.author -> Aberjhani\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares"], "ground_truth": ["Columbia University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1569", "prediction": ["# Reasoning Path:\nJamaica -> location.country.currency_used -> Jamaican dollar -> common.topic.notable_types -> Currency\n# Answer:\nJamaican dollar", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Hurricane Dean -> meteorology.tropical_cyclone.affected_areas -> Belize\n# Answer:\nHurricane Dean", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Hurricane Dean -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\nHurricane Dean", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Hurricane Dean -> meteorology.tropical_cyclone.affected_areas -> Cayman Islands\n# Answer:\nHurricane Dean", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Louisiana\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Hurricane Dean -> meteorology.tropical_cyclone.affected_areas -> Cuba\n# Answer:\nHurricane Dean"], "ground_truth": ["Jamaican dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-157", "prediction": ["# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> Ventura County -> location.location.containedby -> California\n# Answer:\nVentura County", "# Reasoning Path:\nMission San Buenaventura -> common.topic.article -> m.0gbsh\n# Answer:\nm.0gbsh", "# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> Ventura County -> location.location.containedby -> Greater Los Angeles Area\n# Answer:\nVentura County", "# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> 93001 -> location.location.containedby -> Ventura County\n# Answer:\n93001", "# Reasoning Path:\nMission San Buenaventura -> common.topic.notable_for -> g.1259zm5db\n# Answer:\ng.1259zm5db", "# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> Ventura County -> location.location.containedby -> United States of America\n# Answer:\nVentura County", "# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> Ventura County -> location.location.people_born_here -> M. Ward\n# Answer:\nVentura County", "# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> Ventura County -> location.statistical_region.population -> g.11b66gyw72\n# Answer:\nVentura County", "# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> Ventura County -> location.location.people_born_here -> Alexander Agate\n# Answer:\nVentura County", "# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> 93001 -> location.location.geolocation -> m.03dnfyg\n# Answer:\n93001"], "ground_truth": ["Ventura County", "93001"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1570", "prediction": ["# Reasoning Path:\nArizona Cardinals -> sports.sports_team.arena_stadium -> University of Phoenix Stadium -> location.location.containedby -> Glendale\n# Answer:\nUniversity of Phoenix Stadium", "# Reasoning Path:\nArizona Cardinals -> sports.sports_team.location -> Arizona -> location.us_state.capital -> Phoenix\n# Answer:\nArizona", "# Reasoning Path:\nArizona Cardinals -> sports.sports_team.arena_stadium -> University of Phoenix Stadium -> common.topic.image -> UofPStadiumLogo\n# Answer:\nUniversity of Phoenix Stadium", "# Reasoning Path:\nArizona Cardinals -> sports.sports_team.arena_stadium -> University of Phoenix Stadium -> location.location.events -> 2007 BCS National Championship Game\n# Answer:\nUniversity of Phoenix Stadium", "# Reasoning Path:\nArizona Cardinals -> sports.sports_team.location -> Arizona -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nArizona", "# Reasoning Path:\nArizona Cardinals -> sports.sports_team.location -> Arizona -> law.court_jurisdiction_area.courts -> United States Bankruptcy Court, District of Arizona\n# Answer:\nArizona", "# Reasoning Path:\nArizona Cardinals -> sports.sports_team.arena_stadium -> University of Phoenix Stadium -> location.location.events -> 2007 Fiesta Bowl\n# Answer:\nUniversity of Phoenix Stadium", "# Reasoning Path:\nArizona Cardinals -> sports.sports_team.arena_stadium -> University of Phoenix Stadium -> location.location.events -> 2008 Fiesta Bowl\n# Answer:\nUniversity of Phoenix Stadium", "# Reasoning Path:\nArizona Cardinals -> american_football.football_team.away_games -> Arizona Cardinals at Carolina Panthers, 2008-10-26 -> american_football.football_game.receiving -> m.07948d4\n# Answer:\nArizona Cardinals at Carolina Panthers, 2008-10-26", "# Reasoning Path:\nArizona Cardinals -> american_football.football_team.away_games -> Arizona Cardinals at Carolina Panthers, 2009-01-10 -> american_football.football_game.receiving -> m.0794b1q\n# Answer:\nArizona Cardinals at Carolina Panthers, 2009-01-10"], "ground_truth": ["Arizona"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1571", "prediction": ["# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> Newton -> astronomy.extraterrestrial_location.geolocation -> m.0w2sw5y\n# Answer:\nNewton", "# Reasoning Path:\nIsaac Newton -> base.argumentmaps.innovator.original_ideas -> Newton's law of universal gravitation -> common.topic.article -> m.01kffv\n# Answer:\nNewton's law of universal gravitation", "# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> Newton -> common.topic.notable_for -> g.12552h6gp\n# Answer:\nNewton", "# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> 8000 Isaac Newton -> astronomy.orbital_relationship.orbits -> Sun\n# Answer:\n8000 Isaac Newton", "# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> 8000 Isaac Newton -> common.topic.notable_types -> Asteroid\n# Answer:\n8000 Isaac Newton", "# Reasoning Path:\nIsaac Newton -> base.argumentmaps.innovator.original_ideas -> Newton's law of universal gravitation -> common.topic.image -> Isaacnewton\n# Answer:\nNewton's law of universal gravitation", "# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> Newton -> common.topic.notable_for -> g.1257032kq\n# Answer:\nNewton", "# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> Isaac Newton Institute -> common.topic.article -> m.036_xn\n# Answer:\nIsaac Newton Institute", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.0101ddrk\n# Answer:\nPhysicist", "# Reasoning Path:\nIsaac Newton -> base.argumentmaps.innovator.original_ideas -> Newton's law of universal gravitation -> common.topic.image -> WMAP image of the (extremely tiny) anisotropies in the cosmic background radiation\n# Answer:\nNewton's law of universal gravitation"], "ground_truth": ["Newton's law of universal gravitation"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1572", "prediction": ["# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Alcatraz Island -> base.usnris.nris_listing.significance_level -> National\n# Answer:\nAlcatraz Island", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Palace of the Legion of Honor -> architecture.structure.architectural_style -> Neoclassical architecture\n# Answer:\nPalace of the Legion of Honor", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Golden Gate Park -> travel.travel_destination.tourist_attractions -> AIDS Memorial Grove\n# Answer:\nGolden Gate Park", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Palace of the Legion of Honor -> architecture.structure.architect -> Edward Larrabee Barnes\n# Answer:\nPalace of the Legion of Honor", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Alcatraz Island -> common.topic.notable_types -> Tourist attraction\n# Answer:\nAlcatraz Island", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Alcatraz Island -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nAlcatraz Island", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Golden Gate Park -> travel.travel_destination.tourist_attractions -> California Academy of Sciences\n# Answer:\nGolden Gate Park", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Palace of the Legion of Honor -> architecture.structure.architect -> George Applegarth\n# Answer:\nPalace of the Legion of Honor", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Golden Gate Park -> travel.travel_destination.tourist_attractions -> Conservatory of Flowers\n# Answer:\nGolden Gate Park", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Golden Gate Park -> location.location.containedby -> California\n# Answer:\nGolden Gate Park"], "ground_truth": ["Exploratorium", "Cartoon Art Museum", "Consulate General of Mexico, San Francisco", "Union Square", "Haas-Lilienthal House", "Golden Gate Bridge", "Museum of the African Diaspora", "Angel Island", "San Francisco Ferry Building", "Palace of the Legion of Honor", "Asian Art Museum of San Francisco", "Camera Obscura", "Yerba Buena Center for the Arts", "Golden Gate Park", "San Francisco Museum of Modern Art", "Lombard Street", "San Francisco Fire Department Museum", "Travefy", "San Francisco cable car system", "San Francisco Railway Museum", "Fisherman's Wharf", "San Francisco City Hall", "Ripley's Believe It or Not! Museum", "Japanese Tea Garden", "Ghirardelli Square", "Twin Peaks", "Chinatown", "Contemporary Jewish Museum", "Alcatraz Island", "Mus\u00e9e M\u00e9canique", "Presidio of San Francisco", "Coit Tower", "St. Regis Museum Tower", "Crissy Field", "Baker Beach"], "ans_acc": 0.08571428571428572, "ans_hit": 1, "ans_f1": 0.15789473684210528, "ans_precission": 1.0, "ans_recall": 0.08571428571428572, "path_f1": 0.15789473684210528, "path_precision": 1.0, "path_recall": 0.08571428571428572, "path_ans_f1": 0.15789473684210528, "path_ans_precision": 1.0, "path_ans_recall": 0.08571428571428572}
{"id": "WebQTest-1573", "prediction": ["# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Sara Murphy -> influence.influence_node.influenced -> Archibald MacLeish\n# Answer:\nSara Murphy", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Sara Murphy -> influence.influence_node.influenced -> Dorothy Parker\n# Answer:\nSara Murphy", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Sara Murphy -> common.topic.article -> m.03p_c3\n# Answer:\nSara Murphy", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Diego Vel\u00e1zquez -> influence.influence_node.influenced -> Claudio Bravo\n# Answer:\nDiego Vel\u00e1zquez", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Sara Murphy -> influence.influence_node.influenced -> Ernest Hemingway\n# Answer:\nSara Murphy", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Sara Murphy -> people.person.spouse_s -> m.04jtg5c\n# Answer:\nSara Murphy", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Diego Vel\u00e1zquez -> influence.influence_node.influenced -> Francis Bacon\n# Answer:\nDiego Vel\u00e1zquez", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Diego Vel\u00e1zquez -> people.person.nationality -> Spain\n# Answer:\nDiego Vel\u00e1zquez", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Eug\u00e8ne Delacroix -> influence.influence_node.influenced -> Vincent van Gogh\n# Answer:\nEug\u00e8ne Delacroix", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Diego Vel\u00e1zquez -> influence.influence_node.influenced -> Salvador Domingo Felipe Jacinto Dal\u00ed i Dom\u00e8nech\n# Answer:\nDiego Vel\u00e1zquez"], "ground_truth": ["Diego Vel\u00e1zquez", "Eug\u00e8ne Delacroix", "Francisco Goya", "\u00c9douard Manet", "Vincent van Gogh", "Henri Rousseau", "Paul Gauguin", "Alfred Jarry", "Jean-Auguste-Dominique Ingres", "Henri de Toulouse-Lautrec", "Paul C\u00e9zanne", "Sara Murphy"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 1.0, "ans_recall": 0.25, "path_f1": 0.4, "path_precision": 1.0, "path_recall": 0.25, "path_ans_f1": 0.5, "path_ans_precision": 1.0, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1574", "prediction": ["# Reasoning Path:\nPennsylvania -> location.location.time_zones -> Eastern Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nPennsylvania -> location.location.time_zones -> UTC\u221205:00 -> common.topic.notable_types -> Time Zone\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nPennsylvania -> location.location.time_zones -> UTC\u221205:00 -> common.topic.notable_for -> g.125fnb_r1\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.title -> Director, Office of Health Care Reform\n# Answer:\nm.04ds08c", "# Reasoning Path:\nPennsylvania -> location.location.time_zones -> UTC\u221205:00 -> common.topic.image -> Timezones2008 UTC-5\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.person -> Rosemarie B. Greco\n# Answer:\nm.04ds08c", "# Reasoning Path:\nPennsylvania -> common.topic.webpage -> m.04lsvtk -> common.webpage.resource -> m.0blz_lk\n# Answer:\nm.04lsvtk", "# Reasoning Path:\nPennsylvania -> common.topic.webpage -> m.09wcxfl -> common.webpage.resource -> Exclusive: Jon Gosselin tells EW 'I am not guilty of cheating on my wife'\n# Answer:\nm.09wcxfl", "# Reasoning Path:\nPennsylvania -> common.topic.webpage -> m.04lsvtk -> common.webpage.category -> Official Website\n# Answer:\nm.04lsvtk", "# Reasoning Path:\nPennsylvania -> common.topic.webpage -> m.09wcyd_ -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09wcyd_"], "ground_truth": ["UTC\u221205:00", "Eastern Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1575", "prediction": ["# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> common.topic.subjects -> App Store\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> organization.organization_partnership.members -> Redcorus\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Chrome -> common.topic.subjects -> Google Apps for Work\n# Answer:\nGoogle Chrome", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> common.topic.subject_of -> Google Chrome\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> common.topic.subjects -> Google Apps for Work -> common.topic.subjects -> App Store\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> common.topic.subjects -> Gmail\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> common.topic.subjects -> Google Apps for Work -> common.topic.subject_of -> Google Chrome\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> common.topic.subjects -> Google Apps for Work -> organization.organization_partnership.members -> Redcorus\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> common.topic.subjects -> Google Sheets -> common.topic.subjects -> Google Apps for Work\n# Answer:\nGoogle Sheets", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> common.topic.subject_of -> Google Docs, Sheets, and Slides\n# Answer:\nGoogle Apps for Work"], "ground_truth": ["Nexus S", "Nexus Q", "Google Maps", "Google Earth", "Google Classroom", "Google Buzz", "Apache Wave", "Nexus 7", "Google Docs, Sheets, and Slides", "Google Glass", "Google Drive", "Chromecast", "Nexus 10"], "ans_acc": 0.07692307692307693, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.08695652173913043, "path_ans_precision": 0.1, "path_ans_recall": 0.07692307692307693}
{"id": "WebQTest-1577", "prediction": ["# Reasoning Path:\nCliff Lee -> sports.pro_athlete.teams -> m.0j2qj2k -> sports.sports_team_roster.team -> Cleveland Indians\n# Answer:\nm.0j2qj2k", "# Reasoning Path:\nCliff Lee -> baseball.baseball_player.batting_stats -> m.06r74yq -> baseball.batting_statistics.team -> Cleveland Indians\n# Answer:\nm.06r74yq", "# Reasoning Path:\nCliff Lee -> sports.pro_athlete.teams -> m.0j2s9p9 -> sports.sports_team_roster.team -> Philadelphia Phillies\n# Answer:\nm.0j2s9p9", "# Reasoning Path:\nCliff Lee -> sports.pro_athlete.teams -> m.0j2qj2k -> sports.sports_team_roster.position -> Pitcher\n# Answer:\nm.0j2qj2k", "# Reasoning Path:\nCliff Lee -> baseball.baseball_player.batting_stats -> m.06r74_5 -> baseball.batting_statistics.team -> Cleveland Indians\n# Answer:\nm.06r74_5", "# Reasoning Path:\nCliff Lee -> baseball.baseball_player.batting_stats -> m.06r74yq -> baseball.batting_statistics.season -> 2004 Major League Baseball season\n# Answer:\nm.06r74yq", "# Reasoning Path:\nCliff Lee -> baseball.baseball_player.batting_stats -> m.06r74vn -> baseball.batting_statistics.team -> Cleveland Indians\n# Answer:\nm.06r74vn", "# Reasoning Path:\nCliff Lee -> people.person.profession -> Baseball player -> common.topic.notable_types -> Profession\n# Answer:\nBaseball player", "# Reasoning Path:\nCliff Lee -> sports.pro_athlete.teams -> m.0j2s9p9 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0j2s9p9", "# Reasoning Path:\nCliff Lee -> sports.pro_athlete.teams -> m.0j2s9p9 -> sports.sports_team_roster.position -> Pitcher\n# Answer:\nm.0j2s9p9"], "ground_truth": ["Philadelphia Phillies"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1580", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> American Samoa\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\ng.125_r6z32", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Germany\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Germany\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guam\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nm.010r7f0z", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Mexico\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Marshall Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> periodicals.newspaper_circulation_area.newspapers -> American Free Press -> book.periodical.language -> English Language\n# Answer:\nAmerican Free Press"], "ground_truth": ["Constitutional republic", "Presidential system", "Federal republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1582", "prediction": ["# Reasoning Path:\nHong Kong -> location.location.time_zones -> Hong Kong Time Zone -> time.time_zone.locations_in_this_time_zone -> Asia\n# Answer:\nHong Kong Time Zone", "# Reasoning Path:\nHong Kong -> location.location.time_zones -> Hong Kong Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nHong Kong Time Zone", "# Reasoning Path:\nHong Kong -> location.statistical_region.gdp_nominal_per_capita -> g.11b60qhw67\n# Answer:\ng.11b60qhw67", "# Reasoning Path:\nHong Kong -> location.location.time_zones -> Hong Kong Time Zone -> time.time_zone.locations_in_this_time_zone -> Central and Western District\n# Answer:\nHong Kong Time Zone", "# Reasoning Path:\nHong Kong -> location.statistical_region.co2_emissions_per_capita -> g.1245_2gxn\n# Answer:\ng.1245_2gxn", "# Reasoning Path:\nHong Kong -> location.location.time_zones -> Hong Kong Time Zone -> time.time_zone.locations_in_this_time_zone -> Cheung Chau\n# Answer:\nHong Kong Time Zone", "# Reasoning Path:\nHong Kong -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6g4jl\n# Answer:\ng.12tb6g4jl", "# Reasoning Path:\nHong Kong -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc37pmj\n# Answer:\ng.1hhc37pmj", "# Reasoning Path:\nHong Kong -> location.statistical_region.co2_emissions_per_capita -> g.1245_3w31\n# Answer:\ng.1245_3w31", "# Reasoning Path:\nHong Kong -> location.statistical_region.co2_emissions_per_capita -> g.1245__rsm\n# Answer:\ng.1245__rsm"], "ground_truth": ["Hong Kong Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1583", "prediction": ["# Reasoning Path:\nTaylor Lautner -> award.award_winner.awards_won -> m.0kb9_qw -> award.award_honor.honored_for -> Abduction\n# Answer:\nm.0kb9_qw", "# Reasoning Path:\nTaylor Lautner -> award.award_winner.awards_won -> m.0kb9_qw -> award.award_honor.ceremony -> 2012 Teen Choice Awards\n# Answer:\nm.0kb9_qw", "# Reasoning Path:\nTaylor Lautner -> award.award_winner.awards_won -> m.01086803 -> award.award_honor.honored_for -> Twilight\n# Answer:\nm.01086803", "# Reasoning Path:\nTaylor Lautner -> award.award_nominee.award_nominations -> m.0z837pz -> award.award_nomination.nominated_for -> Abduction\n# Answer:\nm.0z837pz", "# Reasoning Path:\nTaylor Lautner -> award.award_nominee.award_nominations -> m.0j2vjzw -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nm.0j2vjzw", "# Reasoning Path:\nTaylor Lautner -> award.award_winner.awards_won -> m.09rpwdk -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nm.09rpwdk", "# Reasoning Path:\nTaylor Lautner -> award.award_nominee.award_nominations -> m.0z837pz -> award.award_nomination.award -> Teen Choice Award for Choice Movie Actor: Drama/Action Adventure\n# Answer:\nm.0z837pz", "# Reasoning Path:\nTaylor Lautner -> award.award_winner.awards_won -> m.01086803 -> award.award_honor.award -> Scream Awards for Male Breakout Performance\n# Answer:\nm.01086803", "# Reasoning Path:\nTaylor Lautner -> award.award_winner.awards_won -> m.09rpwdk -> award.award_honor.ceremony -> 36th People's Choice Awards\n# Answer:\nm.09rpwdk", "# Reasoning Path:\nTaylor Lautner -> award.award_nominee.award_nominations -> m.0j2vjzw -> award.award_nomination.nominated_for -> Abduction\n# Answer:\nm.0j2vjzw"], "ground_truth": ["Tracers", "The Twilight Saga: Breaking Dawn - Part 1", "The Nick and Jessica Variety Hour", "Grown Ups 2", "The Twilight Saga: New Moon", "The Twilight Saga: Breaking Dawn - Part 2", "Abduction", "Run the Tide", "Eclipse", "Twilight", "Northern Lights", "Cheaper by the Dozen 2", "Valentine's Day", "Field of Dreams 2: Lockout", "The Ridiculous Six", "He's a Bully, Charlie Brown", "The Adventures of Sharkboy and Lavagirl", "Incarceron", "Shadow Fury"], "ans_acc": 0.15789473684210525, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.13157894736842105, "path_precision": 0.5, "path_recall": 0.07575757575757576, "path_ans_f1": 0.23999999999999996, "path_ans_precision": 0.5, "path_ans_recall": 0.15789473684210525}
{"id": "WebQTest-1584", "prediction": ["# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Complications from pneumonia -> people.cause_of_death.people -> Aram Haigaz\n# Answer:\nComplications from pneumonia", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Complications from pneumonia -> common.topic.notable_for -> g.1yl5vldtz\n# Answer:\nComplications from pneumonia", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Complications from pneumonia -> people.cause_of_death.parent_cause_of_death -> Pneumonia\n# Answer:\nComplications from pneumonia", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Complications from pneumonia -> people.cause_of_death.people -> Blanca Vicu\u00f1a Ardohain\n# Answer:\nComplications from pneumonia", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Complications from pneumonia -> people.cause_of_death.people -> Charlton Heston\n# Answer:\nComplications from pneumonia", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> medicine.disease.symptoms -> Cutaneous manifestations of sarcoidosis\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> common.topic.subject_of -> Del Sorbo Antonio\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> common.topic.image -> 244.mac.bernie.092706.jpg -> common.image.size -> m.02hsygv\n# Answer:\n244.mac.bernie.092706.jpg", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> medicine.disease.symptoms -> Garland's triad\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> medicine.disease_cause.diseases -> Erythema nodosum\n# Answer:\nSarcoidosis"], "ground_truth": ["Complications from pneumonia", "Sarcoidosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1585", "prediction": ["# Reasoning Path:\nSpike Spiegel -> base.schemastaging.tv_character_extra.regular_dubbing_performances -> m.0nh3wqf -> base.schemastaging.tv_star_dubbing_performance.language -> English Language\n# Answer:\nm.0nh3wqf", "# Reasoning Path:\nSpike Spiegel -> base.schemastaging.tv_character_extra.regular_dubbing_performances -> m.0nh3wqf -> base.schemastaging.tv_star_dubbing_performance.program -> Cowboy Bebop\n# Answer:\nm.0nh3wqf", "# Reasoning Path:\nSpike Spiegel -> tv.tv_character.appeared_in_tv_program -> m.0gj1nnq -> tv.regular_tv_appearance.series -> Cowboy Bebop\n# Answer:\nm.0gj1nnq", "# Reasoning Path:\nSpike Spiegel -> film.film_character.portrayed_in_films -> m.03lg03s -> film.performance.actor -> K\u014dichi Yamadera\n# Answer:\nm.03lg03s", "# Reasoning Path:\nSpike Spiegel -> tv.tv_character.appeared_in_tv_program -> m.0gj1nnq -> tv.regular_tv_appearance.special_performance_type -> Voice acting in Japan\n# Answer:\nm.0gj1nnq", "# Reasoning Path:\nSpike Spiegel -> film.film_character.portrayed_in_films -> m.03lg03s -> film.performance.film -> Cowboy Bebop: The Movie\n# Answer:\nm.03lg03s", "# Reasoning Path:\nSpike Spiegel -> film.film_character.portrayed_in_films -> m.03lg034 -> film.performance.actor -> Steven Blum\n# Answer:\nm.03lg034", "# Reasoning Path:\nSpike Spiegel -> tv.tv_character.appeared_in_tv_program -> m.0gj1nnq -> tv.regular_tv_appearance.actor -> K\u014dichi Yamadera\n# Answer:\nm.0gj1nnq", "# Reasoning Path:\nSpike Spiegel -> film.film_character.portrayed_in_films -> m.03lg034 -> film.performance.film -> Cowboy Bebop: The Movie\n# Answer:\nm.03lg034"], "ground_truth": ["Steven Blum", "K\u014dichi Yamadera"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.46153846153846156, "path_precision": 0.3333333333333333, "path_recall": 0.75, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-1586", "prediction": ["# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> Isabella of Portugal, Queen of Castile -> people.person.parents -> Isabel of Barcelos\n# Answer:\nIsabella of Portugal, Queen of Castile", "# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> Isabella of Portugal, Queen of Castile -> people.person.parents -> John, Constable of Portugal\n# Answer:\nIsabella of Portugal, Queen of Castile", "# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> Isabella of Portugal, Queen of Castile -> people.person.nationality -> Portugal\n# Answer:\nIsabella of Portugal, Queen of Castile", "# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> John II of Castile -> people.person.parents -> Catherine of Lancaster\n# Answer:\nJohn II of Castile", "# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> Isabella of Portugal, Queen of Castile -> people.person.children -> Alfonso, Prince of Asturias\n# Answer:\nIsabella of Portugal, Queen of Castile", "# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> John II of Castile -> people.person.parents -> Henry III of Castile\n# Answer:\nJohn II of Castile", "# Reasoning Path:\nIsabella I of Castile -> people.person.religion -> Catholicism -> religion.religion.holidays -> All Saints' Day\n# Answer:\nCatholicism", "# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> John II of Castile -> people.person.gender -> Male\n# Answer:\nJohn II of Castile", "# Reasoning Path:\nIsabella I of Castile -> symbols.name_source.namesakes -> Isabela Island -> location.location.containedby -> Isabela Canton\n# Answer:\nIsabela Island", "# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> John II of Castile -> people.person.spouse_s -> m.0j4lydq\n# Answer:\nJohn II of Castile"], "ground_truth": ["Isabella of Portugal, Queen of Castile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1587", "prediction": ["# Reasoning Path:\nChris Hemsworth -> people.person.spouse_s -> m.0g5bzqc -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0g5bzqc", "# Reasoning Path:\nChris Hemsworth -> people.person.spouse_s -> m.0g5bzqc -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.0g5bzqc", "# Reasoning Path:\nChris Hemsworth -> people.person.spouse_s -> m.0g5bzqc -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0g5bzqc", "# Reasoning Path:\nChris Hemsworth -> award.award_nominee.award_nominations -> m.010wr28j -> award.award_nomination.nominated_for -> Thor: The Dark World\n# Answer:\nm.010wr28j", "# Reasoning Path:\nChris Hemsworth -> award.award_nominee.award_nominations -> m.010wr28j -> award.award_nomination.award -> Teen Choice Award for Choice Movie Actor - Sci-Fi/Fantasy\n# Answer:\nm.010wr28j", "# Reasoning Path:\nChris Hemsworth -> award.award_nominee.award_nominations -> m.0_vwd0g -> award.award_nomination.award -> MTV Movie Award for Best Hero\n# Answer:\nm.0_vwd0g", "# Reasoning Path:\nChris Hemsworth -> tv.tv_actor.starring_roles -> m.04hbs86 -> tv.regular_tv_appearance.seasons -> Home and Away - Season 17\n# Answer:\nm.04hbs86", "# Reasoning Path:\nChris Hemsworth -> tv.tv_actor.starring_roles -> m.0bv3_0q -> tv.regular_tv_appearance.series -> Guinevere Jones\n# Answer:\nm.0bv3_0q", "# Reasoning Path:\nChris Hemsworth -> award.award_nominee.award_nominations -> m.0_vw1t_ -> award.award_nomination.award -> MTV Movie Award for Best Shirtless Performance\n# Answer:\nm.0_vw1t_", "# Reasoning Path:\nChris Hemsworth -> tv.tv_actor.starring_roles -> m.04hbs86 -> tv.regular_tv_appearance.character -> Kim Hyde\n# Answer:\nm.04hbs86"], "ground_truth": ["Elsa Pataky"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1589", "prediction": ["# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Winnipeg -> location.location.containedby -> Winnipeg Capital Region\n# Answer:\nWinnipeg", "# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Winnipeg -> location.location.containedby -> Canada\n# Answer:\nWinnipeg", "# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Winnipeg -> location.location.containedby -> Area code 204\n# Answer:\nWinnipeg", "# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Winnipeg -> location.location.events -> 1987 Reel Pride Film Festival\n# Answer:\nWinnipeg", "# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Rural Municipality of Headingley -> location.location.containedby -> Winnipeg Capital Region\n# Answer:\nRural Municipality of Headingley", "# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Winnipeg -> location.statistical_region.population -> g.11b7tlss9k\n# Answer:\nWinnipeg", "# Reasoning Path:\nMTS Iceplex -> sports.sports_facility.teams -> Winnipeg Blues -> sports.sports_team.sport -> Ice Hockey\n# Answer:\nWinnipeg Blues", "# Reasoning Path:\nMTS Iceplex -> sports.sports_facility.home_venue_for -> m.0wz2_y_ -> sports.team_venue_relationship.team -> Winnipeg Blues\n# Answer:\nm.0wz2_y_", "# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Winnipeg -> location.location.events -> 1988 Reel Pride Film Festival\n# Answer:\nWinnipeg", "# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Rural Municipality of Headingley -> location.location.containedby -> Manitoba\n# Answer:\nRural Municipality of Headingley"], "ground_truth": ["Rural Municipality of Headingley", "Winnipeg"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-159", "prediction": ["# Reasoning Path:\nHilo -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> common.topic.article -> m.02lctt\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nHilo -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nHilo -> location.location.geolocation -> m.0kbnpg\n# Answer:\nm.0kbnpg", "# Reasoning Path:\nHilo -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> common.topic.notable_for -> g.125bn2ktb\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nHilo -> location.location.people_born_here -> Faye Hanohano -> people.person.nationality -> United States of America\n# Answer:\nFaye Hanohano", "# Reasoning Path:\nHilo -> location.location.people_born_here -> Albert Spencer Wilcox -> common.topic.image -> Kauai-AlbertSpencerWilcox-beachhouse\n# Answer:\nAlbert Spencer Wilcox", "# Reasoning Path:\nHilo -> location.location.people_born_here -> Bennett N\u0101m\u0101k\u0113h\u0101 -> people.person.parents -> Kinoiki Kekaulike\n# Answer:\nBennett N\u0101m\u0101k\u0113h\u0101", "# Reasoning Path:\nHilo -> location.location.people_born_here -> Albert Spencer Wilcox -> people.person.parents -> Abner Wilcox\n# Answer:\nAlbert Spencer Wilcox", "# Reasoning Path:\nHilo -> location.location.people_born_here -> Albert Spencer Wilcox -> people.deceased_person.place_of_death -> Puhi\n# Answer:\nAlbert Spencer Wilcox", "# Reasoning Path:\nHilo -> location.location.people_born_here -> Bennett N\u0101m\u0101k\u0113h\u0101 -> people.deceased_person.place_of_burial -> Royal Mausoleum of Hawaii\n# Answer:\nBennett N\u0101m\u0101k\u0113h\u0101"], "ground_truth": ["Hawaii-Aleutian Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1590", "prediction": ["# Reasoning Path:\nIsrael -> location.statistical_region.religions -> m.0468s5y -> location.religion_percentage.religion -> Judaism\n# Answer:\nm.0468s5y", "# Reasoning Path:\nIsrael -> symbols.namesake.named_after -> Jacob -> people.person.religion -> Judaism\n# Answer:\nJacob", "# Reasoning Path:\nIsrael -> media_common.quotation.author -> Aaron Friedman -> people.person.religion -> Judaism\n# Answer:\nAaron Friedman", "# Reasoning Path:\nIsrael -> symbols.namesake.named_after -> Jacob -> people.deceased_person.place_of_death -> Egypt\n# Answer:\nJacob", "# Reasoning Path:\nIsrael -> media_common.quotation.author -> Aaron Friedman -> people.person.nationality -> United States of America\n# Answer:\nAaron Friedman"], "ground_truth": ["Judaism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.6666666666666665, "path_precision": 0.6, "path_recall": 0.75, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1591", "prediction": ["# Reasoning Path:\nBarack Obama -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Senator\n# Answer:\nPolitician", "# Reasoning Path:\nBarack Obama -> people.person.profession -> Politician -> freebase.type_profile.kind -> Title\n# Answer:\nPolitician", "# Reasoning Path:\nBarack Obama -> people.person.profession -> Politician -> type.type.expected_by -> Officeholder\n# Answer:\nPolitician", "# Reasoning Path:\nBarack Obama -> people.person.profession -> Lawyer -> common.topic.notable_types -> Profession\n# Answer:\nLawyer", "# Reasoning Path:\nBarack Obama -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> Ambassador\n# Answer:\nPolitician", "# Reasoning Path:\nBarack Obama -> people.person.profession -> Lawyer -> freebase.notability_hints.notable_for -> Profession\n# Answer:\nLawyer", "# Reasoning Path:\nBarack Obama -> people.person.profession -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor", "# Reasoning Path:\nBarack Obama -> government.politician.government_positions_held -> m.03h96h6 -> government.government_position_held.office_position_or_title -> Illinois State Senator\n# Answer:\nm.03h96h6", "# Reasoning Path:\nBarack Obama -> government.politician.government_positions_held -> m.03h9hn4 -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nm.03h9hn4", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.subjects -> United States elections, 2012\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares"], "ground_truth": ["Politician", "Lawyer", "Writer", "Author", "Law professor"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.6461538461538462, "ans_precission": 0.7, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.8470588235294118, "path_ans_precision": 0.9, "path_ans_recall": 0.8}
{"id": "WebQTest-1592", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.featured_artists -> Ludacris\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> Somebody to Love\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.artist -> Ludacris\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.role -> Vocals\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> music.composition.composer -> Andre Harris\n# Answer:\nAll Bad", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> music.album.release_type -> Single\n# Answer:\nAll Bad", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All That Matters -> music.composition.composer -> Andre Harris\n# Answer:\nAll That Matters", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All That Matters -> award.award_nominated_work.award_nominations -> m.0_vw6nn\n# Answer:\nAll That Matters", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> music.composition.composer -> Dre & Vidal\n# Answer:\nAll Bad"], "ground_truth": ["Roller Coaster", "Wait for a Minute", "Right Here", "Never Let You Go", "Beauty And A Beat", "Live My Life", "As Long as You Love Me", "Pray", "Die in Your Arms", "Home to Mama", "#thatPower", "All Bad", "Recovery", "Eenie Meenie", "Lolly", "PYD", "Beautiful", "Change Me", "First Dance", "Bad Day", "Thought Of You", "Confident", "Boyfriend", "Heartbreaker", "All Around The World", "All That Matters", "Never Say Never", "Baby", "Somebody to Love", "Turn to You (Mother's Day Dedication)", "Hold Tight", "Bigger"], "ans_acc": 0.125, "ans_hit": 1, "ans_f1": 0.1653543307086614, "ans_precission": 0.7, "ans_recall": 0.09375, "path_f1": 0.05405405405405406, "path_precision": 0.2, "path_recall": 0.03125, "path_ans_f1": 0.21621621621621623, "path_ans_precision": 0.8, "path_ans_recall": 0.125}
{"id": "WebQTest-1593", "prediction": ["# Reasoning Path:\nRiver Thames -> geography.river.origin -> Thames Head -> common.topic.notable_for -> g.1258953bg\n# Answer:\nThames Head", "# Reasoning Path:\nRiver Thames -> geography.river.origin -> Thames Head -> common.topic.article -> m.02pj_ts\n# Answer:\nThames Head", "# Reasoning Path:\nRiver Thames -> geography.river.origin -> Thames Head -> common.topic.notable_types -> Location\n# Answer:\nThames Head", "# Reasoning Path:\nRiver Thames -> geography.body_of_water.islands -> Brentford Ait -> geography.island.body_of_water -> Tideway\n# Answer:\nBrentford Ait", "# Reasoning Path:\nRiver Thames -> geography.body_of_water.islands -> Brentford Ait -> common.topic.notable_for -> g.1255whj4q\n# Answer:\nBrentford Ait", "# Reasoning Path:\nRiver Thames -> geography.body_of_water.islands -> Canvey Island -> geography.island.island_group -> British Isles\n# Answer:\nCanvey Island", "# Reasoning Path:\nRiver Thames -> geography.body_of_water.islands -> Brentford Ait -> common.topic.article -> m.03whqzr\n# Answer:\nBrentford Ait", "# Reasoning Path:\nRiver Thames -> geography.body_of_water.islands -> Chiswick Eyot -> location.location.containedby -> United Kingdom\n# Answer:\nChiswick Eyot", "# Reasoning Path:\nRiver Thames -> geography.body_of_water.islands -> Canvey Island -> common.topic.notable_for -> g.125cbkj4r\n# Answer:\nCanvey Island", "# Reasoning Path:\nRiver Thames -> location.location.containedby -> England -> location.location.containedby -> Great Britain\n# Answer:\nEngland"], "ground_truth": ["Thames Head"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1595", "prediction": ["# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nm.0z83xlj", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83n09 -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nm.0z83n09", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z87qmv -> award.award_nomination.nominated_for -> Water for Elephants\n# Answer:\nm.0z87qmv", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.award_nominee -> Kristen Stewart\n# Answer:\nm.0z83xlj", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.ceremony -> 2012 Teen Choice Awards\n# Answer:\nm.0z83xlj", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83n09 -> award.award_nomination.award -> Teen Choice Award for Choice Movie Actor: Romance\n# Answer:\nm.0z83n09", "# Reasoning Path:\nRobert Pattinson -> award.award_winner.awards_won -> m.0100q0tx -> award.award_honor.honored_for -> The Twilight Saga: Breaking Dawn - Part 2\n# Answer:\nm.0100q0tx", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z87qmv -> award.award_nomination.ceremony -> 2011 Teen Choice Awards\n# Answer:\nm.0z87qmv", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z87qmv -> freebase.valuenotation.is_reviewed -> Award\n# Answer:\nm.0z87qmv", "# Reasoning Path:\nRobert Pattinson -> people.person.sibling_s -> m.07sh9v2 -> people.sibling_relationship.sibling -> Lizzy Pattinson\n# Answer:\nm.07sh9v2"], "ground_truth": ["Bel Ami", "Hold on to Me", "Little Ashes", "The Summer House", "Mission: Blacklist", "Harry Potter and the Goblet of Fire", "The Twilight Saga: Breaking Dawn - Part 1", "The Twilight Saga: New Moon", "The Twilight Saga: Breaking Dawn - Part 2", "The Bad Mother's Handbook", "The Haunted Airman", "Harry Potter and the Order of the Phoenix", "Eclipse", "Twilight", "Cosmopolis", "Remember Me", "Queen of the Desert", "Dark Kingdom: The Dragon King", "Maps to the Stars", "Vanity Fair", "The Rover", "The Childhood of a Leader", "Love & Distrust", "Unbound Captives", "How to Be", "Life", "Water for Elephants"], "ans_acc": 0.14814814814814814, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.0784313725490196, "path_precision": 0.4, "path_recall": 0.043478260869565216, "path_ans_f1": 0.21621621621621623, "path_ans_precision": 0.4, "path_ans_recall": 0.14814814814814814}
{"id": "WebQTest-1596", "prediction": ["# Reasoning Path:\nPakistan -> location.country.languages_spoken -> Hindustani language -> language.human_language.language_family -> Indo-Aryan languages\n# Answer:\nHindustani language", "# Reasoning Path:\nPakistan -> location.country.languages_spoken -> Hindko dialect -> language.human_language.region -> Asia\n# Answer:\nHindko dialect", "# Reasoning Path:\nPakistan -> location.country.official_language -> Urdu Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nUrdu Language", "# Reasoning Path:\nPakistan -> location.country.languages_spoken -> Hindustani language -> common.topic.notable_types -> Human Language\n# Answer:\nHindustani language", "# Reasoning Path:\nPakistan -> location.country.languages_spoken -> Urdu Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nUrdu Language", "# Reasoning Path:\nPakistan -> location.country.languages_spoken -> Hindustani language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nHindustani language", "# Reasoning Path:\nPakistan -> location.country.languages_spoken -> Hindustani language -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nHindustani language", "# Reasoning Path:\nPakistan -> location.country.official_language -> Urdu Language -> common.topic.notable_types -> Human Language\n# Answer:\nUrdu Language", "# Reasoning Path:\nPakistan -> location.statistical_region.consumer_price_index -> g.11b60w91bg\n# Answer:\ng.11b60w91bg", "# Reasoning Path:\nPakistan -> location.country.languages_spoken -> Hindko dialect -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nHindko dialect"], "ground_truth": ["Urdu Language", "English Language"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.37499999999999994, "ans_precission": 0.3, "ans_recall": 0.5, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.37499999999999994, "path_ans_precision": 0.3, "path_ans_recall": 0.5}
{"id": "WebQTest-1597", "prediction": ["# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Athletic Shoes & Apparel -> business.industry.companies -> Crocs\n# Answer:\nAthletic Shoes & Apparel", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Athletic Shoes & Apparel -> common.topic.notable_types -> Industry\n# Answer:\nAthletic Shoes & Apparel", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Athletic Shoes & Apparel -> business.industry.companies -> Deckers Outdoor Corporation\n# Answer:\nAthletic Shoes & Apparel", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Footwear -> business.industry.companies -> AND1\n# Answer:\nFootwear", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Footwear -> common.topic.image -> High-heeled shoe\n# Answer:\nFootwear", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Athletic Shoes & Apparel -> business.industry.companies -> LaCrosse Footwear\n# Answer:\nAthletic Shoes & Apparel", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Athletic Shoes & Apparel -> business.industry.parent_industry -> Footwear Manufacturing\n# Answer:\nAthletic Shoes & Apparel", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Footwear -> fashion.garment.more_specialized_forms -> Shoe\n# Answer:\nFootwear", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Sports equipment -> business.industry.child_industry -> Fabricated Rubber Products, NEC (wet suits)\n# Answer:\nSports equipment", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Sports equipment -> business.industry.parent_industry -> Other Miscellaneous Manufacturing\n# Answer:\nSports equipment"], "ground_truth": ["Footwear", "Fashion accessory", "Sports equipment", "Athletic Shoes & Apparel", "Footwear Manufacturing", "Clothing"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1598", "prediction": ["# Reasoning Path:\nLyndon B. Johnson -> people.person.children -> Luci Baines Johnson -> people.person.children -> Patrick Lyndon Nugent\n# Answer:\nLuci Baines Johnson", "# Reasoning Path:\nLyndon B. Johnson -> people.person.children -> Luci Baines Johnson -> people.person.children -> Claudia Taylor Nugent\n# Answer:\nLuci Baines Johnson", "# Reasoning Path:\nLyndon B. Johnson -> people.person.children -> Luci Baines Johnson -> people.person.spouse_s -> m.05kg486\n# Answer:\nLuci Baines Johnson", "# Reasoning Path:\nLyndon B. Johnson -> people.person.spouse_s -> m.0j4k73z -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0j4k73z", "# Reasoning Path:\nLyndon B. Johnson -> people.person.children -> Luci Baines Johnson -> people.person.children -> Nicole Marie Nugent\n# Answer:\nLuci Baines Johnson", "# Reasoning Path:\nLyndon B. Johnson -> people.person.children -> Luci Baines Johnson -> organization.organization_member.member_of -> m.05kg44b\n# Answer:\nLuci Baines Johnson", "# Reasoning Path:\nLyndon B. Johnson -> people.person.children -> Luci Baines Johnson -> people.person.spouse_s -> m.0j4mjp9\n# Answer:\nLuci Baines Johnson", "# Reasoning Path:\nLyndon B. Johnson -> people.person.children -> Lynda Bird Johnson Robb -> people.person.children -> Catherine Lewis Robb\n# Answer:\nLynda Bird Johnson Robb", "# Reasoning Path:\nLyndon B. Johnson -> people.person.spouse_s -> m.0j4k73z -> freebase.valuenotation.is_reviewed -> Spouse\n# Answer:\nm.0j4k73z", "# Reasoning Path:\nLyndon B. Johnson -> people.person.spouse_s -> m.0j4k73z -> people.marriage.spouse -> Lady Bird Johnson\n# Answer:\nm.0j4k73z"], "ground_truth": ["Lady Bird Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.10526315789473685, "path_precision": 0.1, "path_recall": 0.1111111111111111, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1599", "prediction": ["# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_death -> Pyongyang -> location.location.contains -> Kim Il-sung Stadium\n# Answer:\nPyongyang", "# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_burial -> Kumsusan Palace of the Sun -> location.location.containedby -> Pyongyang\n# Answer:\nKumsusan Palace of the Sun", "# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_death -> Pyongyang -> common.topic.notable_types -> City/Town/Village\n# Answer:\nPyongyang", "# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_death -> Pyongyang -> location.location.contains -> Arch of Triumph\n# Answer:\nPyongyang", "# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_death -> Pyongyang -> location.location.events -> 1987 Pyongyang Film Festival\n# Answer:\nPyongyang", "# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_burial -> Kumsusan Palace of the Sun -> location.location.containedby -> North Korea\n# Answer:\nKumsusan Palace of the Sun", "# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_burial -> Kumsusan Palace of the Sun -> people.place_of_interment.interred_here -> Kim Il-sung\n# Answer:\nKumsusan Palace of the Sun", "# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_death -> Pyongyang -> location.location.contains -> Kim Chaek University of Technology\n# Answer:\nPyongyang", "# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_burial -> Kumsusan Palace of the Sun -> common.topic.article -> m.027nsbr\n# Answer:\nKumsusan Palace of the Sun", "# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_death -> Pyongyang -> location.location.events -> 1990 Pyongyang Film Festival\n# Answer:\nPyongyang"], "ground_truth": ["Pyongyang"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Scotland -> base.aareas.schema.administrative_area.administrative_children -> Falkirk\n# Answer:\nScotland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> base.aareas.schema.administrative_area.administrative_children -> Anglesey\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Scotland -> location.country.first_level_divisions -> Falkirk\n# Answer:\nScotland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> base.aareas.schema.administrative_area.administrative_children -> Blaenau Gwent\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> location.country.first_level_divisions -> Anglesey\n# Answer:\nWales"], "ground_truth": ["England", "Wales", "Scotland", "Northern Ireland"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.6, "path_precision": 0.5, "path_recall": 0.75, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-160", "prediction": ["# Reasoning Path:\nRudolf Virchow -> people.person.employment_history -> m.04hx7xj -> business.employment_tenure.company -> University of W\u00fcrzburg\n# Answer:\nm.04hx7xj", "# Reasoning Path:\nRudolf Virchow -> people.person.employment_history -> m.04hx7xp -> business.employment_tenure.company -> Humboldt University of Berlin\n# Answer:\nm.04hx7xp", "# Reasoning Path:\nRudolf Virchow -> education.academic.advisees -> Edwin Klebs -> people.person.employment_history -> m.04hqvc_\n# Answer:\nEdwin Klebs", "# Reasoning Path:\nRudolf Virchow -> base.kwebbase.kwtopic.connections_to -> carl weigert assistant to rudolf carl virchow -> base.kwebbase.kwconnection.sentence -> At the University of Berlin for a time he worked as personal assistant to the pathologist Virchow.\n# Answer:\ncarl weigert assistant to rudolf carl virchow", "# Reasoning Path:\nRudolf Virchow -> base.kwebbase.kwtopic.connections_to -> carl weigert assistant to rudolf carl virchow -> base.kwebbase.kwconnection.subject -> Karl Weigert\n# Answer:\ncarl weigert assistant to rudolf carl virchow", "# Reasoning Path:\nRudolf Virchow -> base.kwebbase.kwtopic.connections_to -> carl weigert assistant to rudolf carl virchow -> base.kwebbase.kwconnection.relation -> assistant to\n# Answer:\ncarl weigert assistant to rudolf carl virchow", "# Reasoning Path:\nRudolf Virchow -> base.kwebbase.kwtopic.connections_to -> ernst haeckel taught by rudolf carl virchow -> base.kwebbase.kwconnection.subject -> Ernst Haeckel\n# Answer:\nernst haeckel taught by rudolf carl virchow", "# Reasoning Path:\nRudolf Virchow -> base.kwebbase.kwtopic.connections_to -> cesare lombroso read work of rudolf carl virchow -> base.kwebbase.kwconnection.relation -> read work of\n# Answer:\ncesare lombroso read work of rudolf carl virchow", "# Reasoning Path:\nRudolf Virchow -> education.academic.advisees -> Edwin Klebs -> people.person.education -> m.04fv328\n# Answer:\nEdwin Klebs", "# Reasoning Path:\nRudolf Virchow -> education.academic.advisees -> Edwin Klebs -> influence.influence_node.influenced -> Arnold Klebs\n# Answer:\nEdwin Klebs"], "ground_truth": ["University of W\u00fcrzburg", "Humboldt University of Berlin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1603", "prediction": ["# Reasoning Path:\nTaoism -> religion.religion.texts -> Daozang -> common.topic.notable_types -> Religious Text\n# Answer:\nDaozang", "# Reasoning Path:\nTaoism -> religion.religion.texts -> Daozang -> base.schemastaging.context_name.pronunciation -> g.125_nd16s\n# Answer:\nDaozang", "# Reasoning Path:\nTaoism -> religion.religion.texts -> Daozang -> common.topic.notable_for -> g.1256kt21z\n# Answer:\nDaozang", "# Reasoning Path:\nTaoism -> religion.religion.texts -> Tao Te Ching -> book.book.editions -> Tao te ching\n# Answer:\nTao Te Ching", "# Reasoning Path:\nTaoism -> religion.religion.texts -> I Ching -> book.written_work.subjects -> Bagua\n# Answer:\nI Ching", "# Reasoning Path:\nTaoism -> religion.religion.texts -> I Ching -> book.written_work.subjects -> Hexagram\n# Answer:\nI Ching", "# Reasoning Path:\nTaoism -> book.book_subject.works -> Tao Te Ching -> book.book.editions -> Tao te ching\n# Answer:\nTao Te Ching", "# Reasoning Path:\nTaoism -> religion.religion.texts -> I Ching -> book.written_work.subjects -> Yellow River Map\n# Answer:\nI Ching", "# Reasoning Path:\nTaoism -> religion.religion.texts -> I Ching -> common.topic.image -> H\u00ecnh:Hado_lacthu\n# Answer:\nI Ching", "# Reasoning Path:\nTaoism -> religion.religion.texts -> Tao Te Ching -> book.book.editions -> Dao de jing\n# Answer:\nTao Te Ching"], "ground_truth": ["I Ching", "Zhuang Zhou", "Daozang", "Tao Te Ching"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-1604", "prediction": ["# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Republic of Ireland -> base.locations.countries.continent -> Europe\n# Answer:\nRepublic of Ireland", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Republic of Ireland -> base.aareas.schema.administrative_area.administrative_area_type -> Sovereign state\n# Answer:\nRepublic of Ireland", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Australia -> sports.sport_country.multi_event_tournaments_participated_in -> 2010 Commonwealth Games\n# Answer:\nAustralia", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Kingdom of Great Britain -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nKingdom of Great Britain", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Republic of Ireland -> location.country.languages_spoken -> Forth and Bargy dialect\n# Answer:\nRepublic of Ireland", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Kingdom of Great Britain -> location.location.containedby -> British Isles\n# Answer:\nKingdom of Great Britain", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Australia -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\nAustralia", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Lesotho -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\nLesotho", "# Reasoning Path:\nEnglish Language -> education.school_category.schools_of_this_kind -> Piers Midwinter -> people.person.quotations -> Learn with a NEST. Please be my GUEST. I will give you my BEST,\n# Answer:\nPiers Midwinter"], "ground_truth": ["United Kingdom", "New Zealand", "Kingdom of Great Britain", "Canada", "Republic of Ireland", "United States of America", "Australia"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5316455696202531, "ans_precission": 0.7, "ans_recall": 0.42857142857142855, "path_f1": 0.36363636363636365, "path_precision": 0.5, "path_recall": 0.2857142857142857, "path_ans_f1": 0.5316455696202531, "path_ans_precision": 0.7, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1607", "prediction": ["# Reasoning Path:\nFedEx Cup -> sports.sports_award_type.winners -> m.0n1v8cy -> sports.sports_award.season -> 2012 PGA Tour\n# Answer:\nm.0n1v8cy", "# Reasoning Path:\nFedEx Cup -> common.topic.article -> m.08q5wy\n# Answer:\nm.08q5wy", "# Reasoning Path:\nFedEx Cup -> sports.sports_award_type.winners -> m.0n1v8cy -> sports.sports_award.award_winner -> Brandt Snedeker\n# Answer:\nm.0n1v8cy", "# Reasoning Path:\nFedEx Cup -> common.topic.notable_for -> g.12559n8g_\n# Answer:\ng.12559n8g_"], "ground_truth": ["Brandt Snedeker"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1608", "prediction": ["# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.region -> Africa\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> base.rosetta.languoid.parent -> Arabic Group\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Modern Standard Arabic -> base.rosetta.languoid.parent -> Arabic Group\n# Answer:\nModern Standard Arabic", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.writing_system -> Arabic alphabet\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.writing_system -> Arabic alphabet\n# Answer:\nArabic Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Modern Standard Arabic -> language.human_language.language_family -> Afroasiatic languages\n# Answer:\nModern Standard Arabic", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\ng.12cp_k6sh", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Modern Standard Arabic -> common.topic.notable_types -> Human Language\n# Answer:\nModern Standard Arabic", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.dialects -> Arabic, Sudanese Spoken Language\n# Answer:\nArabic Language", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\ng.1hhc3_4cn"], "ground_truth": ["Siwi Language", "Egyptian Arabic", "Bedawi Arabic", "Sa'idi Arabic"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1609", "prediction": ["# Reasoning Path:\nALF -> tv.tv_program.program_creator -> Paul Fusco -> people.person.profession -> Puppeteer\n# Answer:\nPaul Fusco", "# Reasoning Path:\nALF -> tv.tv_program.program_creator -> Paul Fusco -> people.person.profession -> Television producer\n# Answer:\nPaul Fusco", "# Reasoning Path:\nALF -> tv.tv_program.program_creator -> Paul Fusco -> tv.tv_actor.starring_roles -> m.04m3yb2\n# Answer:\nPaul Fusco", "# Reasoning Path:\nALF -> tv.tv_program.program_creator -> Tom Patchett -> common.topic.notable_for -> g.1259mrnmt\n# Answer:\nTom Patchett", "# Reasoning Path:\nALF -> award.award_nominated_work.award_nominations -> m.0njljv9 -> award.award_nomination.award -> People's Choice Award for Favorite New TV Comedy\n# Answer:\nm.0njljv9", "# Reasoning Path:\nALF -> tv.tv_program.program_creator -> Paul Fusco -> people.person.profession -> Voice Actor\n# Answer:\nPaul Fusco", "# Reasoning Path:\nALF -> tv.tv_program.program_creator -> Paul Fusco -> tv.tv_director.episodes_directed -> Funeral for a Friend\n# Answer:\nPaul Fusco", "# Reasoning Path:\nALF -> tv.tv_program.program_creator -> Paul Fusco -> tv.tv_actor.starring_roles -> m.07y5rmb\n# Answer:\nPaul Fusco", "# Reasoning Path:\nALF -> award.award_nominated_work.award_nominations -> m.0njljv9 -> freebase.valuenotation.has_value -> Award Nominee\n# Answer:\nm.0njljv9", "# Reasoning Path:\nALF -> award.award_nominated_work.award_nominations -> m.0sgq4fm -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Show\n# Answer:\nm.0sgq4fm"], "ground_truth": ["Paul Fusco"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-161", "prediction": ["# Reasoning Path:\nGordon Brown -> people.person.education -> m.02wp5xk -> education.education.institution -> University of Edinburgh\n# Answer:\nm.02wp5xk", "# Reasoning Path:\nGordon Brown -> people.person.education -> m.0k1qt_h -> education.education.institution -> Kirkcaldy High School\n# Answer:\nm.0k1qt_h", "# Reasoning Path:\nGordon Brown -> people.person.education -> m.0k1qvbm -> education.education.institution -> Kirkcaldy West Primary School\n# Answer:\nm.0k1qvbm", "# Reasoning Path:\nGordon Brown -> people.person.education -> m.02wp5xk -> education.education.degree -> Master of Arts\n# Answer:\nm.02wp5xk", "# Reasoning Path:\nGordon Brown -> people.person.education -> m.02wp5xk -> education.education.major_field_of_study -> History\n# Answer:\nm.02wp5xk", "# Reasoning Path:\nGordon Brown -> common.topic.image -> Gordon Brown Davos Jan 08 -> common.image.appears_in_topic_gallery -> Brown ministry\n# Answer:\nGordon Brown Davos Jan 08", "# Reasoning Path:\nGordon Brown -> common.topic.image -> Gordon Brown Davos Jan 08 -> common.image.size -> m.04y43w2\n# Answer:\nGordon Brown Davos Jan 08", "# Reasoning Path:\nGordon Brown -> common.topic.image -> Gordon Brown, the Chancellor of the Exchequer who abolished ACT and introduced the quarterly instalment r\u00e9gime in 1999 -> common.image.appears_in_topic_gallery -> United Kingdom corporation tax\n# Answer:\nGordon Brown, the Chancellor of the Exchequer who abolished ACT and introduced the quarterly instalment r\u00e9gime in 1999", "# Reasoning Path:\nGordon Brown -> people.person.profession -> Historian -> people.profession.specializations -> American Historian\n# Answer:\nHistorian", "# Reasoning Path:\nGordon Brown -> people.person.profession -> Historian -> book.book_subject.works -> Photography Exhibit Commemorates 150th Anniversary of American Civil War (part 1)\n# Answer:\nHistorian"], "ground_truth": ["University of Edinburgh"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1610", "prediction": ["# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> common.topic.notable_types -> Film character\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> base.ontologies.ontology_instance.equivalent_instances -> m.07nfg4c\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> common.topic.notable_types -> US Vice President\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> people.person.sibling_s -> m.05hlvsz\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> people.person.sibling_s -> m.05px29t\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> people.person.sibling_s -> m.05px29z\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> people.person.quotations -> A strong defense is the surest way to peace. Strength makes detente attainable. Weakness invites war, as my generationmy generationknows from four very bitter experiences. Just as Americas will for peace is second to none, so will Americas strength be second to none. We cannot rely on the forbearance of others to protect this Nation. The power and diversity of the Armed Forces, active Guard and Reserve, the resolve of our fellow citizens, the flexibility in our command to navigate international waters that remain troubled are all essential to our security. -> media_common.quotation.subjects -> Uncategorised\n# Answer:\nA strong defense is the surest way to peace. Strength makes detente attainable. Weakness invites war, as my generationmy generationknows from four very bitter experiences. Just as Americas will for peace is second to none, so will Americas strength be second to none. We cannot rely on the forbearance of others to protect this Nation. The power and diversity of the Armed Forces, active Guard and Reserve, the resolve of our fellow citizens, the flexibility in our command to navigate international waters that remain troubled are all essential to our security.", "# Reasoning Path:\nGerald Ford -> government.political_appointer.appointees -> m.01xrx8h -> government.government_position_held.office_holder -> George H. W. Bush\n# Answer:\nm.01xrx8h", "# Reasoning Path:\nGerald Ford -> government.political_appointer.appointees -> m.01z0qwq -> government.government_position_held.office_holder -> Dick Cheney\n# Answer:\nm.01z0qwq", "# Reasoning Path:\nGerald Ford -> people.person.quotations -> All of us who served in one war or another know very well that all wars are the glory and the agony of the young. -> common.topic.notable_types -> Quotation\n# Answer:\nAll of us who served in one war or another know very well that all wars are the glory and the agony of the young."], "ground_truth": ["Nelson Rockefeller"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1611", "prediction": ["# Reasoning Path:\nArmenia -> location.country.currency_used -> Armenian dram -> finance.currency.countries_used -> Nagorno-Karabakh Republic\n# Answer:\nArmenian dram", "# Reasoning Path:\nArmenia -> location.country.currency_used -> Armenian dram -> common.topic.webpage -> m.04m1q3h\n# Answer:\nArmenian dram", "# Reasoning Path:\nArmenia -> location.country.currency_used -> Armenian dram -> common.topic.notable_for -> g.125gnq5zv\n# Answer:\nArmenian dram", "# Reasoning Path:\nArmenia -> location.statistical_region.energy_use_per_capita -> g.1245_391m\n# Answer:\ng.1245_391m", "# Reasoning Path:\nArmenia -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60znv7_\n# Answer:\ng.11b60znv7_", "# Reasoning Path:\nArmenia -> location.statistical_region.energy_use_per_capita -> g.1245_f6xm\n# Answer:\ng.1245_f6xm", "# Reasoning Path:\nArmenia -> location.statistical_region.energy_use_per_capita -> g.1245_jsph\n# Answer:\ng.1245_jsph", "# Reasoning Path:\nArmenia -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fsr3\n# Answer:\ng.12tb6fsr3", "# Reasoning Path:\nArmenia -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc37pgh\n# Answer:\ng.1hhc37pgh"], "ground_truth": ["Armenian dram"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-1612", "prediction": ["# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Drawing -> education.field_of_study.students_majoring -> m.0103397j\n# Answer:\nDrawing", "# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Drawing -> education.field_of_study.students_majoring -> m.010bfdc7\n# Answer:\nDrawing", "# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Drawing -> interests.hobby.people_with_this_hobby -> David Veldt\n# Answer:\nDrawing", "# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Painting -> business.product_theme.products -> Andy Warhol Registry\n# Answer:\nPainting", "# Reasoning Path:\nRaphael -> common.topic.article -> m.02mx4l\n# Answer:\nm.02mx4l", "# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Drawing -> education.field_of_study.students_majoring -> m.010fyvc0\n# Answer:\nDrawing", "# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Auguste Rodin: Drawings & Watercolors\n# Answer:\nDrawing", "# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Drawing -> interests.hobby.people_with_this_hobby -> Mukil Elango\n# Answer:\nDrawing", "# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Painting -> business.product_theme.products -> Banksy Registry\n# Answer:\nPainting", "# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Painting -> base.descriptive_names.names.descriptive_name -> m.0109p390\n# Answer:\nPainting"], "ground_truth": ["Drawing", "Painting"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1613", "prediction": ["# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Botswana\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.main_country -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> Maltese Language -> language.human_language.region -> Europe\n# Answer:\nMaltese Language", "# Reasoning Path:\nMalta -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.statistical_region.deposit_interest_rate -> g.1hhc37h6m\n# Answer:\ng.1hhc37h6m", "# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Cyprus\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.main_country -> Kingdom of Great Britain\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Botswana\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> Australia\n# Answer:\nEnglish Language"], "ground_truth": ["Maltese Language", "English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1614", "prediction": ["# Reasoning Path:\nLouis XIV of France -> people.person.religion -> Catholicism -> religion.religion.types_of_places_of_worship -> Church\n# Answer:\nCatholicism", "# Reasoning Path:\nLouis XIV of France -> people.person.religion -> Catholicism -> religion.religion.types_of_places_of_worship -> Basilica\n# Answer:\nCatholicism", "# Reasoning Path:\nLouis XIV of France -> people.person.religion -> Catholicism -> religion.religion.types_of_places_of_worship -> Cathedral\n# Answer:\nCatholicism", "# Reasoning Path:\nLouis XIV of France -> symbols.name_source.namesakes -> Fortress of Louisbourg -> base.schemastaging.context_name.pronunciation -> g.125_p429s\n# Answer:\nFortress of Louisbourg", "# Reasoning Path:\nLouis XIV of France -> symbols.name_source.namesakes -> Louis XV of France -> people.person.religion -> Catholicism\n# Answer:\nLouis XV of France", "# Reasoning Path:\nLouis XIV of France -> base.kwebbase.kwtopic.has_sentences -> A change of ministry in England in 1710 fortunately displaced the party in favour of continuing the war, and the new government, pledged to peace, signed the Treaty of Utrecht with France in April 1713. -> base.kwebbase.kwsentence.next_sentence -> Louis XIV died at Versailles two years later.\n# Answer:\nA change of ministry in England in 1710 fortunately displaced the party in favour of continuing the war, and the new government, pledged to peace, signed the Treaty of Utrecht with France in April 1713.", "# Reasoning Path:\nLouis XIV of France -> symbols.name_source.namesakes -> Fortress of Louisbourg -> common.topic.image -> Louisbourg04\n# Answer:\nFortress of Louisbourg", "# Reasoning Path:\nLouis XIV of France -> symbols.name_source.namesakes -> Illinois Country -> common.topic.notable_for -> g.125f9cg7n\n# Answer:\nIllinois Country", "# Reasoning Path:\nLouis XIV of France -> base.kwebbase.kwtopic.has_sentences -> Always keen on expansion, after the death of his father-in-law in 1665, Louis made claim to part of the Spanish Netherlands and launched the War of Dutch Devolution (1667-68). -> base.kwebbase.kwsentence.next_sentence -> This provoked an alliance between Britain, Holland and Sweden and ended with the  Treaty of Aix-La-Chapelle.\n# Answer:\nAlways keen on expansion, after the death of his father-in-law in 1665, Louis made claim to part of the Spanish Netherlands and launched the War of Dutch Devolution (1667-68).", "# Reasoning Path:\nLouis XIV of France -> symbols.name_source.namesakes -> Fortress of Louisbourg -> location.location.events -> Louisbourg Expedition (1757)\n# Answer:\nFortress of Louisbourg"], "ground_truth": ["Catholicism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1615", "prediction": ["# Reasoning Path:\nMike Wright -> sports.pro_athlete.teams -> m.0j3qfq_ -> sports.sports_team_roster.team -> New England Patriots\n# Answer:\nm.0j3qfq_", "# Reasoning Path:\nMike Wright -> sports.pro_athlete.teams -> m.0j3qfq_ -> sports.sports_team_roster.position -> Defensive end\n# Answer:\nm.0j3qfq_", "# Reasoning Path:\nMike Wright -> sports.pro_athlete.teams -> m.0j3qfq_ -> freebase.valuenotation.has_value -> To\n# Answer:\nm.0j3qfq_", "# Reasoning Path:\nMike Wright -> american_football.football_player.games -> m.07nvh58 -> american_football.player_game_statistics.team -> New England Patriots\n# Answer:\nm.07nvh58", "# Reasoning Path:\nMike Wright -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Kansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nMike Wright -> american_football.football_player.games -> m.07sgss2 -> american_football.player_game_statistics.team -> New England Patriots\n# Answer:\nm.07sgss2", "# Reasoning Path:\nMike Wright -> american_football.football_player.games -> m.07t7ftb -> american_football.player_game_statistics.team -> New England Patriots\n# Answer:\nm.07t7ftb", "# Reasoning Path:\nMike Wright -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Kansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nMike Wright -> people.person.nationality -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Kansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nMike Wright -> american_football.football_player.games -> m.07nvh58 -> american_football.player_game_statistics.season -> 2008 NFL season\n# Answer:\nm.07nvh58"], "ground_truth": ["New England Patriots"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2857142857142857, "path_precision": 0.4, "path_recall": 0.2222222222222222, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1616", "prediction": ["# Reasoning Path:\nShannon Hoon -> people.deceased_person.cause_of_death -> Drug overdose -> people.cause_of_death.includes_causes_of_death -> Cocaine overdose\n# Answer:\nDrug overdose", "# Reasoning Path:\nShannon Hoon -> people.deceased_person.cause_of_death -> Drug overdose -> people.cause_of_death.includes_causes_of_death -> Heroin overdose\n# Answer:\nDrug overdose", "# Reasoning Path:\nShannon Hoon -> people.deceased_person.cause_of_death -> Drug overdose -> people.cause_of_death.parent_cause_of_death -> Assassination by 'Accident'\n# Answer:\nDrug overdose", "# Reasoning Path:\nShannon Hoon -> people.deceased_person.cause_of_death -> Drug overdose -> fictional_universe.medical_condition_in_fiction.characters_with_this_condition -> Leo Taylor\n# Answer:\nDrug overdose", "# Reasoning Path:\nShannon Hoon -> people.deceased_person.cause_of_death -> Drug overdose -> people.cause_of_death.parent_cause_of_death -> Assassination by 'Suicide'\n# Answer:\nDrug overdose", "# Reasoning Path:\nShannon Hoon -> people.deceased_person.cause_of_death -> Drug overdose -> fictional_universe.medical_condition_in_fiction.characters_with_this_condition -> Natasha Andersen\n# Answer:\nDrug overdose", "# Reasoning Path:\nShannon Hoon -> common.topic.notable_types -> Composer -> type.type.expected_by -> composer\n# Answer:\nComposer", "# Reasoning Path:\nShannon Hoon -> common.topic.notable_types -> Composer -> base.schemastaging.context_name.plural -> m.0zzddm_\n# Answer:\nComposer", "# Reasoning Path:\nShannon Hoon -> music.artist.track_contributions -> m.012d6m_t -> music.track_contribution.track -> Lemonade\n# Answer:\nm.012d6m_t", "# Reasoning Path:\nShannon Hoon -> common.topic.notable_types -> Composer -> type.type.expected_by -> Arranger\n# Answer:\nComposer"], "ground_truth": ["Drug overdose"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1617", "prediction": ["# Reasoning Path:\nNicolaus Copernicus -> base.argumentmaps.innovator.original_ideas -> Heliocentrism -> base.schemastaging.context_name.pronunciation -> g.125_rtnb2\n# Answer:\nHeliocentrism", "# Reasoning Path:\nNicolaus Copernicus -> symbols.name_source.namesakes -> Copernicus -> common.topic.notable_types -> Extraterrestrial location\n# Answer:\nCopernicus", "# Reasoning Path:\nNicolaus Copernicus -> base.argumentmaps.innovator.original_ideas -> Heliocentrism -> common.topic.image -> Heliocentric\n# Answer:\nHeliocentrism", "# Reasoning Path:\nNicolaus Copernicus -> symbols.name_source.namesakes -> Copernicus -> astronomy.extraterrestrial_location.on_celestial_object -> Moon\n# Answer:\nCopernicus", "# Reasoning Path:\nNicolaus Copernicus -> base.argumentmaps.innovator.original_ideas -> Copernican Revolution -> common.topic.article -> m.02p6y31\n# Answer:\nCopernican Revolution", "# Reasoning Path:\nNicolaus Copernicus -> base.argumentmaps.innovator.original_ideas -> Copernican heliocentrism -> base.argumentmaps.explanation.explained_thing -> Day\n# Answer:\nCopernican heliocentrism", "# Reasoning Path:\nNicolaus Copernicus -> symbols.name_source.namesakes -> Copernicus -> common.topic.article -> m.01z1rs\n# Answer:\nCopernicus", "# Reasoning Path:\nNicolaus Copernicus -> base.argumentmaps.innovator.original_ideas -> Copernican heliocentrism -> base.argumentmaps.explanation.explained_thing -> Night\n# Answer:\nCopernican heliocentrism", "# Reasoning Path:\nNicolaus Copernicus -> base.argumentmaps.innovator.original_ideas -> Copernican heliocentrism -> common.topic.image -> 1660 engraving by an unknown author: Scenographia Systematis Copernicani\n# Answer:\nCopernican heliocentrism", "# Reasoning Path:\nNicolaus Copernicus -> symbols.name_source.namesakes -> Copernican Revolution -> common.topic.article -> m.02p6y31\n# Answer:\nCopernican Revolution"], "ground_truth": ["Heliocentrism", "Copernican Revolution", "Copernican heliocentrism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.41379310344827586, "path_precision": 0.3, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1618", "prediction": ["# Reasoning Path:\nManny Ramirez -> sports.pro_athlete.teams -> m.010p78y8 -> sports.sports_team_roster.team -> Chicago Cubs\n# Answer:\nm.010p78y8", "# Reasoning Path:\nManny Ramirez -> sports.pro_athlete.teams -> m.0w483f0 -> sports.sports_team_roster.team -> Chicago White Sox\n# Answer:\nm.0w483f0", "# Reasoning Path:\nManny Ramirez -> sports.pro_athlete.teams -> m.010p78y8 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.010p78y8", "# Reasoning Path:\nManny Ramirez -> sports.pro_athlete.teams -> m.04fcvk4 -> sports.sports_team_roster.team -> Cleveland Indians\n# Answer:\nm.04fcvk4", "# Reasoning Path:\nManny Ramirez -> baseball.baseball_player.batting_stats -> m.06s7mwn -> baseball.batting_statistics.team -> Boston Red Sox\n# Answer:\nm.06s7mwn", "# Reasoning Path:\nManny Ramirez -> sports.pro_athlete.teams -> m.010p78y8 -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.010p78y8", "# Reasoning Path:\nManny Ramirez -> baseball.baseball_player.batting_stats -> m.06s7m7d -> baseball.batting_statistics.team -> Cleveland Indians\n# Answer:\nm.06s7m7d", "# Reasoning Path:\nManny Ramirez -> sports.pro_athlete.teams -> m.010p78y8 -> freebase.valuenotation.has_value -> Position\n# Answer:\nm.010p78y8", "# Reasoning Path:\nManny Ramirez -> baseball.baseball_player.batting_stats -> m.06s7mwn -> baseball.batting_statistics.season -> 2006 Major League Baseball season\n# Answer:\nm.06s7mwn", "# Reasoning Path:\nManny Ramirez -> baseball.baseball_player.batting_stats -> m.06s7m94 -> baseball.batting_statistics.season -> 1994 Major League Baseball Season\n# Answer:\nm.06s7m94"], "ground_truth": ["Boston Red Sox", "Chicago Cubs", "Chicago White Sox", "Oakland Athletics", "Tampa Bay Rays", "EDA Rhinos", "Cleveland Indians", "Los Angeles Dodgers", "Texas Rangers"], "ans_acc": 0.4444444444444444, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25641025641025644, "path_precision": 0.5, "path_recall": 0.1724137931034483, "path_ans_f1": 0.47058823529411764, "path_ans_precision": 0.5, "path_ans_recall": 0.4444444444444444}
{"id": "WebQTest-1619", "prediction": ["# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> Qu\u00e9bec -> base.aareas.schema.administrative_area.capital -> Quebec City\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nAppalachian Mountains -> location.location.containedby -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> Qu\u00e9bec -> government.governmental_jurisdiction.agencies -> Autorit\u00e9 des march\u00e9s financiers\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Jersey -> location.location.partially_contains -> Appalachian National Scenic Trail\n# Answer:\nNew Jersey", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> Qu\u00e9bec -> location.statistical_region.population -> m.010f3t4y\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Jersey -> location.location.partiallycontains -> m.0wg8ml3\n# Answer:\nNew Jersey", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> symbols.name_source.namesakes -> USS New Hampshire\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> location.location.containedby -> North America -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nNorth America", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> Qu\u00e9bec -> government.governmental_jurisdiction.agencies -> Commission scolaire de la Pointe-de-l'\u00cele\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Jersey -> location.location.partially_contains -> Delaware River\n# Answer:\nNew Jersey"], "ground_truth": ["North America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-162", "prediction": ["# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Republic of Macedonia\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Albania\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics -> olympics.olympic_games.participating_countries -> Bulgaria\n# Answer:\n1896 Summer Olympics", "# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Australia\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.sports -> Ice Hockey\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 1900 Summer Olympics -> olympics.olympic_games.participating_countries -> Australia\n# Answer:\n1900 Summer Olympics", "# Reasoning Path:\nGreece -> location.location.partially_contains -> Baba Mountain, Macedonia -> location.location.partially_containedby -> Republic of Macedonia\n# Answer:\nBaba Mountain, Macedonia", "# Reasoning Path:\nGreece -> location.location.partially_contains -> Ao\u00f6s -> geography.river.basin_countries -> Albania\n# Answer:\nAo\u00f6s", "# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> freebase.valuenotation.is_reviewed -> Location(s)\n# Answer:\n2014 Winter Olympics"], "ground_truth": ["Turkey", "Albania", "Republic of Macedonia", "Bulgaria"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.09195402298850576, "path_precision": 0.4, "path_recall": 0.05194805194805195, "path_ans_f1": 0.6, "path_ans_precision": 0.5, "path_ans_recall": 0.75}
{"id": "WebQTest-1620", "prediction": ["# Reasoning Path:\nWoodrow Wilson -> book.written_work.subjects -> 20th century -> time.event.includes_event -> World War I\n# Answer:\n20th century", "# Reasoning Path:\nWoodrow Wilson -> book.written_work.subjects -> History of the United States\n# Answer:\nHistory of the United States", "# Reasoning Path:\nWoodrow Wilson -> book.written_work.subjects -> 20th century -> common.topic.notable_types -> Event\n# Answer:\n20th century", "# Reasoning Path:\nWoodrow Wilson -> book.written_work.subjects -> United States of America -> location.country.administrative_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nWoodrow Wilson -> event.public_speaker.speeches_or_presentations -> m.05rlb3g -> event.speech_or_presentation.speech_topic -> World War I\n# Answer:\nm.05rlb3g", "# Reasoning Path:\nWoodrow Wilson -> common.topic.notable_for -> g.1255d7s1s\n# Answer:\ng.1255d7s1s", "# Reasoning Path:\nWoodrow Wilson -> book.written_work.subjects -> United States of America -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nWoodrow Wilson -> event.public_speaker.speeches_or_presentations -> m.05rlb3g -> event.speech_or_presentation.presented_work -> Fourteen Points\n# Answer:\nm.05rlb3g", "# Reasoning Path:\nWoodrow Wilson -> common.topic.notable_for -> g.12564bs70\n# Answer:\ng.12564bs70", "# Reasoning Path:\nWoodrow Wilson -> book.written_work.subjects -> United States of America -> location.country.administrative_divisions -> Alabama\n# Answer:\nUnited States of America"], "ground_truth": ["1913-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1621", "prediction": ["# Reasoning Path:\nJerry Brown -> government.politician.election_campaigns -> Jerry Brown for Governor, 2010 -> government.election_campaign.party -> Democratic Party\n# Answer:\nJerry Brown for Governor, 2010", "# Reasoning Path:\nJerry Brown -> government.politician.election_campaigns -> Jerry Brown for Governor, 2010 -> common.topic.notable_for -> g.1255xf_wq\n# Answer:\nJerry Brown for Governor, 2010", "# Reasoning Path:\nJerry Brown -> government.politician.party -> m.040ptrq -> government.political_party_tenure.party -> Democratic Party\n# Answer:\nm.040ptrq", "# Reasoning Path:\nJerry Brown -> common.image.appears_in_topic_gallery -> California Attorney General election, 2006\n# Answer:\nCalifornia Attorney General election, 2006", "# Reasoning Path:\nJerry Brown -> government.politician.party -> m.0lr11c0 -> government.political_party_tenure.party -> California Democratic Party\n# Answer:\nm.0lr11c0", "# Reasoning Path:\nJerry Brown -> common.image.appears_in_topic_gallery -> California gubernatorial election, 1974\n# Answer:\nCalifornia gubernatorial election, 1974", "# Reasoning Path:\nJerry Brown -> common.image.appears_in_topic_gallery -> California gubernatorial election, 1978\n# Answer:\nCalifornia gubernatorial election, 1978"], "ground_truth": ["Democratic Party", "California Democratic Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.6, "path_precision": 0.42857142857142855, "path_recall": 1.0, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-1622", "prediction": ["# Reasoning Path:\nHawaii -> location.location.containedby -> Hawaii County -> location.location.containedby -> Hawaiian Islands\n# Answer:\nHawaii County", "# Reasoning Path:\nHawaii -> location.location.containedby -> Hawaiian Islands -> location.location.containedby -> Kingdom of Hawaii\n# Answer:\nHawaiian Islands", "# Reasoning Path:\nHawaii -> location.location.containedby -> Pacific Ocean -> location.location.events -> Pacific War\n# Answer:\nPacific Ocean", "# Reasoning Path:\nHawaii -> location.location.contains -> Bobcat Trail Habitation Cave -> location.location.containedby -> Hawaii County\n# Answer:\nBobcat Trail Habitation Cave", "# Reasoning Path:\nHawaii -> location.location.containedby -> Pacific Ocean -> location.location.contains -> Oceania\n# Answer:\nPacific Ocean"], "ground_truth": ["Pacific Ocean"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1623", "prediction": ["# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Guaran\u00ed language -> common.topic.notable_types -> Human Language\n# Answer:\nGuaran\u00ed language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Bolivia\n# Answer:\nSpanish Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Kayak II -> biology.organism.organism_type -> Horse\n# Answer:\nKayak II", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Chile\n# Answer:\nSpanish Language", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Kayak II -> base.thoroughbredracing.thoroughbred_racehorse.sex -> Stallion\n# Answer:\nKayak II", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nSpanish Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nItalian Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Italian Language -> common.topic.notable_types -> Human Language\n# Answer:\nItalian Language", "# Reasoning Path:\nArgentina -> location.location.partially_contains -> Llullaillaco -> common.topic.notable_types -> Mountain\n# Answer:\nLlullaillaco"], "ground_truth": ["Spanish Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1624", "prediction": ["# Reasoning Path:\nTunisia -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Algeria\n# Answer:\nArabic Language", "# Reasoning Path:\nTunisia -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Egypt\n# Answer:\nArabic Language", "# Reasoning Path:\nTunisia -> location.country.official_language -> Arabic Language -> language.human_language.countries_spoken_in -> Algeria\n# Answer:\nArabic Language", "# Reasoning Path:\nTunisia -> location.statistical_region.gdp_nominal_per_capita -> g.11b60srfh8\n# Answer:\ng.11b60srfh8", "# Reasoning Path:\nTunisia -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Libya\n# Answer:\nArabic Language", "# Reasoning Path:\nTunisia -> location.country.languages_spoken -> French -> language.human_language.countries_spoken_in -> Algeria\n# Answer:\nFrench", "# Reasoning Path:\nTunisia -> location.country.languages_spoken -> French -> language.human_language.main_country -> France\n# Answer:\nFrench", "# Reasoning Path:\nTunisia -> location.country.official_language -> Arabic Language -> language.human_language.countries_spoken_in -> Egypt\n# Answer:\nArabic Language", "# Reasoning Path:\nTunisia -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6f6x9\n# Answer:\ng.12tb6f6x9", "# Reasoning Path:\nTunisia -> location.country.official_language -> Arabic Language -> language.human_language.countries_spoken_in -> Libya\n# Answer:\nArabic Language"], "ground_truth": ["Arabic Language", "French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1625", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Armored car -> common.topic.article -> m.0y8z\n# Answer:\nArmored car", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Armored car -> base.ontologies.ontology_instance.equivalent_instances -> m.07ndlr9\n# Answer:\nArmored car", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Triple Barrel Canon -> common.topic.notable_for -> g.1yl5vgrll\n# Answer:\nTriple Barrel Canon", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Triple Barrel Canon -> common.topic.notable_types -> Invention\n# Answer:\nTriple Barrel Canon", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Aerial Screw -> common.topic.notable_for -> g.1yl5kdffr\n# Answer:\nAerial Screw", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> influence.influence_node.influenced_by -> Melozzo da Forl\u00ec\n# Answer:\nAntonio da Correggio", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi -> visual_art.artwork.locations -> m.0pcqbm3\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Aerial Screw -> common.topic.notable_types -> Invention\n# Answer:\nAerial Screw", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi -> visual_art.artwork.art_subject -> Biblical Magi\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi -> visual_art.artwork.media -> Oil paint\n# Answer:\nAdoration of the Magi"], "ground_truth": ["33-Barreled Organ", "Double hull", "Diving suit", "Aerial Screw", "Triple Barrel Canon", "Ornithopter", "Armored car", "Parachute", "Viola organista", "Anemometer"], "ans_acc": 0.3, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.6, "ans_recall": 0.3, "path_f1": 0.4, "path_precision": 0.6, "path_recall": 0.3, "path_ans_f1": 0.4, "path_ans_precision": 0.6, "path_ans_recall": 0.3}
{"id": "WebQTest-1626", "prediction": ["# Reasoning Path:\nSt Trinian's School -> common.topic.notable_types -> Fictional Universe -> type.type.expected_by -> Universe\n# Answer:\nFictional Universe", "# Reasoning Path:\nSt Trinian's School -> common.topic.notable_for -> g.12596535g\n# Answer:\ng.12596535g", "# Reasoning Path:\nSt Trinian's School -> common.topic.notable_types -> Fictional Universe -> type.type.expected_by -> Appears In These Fictional Universes\n# Answer:\nFictional Universe", "# Reasoning Path:\nSt Trinian's School -> common.topic.notable_types -> Fictional Universe -> type.type.expected_by -> Appears in Fictional Universe\n# Answer:\nFictional Universe", "# Reasoning Path:\nSt Trinian's School -> common.topic.notable_types -> Fictional Universe -> freebase.type_hints.included_types -> Topic\n# Answer:\nFictional Universe", "# Reasoning Path:\nSt Trinian's School -> fictional_universe.fictional_universe.created_by -> Ronald Searle -> people.person.profession -> Cartoonist\n# Answer:\nRonald Searle", "# Reasoning Path:\nSt Trinian's School -> common.topic.notable_types -> Fictional Universe -> type.type.properties -> Calendar systems\n# Answer:\nFictional Universe", "# Reasoning Path:\nSt Trinian's School -> common.topic.notable_types -> Fictional Universe -> type.type.properties -> Characters\n# Answer:\nFictional Universe", "# Reasoning Path:\nSt Trinian's School -> fictional_universe.fictional_universe.created_by -> Ronald Searle -> influence.influence_node.influenced -> Harry Harrison\n# Answer:\nRonald Searle", "# Reasoning Path:\nSt Trinian's School -> common.topic.notable_types -> Fictional Universe -> type.type.properties -> Contains fictional settings\n# Answer:\nFictional Universe"], "ground_truth": ["Ronald Searle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1627", "prediction": ["# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.webpage -> m.03lfx0d\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.notable_types -> Currency\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.notable_for -> g.1258v8707\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.statistical_region.size_of_armed_forces -> g.12cp_jm8t\n# Answer:\ng.12cp_jm8t", "# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.webpage -> m.04m10rm\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.statistical_region.gdp_nominal_per_capita -> g.11b60thpy3\n# Answer:\ng.11b60thpy3", "# Reasoning Path:\nMalaysia -> location.statistical_region.size_of_armed_forces -> g.1hhc37x7v\n# Answer:\ng.1hhc37x7v", "# Reasoning Path:\nMalaysia -> location.statistical_region.size_of_armed_forces -> g.1hhc39wxf\n# Answer:\ng.1hhc39wxf", "# Reasoning Path:\nMalaysia -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6gh1z\n# Answer:\ng.12tb6gh1z", "# Reasoning Path:\nMalaysia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc38hmv\n# Answer:\ng.1hhc38hmv"], "ground_truth": ["Malaysian ringgit"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1628", "prediction": ["# Reasoning Path:\nMexico -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Guatemala\n# Answer:\nConstitutional republic", "# Reasoning Path:\nMexico -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> United States of America\n# Answer:\nFederal republic", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_0s31\n# Answer:\ng.1245_0s31", "# Reasoning Path:\nMexico -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.11b60v4rn6\n# Answer:\ng.11b60v4rn6", "# Reasoning Path:\nMexico -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guatemala\n# Answer:\nPresidential system", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_38p7\n# Answer:\ng.1245_38p7", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_569j\n# Answer:\ng.1245_569j", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.12tb6hbx9\n# Answer:\ng.12tb6hbx9", "# Reasoning Path:\nMexico -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> United States of America\n# Answer:\nPresidential system"], "ground_truth": ["Constitutional republic", "Presidential system", "Federal republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1629", "prediction": ["# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.article -> m.04lt88\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.webpage -> m.04m2gwk\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.statistical_region.co2_emissions_per_capita -> g.1245_1qfx\n# Answer:\ng.1245_1qfx", "# Reasoning Path:\nDominican Republic -> location.statistical_region.internet_users_percent_population -> g.11b60vv5zn\n# Answer:\ng.11b60vv5zn", "# Reasoning Path:\nDominican Republic -> location.statistical_region.co2_emissions_per_capita -> g.1245_22j7\n# Answer:\ng.1245_22j7", "# Reasoning Path:\nDominican Republic -> location.statistical_region.co2_emissions_per_capita -> g.1245_4lf3\n# Answer:\ng.1245_4lf3", "# Reasoning Path:\nDominican Republic -> location.statistical_region.internet_users_percent_population -> g.1245_22jn\n# Answer:\ng.1245_22jn", "# Reasoning Path:\nDominican Republic -> location.statistical_region.internet_users_percent_population -> g.1245_m66j\n# Answer:\ng.1245_m66j"], "ground_truth": ["Dominican peso"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-163", "prediction": ["# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> time.event.instance_of_recurring_event -> Super Bowl\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> freebase.valuenotation.is_reviewed -> Champion\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> freebase.valuenotation.has_no_value -> Comment\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> 2006 NFC Championship Game -> common.topic.notable_for -> g.1z2spvm2w\n# Answer:\n2006 NFC Championship Game", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> 2006 NFC Championship Game -> time.event.locations -> CenturyLink Field\n# Answer:\n2006 NFC Championship Game", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> freebase.valuenotation.is_reviewed -> Championship\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.season_record -> m.075fsd1 -> sports.sports_team_season_record.season -> 2008 NFL season\n# Answer:\nm.075fsd1", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> 2006 NFC Championship Game -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n2006 NFC Championship Game", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> 2014 NFC Championship Game -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n2014 NFC Championship Game"], "ground_truth": ["Super Bowl XLVIII"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1630", "prediction": ["# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> Charlie Chaplin -> influence.influence_node.influenced -> Redd Foxx\n# Answer:\nCharlie Chaplin", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> Charlie Chaplin -> people.person.gender -> Male\n# Answer:\nCharlie Chaplin", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> James Brown -> influence.influence_node.influenced -> Lenny Henry\n# Answer:\nJames Brown", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> Charlie Chaplin -> freebase.valuenotation.is_reviewed -> Children\n# Answer:\nCharlie Chaplin", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> James Brown -> influence.influence_node.influenced -> Nipsey Russell\n# Answer:\nJames Brown", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> Walt Disney -> people.person.profession -> Actor\n# Answer:\nWalt Disney", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> Charlie Chaplin -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nCharlie Chaplin", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> Walt Disney -> freebase.valuenotation.is_reviewed -> Children\n# Answer:\nWalt Disney", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> James Brown -> music.artist.genre -> Funk\n# Answer:\nJames Brown", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> Walt Disney -> people.person.profession -> Businessperson\n# Answer:\nWalt Disney"], "ground_truth": ["Redd Foxx", "Charlie Chaplin", "James Brown", "Nipsey Russell", "Walt Disney"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 1.0, "ans_recall": 0.6, "path_f1": 0.7499999999999999, "path_precision": 1.0, "path_recall": 0.6, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1631", "prediction": ["# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0gy -> government.government_position_held.office_holder -> Charles Allen Culberson\n# Answer:\nm.04ks0gy", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0f5 -> government.government_position_held.office_holder -> Sam Houston\n# Answer:\nm.04ks0f5", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0gy -> government.government_position_held.office_position_or_title -> Governor of Texas\n# Answer:\nm.04ks0gy", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.0101mqt4 -> government.government_position_held.office_holder -> Susan King\n# Answer:\nm.0101mqt4", "# Reasoning Path:\nTexas -> government.political_district.representatives -> m.0bfmmss -> government.government_position_held.office_holder -> Morris Sheppard\n# Answer:\nm.0bfmmss", "# Reasoning Path:\nTexas -> government.political_district.representatives -> m.0bfmmq0 -> government.government_position_held.office_holder -> Richard Coke\n# Answer:\nm.0bfmmq0", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0f5 -> government.government_position_held.basic_title -> Governor\n# Answer:\nm.04ks0f5", "# Reasoning Path:\nTexas -> government.political_district.representatives -> m.0bfmmss -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nm.0bfmmss", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0f5 -> government.government_position_held.office_position_or_title -> Governor of Texas\n# Answer:\nm.04ks0f5", "# Reasoning Path:\nTexas -> government.political_district.representatives -> m.0bfmmss -> government.government_position_held.basic_title -> Senator\n# Answer:\nm.0bfmmss"], "ground_truth": ["Sam Houston", "William A. Blakley", "Earle Bradford Mayfield", "John Cornyn", "Phil Gramm", "John Henninger Reagan", "James W. Flanagan", "Morgan C. Hamilton", "Joseph Weldon Bailey", "Horace Chilton", "Andrew Jackson Houston", "James Pinckney Henderson", "Ralph Yarborough", "Bob Krueger", "John Hemphill", "Kay Bailey Hutchison", "Lloyd Bentsen", "Samuel B. Maxey", "W. Lee O'Daniel", "Charles Allen Culberson", "Lyndon B. Johnson", "Roger Q. Mills", "Morris Sheppard", "Price Daniel", "Rienzi Melville Johnston", "Thomas Jefferson Rusk", "Tom Connally", "John Tower", "Louis Wigfall", "Matthias Ward", "Ted Cruz", "Richard Coke"], "ans_acc": 0.125, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14035087719298245, "path_precision": 0.4, "path_recall": 0.0851063829787234, "path_ans_f1": 0.19047619047619047, "path_ans_precision": 0.4, "path_ans_recall": 0.125}
{"id": "WebQTest-1632", "prediction": ["# Reasoning Path:\nLondon -> travel.travel_destination.tourist_attractions -> London Charterhouse -> location.location.geolocation -> m.02_tkdf\n# Answer:\nLondon Charterhouse", "# Reasoning Path:\nLondon -> travel.travel_destination.tourist_attractions -> London Charterhouse -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.0djwn2k\n# Answer:\nLondon Charterhouse", "# Reasoning Path:\nLondon -> travel.travel_destination.tourist_attractions -> Victoria and Albert Museum -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.0jx2dvh\n# Answer:\nVictoria and Albert Museum", "# Reasoning Path:\nLondon -> travel.travel_destination.tourist_attractions -> London Palladium -> location.location.containedby -> City of Westminster\n# Answer:\nLondon Palladium", "# Reasoning Path:\nLondon -> travel.travel_destination.tourist_attractions -> Victoria and Albert Museum -> common.topic.notable_types -> Museum\n# Answer:\nVictoria and Albert Museum", "# Reasoning Path:\nLondon -> travel.travel_destination.tourist_attractions -> Victoria and Albert Museum -> travel.tourist_attraction.near_travel_destination -> City of London\n# Answer:\nVictoria and Albert Museum", "# Reasoning Path:\nLondon -> travel.travel_destination.tourist_attractions -> London Palladium -> architecture.structure.architect -> Frank Matcham\n# Answer:\nLondon Palladium", "# Reasoning Path:\nLondon -> periodicals.newspaper_circulation_area.newspapers -> i -> book.periodical.frequency_or_issues_per_year -> m.0jw2n9_\n# Answer:\ni", "# Reasoning Path:\nLondon -> periodicals.newspaper_circulation_area.newspapers -> i -> common.topic.article -> m.0dk0p1s\n# Answer:\ni", "# Reasoning Path:\nLondon -> travel.travel_destination.tourist_attractions -> Victoria and Albert Museum -> common.topic.notable_types -> Structure\n# Answer:\nVictoria and Albert Museum"], "ground_truth": ["Design Museum", "Henman Hill", "Southgate, London", "Whetstone, London", "Waterlily House", "British Museum", "London Palladium", "Tower of London", "London Charterhouse", "Buckingham Palace", "London School of Economics and Political Science", "Bank of England Museum", "Newington, London", "William Wilkins's Building", "Diana, Princess of Wales Memorial Fountain", "London Victoria station", "City University London", "Leighton House Museum", "Tramp", "Chinawhite", "London Paddington station", "Travefy", "Earls Court Exhibition Centre", "Wallace Collection", "Liverpool Street station", "Tower Bridge", "The Nash Conservatory", "Tate Gallery, Britain", "St Paul's Cathedral", "Royal Institution", "Trafalgar Square", "Polish Institute and Sikorski Museum", "Sir John Soane's Museum", "Big Ben", "Smithfield, London", "Barbican Centre", "Serpentine Galleries", "Museum of London", "Queen's House", "Horniman Museum", "London Marathon", "Apsley House", "St. James's Park", "G-A-Y", "Selfridges, Oxford Street", "Wellcome Collection", "Royal Albert Hall, London", "Euston railway station", "Science Museum, London", "RAF Bomber Command Memorial", "Hyde Park", "Strand, London", "Chessington World of Adventures", "New London Architecture", "National Portrait Gallery, London", "Palace of Westminster", "Wallington, London", "Sutcliffe Park", "Imperial War Museum London", "Crystal Palace Park", "Victoria and Albert Museum", "The Building Centre", "Central London", "Sainsbury Wing", "Olympia", "Madame Tussauds London", "Imperial College London", "London Eye", "Natural History Museum, London", "British Museum Reading Room", "University College London", "National Maritime Museum", "London Underground", "Chokushi-Mon", "Westminster Abbey", "Duke of York's Headquarters", "V&A Museum of Childhood", "The Clink", "Wasps RFC", "London Zoo", "Tate Modern, London", "Wimbledon", "Holloway", "National Police Memorial", "Churchill War Rooms", "Museum of London Docklands", "Hippodrome, London", "University of London", "Regent's Park", "Jewish Museum London", "London Bridge"], "ans_acc": 0.03296703296703297, "ans_hit": 1, "ans_f1": 0.0633245382585752, "ans_precission": 0.8, "ans_recall": 0.03296703296703297, "path_f1": 0.08247422680412371, "path_precision": 0.8, "path_recall": 0.043478260869565216, "path_ans_f1": 0.0633245382585752, "path_ans_precision": 0.8, "path_ans_recall": 0.03296703296703297}
{"id": "WebQTest-1633", "prediction": ["# Reasoning Path:\nMona Lisa -> visual_art.artwork.period_or_movement -> The Renaissance -> visual_art.art_period_movement.associated_artists -> Alessandro Tiarini\n# Answer:\nThe Renaissance", "# Reasoning Path:\nMona Lisa -> visual_art.artwork.period_or_movement -> The Renaissance -> base.ontologies.ontology_instance.equivalent_instances -> m.07nfqtv\n# Answer:\nThe Renaissance", "# Reasoning Path:\nMona Lisa -> visual_art.artwork.period_or_movement -> The Renaissance -> visual_art.art_period_movement.associated_artists -> Alfred Stevens\n# Answer:\nThe Renaissance", "# Reasoning Path:\nMona Lisa -> visual_art.artwork.period_or_movement -> The Renaissance -> common.topic.webpage -> m.09xk6x8\n# Answer:\nThe Renaissance", "# Reasoning Path:\nMona Lisa -> visual_art.artwork.period_or_movement -> The Renaissance -> visual_art.art_period_movement.associated_artists -> Andrea Riccio\n# Answer:\nThe Renaissance", "# Reasoning Path:\nMona Lisa -> visual_art.artwork.period_or_movement -> The Renaissance -> common.topic.webpage -> m.09y4hf0\n# Answer:\nThe Renaissance", "# Reasoning Path:\nMona Lisa -> visual_art.artwork.period_or_movement -> The Renaissance -> common.topic.webpage -> m.09ybgy7\n# Answer:\nThe Renaissance", "# Reasoning Path:\nMona Lisa -> exhibitions.exhibit.exhibitions_displayed_in -> Mona Lisa by Leonardo da Vinci -> exhibitions.exhibition.venues -> m.0kbt5rx\n# Answer:\nMona Lisa by Leonardo da Vinci", "# Reasoning Path:\nMona Lisa -> exhibitions.exhibit.exhibitions_displayed_in -> Mona Lisa by Leonardo da Vinci -> exhibitions.exhibition.exhibition_types -> Art exhibition\n# Answer:\nMona Lisa by Leonardo da Vinci", "# Reasoning Path:\nMona Lisa -> exhibitions.exhibit.exhibitions_displayed_in -> Mona Lisa by Leonardo da Vinci -> common.topic.webpage -> m.0kbthtd\n# Answer:\nMona Lisa by Leonardo da Vinci"], "ground_truth": ["Portrait"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1634", "prediction": ["# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> people.person.profession -> Television producer\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> people.person.profession -> Actor\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> people.person.profession -> Film Producer\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.parents -> Mickey McFinnegan -> fictional_universe.fictional_character.children -> Thaddeus Griffin\n# Answer:\nMickey McFinnegan", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.010gysx9\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> tv.tv_producer.programs_produced -> m.010hsx_r\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.parents -> Mickey McFinnegan -> fictional_universe.fictional_character.gender -> Male\n# Answer:\nMickey McFinnegan", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.02kk65p\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.parents -> Mickey McFinnegan -> common.topic.notable_types -> Fictional Character\n# Answer:\nMickey McFinnegan", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.02ntr0h\n# Answer:\nSeth MacFarlane"], "ground_truth": ["Seth MacFarlane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1635", "prediction": ["# Reasoning Path:\nTony Romo -> american_football.football_player.passing -> m.0791bch -> american_football.player_passing_statistics.season -> 2008 NFL season\n# Answer:\nm.0791bch", "# Reasoning Path:\nTony Romo -> base.popstra.celebrity.friendship -> m.063hcgf -> base.popstra.friendship.participant -> Carrie Underwood\n# Answer:\nm.063hcgf", "# Reasoning Path:\nTony Romo -> american_football.football_player.passing -> m.0791bch -> american_football.player_passing_statistics.team -> Dallas Cowboys\n# Answer:\nm.0791bch", "# Reasoning Path:\nTony Romo -> american_football.football_player.games -> m.07nvmcd -> american_football.player_game_statistics.team -> Dallas Cowboys\n# Answer:\nm.07nvmcd", "# Reasoning Path:\nTony Romo -> american_football.football_player.passing -> m.07mmkjm -> american_football.player_passing_statistics.season -> 2009 NFL season\n# Answer:\nm.07mmkjm", "# Reasoning Path:\nTony Romo -> base.popstra.celebrity.friendship -> m.065pg0l -> base.popstra.friendship.participant -> Donald Faison\n# Answer:\nm.065pg0l", "# Reasoning Path:\nTony Romo -> american_football.football_player.passing -> m.07mmkjm -> american_football.player_passing_statistics.team -> Dallas Cowboys\n# Answer:\nm.07mmkjm", "# Reasoning Path:\nTony Romo -> american_football.football_player.passing -> m.07sh121 -> american_football.player_passing_statistics.season -> 2009 NFL season\n# Answer:\nm.07sh121", "# Reasoning Path:\nTony Romo -> base.popstra.celebrity.friendship -> m.065pj22 -> base.popstra.friendship.participant -> CaCee Cobb\n# Answer:\nm.065pj22", "# Reasoning Path:\nTony Romo -> american_football.football_player.passing -> m.07sh121 -> american_football.player_passing_statistics.team -> Dallas Cowboys\n# Answer:\nm.07sh121"], "ground_truth": ["2002"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1637", "prediction": ["# Reasoning Path:\nCatholicism -> religion.religion.holidays -> All Saints' Day -> time.holiday.day_of_year -> November 1\n# Answer:\nAll Saints' Day", "# Reasoning Path:\nCatholicism -> religion.religion.holidays -> All Saints' Day -> common.topic.notable_for -> g.1255bx1xr\n# Answer:\nAll Saints' Day", "# Reasoning Path:\nCatholicism -> religion.religion.holidays -> All Saints' Day -> base.schemastaging.holiday_extra.observance_rule -> All Saints' Day observance rule (Austria, 2000 - now)\n# Answer:\nAll Saints' Day", "# Reasoning Path:\nCatholicism -> religion.religion.holidays -> Euthymius the Great's Feast Day -> common.topic.notable_types -> Holiday\n# Answer:\nEuthymius the Great's Feast Day", "# Reasoning Path:\nCatholicism -> religion.religion.holidays -> All Saints' Day -> base.schemastaging.holiday_extra.observance_rule -> All Saints' Day observance rule (Denmark, 2000 - now)\n# Answer:\nAll Saints' Day", "# Reasoning Path:\nCatholicism -> religion.religion.holidays -> Corpus Christi -> base.schemastaging.holiday_extra.observance_rule -> Corpus Christi observance rule (Austria, 2000 - now)\n# Answer:\nCorpus Christi", "# Reasoning Path:\nCatholicism -> base.argumentmaps.thing_of_disputed_value.disparagement -> A Connecticut Yankee in King Arthur's Court -> book.written_work.subjects -> Classics\n# Answer:\nA Connecticut Yankee in King Arthur's Court", "# Reasoning Path:\nCatholicism -> base.argumentmaps.thing_of_disputed_value.disparagement -> A Connecticut Yankee in King Arthur's Court -> film.film.subjects -> Time travel\n# Answer:\nA Connecticut Yankee in King Arthur's Court", "# Reasoning Path:\nCatholicism -> base.argumentmaps.thing_of_disputed_value.disparagement -> A Moral Reckoning -> book.written_work.subjects -> Religion\n# Answer:\nA Moral Reckoning", "# Reasoning Path:\nCatholicism -> religion.religion.holidays -> All Saints' Day -> base.schemastaging.holiday_extra.observance_rule -> All Saints' Day observance rule (Earth, 2000 - now)\n# Answer:\nAll Saints' Day"], "ground_truth": ["St Joseph's Day", "St. Stephen's Day", "Feast of St Francis of Assisi", "Saint Patrick's Day", "Saint Nicholas' Eve", "Corpus Christi", "St Crispin's Day", "St. Anthony's Day", "Solemnity of Mary, Mother of God", "Saints Cyril and Methodius Day", "Name day", "Palm Sunday", "Feast of Christ the King", "Olsok", "Feast of Saints Peter and Paul", "Nativity of Mary", "Our Lady of Aparecida's day", "Feast of Assumption", "Feast of Our Lady of the Rosary", "All Saints' Day", "Feast of Our Lady of Sorrows", "Fat Thursday", "Feast of the Cross", "Maundy Thursday", "Euthymius the Great's Feast Day", "Feast of the Immaculate Conception", "St Nicholas Day", "Feast of Our Lady of Hungary", "St Casimir's Day", "St. Spiridon Day", "The Feast of Our Lady of Mount Carmel", "World Day of Peace", "Our Lady of Guadalupe Day", "Feast of St. Margaret of the \u00c1rp\u00e1d House", "Father Damien Day"], "ans_acc": 0.08571428571428572, "ans_hit": 1, "ans_f1": 0.1527272727272727, "ans_precission": 0.7, "ans_recall": 0.08571428571428572, "path_f1": 0.1527272727272727, "path_precision": 0.7, "path_recall": 0.08571428571428572, "path_ans_f1": 0.1527272727272727, "path_ans_precision": 0.7, "path_ans_recall": 0.08571428571428572}
{"id": "WebQTest-1638", "prediction": ["# Reasoning Path:\nBelgium -> location.country.languages_spoken -> West Flemish -> language.human_language.language_family -> Germanic languages\n# Answer:\nWest Flemish", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> West Flemish -> language.human_language.language_family -> Indo-European languages\n# Answer:\nWest Flemish", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> West Flemish -> language.human_language.countries_spoken_in -> County of Flanders\n# Answer:\nWest Flemish", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> German Language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nGerman Language", "# Reasoning Path:\nBelgium -> location.statistical_region.gni_in_ppp_dollars -> g.11b61jk1pw\n# Answer:\ng.11b61jk1pw", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> West Flemish -> language.human_language.language_family -> West Germanic languages\n# Answer:\nWest Flemish", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> German Language -> language.human_language.region -> Europe\n# Answer:\nGerman Language", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> French -> language.human_language.language_family -> Indo-European languages\n# Answer:\nFrench", "# Reasoning Path:\nBelgium -> location.country.official_language -> German Language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nGerman Language"], "ground_truth": ["German Language", "Dutch Language", "French"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.5, "ans_recall": 0.6666666666666666, "path_f1": 0.25, "path_precision": 0.2, "path_recall": 0.3333333333333333, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.5, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1639", "prediction": ["# Reasoning Path:\nL. Frank Baum -> people.deceased_person.place_of_death -> Hollywood -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nHollywood", "# Reasoning Path:\nL. Frank Baum -> people.deceased_person.place_of_death -> Hollywood -> base.barbie.barbie_theme.dolls_with_this_theme -> Barbie as Dorothy in The Wizard of Oz\n# Answer:\nHollywood", "# Reasoning Path:\nL. Frank Baum -> people.deceased_person.place_of_death -> Hollywood -> location.neighborhood.neighborhood_of -> Los Angeles\n# Answer:\nHollywood", "# Reasoning Path:\nL. Frank Baum -> people.deceased_person.place_of_death -> Hollywood -> base.barbie.barbie_theme.dolls_with_this_theme -> Barbie as Glinda the Good Witch in The Wizard of Oz #14901\n# Answer:\nHollywood", "# Reasoning Path:\nL. Frank Baum -> film.film_story_contributor.film_story_credits -> Ay\u015fecik ve Sihirli C\u00fcceler R\u00fcyalar \u00dclkesinde -> film.film.music -> Turgud \u00d6ren\n# Answer:\nAy\u015fecik ve Sihirli C\u00fcceler R\u00fcyalar \u00dclkesinde", "# Reasoning Path:\nL. Frank Baum -> film.film_story_contributor.film_story_credits -> Ay\u015fecik ve Sihirli C\u00fcceler R\u00fcyalar \u00dclkesinde -> freebase.valuenotation.has_value -> Edited by\n# Answer:\nAy\u015fecik ve Sihirli C\u00fcceler R\u00fcyalar \u00dclkesinde", "# Reasoning Path:\nL. Frank Baum -> book.author.works_written -> The Wizard of Oz -> film.film.dubbing_performances -> m.0pwyctw\n# Answer:\nThe Wizard of Oz", "# Reasoning Path:\nL. Frank Baum -> film.film_story_contributor.film_story_credits -> Fantasy 3 -> film.film.story_by -> Brothers Grimm\n# Answer:\nFantasy 3", "# Reasoning Path:\nL. Frank Baum -> film.film_story_contributor.film_story_credits -> Fantasy 3 -> film.film.genre -> Fantasy\n# Answer:\nFantasy 3", "# Reasoning Path:\nL. Frank Baum -> book.author.works_written -> The Wizard of Oz -> film.film.dubbing_performances -> m.0pwydjk\n# Answer:\nThe Wizard of Oz"], "ground_truth": ["Hollywood"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-164", "prediction": ["# Reasoning Path:\nStephen Hawking -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.0101ddrk\n# Answer:\nPhysicist", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Mathematician -> fictional_universe.character_occupation.characters_with_this_occupation -> Beremiz Samir\n# Answer:\nMathematician", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Physicist -> people.profession.part_of_professional_field -> Physics\n# Answer:\nPhysicist", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.01066smw\n# Answer:\nPhysicist", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Daniel Faraday\n# Answer:\nPhysicist", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Mathematician -> fictional_universe.character_occupation.characters_with_this_occupation -> Dyhianna Selei\n# Answer:\nMathematician", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.01066sxg\n# Answer:\nPhysicist", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Mathematician -> fictional_universe.character_occupation.characters_with_this_occupation -> Hari Seldon\n# Answer:\nMathematician", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Mathematician -> common.topic.notable_for -> g.12553cs6s\n# Answer:\nMathematician", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Scientist -> common.topic.subject_of -> Autobiography\n# Answer:\nScientist"], "ground_truth": ["Professor", "Physicist", "Astronomer", "Science writer", "Astrophysicist", "Cosmologist", "Writer", "Author", "Scientist", "Mathematician"], "ans_acc": 0.3, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 1.0, "ans_recall": 0.3, "path_f1": 0.4615384615384615, "path_precision": 1.0, "path_recall": 0.3, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 1.0, "path_ans_recall": 0.3}
{"id": "WebQTest-1640", "prediction": ["# Reasoning Path:\nTunis -> location.location.containedby -> Tunisia -> location.location.containedby -> Africa\n# Answer:\nAfrica", "# Reasoning Path:\nTunisia -> location.location.containedby -> Arab world -> location.location.contains -> Algeria\n# Answer:\nArab world", "# Reasoning Path:\nTunis -> location.location.containedby -> Tunisia -> location.location.containedby -> Arab world\n# Answer:\nArab world", "# Reasoning Path:\nTunis -> location.location.containedby -> Tunisia -> base.locations.countries.continent -> Africa\n# Answer:\nAfrica", "# Reasoning Path:\nTunisia -> location.location.containedby -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica", "# Reasoning Path:\nTunis -> location.location.containedby -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica", "# Reasoning Path:\nTunis -> location.location.containedby -> Tunisia -> location.location.containedby -> Greater Maghreb\n# Answer:\nGreater Maghreb", "# Reasoning Path:\nTunisia -> base.locations.countries.continent -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica", "# Reasoning Path:\nTunis -> location.location.containedby -> Tunis Governorate -> location.location.containedby -> Tunisia\n# Answer:\nTunis Governorate", "# Reasoning Path:\nTunis -> location.location.containedby -> Africa -> base.locations.continents.countries_within -> Tunisia\n# Answer:\nAfrica"], "ground_truth": ["Tunisia", "Africa", "Tunis Governorate"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6829268292682926, "ans_precission": 0.7, "ans_recall": 0.6666666666666666, "path_f1": 0.8, "path_precision": 1.0, "path_recall": 0.6666666666666666, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1641", "prediction": ["# Reasoning Path:\nNicolas Cage -> people.person.spouse_s -> m.025dsbw -> people.marriage.spouse -> Patricia Arquette\n# Answer:\nm.025dsbw", "# Reasoning Path:\nNicolas Cage -> people.person.spouse_s -> m.025dsc3 -> people.marriage.spouse -> Lisa Marie Presley\n# Answer:\nm.025dsc3", "# Reasoning Path:\nNicolas Cage -> people.person.spouse_s -> m.025dscc -> people.marriage.spouse -> Alice Kim\n# Answer:\nm.025dscc", "# Reasoning Path:\nNicolas Cage -> people.person.spouse_s -> m.025dsbw -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.025dsbw", "# Reasoning Path:\nNicolas Cage -> people.person.spouse_s -> m.025dsbw -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.025dsbw", "# Reasoning Path:\nNicolas Cage -> base.popstra.celebrity.dated -> m.063g718 -> base.popstra.dated.participant -> Brooke Shields\n# Answer:\nm.063g718", "# Reasoning Path:\nNicolas Cage -> celebrities.celebrity.sexual_orientation -> m.05n7hz_ -> celebrities.sexual_orientation_phase.sexual_orientation -> Heterosexuality\n# Answer:\nm.05n7hz_", "# Reasoning Path:\nNicolas Cage -> people.person.spouse_s -> m.025dsc3 -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.025dsc3", "# Reasoning Path:\nNicolas Cage -> people.person.spouse_s -> m.025dsc3 -> people.marriage.location_of_ceremony -> Hawaii\n# Answer:\nm.025dsc3", "# Reasoning Path:\nNicolas Cage -> people.person.spouse_s -> m.025dscc -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.025dscc"], "ground_truth": ["Alice Kim"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1643", "prediction": ["# Reasoning Path:\nChild's Play -> film.film.starring -> m.03jssz4 -> film.performance.actor -> Dinah Manoff\n# Answer:\nm.03jssz4", "# Reasoning Path:\nChild's Play -> film.film.starring -> m.03jt26n -> film.performance.actor -> Alex Vincent\n# Answer:\nm.03jt26n", "# Reasoning Path:\nChild's Play -> film.film.starring -> m.09hxz1j -> film.performance.actor -> Jack Colvin\n# Answer:\nm.09hxz1j", "# Reasoning Path:\nChild's Play -> film.film.starring -> m.03jssz4 -> film.performance.character -> Maggie Peterson\n# Answer:\nm.03jssz4", "# Reasoning Path:\nChild's Play -> film.film.directed_by -> Tom Holland -> film.producer.films_executive_produced -> The Owl\n# Answer:\nTom Holland", "# Reasoning Path:\nChild's Play -> film.film.starring -> m.03jt26n -> film.performance.character -> Andy Barclay\n# Answer:\nm.03jt26n", "# Reasoning Path:\nChild's Play -> film.film.distributors -> m.059s795 -> film.film_film_distributor_relationship.distributor -> Universal Studios\n# Answer:\nm.059s795", "# Reasoning Path:\nChild's Play -> film.film.starring -> m.09hxz1j -> film.performance.character -> Dr. Ardmore\n# Answer:\nm.09hxz1j", "# Reasoning Path:\nChild's Play -> film.film.distributors -> m.07snx83 -> film.film_film_distributor_relationship.distributor -> DreamWorks Studios\n# Answer:\nm.07snx83", "# Reasoning Path:\nChild's Play -> film.film.directed_by -> Tom Holland -> film.writer.film -> Class of 1984\n# Answer:\nTom Holland"], "ground_truth": ["Brad Dourif", "John Franklin", "Edan Gross"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1644", "prediction": ["# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> base.activism.activism_issue.activists -> Abby Morton Diaz\n# Answer:\nWomen's rights", "# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> base.activism.activism_issue.activists -> Abigail Bush\n# Answer:\nWomen's rights", "# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> organization.organization_sector.organizations_in_this_sector -> Abortion Law Reform Association of New Zealand\n# Answer:\nWomen's rights", "# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> base.activism.activism_issue.activists -> Alice Moore Hubbard\n# Answer:\nWomen's rights", "# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> book.book_subject.works -> Maintenance for Women Rhetoric of Equality\n# Answer:\nWomen's rights", "# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> organization.organization_sector.organizations_in_this_sector -> Abortion Rights\n# Answer:\nWomen's rights", "# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> organization.organization_sector.organizations_in_this_sector -> Abortion Rights Coalition of Canada\n# Answer:\nWomen's rights", "# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> book.book_subject.works -> Report on 2011 International Year Part 5 Haiti's Poetics of Pain and Resilience\n# Answer:\nWomen's rights", "# Reasoning Path:\nMary Wollstonecraft -> people.person.profession -> Writer -> people.profession.specializations -> Novelist\n# Answer:\nWriter", "# Reasoning Path:\nMary Wollstonecraft -> base.kwebbase.kwtopic.has_sentences -> At Johnson's home she met the radical philosophers Godwin and Paine, the Swiss artist Fuseli and the artist and poet Blake. -> base.kwebbase.kwsentence.previous_sentence -> She introduced the principles of education for women in her reviews and she singled out for praise writers who gave equal respect to women's intellect.\n# Answer:\nAt Johnson's home she met the radical philosophers Godwin and Paine, the Swiss artist Fuseli and the artist and poet Blake."], "ground_truth": ["Women's rights"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1646", "prediction": ["# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.league -> m.0crt4k0 -> sports.sports_league_participation.league -> Major League Baseball\n# Answer:\nm.0crt4k0", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.league -> m.0crt7bg -> sports.sports_league_participation.league -> National League\n# Answer:\nm.0crt7bg", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.league -> m.0crtdbg -> sports.sports_league_participation.league -> National League East\n# Answer:\nm.0crtdbg", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.championships -> 1980 World Series -> common.topic.image -> 1980 World Series Program\n# Answer:\n1980 World Series", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.championships -> 2008 National League Championship Series -> common.topic.article -> m.04f7kvq\n# Answer:\n2008 National League Championship Series", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.championships -> 1980 World Series -> sports.sports_championship_event.championship -> World Series\n# Answer:\n1980 World Series", "# Reasoning Path:\nPhiladelphia Phillies -> sports.professional_sports_team.draft_picks -> m.04vw__p -> sports.sports_league_draft_pick.school -> University of Oklahoma\n# Answer:\nm.04vw__p", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.championships -> 2008 National League Championship Series -> sports.sports_championship_event.runner_up -> Los Angeles Dodgers\n# Answer:\n2008 National League Championship Series", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.championships -> 2008 World Series -> base.newsevents.news_reported_event.news_report_s -> m.07w2gtg\n# Answer:\n2008 World Series", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.championships -> 2008 World Series -> sports.sports_championship_event.runner_up -> Tampa Bay Rays\n# Answer:\n2008 World Series"], "ground_truth": ["1883"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1648", "prediction": ["# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> language.human_language.region -> Europe\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.official_language -> Icelandic Language -> language.human_language.region -> Europe\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> base.rosetta.languoid.local_name -> Icelandic\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> common.topic.article -> m.01gmks\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.statistical_region.gni_in_ppp_dollars -> g.11b61jk1t5\n# Answer:\ng.11b61jk1t5", "# Reasoning Path:\nIceland -> location.country.official_language -> Icelandic Language -> base.rosetta.languoid.local_name -> Icelandic\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> common.topic.article -> m.02nt727\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.official_language -> Icelandic Language -> common.topic.article -> m.01gmks\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.statistical_region.gni_in_ppp_dollars -> g.1245_7d01\n# Answer:\ng.1245_7d01", "# Reasoning Path:\nIceland -> location.country.official_language -> Icelandic Language -> common.topic.article -> m.02nt727\n# Answer:\nIcelandic Language"], "ground_truth": ["Icelandic Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1649", "prediction": ["# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> theater.theater_production_staff_role.people_who_have_had_this_role -> m.0111n9h6\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> common.topic.subject_of -> Dum-Doodles\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Artist -> people.profession.specializations -> Visual Artist\n# Answer:\nArtist", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> theater.theater_production_staff_role.people_who_have_had_this_role -> m.0_grv_v\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Television producer -> common.topic.subjects -> Billy Sorrentino\n# Answer:\nTelevision producer", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Television producer -> base.lightweight.profession.specialization_of -> Producers and Directors\n# Answer:\nTelevision producer", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> theater.theater_production_staff_role.people_who_have_had_this_role -> m.0w3l6yj\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> base.descriptive_names.names.descriptive_name -> m.01066c2h\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Artist -> people.profession.specializations -> Cartoonist\n# Answer:\nArtist", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Television producer -> common.topic.subjects -> Michael Palance\n# Answer:\nTelevision producer"], "ground_truth": ["Theodor Seuss Geisel"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-165", "prediction": ["# Reasoning Path:\nDonna Noble -> tv.tv_character.appeared_in_tv_program -> m.010pxvzy -> tv.regular_tv_appearance.actor -> Catherine Tate\n# Answer:\nm.010pxvzy", "# Reasoning Path:\nDonna Noble -> tv.tv_character.appeared_in_tv_program -> m.010pxvzy -> tv.regular_tv_appearance.series -> Doctor Who\n# Answer:\nm.010pxvzy", "# Reasoning Path:\nDonna Noble -> tv.tv_character.appeared_in_tv_program -> m.010pxvzy -> tv.regular_tv_appearance.seasons -> Doctor Who Series 4 (2008)\n# Answer:\nm.010pxvzy", "# Reasoning Path:\nDonna Noble -> fictional_universe.fictional_character.occupation -> office temp-worker -> fictional_universe.character_occupation.characters_with_this_occupation -> Wendy Watson\n# Answer:\noffice temp-worker", "# Reasoning Path:\nDonna Noble -> fictional_universe.fictional_character.occupation -> office temp-worker -> common.topic.notable_for -> g.1259jvdvh\n# Answer:\noffice temp-worker", "# Reasoning Path:\nDonna Noble -> fictional_universe.fictional_character.occupation -> office temp-worker -> fictional_universe.fictional_job_title.fictional_characters_with_this_job -> m.046d77x\n# Answer:\noffice temp-worker", "# Reasoning Path:\nDonna Noble -> tv.tv_character.appeared_in_tv_episodes -> m.04gf26q -> tv.tv_guest_role.actor -> Catherine Tate\n# Answer:\nm.04gf26q", "# Reasoning Path:\nDonna Noble -> tv.tv_character.appeared_in_tv_episodes -> m.04gf26q -> tv.tv_guest_role.episodes_appeared_in -> The Runaway Bride\n# Answer:\nm.04gf26q", "# Reasoning Path:\nDonna Noble -> tv.tv_character.appeared_in_tv_episodes -> m.04gf29d -> tv.tv_guest_role.episodes_appeared_in -> Turn Left\n# Answer:\nm.04gf29d", "# Reasoning Path:\nDonna Noble -> fictional_universe.fictional_character.occupation -> office temp-worker -> fictional_universe.fictional_job_title.fictional_characters_with_this_job -> m.04gf2gb\n# Answer:\noffice temp-worker"], "ground_truth": ["Catherine Tate"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14814814814814817, "path_precision": 0.2, "path_recall": 0.11764705882352941, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1650", "prediction": ["# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> Choice USA -> common.topic.notable_types -> Organization\n# Answer:\nChoice USA", "# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> Choice USA -> common.topic.article -> m.02vvc40\n# Answer:\nChoice USA", "# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> Ms. Foundation for Women -> organization.organization.founders -> Letty Cottin Pogrebin\n# Answer:\nMs. Foundation for Women", "# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> Ms. Foundation for Women -> award.award_honor.ceremony -> 2007 19th Annual Gloria Award\n# Answer:\nMs. Foundation for Women", "# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> National Women's Political Caucus -> common.topic.notable_for -> g.12591d_31\n# Answer:\nNational Women's Political Caucus", "# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> Ms. Foundation for Women -> organization.organization.founders -> Marlo Thomas\n# Answer:\nMs. Foundation for Women", "# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> National Women's Political Caucus -> common.topic.article -> m.04wwk1\n# Answer:\nNational Women's Political Caucus", "# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> Ms. Foundation for Women -> award.award_honor.award_winner -> Ai-jen Poo\n# Answer:\nMs. Foundation for Women", "# Reasoning Path:\nGloria Steinem -> book.author.works_written -> Doing Sixty and Seventy -> common.topic.notable_types -> Book\n# Answer:\nDoing Sixty and Seventy", "# Reasoning Path:\nGloria Steinem -> book.author.works_written -> Feminist Family Values -> common.topic.notable_for -> g.1255tp2p4\n# Answer:\nFeminist Family Values"], "ground_truth": ["National Women's Political Caucus", "Coalition of Labor Union Women", "Choice USA", "Women's Action Alliance", "Ms. Foundation for Women", "New York Media, LLC", "Feminist Majority Foundation"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5581395348837209, "ans_precission": 0.8, "ans_recall": 0.42857142857142855, "path_f1": 0.5581395348837209, "path_precision": 0.8, "path_recall": 0.42857142857142855, "path_ans_f1": 0.5581395348837209, "path_ans_precision": 0.8, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1652", "prediction": ["# Reasoning Path:\nScotland -> location.country.languages_spoken -> Old English -> language.human_language.countries_spoken_in -> England\n# Answer:\nOld English", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Old English -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nOld English", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish English -> base.schemastaging.context_name.pronunciation -> g.125_nyqwz\n# Answer:\nScottish English", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.countries_spoken_in -> Kingdom of Great Britain\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.main_country -> United Kingdom\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Old English -> language.human_language.main_country -> England\n# Answer:\nOld English", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Old English -> language.human_language.main_country -> United Kingdom\n# Answer:\nOld English", "# Reasoning Path:\nScotland -> location.country.first_level_divisions -> Aberdeenshire -> location.administrative_division.second_level_division_of -> United Kingdom\n# Answer:\nAberdeenshire"], "ground_truth": ["English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1653", "prediction": ["# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> education.education.institution -> Mercy College\n# Answer:\nm.0j_gm2q", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.04hc7zn -> education.education.institution -> Phillips Exeter Academy\n# Answer:\nm.04hc7zn", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.02n93cn -> education.education.institution -> Harvard University\n# Answer:\nm.02n93cn", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nm.0j_gm2q", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_value -> End Date\n# Answer:\nm.0j_gm2q", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nm.0j_gm2q", "# Reasoning Path:\nMark Zuckerberg -> influence.influence_node.influenced -> Austin Cameron -> influence.influence_node.influenced_by -> Bill Gates\n# Answer:\nAustin Cameron", "# Reasoning Path:\nMark Zuckerberg -> influence.influence_node.influenced -> Paul Scolnick -> people.person.education -> m.01314533\n# Answer:\nPaul Scolnick", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.02n93cn -> freebase.valuenotation.has_value -> End Date\n# Answer:\nm.02n93cn", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Specialization\n# Answer:\nm.0j_gm2q"], "ground_truth": ["Phillips Exeter Academy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1654", "prediction": ["# Reasoning Path:\nBigos -> food.dish.cuisine -> Polish food -> dining.cuisine.dishes -> P\u0105czki\n# Answer:\nPolish food", "# Reasoning Path:\nBigos -> food.dish.cuisine -> Polish food -> dining.cuisine.dishes -> Kugelis\n# Answer:\nPolish food", "# Reasoning Path:\nBigos -> food.dish.cuisine -> Polish food -> dining.cuisine.dishes -> Borscht\n# Answer:\nPolish food", "# Reasoning Path:\nBigos -> food.dish.cuisine -> Polish food -> common.topic.notable_types -> Cuisine\n# Answer:\nPolish food", "# Reasoning Path:\nBigos -> food.dish.cuisine -> Polish food -> common.topic.image -> A meat dish in Poland\n# Answer:\nPolish food", "# Reasoning Path:\nBigos -> food.dish.ingredients -> Cabbage -> food.food.nutrients -> m.07x7v_9\n# Answer:\nCabbage", "# Reasoning Path:\nBigos -> food.dish.cuisine -> Polish food -> common.topic.image -> Polish pastries\n# Answer:\nPolish food", "# Reasoning Path:\nBigos -> food.dish.ingredients -> Cabbage -> biology.organism.organism_type -> Brassica oleracea\n# Answer:\nCabbage", "# Reasoning Path:\nBigos -> common.topic.notable_types -> Dish -> type.type.properties -> Type of dish\n# Answer:\nDish", "# Reasoning Path:\nBigos -> food.dish.ingredients -> Cabbage -> food.food.nutrients -> m.07x7v_h\n# Answer:\nCabbage"], "ground_truth": ["Stew"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1655", "prediction": ["# Reasoning Path:\nTina Turner -> people.person.children -> Raymond Craig Turner -> people.person.parents -> Ike Turner\n# Answer:\nRaymond Craig Turner", "# Reasoning Path:\nTina Turner -> people.person.children -> Raymond Craig Turner -> people.person.nationality -> United States of America\n# Answer:\nRaymond Craig Turner", "# Reasoning Path:\nTina Turner -> people.person.children -> Ronnie Turner -> people.person.parents -> Ike Turner\n# Answer:\nRonnie Turner", "# Reasoning Path:\nTina Turner -> people.person.children -> Raymond Craig Turner -> people.person.place_of_birth -> United States of America\n# Answer:\nRaymond Craig Turner", "# Reasoning Path:\nTina Turner -> people.person.children -> Raymond Craig Turner -> people.person.parents -> Raymond Hill\n# Answer:\nRaymond Craig Turner", "# Reasoning Path:\nTina Turner -> people.person.children -> Ronnie Turner -> people.person.spouse_s -> m.0t60dj8\n# Answer:\nRonnie Turner", "# Reasoning Path:\nTina Turner -> people.person.children -> Ronnie Turner -> common.topic.notable_types -> Film actor\n# Answer:\nRonnie Turner", "# Reasoning Path:\nTina Turner -> award.award_nominee.award_nominations -> m.05bnxy1 -> award.award_nomination.award_nominee -> Bernie Grundman\n# Answer:\nm.05bnxy1", "# Reasoning Path:\nTina Turner -> award.award_nominee.award_nominations -> m.05bnxy1 -> award.award_nomination.nominated_for -> River: The Joni Letters\n# Answer:\nm.05bnxy1", "# Reasoning Path:\nTina Turner -> award.award_nominee.award_nominations -> m.0_tlmwl -> award.award_nomination.nominated_for -> Mad Max Beyond Thunderdome\n# Answer:\nm.0_tlmwl"], "ground_truth": ["Ronnie Turner", "Raymond Craig Turner"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1656", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Armored car -> common.topic.article -> m.0y8z\n# Answer:\nArmored car", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Armored car -> base.ontologies.ontology_instance.equivalent_instances -> m.07ndlr9\n# Answer:\nArmored car", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Triple Barrel Canon -> common.topic.notable_for -> g.1yl5vgrll\n# Answer:\nTriple Barrel Canon", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Triple Barrel Canon -> common.topic.notable_types -> Invention\n# Answer:\nTriple Barrel Canon", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> influence.influence_node.influenced_by -> Melozzo da Forl\u00ec\n# Answer:\nAntonio da Correggio", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Ornithopter -> aviation.aircraft_type.aircraft_of_this_type -> SmartBird\n# Answer:\nOrnithopter", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Ornithopter -> common.topic.image -> Cybird\n# Answer:\nOrnithopter", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi -> visual_art.artwork.art_subject -> Biblical Magi\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi -> visual_art.artwork.locations -> m.0pcqbm3\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Ornithopter -> common.topic.image -> Skybird\n# Answer:\nOrnithopter"], "ground_truth": ["33-Barreled Organ", "Double hull", "Diving suit", "Aerial Screw", "Triple Barrel Canon", "Ornithopter", "Armored car", "Parachute", "Viola organista", "Anemometer"], "ans_acc": 0.3, "ans_hit": 1, "ans_f1": 0.42, "ans_precission": 0.7, "ans_recall": 0.3, "path_f1": 0.42, "path_precision": 0.7, "path_recall": 0.3, "path_ans_f1": 0.42, "path_ans_precision": 0.7, "path_ans_recall": 0.3}
{"id": "WebQTest-1657", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.season -> 1992\u201393 NBA season\n# Answer:\nm.02kbc3w", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc5c -> sports.sports_award.season -> 1999\u20132000 NBA season\n# Answer:\nm.02kbc5c", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.team -> Orlando Magic\n# Answer:\nm.02kbc3w", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc58 -> sports.sports_award.season -> 1999\u20132000 NBA season\n# Answer:\nm.02kbc58", "# Reasoning Path:\nShaquille O'Neal -> sports.drafted_athlete.drafted -> m.04fw77r -> sports.sports_league_draft_pick.draft -> 1992 NBA draft\n# Answer:\nm.04fw77r", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.season -> 1992\u201393 NBA season\n# Answer:\nm.04qjyzm", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc5c -> sports.sports_award.team -> Los Angeles Lakers\n# Answer:\nm.02kbc5c", "# Reasoning Path:\nShaquille O'Neal -> sports.drafted_athlete.drafted -> m.04fw77r -> sports.sports_league_draft_pick.team -> Orlando Magic\n# Answer:\nm.04fw77r", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.season -> 2003\u201304 NBA season\n# Answer:\nm.04q9532", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nm.04qjyzm"], "ground_truth": ["1992 NBA draft"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1658", "prediction": ["# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Republic of Kosovo -> location.location.contains -> Community of Serb municipalities, Kosovo\n# Answer:\nRepublic of Kosovo", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Austria -> location.country.languages_spoken -> Bosnian language\n# Answer:\nAustria", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Republic of Kosovo -> location.location.contains -> Crnoljeva\n# Answer:\nRepublic of Kosovo", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Republic of Kosovo -> location.country.official_language -> Albanian language\n# Answer:\nRepublic of Kosovo", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Austria -> location.country.languages_spoken -> German Language\n# Answer:\nAustria", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Republic of Kosovo -> location.location.contains -> Erenik\n# Answer:\nRepublic of Kosovo", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Republic of Kosovo -> base.schemastaging.disputed_location_claimant.disputed_territories -> Bistrica\n# Answer:\nRepublic of Kosovo", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Austria -> location.country.languages_spoken -> Serbian language\n# Answer:\nAustria", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Austria -> location.country.administrative_divisions -> Burgenland\n# Answer:\nAustria", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Turkey -> location.country.administrative_divisions -> Adana Province\n# Answer:\nTurkey"], "ground_truth": ["Cyprus", "Turkey", "Republic of Kosovo", "Republic of Macedonia", "Austria"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 1.0, "ans_recall": 0.6, "path_f1": 0.5538461538461539, "path_precision": 0.9, "path_recall": 0.4, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 1.0, "path_ans_recall": 0.6}
{"id": "WebQTest-1659", "prediction": ["# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94596 -> common.topic.notable_for -> g.1258sqjf5\n# Answer:\n94596", "# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94596 -> location.location.geolocation -> m.03dnkgn\n# Answer:\n94596", "# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94597 -> location.postal_code.country -> United States of America\n# Answer:\n94597", "# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94596 -> location.location.containedby -> Contra Costa County\n# Answer:\n94596", "# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94595 -> location.location.geolocation -> m.03dnkgg\n# Answer:\n94595", "# Reasoning Path:\nWalnut Creek -> location.statistical_region.population -> g.11b66b709v\n# Answer:\ng.11b66b709v", "# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94597 -> common.topic.notable_types -> Postal Code\n# Answer:\n94597", "# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94597 -> common.topic.notable_for -> g.125bwr97g\n# Answer:\n94597", "# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94595 -> common.topic.notable_types -> Postal Code\n# Answer:\n94595", "# Reasoning Path:\nWalnut Creek -> location.statistical_region.population -> g.11bymm147x\n# Answer:\ng.11bymm147x"], "ground_truth": ["94595", "94597", "94596", "94598"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.8, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.8, "path_ans_recall": 0.75}
{"id": "WebQTest-166", "prediction": ["# Reasoning Path:\nTheodore Lesieg -> people.person.education -> m.0n1kynd -> education.education.institution -> University of Oxford\n# Answer:\nm.0n1kynd", "# Reasoning Path:\nTheodore Lesieg -> people.person.education -> m.04ytk85 -> education.education.institution -> Dartmouth College\n# Answer:\nm.04ytk85", "# Reasoning Path:\nTheodore Lesieg -> people.person.education -> m.03p87mv -> education.education.institution -> Lincoln College, Oxford\n# Answer:\nm.03p87mv", "# Reasoning Path:\nTheodore Lesieg -> book.author.works_written -> ARE YOU MY MOTHER MINI PB -> common.topic.notable_for -> g.125fbm0pz\n# Answer:\nARE YOU MY MOTHER MINI PB", "# Reasoning Path:\nTheodore Lesieg -> book.author.works_written -> ARE YOU MY MOTHER MINI PB -> common.topic.notable_types -> Book\n# Answer:\nARE YOU MY MOTHER MINI PB", "# Reasoning Path:\nTheodore Lesieg -> book.author.works_written -> Bartholomew and the Oobleck -> freebase.valuenotation.has_value -> Date written\n# Answer:\nBartholomew and the Oobleck", "# Reasoning Path:\nTheodore Lesieg -> music.artist.track -> A Quarter of Dawn -> music.recording.releases -> Dr. Seuss' How the Grinch Stole Christmas! & Horton Hears a Who!\n# Answer:\nA Quarter of Dawn", "# Reasoning Path:\nTheodore Lesieg -> music.artist.track -> A Quarter of Dawn -> music.recording.contributions -> m.0110d72d\n# Answer:\nA Quarter of Dawn", "# Reasoning Path:\nTheodore Lesieg -> music.artist.track -> Be Kind to Your Small Person Friends -> music.recording.tracks -> Be Kind to Your Small Person Friends (song only) (Horton)\n# Answer:\nBe Kind to Your Small Person Friends", "# Reasoning Path:\nTheodore Lesieg -> book.author.works_written -> And to Think That I Saw It on Mulberry Street -> common.topic.article -> m.05_th_\n# Answer:\nAnd to Think That I Saw It on Mulberry Street"], "ground_truth": ["Lincoln College, Oxford", "Dartmouth College", "University of Oxford"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1661", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.actor -> James Earl Jones\n# Answer:\nm.02nv74t", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.actor -> James Earl Jones\n# Answer:\nm.02nwtmz", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.film -> Star Wars\n# Answer:\nm.02nv74t", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Uncredited\n# Answer:\nm.02nv74t", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Voice\n# Answer:\nm.02nv74t", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.film -> Return of the Jedi\n# Answer:\nm.02nwtmz"], "ground_truth": ["James Earl Jones"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1662", "prediction": ["# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Hong Kong -> location.location.containedby -> Asia\n# Answer:\nHong Kong", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Lesotho -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\nLesotho", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.locations.countries.continent -> North America\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> education.school_category.schools_of_this_kind -> Piers Midwinter -> people.person.quotations -> Learn with a NEST. Please be my GUEST. I will give you my BEST,\n# Answer:\nPiers Midwinter", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.country.form_of_government -> Parliamentary system\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Lesotho -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nLesotho", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Hong Kong -> location.location.containedby -> China\n# Answer:\nHong Kong", "# Reasoning Path:\nEnglish Language -> education.school_category.schools_of_this_kind -> Piers Midwinter -> common.topic.notable_types -> Organization founder\n# Answer:\nPiers Midwinter", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Lesotho -> location.location.containedby -> Africa\n# Answer:\nLesotho"], "ground_truth": ["Japan", "Qatar", "Kiribati", "Rwanda", "Samoa", "Zimbabwe", "Australia", "Jordan", "Bangladesh", "Turks and Caicos Islands", "State of Palestine", "Saint Vincent and the Grenadines", "United Kingdom", "Sudan", "Philippines", "Kingdom of Great Britain", "Malta", "Mandatory Palestine", "India", "Lesotho", "Wales", "South Africa", "Bermuda", "Kenya", "Ghana", "England", "Territory of New Guinea", "Malaysia", "South Yemen", "Barbados", "Cura\u00e7ao", "Bahamas", "Transkei", "Singapore", "Saint Kitts and Nevis", "Liberia", "Israel", "Puerto Rico", "Antigua and Barbuda", "Grenada", "Republic of Ireland", "Bonaire", "Swaziland", "Sri Lanka", "Timor-Leste", "Cook Islands", "Vanuatu", "Nauru", "Cyprus", "Marshall Islands", "China", "Montserrat", "Territory of Papua and New Guinea", "Uganda", "Gazankulu", "Indonesia", "Guam", "Dominica", "Pakistan", "Ethiopia", "Saint Lucia", "Tuvalu", "New Zealand", "Tokelau", "Honduras", "Papua New Guinea", "Gambia", "Canada", "Cayman Islands", "United States of America", "Tanzania", "Zambia", "Botswana", "Cameroon", "Hong Kong", "Nigeria", "Isle of Man", "Belize", "Gibraltar", "Namibia", "Laos", "Guyana", "Jersey", "Fiji", "Brunei", "Vatican City", "Sierra Leone"], "ans_acc": 0.04597701149425287, "ans_hit": 1, "ans_f1": 0.06611570247933884, "ans_precission": 0.8, "ans_recall": 0.034482758620689655, "path_f1": 0.06611570247933884, "path_precision": 0.8, "path_recall": 0.034482758620689655, "path_ans_f1": 0.08695652173913043, "path_ans_precision": 0.8, "path_ans_recall": 0.04597701149425287}
{"id": "WebQTest-1663", "prediction": ["# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.instrumentalists -> Bix Beiderbecke\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> common.topic.notable_types -> Musical instrument\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> music.instrument.family -> Brass instrument\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.family -> Brass instrument\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.artist.genre -> Dixieland -> music.genre.albums -> Jazz\n# Answer:\nDixieland", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.instrumentalists -> James Morrison\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.instrumentalists -> King Oliver\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> book.written_work.subjects -> Trumpeter\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.artist.genre -> Traditional pop music -> common.topic.notable_types -> Musical genre\n# Answer:\nTraditional pop music", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> common.topic.notable_types -> Book\n# Answer:\nTrumpet"], "ground_truth": ["Scat singing", "Traditional pop music", "Dixieland", "Swing music", "Jazz"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.26666666666666666, "ans_precission": 0.2, "ans_recall": 0.4, "path_f1": 0.26666666666666666, "path_precision": 0.2, "path_recall": 0.4, "path_ans_f1": 0.3, "path_ans_precision": 0.2, "path_ans_recall": 0.6}
{"id": "WebQTest-1664", "prediction": ["# Reasoning Path:\nGermany -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nGermany -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> Europe\n# Answer:\nEarth", "# Reasoning Path:\nGermany -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> Sovereign state\n# Answer:\nEarth", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\ng.11b60v9zy7", "# Reasoning Path:\nGermany -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.uncommon.topic.exceptions -> City/Town/Village should pertain to Ghana, since City/Town/Village subdivides Ghanaian Municipal District.\n# Answer:\nEarth", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fdl8\n# Answer:\ng.12tb6fdl8", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc38c_4\n# Answer:\ng.1hhc38c_4"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 1.0}
{"id": "WebQTest-1665", "prediction": ["# Reasoning Path:\nLatin Language -> base.rosetta.languoid.parent -> Latino-Faliscan Group -> base.rosetta.languoid.parent -> Italic Group\n# Answer:\nLatino-Faliscan Group", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> language.language_family.languages -> Aequian language\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> common.topic.image -> Iron Age Italy\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> common.topic.notable_for -> g.125d12rwn\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> language.language_family.languages -> Angevin dialect\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Indo-European languages -> language.language_family.sub_families -> Italic languages\n# Answer:\nIndo-European languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> language.language_family.languages -> Augeron\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Indo-European languages -> common.topic.notable_types -> Language Family\n# Answer:\nIndo-European languages", "# Reasoning Path:\nLatin Language -> base.rosetta.languoid.parent -> Latino-Faliscan Group -> base.rosetta.languoid.languoid_class -> Group\n# Answer:\nLatino-Faliscan Group", "# Reasoning Path:\nLatin Language -> music.genre.albums -> Acustico -> music.album.artist -> La 5\u00aa Estaci\u00f3n\n# Answer:\nAcustico"], "ground_truth": ["Indo-European languages", "Italic languages"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1666", "prediction": ["# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Altare della Patria -> common.topic.notable_types -> Tourist attraction\n# Answer:\nAltare della Patria", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Vatican City -> travel.travel_destination.tourist_attractions -> Apostolic Palace\n# Answer:\nVatican City", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Altare della Patria -> common.topic.image -> Monumento Vittorio Emmanuele II Rom\n# Answer:\nAltare della Patria", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Palazzo Venezia -> architecture.building.building_function -> Art Gallery\n# Answer:\nPalazzo Venezia", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Palazzo Venezia -> common.topic.notable_for -> g.125dc9xn4\n# Answer:\nPalazzo Venezia", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Vatican City -> travel.travel_destination.tourist_attractions -> Castel Sant'Angelo\n# Answer:\nVatican City", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Altare della Patria -> common.topic.image -> The Monument of Victor Emmanuel II\n# Answer:\nAltare della Patria", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Palazzo Venezia -> architecture.building.building_function -> Palace\n# Answer:\nPalazzo Venezia", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Vatican City -> travel.travel_destination.tourist_attractions -> Gardens of Vatican City\n# Answer:\nVatican City", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Altare della Patria -> common.topic.image -> The monument of Victor Emmanuel II\n# Answer:\nAltare della Patria"], "ground_truth": ["Appian Way", "Colosseum", "Palazzo Barberini", "Pyramid of Cestius", "Seven Pilgrim Churches of Rome", "Galleria Borghese", "Sistine Chapel", "Santa Maria sopra Minerva", "Piazza Navona", "Basilica di Santa Maria Maggiore", "Quirinal Palace", "Palazzo Spada", "Apostolic Palace", "Altare della Patria", "Palazzo Chigi", "Baths of Caracalla", "Capitoline Hill", "Roma-Ostia Half Marathon", "Palazzo Venezia", "Villa d'Este", "Policlinico Umberto I", "Rome Observatory", "Piazza Venezia", "Spanish Steps", "Castel Sant'Angelo", "Trevi Fountain", "Raphael Rooms", "Piazza del Popolo", "Via Veneto", "Domus Aurea", "St. Peter's Basilica", "Tabularium", "Bocca della Verit\u00e0", "Basilica of Saint Paul Outside the Walls", "Roman Forum", "Vatican Museums", "Circus Maximus", "Bioparco di Roma", "Hadrian's Villa", "Vatican City", "Catacombs of Rome", "Fontana delle Naiadi", "Pantheon", "Archbasilica of St. John Lateran", "Palazzo Farnese", "San Nicola in Carcere"], "ans_acc": 0.10869565217391304, "ans_hit": 1, "ans_f1": 0.12244897959183672, "ans_precission": 1.0, "ans_recall": 0.06521739130434782, "path_f1": 0.12244897959183672, "path_precision": 1.0, "path_recall": 0.06521739130434782, "path_ans_f1": 0.19607843137254902, "path_ans_precision": 1.0, "path_ans_recall": 0.10869565217391304}
{"id": "WebQTest-1667", "prediction": ["# Reasoning Path:\nSolomon -> people.person.children -> Rehoboam -> people.person.children -> Abijah of Judah\n# Answer:\nRehoboam", "# Reasoning Path:\nSolomon -> people.person.children -> Menelik I -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nMenelik I", "# Reasoning Path:\nSolomon -> people.person.children -> Menelik I -> common.topic.article -> m.01qn7g\n# Answer:\nMenelik I", "# Reasoning Path:\nSolomon -> people.person.children -> Menelik I -> people.person.gender -> Male\n# Answer:\nMenelik I", "# Reasoning Path:\nSolomon -> people.person.children -> Menelik I -> freebase.valuenotation.has_value -> Date of death\n# Answer:\nMenelik I", "# Reasoning Path:\nSolomon -> people.person.children -> Rehoboam -> people.person.religion -> Judaism\n# Answer:\nRehoboam", "# Reasoning Path:\nSolomon -> people.person.children -> Menelik I -> freebase.valuenotation.has_value -> Place of death\n# Answer:\nMenelik I", "# Reasoning Path:\nSolomon -> people.person.children -> Rehoboam -> royalty.monarch.royal_line -> Davidic line\n# Answer:\nRehoboam", "# Reasoning Path:\nSolomon -> royalty.noble_person.titles -> m.0cvmm_r -> royalty.noble_title_tenure.noble_title -> King of Israel\n# Answer:\nm.0cvmm_r", "# Reasoning Path:\nSolomon -> film.film_subject.films -> Solomon & Sheba -> film.film.starring -> m.0wc5tbb\n# Answer:\nSolomon & Sheba"], "ground_truth": ["Menelik I", "Rehoboam"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1668", "prediction": ["# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.causes -> Cocaine\n# Answer:\nheart attack", "# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.causes -> Amphetamine\n# Answer:\nheart attack", "# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> ACE inhibitor\n# Answer:\nheart attack", "# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.causes -> Aneurysm\n# Answer:\nheart attack", "# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w0hpl\n# Answer:\nheart attack", "# Reasoning Path:\nAndy Irons -> sports.pro_athlete.sports_played_professionally -> m.0z6442d -> sports.pro_sports_played.sport -> Surfing\n# Answer:\nm.0z6442d", "# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> Acebutolol\n# Answer:\nheart attack", "# Reasoning Path:\nAndy Irons -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> Anisindione\n# Answer:\nheart attack", "# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w4lwz\n# Answer:\nheart attack"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1669", "prediction": ["# Reasoning Path:\nAuburn University -> location.location.containedby -> Auburn -> location.location.containedby -> Alabama\n# Answer:\nAuburn", "# Reasoning Path:\nAuburn University -> location.location.containedby -> Alabama -> location.location.containedby -> United States of America\n# Answer:\nAlabama", "# Reasoning Path:\nAuburn University -> location.location.containedby -> Auburn -> location.location.containedby -> Lee County\n# Answer:\nAuburn", "# Reasoning Path:\nAuburn University -> location.location.containedby -> Auburn -> location.location.containedby -> United States of America\n# Answer:\nAuburn", "# Reasoning Path:\nAuburn University -> location.location.containedby -> Auburn -> location.location.contains -> Auburn University Historic District\n# Answer:\nAuburn", "# Reasoning Path:\nAuburn University -> location.location.containedby -> Auburn University Historic District -> location.location.containedby -> Auburn\n# Answer:\nAuburn University Historic District", "# Reasoning Path:\nAuburn University -> location.location.containedby -> Auburn -> location.statistical_region.population -> g.11b674pwnw\n# Answer:\nAuburn", "# Reasoning Path:\nAuburn University -> location.location.containedby -> Alabama -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nAlabama", "# Reasoning Path:\nAuburn University -> education.university.international_tuition -> m.0zsbgkr -> measurement_unit.dated_money_value.currency -> United States Dollar\n# Answer:\nm.0zsbgkr", "# Reasoning Path:\nAuburn University -> location.location.containedby -> Alabama -> base.aareas.schema.administrative_area.capital -> Montgomery\n# Answer:\nAlabama"], "ground_truth": ["Auburn University Historic District", "Alabama", "United States of America", "Auburn"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.8181818181818182, "path_precision": 0.9, "path_recall": 0.75, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-167", "prediction": ["# Reasoning Path:\nQueensland -> symbols.namesake.named_after -> Queen Victoria\n# Answer:\nQueen Victoria", "# Reasoning Path:\nQueensland -> government.political_district.representatives -> m.04_87_t -> government.government_position_held.office_position_or_title -> Australian Senator\n# Answer:\nm.04_87_t", "# Reasoning Path:\nQueensland -> government.political_district.representatives -> m.04_87_t -> government.government_position_held.office_holder -> Len Harris\n# Answer:\nm.04_87_t", "# Reasoning Path:\nQueensland -> government.political_district.representatives -> m.04_8828 -> government.government_position_held.governmental_body -> Australian Senate\n# Answer:\nm.04_8828", "# Reasoning Path:\nQueensland -> government.political_district.representatives -> m.04_fgk9 -> government.government_position_held.office_holder -> Brenda Gibbs\n# Answer:\nm.04_fgk9", "# Reasoning Path:\nQueensland -> government.governmental_jurisdiction.governing_officials -> m.0104n88_ -> government.government_position_held.office_holder -> Francis Isidore Power\n# Answer:\nm.0104n88_", "# Reasoning Path:\nQueensland -> government.governmental_jurisdiction.governing_officials -> m.0107h9yk -> freebase.valuenotation.is_reviewed -> To\n# Answer:\nm.0107h9yk", "# Reasoning Path:\nQueensland -> government.political_district.representatives -> m.04_8828 -> government.government_position_held.office_holder -> John Herron\n# Answer:\nm.04_8828", "# Reasoning Path:\nQueensland -> government.governmental_jurisdiction.governing_officials -> m.0104n88_ -> government.government_position_held.basic_title -> Member of Legislative Council\n# Answer:\nm.0104n88_", "# Reasoning Path:\nQueensland -> government.political_district.representatives -> m.04_8828 -> government.government_position_held.office_position_or_title -> Australian Senator\n# Answer:\nm.04_8828"], "ground_truth": ["Queen Victoria"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1670", "prediction": ["# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sonia Ali -> common.topic.notable_for -> g.125980l9b\n# Answer:\nSonia Ali", "# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sonia Ali -> people.person.children -> Anastasia N. Ali\n# Answer:\nSonia Ali", "# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sonia Ali -> freebase.valuenotation.has_value -> Parents\n# Answer:\nSonia Ali", "# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sonia Ali -> people.person.children -> Kimberly Ali\n# Answer:\nSonia Ali", "# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sheriff Ali -> people.person.children -> Anastasia N. Ali\n# Answer:\nSheriff Ali", "# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sheriff Ali -> common.topic.notable_types -> Person\n# Answer:\nSheriff Ali", "# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sonia Ali -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nSonia Ali", "# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sheriff Ali -> people.person.spouse_s -> m.0r8wjcd\n# Answer:\nSheriff Ali", "# Reasoning Path:\nTatyana Ali -> common.topic.webpage -> m.03l4yjy -> common.webpage.resource -> m.0bjkgr1\n# Answer:\nm.03l4yjy", "# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sheriff Ali -> people.person.children -> Kimberly Ali\n# Answer:\nSheriff Ali"], "ground_truth": ["Sonia Ali", "Sheriff Ali"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1671", "prediction": ["# Reasoning Path:\nLeif Eriksson -> people.person.place_of_birth -> K\u00f6ping -> common.topic.notable_types -> Statistical region\n# Answer:\nK\u00f6ping", "# Reasoning Path:\nLeif Eriksson -> people.person.place_of_birth -> K\u00f6ping -> location.location.people_born_here -> Daniel Gunnarsson\n# Answer:\nK\u00f6ping", "# Reasoning Path:\nLeif Eriksson -> people.person.place_of_birth -> K\u00f6ping -> common.topic.notable_for -> g.1259m4hv9\n# Answer:\nK\u00f6ping", "# Reasoning Path:\nLeif Eriksson -> sports.pro_athlete.sports_played_professionally -> m.0d5r_4t -> sports.pro_sports_played.sport -> Football\n# Answer:\nm.0d5r_4t", "# Reasoning Path:\nLeif Eriksson -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nLeif Eriksson -> people.person.gender -> Male -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Males\n# Answer:\nMale", "# Reasoning Path:\nLeif Eriksson -> people.person.gender -> Male -> common.topic.notable_for -> g.1255p_jgc\n# Answer:\nMale", "# Reasoning Path:\nLeif Eriksson -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nLeif Eriksson -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Acute myeloid leukemia\n# Answer:\nMale"], "ground_truth": ["K\u00f6ping"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-1672", "prediction": ["# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ng.1256fv3pv", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.parent -> Ongamo-Maa Group\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.countries_spoken_in -> Kenya\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> common.topic.article -> m.064klf\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.countries_spoken_in -> Tanzania\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai -> common.image.size -> m.02bgrp_\n# Answer:\nMaasai", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai women and children -> common.image.size -> m.03qy0z9\n# Answer:\nMaasai women and children", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai-jump -> common.image.size -> m.02br_p7\n# Answer:\nMaasai-jump"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1673", "prediction": ["# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> base.schemastaging.context_name.pronunciation -> g.125_pt37m\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ng.1254zbtvv", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> book.book_subject.works -> Anne Frank and the Children of the Holocaust\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> government.politician.party -> m.0btmmq1 -> government.political_party_tenure.party -> German Workers' Party\n# Answer:\nm.0btmmq1", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> book.periodical_publisher.periodicals_published -> m.0106rcrc\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Hitler Youth -> government.governmental_body.members -> g.11btv6zshl\n# Answer:\nHitler Youth", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Hitler Youth -> common.topic.notable_for -> g.1254xhwx2\n# Answer:\nHitler Youth", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> book.book_subject.works -> The Hidden Life of Otto Frank\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> government.politician.party -> m.075rkrk -> government.political_party_tenure.party -> Nazi Party\n# Answer:\nm.075rkrk", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> book.periodical_publisher.periodicals_published -> m.010dc3hw\n# Answer:\nNazi Party"], "ground_truth": ["Nazi Party", "German Workers' Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1674", "prediction": ["# Reasoning Path:\nNorth Korea -> government.governmental_jurisdiction.governing_officials -> m.0c9_j0f -> government.government_position_held.office_holder -> Kim Il-sung\n# Answer:\nm.0c9_j0f", "# Reasoning Path:\nNorth Korea -> government.governmental_jurisdiction.governing_officials -> m.010r1v7b -> government.government_position_held.office_holder -> Kim Il-sung\n# Answer:\nm.010r1v7b", "# Reasoning Path:\nNorth Korea -> government.governmental_jurisdiction.governing_officials -> m.0c9_j0f -> government.government_position_held.basic_title -> President\n# Answer:\nm.0c9_j0f", "# Reasoning Path:\nNorth Korea -> government.governmental_jurisdiction.governing_officials -> m.0c9_h__ -> government.government_position_held.office_holder -> Kim Jong-il\n# Answer:\nm.0c9_h__", "# Reasoning Path:\nNorth Korea -> location.location.events -> UN Offensive, 1950 -> base.culturalevent.event.entity_involved -> Kim Il-sung\n# Answer:\nUN Offensive, 1950", "# Reasoning Path:\nNorth Korea -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f719\n# Answer:\ng.12tb6f719", "# Reasoning Path:\nNorth Korea -> government.governmental_jurisdiction.governing_officials -> m.010r1v7b -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.010r1v7b", "# Reasoning Path:\nNorth Korea -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6fdwt\n# Answer:\ng.12tb6fdwt", "# Reasoning Path:\nNorth Korea -> government.governmental_jurisdiction.governing_officials -> m.0c9_h__ -> government.government_position_held.office_position_or_title -> Supreme Leader of North Korea\n# Answer:\nm.0c9_h__", "# Reasoning Path:\nNorth Korea -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6flyr\n# Answer:\ng.12tb6flyr"], "ground_truth": ["Kim Jong-un"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1675", "prediction": ["# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> Qu\u00e9bec -> base.aareas.schema.administrative_area.capital -> Quebec City\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> Qu\u00e9bec -> government.governmental_jurisdiction.agencies -> Autorit\u00e9 des march\u00e9s financiers\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> symbols.name_source.namesakes -> USS New Hampshire\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> Qu\u00e9bec -> location.statistical_region.population -> m.010f3t4y\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Jersey -> location.location.partially_contains -> Appalachian National Scenic Trail\n# Answer:\nNew Jersey", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> Qu\u00e9bec -> government.governmental_jurisdiction.agencies -> Commission scolaire de la Pointe-de-l'\u00cele\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> symbols.name_source.namesakes -> USS New Hampshire (1864)\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> government.political_district.representatives -> m.010d4j50\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> geography.mountain_range.passes -> Appalachian Gap -> common.topic.notable_for -> g.1258dt_t3\n# Answer:\nAppalachian Gap", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Jersey -> location.location.partiallycontains -> m.0wg8ml3\n# Answer:\nNew Jersey"], "ground_truth": ["New Jersey", "South Carolina", "Nova Scotia", "North Carolina", "New York", "New Brunswick", "Newfoundland and Labrador", "Massachusetts", "Georgia", "Virginia", "Tennessee", "Qu\u00e9bec", "West Virginia", "Alabama", "New Hampshire", "Kentucky", "Ohio", "New England", "Maine", "Prince Edward Island", "Pennsylvania", "Saint Pierre and Miquelon", "Vermont", "Connecticut"], "ans_acc": 0.125, "ans_hit": 1, "ans_f1": 0.21951219512195125, "ans_precission": 0.9, "ans_recall": 0.125, "path_f1": 0.21951219512195125, "path_precision": 0.9, "path_recall": 0.125, "path_ans_f1": 0.21951219512195125, "path_ans_precision": 0.9, "path_ans_recall": 0.125}
{"id": "WebQTest-1676", "prediction": ["# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Andorra\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Belgium\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> United States of America\n# Answer:\nEarth", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> France\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> France\n# Answer:\nEarth", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\ng.1hhc37x4q", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> City\n# Answer:\nEarth", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc38hhk\n# Answer:\ng.1hhc38hhk", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> Africa\n# Answer:\nEarth"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1677", "prediction": ["# Reasoning Path:\nTerry Waite -> people.person.place_of_birth -> Bollington -> location.location.containedby -> Macclesfield\n# Answer:\nBollington", "# Reasoning Path:\nTerry Waite -> people.person.place_of_birth -> Bollington -> common.topic.notable_types -> English civil parish\n# Answer:\nBollington", "# Reasoning Path:\nTerry Waite -> people.person.place_of_birth -> Bollington -> location.location.containedby -> United Kingdom\n# Answer:\nBollington", "# Reasoning Path:\nTerry Waite -> people.person.nationality -> United Kingdom -> base.aareas.schema.administrative_area.administrative_parent -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nTerry Waite -> people.person.nationality -> United Kingdom -> base.popstra.location.arrestee -> m.0ghc35h\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nTerry Waite -> people.person.nationality -> United Kingdom -> location.statistical_region.long_term_unemployment_rate -> g.12cp_jvpx\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nTerry Waite -> book.author.works_written -> A Pub Of Our Own -> common.topic.notable_for -> g.1258sqnrz\n# Answer:\nA Pub Of Our Own", "# Reasoning Path:\nTerry Waite -> book.author.works_written -> A Pub Of Our Own -> book.book.genre -> Fiction\n# Answer:\nA Pub Of Our Own", "# Reasoning Path:\nTerry Waite -> people.person.nationality -> United Kingdom -> location.statistical_region.long_term_unemployment_rate -> g.1hhc390lx\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nTerry Waite -> book.author.works_written -> Footfalls In Memory - Reflections From Solitude -> common.topic.notable_types -> Book\n# Answer:\nFootfalls In Memory - Reflections From Solitude"], "ground_truth": ["Bollington"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1678", "prediction": ["# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> travel.travel_destination.tourist_attractions -> Baconao\n# Answer:\nSantiago de Cuba", "# Reasoning Path:\nBattle of San Juan Hill -> base.culturalevent.event.entity_involved -> Theodore Roosevelt -> people.person.nationality -> United States of America\n# Answer:\nTheodore Roosevelt", "# Reasoning Path:\nBattle of San Juan Hill -> common.image.size -> m.02cgdw0\n# Answer:\nm.02cgdw0", "# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> travel.travel_destination.tourist_attractions -> Bas\u00edlica Santuario Nacional de Nuestra Se\u00f1ora de la Caridad del Cobre\n# Answer:\nSantiago de Cuba", "# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> location.statistical_region.population -> g.11b7tdz1nw\n# Answer:\nSantiago de Cuba", "# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> travel.travel_destination.tourist_attractions -> Carnival of Santiago de Cuba\n# Answer:\nSantiago de Cuba", "# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> location.location.events -> Attack on moncada barracks\n# Answer:\nSantiago de Cuba", "# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> location.statistical_region.population -> g.11bc87rwmn\n# Answer:\nSantiago de Cuba", "# Reasoning Path:\nBattle of San Juan Hill -> base.culturalevent.event.entity_involved -> Arsenio Linares y Pombo -> people.person.place_of_birth -> Valencia\n# Answer:\nArsenio Linares y Pombo", "# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> location.statistical_region.population -> g.11btt6x2r4\n# Answer:\nSantiago de Cuba"], "ground_truth": ["Santiago de Cuba"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1679", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> American Samoa\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\ng.125_r6z32", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Germany\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Germany\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guam\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nm.010r7f0z", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Mexico\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Marshall Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> periodicals.newspaper_circulation_area.newspapers -> American Free Press -> book.periodical.language -> English Language\n# Answer:\nAmerican Free Press"], "ground_truth": ["Constitutional republic", "Presidential system", "Federal republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-168", "prediction": ["# Reasoning Path:\nJames Madison -> book.author.works_written -> Federalist No. 14 -> common.topic.notable_types -> Short Non-fiction\n# Answer:\nFederalist No. 14", "# Reasoning Path:\nJames Madison -> book.author.works_written -> Federalist No. 14 -> common.topic.image -> James Madison, authorof Federalist No. 47\n# Answer:\nFederalist No. 14", "# Reasoning Path:\nJames Madison -> organization.organization_founder.organizations_founded -> 30th Indiana Infantry Regiment -> military.military_unit.place_of_origin -> Indiana\n# Answer:\n30th Indiana Infantry Regiment", "# Reasoning Path:\nJames Madison -> book.author.works_written -> A vocabulary of New Jersey Delaware -> common.topic.notable_for -> g.1254ztrh7\n# Answer:\nA vocabulary of New Jersey Delaware", "# Reasoning Path:\nJames Madison -> organization.organization_founder.organizations_founded -> Democratic Party -> organization.organization.founders -> Thomas Jefferson\n# Answer:\nDemocratic Party", "# Reasoning Path:\nJames Madison -> book.author.works_written -> All impressments unlawful and inadmissible -> common.topic.notable_for -> g.1255wc3m2\n# Answer:\nAll impressments unlawful and inadmissible", "# Reasoning Path:\nJames Madison -> symbols.name_source.namesakes -> Madison -> location.statistical_region.population -> g.11b66b70hd\n# Answer:\nMadison", "# Reasoning Path:\nJames Madison -> book.author.works_written -> All impressments unlawful and inadmissible -> common.topic.notable_types -> Book\n# Answer:\nAll impressments unlawful and inadmissible", "# Reasoning Path:\nJames Madison -> organization.organization_founder.organizations_founded -> 30th Indiana Infantry Regiment -> military.military_unit.armed_force -> Union Army\n# Answer:\n30th Indiana Infantry Regiment", "# Reasoning Path:\nJames Madison -> organization.organization_founder.organizations_founded -> Democratic-Republican Party -> organization.organization.founders -> Thomas Jefferson\n# Answer:\nDemocratic-Republican Party"], "ground_truth": ["The Papers of James Madison, Secretary of State Series, Vol. 3", "Federalist No. 18", "All impressments unlawful and inadmissible", "James Madison: Writings", "Federalist No. 58", "An examination of the British doctrine", "The Federalist Papers", "The complete Madison", "Federalist No. 47", "The Papers of James Madison, Secretary of State Series, Vol. 4", "Federalist No. 45", "Papers of James Madison Volume 10: May 27, 1787-March 3, 1788", "Calendar of the correspondence of James Madison", "Jonathan Bull and Mary Bull", "Federalist No. 50", "Letters and other writings of James Madison", "The Papers of James Madison, Secretary of State Series, Vol. 6", "Federalist No. 46", "Federalist No. 49", "The Papers of James Madison, Presidential Series Vol. 3", "Federalist No. 37", "Federalist No. 53", "The Papers of James Madison, Secretary of State Series, Vol. 7", "An address delivered before the Agricultural Society of Albemarle, on Tuesday, May 12, 1818", "The Papers of James Madison, Secretary of State Series, Vol. 8", "Federalist No. 41", "Religious freedom", "Federalist No. 19", "Federalist No. 20", "Federalist No. 39", "President Madison's inaugural speech", "Federalist No. 56", "Federalist No. 62", "Federalist No. 63", "Letters from the Secretary of State to Messrs. Monroe and Pinkney, on subjects committed to their joint negotiations", "The Papers of James Madison, Presidential Series Vol. 2", "Federalist No. 40", "Federalist No. 54", "Federalist No. 43", "The reply of Mr. Madison, in answer to Mr. Rose, in discussing the affair of the Chesapeake", "Federalist No. 42", "The mind of the founder", "Letters from the Secretary of State to Mr. Monroe, on the subject of impressments, &c", "Equal religious liberty stated and defended", "The Papers of James Madison, Secretary of State Series, Vol. 1", "Federalist No. 48", "Federalist No. 38", "Mr. Madison's motion for commercial restrictions", "Extract of a letter from the Secretary of State to Mr. Monroe, relative to impressments", "The Papers of James Madison, Secretary of State Series, Vol. 9", "The Papers of James Madison, Presidential Series Vol. 5", "The forging of American federalism", "James Madison, 1751-1836", "The Papers of James Madison Retirement Series, Volume 1: 4 March 1817-31 January 1820", "Federalist No. 44", "Federalist No. 14", "Federalist No. 52", "Selections from the private correspondence of James Madison, from 1813 to 1836", "The James Madison Papers, 1723 - 1836", "The Papers of James Madison, Secretary of State Series, Vol. 5", "The Papers of James Madison, Presidential Series Vol. 4", "The Papers of James Madison, Presidential Series Vol. 6", "A vocabulary of New Jersey Delaware", "Federalist No. 10", "An examination of the British doctrine, which subjects to capture a neutral trade, not open in time of peace", "The Papers of James Madison, Vol. 4", "The Papers of James Madison, Secretary of State Series, Vol. 2", "Federalist No. 55", "The Papers of James Madison, Presidential Series Vol. 7", "The Papers of James Madison, Presidential Series Vol. 1", "Federalist No. 57", "Federalist No. 51", "The Papers of James Madison Congressional Series, Vol. 12: 2 October 1789 - 20 January 1790"], "ans_acc": 0.0547945205479452, "ans_hit": 1, "ans_f1": 0.0759493670886076, "ans_precission": 0.5, "ans_recall": 0.0410958904109589, "path_f1": 0.0759493670886076, "path_precision": 0.5, "path_recall": 0.0410958904109589, "path_ans_f1": 0.09876543209876543, "path_ans_precision": 0.5, "path_ans_recall": 0.0547945205479452}
{"id": "WebQTest-1680", "prediction": ["# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1983 World Series -> sports.sports_championship_event.championship -> World Series\n# Answer:\n1983 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1983 World Series -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1983 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1970 World Series -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1970 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1970 World Series -> sports.sports_championship_event.runner_up -> Cincinnati Reds\n# Answer:\n1970 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1970 World Series -> sports.sports_championship_event.season -> 1970 Major League Baseball Season\n# Answer:\n1970 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1966 World Series -> sports.sports_championship_event.season -> 1966 Major League Baseball Season\n# Answer:\n1966 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.venue -> m.0wz1z2b -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0wz1z2b", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1966 World Series -> common.topic.notable_for -> g.125br8lv_\n# Answer:\n1966 World Series", "# Reasoning Path:\nBaltimore Orioles -> baseball.baseball_team.team_stats -> m.05n60gg -> baseball.baseball_team_stats.season -> 1954 Major League Baseball season\n# Answer:\nm.05n60gg", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.venue -> m.0wz1z2b -> sports.team_venue_relationship.venue -> Oriole Park at Camden Yards\n# Answer:\nm.0wz1z2b"], "ground_truth": ["1983 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1682", "prediction": ["# Reasoning Path:\nBen Stiller -> base.popstra.celebrity.dated -> m.065q5w_ -> base.popstra.dated.participant -> Janeane Garofalo\n# Answer:\nm.065q5w_", "# Reasoning Path:\nBen Stiller -> base.popstra.celebrity.dated -> m.063fz2s -> base.popstra.dated.participant -> Jeanne Tripplehorn\n# Answer:\nm.063fz2s", "# Reasoning Path:\nBen Stiller -> base.popstra.celebrity.dated -> m.063fz0t -> base.popstra.dated.participant -> Amanda Peet\n# Answer:\nm.063fz0t", "# Reasoning Path:\nBen Stiller -> celebrities.celebrity.sexual_relationships -> m.04dq6cg -> celebrities.romantic_relationship.celebrity -> Jeanne Tripplehorn\n# Answer:\nm.04dq6cg", "# Reasoning Path:\nBen Stiller -> celebrities.celebrity.sexual_relationships -> m.04dq6cq -> celebrities.romantic_relationship.celebrity -> Janeane Garofalo\n# Answer:\nm.04dq6cq", "# Reasoning Path:\nBen Stiller -> celebrities.celebrity.sexual_relationships -> m.04dq6cg -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nm.04dq6cg", "# Reasoning Path:\nBen Stiller -> celebrities.celebrity.sexual_relationships -> m.04dq6cz -> celebrities.romantic_relationship.celebrity -> Amanda Peet\n# Answer:\nm.04dq6cz", "# Reasoning Path:\nBen Stiller -> celebrities.celebrity.sexual_relationships -> m.04dq6cq -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nm.04dq6cq", "# Reasoning Path:\nBen Stiller -> celebrities.celebrity.sexual_relationships -> m.04dq6cz -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nm.04dq6cz", "# Reasoning Path:\nBen Stiller -> film.actor.film -> m.010g_cgd -> film.performance.film -> Nantucket Film Festival's Comedy Roundtable\n# Answer:\nm.010g_cgd"], "ground_truth": ["Claire Forlani", "Amanda Peet", "Rhea Durham", "Jeanne Tripplehorn", "Christine Taylor", "Janeane Garofalo", "Calista Flockhart"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.41379310344827586, "path_precision": 0.6, "path_recall": 0.3157894736842105, "path_ans_f1": 0.5, "path_ans_precision": 0.6, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1683", "prediction": ["# Reasoning Path:\nNorway -> location.country.official_language -> Norwegian Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nNorwegian Language", "# Reasoning Path:\nNorway -> location.country.official_language -> Nynorsk -> language.human_language.language_family -> Indo-European languages\n# Answer:\nNynorsk", "# Reasoning Path:\nNorway -> location.statistical_region.net_migration -> g.1q5jhf535\n# Answer:\ng.1q5jhf535", "# Reasoning Path:\nNorway -> location.country.official_language -> Nynorsk -> language.human_language.writing_system -> Danish and Norwegian alphabet\n# Answer:\nNynorsk", "# Reasoning Path:\nNorway -> location.country.official_language -> Norwegian Language -> common.topic.image -> Norwegianmalforms\n# Answer:\nNorwegian Language", "# Reasoning Path:\nNorway -> location.statistical_region.gdp_nominal_per_capita -> g.11b60vk8pf\n# Answer:\ng.11b60vk8pf", "# Reasoning Path:\nNorway -> location.country.official_language -> Norwegian Language -> common.topic.notable_types -> Human Language\n# Answer:\nNorwegian Language", "# Reasoning Path:\nNorway -> location.country.official_language -> Bokm\u00e5l -> language.human_language.writing_system -> Danish and Norwegian alphabet\n# Answer:\nBokm\u00e5l", "# Reasoning Path:\nNorway -> location.country.official_language -> Bokm\u00e5l -> common.topic.notable_types -> Human Language\n# Answer:\nBokm\u00e5l", "# Reasoning Path:\nNorway -> location.statistical_region.net_migration -> g.1q5jsymtt\n# Answer:\ng.1q5jsymtt"], "ground_truth": ["Nynorsk", "Bokm\u00e5l", "Norwegian Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1684", "prediction": ["# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Geoffrey Chaucer -> influence.influence_node.influenced_by -> Ovid\n# Answer:\nGeoffrey Chaucer", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Lucian -> influence.influence_node.influenced -> Baltasar Graci\u00e1n\n# Answer:\nLucian", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Geoffrey Chaucer -> common.topic.notable_types -> Author\n# Answer:\nGeoffrey Chaucer", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Geoffrey Chaucer -> people.person.ethnicity -> English people\n# Answer:\nGeoffrey Chaucer", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Lucian -> common.topic.notable_types -> Author\n# Answer:\nLucian", "# Reasoning Path:\nWilliam Shakespeare -> freebase.valuenotation.is_reviewed -> Art Form\n# Answer:\nArt Form", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Lucian -> people.person.gender -> Male\n# Answer:\nLucian", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Thomas More -> people.deceased_person.place_of_death -> London\n# Answer:\nThomas More", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Thomas More -> people.person.nationality -> United Kingdom\n# Answer:\nThomas More", "# Reasoning Path:\nWilliam Shakespeare -> freebase.valuenotation.is_reviewed -> Art Subject\n# Answer:\nArt Subject"], "ground_truth": ["John Pory", "Terence", "Thomas More", "Ovid", "Virgil", "Plautus", "Thomas Kyd", "Plutarch", "Edmund Spenser", "Geoffrey Chaucer", "Seneca the Younger", "Christopher Marlowe", "Michel de Montaigne", "Lucian"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.3380281690140845, "ans_precission": 0.8, "ans_recall": 0.21428571428571427, "path_f1": 0.3380281690140845, "path_precision": 0.8, "path_recall": 0.21428571428571427, "path_ans_f1": 0.4210526315789473, "path_ans_precision": 0.8, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-1685", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> education.education.institution -> Boston Latin School\n# Answer:\nm.040vjzw", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.is_reviewed -> Institution\n# Answer:\nm.040vjzw", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nm.040vjzw", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.has_no_value -> Major/Field Of Study\n# Answer:\nm.040vjzw", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nm.012zbkk5", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nm.040vjzw", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.is_reviewed -> End Date\n# Answer:\nm.040vjzw", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> BenFranklinDuplessis -> common.image.size -> m.02fglp1\n# Answer:\nBenFranklinDuplessis", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.is_reviewed -> Start Date\n# Answer:\nm.040vjzw", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> :Library and information science\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze"], "ground_truth": ["Boston Latin School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1686", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y42 -> government.government_position_held.office_holder -> Ernest McFarland\n# Answer:\nm.04j8y42", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y46 -> government.government_position_held.office_holder -> Paul Fannin\n# Answer:\nm.04j8y46", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.0hz834l -> government.government_position_held.office_holder -> Jan Brewer\n# Answer:\nm.0hz834l", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y42 -> government.government_position_held.office_position_or_title -> Governor of Arizona\n# Answer:\nm.04j8y42", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y46 -> government.government_position_held.basic_title -> Governor\n# Answer:\nm.04j8y46", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w1gvc", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.0hz834l -> government.government_position_held.basic_title -> Governor\n# Answer:\nm.0hz834l", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.0hz834l -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nm.0hz834l", "# Reasoning Path:\nArizona -> location.statistical_region.religions -> m.04403h9 -> location.religion_percentage.religion -> Pentecostalism\n# Answer:\nm.04403h9", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.resource -> ATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!\n# Answer:\nm.09w1gvc"], "ground_truth": ["Janet Napolitano", "Jan Brewer"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 0.1, "path_ans_recall": 0.5}
{"id": "WebQTest-1687", "prediction": ["# Reasoning Path:\nSpike Lee -> film.director.film -> 25th Hour -> film.film.rating -> R (USA)\n# Answer:\n25th Hour", "# Reasoning Path:\nSpike Lee -> film.director.film -> 25th Hour -> film.film.language -> English Language\n# Answer:\n25th Hour", "# Reasoning Path:\nSpike Lee -> film.producer.film -> Love & Basketball -> film.film.subjects -> Basketball\n# Answer:\nLove & Basketball", "# Reasoning Path:\nSpike Lee -> film.director.film -> Clockers -> film.film.genre -> Coming of age\n# Answer:\nClockers", "# Reasoning Path:\nSpike Lee -> film.director.film -> Clockers -> film.film.rating -> R (USA)\n# Answer:\nClockers", "# Reasoning Path:\nSpike Lee -> film.producer.film -> Love & Basketball -> media_common.netflix_title.netflix_genres -> Basketball\n# Answer:\nLove & Basketball", "# Reasoning Path:\nSpike Lee -> film.producer.film -> 25th Hour -> film.film.rating -> R (USA)\n# Answer:\n25th Hour", "# Reasoning Path:\nSpike Lee -> film.director.film -> 25th Hour -> media_common.netflix_title.netflix_genres -> Crime Dramas\n# Answer:\n25th Hour", "# Reasoning Path:\nSpike Lee -> film.director.film -> 4 Little Girls -> film.film.cinematography -> Ellen Kuras\n# Answer:\n4 Little Girls", "# Reasoning Path:\nSpike Lee -> film.producer.film -> Love & Basketball -> common.topic.notable_types -> Film\n# Answer:\nLove & Basketball"], "ground_truth": ["Oldboy", "Mo' Better Blues", "Bad 25", "Lumi\u00e8re and Company", "Lovers & Haters", "Miracle at St. Anna", "Da Sweet Blood of Jesus", "Do the Right Thing", "A Huey P. Newton Story", "All the Invisible Children", "Jungle Fever", "And Ya Don't Stop: Hip Hop's Greatest Videos, Vol. 1", "When the Levees Broke: A Requiem in Four Acts", "She Hate Me", "He Got Game", "New Jersey Drive", "Clockers", "Concert For New York City", "The Original Kings of Comedy", "Joe's Bed-Stuy Barbershop: We Cut Heads", "Malcolm X", "Summer of Sam", "Get on the Bus", "4 Little Girls", "Inside Man", "Crooklyn", "Ten Minutes Older: The Trumpet", "Sucker Free City", "The Best Man", "Saint John of Las Vegas", "Passing Strange", "Chiraq", "HIStory on Film, Volume II", "25th Hour", "Red Hook Summer", "Kobe Doin' Work", "School Daze", "Jim Brown: All-American", "M.O.N.Y.", "Girl 6", "Mike Tyson: Undisputed Truth", "Love & Basketball", "Last Hustle in Brooklyn", "Freak", "Bamboozled", "If God Is Willing and da Creek Don't Rise", "She's Gotta Have It"], "ans_acc": 0.0851063829787234, "ans_hit": 1, "ans_f1": 0.1568627450980392, "ans_precission": 1.0, "ans_recall": 0.0851063829787234, "path_f1": 0.11538461538461536, "path_precision": 0.6, "path_recall": 0.06382978723404255, "path_ans_f1": 0.1568627450980392, "path_ans_precision": 1.0, "path_ans_recall": 0.0851063829787234}
{"id": "WebQTest-1688", "prediction": ["# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Complications from pneumonia -> people.cause_of_death.people -> Aram Haigaz\n# Answer:\nComplications from pneumonia", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Complications from pneumonia -> common.topic.notable_for -> g.1yl5vldtz\n# Answer:\nComplications from pneumonia", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Complications from pneumonia -> people.cause_of_death.parent_cause_of_death -> Pneumonia\n# Answer:\nComplications from pneumonia", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Complications from pneumonia -> people.cause_of_death.people -> Blanca Vicu\u00f1a Ardohain\n# Answer:\nComplications from pneumonia", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> common.topic.subject_of -> Del Sorbo Antonio\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Complications from pneumonia -> people.cause_of_death.people -> Charlton Heston\n# Answer:\nComplications from pneumonia", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> common.topic.notable_for -> g.1254zhcvf\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> common.topic.subject_of -> JAMA Dermatology\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> medicine.disease_cause.diseases -> Erythema nodosum\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> common.topic.image -> 244.mac.bernie.092706.jpg -> common.image.size -> m.02hsygv\n# Answer:\n244.mac.bernie.092706.jpg"], "ground_truth": ["Complications from pneumonia", "Sarcoidosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-169", "prediction": ["# Reasoning Path:\nBelgium -> location.country.languages_spoken -> West Flemish -> language.human_language.language_family -> Germanic languages\n# Answer:\nWest Flemish", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> West Flemish -> language.human_language.language_family -> Indo-European languages\n# Answer:\nWest Flemish", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> West Flemish -> language.human_language.countries_spoken_in -> County of Flanders\n# Answer:\nWest Flemish", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> German Language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nGerman Language", "# Reasoning Path:\nBelgium -> location.statistical_region.gni_in_ppp_dollars -> g.11b61jk1pw\n# Answer:\ng.11b61jk1pw", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> West Flemish -> language.human_language.language_family -> West Germanic languages\n# Answer:\nWest Flemish", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> German Language -> language.human_language.region -> Europe\n# Answer:\nGerman Language", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nBelgium -> location.country.official_language -> German Language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nGerman Language", "# Reasoning Path:\nBelgium -> location.country.official_language -> Dutch Language -> language.human_language.main_country -> Netherlands\n# Answer:\nDutch Language"], "ground_truth": ["German Language", "Dutch Language", "French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.41379310344827586, "path_precision": 0.3, "path_recall": 0.6666666666666666, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1690", "prediction": ["# Reasoning Path:\nElectrolux -> organization.organization.headquarters -> m.03ll14c -> location.mailing_address.citytown -> Stockholm\n# Answer:\nm.03ll14c", "# Reasoning Path:\nElectrolux -> organization.organization.headquarters -> m.03ll14c -> common.topic.notable_for -> g.1257hvmfs\n# Answer:\nm.03ll14c", "# Reasoning Path:\nElectrolux -> organization.organization.headquarters -> m.03ll14c -> freebase.valuenotation.has_no_value -> Street Address 2\n# Answer:\nm.03ll14c", "# Reasoning Path:\nElectrolux -> business.business_operation.net_profit -> m.04j7kvv -> measurement_unit.dated_money_value.currency -> Swedish krona\n# Answer:\nm.04j7kvv", "# Reasoning Path:\nElectrolux -> organization.organization.headquarters -> m.03ll14c -> freebase.valuenotation.has_no_value -> Street Address 3\n# Answer:\nm.03ll14c", "# Reasoning Path:\nElectrolux -> organization.organization.headquarters -> m.03ll14c -> freebase.valuenotation.has_no_value -> Street Address 4\n# Answer:\nm.03ll14c", "# Reasoning Path:\nElectrolux -> base.schemastaging.organization_extra.contact_webpages -> m.010b0w6t -> internet.localized_uri.location -> United States of America\n# Answer:\nm.010b0w6t", "# Reasoning Path:\nElectrolux -> base.schemastaging.organization_extra.contact_webpages -> m.010b0w6t -> internet.localized_uri.language -> English\n# Answer:\nm.010b0w6t", "# Reasoning Path:\nElectrolux -> base.schemastaging.organization_extra.contact_webpages -> m.010b0w9_ -> internet.localized_uri.location -> Canada\n# Answer:\nm.010b0w9_", "# Reasoning Path:\nElectrolux -> base.schemastaging.organization_extra.contact_webpages -> m.010b0x4l -> internet.localized_uri.location -> Sweden\n# Answer:\nm.010b0x4l"], "ground_truth": ["Stockholm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1691", "prediction": ["# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.statistical_region.population -> g.11b66fk05m\n# Answer:\ng.11b66fk05m", "# Reasoning Path:\nHouston -> location.statistical_region.population -> g.11b7tcbqts\n# Answer:\ng.11b7tcbqts", "# Reasoning Path:\nHouston -> location.place_with_neighborhoods.neighborhoods -> Aldine -> location.location.time_zones -> Central Time Zone\n# Answer:\nAldine", "# Reasoning Path:\nHouston -> location.place_with_neighborhoods.neighborhoods -> Addicks -> location.location.containedby -> Greater Houston\n# Answer:\nAddicks", "# Reasoning Path:\nHouston -> location.place_with_neighborhoods.neighborhoods -> Addicks -> location.location.people_born_here -> Jesse Lee Turner\n# Answer:\nAddicks", "# Reasoning Path:\nHouston -> location.statistical_region.population -> g.11btt54h79\n# Answer:\ng.11btt54h79"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1694", "prediction": ["# Reasoning Path:\nWilliam Shakespeare -> people.person.parents -> John Shakespeare -> people.person.children -> Anne Shakespeare\n# Answer:\nJohn Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Hamnet Shakespeare -> people.person.parents -> Anne Hathaway\n# Answer:\nHamnet Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.parents -> Mary Shakespeare -> people.person.children -> Anne Shakespeare\n# Answer:\nMary Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Judith Quiney -> people.person.parents -> Anne Hathaway\n# Answer:\nJudith Quiney", "# Reasoning Path:\nWilliam Shakespeare -> people.person.parents -> John Shakespeare -> people.person.spouse_s -> m.0p8txn6\n# Answer:\nJohn Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.spouse_s -> m.02wtqtm -> people.marriage.spouse -> Anne Hathaway\n# Answer:\nm.02wtqtm", "# Reasoning Path:\nWilliam Shakespeare -> people.person.parents -> John Shakespeare -> people.person.children -> Edmund Shakespeare\n# Answer:\nJohn Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.parents -> Mary Shakespeare -> people.person.spouse_s -> m.0p8txn6\n# Answer:\nMary Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.parents -> John Shakespeare -> people.person.children -> Gilbert Shakespeare\n# Answer:\nJohn Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.parents -> John Shakespeare -> people.person.profession -> Politician\n# Answer:\nJohn Shakespeare"], "ground_truth": ["Anne Hathaway"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1695", "prediction": ["# Reasoning Path:\nHarry S. Truman -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nHarry S. Truman -> book.written_work.subjects -> 20th century -> time.event.includes_event -> World War I\n# Answer:\n20th century", "# Reasoning Path:\nHarry S. Truman -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Chicago\n# Answer:\nUnited States of America", "# Reasoning Path:\nHarry S. Truman -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nHarry S. Truman -> book.written_work.subjects -> United States of America -> location.country.administrative_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nHarry S. Truman -> people.person.nationality -> United States of America -> military.military_combatant.military_commanders -> m.04jx14r\n# Answer:\nUnited States of America", "# Reasoning Path:\nHarry S. Truman -> book.written_work.subjects -> 20th century -> time.event.includes_event -> World War II\n# Answer:\n20th century", "# Reasoning Path:\nHarry S. Truman -> book.written_work.subjects -> 20th century -> common.topic.notable_types -> Event\n# Answer:\n20th century", "# Reasoning Path:\nHarry S. Truman -> book.written_work.subjects -> Family\n# Answer:\nFamily", "# Reasoning Path:\nHarry S. Truman -> book.written_work.subjects -> United States of America -> military.military_combatant.military_commanders -> m.04jx14r\n# Answer:\nUnited States of America"], "ground_truth": ["1945-04-12"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1696", "prediction": ["# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Sailor -> common.topic.notable_types -> Profession\n# Answer:\nSailor", "# Reasoning Path:\nFerdinand Magellan -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> g.11btv6zfvr\n# Answer:\ng.11btv6zfvr", "# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Sailor -> fictional_universe.character_occupation.characters_with_this_occupation -> Alfred King\n# Answer:\nSailor", "# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Explorer -> base.descriptive_names.names.descriptive_name -> m.0106bf9p\n# Answer:\nExplorer", "# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Sailor -> base.descriptive_names.names.descriptive_name -> m.0105cyc4\n# Answer:\nSailor", "# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Explorer -> common.topic.notable_types -> Profession\n# Answer:\nExplorer", "# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Navigator -> people.profession.people_with_this_profession -> Abel Tasman\n# Answer:\nNavigator", "# Reasoning Path:\nFerdinand Magellan -> symbols.name_source.namesakes -> Magellan -> common.topic.notable_types -> Project focus\n# Answer:\nMagellan", "# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Sailor -> fictional_universe.character_occupation.characters_with_this_occupation -> Bussell\n# Answer:\nSailor", "# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Explorer -> base.descriptive_names.names.descriptive_name -> m.0106bhcr\n# Answer:\nExplorer"], "ground_truth": ["Explorer", "Navigator", "Sailor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1698", "prediction": ["# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Thailand -> location.country.capital -> Bangkok\n# Answer:\nThailand", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Thailand -> location.country.official_language -> Thai Language\n# Answer:\nThailand", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Myanmar -> location.location.containedby -> Asia\n# Answer:\nMyanmar", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Thailand -> location.country.languages_spoken -> Akha Language\n# Answer:\nThailand", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Myanmar -> location.country.official_language -> Burmese Language\n# Answer:\nMyanmar", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Thailand -> location.country.languages_spoken -> Cham language\n# Answer:\nThailand", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Myanmar -> location.location.containedby -> Eurasia\n# Answer:\nMyanmar", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Thailand -> location.country.languages_spoken -> Hmong language\n# Answer:\nThailand", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Myanmar -> location.location.containedby -> Southeast Asia\n# Answer:\nMyanmar", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Myanmar -> location.statistical_region.consumer_price_index -> g.11b60wz9qs\n# Answer:\nMyanmar"], "ground_truth": ["Myanmar", "Thailand"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1699", "prediction": ["# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0hhzbby -> film.performance.actor -> Chris Pine\n# Answer:\nm.0hhzbby", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.03ld2ym -> film.performance.actor -> Chris Pine\n# Answer:\nm.03ld2ym", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8ffv -> film.performance.actor -> William Shatner\n# Answer:\nm.02h8ffv", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0hhzbby -> film.performance.film -> Star Trek Into Darkness\n# Answer:\nm.0hhzbby", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.03ld2ym -> film.performance.film -> Star Trek\n# Answer:\nm.03ld2ym", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8ffv -> film.performance.film -> Star Trek III: The Search for Spock\n# Answer:\nm.02h8ffv", "# Reasoning Path:\nJames T. Kirk -> common.topic.notable_types -> Film character -> freebase.type_profile.kind -> Annotation\n# Answer:\nFilm character", "# Reasoning Path:\nJames T. Kirk -> common.topic.notable_types -> Film character -> type.type.expected_by -> Character\n# Answer:\nFilm character", "# Reasoning Path:\nJames T. Kirk -> common.topic.webpage -> m.09wcz0z -> common.webpage.resource -> Wolverines! Chris Hemsworth nabs lead in 'Red Dawn'\n# Answer:\nm.09wcz0z", "# Reasoning Path:\nJames T. Kirk -> common.topic.webpage -> m.09wcz0z -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09wcz0z"], "ground_truth": ["Chris Pine", "Jimmy Bennett", "William Shatner"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.24, "path_precision": 0.3, "path_recall": 0.2, "path_ans_f1": 0.41379310344827586, "path_ans_precision": 0.3, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-170", "prediction": ["# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.0gwn5hf -> film.performance.actor -> Ian Holm\n# Answer:\nm.0gwn5hf", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.0glw4lg -> film.performance.actor -> Ian Holm\n# Answer:\nm.0glw4lg", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.03l6qv_ -> film.performance.actor -> Ian Holm\n# Answer:\nm.03l6qv_", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.0gwn5hf -> film.performance.film -> The Hobbit: The Desolation of Smaug\n# Answer:\nm.0gwn5hf", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.0glw4lg -> film.performance.film -> The Hobbit: An Unexpected Journey\n# Answer:\nm.0glw4lg", "# Reasoning Path:\nOld Bilbo -> fictional_universe.fictional_character.powers_or_abilities -> Invisibility -> book.book_subject.works -> Memoirs of an Invisible Man\n# Answer:\nInvisibility", "# Reasoning Path:\nOld Bilbo -> book.book_character.appears_in_book -> The Hobbit -> book.book_edition.author_editor -> J. R. R. Tolkien\n# Answer:\nThe Hobbit", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.03l6qv_ -> film.performance.film -> The Lord of the Rings: The Return of the King\n# Answer:\nm.03l6qv_", "# Reasoning Path:\nOld Bilbo -> fictional_universe.fictional_character.powers_or_abilities -> Invisibility -> fictional_universe.character_powers.characters_with_this_ability -> Ray\n# Answer:\nInvisibility", "# Reasoning Path:\nOld Bilbo -> fictional_universe.fictional_character.powers_or_abilities -> Invisibility -> film.film_subject.films -> The Unknown Purple\n# Answer:\nInvisibility"], "ground_truth": ["Norman Bird"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1700", "prediction": ["# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.containedby -> Washington\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.containedby -> Portland-Vancouver-Beaverton, OR-WA Metropolitan Statistical Area\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.contains -> Skamania\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.containedby -> Contiguous United States\n# Answer:\nWashington", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nMount St. Helens -> geography.mountain.mountain_range -> Cascade Range -> location.location.containedby -> North America\n# Answer:\nCascade Range", "# Reasoning Path:\nMount St. Helens -> common.image.size -> m.02bkwf9\n# Answer:\nm.02bkwf9", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.containedby -> Northwestern United States\n# Answer:\nWashington", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.partially_contains -> Columbia River\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.containedby -> Pacific Northwest\n# Answer:\nWashington"], "ground_truth": ["Washington", "Cascade Range", "North America", "United States of America", "Skamania County"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.8470588235294118, "ans_precission": 0.9, "ans_recall": 0.8, "path_f1": 0.6857142857142857, "path_precision": 0.8, "path_recall": 0.6, "path_ans_f1": 0.8470588235294118, "path_ans_precision": 0.9, "path_ans_recall": 0.8}
{"id": "WebQTest-1701", "prediction": ["# Reasoning Path:\nSaint Paul -> location.location.containedby -> Ramsey County -> location.location.containedby -> Minnesota\n# Answer:\nRamsey County", "# Reasoning Path:\nSaint Paul -> location.location.containedby -> Minnesota -> location.location.containedby -> Contiguous United States\n# Answer:\nMinnesota", "# Reasoning Path:\nSaint Paul -> location.location.containedby -> Area code 651 -> location.location.containedby -> Minnesota\n# Answer:\nArea code 651", "# Reasoning Path:\nSaint Paul -> location.location.containedby -> Minnesota -> location.location.containedby -> Midwestern United States\n# Answer:\nMinnesota", "# Reasoning Path:\nSaint Paul -> location.statistical_region.population -> g.11b66dwnhr\n# Answer:\ng.11b66dwnhr", "# Reasoning Path:\nSaint Paul -> location.location.containedby -> Minnesota -> location.location.containedby -> United States of America\n# Answer:\nMinnesota", "# Reasoning Path:\nSaint Paul -> location.location.containedby -> Ramsey County -> location.location.containedby -> Minneapolis-St. Paul-Bloomington, MN-WI Metropolitan Statistical Area\n# Answer:\nRamsey County", "# Reasoning Path:\nSaint Paul -> location.location.containedby -> Minnesota -> government.political_district.representatives -> m.0598jps\n# Answer:\nMinnesota", "# Reasoning Path:\nSaint Paul -> location.citytown.postal_codes -> 55101 -> location.location.containedby -> Ramsey County\n# Answer:\n55101", "# Reasoning Path:\nSaint Paul -> location.citytown.postal_codes -> 55102 -> location.location.containedby -> Ramsey County\n# Answer:\n55102"], "ground_truth": ["Ramsey County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1702", "prediction": ["# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> common.topic.article -> m.02sx6l\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.spouse_s -> m.03mlj1z\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> common.topic.article -> m.0hn9rcf\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.place_of_birth -> Blackmore\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Edward VI of England -> people.person.religion -> Protestantism\n# Answer:\nEdward VI of England", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> people.person.nationality -> Kingdom of England\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> symbols.name_source.namesakes -> Henry FitzAlan, 19th Earl of Arundel -> people.person.children -> Henry Lord Maltravers\n# Answer:\nHenry FitzAlan, 19th Earl of Arundel", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> royalty.noble_person.titles -> m.0hqncyh\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> symbols.name_source.namesakes -> Henry FitzAlan, 19th Earl of Arundel -> people.person.children -> Jane Lumley\n# Answer:\nHenry FitzAlan, 19th Earl of Arundel", "# Reasoning Path:\nHenry VIII of England -> symbols.name_source.namesakes -> Henry FitzAlan, 19th Earl of Arundel -> people.person.profession -> Courtier\n# Answer:\nHenry FitzAlan, 19th Earl of Arundel"], "ground_truth": ["Henry, Duke of Cornwall", "Henry FitzRoy, 1st Duke of Richmond and Somerset", "Edward VI of England"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1705", "prediction": ["# Reasoning Path:\nFranz Ferdinand -> award.award_nominee.award_nominations -> m.0_l3qm4 -> award.award_nomination.award -> NME Award for Best Music Video\n# Answer:\nm.0_l3qm4", "# Reasoning Path:\nFranz Ferdinand -> award.award_nominee.award_nominations -> m.0_l3qm4 -> award.award_nomination.nominated_for -> Do You Want To\n# Answer:\nm.0_l3qm4", "# Reasoning Path:\nFranz Ferdinand -> music.featured_artist.recordings -> The Dark of the Matinee -> music.recording.tracks -> The Dark of the Matin\u00e9e\n# Answer:\nThe Dark of the Matinee", "# Reasoning Path:\nFranz Ferdinand -> award.award_nominee.award_nominations -> m.0_l54ck -> freebase.valuenotation.is_reviewed -> Award\n# Answer:\nm.0_l54ck", "# Reasoning Path:\nFranz Ferdinand -> award.award_nominee.award_nominations -> m.0_l54ck -> award.award_nomination.award -> NME Award for Best Live Band\n# Answer:\nm.0_l54ck", "# Reasoning Path:\nFranz Ferdinand -> music.artist.album -> 2004-11-15: Heineken Music Hall, Amsterdam, Netherlands -> music.album.album_content_type -> Live Album\n# Answer:\n2004-11-15: Heineken Music Hall, Amsterdam, Netherlands", "# Reasoning Path:\nFranz Ferdinand -> music.artist.album -> 2009-03-05: Glasgow Barrowlands, Scotland, UK -> music.album.album_content_type -> Live Album\n# Answer:\n2009-03-05: Glasgow Barrowlands, Scotland, UK", "# Reasoning Path:\nFranz Ferdinand -> award.award_nominee.award_nominations -> m.0_l6djw -> freebase.valuenotation.is_reviewed -> Award\n# Answer:\nm.0_l6djw", "# Reasoning Path:\nFranz Ferdinand -> award.award_nominee.award_nominations -> m.0_l6djw -> award.award_nomination.nominated_for -> Do You Want To\n# Answer:\nm.0_l6djw", "# Reasoning Path:\nFranz Ferdinand -> music.artist.album -> 2004-11-15: Heineken Music Hall, Amsterdam, Netherlands -> freebase.valuenotation.has_value -> Initial release date\n# Answer:\n2004-11-15: Heineken Music Hall, Amsterdam, Netherlands"], "ground_truth": ["Glasgow"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1706", "prediction": ["# Reasoning Path:\nBenedict Arnold -> base.honouriam.dishonoured_person.dishonor_bestowed -> Traitor\n# Answer:\nTraitor", "# Reasoning Path:\nBenedict Arnold -> common.topic.notable_types -> Film character -> freebase.type_profile.published -> Published\n# Answer:\nFilm character", "# Reasoning Path:\nBenedict Arnold -> common.topic.notable_types -> Film character -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nFilm character", "# Reasoning Path:\nBenedict Arnold -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Topic\n# Answer:\nFilm character", "# Reasoning Path:\nBenedict Arnold -> military.military_commander.military_commands -> m.049yl9_ -> military.military_command.military_conflict -> Battle of Quebec\n# Answer:\nm.049yl9_", "# Reasoning Path:\nBenedict Arnold -> common.topic.notable_types -> Military Commander -> freebase.type_profile.strict_included_types -> Person\n# Answer:\nMilitary Commander", "# Reasoning Path:\nBenedict Arnold -> common.topic.notable_types -> Military Commander -> type.type.properties -> Military Commands\n# Answer:\nMilitary Commander", "# Reasoning Path:\nBenedict Arnold -> military.military_commander.military_commands -> m.049yl9_ -> military.military_command.military_combatant -> Patriot\n# Answer:\nm.049yl9_", "# Reasoning Path:\nBenedict Arnold -> military.military_commander.military_commands -> m.04fvgxs -> military.military_command.military_conflict -> American Revolutionary War\n# Answer:\nm.04fvgxs", "# Reasoning Path:\nBenedict Arnold -> common.topic.notable_types -> Military Commander -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nMilitary Commander"], "ground_truth": ["Traitor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1707", "prediction": ["# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.03bxf_b -> tv.regular_tv_appearance.actor -> Michael Rosenbaum\n# Answer:\nm.03bxf_b", "# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.02t8f_n -> tv.regular_tv_appearance.actor -> Ian Somerhalder\n# Answer:\nm.02t8f_n", "# Reasoning Path:\nSmallville -> award.award_nominated_work.award_nominations -> m.0n4pm1k -> award.award_nomination.award -> Satellite Award for Best Supporting Actor - Drama Series\n# Answer:\nm.0n4pm1k", "# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.02t8q51 -> tv.regular_tv_appearance.actor -> John Glover\n# Answer:\nm.02t8q51", "# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.03bxf_b -> tv.regular_tv_appearance.seasons -> Smallville - Season 1\n# Answer:\nm.03bxf_b", "# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.02t8f_n -> tv.regular_tv_appearance.seasons -> Smallville - Season 3\n# Answer:\nm.02t8f_n", "# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.02t8f_n -> tv.regular_tv_appearance.character -> Adam Knight\n# Answer:\nm.02t8f_n", "# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.03bxf_b -> tv.regular_tv_appearance.seasons -> Smallville - Season 10\n# Answer:\nm.03bxf_b", "# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.03bxf_b -> tv.regular_tv_appearance.seasons -> Smallville - Season 2\n# Answer:\nm.03bxf_b", "# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.02t8q51 -> tv.regular_tv_appearance.character -> Lionel Luthor\n# Answer:\nm.02t8q51"], "ground_truth": ["Michael Rosenbaum"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.11764705882352941, "path_precision": 0.1, "path_recall": 0.14285714285714285, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1708", "prediction": ["# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Blue -> book.book_subject.works -> Blue: The History of a Color\n# Answer:\nBlue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Blue -> common.topic.notable_for -> g.125gl7_n4\n# Answer:\nBlue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Blue -> base.schemastaging.visual_color_extra.hsv -> m.010nmgqr\n# Answer:\nBlue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Navy Blue -> base.schemastaging.visual_color_extra.srgb -> m.010q_4pn\n# Answer:\nNavy Blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Royal blue -> common.topic.notable_types -> Color\n# Answer:\nRoyal blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Navy Blue -> base.schemastaging.visual_color_extra.cmyk -> m.010q_4qh\n# Answer:\nNavy Blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Navy Blue -> common.topic.notable_types -> Color\n# Answer:\nNavy Blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Royal blue -> common.topic.notable_for -> g.1255wtr8_\n# Answer:\nRoyal blue", "# Reasoning Path:\nDallas Cowboys -> american_football.football_team.away_games -> Dallas Cowboys at Arizona Cardinals, 2008-10-12 -> american_football.football_game.passing -> m.079397s\n# Answer:\nDallas Cowboys at Arizona Cardinals, 2008-10-12", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Royal blue -> common.topic.article -> m.06kqt8\n# Answer:\nRoyal blue"], "ground_truth": ["Navy Blue", "Blue", "White", "Royal blue", "Silver"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7200000000000001, "ans_precission": 0.9, "ans_recall": 0.6, "path_f1": 0.7200000000000001, "path_precision": 0.9, "path_recall": 0.6, "path_ans_f1": 0.7200000000000001, "path_ans_precision": 0.9, "path_ans_recall": 0.6}
{"id": "WebQTest-171", "prediction": ["# Reasoning Path:\nDestin -> location.location.nearby_airports -> Destin\u2013Fort Walton Beach Airport -> common.topic.article -> m.01q6d5\n# Answer:\nDestin\u2013Fort Walton Beach Airport", "# Reasoning Path:\nDestin -> location.location.nearby_airports -> Destin\u2013Fort Walton Beach Airport -> location.location.containedby -> Eglin Air Force Base\n# Answer:\nDestin\u2013Fort Walton Beach Airport", "# Reasoning Path:\nDestin -> location.location.nearby_airports -> Destin\u2013Fort Walton Beach Airport -> common.topic.image -> Eglinafb-15feb1999\n# Answer:\nDestin\u2013Fort Walton Beach Airport", "# Reasoning Path:\nDestin -> location.location.nearby_airports -> Destin Executive Airport -> common.topic.article -> m.02qzm82\n# Answer:\nDestin Executive Airport", "# Reasoning Path:\nDestin -> location.location.nearby_airports -> Destin Executive Airport -> location.location.containedby -> 32541\n# Answer:\nDestin Executive Airport", "# Reasoning Path:\nDestin -> travel.travel_destination.tourist_attractions -> Big Kahuna's -> common.topic.article -> m.0dz54q\n# Answer:\nBig Kahuna's", "# Reasoning Path:\nDestin -> location.location.nearby_airports -> Destin\u2013Fort Walton Beach Airport -> common.topic.image -> Gates at the Okaloosa Regional Airport.\n# Answer:\nDestin\u2013Fort Walton Beach Airport", "# Reasoning Path:\nDestin -> location.location.contains -> Destin Community Center -> common.topic.notable_for -> g.1256_p6_r\n# Answer:\nDestin Community Center", "# Reasoning Path:\nDestin -> location.location.nearby_airports -> Destin Executive Airport -> location.location.containedby -> Okaloosa County\n# Answer:\nDestin Executive Airport", "# Reasoning Path:\nDestin -> travel.travel_destination.tourist_attractions -> Big Kahuna's -> location.location.geolocation -> m.0wmx861\n# Answer:\nBig Kahuna's"], "ground_truth": ["Destin Executive Airport", "Destin\u2013Fort Walton Beach Airport"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1710", "prediction": ["# Reasoning Path:\nCzech Republic -> location.country.currency_used -> Czech koruna -> common.topic.notable_types -> Currency\n# Answer:\nCzech koruna", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> language.human_language.countries_spoken_in -> Czechoslovakia\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> language.human_language.countries_spoken_in -> Croatia\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> media_common.netflix_genre.titles -> Lunacy\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> media_common.netflix_genre.titles -> Adelheid\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> Lusen -> location.location.partially_contained_by -> m.0wg8sf9\n# Answer:\nLusen", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> B\u00f3br -> common.topic.notable_types -> River\n# Answer:\nB\u00f3br", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> Beskids -> location.location.partially_containedby -> Poland\n# Answer:\nBeskids", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> Beskids -> location.location.partially_contained_by -> m.0wg8sdh\n# Answer:\nBeskids", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> media_common.netflix_genre.titles -> All My Loved Ones\n# Answer:\nCzech Language"], "ground_truth": ["Czech koruna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1711", "prediction": ["# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> time.event.instance_of_recurring_event -> Stanley Cup Finals\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2012 Stanley Cup Finals -> common.topic.article -> m.0jt52q2\n# Answer:\n2012 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2012 Stanley Cup Finals -> sports.sports_championship_event.runner_up -> New Jersey Devils\n# Answer:\n2012 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.roster -> g.11b76d7nzz\n# Answer:\ng.11b76d7nzz", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup Finals\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2012 Stanley Cup Finals -> time.event.locations -> Prudential Center\n# Answer:\n2012 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> award.award_nominee.award_nominations -> m.010_rx6h -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nm.010_rx6h", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2012 Stanley Cup Finals -> time.event.locations -> Staples Center\n# Answer:\n2012 Stanley Cup Finals"], "ground_truth": ["2014 Stanley Cup Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1712", "prediction": ["# Reasoning Path:\nSpencer Pratt -> people.person.education -> m.0n1cn_d -> education.education.institution -> University of Southern California\n# Answer:\nm.0n1cn_d", "# Reasoning Path:\nSpencer Pratt -> people.person.place_of_birth -> Los Angeles -> periodicals.newspaper_circulation_area.newspapers -> The Tidings\n# Answer:\nLos Angeles", "# Reasoning Path:\nSpencer Pratt -> people.person.place_of_birth -> Los Angeles -> location.location.containedby -> California\n# Answer:\nLos Angeles", "# Reasoning Path:\nSpencer Pratt -> people.person.place_of_birth -> Los Angeles -> business.business_location.parent_company -> Best Quality Termite and Pest Control\n# Answer:\nLos Angeles", "# Reasoning Path:\nSpencer Pratt -> base.popstra.celebrity.dated -> m.065pxjn -> base.popstra.dated.participant -> Kelly Durbin\n# Answer:\nm.065pxjn", "# Reasoning Path:\nSpencer Pratt -> people.person.place_of_birth -> Los Angeles -> business.business_location.parent_company -> Pest Control of Los Angeles\n# Answer:\nLos Angeles", "# Reasoning Path:\nSpencer Pratt -> people.person.place_of_birth -> Los Angeles -> business.business_location.parent_company -> Superior Termite Control\n# Answer:\nLos Angeles", "# Reasoning Path:\nSpencer Pratt -> people.person.place_of_birth -> Los Angeles -> location.location.containedby -> Los Angeles County\n# Answer:\nLos Angeles", "# Reasoning Path:\nSpencer Pratt -> base.popstra.celebrity.dated -> m.065pyr3 -> base.popstra.dated.participant -> Heidi Montag\n# Answer:\nm.065pyr3", "# Reasoning Path:\nSpencer Pratt -> people.person.place_of_birth -> Los Angeles -> location.location.containedby -> United States of America\n# Answer:\nLos Angeles"], "ground_truth": ["University of Southern California"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1713", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Smallpox -> medicine.disease.notable_people_with_this_condition -> Andrew Jackson\n# Answer:\nSmallpox", "# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Strabismus -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nStrabismus", "# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Marfan syndrome -> symbols.namesake.named_after -> Antoine Marfan\n# Answer:\nMarfan syndrome", "# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Smallpox -> medicine.disease.notable_people_with_this_condition -> George Washington\n# Answer:\nSmallpox", "# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Smallpox -> medicine.icd_9_cm_classification.includes_classifications -> Alastrim\n# Answer:\nSmallpox", "# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Smallpox -> medicine.disease.includes_diseases -> Alastrim\n# Answer:\nSmallpox", "# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Smallpox -> medicine.icd_9_cm_classification.includes_classifications -> Modified smallpox\n# Answer:\nSmallpox", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pgr_5 -> people.place_lived.location -> Kentucky\n# Answer:\nm.03pgr_5", "# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Smallpox -> medicine.icd_9_cm_classification.includes_classifications -> Variola major\n# Answer:\nSmallpox", "# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Smallpox -> medicine.disease.includes_diseases -> Modified smallpox\n# Answer:\nSmallpox"], "ground_truth": ["Smallpox", "Marfan syndrome", "Strabismus"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1714", "prediction": ["# Reasoning Path:\nVancouver Canucks -> sports.sports_team.coaches -> m.0113n9pr -> sports.sports_team_coach_tenure.coach -> Doug Lidster\n# Answer:\nm.0113n9pr", "# Reasoning Path:\nVancouver Canucks -> sports.sports_team.coaches -> m.0w1yrsm -> sports.sports_team_coach_tenure.coach -> John Tortorella\n# Answer:\nm.0w1yrsm", "# Reasoning Path:\nVancouver Canucks -> sports.sports_team.coaches -> m.0vbljnd -> sports.sports_team_coach_tenure.coach -> Alain Vigneault\n# Answer:\nm.0vbljnd", "# Reasoning Path:\nVancouver Canucks -> sports.sports_team.coaches -> m.0113n9pr -> sports.sports_team_coach_tenure.position -> Assistant Coach\n# Answer:\nm.0113n9pr", "# Reasoning Path:\nVancouver Canucks -> ice_hockey.hockey_team.coach -> Willie Desjardins -> people.person.place_of_birth -> Climax\n# Answer:\nWillie Desjardins", "# Reasoning Path:\nVancouver Canucks -> sports.sports_team.coaches -> m.0w1yrsm -> sports.sports_team_coach_tenure.position -> Head coach\n# Answer:\nm.0w1yrsm", "# Reasoning Path:\nVancouver Canucks -> sports.sports_team.coaches -> m.0vbljnd -> sports.sports_team_coach_tenure.position -> Head coach\n# Answer:\nm.0vbljnd", "# Reasoning Path:\nVancouver Canucks -> ice_hockey.hockey_team.coach -> Willie Desjardins -> common.topic.notable_for -> g.1259nc0p2\n# Answer:\nWillie Desjardins", "# Reasoning Path:\nVancouver Canucks -> ice_hockey.hockey_team.coach -> Willie Desjardins -> people.person.gender -> Male\n# Answer:\nWillie Desjardins", "# Reasoning Path:\nVancouver Canucks -> sports.sports_team.sport -> Ice Hockey -> media_common.netflix_genre.titles -> 1998 Olympic Hockey Highlights\n# Answer:\nIce Hockey"], "ground_truth": ["Alain Vigneault", "Doug Lidster", "John Tortorella", "Willie Desjardins"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2727272727272727, "ans_precission": 0.3, "ans_recall": 0.25, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1715", "prediction": ["# Reasoning Path:\nC. S. Lewis -> people.person.place_of_birth -> Belfast -> travel.travel_destination.tourist_attractions -> Belfast Castle\n# Answer:\nBelfast", "# Reasoning Path:\nC. S. Lewis -> influence.influence_node.influenced -> Barton Paul Levenson -> people.person.place_of_birth -> Chapel Hill\n# Answer:\nBarton Paul Levenson", "# Reasoning Path:\nC. S. Lewis -> people.person.place_of_birth -> Belfast -> location.location.nearby_airports -> George Best Belfast City Airport\n# Answer:\nBelfast", "# Reasoning Path:\nC. S. Lewis -> people.person.places_lived -> m.04hm5pn -> people.place_lived.location -> Belfast\n# Answer:\nm.04hm5pn", "# Reasoning Path:\nC. S. Lewis -> people.person.place_of_birth -> Belfast -> travel.travel_destination.tourist_attractions -> Belfast City Hall\n# Answer:\nBelfast", "# Reasoning Path:\nC. S. Lewis -> people.person.place_of_birth -> Belfast -> location.location.contains -> Belfast Castle\n# Answer:\nBelfast", "# Reasoning Path:\nC. S. Lewis -> influence.influence_node.influenced -> Carol Plum-Ucci -> influence.influence_node.influenced_by -> J. D. Salinger\n# Answer:\nCarol Plum-Ucci", "# Reasoning Path:\nC. S. Lewis -> people.person.place_of_birth -> Belfast -> travel.travel_destination.tourist_attractions -> Belfast Zoo\n# Answer:\nBelfast", "# Reasoning Path:\nC. S. Lewis -> influence.influence_node.influenced -> Christopher Derrick -> people.person.place_of_birth -> Hungerford\n# Answer:\nChristopher Derrick", "# Reasoning Path:\nC. S. Lewis -> people.person.place_of_birth -> Belfast -> location.location.contains -> Belfast City Hall\n# Answer:\nBelfast"], "ground_truth": ["Belfast"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1716", "prediction": ["# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.nationality -> Dominican Republic -> location.country.administrative_divisions -> Azua Province\n# Answer:\nDominican Republic", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.nationality -> Dominican Republic -> location.country.administrative_divisions -> Baoruco Province\n# Answer:\nDominican Republic", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.nationality -> Dominican Republic -> base.aareas.schema.administrative_area.administrative_children -> Azua Province\n# Answer:\nDominican Republic", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.place_of_birth -> Santo Domingo -> location.location.containedby -> Dominican Republic\n# Answer:\nSanto Domingo", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.nationality -> Dominican Republic -> location.country.administrative_divisions -> Barahona Province\n# Answer:\nDominican Republic", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.nationality -> Dominican Republic -> location.country.first_level_divisions -> Azua Province\n# Answer:\nDominican Republic", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.nationality -> Dominican Republic -> base.aareas.schema.administrative_area.administrative_children -> Baoruco Province\n# Answer:\nDominican Republic", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.place_of_birth -> Santo Domingo -> location.location.containedby -> Hispaniola\n# Answer:\nSanto Domingo", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.nationality -> Dominican Republic -> base.aareas.schema.administrative_area.administrative_children -> Barahona Province\n# Answer:\nDominican Republic", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.nationality -> Dominican Republic -> location.country.first_level_divisions -> Baoruco Province\n# Answer:\nDominican Republic"], "ground_truth": ["Dominican Republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1719", "prediction": ["# Reasoning Path:\nJane Wyman -> people.person.spouse_s -> m.0hmv78j -> people.marriage.spouse -> Fred Karger\n# Answer:\nm.0hmv78j", "# Reasoning Path:\nJane Wyman -> people.person.spouse_s -> m.0hmv78j -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0hmv78j", "# Reasoning Path:\nJane Wyman -> people.person.spouse_s -> m.0hmv78j -> people.marriage.location_of_ceremony -> Santa Barbara\n# Answer:\nm.0hmv78j", "# Reasoning Path:\nJane Wyman -> people.person.spouse_s -> m.0hmv798 -> people.marriage.spouse -> Fred Karger\n# Answer:\nm.0hmv798", "# Reasoning Path:\nJane Wyman -> people.person.children -> Michael Reagan -> people.person.parents -> Ronald Reagan\n# Answer:\nMichael Reagan", "# Reasoning Path:\nJane Wyman -> people.person.children -> Christine Reagan -> people.person.parents -> Ronald Reagan\n# Answer:\nChristine Reagan", "# Reasoning Path:\nJane Wyman -> film.actor.film -> g.11b6s3lhg1\n# Answer:\ng.11b6s3lhg1", "# Reasoning Path:\nJane Wyman -> people.person.spouse_s -> m.0hmv78v -> people.marriage.spouse -> Myron Futterman\n# Answer:\nm.0hmv78v", "# Reasoning Path:\nJane Wyman -> people.person.children -> Maureen Reagan -> people.person.children -> Rita Mirembe Revell\n# Answer:\nMaureen Reagan", "# Reasoning Path:\nJane Wyman -> people.person.spouse_s -> m.0hmv78v -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0hmv78v"], "ground_truth": ["Fred Karger"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-172", "prediction": ["# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> United States Senator -> government.government_office_or_title.governmental_body_if_any -> United States Congress\n# Answer:\nUnited States Senator", "# Reasoning Path:\nUnited States Senate -> common.topic.image -> 600px-Senate_Seal.svg.png -> common.image.appears_in_topic_gallery -> United States Senator\n# Answer:\n600px-Senate_Seal.svg.png", "# Reasoning Path:\nUnited States Senate -> common.topic.image -> 600px-Senate_Seal.svg.png -> common.image.size -> m.01x3k95\n# Answer:\n600px-Senate_Seal.svg.png", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> United States Senator -> base.descriptive_names.names.descriptive_name -> m.0100zlls\n# Answer:\nUnited States Senator", "# Reasoning Path:\nUnited States Senate -> common.topic.webpage -> m.03ldgbc -> common.webpage.category -> Official Website\n# Answer:\nm.03ldgbc", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> United States Senator -> fictional_universe.character_occupation.characters_with_this_occupation -> John \\\"Bluto\\\" Blutarsky\n# Answer:\nUnited States Senator", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> Assistant Minority Leader of the United States Senate -> common.topic.notable_for -> g.125f9cfv0\n# Answer:\nAssistant Minority Leader of the United States Senate", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> United States Senator -> base.descriptive_names.names.descriptive_name -> m.01011v2m\n# Answer:\nUnited States Senator", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> Assistant Majority Leader of the United States Senate -> common.topic.notable_types -> Government Office or Title\n# Answer:\nAssistant Majority Leader of the United States Senate", "# Reasoning Path:\nUnited States Senate -> common.topic.webpage -> m.0422l7l -> common.webpage.category -> Topic Webpage\n# Answer:\nm.0422l7l"], "ground_truth": ["Senate majority leader", "Chief of staff", "Senator", "State Senator", "President pro tempore", "President of the Senate"], "ans_acc": 0.16666666666666666, "ans_hit": 1, "ans_f1": 0.23529411764705882, "ans_precission": 0.4, "ans_recall": 0.16666666666666666, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0.25, "path_ans_precision": 0.5, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTest-1721", "prediction": ["# Reasoning Path:\nPeru -> location.country.languages_spoken -> Waorani Language -> language.human_language.language_family -> Language isolate\n# Answer:\nWaorani Language", "# Reasoning Path:\nPeru -> location.country.languages_spoken -> Waorani Language -> language.human_language.countries_spoken_in -> Ecuador\n# Answer:\nWaorani Language", "# Reasoning Path:\nPeru -> location.country.languages_spoken -> Waorani Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nWaorani Language", "# Reasoning Path:\nPeru -> location.country.official_language -> Quechuan languages -> language.human_language.countries_spoken_in -> Argentina\n# Answer:\nQuechuan languages", "# Reasoning Path:\nPeru -> location.country.official_language -> Aymara language -> language.human_language.main_country -> Bolivia\n# Answer:\nAymara language", "# Reasoning Path:\nPeru -> location.country.languages_spoken -> Omagua dialect -> common.topic.article -> m.01ypnf\n# Answer:\nOmagua dialect", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.11b71n53cl\n# Answer:\ng.11b71n53cl", "# Reasoning Path:\nPeru -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Argentina\n# Answer:\nSpanish Language", "# Reasoning Path:\nPeru -> location.country.languages_spoken -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nPeru -> location.country.official_language -> Aymara language -> language.human_language.language_family -> Aymaran languages\n# Answer:\nAymara language"], "ground_truth": ["Aymara language", "Puquina Language", "Ayacucho Quechua", "Quechuan languages", "Waorani Language", "Omagua dialect", "Spanish Language"], "ans_acc": 0.7142857142857143, "ans_hit": 1, "ans_f1": 0.7964601769911505, "ans_precission": 0.9, "ans_recall": 0.7142857142857143, "path_f1": 0.6292134831460674, "path_precision": 0.7, "path_recall": 0.5714285714285714, "path_ans_f1": 0.7964601769911505, "path_ans_precision": 0.9, "path_ans_recall": 0.7142857142857143}
{"id": "WebQTest-1723", "prediction": ["# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Hong Kong -> location.location.containedby -> Asia\n# Answer:\nHong Kong", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Lesotho -> location.location.containedby -> Africa\n# Answer:\nLesotho", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.location.containedby -> Americas\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> education.school_category.schools_of_this_kind -> Piers Midwinter -> people.person.quotations -> Learn with a NEST. Please be my GUEST. I will give you my BEST,\n# Answer:\nPiers Midwinter", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.locations.countries.continent -> North America\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Lesotho -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\nLesotho", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Hong Kong -> location.location.containedby -> China\n# Answer:\nHong Kong", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.location.containedby -> North America\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> education.school_category.schools_of_this_kind -> Piers Midwinter -> common.topic.notable_types -> Organization founder\n# Answer:\nPiers Midwinter"], "ground_truth": ["Japan", "Qatar", "Kiribati", "Rwanda", "Samoa", "Zimbabwe", "Australia", "Jordan", "Bangladesh", "Turks and Caicos Islands", "State of Palestine", "Saint Vincent and the Grenadines", "United Kingdom", "Sudan", "Philippines", "Kingdom of Great Britain", "Malta", "Mandatory Palestine", "India", "Lesotho", "Wales", "South Africa", "Bermuda", "Kenya", "Ghana", "England", "Territory of New Guinea", "Malaysia", "South Yemen", "Barbados", "Cura\u00e7ao", "Bahamas", "Transkei", "Singapore", "Saint Kitts and Nevis", "Liberia", "Israel", "Puerto Rico", "Antigua and Barbuda", "Grenada", "Republic of Ireland", "Bonaire", "Swaziland", "Sri Lanka", "Timor-Leste", "Cook Islands", "Vanuatu", "Nauru", "Cyprus", "Marshall Islands", "China", "Montserrat", "Territory of Papua and New Guinea", "Uganda", "Gazankulu", "Indonesia", "Guam", "Dominica", "Pakistan", "Ethiopia", "Saint Lucia", "Tuvalu", "New Zealand", "Tokelau", "Honduras", "Papua New Guinea", "Gambia", "Canada", "Cayman Islands", "United States of America", "Tanzania", "Zambia", "Botswana", "Cameroon", "Hong Kong", "Nigeria", "Isle of Man", "Belize", "Gibraltar", "Namibia", "Laos", "Guyana", "Jersey", "Fiji", "Brunei", "Vatican City", "Sierra Leone"], "ans_acc": 0.04597701149425287, "ans_hit": 1, "ans_f1": 0.06611570247933884, "ans_precission": 0.8, "ans_recall": 0.034482758620689655, "path_f1": 0.06611570247933884, "path_precision": 0.8, "path_recall": 0.034482758620689655, "path_ans_f1": 0.08695652173913043, "path_ans_precision": 0.8, "path_ans_recall": 0.04597701149425287}
{"id": "WebQTest-1724", "prediction": ["# Reasoning Path:\nArsenal F.C. -> sports.sports_team.championships -> 2014 FA Cup Final -> time.event.locations -> Wembley Stadium\n# Answer:\n2014 FA Cup Final", "# Reasoning Path:\nArsenal F.C. -> sports.sports_team.championships -> 2014 FA Cup Final -> sports.sports_championship_event.championship -> FA Cup Final\n# Answer:\n2014 FA Cup Final", "# Reasoning Path:\nArsenal F.C. -> sports.sports_team.championships -> 2014 FA Cup Final -> sports.sports_championship_event.runner_up -> Hull City A.F.C.\n# Answer:\n2014 FA Cup Final", "# Reasoning Path:\nArsenal F.C. -> soccer.football_team.matches -> 1971 FA Cup Final -> time.event.locations -> Wembley Stadium\n# Answer:\n1971 FA Cup Final", "# Reasoning Path:\nArsenal F.C. -> sports.sports_team.championships -> 1929\u201330 FA Cup -> time.event.next_in_series -> 1930\u201331 FA Cup\n# Answer:\n1929\u201330 FA Cup", "# Reasoning Path:\nArsenal F.C. -> soccer.football_team.player_statistics -> m.0w8_167 -> soccer.football_player_stats.player -> Nacer Barazite\n# Answer:\nm.0w8_167", "# Reasoning Path:\nArsenal F.C. -> sports.sports_team.championships -> 1929\u201330 FA Cup -> time.event.previous_in_series -> 1928\u201329 FA Cup\n# Answer:\n1929\u201330 FA Cup", "# Reasoning Path:\nArsenal F.C. -> sports.sports_team.championships -> 1935\u201336 FA Cup -> common.topic.notable_for -> g.1259xm0g8\n# Answer:\n1935\u201336 FA Cup", "# Reasoning Path:\nArsenal F.C. -> soccer.football_team.matches -> 2014 FA Cup Final -> time.event.locations -> Wembley Stadium\n# Answer:\n2014 FA Cup Final", "# Reasoning Path:\nArsenal F.C. -> sports.sports_team.championships -> 1935\u201336 FA Cup -> time.event.previous_in_series -> 1934\u201335 FA Cup\n# Answer:\n1935\u201336 FA Cup"], "ground_truth": ["1978\u201379 FA Cup", "1992\u201393 FA Cup", "2004\u201305 FA Cup", "1997\u201398 FA Cup", "1970\u201371 FA Cup", "2013\u201314 FA Cup", "2002\u201303 FA Cup", "2001\u201302 FA Cup"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1725", "prediction": ["# Reasoning Path:\nPhoenix -> location.location.time_zones -> Mountain Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nPhoenix -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nPhoenix -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nPhoenix -> location.citytown.postal_codes -> 85001 -> location.postal_code.country -> United States of America\n# Answer:\n85001", "# Reasoning Path:\nPhoenix -> education.school_mascot.school -> Holy Ghost Preparatory School\n# Answer:\nHoly Ghost Preparatory School", "# Reasoning Path:\nPhoenix -> location.citytown.postal_codes -> 85001 -> common.topic.notable_types -> Postal Code\n# Answer:\n85001", "# Reasoning Path:\nPhoenix -> location.citytown.postal_codes -> 85002 -> location.postal_code.country -> United States of America\n# Answer:\n85002", "# Reasoning Path:\nPhoenix -> location.citytown.postal_codes -> 85001 -> common.topic.notable_for -> g.1256kyvfk\n# Answer:\n85001", "# Reasoning Path:\nPhoenix -> location.citytown.postal_codes -> 85003 -> location.location.geolocation -> m.03dmzdn\n# Answer:\n85003", "# Reasoning Path:\nPhoenix -> education.school_mascot.school -> Aquinas University -> education.educational_institution.colors -> Black\n# Answer:\nAquinas University"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1726", "prediction": ["# Reasoning Path:\nSouth Portland -> location.location.containedby -> Maine -> location.location.containedby -> New England\n# Answer:\nMaine", "# Reasoning Path:\nSouth Portland -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nSouth Portland -> location.location.containedby -> Cumberland County -> location.location.containedby -> Maine\n# Answer:\nCumberland County", "# Reasoning Path:\nSouth Portland -> location.hud_county_place.county -> Cumberland County -> location.location.containedby -> Maine\n# Answer:\nCumberland County", "# Reasoning Path:\nSouth Portland -> location.location.containedby -> Maine -> location.location.containedby -> United States of America\n# Answer:\nMaine", "# Reasoning Path:\nSouth Portland -> location.location.containedby -> Maine -> location.location.containedby -> United States, with Territories\n# Answer:\nMaine", "# Reasoning Path:\nSouth Portland -> location.location.containedby -> Maine -> base.aareas.schema.administrative_area.capital -> Augusta\n# Answer:\nMaine", "# Reasoning Path:\nSouth Portland -> location.location.containedby -> Maine -> location.location.partially_contains -> Aroostook River\n# Answer:\nMaine", "# Reasoning Path:\nSouth Portland -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Maine\n# Answer:\nUnited States of America", "# Reasoning Path:\nSouth Portland -> location.statistical_region.population -> g.11b66h2c5f\n# Answer:\ng.11b66h2c5f"], "ground_truth": ["Maine", "Cumberland County", "United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1727", "prediction": ["# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Luxembourg -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics\n# Answer:\nLuxembourg", "# Reasoning Path:\nFrench -> language.human_language.main_country -> France -> location.country.first_level_divisions -> Martinique\n# Answer:\nFrance", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Luxembourg -> location.location.time_zones -> Central European Time Zone\n# Answer:\nLuxembourg", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Luxembourg -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics\n# Answer:\nLuxembourg", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Luxembourg -> location.country.official_language -> German Language\n# Answer:\nLuxembourg", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Monaco -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\nMonaco", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Monaco -> location.country.languages_spoken -> Occitan language\n# Answer:\nMonaco", "# Reasoning Path:\nFrench -> language.human_language.main_country -> France -> organization.organization_founder.organizations_founded -> World Trade Organization\n# Answer:\nFrance", "# Reasoning Path:\nFrench -> language.human_language.main_country -> France -> base.locations.countries.planet -> Earth\n# Answer:\nFrance", "# Reasoning Path:\nFrench -> language.human_language.countries_spoken_in -> Vatican City -> location.statistical_region.population -> g.11b7tfzdvm\n# Answer:\nVatican City"], "ground_truth": ["Ivory Coast", "Monaco", "Seychelles", "Belgium", "Rwanda", "Central African Republic", "Vanuatu", "Burkina Faso", "Haiti", "France", "Martinique", "Cameroon", "Chad", "Guinea", "Equatorial Guinea", "Djibouti", "Madagascar", "Mali", "Switzerland", "Senegal", "Niger", "Jersey", "Comoros", "Congo", "Gabon", "Burundi", "Luxembourg", "Benin"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0.19148936170212766, "ans_precission": 0.9, "ans_recall": 0.10714285714285714, "path_f1": 0.1276595744680851, "path_precision": 0.6, "path_recall": 0.07142857142857142, "path_ans_f1": 0.24657534246575338, "path_ans_precision": 0.9, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-1728", "prediction": ["# Reasoning Path:\nPtolemy -> people.person.profession -> Mathematician -> base.descriptive_names.names.descriptive_name -> m.01066gkh\n# Answer:\nMathematician", "# Reasoning Path:\nPtolemy -> people.person.profession -> Astrologer -> common.topic.notable_types -> Profession\n# Answer:\nAstrologer", "# Reasoning Path:\nPtolemy -> people.person.profession -> Mathematician -> base.descriptive_names.names.descriptive_name -> m.010f3g_r\n# Answer:\nMathematician", "# Reasoning Path:\nPtolemy -> people.person.profession -> Mathematician -> fictional_universe.character_occupation.characters_with_this_occupation -> Beremiz Samir\n# Answer:\nMathematician", "# Reasoning Path:\nPtolemy -> people.person.profession -> Mathematician -> base.descriptive_names.names.descriptive_name -> m.010f3gnj\n# Answer:\nMathematician", "# Reasoning Path:\nPtolemy -> people.person.profession -> Mathematician -> common.topic.image -> GodfreyKneller-IsaacNewton-1689\n# Answer:\nMathematician", "# Reasoning Path:\nPtolemy -> people.person.profession -> Astrologer -> people.profession.specialization_of -> Prophet\n# Answer:\nAstrologer", "# Reasoning Path:\nPtolemy -> people.person.profession -> Philosopher -> base.descriptive_names.names.descriptive_name -> m.0105dm0j\n# Answer:\nPhilosopher", "# Reasoning Path:\nPtolemy -> people.person.profession -> Philosopher -> common.topic.notable_for -> g.125h79w9z\n# Answer:\nPhilosopher", "# Reasoning Path:\nPtolemy -> people.person.profession -> Mathematician -> fictional_universe.character_occupation.characters_with_this_occupation -> Dyhianna Selei\n# Answer:\nMathematician"], "ground_truth": ["Philosopher", "Astronomer", "Astrologer", "Mathematician", "Geographer"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 1.0, "ans_recall": 0.6, "path_f1": 0.7499999999999999, "path_precision": 1.0, "path_recall": 0.6, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 1.0, "path_ans_recall": 0.6}
{"id": "WebQTest-1729", "prediction": ["# Reasoning Path:\nHenry VIII of England -> people.person.spouse_s -> m.0j4kx1_ -> people.marriage.spouse -> Anne Boleyn\n# Answer:\nm.0j4kx1_", "# Reasoning Path:\nHenry VIII of England -> people.person.spouse_s -> m.0j4kx1_ -> people.marriage.location_of_ceremony -> Westminster Abbey\n# Answer:\nm.0j4kx1_", "# Reasoning Path:\nHenry VIII of England -> people.person.spouse_s -> m.03l46gn -> people.marriage.spouse -> Catherine Parr\n# Answer:\nm.03l46gn", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Elizabeth I of England -> people.person.parents -> Anne Boleyn\n# Answer:\nElizabeth I of England", "# Reasoning Path:\nHenry VIII of England -> people.person.spouse_s -> m.0j4kx1_ -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0j4kx1_", "# Reasoning Path:\nHenry VIII of England -> people.person.spouse_s -> m.0j4kx27 -> people.marriage.spouse -> Jane Seymour\n# Answer:\nm.0j4kx27", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> people.person.parents -> Catherine of Aragon\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> symbols.name_source.namesakes -> Henry FitzAlan, 19th Earl of Arundel -> people.person.spouse_s -> m.07vyqz1\n# Answer:\nHenry FitzAlan, 19th Earl of Arundel", "# Reasoning Path:\nHenry VIII of England -> people.person.spouse_s -> m.03l46gn -> people.marriage.location_of_ceremony -> Hampton Court Palace\n# Answer:\nm.03l46gn", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Elizabeth I of England -> common.topic.notable_types -> Monarch\n# Answer:\nElizabeth I of England"], "ground_truth": ["Catherine of Aragon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.13333333333333333, "path_precision": 0.1, "path_recall": 0.2, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-173", "prediction": ["# Reasoning Path:\nBaltic states -> location.location.contains -> Lithuania -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc388jk\n# Answer:\nLithuania", "# Reasoning Path:\nBaltic states -> location.location.contains -> Estonia -> location.location.containedby -> Eurasia\n# Answer:\nEstonia", "# Reasoning Path:\nBaltic states -> location.location.contains -> Latvia -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nLatvia", "# Reasoning Path:\nBaltic states -> location.location.contains -> Lithuania -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc3bf3d\n# Answer:\nLithuania", "# Reasoning Path:\nBaltic states -> location.location.contains -> Lithuania -> location.location.partiallycontains -> m.0wg8w49\n# Answer:\nLithuania", "# Reasoning Path:\nBaltic states -> location.location.contains -> Lithuania -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc3cwc1\n# Answer:\nLithuania", "# Reasoning Path:\nBaltic states -> location.location.contains -> Lithuania -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d4pylk\n# Answer:\nLithuania", "# Reasoning Path:\nBaltic states -> location.location.contains -> Latvia -> common.topic.notable_types -> Country\n# Answer:\nLatvia", "# Reasoning Path:\nBaltic states -> location.location.contains -> Lithuania -> location.location.partiallycontains -> m.0wg8w4g\n# Answer:\nLithuania", "# Reasoning Path:\nBaltic states -> location.location.contains -> Estonia -> location.location.containedby -> Europe\n# Answer:\nEstonia"], "ground_truth": ["Estonia", "Lithuania", "Latvia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1730", "prediction": ["# Reasoning Path:\nParan\u00e1 River -> location.location.containedby -> South America -> location.location.containedby -> Americas\n# Answer:\nSouth America", "# Reasoning Path:\nParan\u00e1 River -> location.location.containedby -> South America -> location.location.containedby -> DVD Region 4\n# Answer:\nSouth America", "# Reasoning Path:\nParan\u00e1 River -> location.location.containedby -> South America -> location.location.containedby -> Western Hemisphere\n# Answer:\nSouth America", "# Reasoning Path:\nParan\u00e1 River -> location.location.containedby -> South America -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Maricoxi\n# Answer:\nSouth America", "# Reasoning Path:\nParan\u00e1 River -> geography.river.basin_countries -> Argentina -> location.location.containedby -> South America\n# Answer:\nArgentina", "# Reasoning Path:\nParan\u00e1 River -> location.location.containedby -> South America -> book.book_subject.works -> A Brazilian Alphabet for the Younger Reader\n# Answer:\nSouth America", "# Reasoning Path:\nParan\u00e1 River -> geography.river.basin_countries -> Brazil -> location.location.containedby -> South America\n# Answer:\nBrazil", "# Reasoning Path:\nParan\u00e1 River -> location.location.containedby -> South America -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Mono Grande\n# Answer:\nSouth America", "# Reasoning Path:\nParan\u00e1 River -> geography.river.basin_countries -> Argentina -> location.location.containedby -> Americas\n# Answer:\nArgentina", "# Reasoning Path:\nParan\u00e1 River -> geography.river.basin_countries -> Paraguay -> location.location.containedby -> Americas\n# Answer:\nParaguay"], "ground_truth": ["South America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1731", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.children -> Alexander Richard Eisenhower\n# Answer:\nJulie Nixon Eisenhower", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.parents -> Pat Nixon\n# Answer:\nJulie Nixon Eisenhower", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.children -> Jennie Eisenhower\n# Answer:\nJulie Nixon Eisenhower", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> people.person.parents -> Pat Nixon\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.children -> Melanie Catherine Eisenhower\n# Answer:\nJulie Nixon Eisenhower", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.employment_history -> m.0k0dcyp\n# Answer:\nJulie Nixon Eisenhower", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> people.person.children -> Christopher Nixon Cox\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> film.person_or_entity_appearing_in_film.films -> m.0vpghfz\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.film -> End of the Road: How Money Became Worthless\n# Answer:\nm.010pgj7k"], "ground_truth": ["Richard Nixon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1732", "prediction": ["# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0n0tzlj -> education.education.institution -> Humboldt University of Berlin\n# Answer:\nm.0n0tzlj", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmy28 -> education.education.institution -> University of Bonn\n# Answer:\nm.0vzmy28", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmxyk -> education.education.institution -> University of W\u00fcrzburg\n# Answer:\nm.0vzmxyk", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0n0tzlj -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nm.0n0tzlj", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmxyk -> freebase.valuenotation.has_value -> End Date\n# Answer:\nm.0vzmxyk", "# Reasoning Path:\nTheodor Schwann -> people.deceased_person.place_of_death -> Cologne -> travel.travel_destination.tourist_attractions -> K\u00e4the Kollwitz Museum\n# Answer:\nCologne", "# Reasoning Path:\nTheodor Schwann -> common.topic.image -> Theodore Schwann -> common.image.size -> m.0klz7g\n# Answer:\nTheodore Schwann", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmy28 -> freebase.valuenotation.has_value -> End Date\n# Answer:\nm.0vzmy28", "# Reasoning Path:\nTheodor Schwann -> people.deceased_person.place_of_death -> Cologne -> location.location.contains -> University of Bonn\n# Answer:\nCologne", "# Reasoning Path:\nTheodor Schwann -> people.deceased_person.place_of_death -> Cologne -> travel.travel_destination.tourist_attractions -> Cologne Cable Car\n# Answer:\nCologne"], "ground_truth": ["University of W\u00fcrzburg", "Humboldt University of Berlin", "University of Bonn"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1733", "prediction": ["# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Georges I. Gurdjieff and Thomas De Hartmann -> influence.influence_node.influenced -> Colin Wilson\n# Answer:\nGeorges I. Gurdjieff and Thomas De Hartmann", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Georges I. Gurdjieff and Thomas De Hartmann -> influence.influence_node.influenced -> E. J. Gold\n# Answer:\nGeorges I. Gurdjieff and Thomas De Hartmann", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Henry David Thoreau -> influence.influence_node.influenced_by -> Charles Darwin\n# Answer:\nHenry David Thoreau", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Georges I. Gurdjieff and Thomas De Hartmann -> influence.influence_node.influenced -> Julius Evola\n# Answer:\nGeorges I. Gurdjieff and Thomas De Hartmann", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Ebenezer Howard -> influence.influence_node.influenced_by -> Edward Bellamy\n# Answer:\nEbenezer Howard", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Georges I. Gurdjieff and Thomas De Hartmann -> people.person.profession -> Philosopher\n# Answer:\nGeorges I. Gurdjieff and Thomas De Hartmann", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Henry David Thoreau -> influence.influence_node.influenced_by -> Immanuel Kant\n# Answer:\nHenry David Thoreau", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Georges I. Gurdjieff and Thomas De Hartmann -> music.artist.album -> Alain Kremski - Gurdjieff-De Hartmann: Dervishes trembleurs\n# Answer:\nGeorges I. Gurdjieff and Thomas De Hartmann", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Henry David Thoreau -> influence.influence_node.influenced_by -> Johann Wolfgang von Goethe\n# Answer:\nHenry David Thoreau", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Ebenezer Howard -> influence.influence_node.influenced_by -> Ralph Waldo Emerson\n# Answer:\nEbenezer Howard"], "ground_truth": ["Elizabeth Gordon", "Bruce Price", "Louis Sullivan", "Henry David Thoreau", "Ebenezer Howard", "Georges I. Gurdjieff and Thomas De Hartmann", "William Morris"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 1.0, "ans_recall": 0.42857142857142855, "path_f1": 0.6, "path_precision": 1.0, "path_recall": 0.42857142857142855, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1734", "prediction": ["# Reasoning Path:\nHouston Rockets -> sports.sports_team.championships -> 1994 NBA Finals -> sports.sports_championship_event.runner_up -> New York Knicks\n# Answer:\n1994 NBA Finals", "# Reasoning Path:\nHouston Rockets -> sports.sports_team.championships -> 1995 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n1995 NBA Finals", "# Reasoning Path:\nHouston Rockets -> sports.sports_team.championships -> 1994 NBA Finals -> common.topic.article -> m.08x9cw\n# Answer:\n1994 NBA Finals", "# Reasoning Path:\nHouston Rockets -> sports.sports_team.championships -> 1995 NBA Finals -> sports.sports_championship_event.runner_up -> Orlando Magic\n# Answer:\n1995 NBA Finals", "# Reasoning Path:\nHouston Rockets -> sports.sports_team.championships -> 1995 NBA Finals -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1995 NBA Finals", "# Reasoning Path:\nHouston Rockets -> sports.sports_team.roster -> m.010b9n9r -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.010b9n9r", "# Reasoning Path:\nHouston Rockets -> base.sportbase.sport_sport_club.involves_in -> Basketball -> sports.sport.positions -> Center\n# Answer:\nBasketball", "# Reasoning Path:\nHouston Rockets -> base.sportbase.sport_sport_club.involves_in -> Basketball -> base.sportbase.sport.played_by_clubs -> Dallas Mavericks\n# Answer:\nBasketball", "# Reasoning Path:\nHouston Rockets -> base.sportbase.sport_sport_club.involves_in -> Basketball -> organization.organization_sector.organizations_in_this_sector -> National Basketball Association\n# Answer:\nBasketball", "# Reasoning Path:\nHouston Rockets -> base.sportbase.sport_sport_club.involves_in -> Basketball -> sports.sport.positions -> Forward\n# Answer:\nBasketball"], "ground_truth": ["1994 NBA Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1735", "prediction": ["# Reasoning Path:\nRussia -> base.aareas.schema.administrative_area.administrative_children -> Central Federal District -> base.aareas.schema.administrative_area.administrative_children -> Moscow\n# Answer:\nCentral Federal District", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.11b60vv5th\n# Answer:\ng.11b60vv5th", "# Reasoning Path:\nRussia -> location.statistical_region.co2_emissions_per_capita -> g.1245_8036\n# Answer:\ng.1245_8036", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.12tb6gh0b\n# Answer:\ng.12tb6gh0b", "# Reasoning Path:\nRussia -> base.aareas.schema.administrative_area.administrative_children -> Far Eastern Federal District -> base.aareas.schema.administrative_area.administrative_area_type -> Russian federal district\n# Answer:\nFar Eastern Federal District", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.1hhc3f_fx\n# Answer:\ng.1hhc3f_fx", "# Reasoning Path:\nRussia -> location.statistical_region.co2_emissions_per_capita -> g.1245_9w0x\n# Answer:\ng.1245_9w0x", "# Reasoning Path:\nRussia -> location.statistical_region.co2_emissions_per_capita -> g.1245_chng\n# Answer:\ng.1245_chng", "# Reasoning Path:\nRussia -> base.aareas.schema.administrative_area.administrative_children -> Northwestern Federal District -> base.aareas.schema.administrative_area.administrative_children -> Republic of Karelia\n# Answer:\nNorthwestern Federal District", "# Reasoning Path:\nRussia -> base.aareas.schema.administrative_area.administrative_children -> Northwestern Federal District -> base.aareas.schema.administrative_area.administrative_children -> Saint Petersburg\n# Answer:\nNorthwestern Federal District"], "ground_truth": ["Central Federal District"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1736", "prediction": ["# Reasoning Path:\nHoly Roman Empire -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nHoly Roman Empire -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Austria\n# Answer:\nEurope", "# Reasoning Path:\nHoly Roman Empire -> location.location.containedby -> Europe -> location.location.partially_contains -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nHoly Roman Empire -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nHoly Roman Empire -> location.location.containedby -> Europe -> location.location.contains_major_portion_of -> France\n# Answer:\nEurope", "# Reasoning Path:\nHoly Roman Empire -> location.location.containedby -> Europe -> location.location.partially_contains -> France\n# Answer:\nEurope", "# Reasoning Path:\nHoly Roman Empire -> location.location.containedby -> Europe -> location.location.partially_contains -> Georgia\n# Answer:\nEurope", "# Reasoning Path:\nHoly Roman Empire -> location.location.containedby -> Europe -> location.location.contains_major_portion_of -> Kingdom of the Netherlands\n# Answer:\nEurope", "# Reasoning Path:\nHoly Roman Empire -> book.book_subject.works -> Charlemagne -> government.politician.government_positions_held -> m.03w5qd9\n# Answer:\nCharlemagne", "# Reasoning Path:\nHoly Roman Empire -> location.location.events -> Battle of M\u00fchlberg -> military.military_conflict.combatants -> m.04fvpp7\n# Answer:\nBattle of M\u00fchlberg"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1737", "prediction": ["# Reasoning Path:\nLiam Hemsworth -> award.award_nominee.award_nominations -> m.0z83xb2 -> award.award_nomination.nominated_for -> The Hunger Games\n# Answer:\nm.0z83xb2", "# Reasoning Path:\nLiam Hemsworth -> film.actor.film -> m.0lcw1hd -> film.performance.film -> Paranoia\n# Answer:\nm.0lcw1hd", "# Reasoning Path:\nLiam Hemsworth -> people.person.place_of_birth -> Melbourne -> common.topic.image -> Melbourne Infobox Montage\n# Answer:\nMelbourne", "# Reasoning Path:\nLiam Hemsworth -> award.award_nominee.award_nominations -> m.0z8jj89 -> award.award_nomination.nominated_for -> The Last Song\n# Answer:\nm.0z8jj89", "# Reasoning Path:\nLiam Hemsworth -> people.person.place_of_birth -> Melbourne -> common.topic.image -> Melbourneskyline\n# Answer:\nMelbourne", "# Reasoning Path:\nLiam Hemsworth -> people.person.place_of_birth -> Melbourne -> location.statistical_region.population -> g.11bcdlkx9x\n# Answer:\nMelbourne", "# Reasoning Path:\nLiam Hemsworth -> film.actor.film -> g.11b6dw3r61\n# Answer:\ng.11b6dw3r61", "# Reasoning Path:\nLiam Hemsworth -> people.person.place_of_birth -> Melbourne -> common.topic.image -> m.0292mf_\n# Answer:\nMelbourne", "# Reasoning Path:\nLiam Hemsworth -> people.person.place_of_birth -> Melbourne -> periodicals.newspaper_circulation_area.newspapers -> Il Globo\n# Answer:\nMelbourne", "# Reasoning Path:\nLiam Hemsworth -> award.award_nominee.award_nominations -> m.0z83xb2 -> award.award_nomination.award -> Teen Choice Award for Choice Movie Scene Stealer: Male\n# Answer:\nm.0z83xb2"], "ground_truth": ["The Hunger Games", "By Way of Helena", "The Hunger Games: Catching Fire", "The Last Song", "Love and Honor", "Triangle", "Paranoia", "The Hunger Games: Mockingjay, Part 1", "The Dressmaker", "The Expendables 2", "Aurora Rising", "Empire State", "Arabian Nights", "Knowing", "The Hunger Games: Mockingjay, Part 2", "Draft:Independence Day 2", "Cut Bank", "Timeless"], "ans_acc": 0.16666666666666666, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15, "path_precision": 0.3, "path_recall": 0.1, "path_ans_f1": 0.21428571428571427, "path_ans_precision": 0.3, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTest-1738", "prediction": ["# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.regular_cast -> m.010g2jfs -> tv.regular_tv_appearance.character -> Victoria Newman\n# Answer:\nm.010g2jfs", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.regular_cast -> m.010g2x3j -> tv.regular_tv_appearance.actor -> Daniel Polo\n# Answer:\nm.010g2x3j", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.regular_cast -> m.010g2jfs -> tv.regular_tv_appearance.actor -> Amelia Heinle\n# Answer:\nm.010g2jfs", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.regular_cast -> m.010g6jxk -> tv.regular_tv_appearance.character -> Dylan McAvoy\n# Answer:\nm.010g6jxk", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.recurring_writers -> m.0bmlttr -> tv.tv_program_writer_relationship.writer -> Jim Houghton\n# Answer:\nm.0bmlttr", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.regular_cast -> m.010g6jxk -> tv.regular_tv_appearance.actor -> Steve Burton\n# Answer:\nm.010g6jxk", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.program_creator -> William J. Bell -> award.award_nominee.award_nominations -> m.0n3h18m\n# Answer:\nWilliam J. Bell", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.program_creator -> William J. Bell -> award.award_nominee.award_nominations -> m.0n3mk4c\n# Answer:\nWilliam J. Bell", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.program_creator -> William J. Bell -> people.person.profession -> Television producer\n# Answer:\nWilliam J. Bell", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.recurring_writers -> m.0bnfr70 -> tv.tv_program_writer_relationship.writer -> James E. Reilly\n# Answer:\nm.0bnfr70"], "ground_truth": ["Joshua Morrow"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1739", "prediction": ["# Reasoning Path:\nSouth Park -> tv.tv_program.program_creator -> Trey Parker -> music.producer.releases_produced -> Chocolate Salty Balls\n# Answer:\nTrey Parker", "# Reasoning Path:\nSouth Park -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> South Park Universe -> fictional_universe.fictional_universe.works_set_here -> South Park: Bigger, Longer & Uncut\n# Answer:\nSouth Park Universe", "# Reasoning Path:\nSouth Park -> tv.tv_program.program_creator -> Trey Parker -> people.person.nationality -> United States of America\n# Answer:\nTrey Parker", "# Reasoning Path:\nSouth Park -> tv.tv_program.program_creator -> Trey Parker -> film.producer.film -> Cannibal! The Musical\n# Answer:\nTrey Parker", "# Reasoning Path:\nSouth Park -> tv.tv_program.languages -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nSouth Park -> tv.tv_program.program_creator -> Trey Parker -> film.producer.film -> Orgazmo\n# Answer:\nTrey Parker", "# Reasoning Path:\nSouth Park -> tv.tv_program.program_creator -> Matt Stone -> fictional_universe.fictional_character_creator.fictional_characters_created -> Eric Cartman\n# Answer:\nMatt Stone", "# Reasoning Path:\nSouth Park -> tv.tv_program.program_creator -> Trey Parker -> film.producer.film -> South Park: Bigger, Longer & Uncut\n# Answer:\nTrey Parker", "# Reasoning Path:\nSouth Park -> tv.tv_program.languages -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nSouth Park -> tv.tv_program.program_creator -> Matt Stone -> fictional_universe.fictional_character_creator.fictional_characters_created -> Bebe Stevens\n# Answer:\nMatt Stone"], "ground_truth": ["Trey Parker"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-174", "prediction": ["# Reasoning Path:\nColin Kaepernick -> sports.pro_athlete.teams -> m.0hqf2xx -> sports.sports_team_roster.team -> Nevada Wolf Pack football\n# Answer:\nm.0hqf2xx", "# Reasoning Path:\nColin Kaepernick -> sports.pro_athlete.teams -> m.0hqf2y4 -> sports.sports_team_roster.team -> San Francisco 49ers\n# Answer:\nm.0hqf2y4", "# Reasoning Path:\nColin Kaepernick -> people.person.profession -> American football player -> common.topic.notable_types -> Profession\n# Answer:\nAmerican football player", "# Reasoning Path:\nColin Kaepernick -> common.topic.notable_types -> American football player -> common.topic.notable_types -> Profession\n# Answer:\nAmerican football player", "# Reasoning Path:\nColin Kaepernick -> sports.pro_athlete.teams -> m.0hqf2xx -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hqf2xx", "# Reasoning Path:\nColin Kaepernick -> sports.pro_athlete.teams -> m.0hqf2y4 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0hqf2y4", "# Reasoning Path:\nColin Kaepernick -> people.person.profession -> American football player -> people.profession.specialization_of -> Athlete\n# Answer:\nAmerican football player", "# Reasoning Path:\nColin Kaepernick -> sports.pro_athlete.teams -> m.0hqf2y4 -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.0hqf2y4", "# Reasoning Path:\nColin Kaepernick -> people.person.profession -> American football player -> fictional_universe.character_occupation.characters_with_this_occupation -> A. A. Ron Balakay\n# Answer:\nAmerican football player", "# Reasoning Path:\nColin Kaepernick -> common.topic.notable_types -> American football player -> people.profession.specialization_of -> Athlete\n# Answer:\nAmerican football player"], "ground_truth": ["San Francisco 49ers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1740", "prediction": ["# Reasoning Path:\nAngelina Jolie -> people.person.children -> Knox Leon Jolie-Pitt -> people.person.parents -> Brad Pitt\n# Answer:\nKnox Leon Jolie-Pitt", "# Reasoning Path:\nAngelina Jolie -> people.person.children -> Maddox Chivan Jolie-Pitt -> people.person.parents -> Brad Pitt\n# Answer:\nMaddox Chivan Jolie-Pitt", "# Reasoning Path:\nAngelina Jolie -> people.person.children -> Pax Thien Jolie-Pitt -> people.person.parents -> Brad Pitt\n# Answer:\nPax Thien Jolie-Pitt", "# Reasoning Path:\nAngelina Jolie -> people.person.children -> Knox Leon Jolie-Pitt -> common.topic.notable_for -> g.12556r571\n# Answer:\nKnox Leon Jolie-Pitt", "# Reasoning Path:\nAngelina Jolie -> people.person.children -> Maddox Chivan Jolie-Pitt -> base.cinemainspector.person_sign.star_sign -> Leo\n# Answer:\nMaddox Chivan Jolie-Pitt", "# Reasoning Path:\nAngelina Jolie -> award.award_nominee.award_nominations -> m.010wr37v -> award.award_nomination.nominated_for -> Maleficent\n# Answer:\nm.010wr37v", "# Reasoning Path:\nAngelina Jolie -> people.person.children -> Knox Leon Jolie-Pitt -> people.person.nationality -> France\n# Answer:\nKnox Leon Jolie-Pitt", "# Reasoning Path:\nAngelina Jolie -> people.person.children -> Maddox Chivan Jolie-Pitt -> people.person.gender -> Male\n# Answer:\nMaddox Chivan Jolie-Pitt", "# Reasoning Path:\nAngelina Jolie -> people.person.children -> Pax Thien Jolie-Pitt -> people.person.profession -> Actor\n# Answer:\nPax Thien Jolie-Pitt", "# Reasoning Path:\nAngelina Jolie -> book.author.book_editions_published -> Notes from My Travels -> common.topic.notable_types -> Book\n# Answer:\nNotes from My Travels"], "ground_truth": ["James Haven"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1741", "prediction": ["# Reasoning Path:\nBank of America Corporation -> organization.organization.headquarters -> m.03lfgp9 -> location.mailing_address.citytown -> Charlotte\n# Answer:\nm.03lfgp9", "# Reasoning Path:\nBank of America Corporation -> organization.organization.headquarters -> m.03lfgp9 -> location.mailing_address.country -> United States of America\n# Answer:\nm.03lfgp9", "# Reasoning Path:\nBank of America Corporation -> organization.organization.headquarters -> m.03lfgp9 -> location.mailing_address.state_province_region -> North Carolina\n# Answer:\nm.03lfgp9", "# Reasoning Path:\nBank of America Corporation -> organization.organization.locations -> Bank of America, Bangor Mall -> common.topic.notable_types -> Organization Location\n# Answer:\nBank of America, Bangor Mall", "# Reasoning Path:\nBank of America Corporation -> common.topic.webpage -> m.01xwxgw -> common.webpage.category -> Topic Webpage\n# Answer:\nm.01xwxgw", "# Reasoning Path:\nBank of America Corporation -> common.topic.webpage -> m.0422j0r -> common.webpage.category -> Topic Webpage\n# Answer:\nm.0422j0r", "# Reasoning Path:\nBank of America Corporation -> common.topic.webpage -> m.01xwxgw -> common.webpage.resource -> Bank of America\n# Answer:\nm.01xwxgw", "# Reasoning Path:\nBank of America Corporation -> organization.organization.locations -> Bank of America, Bangor Mall -> common.topic.notable_for -> g.125d131hy\n# Answer:\nBank of America, Bangor Mall", "# Reasoning Path:\nBank of America Corporation -> organization.organization.locations -> Bank of America, Burlington Mall -> business.business_location.in_shopping_center -> Burlington Mall\n# Answer:\nBank of America, Burlington Mall", "# Reasoning Path:\nBank of America Corporation -> organization.organization.locations -> Bank of America, Bangor Mall -> business.business_location.in_shopping_center -> Bangor Mall\n# Answer:\nBank of America, Bangor Mall"], "ground_truth": ["Charlotte"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1742", "prediction": ["# Reasoning Path:\nJapan -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Australia\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\ng.1245_2h9t", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Canada\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.11b71r82sc\n# Answer:\ng.11b71r82sc", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> New Zealand\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_4lv6\n# Answer:\ng.1245_4lv6", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_6fj1\n# Answer:\ng.1245_6fj1", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.12cp_jcjg\n# Answer:\ng.12cp_jcjg"], "ground_truth": ["Constitutional monarchy", "Parliamentary system", "Unitary state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1744", "prediction": ["# Reasoning Path:\nJosh Hamilton -> base.schemastaging.athlete_extra.salary -> m.01064_wv -> base.schemastaging.athlete_salary.team -> Los Angeles Angels of Anaheim\n# Answer:\nm.01064_wv", "# Reasoning Path:\nJosh Hamilton -> base.schemastaging.athlete_extra.salary -> m.0j2ql15 -> base.schemastaging.athlete_salary.team -> Texas Rangers\n# Answer:\nm.0j2ql15", "# Reasoning Path:\nJosh Hamilton -> base.schemastaging.athlete_extra.salary -> m.0r66q69 -> base.schemastaging.athlete_salary.team -> Los Angeles Angels of Anaheim\n# Answer:\nm.0r66q69", "# Reasoning Path:\nJosh Hamilton -> base.schemastaging.athlete_extra.salary -> m.01064_wv -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.01064_wv", "# Reasoning Path:\nJosh Hamilton -> sports.pro_athlete.teams -> m.0nbmmjd -> sports.sports_team_roster.team -> Texas Rangers\n# Answer:\nm.0nbmmjd", "# Reasoning Path:\nJosh Hamilton -> sports.pro_athlete.teams -> m.0nbmmjd -> sports.sports_team_roster.position -> Outfielder\n# Answer:\nm.0nbmmjd", "# Reasoning Path:\nJosh Hamilton -> base.schemastaging.athlete_extra.salary -> m.0j2ql15 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0j2ql15", "# Reasoning Path:\nJosh Hamilton -> base.schemastaging.athlete_extra.salary -> m.0r66q69 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0r66q69", "# Reasoning Path:\nJosh Hamilton -> sports.pro_athlete.teams -> m.0p29q5y -> sports.sports_team_roster.team -> Los Angeles Angels of Anaheim\n# Answer:\nm.0p29q5y", "# Reasoning Path:\nJosh Hamilton -> sports.pro_athlete.teams -> m.0r66mqx -> sports.sports_team_roster.team -> Cincinnati Reds\n# Answer:\nm.0r66mqx"], "ground_truth": ["Los Angeles Angels of Anaheim"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1746", "prediction": ["# Reasoning Path:\nBurundi -> location.location.contains -> Bujumbura -> location.location.containedby -> Bujumbura Rural Province\n# Answer:\nBujumbura", "# Reasoning Path:\nBurundi -> location.location.contains -> Bubanza -> location.location.containedby -> Bubanza Province\n# Answer:\nBubanza", "# Reasoning Path:\nBurundi -> location.location.contains -> Bubanza -> common.topic.image -> Location of Bubanza in Burundi\n# Answer:\nBubanza", "# Reasoning Path:\nBurundi -> location.location.containedby -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica", "# Reasoning Path:\nBurundi -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b71nzld4\n# Answer:\ng.11b71nzld4", "# Reasoning Path:\nBurundi -> location.location.contains -> Bubanza -> location.location.geolocation -> m.0238tfq\n# Answer:\nBubanza", "# Reasoning Path:\nBurundi -> location.location.containedby -> Africa -> base.locations.continents.countries_within -> Angola\n# Answer:\nAfrica", "# Reasoning Path:\nBurundi -> location.location.containedby -> Africa -> location.location.time_zones -> Central Africa Time Zone\n# Answer:\nAfrica", "# Reasoning Path:\nBurundi -> location.location.contains -> Bubanza -> location.location.containedby -> Commune of Bubanza\n# Answer:\nBubanza", "# Reasoning Path:\nBurundi -> location.statistical_region.agriculture_as_percent_of_gdp -> g.1hhc37crm\n# Answer:\ng.1hhc37crm"], "ground_truth": ["Africa"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1748", "prediction": ["# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Mali -> location.country.administrative_divisions -> Bamako\n# Answer:\nMali", "# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Mali -> location.location.partiallycontains -> m.0wg8_56\n# Answer:\nMali", "# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Niger -> location.location.partially_contains -> M\u00e9krou River\n# Answer:\nNiger", "# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Niger -> base.locations.countries.continent -> Africa\n# Answer:\nNiger", "# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Mali -> location.country.administrative_divisions -> Gao Region\n# Answer:\nMali", "# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Niger -> location.location.partially_contains -> Sahara\n# Answer:\nNiger", "# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Niger -> location.country.languages_spoken -> Fula language\n# Answer:\nNiger", "# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Mali -> location.country.administrative_divisions -> Kayes Region\n# Answer:\nMali", "# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Mali -> location.location.partially_contains -> Bafing River\n# Answer:\nMali", "# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Niger -> location.location.partially_contains -> Sahel\n# Answer:\nNiger"], "ground_truth": ["Niger", "Nigeria", "Mali"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 1.0, "ans_recall": 0.6666666666666666, "path_f1": 0.4, "path_precision": 0.5, "path_recall": 0.3333333333333333, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-175", "prediction": ["# Reasoning Path:\nJoe Flacco -> sports.drafted_athlete.drafted -> m.05bj8xr -> sports.sports_league_draft_pick.school -> University of Delaware\n# Answer:\nm.05bj8xr", "# Reasoning Path:\nJoe Flacco -> people.person.education -> m.0hpxy33 -> education.education.institution -> University of Delaware\n# Answer:\nm.0hpxy33", "# Reasoning Path:\nJoe Flacco -> sports.drafted_athlete.drafted -> m.05bj8xr -> sports.sports_league_draft_pick.team -> Baltimore Ravens\n# Answer:\nm.05bj8xr", "# Reasoning Path:\nJoe Flacco -> sports.drafted_athlete.drafted -> m.05bj8xr -> sports.sports_league_draft_pick.draft -> 2008 NFL draft\n# Answer:\nm.05bj8xr", "# Reasoning Path:\nJoe Flacco -> people.person.places_lived -> m.0wkkybd -> people.place_lived.location -> Audubon\n# Answer:\nm.0wkkybd"], "ground_truth": ["University of Delaware"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1750", "prediction": ["# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> location.location.people_born_here -> Alex Burkart\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> people.person.places_lived -> m.03pt7rq -> people.place_lived.location -> Janesville\n# Answer:\nm.03pt7rq", "# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> location.location.containedby -> Rock County\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> location.location.people_born_here -> Aurelia Harwood\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> location.location.people_born_here -> Bob Strampe\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> location.location.containedby -> Wisconsin\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> location.statistical_region.population -> g.11b66f8swj\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> location.statistical_region.population -> g.11bc88b5vp\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> location.statistical_region.population -> g.11x1cgp16\n# Answer:\nJanesville"], "ground_truth": ["Janesville"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1751", "prediction": ["# Reasoning Path:\nAcadia University -> location.location.containedby -> Wolfville -> location.location.containedby -> Canada\n# Answer:\nWolfville", "# Reasoning Path:\nAcadia University -> location.location.containedby -> Nova Scotia -> location.location.containedby -> Canada\n# Answer:\nNova Scotia", "# Reasoning Path:\nAcadia University -> location.location.containedby -> Canada -> location.location.containedby -> North America\n# Answer:\nCanada", "# Reasoning Path:\nAcadia University -> location.location.containedby -> Wolfville -> location.location.containedby -> Kings County\n# Answer:\nWolfville", "# Reasoning Path:\nAcadia University -> location.location.containedby -> Wolfville -> film.film_location.featured_in_films -> Amelia\n# Answer:\nWolfville", "# Reasoning Path:\nAcadia University -> education.educational_institution.phone_number -> m.05sq0xy\n# Answer:\nm.05sq0xy", "# Reasoning Path:\nAcadia University -> location.location.containedby -> Wolfville -> location.location.people_born_here -> Angus Elderkin\n# Answer:\nWolfville", "# Reasoning Path:\nAcadia University -> location.location.containedby -> Nova Scotia -> travel.travel_destination.tourist_attractions -> Cabot Trail\n# Answer:\nNova Scotia", "# Reasoning Path:\nAcadia University -> location.location.containedby -> Canada -> base.locations.countries.continent -> North America\n# Answer:\nCanada", "# Reasoning Path:\nAcadia University -> education.educational_institution.phone_number -> m.05sq0y3\n# Answer:\nm.05sq0y3"], "ground_truth": ["Wolfville"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1752", "prediction": ["# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Novelist -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nNovelist", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Novelist -> people.profession.specialization_of -> Writer\n# Answer:\nNovelist", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Writer -> people.profession.specializations -> Novelist\n# Answer:\nWriter", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Novelist -> common.topic.notable_types -> Profession\n# Answer:\nNovelist", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Writer -> people.profession.specializations -> Screenwriter\n# Answer:\nWriter", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Writer -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nWriter", "# Reasoning Path:\nF. Scott Fitzgerald -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Male\n# Answer:\nheart attack", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Writer -> common.topic.notable_types -> Profession\n# Answer:\nWriter", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Poet -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nPoet", "# Reasoning Path:\nF. Scott Fitzgerald -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.parent_disease -> Cardiovascular disease\n# Answer:\nheart attack"], "ground_truth": ["Writer", "Novelist", "Poet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1753", "prediction": ["# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdxf42 -> base.schemastaging.phone_sandbox.service_location -> Switzerland\n# Answer:\nm.0qdxf42", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdxcyk -> base.schemastaging.phone_sandbox.service_location -> Belgium\n# Answer:\nm.0qdxcyk", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdxf42 -> base.schemastaging.phone_sandbox.caller_category -> Consumer Service\n# Answer:\nm.0qdxf42", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdx6cj -> base.schemastaging.phone_sandbox.service_location -> France\n# Answer:\nm.0qdx6cj", "# Reasoning Path:\nTomtom Nv -> common.topic.notable_for -> g.125dysf1w\n# Answer:\ng.125dysf1w", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdxf42 -> base.schemastaging.phone_sandbox.contact_category -> Technical Support\n# Answer:\nm.0qdxf42", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdx6cj -> base.schemastaging.phone_sandbox.contact_category -> Technical Support\n# Answer:\nm.0qdx6cj", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdx6cj -> base.schemastaging.phone_sandbox.caller_category -> Existing Customers\n# Answer:\nm.0qdx6cj", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdxcyk -> base.schemastaging.phone_sandbox.service_language -> French\n# Answer:\nm.0qdxcyk", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdxcyk -> base.schemastaging.phone_sandbox.caller_category -> Consumer Service\n# Answer:\nm.0qdxcyk"], "ground_truth": ["United Kingdom", "Belgium", "Canada", "Switzerland", "France", "United States of America", "Australia"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.28571428571428564, "path_precision": 0.3, "path_recall": 0.2727272727272727, "path_ans_f1": 0.3529411764705882, "path_ans_precision": 0.3, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1754", "prediction": ["# Reasoning Path:\nIran -> location.country.official_language -> Persian Language -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.official_language -> Persian Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Persian Language -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.official_language -> Persian Language -> common.topic.notable_types -> Human Language\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Persian Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal_per_capita -> g.11b60rrlgr\n# Answer:\ng.11b60rrlgr", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Persian Language -> common.topic.notable_types -> Human Language\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nArabic Language", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6fdrh\n# Answer:\ng.12tb6fdrh", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Arabic Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nArabic Language"], "ground_truth": ["Persian Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1755", "prediction": ["# Reasoning Path:\nSecond Battle of Ypres -> time.event.locations -> Ypres -> location.location.contains -> Bellewaerde\n# Answer:\nYpres", "# Reasoning Path:\nSecond Battle of Ypres -> time.event.locations -> Ypres -> location.location.geolocation -> m.0239wqc\n# Answer:\nYpres", "# Reasoning Path:\nSecond Battle of Ypres -> time.event.locations -> Ypres -> location.location.people_born_here -> Adolph, Landgrave of Hesse-Philippsthal-Barchfeld\n# Answer:\nYpres", "# Reasoning Path:\nSecond Battle of Ypres -> time.event.locations -> Ypres -> location.location.people_born_here -> Albert Dev\u00e8ze\n# Answer:\nYpres", "# Reasoning Path:\nSecond Battle of Ypres -> time.event.locations -> Ypres -> location.location.people_born_here -> Andreas Hyperius\n# Answer:\nYpres", "# Reasoning Path:\nSecond Battle of Ypres -> military.military_conflict.commanders -> m.04m1gm_ -> military.military_command.military_commander -> Herbert Plumer, 1st Viscount Plumer\n# Answer:\nm.04m1gm_", "# Reasoning Path:\nSecond Battle of Ypres -> military.military_conflict.commanders -> m.08n9zdj -> military.military_command.military_combatant -> United Kingdom\n# Answer:\nm.08n9zdj", "# Reasoning Path:\nSecond Battle of Ypres -> military.military_conflict.commanders -> m.08n9zdj -> military.military_command.military_commander -> Horace Smith-Dorrien\n# Answer:\nm.08n9zdj", "# Reasoning Path:\nSecond Battle of Ypres -> military.military_conflict.military_personnel_involved -> Archibald Wavell, 1st Earl Wavell -> military.military_person.participated_in_conflicts -> North African Campaign\n# Answer:\nArchibald Wavell, 1st Earl Wavell", "# Reasoning Path:\nSecond Battle of Ypres -> military.military_conflict.military_personnel_involved -> Archibald Wavell, 1st Earl Wavell -> military.military_person.service -> m.04m0g6y\n# Answer:\nArchibald Wavell, 1st Earl Wavell"], "ground_truth": ["Ypres"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1756", "prediction": ["# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Basque Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nBasque Language", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> Andorra\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.region -> Europe\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\ng.1hhc37x4q", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> language.human_language.countries_spoken_in -> Andorra\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Andorra\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.region -> Europe\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> Gibraltar\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc38hhk\n# Answer:\ng.1hhc38hhk"], "ground_truth": ["Spanish Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1757", "prediction": ["# Reasoning Path:\nMichael Crabtree -> common.topic.notable_types -> American football player -> common.topic.notable_types -> Profession\n# Answer:\nAmerican football player", "# Reasoning Path:\nMichael Crabtree -> common.topic.notable_types -> American football player -> people.profession.specialization_of -> Football player\n# Answer:\nAmerican football player", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.receiving -> m.07ychkl -> american_football.player_receiving_statistics.team -> San Francisco 49ers\n# Answer:\nm.07ychkl", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.receiving -> m.07z2fqm -> american_football.player_receiving_statistics.team -> San Francisco 49ers\n# Answer:\nm.07z2fqm", "# Reasoning Path:\nMichael Crabtree -> common.topic.notable_types -> American football player -> fictional_universe.character_occupation.characters_with_this_occupation -> A. A. Ron Balakay\n# Answer:\nAmerican football player", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.receiving -> m.088yq56 -> american_football.player_receiving_statistics.team -> San Francisco 49ers\n# Answer:\nm.088yq56", "# Reasoning Path:\nMichael Crabtree -> common.topic.notable_types -> American football player -> people.profession.specialization_of -> Athlete\n# Answer:\nAmerican football player", "# Reasoning Path:\nMichael Crabtree -> common.topic.notable_types -> American football player -> fictional_universe.character_occupation.characters_with_this_occupation -> D'Glester Hardunkichud\n# Answer:\nAmerican football player", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.receiving -> m.07ychkl -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\nm.07ychkl", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.games -> m.07ych9z -> american_football.player_game_statistics.team -> San Francisco 49ers\n# Answer:\nm.07ych9z"], "ground_truth": ["Athlete", "Wide receiver", "American football player"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.5, "ans_recall": 0.3333333333333333, "path_f1": 0.4, "path_precision": 0.5, "path_recall": 0.3333333333333333, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.5, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1758", "prediction": ["# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Botswana -> location.location.adjoin_s -> m.03x68v0\n# Answer:\nBotswana", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Gazankulu -> location.location.containedby -> South Africa\n# Answer:\nGazankulu", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Botswana -> location.country.languages_spoken -> English Language\n# Answer:\nBotswana", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Transkei -> location.location.containedby -> South Africa\n# Answer:\nTranskei", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Botswana -> location.location.adjoin_s -> m.0441kl4\n# Answer:\nBotswana", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Gazankulu -> common.topic.notable_for -> g.1255j1dmx\n# Answer:\nGazankulu", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Botswana -> location.location.adjoin_s -> m.0441kll\n# Answer:\nBotswana", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Botswana -> location.country.administrative_divisions -> Central District\n# Answer:\nBotswana", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Gazankulu -> location.country.capital -> Giyani\n# Answer:\nGazankulu", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Botswana -> location.country.administrative_divisions -> Ghanzi District\n# Answer:\nBotswana"], "ground_truth": ["Zambia", "Namibia", "Botswana", "Transkei", "Gazankulu", "Malawi", "South Africa"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 1.0, "ans_recall": 0.42857142857142855, "path_f1": 0.6, "path_precision": 1.0, "path_recall": 0.42857142857142855, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 1.0, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-1759", "prediction": ["# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> organization.organization.geographic_scope -> Alabama\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> CRHOME -> organization.organization.geographic_scope -> Alabama\n# Answer:\nCRHOME", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> organization.organization.geographic_scope -> Arkansas\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> organization.organization.geographic_scope -> California\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> B'nai B'rith District Grand Lodge 7 -> organization.organization.geographic_scope -> Alabama\n# Answer:\nB'nai B'rith District Grand Lodge 7", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> common.topic.subjects -> Automation\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> CRHOME -> organization.organization.geographic_scope -> Arizona\n# Answer:\nCRHOME", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> CRHOME -> organization.organization.place_founded -> Los Angeles\n# Answer:\nCRHOME", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> organization.organization.sectors -> Automation\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> CRHOME -> organization.organization.geographic_scope -> Arkansas\n# Answer:\nCRHOME"], "ground_truth": ["Alabama", "Georgia"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615385, "path_precision": 0.3, "path_recall": 0.10344827586206896, "path_ans_f1": 0.37499999999999994, "path_ans_precision": 0.3, "path_ans_recall": 0.5}
{"id": "WebQTest-176", "prediction": ["# Reasoning Path:\nEmma Stone -> people.person.parents -> Jeff Stone -> people.person.parents -> Conrad Ostberg Stone\n# Answer:\nJeff Stone", "# Reasoning Path:\nEmma Stone -> people.person.parents -> Jeff Stone -> common.topic.notable_for -> g.1258vxmmk\n# Answer:\nJeff Stone", "# Reasoning Path:\nEmma Stone -> people.person.parents -> Jeff Stone -> people.person.spouse_s -> m.0j4jwqj\n# Answer:\nJeff Stone", "# Reasoning Path:\nEmma Stone -> people.person.parents -> Krista Stone -> freebase.valuenotation.has_value -> Parents\n# Answer:\nKrista Stone", "# Reasoning Path:\nEmma Stone -> people.person.parents -> Krista Stone -> people.person.profession -> Homemaking\n# Answer:\nKrista Stone", "# Reasoning Path:\nEmma Stone -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nEmma Stone -> people.person.parents -> Krista Stone -> people.person.children -> Spencer Stone\n# Answer:\nKrista Stone", "# Reasoning Path:\nEmma Stone -> people.person.parents -> Krista Stone -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nKrista Stone", "# Reasoning Path:\nEmma Stone -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nEmma Stone -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth"], "ground_truth": ["Jeff Stone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1760", "prediction": ["# Reasoning Path:\nMossad Agent Ziva David -> tv.tv_character.appeared_in_tv_program -> m.043zbc6 -> tv.regular_tv_appearance.actor -> Cote de Pablo\n# Answer:\nm.043zbc6", "# Reasoning Path:\nMossad Agent Ziva David -> tv.tv_character.appeared_in_tv_program -> m.043zbc6 -> tv.regular_tv_appearance.seasons -> NCIS - Season 10\n# Answer:\nm.043zbc6", "# Reasoning Path:\nMossad Agent Ziva David -> tv.tv_character.appeared_in_tv_program -> m.043zbc6 -> tv.regular_tv_appearance.seasons -> NCIS - Season 11\n# Answer:\nm.043zbc6", "# Reasoning Path:\nMossad Agent Ziva David -> tv.tv_character.appeared_in_tv_program -> m.043zbc6 -> tv.regular_tv_appearance.seasons -> NCIS - Season 3\n# Answer:\nm.043zbc6", "# Reasoning Path:\nMossad Agent Ziva David -> fictional_universe.fictional_character.character_created_by -> Donald P. Bellisario -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nDonald P. Bellisario", "# Reasoning Path:\nMossad Agent Ziva David -> tv.tv_character.appeared_in_tv_episodes -> m.03ly823 -> tv.tv_guest_role.actor -> Cote de Pablo\n# Answer:\nm.03ly823", "# Reasoning Path:\nMossad Agent Ziva David -> tv.tv_character.appeared_in_tv_episodes -> m.09nj77s -> tv.tv_guest_role.episodes_appeared_in -> Kill Ari - Part 2\n# Answer:\nm.09nj77s", "# Reasoning Path:\nMossad Agent Ziva David -> tv.tv_character.appeared_in_tv_episodes -> m.03ly823 -> tv.tv_guest_role.episodes_appeared_in -> Kill Ari - Part 1\n# Answer:\nm.03ly823", "# Reasoning Path:\nMossad Agent Ziva David -> fictional_universe.fictional_character.character_created_by -> Donald P. Bellisario -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nDonald P. Bellisario", "# Reasoning Path:\nMossad Agent Ziva David -> fictional_universe.fictional_character.character_created_by -> Donald P. Bellisario -> award.award_nominee.award_nominations -> m.0lvvn8_\n# Answer:\nDonald P. Bellisario"], "ground_truth": ["Cote de Pablo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.26666666666666666, "path_precision": 0.2, "path_recall": 0.4, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1761", "prediction": ["# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> common.topic.article -> m.02sx6l\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.spouse_s -> m.03mlj1z\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> common.topic.article -> m.0hn9rcf\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.place_of_birth -> Blackmore\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Edward VI of England -> people.person.religion -> Protestantism\n# Answer:\nEdward VI of England", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> people.person.nationality -> Kingdom of England\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> royalty.noble_person.titles -> m.0hqncyh\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> symbols.name_source.namesakes -> Henry FitzAlan, 19th Earl of Arundel -> people.person.children -> Henry Lord Maltravers\n# Answer:\nHenry FitzAlan, 19th Earl of Arundel", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Edward VI of England -> people.deceased_person.place_of_burial -> Henry VII Chapel\n# Answer:\nEdward VI of England", "# Reasoning Path:\nHenry VIII of England -> symbols.name_source.namesakes -> Henry FitzAlan, 19th Earl of Arundel -> people.person.children -> Jane Lumley\n# Answer:\nHenry FitzAlan, 19th Earl of Arundel"], "ground_truth": ["Edward VI of England"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1763", "prediction": ["# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_coaches -> m.0514q1f -> baseball.current_coaching_tenure.baseball_coach -> Dave Duncan\n# Answer:\nm.0514q1f", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_coaches -> m.0514q0p -> baseball.current_coaching_tenure.baseball_coach -> Jos\u00e9 Oquendo\n# Answer:\nm.0514q0p", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_manager -> Mike Matheny -> people.person.gender -> Male\n# Answer:\nMike Matheny", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_manager -> Mike Matheny -> baseball.baseball_player.batting_stats -> m.06rgj_g\n# Answer:\nMike Matheny", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_coaches -> m.0514q1f -> baseball.current_coaching_tenure.coaching_position -> Pitching Coach\n# Answer:\nm.0514q1f", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_coaches -> m.0514q0y -> baseball.current_coaching_tenure.baseball_coach -> Joe Pettini\n# Answer:\nm.0514q0y", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_manager -> Mike Matheny -> baseball.baseball_player.batting_stats -> m.06rgjmg\n# Answer:\nMike Matheny", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_coaches -> m.0514q0p -> baseball.current_coaching_tenure.coaching_position -> Third Base Coach\n# Answer:\nm.0514q0p", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_manager -> Mike Matheny -> baseball.baseball_player.batting_stats -> m.06rgjnw\n# Answer:\nMike Matheny", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_coaches -> m.0514q0y -> baseball.current_coaching_tenure.coaching_position -> Bench Coach\n# Answer:\nm.0514q0y"], "ground_truth": ["Jos\u00e9 Oquendo", "Dave Duncan", "Marty Mason", "Dave McKay", "Hal McRae", "Joe Pettini"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.37499999999999994, "path_ans_precision": 0.3, "path_ans_recall": 0.5}
{"id": "WebQTest-1764", "prediction": ["# Reasoning Path:\nDawood Ibrahim -> people.person.place_of_birth -> Ratnagiri district -> location.location.containedby -> Konkan\n# Answer:\nRatnagiri district", "# Reasoning Path:\nDawood Ibrahim -> people.person.place_of_birth -> Ratnagiri district -> common.topic.notable_for -> g.1256k4jqk\n# Answer:\nRatnagiri district", "# Reasoning Path:\nDawood Ibrahim -> people.person.place_of_birth -> Ratnagiri district -> location.location.contains -> Ratnagiri\n# Answer:\nRatnagiri district", "# Reasoning Path:\nDawood Ibrahim -> people.person.place_of_birth -> Ratnagiri district -> location.location.containedby -> Maharashtra\n# Answer:\nRatnagiri district", "# Reasoning Path:\nDawood Ibrahim -> people.person.spouse_s -> m.0n91dwt -> freebase.valuenotation.has_value -> From\n# Answer:\nm.0n91dwt", "# Reasoning Path:\nDawood Ibrahim -> people.person.spouse_s -> m.0n91dwt -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0n91dwt", "# Reasoning Path:\nDawood Ibrahim -> base.crime.convicted_criminal.convictions -> m.05h1g7c -> base.crime.criminal_conviction.guilty_of -> Illegal drug trade\n# Answer:\nm.05h1g7c", "# Reasoning Path:\nDawood Ibrahim -> people.person.spouse_s -> m.0n91dwt -> people.marriage.spouse -> Zubeena Zareen\n# Answer:\nm.0n91dwt", "# Reasoning Path:\nDawood Ibrahim -> people.person.place_of_birth -> Ratnagiri district -> location.location.contains -> Alore\n# Answer:\nRatnagiri district", "# Reasoning Path:\nDawood Ibrahim -> people.person.spouse_s -> m.0n91dwt -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.0n91dwt"], "ground_truth": ["Maharashtra"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1765", "prediction": ["# Reasoning Path:\nTurkey -> base.locations.countries.continent -> Eurasia -> base.locations.continents.countries_within -> Azerbaijan\n# Answer:\nEurasia", "# Reasoning Path:\nTurkey -> base.locations.countries.continent -> Eurasia -> common.topic.notable_types -> Location\n# Answer:\nEurasia", "# Reasoning Path:\nTurkey -> location.statistical_region.minimum_wage -> g.11b60lkkk3\n# Answer:\ng.11b60lkkk3", "# Reasoning Path:\nTurkey -> base.locations.countries.continent -> Eurasia -> base.locations.continents.countries_within -> France\n# Answer:\nEurasia", "# Reasoning Path:\nTurkey -> location.statistical_region.minimum_wage -> g.11b7gsv6fr\n# Answer:\ng.11b7gsv6fr", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.065sxsl -> military.military_combatant_group.conflict -> Korean War\n# Answer:\nm.065sxsl", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.066402y -> military.military_combatant_group.conflict -> Iraqi no-fly zones\n# Answer:\nm.066402y", "# Reasoning Path:\nTurkey -> location.statistical_region.minimum_wage -> g.1245_1hx9\n# Answer:\ng.1245_1hx9"], "ground_truth": ["Eurasia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1766", "prediction": ["# Reasoning Path:\nRussia -> location.country.languages_spoken -> Abaza Language -> language.human_language.region -> Europe\n# Answer:\nAbaza Language", "# Reasoning Path:\nRussian -> people.ethnicity.languages_spoken -> Russian Language -> language.human_language.countries_spoken_in -> Russia\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> common.topic.notable_types -> Human Language\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.11b60vv5th\n# Answer:\ng.11b60vv5th", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Abaza Language -> common.topic.notable_types -> Human Language\n# Answer:\nAbaza Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> language.human_language.countries_spoken_in -> Belarus\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> language.human_language.region -> Eurasia\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Osetin Language -> language.human_language.main_country -> Georgia\n# Answer:\nOsetin Language", "# Reasoning Path:\nRussian -> people.ethnicity.languages_spoken -> Russian Language -> language.human_language.countries_spoken_in -> Lithuania\n# Answer:\nRussian Language", "# Reasoning Path:\nRussian -> people.ethnicity.languages_spoken -> Russian Language -> common.topic.notable_types -> Human Language\n# Answer:\nRussian Language"], "ground_truth": ["Lezgi Language", "Azerbaijani language", "Lak Language", "Erzya Language", "Ingush Language", "Nogai Language", "Ukrainian Language", "Udmurt Language", "Osetin Language", "Aghul language", "Yiddish Language", "Buryat language", "Mari language", "Rutul language", "Tatar Language", "Tabassaran Language", "Adyghe Language", "Moksha Language", "Tsakhur Language", "Crimean Turkish Language", "Bashkir Language", "Kalmyk-Oirat Language", "Altai language", "Yakut Language", "Kumyk Language", "Abaza Language", "Komi language", "Karachay-Balkar Language", "Tuvin Language", "Chechen Language", "Russian Language", "Khakas Language", "Kabardian Language", "Avar Language", "Dargwa Language"], "ans_acc": 0.08571428571428572, "ans_hit": 1, "ans_f1": 0.1565217391304348, "ans_precission": 0.9, "ans_recall": 0.08571428571428572, "path_f1": 0.10069930069930069, "path_precision": 0.9, "path_recall": 0.05333333333333334, "path_ans_f1": 0.1565217391304348, "path_ans_precision": 0.9, "path_ans_recall": 0.08571428571428572}
{"id": "WebQTest-1767", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_burial -> Martin Luther King, Jr. National Historic Site -> travel.tourist_attraction.near_travel_destination -> Atlanta\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_burial -> Martin Luther King, Jr. National Historic Site -> people.place_of_interment.interred_here -> Ann Nixon Cooper\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_burial -> Martin Luther King, Jr. National Historic Site -> location.location.geolocation -> m.0wmyhzk\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> travel.tourist_attraction.near_travel_destination -> Atlanta\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_burial -> Martin Luther King, Jr. National Historic Site -> people.place_of_interment.interred_here -> Benjamin F. Ward\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> location.location.geolocation -> m.0wmyhzk\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_burial -> Martin Luther King, Jr. National Historic Site -> people.place_of_interment.interred_here -> Ethel Roosevelt Derby\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.06_41f8\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nm.011j_4sh", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> common.topic.article -> m.05nnh0\n# Answer:\nDexter Avenue Baptist Church"], "ground_truth": ["Martin Luther King, Jr. National Historic Site"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1768", "prediction": ["# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Guaran\u00ed language -> common.topic.notable_types -> Human Language\n# Answer:\nGuaran\u00ed language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> language.language_family.geographic_distribution -> Bolivia\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> common.topic.notable_types -> Human Language\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Bolivia\n# Answer:\nSpanish Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> language.language_family.geographic_distribution -> Peru\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Kayak II -> biology.organism.organism_type -> Horse\n# Answer:\nKayak II", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Chile\n# Answer:\nSpanish Language", "# Reasoning Path:\nArgentina -> location.location.partially_contains -> Alto San Juan -> location.location.partially_contained_by -> m.0wg8lvc\n# Answer:\nAlto San Juan", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Kayak II -> base.thoroughbredracing.thoroughbred_racehorse.sex -> Stallion\n# Answer:\nKayak II"], "ground_truth": ["Spanish Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1769", "prediction": ["# Reasoning Path:\nWales -> location.location.containedby -> United Kingdom -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nWales -> base.biblioness.bibs_location.country -> United Kingdom -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nWales -> location.location.containedby -> United Kingdom -> location.location.containedby -> Europe\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nWales -> location.location.containedby -> United Kingdom -> base.aareas.schema.administrative_area.pertinent_type -> UK constituent country\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nWales -> location.country.first_level_divisions -> Monmouthshire -> location.location.containedby -> United Kingdom\n# Answer:\nMonmouthshire", "# Reasoning Path:\nWales -> location.location.containedby -> United Kingdom, with Dependencies and Territories -> base.aareas.schema.administrative_area.administrative_children -> United Kingdom\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nWales -> location.location.containedby -> United Kingdom -> location.country.first_level_divisions -> England\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nWales -> base.biblioness.bibs_location.country -> United Kingdom -> location.location.containedby -> Europe\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nWales -> base.biblioness.bibs_location.country -> United Kingdom -> base.aareas.schema.administrative_area.pertinent_type -> UK constituent country\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nWales -> location.location.containedby -> United Kingdom -> location.country.first_level_divisions -> Northern Ireland\n# Answer:\nUnited Kingdom"], "ground_truth": ["United Kingdom, with Dependencies and Territories", "United Kingdom"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-177", "prediction": ["# Reasoning Path:\nTheodore Roosevelt -> people.deceased_person.place_of_burial -> Youngs Memorial Cemetery -> people.place_of_interment.interred_here -> Archibald Bulloch Roosevelt Jr.\n# Answer:\nYoungs Memorial Cemetery", "# Reasoning Path:\nTheodore Roosevelt -> people.deceased_person.place_of_burial -> Youngs Memorial Cemetery -> common.topic.notable_types -> Cemetery\n# Answer:\nYoungs Memorial Cemetery", "# Reasoning Path:\nTheodore Roosevelt -> people.deceased_person.place_of_burial -> Youngs Memorial Cemetery -> common.topic.article -> m.05230r2\n# Answer:\nYoungs Memorial Cemetery", "# Reasoning Path:\nTheodore Roosevelt -> people.person.children -> Archibald Roosevelt -> people.deceased_person.place_of_burial -> Youngs Memorial Cemetery\n# Answer:\nArchibald Roosevelt", "# Reasoning Path:\nTheodore Roosevelt -> people.deceased_person.place_of_burial -> Youngs Memorial Cemetery -> people.place_of_interment.interred_here -> Archibald Roosevelt\n# Answer:\nYoungs Memorial Cemetery", "# Reasoning Path:\nTheodore Roosevelt -> people.deceased_person.place_of_burial -> Youngs Memorial Cemetery -> people.place_of_interment.interred_here -> Edith Roosevelt\n# Answer:\nYoungs Memorial Cemetery", "# Reasoning Path:\nTheodore Roosevelt -> people.person.children -> Alice Roosevelt Longworth -> people.deceased_person.place_of_burial -> Rock Creek Cemetery\n# Answer:\nAlice Roosevelt Longworth", "# Reasoning Path:\nTheodore Roosevelt -> people.person.children -> Archibald Roosevelt -> people.person.children -> Archibald Bulloch Roosevelt Jr.\n# Answer:\nArchibald Roosevelt", "# Reasoning Path:\nTheodore Roosevelt -> people.person.children -> Ethel Roosevelt Derby -> people.deceased_person.place_of_burial -> Martin Luther King, Jr. National Historic Site\n# Answer:\nEthel Roosevelt Derby", "# Reasoning Path:\nTheodore Roosevelt -> people.person.children -> Archibald Roosevelt -> people.person.parents -> Edith Roosevelt\n# Answer:\nArchibald Roosevelt"], "ground_truth": ["Youngs Memorial Cemetery"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1770", "prediction": ["# Reasoning Path:\nRafael Nadal -> people.person.nationality -> Spain -> common.topic.notable_types -> Country\n# Answer:\nSpain", "# Reasoning Path:\nRafael Nadal -> people.person.nationality -> Spain -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\nSpain", "# Reasoning Path:\nRafael Nadal -> people.person.nationality -> Spain -> sports.sport_country.athletic_performances -> m.0fp9_m_\n# Answer:\nSpain", "# Reasoning Path:\nRafael Nadal -> tennis.tennis_player.matches_won -> m.010h70y7 -> tennis.tennis_match.match_format -> Men's singles\n# Answer:\nm.010h70y7", "# Reasoning Path:\nRafael Nadal -> award.award_nominee.award_nominations -> m.010_zj6l -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nm.010_zj6l", "# Reasoning Path:\nRafael Nadal -> tennis.tennis_player.matches_won -> m.010h70y7 -> tennis.tennis_match.event -> Miami Open\n# Answer:\nm.010h70y7", "# Reasoning Path:\nRafael Nadal -> tennis.tennis_player.matches_won -> m.010hdsln -> tennis.tennis_match.loser -> Roberto Bautista Agut\n# Answer:\nm.010hdsln", "# Reasoning Path:\nRafael Nadal -> tennis.tennis_player.matches_won -> m.010h70y7 -> tennis.tennis_match.loser -> Milos Raonic\n# Answer:\nm.010h70y7", "# Reasoning Path:\nRafael Nadal -> tennis.tennis_player.matches_won -> m.010hvfy9 -> tennis.tennis_match.loser -> Kei Nishikori\n# Answer:\nm.010hvfy9", "# Reasoning Path:\nRafael Nadal -> award.award_nominee.award_nominations -> m.010_zj6l -> award.award_nomination.award -> Best Male Tennis Player ESPY Award\n# Answer:\nm.010_zj6l"], "ground_truth": ["Spain"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1771", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.beliefs -> \u1e6c\u016bb\u0101 -> common.topic.notable_for -> g.1q6hmhsk5\n# Answer:\n\u1e6c\u016bb\u0101", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> common.topic.notable_for -> g.1q69mrtxz\n# Answer:\nEntering Heaven alive", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.beliefs -> End time\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> \u1e6c\u016bb\u0101 -> common.topic.article -> m.0hr6vbt\n# Answer:\n\u1e6c\u016bb\u0101", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> common.topic.article -> m.02wvcg8\n# Answer:\nEntering Heaven alive", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> book.book_subject.works -> Beside Still Waters\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> religion.belief.belief_of -> Anglicanism\n# Answer:\nEntering Heaven alive", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> base.schemastaging.context_name.pronunciation -> g.125_r5my9\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> base.ontologies.ontology_instance.equivalent_instances -> m.07nfc1g\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> book.book_subject.works -> I, the Sun\n# Answer:\nMonotheism"], "ground_truth": ["Predestination in Islam", "Islamic holy books", "God in Islam", "Qiyamah", "Prophets in Islam", "\u1e6c\u016bb\u0101", "Islamic view of angels", "Tawhid", "Monotheism", "Masih ad-Dajjal", "Mahdi", "Sharia", "Entering Heaven alive"], "ans_acc": 0.23076923076923078, "ans_hit": 1, "ans_f1": 0.3582089552238806, "ans_precission": 0.8, "ans_recall": 0.23076923076923078, "path_f1": 0.3582089552238806, "path_precision": 0.8, "path_recall": 0.23076923076923078, "path_ans_f1": 0.3582089552238806, "path_ans_precision": 0.8, "path_ans_recall": 0.23076923076923078}
{"id": "WebQTest-1772", "prediction": ["# Reasoning Path:\nSpain -> government.governmental_jurisdiction.governing_officials -> m.0hnsn5h -> government.government_position_held.office_holder -> Mariano Rajoy\n# Answer:\nm.0hnsn5h", "# Reasoning Path:\nSpain -> government.governmental_jurisdiction.governing_officials -> m.010wrj1t -> government.government_position_held.office_holder -> Mariano Rajoy\n# Answer:\nm.010wrj1t", "# Reasoning Path:\nSpain -> government.governmental_jurisdiction.governing_officials -> m.0hnsn5h -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0hnsn5h", "# Reasoning Path:\nSpain -> government.governmental_jurisdiction.governing_officials -> m.0hnsn5h -> government.government_position_held.basic_title -> Prime minister\n# Answer:\nm.0hnsn5h", "# Reasoning Path:\nSpain -> government.governmental_jurisdiction.governing_officials -> m.010wrg7c -> government.government_position_held.office_holder -> Alejandro Rodr\u00edguez de Valc\u00e1rcel\n# Answer:\nm.010wrg7c", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\ng.1hhc37x4q", "# Reasoning Path:\nSpain -> government.governmental_jurisdiction.governing_officials -> m.010wrj1t -> government.government_position_held.basic_title -> Deputy prime minister\n# Answer:\nm.010wrj1t", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60xs9d2\n# Answer:\ng.11b60xs9d2", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc38hhk\n# Answer:\ng.1hhc38hhk", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc3dvnk\n# Answer:\ng.1hhc3dvnk"], "ground_truth": ["Jos\u00e9 Luis Rodr\u00edguez Zapatero", "Mariano Rajoy"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.28571428571428575, "path_ans_precision": 0.2, "path_ans_recall": 0.5}
{"id": "WebQTest-1773", "prediction": ["# Reasoning Path:\nLeonidas I -> people.person.children -> Pleistarchus -> people.person.place_of_birth -> Sparta\n# Answer:\nPleistarchus", "# Reasoning Path:\nLeonidas I -> people.person.children -> Pleistarchus -> people.person.parents -> Gorgo, Queen of Sparta\n# Answer:\nPleistarchus", "# Reasoning Path:\nLeonidas I -> people.person.children -> Pleistarchus -> freebase.valuenotation.has_value -> Cause of death\n# Answer:\nPleistarchus", "# Reasoning Path:\nLeonidas I -> people.person.children -> Pleistarchus -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nPleistarchus", "# Reasoning Path:\nLeonidas I -> people.person.children -> Pleistarchus -> freebase.valuenotation.has_value -> Place of death\n# Answer:\nPleistarchus", "# Reasoning Path:\nLeonidas I -> common.topic.webpage -> m.09xtfr4 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09xtfr4", "# Reasoning Path:\nLeonidas I -> common.topic.webpage -> m.09xv7nj -> common.webpage.resource -> Reviewing the Reviews: '300'\n# Answer:\nm.09xv7nj", "# Reasoning Path:\nLeonidas I -> common.topic.webpage -> m.09xtfr4 -> common.webpage.resource -> Are you eager to G(r)eek out and see '300'?\n# Answer:\nm.09xtfr4", "# Reasoning Path:\nLeonidas I -> common.topic.webpage -> m.09xv7nj -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09xv7nj", "# Reasoning Path:\nLeonidas I -> common.topic.webpage -> m.09yc9vz -> common.webpage.resource -> '300' sequel: Zack Snyder says he's up for it...but are you?\n# Answer:\nm.09yc9vz"], "ground_truth": ["Pleistarchus"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1774", "prediction": ["# Reasoning Path:\nJudi Dench -> people.person.spouse_s -> m.0hyl98h -> people.marriage.spouse -> Michael Williams\n# Answer:\nm.0hyl98h", "# Reasoning Path:\nJudi Dench -> people.person.spouse_s -> m.0hyl98h -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.0hyl98h", "# Reasoning Path:\nJudi Dench -> people.person.spouse_s -> m.0hyl98h -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0hyl98h", "# Reasoning Path:\nJudi Dench -> film.actor.film -> m.0113rwfg -> film.performance.character -> Evelyn Greenslade\n# Answer:\nm.0113rwfg", "# Reasoning Path:\nJudi Dench -> film.actor.film -> m.0113rwfg -> film.performance.film -> The Second Best Exotic Marigold Hotel\n# Answer:\nm.0113rwfg", "# Reasoning Path:\nJudi Dench -> people.person.profession -> Actor -> people.profession.specializations -> Narrator\n# Answer:\nActor", "# Reasoning Path:\nJudi Dench -> people.person.profession -> Actor -> type.property.reverse_property -> Theater Roles\n# Answer:\nActor", "# Reasoning Path:\nJudi Dench -> film.actor.film -> m.012zj5xp -> film.performance.film -> Tulip Fever\n# Answer:\nm.012zj5xp", "# Reasoning Path:\nJudi Dench -> people.person.profession -> Actor -> award.award_discipline.awards_in_this_discipline -> Golden Globe Award for Best Actress \u2013 Motion Picture \u2013 Drama\n# Answer:\nActor", "# Reasoning Path:\nJudi Dench -> people.person.profession -> Musician -> common.topic.notable_types -> Profession\n# Answer:\nMusician"], "ground_truth": ["Michael Williams"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1775", "prediction": ["# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Basque Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nBasque Language", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> Andorra\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.region -> Europe\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\ng.1hhc37x4q", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> language.human_language.countries_spoken_in -> Andorra\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Andorra\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.region -> Europe\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> Gibraltar\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc38hhk\n# Answer:\ng.1hhc38hhk"], "ground_truth": ["Catalan language", "Galician Language", "Spanish Language", "Basque Language", "Occitan language"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6857142857142857, "ans_precission": 0.8, "ans_recall": 0.6, "path_f1": 0.5454545454545454, "path_precision": 0.5, "path_recall": 0.6, "path_ans_f1": 0.6857142857142857, "path_ans_precision": 0.8, "path_ans_recall": 0.6}
{"id": "WebQTest-1776", "prediction": ["# Reasoning Path:\nSahara -> location.location.containedby -> North Africa -> location.location.containedby -> Africa\n# Answer:\nNorth Africa", "# Reasoning Path:\nSahara -> book.written_work.previous_in_series -> Hemingway Adventure\n# Answer:\nHemingway Adventure", "# Reasoning Path:\nSahara -> location.location.containedby -> North Africa -> location.location.contains -> Al Sharqia Governorate\n# Answer:\nNorth Africa", "# Reasoning Path:\nSahara -> geography.geographical_feature.category -> Desert -> common.topic.webpage -> m.09y18gb\n# Answer:\nDesert", "# Reasoning Path:\nSahara -> location.location.containedby -> North Africa -> location.location.partiallycontains -> m.0wg920z\n# Answer:\nNorth Africa", "# Reasoning Path:\nSahara -> location.location.containedby -> North Africa -> location.location.contains -> Alexandria Governorate\n# Answer:\nNorth Africa", "# Reasoning Path:\nSahara -> geography.geographical_feature.category -> Desert -> common.topic.webpage -> m.0b6rhtw\n# Answer:\nDesert", "# Reasoning Path:\nSahara -> geography.geographical_feature.category -> Desert -> base.ecology.type_of_ecosystem.ecosystems_of_this_type -> Colorado Desert\n# Answer:\nDesert", "# Reasoning Path:\nSahara -> location.location.containedby -> North Africa -> location.location.contains -> Ancient Greece\n# Answer:\nNorth Africa", "# Reasoning Path:\nSahara -> location.location.containedby -> North Africa -> location.location.partiallycontains -> m.0wg9213\n# Answer:\nNorth Africa"], "ground_truth": ["North Africa"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1777", "prediction": ["# Reasoning Path:\nRihanna -> people.person.parents -> Monica Braithwaite -> freebase.valuenotation.has_value -> Parents\n# Answer:\nMonica Braithwaite", "# Reasoning Path:\nRihanna -> people.person.parents -> Monica Braithwaite -> people.person.nationality -> Barbados\n# Answer:\nMonica Braithwaite", "# Reasoning Path:\nRihanna -> people.person.parents -> Monica Braithwaite -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nMonica Braithwaite", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.location.contains -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\ng.11bv1mttmr", "# Reasoning Path:\nRihanna -> people.person.parents -> Monica Braithwaite -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nMonica Braithwaite", "# Reasoning Path:\nRihanna -> people.person.parents -> Ronald Fenty -> people.person.children -> Kandy Fenty\n# Answer:\nRonald Fenty", "# Reasoning Path:\nRihanna -> people.person.parents -> Ronald Fenty -> celebrities.celebrity.substance_abuse_problems -> m.0k8hg6n\n# Answer:\nRonald Fenty", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.country.administrative_divisions -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\ng.11bv383dbd"], "ground_truth": ["Monica Braithwaite"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1779", "prediction": ["# Reasoning Path:\nTheodore Roosevelt -> base.inaugurations.inauguration_speaker.inauguration -> Theodore Roosevelt 1905 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nTheodore Roosevelt 1905 presidential inauguration", "# Reasoning Path:\nTheodore Roosevelt -> people.person.employment_history -> m.0w7nv3t -> business.employment_tenure.title -> President\n# Answer:\nm.0w7nv3t", "# Reasoning Path:\nTheodore Roosevelt -> base.inaugurations.inauguration_speaker.inauguration -> Theodore Roosevelt 1901 presidential inauguration -> common.topic.article -> m.05c4h5h\n# Answer:\nTheodore Roosevelt 1901 presidential inauguration", "# Reasoning Path:\nTheodore Roosevelt -> people.person.employment_history -> m.0w7nv3t -> business.employment_tenure.company -> Federal government of the United States\n# Answer:\nm.0w7nv3t", "# Reasoning Path:\nTheodore Roosevelt -> symbols.name_source.namesakes -> Roosevelt -> location.statistical_region.population -> g.11bc88l5f2\n# Answer:\nRoosevelt", "# Reasoning Path:\nTheodore Roosevelt -> symbols.name_source.namesakes -> Roosevelt -> common.topic.article -> m.02gt_g\n# Answer:\nRoosevelt", "# Reasoning Path:\nTheodore Roosevelt -> base.inaugurations.inauguration_speaker.inauguration -> Theodore Roosevelt 1905 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nTheodore Roosevelt 1905 presidential inauguration", "# Reasoning Path:\nTheodore Roosevelt -> base.inaugurations.inauguration_speaker.inauguration -> Theodore Roosevelt 1905 presidential inauguration -> common.topic.notable_types -> Event\n# Answer:\nTheodore Roosevelt 1905 presidential inauguration", "# Reasoning Path:\nTheodore Roosevelt -> symbols.name_source.namesakes -> Roosevelt -> location.hud_foreclosure_area.hhuniv -> m.07h_1tt\n# Answer:\nRoosevelt", "# Reasoning Path:\nTheodore Roosevelt -> base.inaugurations.inauguration_speaker.inauguration -> Theodore Roosevelt 1905 presidential inauguration -> common.topic.notable_for -> g.125dcmf8k\n# Answer:\nTheodore Roosevelt 1905 presidential inauguration"], "ground_truth": ["1901-09-14"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-178", "prediction": ["# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.05rln07 -> event.speech_or_presentation.speaker_s -> Franklin D. Roosevelt\n# Answer:\nm.05rln07", "# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.05rpn5h -> event.speech_or_presentation.speaker_s -> Harry S. Truman\n# Answer:\nm.05rpn5h", "# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.05rln07 -> event.speech_or_presentation.presented_work -> Arsenal of Democracy\n# Answer:\nm.05rln07", "# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.010nmtv6 -> event.speech_or_presentation.speaker_s -> Y\u014dhei K\u014dno\n# Answer:\nm.010nmtv6", "# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.05rpn5h -> event.speech_or_presentation.presented_work -> Harry S. Truman's 1949 inaugural address\n# Answer:\nm.05rpn5h", "# Reasoning Path:\nWorld War II -> military.military_conflict.commanders -> m.04jx14r -> military.military_command.military_commander -> Harry S. Truman\n# Answer:\nm.04jx14r", "# Reasoning Path:\nWorld War II -> military.military_conflict.commanders -> m.04jx14r -> military.military_command.military_combatant -> United States of America\n# Answer:\nm.04jx14r", "# Reasoning Path:\nWorld War II -> military.military_conflict.commanders -> m.010_09yn -> military.military_command.military_commander -> Hirohito\n# Answer:\nm.010_09yn", "# Reasoning Path:\nWorld War II -> base.skosbase.vocabulary_equivalent_topic.narrower_concept -> World War, 1939-1945--Hostages -> base.skosbase.skos_concept.in_scheme -> Authorities Concept Scheme\n# Answer:\nWorld War, 1939-1945--Hostages", "# Reasoning Path:\nWorld War II -> base.skosbase.vocabulary_equivalent_topic.narrower_concept -> World War, 1939-1945--Hostages -> base.skosbase.skos_concept.broader_topic -> Hostage\n# Answer:\nWorld War, 1939-1945--Hostages"], "ground_truth": ["Harry S. Truman", "Franklin D. Roosevelt"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1780", "prediction": ["# Reasoning Path:\nMichael Oher -> base.schemastaging.athlete_extra.salary -> m.0j2l9cn -> base.schemastaging.athlete_salary.team -> Baltimore Ravens\n# Answer:\nm.0j2l9cn", "# Reasoning Path:\nMichael Oher -> base.schemastaging.athlete_extra.salary -> m.0j2l9dd -> base.schemastaging.athlete_salary.team -> Baltimore Ravens\n# Answer:\nm.0j2l9dd", "# Reasoning Path:\nMichael Oher -> base.schemastaging.athlete_extra.salary -> m.0j2l9cn -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0j2l9cn", "# Reasoning Path:\nMichael Oher -> base.schemastaging.athlete_extra.salary -> m.0j2l9cx -> base.schemastaging.athlete_salary.team -> Baltimore Ravens\n# Answer:\nm.0j2l9cx", "# Reasoning Path:\nMichael Oher -> base.schemastaging.athlete_extra.salary -> m.0j2l9dd -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0j2l9dd", "# Reasoning Path:\nMichael Oher -> american_football.football_player.receiving -> m.07sgvjq -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\nm.07sgvjq", "# Reasoning Path:\nMichael Oher -> american_football.football_player.receiving -> m.09rlxpq -> american_football.player_receiving_statistics.team -> Baltimore Ravens\n# Answer:\nm.09rlxpq", "# Reasoning Path:\nMichael Oher -> base.schemastaging.athlete_extra.salary -> m.0j2l9cx -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0j2l9cx", "# Reasoning Path:\nMichael Oher -> american_football.football_player.receiving -> m.07sgvjq -> american_football.player_receiving_statistics.team -> Baltimore Ravens\n# Answer:\nm.07sgvjq", "# Reasoning Path:\nMichael Oher -> american_football.football_player.games -> m.07sgv1_ -> american_football.player_game_statistics.season -> 2009 NFL season\n# Answer:\nm.07sgv1_"], "ground_truth": ["Baltimore Ravens"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2040816326530612, "path_precision": 0.5, "path_recall": 0.1282051282051282, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1781", "prediction": ["# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Mahatma Gandhi -> influence.influence_node.influenced -> Gene Sharp\n# Answer:\nMahatma Gandhi", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Mahatma Gandhi -> influence.influence_node.influenced -> John Lennon\n# Answer:\nMahatma Gandhi", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Baruch Spinoza -> influence.influence_node.influenced -> Bertrand Russell\n# Answer:\nBaruch Spinoza", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Mahatma Gandhi -> influence.influence_node.influenced -> Martin Luther King, Jr.\n# Answer:\nMahatma Gandhi", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Arthur Schopenhauer -> influence.influence_node.influenced_by -> Buddhism\n# Answer:\nArthur Schopenhauer", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Mahatma Gandhi -> base.activism.activist.area_of_activism -> Humanitarian\n# Answer:\nMahatma Gandhi", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Baruch Spinoza -> influence.influence_node.influenced -> Christopher Hitchens\n# Answer:\nBaruch Spinoza", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Mahatma Gandhi -> influence.influence_node.influenced_by -> Gautama Buddha\n# Answer:\nMahatma Gandhi", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Baruch Spinoza -> influence.influence_node.influenced -> Erwin Schr\u00f6dinger\n# Answer:\nBaruch Spinoza", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Baruch Spinoza -> common.topic.notable_types -> Author\n# Answer:\nBaruch Spinoza"], "ground_truth": ["Henry George", "Fyodor Dostoyevsky", "Isaac Newton", "Moritz Schlick", "Mahatma Gandhi", "Baruch Spinoza", "James Clerk Maxwell", "Karl Pearson", "Hermann Minkowski", "Bernhard Riemann", "Paul Val\u00e9ry", "Arthur Schopenhauer", "Thomas Young", "George Bernard Shaw", "Ernst Mach", "Hendrik Lorentz", "Riazuddin", "David Hume"], "ans_acc": 0.16666666666666666, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 1.0, "ans_recall": 0.16666666666666666, "path_f1": 0.2857142857142857, "path_precision": 1.0, "path_recall": 0.16666666666666666, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 1.0, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTest-1782", "prediction": ["# Reasoning Path:\nEgypt -> location.statistical_region.places_exported_to -> m.048prww -> location.imports_and_exports.exported_to -> Sudan\n# Answer:\nm.048prww", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Sudan\n# Answer:\nArabic Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.countries_spoken_in -> Sudan\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.location.events -> 1964 Arab League summit -> common.topic.notable_types -> Event\n# Answer:\n1964 Arab League summit", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.writing_system -> Arabic alphabet\n# Answer:\nArabic Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.main_country -> Sudan\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Israel\n# Answer:\nArabic Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.region -> Africa\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.location.events -> 1976 Arab League summit -> common.topic.notable_types -> Event\n# Answer:\n1976 Arab League summit", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nArabic Language"], "ground_truth": ["Sudan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.24, "path_precision": 0.3, "path_recall": 0.2, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1784", "prediction": ["# Reasoning Path:\nAfrican Union -> organization.organization.headquarters -> m.0pcb32w -> location.mailing_address.citytown -> Addis Ababa\n# Answer:\nm.0pcb32w", "# Reasoning Path:\nAfrican Union -> organization.organization.headquarters -> m.0pcb32w -> common.topic.notable_types -> Location\n# Answer:\nm.0pcb32w", "# Reasoning Path:\nAfrican Union -> base.schemastaging.context_name.pronunciation -> g.125_l8y1s\n# Answer:\ng.125_l8y1s", "# Reasoning Path:\nAfrican Union -> organization.organization.headquarters -> m.0pcb32w -> location.mailing_address.postal_code -> 378\n# Answer:\nm.0pcb32w", "# Reasoning Path:\nAfrican Union -> organization.membership_organization.members -> m.010nx8yt -> organization.organization_membership.member -> Guinea\n# Answer:\nm.010nx8yt", "# Reasoning Path:\nAfrican Union -> organization.membership_organization.members -> m.010p154h -> organization.organization_membership.member -> Niger\n# Answer:\nm.010p154h", "# Reasoning Path:\nAfrican Union -> organization.membership_organization.members -> m.049yh0_ -> organization.organization_membership.member -> Tanzania\n# Answer:\nm.049yh0_"], "ground_truth": ["Addis Ababa"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-1785", "prediction": ["# Reasoning Path:\nKim Richards -> film.actor.film -> m.0131hj6c -> film.performance.film -> Nanny and the Professor\n# Answer:\nm.0131hj6c", "# Reasoning Path:\nKim Richards -> film.actor.film -> m.02vblh0 -> film.performance.film -> Devil Dog: The Hound of Hell\n# Answer:\nm.02vblh0", "# Reasoning Path:\nKim Richards -> film.actor.film -> m.03jrr4j -> film.performance.film -> No Deposit, No Return\n# Answer:\nm.03jrr4j", "# Reasoning Path:\nKim Richards -> people.person.parents -> Kathleen Richards -> people.person.profession -> Casting Director\n# Answer:\nKathleen Richards", "# Reasoning Path:\nKim Richards -> tv.tv_actor.starring_roles -> m.0bngb3y -> tv.regular_tv_appearance.series -> Hello, Larry\n# Answer:\nm.0bngb3y", "# Reasoning Path:\nKim Richards -> tv.tv_actor.starring_roles -> m.0bngyp9 -> tv.regular_tv_appearance.series -> Nanny and the Professor\n# Answer:\nm.0bngyp9", "# Reasoning Path:\nKim Richards -> film.actor.film -> m.02vblh0 -> film.performance.character -> Bonnie Barry\n# Answer:\nm.02vblh0", "# Reasoning Path:\nKim Richards -> tv.tv_actor.starring_roles -> m.0bngb3y -> tv.regular_tv_appearance.character -> Ruthie Alder\n# Answer:\nm.0bngb3y", "# Reasoning Path:\nKim Richards -> tv.tv_actor.starring_roles -> m.0bngq3l -> tv.regular_tv_appearance.series -> James at 15\n# Answer:\nm.0bngq3l", "# Reasoning Path:\nKim Richards -> people.person.parents -> Kathleen Richards -> people.person.spouse_s -> m.0j4js1t\n# Answer:\nKathleen Richards"], "ground_truth": ["Escape to Witch Mountain", "Nanny and the Professor", "Black Snake Moan", "The Car", "Tuff Turf", "Race to Witch Mountain", "Devil Dog: The Hound of Hell", "The Whiz Kid and the Mystery at Riverton", "Raid on Entebbe", "Meatballs Part II", "No Deposit, No Return", "Return from Witch Mountain"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.34782608695652173, "path_precision": 0.4, "path_recall": 0.3076923076923077, "path_ans_f1": 0.3076923076923077, "path_ans_precision": 0.4, "path_ans_recall": 0.25}
{"id": "WebQTest-1788", "prediction": ["# Reasoning Path:\nHenri Rousseau -> common.topic.article -> m.02rdfj\n# Answer:\nm.02rdfj", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Eyewitness: Reports from an Art World in Crisis\n# Answer:\nModern art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nModern art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Origins, Imitation, Conventions\n# Answer:\nModern art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Na\u00efve art -> visual_art.art_period_movement.associated_artists -> Adolf Dietrich\n# Answer:\nNa\u00efve art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Oscar Bluemner\n# Answer:\nModern art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> visual_art.art_period_movement.associated_artworks -> Elemental: The Power of Illuminated Love\n# Answer:\nModern art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Na\u00efve art -> base.schemastaging.context_name.pronunciation -> g.125_s1w5k\n# Answer:\nNa\u00efve art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Na\u00efve art -> visual_art.art_period_movement.associated_artists -> Alexandre Gr\u00e9goire\n# Answer:\nNa\u00efve art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Na\u00efve art -> visual_art.art_period_movement.associated_artworks -> Boy on the Rocks\n# Answer:\nNa\u00efve art"], "ground_truth": ["Primitivism", "Modern art", "Post-Impressionism", "Na\u00efve art"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6428571428571429, "ans_precission": 0.9, "ans_recall": 0.5, "path_f1": 0.6428571428571429, "path_precision": 0.9, "path_recall": 0.5, "path_ans_f1": 0.6428571428571429, "path_ans_precision": 0.9, "path_ans_recall": 0.5}
{"id": "WebQTest-1789", "prediction": ["# Reasoning Path:\nJohn Edwards -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Representative\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Edwards -> people.person.profession -> Lawyer -> base.descriptive_names.names.descriptive_name -> m.0101pjk9\n# Answer:\nLawyer", "# Reasoning Path:\nJohn Edwards -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Senator\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Edwards -> people.person.profession -> Lawyer -> base.descriptive_names.names.descriptive_name -> m.0101pk4l\n# Answer:\nLawyer", "# Reasoning Path:\nJohn Edwards -> people.person.profession -> Lawyer -> people.profession.specializations -> Advocate\n# Answer:\nLawyer", "# Reasoning Path:\nJohn Edwards -> people.person.profession -> Lawyer -> base.descriptive_names.names.descriptive_name -> m.0101pk69\n# Answer:\nLawyer", "# Reasoning Path:\nJohn Edwards -> people.person.profession -> Lawyer -> common.topic.subject_of -> 1st Claims\n# Answer:\nLawyer", "# Reasoning Path:\nJohn Edwards -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> Ambassador\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Edwards -> people.person.profession -> Lawyer -> people.profession.specializations -> Attorney at law\n# Answer:\nLawyer", "# Reasoning Path:\nJohn Edwards -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdn_\n# Answer:\nPolitician"], "ground_truth": ["Political corruption"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-179", "prediction": ["# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Impressionism -> book.book_subject.works -> Ce\u0301zanne\n# Answer:\nImpressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Impressionism -> book.written_work.subjects -> Art\n# Answer:\nImpressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Impressionism -> base.schemastaging.context_name.pronunciation -> g.125_m6dv2\n# Answer:\nImpressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Impressionism -> book.book_subject.works -> Childe Hassam: Impressionist\n# Answer:\nImpressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Fauvism -> visual_art.art_period_movement.associated_artworks -> Bathers by a River\n# Answer:\nFauvism", "# Reasoning Path:\nHenri Matisse -> book.book_subject.works -> Matisse -> common.topic.notable_for -> g.125dhx5l1\n# Answer:\nMatisse", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Impressionism -> book.book_subject.works -> Edouard Manet\n# Answer:\nImpressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Fauvism -> common.topic.notable_for -> g.1257w3_9g\n# Answer:\nFauvism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Modernism -> visual_art.art_period_movement.associated_artworks -> Dam\n# Answer:\nModernism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Modernism -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nModernism"], "ground_truth": ["Neo-impressionism", "Modernism", "Impressionism", "Modern art", "Fauvism"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7200000000000001, "ans_precission": 0.9, "ans_recall": 0.6, "path_f1": 0.7200000000000001, "path_precision": 0.9, "path_recall": 0.6, "path_ans_f1": 0.7200000000000001, "path_ans_precision": 0.9, "path_ans_recall": 0.6}
{"id": "WebQTest-1790", "prediction": ["# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> location.location.containedby -> California\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> location.place_with_neighborhoods.neighborhoods -> Adams Point\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> location.location.containedby -> Area code 510\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> location.location.containedby -> United States of America\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> location.place_with_neighborhoods.neighborhoods -> Bella Vista\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> film.film_location.featured_in_films -> A View to a Kill\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> people.person.sibling_s -> m.0yqb6x_ -> people.sibling_relationship.sibling -> Lee Edward Newton\n# Answer:\nm.0yqb6x_", "# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> location.place_with_neighborhoods.neighborhoods -> Brookfield Village, Oakland, California\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> film.film_location.featured_in_films -> Arena\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> people.person.sibling_s -> m.0yqb7hh -> people.sibling_relationship.sibling -> Walter Newton\n# Answer:\nm.0yqb7hh"], "ground_truth": ["Oakland"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1791", "prediction": ["# Reasoning Path:\nPrinting press -> law.invention.inventor -> Johannes Gutenberg -> people.person.place_of_birth -> Mainz\n# Answer:\nJohannes Gutenberg", "# Reasoning Path:\nPrinting press -> law.invention.inventor -> Johannes Gutenberg -> people.deceased_person.place_of_death -> Mainz\n# Answer:\nJohannes Gutenberg", "# Reasoning Path:\nPrinting press -> common.topic.article -> m.05sy2\n# Answer:\nm.05sy2", "# Reasoning Path:\nPrinting press -> law.invention.inventor -> Johannes Gutenberg -> base.argumentmaps.innovator.original_ideas -> Movable type\n# Answer:\nJohannes Gutenberg", "# Reasoning Path:\nPrinting press -> base.ontologies.ontology_instance.equivalent_instances -> m.09dvt_4 -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nm.09dvt_4"], "ground_truth": ["1440"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1792", "prediction": ["# Reasoning Path:\nMidlothian -> location.location.containedby -> Scotland -> location.location.containedby -> United Kingdom\n# Answer:\nScotland", "# Reasoning Path:\nMidlothian -> location.location.geolocation -> m.02_h136\n# Answer:\nm.02_h136", "# Reasoning Path:\nMidlothian -> location.location.containedby -> United Kingdom -> location.location.containedby -> Europe\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nMidlothian -> location.location.containedby -> Scotland -> location.country.first_level_divisions -> Aberdeen\n# Answer:\nScotland", "# Reasoning Path:\nMidlothian -> location.location.containedby -> United Kingdom -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nMidlothian -> location.location.contains -> Bilston -> location.location.containedby -> United Kingdom\n# Answer:\nBilston", "# Reasoning Path:\nMidlothian -> location.location.containedby -> Scotland -> location.country.first_level_divisions -> Aberdeenshire\n# Answer:\nScotland", "# Reasoning Path:\nMidlothian -> location.location.containedby -> Scotland -> base.aareas.schema.administrative_area.administrative_children -> Falkirk\n# Answer:\nScotland", "# Reasoning Path:\nMidlothian -> location.location.containedby -> United Kingdom -> location.location.containedby -> Western Europe\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nMidlothian -> location.location.containedby -> United Kingdom -> base.aareas.schema.administrative_area.pertinent_type -> UK constituent country\n# Answer:\nUnited Kingdom"], "ground_truth": ["United Kingdom, with Dependencies and Territories", "United Kingdom", "Scotland"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7272727272727272, "ans_precission": 0.8, "ans_recall": 0.6666666666666666, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1793", "prediction": ["# Reasoning Path:\nMarta -> sports.pro_athlete.teams -> m.04gd4np -> sports.sports_team_roster.team -> Ume\u00e5 IK\n# Answer:\nm.04gd4np", "# Reasoning Path:\nMarta -> sports.pro_athlete.teams -> m.0z3vl6r -> sports.sports_team_roster.team -> Los Angeles Sol\n# Answer:\nm.0z3vl6r", "# Reasoning Path:\nMarta -> sports.pro_athlete.teams -> m.0z3vp21 -> sports.sports_team_roster.team -> Tyres\u00f6 FF\n# Answer:\nm.0z3vp21", "# Reasoning Path:\nMarta -> sports.pro_athlete.teams -> m.04gd4np -> sports.sports_team_roster.position -> Forward\n# Answer:\nm.04gd4np", "# Reasoning Path:\nMarta -> people.person.places_lived -> m.03pr_w0 -> people.place_lived.location -> Alagoas\n# Answer:\nm.03pr_w0", "# Reasoning Path:\nMarta -> sports.pro_athlete.teams -> m.0z3vl6r -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0z3vl6r", "# Reasoning Path:\nMarta -> sports.pro_athlete.teams -> m.0z3vp21 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0z3vp21", "# Reasoning Path:\nMarta -> people.person.places_lived -> m.0kdb3nq -> people.place_lived.location -> Tyres\u00f6 Municipality\n# Answer:\nm.0kdb3nq", "# Reasoning Path:\nMarta -> sports.pro_athlete.teams -> m.0z3vp21 -> sports.sports_team_roster.position -> Forward\n# Answer:\nm.0z3vp21", "# Reasoning Path:\nMarta -> award.award_winner.awards_won -> m.0ysztkj -> award.award_honor.award -> FIFA World Player of the Year\n# Answer:\nm.0ysztkj"], "ground_truth": ["Brazil women's national football team", "Tyres\u00f6 FF"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 0.1, "path_ans_recall": 0.5}
{"id": "WebQTest-1794", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.place_of_birth -> Detroit -> base.biblioness.bibs_location.state -> Michigan\n# Answer:\nDetroit", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.place_of_birth -> Colonia Dubl\u00e1n\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.place_of_birth -> Detroit -> location.location.containedby -> Michigan\n# Answer:\nDetroit", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.parents -> Anna Amelia Romney\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> Lenore Romney -> people.person.place_of_birth -> Logan\n# Answer:\nLenore Romney", "# Reasoning Path:\nMitt Romney -> people.person.place_of_birth -> Detroit -> location.location.containedby -> Area code 313\n# Answer:\nDetroit", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.parents -> Gaskell Romney\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> Lenore Romney -> people.person.parents -> Harold A. Lafount\n# Answer:\nLenore Romney", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0_gvz4k -> film.personal_film_appearance.film -> Mitt\n# Answer:\nm.0_gvz4k", "# Reasoning Path:\nMitt Romney -> people.person.place_of_birth -> Detroit -> location.location.containedby -> United States of America\n# Answer:\nDetroit"], "ground_truth": ["Detroit"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1795", "prediction": ["# Reasoning Path:\nJohn Terry -> people.person.children -> Georgie John Terry -> people.person.parents -> Toni Terry\n# Answer:\nGeorgie John Terry", "# Reasoning Path:\nJohn Terry -> people.person.children -> Georgie John Terry -> common.topic.notable_types -> Person\n# Answer:\nGeorgie John Terry", "# Reasoning Path:\nJohn Terry -> people.person.children -> Summer Rose Terry -> common.topic.notable_types -> Person\n# Answer:\nSummer Rose Terry", "# Reasoning Path:\nJohn Terry -> people.person.children -> Georgie John Terry -> people.person.sibling_s -> m.0k01v5f\n# Answer:\nGeorgie John Terry", "# Reasoning Path:\nJohn Terry -> people.person.children -> Summer Rose Terry -> people.person.parents -> Toni Terry\n# Answer:\nSummer Rose Terry", "# Reasoning Path:\nJohn Terry -> people.person.children -> Summer Rose Terry -> common.topic.notable_for -> g.125fnd4rg\n# Answer:\nSummer Rose Terry", "# Reasoning Path:\nJohn Terry -> common.topic.image -> John-Terry2 -> common.image.size -> m.0292hlm\n# Answer:\nJohn-Terry2", "# Reasoning Path:\nJohn Terry -> tv.tv_actor.guest_roles -> m.09nxf_f -> tv.tv_guest_role.episodes_appeared_in -> Season 12, Episode 4\n# Answer:\nm.09nxf_f", "# Reasoning Path:\nJohn Terry -> tv.tv_actor.guest_roles -> m.09nxf_l -> tv.tv_guest_role.episodes_appeared_in -> Episode 1\n# Answer:\nm.09nxf_l", "# Reasoning Path:\nJohn Terry -> common.topic.image -> JohnTerry -> common.image.size -> m.04r7rwn\n# Answer:\nJohnTerry"], "ground_truth": ["Summer Rose Terry", "Georgie John Terry"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1796", "prediction": ["# Reasoning Path:\nMichael Jackson -> music.artist.origin -> Gary -> location.location.containedby -> United States of America\n# Answer:\nGary", "# Reasoning Path:\nMichael Jackson -> people.person.place_of_birth -> Gary -> location.location.containedby -> United States of America\n# Answer:\nGary", "# Reasoning Path:\nMichael Jackson -> film.person_or_entity_appearing_in_film.films -> m.010wz0tg -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.010wz0tg", "# Reasoning Path:\nMichael Jackson -> film.person_or_entity_appearing_in_film.films -> m.011qz88l -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.011qz88l", "# Reasoning Path:\nMichael Jackson -> film.person_or_entity_appearing_in_film.films -> m.010wz0tg -> film.personal_film_appearance.film -> Live at Wembley July 16, 1988\n# Answer:\nm.010wz0tg", "# Reasoning Path:\nMichael Jackson -> film.person_or_entity_appearing_in_film.films -> m.08mvytl -> film.personal_film_appearance.film -> Michael Jackson's This Is It\n# Answer:\nm.08mvytl", "# Reasoning Path:\nMichael Jackson -> film.person_or_entity_appearing_in_film.films -> m.011qz88l -> film.personal_film_appearance.film -> Michael: The Last Photo Shoots\n# Answer:\nm.011qz88l", "# Reasoning Path:\nMichael Jackson -> film.person_or_entity_appearing_in_film.films -> m.08mvytl -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.08mvytl"], "ground_truth": ["Gary"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1797", "prediction": ["# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> military.military_combatant.military_commanders -> m.048z_8v\n# Answer:\nConfederate States of America", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> John C. Pemberton -> people.person.education -> m.04hw096\n# Answer:\nJohn C. Pemberton", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> military.military_combatant.military_commanders -> m.049y2m5\n# Answer:\nConfederate States of America", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> book.book_subject.works -> An Honorable Defeat: The Last Days of the Confederate Government\n# Answer:\nConfederate States of America", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> military.military_combatant.military_commanders -> m.049y2qw\n# Answer:\nConfederate States of America", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> location.country.capital -> Danville\n# Answer:\nConfederate States of America", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> John C. Pemberton -> people.person.nationality -> Confederate States of America\n# Answer:\nJohn C. Pemberton", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> book.book_subject.works -> Apostles of Disunion: Southern Secession Commissioners and the Causes of the Civil War\n# Answer:\nConfederate States of America", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> John C. Pemberton -> military.military_commander.military_commands -> m.049y2m5\n# Answer:\nJohn C. Pemberton", "# Reasoning Path:\nSiege of Vicksburg -> military.military_conflict.military_personnel_involved -> Carter L. Stevenson -> common.topic.notable_for -> g.1258mw8j_\n# Answer:\nCarter L. Stevenson"], "ground_truth": ["Confederate States of America", "Ulysses S. Grant", "Union", "John C. Pemberton", "United States of America"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.5538461538461539, "ans_precission": 0.9, "ans_recall": 0.4, "path_f1": 0.5538461538461539, "path_precision": 0.9, "path_recall": 0.4, "path_ans_f1": 0.7200000000000001, "path_ans_precision": 0.9, "path_ans_recall": 0.6}
{"id": "WebQTest-1799", "prediction": ["# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Playwright -> base.descriptive_names.names.descriptive_name -> m.01066t9b\n# Answer:\nPlaywright", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Playwright -> people.profession.specialization_of -> Writer\n# Answer:\nPlaywright", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Novelist -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nNovelist", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Playwright -> education.field_of_study.students_majoring -> m.012nj6qx\n# Answer:\nPlaywright", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Playwright -> base.descriptive_names.names.descriptive_name -> m.01066tcy\n# Answer:\nPlaywright", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Novelist -> people.profession.specialization_of -> Writer\n# Answer:\nNovelist", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Playwright -> base.descriptive_names.names.descriptive_name -> m.01066tpc\n# Answer:\nPlaywright", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Novelist -> common.topic.notable_types -> Profession\n# Answer:\nNovelist", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Writer -> people.profession.specializations -> Playwright\n# Answer:\nWriter", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Writer -> people.profession.specializations -> Novelist\n# Answer:\nWriter"], "ground_truth": ["Playwright", "Writer", "Novelist"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-180", "prediction": ["# Reasoning Path:\nMissouri River -> geography.river.mouth -> Mississippi River -> geography.river.mouth -> Gulf of Mexico\n# Answer:\nMississippi River", "# Reasoning Path:\nMissouri River -> geography.river.mouth -> Mississippi River -> location.location.containedby -> North America\n# Answer:\nMississippi River", "# Reasoning Path:\nMissouri River -> geography.river.mouth -> Mississippi River -> location.location.partially_containedby -> Mississippi\n# Answer:\nMississippi River", "# Reasoning Path:\nMissouri River -> geography.river.mouth -> Mississippi River -> location.location.containedby -> United States of America\n# Answer:\nMississippi River", "# Reasoning Path:\nMissouri River -> geography.river.mouth -> Mississippi River -> location.location.partially_containedby -> Missouri\n# Answer:\nMississippi River", "# Reasoning Path:\nMissouri River -> book.book_subject.works -> Bucking the sun -> book.book.editions -> Bucking the Sun\n# Answer:\nBucking the sun", "# Reasoning Path:\nMissouri River -> geography.body_of_water.bridges -> ASB Bridge -> transportation.bridge.locale -> Kansas City\n# Answer:\nASB Bridge", "# Reasoning Path:\nMissouri River -> book.book_subject.works -> Bucking the sun -> book.book.genre -> Fiction\n# Answer:\nBucking the sun", "# Reasoning Path:\nMissouri River -> geography.river.mouth -> Mississippi River -> location.location.partially_containedby -> Arkansas\n# Answer:\nMississippi River", "# Reasoning Path:\nMissouri River -> book.book_subject.works -> Bucking the sun -> common.topic.notable_types -> Book\n# Answer:\nBucking the sun"], "ground_truth": ["Mississippi River"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1800", "prediction": ["# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> sports.sports_league.seasons -> 2012 Major League Baseball season\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> sports.sports_league.seasons -> 1974 Major League Baseball Season\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> organization.organization.child -> m.0ck2ds6\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> business.employer.employees -> m.010vsyz7\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> organization.organization.child -> m.0ck2dsc\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> common.topic.webpage -> m.09w8snm -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w8snm", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> organization.organization.child -> m.0zs6st6\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> business.employer.employees -> m.0cmxjcn\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> common.topic.image -> Crowd outside the 1903 World Series -> common.image.size -> m.02bpf1c\n# Answer:\nCrowd outside the 1903 World Series", "# Reasoning Path:\nWorld Series -> common.topic.webpage -> m.09w8snm -> common.webpage.resource -> Ratings: Obama's ad, World Series clincher top the week\n# Answer:\nm.09w8snm"], "ground_truth": ["1903 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1801", "prediction": ["# Reasoning Path:\nSimon Birch -> film.film_character.portrayed_in_films -> m.0k39m5 -> film.performance.actor -> Ian Michael Smith\n# Answer:\nm.0k39m5", "# Reasoning Path:\nSimon Birch -> film.film.starring -> m.0k39m5 -> film.performance.actor -> Ian Michael Smith\n# Answer:\nm.0k39m5", "# Reasoning Path:\nSimon Birch -> award.award_nominated_work.award_nominations -> m.0n4xs9w -> award.award_nomination.award -> Critics' Choice Movie Award for Best Young Performer\n# Answer:\nm.0n4xs9w", "# Reasoning Path:\nSimon Birch -> award.award_nominated_work.award_nominations -> m.0n4xs9w -> award.award_nomination.ceremony -> 4th Critics' Choice Awards\n# Answer:\nm.0n4xs9w", "# Reasoning Path:\nSimon Birch -> film.film.starring -> m.04m3px3 -> film.performance.actor -> Jim Carrey\n# Answer:\nm.04m3px3", "# Reasoning Path:\nSimon Birch -> film.film.starring -> m.0cfzkm5 -> film.performance.actor -> Dana Ivey\n# Answer:\nm.0cfzkm5", "# Reasoning Path:\nSimon Birch -> film.film.starring -> m.04m3px3 -> film.performance.character -> Joe Wenteworth\n# Answer:\nm.04m3px3", "# Reasoning Path:\nSimon Birch -> film.film.starring -> m.0cfzkm5 -> film.performance.character -> Grandmother Wenteworth\n# Answer:\nm.0cfzkm5"], "ground_truth": ["Ian Michael Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.125, "path_recall": 0.3333333333333333, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1802", "prediction": ["# Reasoning Path:\nUnited States of America -> location.location.time_zones -> Hawaii-Aleutian Time Zone\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nUnited States of America -> location.location.time_zones -> Alaska Time Zone\n# Answer:\nAlaska Time Zone", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\ng.125_r6z32", "# Reasoning Path:\nUnited States of America -> location.location.time_zones -> Samoa Time Zone -> time.time_zone.locations_in_this_time_zone -> American Samoa\n# Answer:\nSamoa Time Zone", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nm.010r7f0z", "# Reasoning Path:\nUnited States of America -> location.location.time_zones -> Samoa Time Zone -> time.time_zone.locations_in_this_time_zone -> Palmyra Atoll\n# Answer:\nSamoa Time Zone", "# Reasoning Path:\nUnited States of America -> periodicals.newspaper_circulation_area.newspapers -> American Free Press -> book.periodical.language -> English Language\n# Answer:\nAmerican Free Press", "# Reasoning Path:\nUnited States of America -> periodicals.newspaper_circulation_area.newspapers -> American Free Press -> common.topic.notable_types -> Newspaper\n# Answer:\nAmerican Free Press", "# Reasoning Path:\nUnited States of America -> periodicals.newspaper_circulation_area.newspapers -> C\u00f3ndor -> book.newspaper.circulation_areas -> Germany\n# Answer:\nC\u00f3ndor", "# Reasoning Path:\nUnited States of America -> periodicals.newspaper_circulation_area.newspapers -> China Daily -> book.newspaper.circulation_areas -> New York City\n# Answer:\nChina Daily"], "ground_truth": ["Pacific Time Zone", "Alaska Time Zone", "Eastern Time Zone", "Central Time Zone", "Mountain Time Zone", "Samoa Time Zone", "Chamorro Time Zone", "Hawaii-Aleutian Time Zone", "Atlantic Time Zone"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.4, "ans_recall": 0.3333333333333333, "path_f1": 0.3636363636363636, "path_precision": 0.4, "path_recall": 0.3333333333333333, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.4, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1803", "prediction": ["# Reasoning Path:\nGeneral Motors -> business.business_operation.industry -> Motor Vehicles and Passenger Car Bodies -> business.industry.companies -> Cadillac\n# Answer:\nMotor Vehicles and Passenger Car Bodies", "# Reasoning Path:\nGeneral Motors -> business.business_operation.industry -> Motor Vehicles and Passenger Car Bodies -> common.topic.notable_types -> Industry\n# Answer:\nMotor Vehicles and Passenger Car Bodies", "# Reasoning Path:\nGeneral Motors -> business.business_operation.industry -> Motor Vehicles and Passenger Car Bodies -> business.industry.companies -> American Motors\n# Answer:\nMotor Vehicles and Passenger Car Bodies", "# Reasoning Path:\nGeneral Motors -> business.business_operation.industry -> Motor Vehicles and Passenger Car Bodies -> business.industry.child_industry -> Motor Vehicles and Passenger Car Bodies (automobiles)\n# Answer:\nMotor Vehicles and Passenger Car Bodies", "# Reasoning Path:\nGeneral Motors -> common.topic.subjects -> GMC -> common.topic.subjects -> GMC Acadia\n# Answer:\nGMC", "# Reasoning Path:\nGeneral Motors -> business.business_operation.industry -> Motor Vehicles and Passenger Car Bodies -> business.industry.companies -> Audi\n# Answer:\nMotor Vehicles and Passenger Car Bodies", "# Reasoning Path:\nGeneral Motors -> business.business_operation.industry -> Motor Vehicles and Passenger Car Bodies -> business.industry.child_industry -> Motor Vehicles and Passenger Car Bodies (heavy duty trucks)\n# Answer:\nMotor Vehicles and Passenger Car Bodies", "# Reasoning Path:\nGeneral Motors -> common.topic.subjects -> GMC -> common.topic.subjects -> GMC Canyon\n# Answer:\nGMC", "# Reasoning Path:\nGeneral Motors -> automotive.company.make_s -> GM Korea -> common.topic.notable_types -> Automobile Company\n# Answer:\nGM Korea", "# Reasoning Path:\nGeneral Motors -> automotive.company.make_s -> GM Korea -> common.topic.notable_for -> g.1z2sst2xz\n# Answer:\nGM Korea"], "ground_truth": ["Motor Vehicles and Passenger Car Bodies", "Automotive industry"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.6, "ans_recall": 0.5, "path_f1": 0.5454545454545454, "path_precision": 0.6, "path_recall": 0.5, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.6, "path_ans_recall": 0.5}
{"id": "WebQTest-1804", "prediction": ["# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> British Sign Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nBritish Sign Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> British Sign Language -> common.topic.notable_types -> Human Language\n# Answer:\nBritish Sign Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> British Sign Language -> language.human_language.region -> Europe\n# Answer:\nBritish Sign Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Scots Language -> common.topic.notable_types -> Human Language\n# Answer:\nScots Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.countries_spoken_in -> Kingdom of Great Britain\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nm.0ghc35h", "# Reasoning Path:\nUnited Kingdom -> location.statistical_region.long_term_unemployment_rate -> g.12cp_jvpx\n# Answer:\ng.12cp_jvpx", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.language_family -> Indo-European languages\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.countries_spoken_in -> Scotland\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.arrested_person -> Gary Glitter\n# Answer:\nm.0ghc35h"], "ground_truth": ["Scottish Gaelic", "Esperanto Language", "British Sign Language", "Welsh Language", "English Language", "Cornish Language", "Scots Language", "Irish"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.48837209302325574, "ans_precission": 0.7, "ans_recall": 0.375, "path_f1": 0.48837209302325574, "path_precision": 0.7, "path_recall": 0.375, "path_ans_f1": 0.48837209302325574, "path_ans_precision": 0.7, "path_ans_recall": 0.375}
{"id": "WebQTest-1805", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Costa Rica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.region -> Americas\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.notable_for -> g.1255wtfxx\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_for -> g.1255wtfxx\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\n1812 Louisiana hurricane"], "ground_truth": ["Jamaican English", "Jamaican Creole English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1806", "prediction": ["# Reasoning Path:\nLeBron James -> sports.pro_athlete.sports_played_professionally -> m.0c3p_f6 -> sports.pro_sports_played.sport -> Basketball\n# Answer:\nm.0c3p_f6", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.position_s -> Point forward -> sports.sports_position.players -> g.11byb3s7cc\n# Answer:\nPoint forward", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.position_s -> Point forward -> sports.sports_position.players -> m.010fwg1m\n# Answer:\nPoint forward", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.position_s -> Point forward -> common.topic.image -> Basketball half-court\n# Answer:\nPoint forward", "# Reasoning Path:\nLeBron James -> sports.pro_athlete.sports_played_professionally -> m.0c3p_f6 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0c3p_f6", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.position_s -> Point forward -> sports.sports_position.players -> m.010fyfng\n# Answer:\nPoint forward", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.position_s -> Point forward -> basketball.basketball_position.players -> Anton Ponkrashov\n# Answer:\nPoint forward", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.position_s -> Point forward -> common.topic.image -> Lebronred\n# Answer:\nPoint forward", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.position_s -> Power forward -> common.topic.image -> Basketball half-court\n# Answer:\nPower forward", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.position_s -> Power forward -> common.topic.notable_for -> g.125509zsm\n# Answer:\nPower forward"], "ground_truth": ["Shooting guard", "Power forward", "Small forward", "Forward", "Point forward"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6857142857142857, "ans_precission": 0.8, "ans_recall": 0.6, "path_f1": 0.47058823529411764, "path_precision": 0.8, "path_recall": 0.3333333333333333, "path_ans_f1": 0.6857142857142857, "path_ans_precision": 0.8, "path_ans_recall": 0.6}
{"id": "WebQTest-1807", "prediction": ["# Reasoning Path:\nWayne Gretzky -> sports.pro_athlete.teams -> m.07vkdw8 -> sports.sports_team_roster.team -> Edmonton Oilers\n# Answer:\nm.07vkdw8", "# Reasoning Path:\nWayne Gretzky -> sports.pro_athlete.teams -> m.07vkdw8 -> sports.sports_team_roster.position -> Centerman\n# Answer:\nm.07vkdw8", "# Reasoning Path:\nWayne Gretzky -> sports.pro_athlete.teams -> m.0125clwk -> sports.sports_team_roster.position -> Centerman\n# Answer:\nm.0125clwk", "# Reasoning Path:\nWayne Gretzky -> people.person.place_of_birth -> Brantford -> travel.travel_destination.tourist_attractions -> Arnold Anderson Stadium at Cockshutt Park\n# Answer:\nBrantford", "# Reasoning Path:\nWayne Gretzky -> people.person.place_of_birth -> Brantford -> government.governmental_jurisdiction.government_bodies -> Brantford City Council\n# Answer:\nBrantford", "# Reasoning Path:\nWayne Gretzky -> people.person.place_of_birth -> Brantford -> sports.sports_team_location.teams -> Brantford Smoke\n# Answer:\nBrantford", "# Reasoning Path:\nWayne Gretzky -> people.person.place_of_birth -> Brantford -> travel.travel_destination.tourist_attractions -> Bell Telephone Memorial\n# Answer:\nBrantford", "# Reasoning Path:\nWayne Gretzky -> people.person.place_of_birth -> Brantford -> travel.travel_destination.tourist_attractions -> Brantford Golf & Country Club\n# Answer:\nBrantford", "# Reasoning Path:\nWayne Gretzky -> people.person.profession -> Athlete -> people.profession.specializations -> Alpine skier\n# Answer:\nAthlete", "# Reasoning Path:\nWayne Gretzky -> people.person.profession -> Athlete -> common.topic.notable_types -> Profession\n# Answer:\nAthlete"], "ground_truth": ["Edmonton Oilers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1808", "prediction": ["# Reasoning Path:\nPeru -> government.governmental_jurisdiction.governing_officials -> m.011c1dkp -> government.government_position_held.appointed_by -> Ollanta Humala\n# Answer:\nm.011c1dkp", "# Reasoning Path:\nPeru -> government.governmental_jurisdiction.governing_officials -> m.0yp_d4w -> government.government_position_held.appointed_by -> Ollanta Humala\n# Answer:\nm.0yp_d4w", "# Reasoning Path:\nPeru -> government.governmental_jurisdiction.governing_officials -> m.011c1dkp -> government.government_position_held.office_holder -> Ana Jara Vel\u00e1squez\n# Answer:\nm.011c1dkp", "# Reasoning Path:\nPeru -> government.governmental_jurisdiction.governing_officials -> m.011c1dkp -> government.government_position_held.basic_title -> Prime minister\n# Answer:\nm.011c1dkp", "# Reasoning Path:\nPeru -> government.governmental_jurisdiction.governing_officials -> m.0yp_d4w -> government.government_position_held.office_holder -> Juan Jim\u00e9nez Mayor\n# Answer:\nm.0yp_d4w", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.11b71n53cl\n# Answer:\ng.11b71n53cl", "# Reasoning Path:\nPeru -> location.statistical_region.energy_use_per_capita -> g.1245_8rt3\n# Answer:\ng.1245_8rt3", "# Reasoning Path:\nPeru -> government.governmental_jurisdiction.governing_officials -> m.0yp_d4w -> government.government_position_held.office_position_or_title -> Prime Minister of Peru\n# Answer:\nm.0yp_d4w", "# Reasoning Path:\nPeru -> government.governmental_jurisdiction.governing_officials -> m.010gf_3c -> government.government_position_held.office_holder -> Alberto Fujimori\n# Answer:\nm.010gf_3c", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.1hhc37ctc\n# Answer:\ng.1hhc37ctc"], "ground_truth": ["Ollanta Humala"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.23529411764705882, "path_precision": 0.2, "path_recall": 0.2857142857142857, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1809", "prediction": ["# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Gibson SG -> music.guitar.guitarists -> Angus Young\n# Answer:\nGibson SG", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Gibson SG -> business.product_line.producer_s -> m.0p62thw\n# Answer:\nGibson SG", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Gibson SG -> common.topic.image -> SG LPbody3\n# Answer:\nGibson SG", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Gibson Les Paul -> music.guitar.brand -> Gibson Guitar Corporation\n# Answer:\nGibson Les Paul", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Gibson SG -> music.guitar.guitarists -> Eric Clapton\n# Answer:\nGibson SG", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Fender Stratocaster -> music.guitar.guitarists -> Adrian Smith\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Fender Stratocaster -> common.topic.image -> Fender strat\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Gibson SG -> music.guitar.guitarists -> Frank Zappa\n# Answer:\nGibson SG", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Fender Stratocaster -> symbols.namesake.named_after -> Leo Fender\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Fender Stratocaster -> music.guitar.guitarists -> Aishwarya Uniyal\n# Answer:\nFender Stratocaster"], "ground_truth": ["Fender Stratocaster", "Gibson SG", "Gibson Les Paul"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-181", "prediction": ["# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yg\n# Answer:\ng.12tb6f6yg", "# Reasoning Path:\nThailand -> location.statistical_region.gdp_growth_rate -> g.11b60tqlwz\n# Answer:\ng.11b60tqlwz", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6gh2z\n# Answer:\ng.12tb6gh2z", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.1hhc37pvk\n# Answer:\ng.1hhc37pvk", "# Reasoning Path:\nThailand -> location.statistical_region.gdp_growth_rate -> g.1245_1qhj\n# Answer:\ng.1245_1qhj", "# Reasoning Path:\nThailand -> location.statistical_region.gdp_growth_rate -> g.1245_2hrc\n# Answer:\ng.1245_2hrc"], "ground_truth": ["Thai baht"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1810", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Raphael and his age\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Leonardo da Vinci and the Art of Sculpture: Inspiration and Invention\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> people.profession.people_with_this_profession -> Sandro Botticelli\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> influence.influence_node.influenced_by -> Melozzo da Forl\u00ec\n# Answer:\nAntonio da Correggio", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi -> visual_art.artwork.locations -> m.0pcqbm3\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> common.topic.subject_of -> BRS Custom Painting\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Bernardino Luini -> visual_art.visual_artist.art_forms -> Painting\n# Answer:\nBernardino Luini", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi -> visual_art.artwork.art_subject -> Biblical Magi\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Annunciation -> common.topic.notable_types -> Artwork\n# Answer:\nAnnunciation"], "ground_truth": ["Drawing", "Sculpture", "Painting"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1811", "prediction": ["# Reasoning Path:\nNew Jersey -> government.political_district.representatives -> m.0y7nq48 -> government.government_position_held.office_holder -> Cory Booker\n# Answer:\nm.0y7nq48", "# Reasoning Path:\nNew Jersey -> government.political_district.representatives -> m.05kcsw0 -> government.government_position_held.office_holder -> Bob Menendez\n# Answer:\nm.05kcsw0", "# Reasoning Path:\nNew Jersey -> government.political_district.representatives -> m.0y7nq48 -> government.government_position_held.basic_title -> Senator\n# Answer:\nm.0y7nq48", "# Reasoning Path:\nNew Jersey -> government.political_district.representatives -> m.05kcjc9 -> government.government_position_held.office_holder -> Frank Lautenberg\n# Answer:\nm.05kcjc9", "# Reasoning Path:\nNew Jersey -> government.governmental_jurisdiction.governing_officials -> m.010f1q8l -> government.government_position_held.office_holder -> Kim Guadagno\n# Answer:\nm.010f1q8l", "# Reasoning Path:\nNew Jersey -> government.governmental_jurisdiction.governing_officials -> m.025m17c -> government.government_position_held.office_holder -> Jon Corzine\n# Answer:\nm.025m17c", "# Reasoning Path:\nNew Jersey -> government.political_district.representatives -> m.05kcsw0 -> government.government_position_held.legislative_sessions -> 109th United States Congress\n# Answer:\nm.05kcsw0", "# Reasoning Path:\nNew Jersey -> government.governmental_jurisdiction.governing_officials -> m.04j5sjd -> government.government_position_held.office_holder -> Woodrow Wilson\n# Answer:\nm.04j5sjd", "# Reasoning Path:\nNew Jersey -> government.political_district.representatives -> m.05kcsw0 -> freebase.valuenotation.is_reviewed -> Appointed By (if Position is Appointed)\n# Answer:\nm.05kcsw0", "# Reasoning Path:\nNew Jersey -> government.political_district.representatives -> m.05kcsw0 -> government.government_position_held.legislative_sessions -> 110th United States Congress\n# Answer:\nm.05kcsw0"], "ground_truth": ["Bob Menendez", "Cory Booker"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1812", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.location.contains -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> base.aareas.schema.administrative_area.administrative_parent -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.country.administrative_divisions -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\ng.11bv1mttmr", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> common.topic.notable_types -> Country\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish -> base.aareas.schema.administrative_area.administrative_parent -> Barbados\n# Answer:\nSaint Michael Parish"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1813", "prediction": ["# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010h62gm -> soccer.football_goal.match -> 2014 Barcelona vs Athletic Bilbao\n# Answer:\nm.010h62gm", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010gkw62 -> soccer.football_goal.point_awarded_to -> FC Barcelona\n# Answer:\nm.010gkw62", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010h62gm -> soccer.football_goal.point_awarded_to -> FC Barcelona\n# Answer:\nm.010h62gm", "# Reasoning Path:\nLionel Messi -> soccer.football_player.matches_played -> m.0c0ltpz -> soccer.football_player_match_participation.match -> 2010 FIFA World Cup Group B - ARG ./. NGA\n# Answer:\nm.0c0ltpz", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010r25mp -> soccer.football_goal.point_awarded_to -> Argentina national football team\n# Answer:\nm.010r25mp", "# Reasoning Path:\nLionel Messi -> base.schemastaging.athlete_extra.salary -> m.0qtkxlg -> base.schemastaging.athlete_salary.team -> FC Barcelona\n# Answer:\nm.0qtkxlg", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010gkw62 -> soccer.football_goal.match -> 2014 Barcelona F.C. vs Real Madrid C.F. football match\n# Answer:\nm.010gkw62", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010r25mp -> soccer.football_goal.match -> 2014 Argentina vs Slovenia friendly match\n# Answer:\nm.010r25mp", "# Reasoning Path:\nLionel Messi -> base.schemastaging.athlete_extra.salary -> m.0110zlj9 -> base.schemastaging.athlete_salary.team -> FC Barcelona\n# Answer:\nm.0110zlj9", "# Reasoning Path:\nLionel Messi -> soccer.football_player.matches_played -> m.0c0ltpz -> soccer.football_player_match_participation.team -> Argentina national football team\n# Answer:\nm.0c0ltpz"], "ground_truth": ["Argentina national football team", "FC Barcelona"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.41379310344827586, "path_precision": 0.6, "path_recall": 0.3157894736842105, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1815", "prediction": ["# Reasoning Path:\nLil Wayne -> people.person.education -> m.0h2_zgr -> education.education.institution -> Lafayette Elementary School\n# Answer:\nm.0h2_zgr", "# Reasoning Path:\nLil Wayne -> people.person.education -> m.0h2_zpr -> education.education.institution -> University of Phoenix\n# Answer:\nm.0h2_zpr", "# Reasoning Path:\nLil Wayne -> people.person.education -> m.0h2_zpj -> education.education.institution -> University of Houston\n# Answer:\nm.0h2_zpj", "# Reasoning Path:\nLil Wayne -> music.featured_artist.albums -> 4 My Town (Play Ball) -> music.album.featured_artists -> Drake\n# Answer:\n4 My Town (Play Ball)", "# Reasoning Path:\nLil Wayne -> people.person.education -> m.0h2_zpr -> education.education.major_field_of_study -> Psychology\n# Answer:\nm.0h2_zpr", "# Reasoning Path:\nLil Wayne -> music.artist.contribution -> m.0rg70rk -> music.recording_contribution.album -> BedRock\n# Answer:\nm.0rg70rk", "# Reasoning Path:\nLil Wayne -> music.featured_artist.albums -> 4 My Town (Play Ball) -> music.composition.composer -> Baby\n# Answer:\n4 My Town (Play Ball)", "# Reasoning Path:\nLil Wayne -> music.featured_artist.albums -> 9 Piece -> music.album.release_type -> Single\n# Answer:\n9 Piece", "# Reasoning Path:\nLil Wayne -> music.featured_artist.albums -> 4 My Town (Play Ball) -> common.topic.notable_types -> Composition\n# Answer:\n4 My Town (Play Ball)", "# Reasoning Path:\nLil Wayne -> music.featured_artist.albums -> 9 Piece -> common.topic.notable_for -> g.126sqdg7v\n# Answer:\n9 Piece"], "ground_truth": ["Mcmain Magnet Secondary School", "University of Phoenix", "University of Houston", "Lafayette Elementary School", "Eleanor McMain Secondary School"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4, "path_precision": 0.3, "path_recall": 0.6, "path_ans_f1": 0.4, "path_ans_precision": 0.3, "path_ans_recall": 0.6}
{"id": "WebQTest-1817", "prediction": ["# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0n1m7cd -> education.education.institution -> University of Oxford\n# Answer:\nm.0n1m7cd", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.02wmyw7 -> education.education.institution -> Exeter College, Oxford\n# Answer:\nm.02wmyw7", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0w48bvz -> education.education.institution -> King Edward's School, Birmingham\n# Answer:\nm.0w48bvz", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0n1m7cd -> freebase.valuenotation.has_value -> End Date\n# Answer:\nm.0n1m7cd", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.02wmyw7 -> education.education.degree -> First Class Honours\n# Answer:\nm.02wmyw7", "# Reasoning Path:\nJ. R. R. Tolkien -> book.author.works_written -> A Middle English Reader and Vocabulary -> book.written_work.author -> Kenneth Sisam\n# Answer:\nA Middle English Reader and Vocabulary", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0n1m7cd -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nm.0n1m7cd", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.employment_history -> m.02ht_80 -> business.employment_tenure.company -> University of Oxford\n# Answer:\nm.02ht_80", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.employment_history -> m.02ht_8b -> business.employment_tenure.company -> University of Oxford\n# Answer:\nm.02ht_8b", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0w48bvz -> freebase.valuenotation.has_value -> End Date\n# Answer:\nm.0w48bvz"], "ground_truth": ["King Edward's School, Birmingham", "St. Philip's School", "University of Oxford", "Exeter College, Oxford"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.625, "path_precision": 0.5, "path_recall": 0.8333333333333334, "path_ans_f1": 0.6, "path_ans_precision": 0.5, "path_ans_recall": 0.75}
{"id": "WebQTest-1818", "prediction": ["# Reasoning Path:\nSaint Louis University -> location.location.containedby -> Missouri -> location.location.containedby -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nSaint Louis University -> location.location.containedby -> Missouri -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nSaint Louis University -> location.location.containedby -> Missouri -> location.location.containedby -> United States, with Territories\n# Answer:\nMissouri", "# Reasoning Path:\nSaint Louis University -> location.location.containedby -> St. Louis -> location.location.containedby -> Missouri\n# Answer:\nSt. Louis", "# Reasoning Path:\nSaint Louis University -> common.topic.article -> m.02tzb3\n# Answer:\nm.02tzb3", "# Reasoning Path:\nSaint Louis University -> location.location.containedby -> Missouri -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nSaint Louis University -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nSaint Louis University -> education.educational_institution.subsidiary_or_constituent_schools -> Saint Louis University School of Law -> location.location.containedby -> Midtown St. Louis\n# Answer:\nSaint Louis University School of Law", "# Reasoning Path:\nSaint Louis University -> education.educational_institution.subsidiary_or_constituent_schools -> Saint Louis University School of Public Health -> organization.organization.headquarters -> m.0wytzz_\n# Answer:\nSaint Louis University School of Public Health", "# Reasoning Path:\nSaint Louis University -> location.location.containedby -> St. Louis -> location.location.containedby -> Area code 314\n# Answer:\nSt. Louis"], "ground_truth": ["Missouri"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1819", "prediction": ["# Reasoning Path:\nCanada -> location.statistical_region.religions -> m.05spzv4 -> location.religion_percentage.religion -> Catholicism\n# Answer:\nm.05spzv4", "# Reasoning Path:\nCanada -> location.statistical_region.religions -> m.05spzvc -> location.religion_percentage.religion -> Islam\n# Answer:\nm.05spzvc", "# Reasoning Path:\nCanada -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nCanada -> location.statistical_region.religions -> m.05spzvl -> location.religion_percentage.religion -> Protestantism\n# Answer:\nm.05spzvl", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.12cp_k6ss\n# Answer:\ng.12cp_k6ss", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.1hhc38hnd\n# Answer:\ng.1hhc38hnd", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.1hhc3d2ty\n# Answer:\ng.1hhc3d2ty", "# Reasoning Path:\nCanada -> location.country.form_of_government -> Federal monarchy -> government.form_of_government.countries -> Australia\n# Answer:\nFederal monarchy", "# Reasoning Path:\nCanada -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Australia\n# Answer:\nParliamentary system", "# Reasoning Path:\nCanada -> location.country.form_of_government -> Federal monarchy -> government.form_of_government.countries -> German Empire\n# Answer:\nFederal monarchy"], "ground_truth": ["Catholicism", "Islam", "Protestantism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4, "path_precision": 0.3, "path_recall": 0.6, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-182", "prediction": ["# Reasoning Path:\nKhlo\u00e9 Kardashian -> people.person.parents -> Kris Jenner -> people.person.children -> Rob Kardashian\n# Answer:\nKris Jenner", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> people.person.parents -> Kris Jenner -> people.person.spouse_s -> m.04d4vvh\n# Answer:\nKris Jenner", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> people.person.parents -> Kris Jenner -> people.person.spouse_s -> m.0hyd8n8\n# Answer:\nKris Jenner", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> tv.tv_program_guest.appeared_on -> m.0j7zbj3 -> tv.tv_guest_personal_appearance.episode -> March 27th, 2009\n# Answer:\nm.0j7zbj3", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> people.person.parents -> Kris Jenner -> people.person.children -> Kendall\n# Answer:\nKris Jenner", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> people.person.parents -> Kris Jenner -> tv.tv_personality.tv_regular_appearances -> m.071g2d1\n# Answer:\nKris Jenner", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> people.person.parents -> Robert Kardashian -> people.person.gender -> Male\n# Answer:\nRobert Kardashian", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> tv.tv_program_guest.appeared_on -> m.0j7zbw4 -> tv.tv_guest_personal_appearance.episode -> Bill Paxton, The Kardashians, The Ting Tings\n# Answer:\nm.0j7zbw4", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> people.person.parents -> Kris Jenner -> people.person.children -> Kim Kardashian\n# Answer:\nKris Jenner", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> people.person.parents -> Kris Jenner -> tv.tv_personality.tv_regular_appearances -> m.0w1lmt3\n# Answer:\nKris Jenner"], "ground_truth": ["Lamar Odom"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1821", "prediction": ["# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Panama -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nPanama", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Panama -> location.location.containedby -> Latin America\n# Answer:\nPanama", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Belize -> location.location.containedby -> Latin America\n# Answer:\nBelize", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Panama -> base.locations.countries.continent -> North America\n# Answer:\nPanama", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Guatemala -> location.country.form_of_government -> Unitary state\n# Answer:\nGuatemala", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Belize -> location.country.languages_spoken -> English Language\n# Answer:\nBelize", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Panama -> location.location.containedby -> Americas\n# Answer:\nPanama", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Belize -> location.location.containedby -> Americas\n# Answer:\nBelize", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Belize -> meteorology.cyclone_affected_area.cyclones -> Hurricane Abby\n# Answer:\nBelize", "# Reasoning Path:\nSpanish Language -> base.rosetta.languoid.local_name -> Spanish -> base.rosetta.local_name.locale -> Panama\n# Answer:\nSpanish"], "ground_truth": ["Uruguay", "Ecuador", "Venezuela", "Honduras", "Panama", "Argentina", "Chile", "Equatorial Guinea", "Paraguay", "Guatemala", "Nicaragua", "Costa Rica", "Peru", "Spain", "Puerto Rico", "Colombia", "Bolivia", "Dominican Republic", "El Salvador", "Cuba"], "ans_acc": 0.1, "ans_hit": 1, "ans_f1": 0.16666666666666669, "ans_precission": 0.5, "ans_recall": 0.1, "path_f1": 0.16666666666666669, "path_precision": 0.5, "path_recall": 0.1, "path_ans_f1": 0.17142857142857143, "path_ans_precision": 0.6, "path_ans_recall": 0.1}
{"id": "WebQTest-1822", "prediction": ["# Reasoning Path:\nPaul Ryan -> people.person.parents -> Paul Murray Ryan -> people.person.nationality -> United States of America\n# Answer:\nPaul Murray Ryan", "# Reasoning Path:\nPaul Ryan -> people.person.parents -> Paul Murray Ryan -> people.deceased_person.cause_of_death -> heart attack\n# Answer:\nPaul Murray Ryan", "# Reasoning Path:\nPaul Ryan -> people.person.parents -> Elizabeth A. Ryan -> people.person.gender -> Female\n# Answer:\nElizabeth A. Ryan", "# Reasoning Path:\nPaul Ryan -> people.person.parents -> Elizabeth A. Ryan -> common.topic.notable_for -> g.126tl4m_x\n# Answer:\nElizabeth A. Ryan", "# Reasoning Path:\nPaul Ryan -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPaul Ryan -> people.person.parents -> Elizabeth A. Ryan -> people.person.spouse_s -> m.0qf0kkq\n# Answer:\nElizabeth A. Ryan", "# Reasoning Path:\nPaul Ryan -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nPaul Ryan -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPaul Ryan -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nPaul Ryan -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#domain -> Person\n# Answer:\nCountry of nationality"], "ground_truth": ["Paul Murray Ryan", "Charles Ryan", "Janna Ryan", "Elizabeth Ryan", "Elizabeth A. Ryan", "Samuel Ryan"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.30769230769230765, "path_precision": 0.5, "path_recall": 0.2222222222222222, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-1823", "prediction": ["# Reasoning Path:\nIraq -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> United States of America\n# Answer:\nFederal republic", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Federal republic -> common.topic.notable_types -> Form of Government\n# Answer:\nFederal republic", "# Reasoning Path:\nIraq -> location.statistical_region.labor_participation_rate -> g.11b71vj1zg\n# Answer:\ng.11b71vj1zg", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Lebanon\n# Answer:\nParliamentary republic", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary republic -> common.topic.notable_types -> Form of Government\n# Answer:\nParliamentary republic", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Iran\n# Answer:\nParliamentary system", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary system -> common.topic.notable_types -> Form of Government\n# Answer:\nParliamentary system", "# Reasoning Path:\nIraq -> location.statistical_region.labor_participation_rate -> g.12cp_j7p6\n# Answer:\ng.12cp_j7p6", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Turkey\n# Answer:\nParliamentary republic", "# Reasoning Path:\nIraq -> location.statistical_region.labor_participation_rate -> g.1hhc38qnx\n# Answer:\ng.1hhc38qnx"], "ground_truth": ["Federal republic", "Republic", "Parliamentary republic", "Parliamentary system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.7241379310344827, "path_precision": 0.7, "path_recall": 0.75, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1824", "prediction": ["# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Andorra\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Belgium\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> United States of America\n# Answer:\nEarth", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> France\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> France\n# Answer:\nEarth", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\ng.1hhc37x4q", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> City\n# Answer:\nEarth", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc38hhk\n# Answer:\ng.1hhc38hhk", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> Africa\n# Answer:\nEarth"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1825", "prediction": ["# Reasoning Path:\nLincoln Steffens -> book.author.works_written -> The Shame of the Cities -> book.book.editions -> The shame of the cities\n# Answer:\nThe Shame of the Cities", "# Reasoning Path:\nLincoln Steffens -> book.author.works_written -> The autobiography of Lincoln Steffens -> common.topic.notable_types -> Book\n# Answer:\nThe autobiography of Lincoln Steffens", "# Reasoning Path:\nLincoln Steffens -> common.topic.notable_for -> g.1255gcgx2\n# Answer:\ng.1255gcgx2", "# Reasoning Path:\nLincoln Steffens -> book.author.works_written -> The autobiography of Lincoln Steffens -> common.topic.notable_for -> g.1255m88ck\n# Answer:\nThe autobiography of Lincoln Steffens", "# Reasoning Path:\nLincoln Steffens -> book.author.works_written -> The Shame of the Cities -> book.written_work.original_language -> English Language\n# Answer:\nThe Shame of the Cities", "# Reasoning Path:\nLincoln Steffens -> book.author.works_written -> The Shame of the Cities -> common.topic.notable_for -> g.1257xhqcw\n# Answer:\nThe Shame of the Cities", "# Reasoning Path:\nLincoln Steffens -> book.author.works_written -> The world of Lincoln Steffens -> common.topic.notable_types -> Book\n# Answer:\nThe world of Lincoln Steffens", "# Reasoning Path:\nLincoln Steffens -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nLincoln Steffens -> book.author.works_written -> The world of Lincoln Steffens -> common.topic.notable_for -> g.1258skx01\n# Answer:\nThe world of Lincoln Steffens", "# Reasoning Path:\nLincoln Steffens -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale"], "ground_truth": ["Upbuilders", "The autobiography of Lincoln Steffens", "Die Geschichte meines Lebens", "Lincoln Steffens speaking", "Into Mexico and out", "John Reed", "The struggle for self-government", "The Shame of the Cities", "The world of Lincoln Steffens", "Boy on horseback", "The Least Of These", "The Old Jim Horse", "Moses in red"], "ans_acc": 0.23076923076923078, "ans_hit": 1, "ans_f1": 0.34710743801652894, "ans_precission": 0.7, "ans_recall": 0.23076923076923078, "path_f1": 0.34710743801652894, "path_precision": 0.7, "path_recall": 0.23076923076923078, "path_ans_f1": 0.34710743801652894, "path_ans_precision": 0.7, "path_ans_recall": 0.23076923076923078}
{"id": "WebQTest-1826", "prediction": ["# Reasoning Path:\nAnders Celsius -> people.person.education -> m.02wp1_z -> education.education.institution -> Uppsala University\n# Answer:\nm.02wp1_z", "# Reasoning Path:\nAnders Celsius -> symbols.name_source.namesakes -> Celsius -> common.topic.article -> m.04v_v4\n# Answer:\nCelsius", "# Reasoning Path:\nAnders Celsius -> symbols.name_source.namesakes -> Celsius -> common.topic.notable_types -> Extraterrestrial location\n# Answer:\nCelsius", "# Reasoning Path:\nAnders Celsius -> base.kwebbase.kwtopic.has_sentences -> Celsius read his famous paper on his thermometer: \\\"Observations  Concerning the Two Constant Degrees on a Thermometer\\\"  to the Swedish  Academy of Sciences in 1742, and it was long known as the \\\"Swedish  thermometer\\\", only around 1800 becoming known as the Celsius thermometer. -> base.kwebbase.kwsentence.previous_sentence -> The Celsius temperature scale, also called the Centigrade temperature scale because of the 100-degree interval between the defined points, is now based on 0\u00b0 for the freezing point of water and 100\u00b0 for the boiling point of water.\n# Answer:\nCelsius read his famous paper on his thermometer: \\\"Observations  Concerning the Two Constant Degrees on a Thermometer\\\"  to the Swedish  Academy of Sciences in 1742, and it was long known as the \\\"Swedish  thermometer\\\", only around 1800 becoming known as the Celsius thermometer.", "# Reasoning Path:\nAnders Celsius -> symbols.name_source.namesakes -> Degree Celsius -> common.topic.article -> m.0216d\n# Answer:\nDegree Celsius", "# Reasoning Path:\nAnders Celsius -> base.kwebbase.kwtopic.has_sentences -> He then travelled to Italy and then to Paris, where in 1735 he met the mathematician and  astronomer Maupertuis, who was preparing an expedition to measure  an arc of meridian in the far north, with the intention of verifying the  Newtonian theory that the Earth is a sphere flattened at the poles. -> base.kwebbase.kwsentence.previous_sentence -> He was the first to realize from these observations that the aurora borealis involved magnetic forces.\n# Answer:\nHe then travelled to Italy and then to Paris, where in 1735 he met the mathematician and  astronomer Maupertuis, who was preparing an expedition to measure  an arc of meridian in the far north, with the intention of verifying the  Newtonian theory that the Earth is a sphere flattened at the poles.", "# Reasoning Path:\nAnders Celsius -> symbols.name_source.namesakes -> Degree Celsius -> common.topic.notable_for -> g.125702_q5\n# Answer:\nDegree Celsius", "# Reasoning Path:\nAnders Celsius -> symbols.name_source.namesakes -> Degree Celsius -> freebase.unit_profile.dimension -> Temperature\n# Answer:\nDegree Celsius", "# Reasoning Path:\nAnders Celsius -> base.kwebbase.kwtopic.has_sentences -> He then travelled to Italy and then to Paris, where in 1735 he met the mathematician and  astronomer Maupertuis, who was preparing an expedition to measure  an arc of meridian in the far north, with the intention of verifying the  Newtonian theory that the Earth is a sphere flattened at the poles. -> base.kwebbase.kwsentence.contains_connections -> anders celsius invited on expedition by pierre-louis moreau de maupertuis\n# Answer:\nHe then travelled to Italy and then to Paris, where in 1735 he met the mathematician and  astronomer Maupertuis, who was preparing an expedition to measure  an arc of meridian in the far north, with the intention of verifying the  Newtonian theory that the Earth is a sphere flattened at the poles.", "# Reasoning Path:\nAnders Celsius -> base.kwebbase.kwtopic.has_sentences -> Celsius read his famous paper on his thermometer: \\\"Observations  Concerning the Two Constant Degrees on a Thermometer\\\"  to the Swedish  Academy of Sciences in 1742, and it was long known as the \\\"Swedish  thermometer\\\", only around 1800 becoming known as the Celsius thermometer. -> base.kwebbase.kwsentence.dates -> m.0c17h2q\n# Answer:\nCelsius read his famous paper on his thermometer: \\\"Observations  Concerning the Two Constant Degrees on a Thermometer\\\"  to the Swedish  Academy of Sciences in 1742, and it was long known as the \\\"Swedish  thermometer\\\", only around 1800 becoming known as the Celsius thermometer."], "ground_truth": ["Uppsala University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1827", "prediction": ["# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Solomon R. Guggenheim Museum -> travel.tourist_attraction.near_travel_destination -> New York\n# Answer:\nSolomon R. Guggenheim Museum", "# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Solomon R. Guggenheim Museum -> architecture.structure.architectural_style -> Modern architecture\n# Answer:\nSolomon R. Guggenheim Museum", "# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Louis Armstrong House -> travel.tourist_attraction.near_travel_destination -> Queens\n# Answer:\nLouis Armstrong House", "# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Solomon R. Guggenheim Museum -> architecture.building.building_function -> Museum\n# Answer:\nSolomon R. Guggenheim Museum", "# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Museum of Modern Art -> architecture.building.building_function -> Museum\n# Answer:\nMuseum of Modern Art", "# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Louis Armstrong House -> common.topic.notable_types -> Museum\n# Answer:\nLouis Armstrong House", "# Reasoning Path:\nNew York City -> book.book_subject.works -> A Christmas Caroline -> book.written_work.subjects -> Christmas\n# Answer:\nA Christmas Caroline", "# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Louis Armstrong House -> location.location.containedby -> New York\n# Answer:\nLouis Armstrong House", "# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Museum of Modern Art -> location.location.containedby -> New York\n# Answer:\nMuseum of Modern Art", "# Reasoning Path:\nNew York City -> visual_art.art_subject.artwork_on_the_subject -> End of 14th Street Crosstown Line -> visual_art.artwork.art_subject -> Labor unrest\n# Answer:\nEnd of 14th Street Crosstown Line"], "ground_truth": ["Gavin Brown's Enterprise", "Headless Horseman Hayrides", "Central Park Zoo", "A.I.R. Gallery", "Henry Clay Frick House", "Japan Society of New York", "International Center of Photography", "New York Aquarium", "George Gustav Heye Center", "New York Mini 10K", "Statue of Liberty", "Madison Square Garden", "UAE Healthy Kidney 10K", "Flatiron Building", "Museum of Modern Art", "Louis Armstrong House", "Chrysler Building", "Frick Collection", "New York International Fringe Festival", "Grand Central Terminal", "Solomon R. Guggenheim Museum", "Staten Island Ferry", "Felix M. Warburg House", "Brooklyn Botanic Garden", "Central Park", "United Nations Headquarters", "St. Patrick's Cathedral", "Imagination Playground at Burling Slip", "Morgan Library & Museum", "Times Square", "Metropolitan Museum of Art", "Franklin D. Roosevelt Presidential Library and Museum", "Rockefeller Center", "Tesla Science Center at Wardenclyffe", "Empire State Building", "New York Public Library for the Performing Arts", "Crocheron Park", "National Academy Museum and School", "Travefy", "Theodore Roosevelt Birthplace National Historic Site", "New York City Half Marathon", "The Cloisters", "American Museum of Natural History", "Andrew Carnegie Mansion", "Freedomland U.S.A.", "Museum of Mathematics", "Battery Park", "American Folk Art Museum", "Chelsea Art Museum", "Chinatown", "Darien Lake", "Broadway Theatre", "Little Italy", "Peking", "Wave Hill", "High Line", "Museum of Arts and Design", "Museum of Sex", "Museum of Modern Art Department of Film", "Statue of Liberty National Monument", "Brooklyn Bridge", "FusionArts Museum"], "ans_acc": 0.04838709677419355, "ans_hit": 1, "ans_f1": 0.09125475285171103, "ans_precission": 0.8, "ans_recall": 0.04838709677419355, "path_f1": 0.11764705882352941, "path_precision": 0.8, "path_recall": 0.06349206349206349, "path_ans_f1": 0.09125475285171103, "path_ans_precision": 0.8, "path_ans_recall": 0.04838709677419355}
{"id": "WebQTest-1828", "prediction": ["# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> French Polynesia\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> New Caledonia\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> Wallis and Futuna\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.11b60tv_69\n# Answer:\ng.11b60tv_69", "# Reasoning Path:\nFrance -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Monaco\n# Answer:\nEuro", "# Reasoning Path:\nFrance -> location.country.currency_used -> Euro -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nEuro", "# Reasoning Path:\nFrance -> location.statistical_region.gni_in_ppp_dollars -> g.11b60wv0lq\n# Answer:\ng.11b60wv0lq", "# Reasoning Path:\nFrance -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Belgium\n# Answer:\nEuro", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6gn9g\n# Answer:\ng.12tb6gn9g", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc37cwf\n# Answer:\ng.1hhc37cwf"], "ground_truth": ["CFP franc", "Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1829", "prediction": ["# Reasoning Path:\nMassachusetts -> location.statistical_region.religions -> m.04405bz -> location.religion_percentage.religion -> Christianity\n# Answer:\nm.04405bz", "# Reasoning Path:\nMassachusetts -> location.statistical_region.religions -> m.04404v_ -> location.religion_percentage.religion -> Jehovah's Witnesses\n# Answer:\nm.04404v_", "# Reasoning Path:\nMassachusetts -> location.statistical_region.religions -> m.04404p3 -> location.religion_percentage.religion -> Judaism\n# Answer:\nm.04404p3", "# Reasoning Path:\nMassachusetts -> location.administrative_division.capital -> m.0jvw4t4 -> location.administrative_division_capital_relationship.capital -> Boston\n# Answer:\nm.0jvw4t4", "# Reasoning Path:\nMassachusetts -> symbols.name_source.namesakes -> 4547 Massachusetts -> astronomy.orbital_relationship.orbits -> Sun\n# Answer:\n4547 Massachusetts", "# Reasoning Path:\nMassachusetts -> symbols.name_source.namesakes -> USS Massachusetts (1845) -> common.topic.notable_for -> g.125c04f_x\n# Answer:\nUSS Massachusetts (1845)", "# Reasoning Path:\nMassachusetts -> symbols.name_source.namesakes -> 4547 Massachusetts -> astronomy.celestial_object.category -> Asteroid\n# Answer:\n4547 Massachusetts", "# Reasoning Path:\nMassachusetts -> symbols.name_source.namesakes -> USS Massachusetts -> common.topic.notable_types -> Location\n# Answer:\nUSS Massachusetts", "# Reasoning Path:\nMassachusetts -> symbols.name_source.namesakes -> 4547 Massachusetts -> astronomy.astronomical_discovery.discoverer -> Kazuro Watanabe\n# Answer:\n4547 Massachusetts", "# Reasoning Path:\nMassachusetts -> symbols.name_source.namesakes -> 4547 Massachusetts -> astronomy.celestial_object.category -> Small Solar System body\n# Answer:\n4547 Massachusetts"], "ground_truth": ["Presbyterianism", "Christianity", "United Church of Christ", "Protestantism", "Churches of Christ", "Baptists", "Methodism", "Catholicism", "Pentecostalism", "Judaism", "Jehovah's Witnesses", "Lutheranism", "Episcopal Church", "Buddhism"], "ans_acc": 0.21428571428571427, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.3, "path_recall": 0.21428571428571427, "path_ans_f1": 0.25, "path_ans_precision": 0.3, "path_ans_recall": 0.21428571428571427}
{"id": "WebQTest-1830", "prediction": ["# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Belize\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Chile\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> Belize\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.statistical_region.gdp_nominal_per_capita -> g.11b60ywwvv\n# Answer:\ng.11b60ywwvv", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Cuba\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Achawa language -> language.human_language.language_family -> Macro-Arawakan languages\n# Answer:\nAchawa language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language"], "ground_truth": ["Ponares Language", "Yucuna Language", "Macaguaje Language", "Ticuna language", "Carijona Language", "Andaqui Language", "Desano Language", "Inga Language", "Coyaima Language", "Tunebo, Western Language", "Guahibo language", "Cocama language", "Macagu\u00e1n Language", "Guayabero Language", "Anserma Language", "Tomedes Language", "Romani, Vlax Language", "Hupd\u00eb Language", "Ocaina Language", "Piapoco Language", "Providencia Sign Language", "Catio language", "Quechua, Napo Lowland Language", "Natagaimas Language", "Tanimuca-Retuar\u00e3 Language", "Cagua Language", "P\u00e1ez language", "Kogi Language", "Tinigua language", "Malayo Language", "Kuna, Border Language", "Piratapuyo Language", "Puinave Language", "Baudo language", "Siona Language", "Tuyuca language", "Tunebo, Barro Negro Language", "Curripaco Language", "Waimaj\u00e3 Language", "Guambiano Language", "Cubeo Language", "Macuna Language", "Palenquero Language", "Chipiajes Language", "Guanano Language", "Totoro Language", "Spanish Language", "Tunebo, Angosturas Language", "Minica Huitoto", "Coxima Language", "Piaroa Language", "Bora Language", "Bar\u00ed Language", "Cumeral Language", "Runa Language", "Barasana Language", "Cof\u00e1n Language", "Muinane Language", "Inga, Jungle Language", "Yukpa Language", "Andoque Language", "Islander Creole English", "Murui Huitoto language", "Nukak language", "Carabayo Language", "Arhuaco Language", "Playero language", "Nheengatu language", "Tunebo, Central Language", "Koreguaje Language", "Cuiba language", "Colombian Sign Language", "Tama Language", "Tucano Language", "Nonuya language", "Wayuu Language", "Cams\u00e1 Language", "Omejes Language", "Siriano Language", "Achawa language", "Ember\u00e1, Northern Language", "Cabiyar\u00ed Language", "Pijao Language", "S\u00e1liba Language", "Awa-Cuaiquer Language", "Uwa language"], "ans_acc": 0.023255813953488372, "ans_hit": 1, "ans_f1": 0.04534005037783375, "ans_precission": 0.9, "ans_recall": 0.023255813953488372, "path_f1": 0.045283018867924525, "path_precision": 0.6, "path_recall": 0.023529411764705882, "path_ans_f1": 0.04534005037783375, "path_ans_precision": 0.9, "path_ans_recall": 0.023255813953488372}
{"id": "WebQTest-1833", "prediction": ["# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\ng.12cp_jcjy", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\ng.11b71vdgdz", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\ng.1hhc390hy", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc39sjp\n# Answer:\ng.1hhc39sjp", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fm01\n# Answer:\ng.12tb6fm01", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc385hl\n# Answer:\ng.1hhc385hl"], "ground_truth": ["Constitutional republic", "Presidential system", "Federal republic"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1835", "prediction": ["# Reasoning Path:\nGregor Mendel -> people.deceased_person.place_of_death -> Brno -> location.location.contains -> AZ Tower\n# Answer:\nBrno", "# Reasoning Path:\nGregor Mendel -> people.person.place_of_birth -> Hyn\u010dice -> location.location.containedby -> Czech Republic\n# Answer:\nHyn\u010dice", "# Reasoning Path:\nGregor Mendel -> people.person.nationality -> Austria-Hungary -> base.datedlocationtest.dated_location_test.split_off -> m.0hp5m3v\n# Answer:\nAustria-Hungary", "# Reasoning Path:\nGregor Mendel -> people.deceased_person.place_of_death -> Brno -> location.location.contains -> Brno Conservatory\n# Answer:\nBrno", "# Reasoning Path:\nGregor Mendel -> people.deceased_person.place_of_death -> Brno -> location.location.events -> 1993 Women's European Volleyball Championship\n# Answer:\nBrno", "# Reasoning Path:\nGregor Mendel -> people.person.place_of_birth -> Hyn\u010dice -> common.topic.article -> m.01h10g\n# Answer:\nHyn\u010dice", "# Reasoning Path:\nGregor Mendel -> people.deceased_person.place_of_death -> Brno -> location.location.contains -> Brno International Business School\n# Answer:\nBrno", "# Reasoning Path:\nGregor Mendel -> people.deceased_person.place_of_death -> Brno -> sports.sports_team_location.teams -> BC Brno\n# Answer:\nBrno", "# Reasoning Path:\nGregor Mendel -> people.person.place_of_birth -> Hyn\u010dice -> location.location.geolocation -> m.0wmmvdg\n# Answer:\nHyn\u010dice", "# Reasoning Path:\nGregor Mendel -> people.deceased_person.place_of_death -> Brno -> location.location.events -> 1997 Febiofest\n# Answer:\nBrno"], "ground_truth": ["Austrian Silesia", "Brno"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.6, "ans_recall": 0.5, "path_f1": 0.5454545454545454, "path_precision": 0.6, "path_recall": 0.5, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.6, "path_ans_recall": 0.5}
{"id": "WebQTest-1836", "prediction": ["# Reasoning Path:\nLaura Ingalls Wilder -> people.person.places_lived -> m.0_ghjwl -> people.place_lived.location -> Kansas\n# Answer:\nm.0_ghjwl", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.places_lived -> m.0_ghjyl -> people.place_lived.location -> De Smet\n# Answer:\nm.0_ghjyl", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.places_lived -> m.0_gghr5 -> people.place_lived.location -> Burr Oak\n# Answer:\nm.0_gghr5", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.places_lived -> m.0_ghjwl -> freebase.valuenotation.has_value -> End date\n# Answer:\nm.0_ghjwl", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> Hard Times on the Prairie -> book.written_work.subjects -> Kansas\n# Answer:\nHard Times on the Prairie", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.places_lived -> m.0_ghjwl -> freebase.valuenotation.has_value -> Start date\n# Answer:\nm.0_ghjwl", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.places_lived -> m.0_ghjyl -> freebase.valuenotation.has_value -> End date\n# Answer:\nm.0_ghjyl", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.sibling_s -> m.0_gghg0 -> people.sibling_relationship.sibling -> g.12148r4f\n# Answer:\nm.0_gghg0", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.places_lived -> m.0_ghjyl -> freebase.valuenotation.has_value -> Start date\n# Answer:\nm.0_ghjyl", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> 1998 Laura Ingalls Wilder Country Engagement Book -> common.topic.notable_types -> Book\n# Answer:\n1998 Laura Ingalls Wilder Country Engagement Book"], "ground_truth": ["Walnut Grove", "Mansfield", "Pepin", "De Smet", "Kansas", "Burr Oak"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3636363636363636, "path_precision": 0.4, "path_recall": 0.3333333333333333, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.4, "path_ans_recall": 0.5}
{"id": "WebQTest-1838", "prediction": ["# Reasoning Path:\nGermany -> sports.sports_team_location.teams -> Alba Berlin -> sports.sports_team.location -> Berlin\n# Answer:\nAlba Berlin", "# Reasoning Path:\nGermany -> sports.sports_team_location.teams -> Germany national handball team -> sports.tournament_team.tournaments_competed_in -> m.09t9_3v\n# Answer:\nGermany national handball team", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\ng.11b60v9zy7", "# Reasoning Path:\nGermany -> sports.sports_team_location.teams -> Germany national handball team -> common.topic.notable_types -> Sports Team\n# Answer:\nGermany national handball team", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_38m_\n# Answer:\ng.1245_38m_", "# Reasoning Path:\nGermany -> sports.sports_team_location.teams -> Germany men's national volleyball team -> common.topic.notable_types -> Sports Team\n# Answer:\nGermany men's national volleyball team", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fdl8\n# Answer:\ng.12tb6fdl8", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc38c_4\n# Answer:\ng.1hhc38c_4", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_67jr\n# Answer:\ng.1245_67jr", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_jlz4\n# Answer:\ng.1245_jlz4"], "ground_truth": ["Germany U20 women's volleyball team", "Germany national beach soccer team", "Germany national handball team", "QTSV Quackenbr\u00fcck", "Germany national football team", "FC Bayern Munich", "Alba Berlin", "Germany women's national field hockey team", "Germany women's national football team", "Germany national American football team", "Germany women's national basketball team", "Germany Fed Cup team", "Germany national rugby union team", "Germany women's national rugby union team", "Germany women's national ice hockey team", "BG G\u00f6ttingen", "K\u00f6ln 99ers", "Germany men's national pitch and putt team", "Germany women's national handball team", "Germany men's national inline hockey team", "Germany men's national field hockey team", "Germany men's national water polo team", "Bayer Giants Leverkusen", "Germany women's national beach handball team", "Germany national cricket team", "EWE Baskets Oldenburg", "Germany Davis Cup team", "German National Ice Hockey Team", "Germany national speedway team", "Telekom Baskets Bonn", "Germany national beach handball team", "Skyliners Frankfurt", "Brose Baskets", "Germany men's national volleyball team", "Germany national rugby league team", "Germany national baseball team", "Germany national korfball team", "Germany women's national volleyball team", "Germany women's national under-18 ice hockey team", "Germany national basketball team", "Germany national badminton team"], "ans_acc": 0.07317073170731707, "ans_hit": 1, "ans_f1": 0.12371134020618556, "ans_precission": 0.4, "ans_recall": 0.07317073170731707, "path_f1": 0.12121212121212122, "path_precision": 0.4, "path_recall": 0.07142857142857142, "path_ans_f1": 0.12371134020618556, "path_ans_precision": 0.4, "path_ans_recall": 0.07317073170731707}
{"id": "WebQTest-1839", "prediction": ["# Reasoning Path:\nOklahoma -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nOklahoma -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nOklahoma -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nOklahoma -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nOklahoma -> common.topic.notable_types -> US State -> freebase.type_profile.strict_included_types -> Administrative Division\n# Answer:\nUS State", "# Reasoning Path:\nOklahoma -> common.topic.notable_types -> US State -> freebase.type_profile.equivalent_topic -> U.S. state\n# Answer:\nUS State", "# Reasoning Path:\nOklahoma -> common.topic.notable_types -> US State -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nUS State", "# Reasoning Path:\nOklahoma -> common.topic.notable_types -> US State -> type.type.expected_by -> Indiana\n# Answer:\nUS State", "# Reasoning Path:\nOklahoma -> government.governmental_jurisdiction.governing_officials -> m.0100vzfb -> government.government_position_held.office_holder -> John R. Bennett\n# Answer:\nm.0100vzfb", "# Reasoning Path:\nOklahoma -> common.topic.notable_types -> US State -> type.type.expected_by -> State\n# Answer:\nUS State"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-184", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.places_lived -> m.03pvr9m -> people.place_lived.location -> Massachusetts\n# Answer:\nm.03pvr9m", "# Reasoning Path:\nMitt Romney -> people.person.places_lived -> m.0123x8q9 -> people.place_lived.location -> Park City\n# Answer:\nm.0123x8q9", "# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Michigan\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> people.person.places_lived -> m.03pqrn3 -> people.place_lived.location -> Bloomfield Hills\n# Answer:\nm.03pqrn3", "# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Michigan\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> people.person.place_of_birth -> Detroit -> base.biblioness.bibs_location.state -> Michigan\n# Answer:\nDetroit", "# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> people.person.place_of_birth -> Detroit -> location.location.containedby -> Michigan\n# Answer:\nDetroit", "# Reasoning Path:\nMitt Romney -> people.person.place_of_birth -> Detroit -> location.location.containedby -> Area code 313\n# Answer:\nDetroit"], "ground_truth": ["Massachusetts"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1840", "prediction": ["# Reasoning Path:\nNorway -> location.country.official_language -> Norwegian Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nNorwegian Language", "# Reasoning Path:\nNorway -> location.country.official_language -> Nynorsk -> language.human_language.language_family -> Indo-European languages\n# Answer:\nNynorsk", "# Reasoning Path:\nNorway -> location.country.official_language -> Nynorsk -> language.human_language.writing_system -> Danish and Norwegian alphabet\n# Answer:\nNynorsk", "# Reasoning Path:\nNorway -> location.country.official_language -> Norwegian Language -> common.topic.image -> Norwegianmalforms\n# Answer:\nNorwegian Language", "# Reasoning Path:\nNorway -> location.statistical_region.net_migration -> g.1q5jhf535\n# Answer:\ng.1q5jhf535", "# Reasoning Path:\nNorway -> location.country.official_language -> Norwegian Language -> common.topic.notable_types -> Human Language\n# Answer:\nNorwegian Language", "# Reasoning Path:\nNorway -> location.country.official_language -> Bokm\u00e5l -> language.human_language.writing_system -> Danish and Norwegian alphabet\n# Answer:\nBokm\u00e5l", "# Reasoning Path:\nNorway -> location.country.official_language -> Bokm\u00e5l -> common.topic.notable_types -> Human Language\n# Answer:\nBokm\u00e5l", "# Reasoning Path:\nNorway -> location.country.languages_spoken -> Saami, North Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nSaami, North Language", "# Reasoning Path:\nNorway -> location.country.official_language -> Nynorsk -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nNynorsk"], "ground_truth": ["Norwegian Language", "Nynorsk", "Saami, Lule Language", "Saami, North Language", "Finnish, Kven Language", "Bokm\u00e5l", "Saami, South Language"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.6990291262135921, "ans_precission": 0.9, "ans_recall": 0.5714285714285714, "path_f1": 0.5, "path_precision": 0.6, "path_recall": 0.42857142857142855, "path_ans_f1": 0.6990291262135921, "path_ans_precision": 0.9, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-1842", "prediction": ["# Reasoning Path:\nSingapore -> location.country.currency_used -> Singapore dollar -> common.topic.notable_types -> Currency\n# Answer:\nSingapore dollar", "# Reasoning Path:\nSingapore -> location.country.currency_used -> Singapore dollar -> common.topic.article -> m.02f32r\n# Answer:\nSingapore dollar", "# Reasoning Path:\nSingapore -> location.statistical_region.deposit_interest_rate -> g.11b71rkqjy\n# Answer:\ng.11b71rkqjy", "# Reasoning Path:\nSingapore -> location.statistical_region.deposit_interest_rate -> g.12cp_j7mk\n# Answer:\ng.12cp_j7mk", "# Reasoning Path:\nSingapore -> location.statistical_region.deposit_interest_rate -> g.12tb6f6tz\n# Answer:\ng.12tb6f6tz", "# Reasoning Path:\nSingapore -> olympics.olympic_participating_country.athletes -> m.0k8qc9m -> olympics.olympic_athlete_affiliation.sport -> Sailing\n# Answer:\nm.0k8qc9m", "# Reasoning Path:\nSingapore -> olympics.olympic_participating_country.athletes -> m.0k8qcb1 -> olympics.olympic_athlete_affiliation.sport -> Sailing\n# Answer:\nm.0k8qcb1", "# Reasoning Path:\nSingapore -> olympics.olympic_participating_country.athletes -> m.0k8qc9m -> olympics.olympic_athlete_affiliation.olympics -> The London 2012 Summer Olympics\n# Answer:\nm.0k8qc9m", "# Reasoning Path:\nSingapore -> olympics.olympic_participating_country.athletes -> m.0k8qcbk -> olympics.olympic_athlete_affiliation.olympics -> The London 2012 Summer Olympics\n# Answer:\nm.0k8qcbk", "# Reasoning Path:\nSingapore -> olympics.olympic_participating_country.athletes -> m.0k8qcb1 -> olympics.olympic_athlete_affiliation.athlete -> Colin Cheng Xin Ru\n# Answer:\nm.0k8qcb1"], "ground_truth": ["Singapore dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1843", "prediction": ["# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> Blackfoot Language -> language.human_language.main_country -> Canada\n# Answer:\nBlackfoot Language", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> Blackfoot Language -> common.topic.image -> Traditional territory in green, reserves and reservations in orange.\n# Answer:\nBlackfoot Language", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> English Language -> language.human_language.dialects -> Scottish English\n# Answer:\nEnglish Language", "# Reasoning Path:\nBlackfoot Confederacy -> common.topic.notable_for -> g.125bpgfw7\n# Answer:\ng.125bpgfw7", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> Blackfoot Language -> common.topic.notable_for -> g.12585j6s4\n# Answer:\nBlackfoot Language", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> English Language -> language.human_language.dialects -> African American Vernacular English\n# Answer:\nEnglish Language", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Hong Kong\n# Answer:\nEnglish Language", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> English Language -> language.human_language.dialects -> American English\n# Answer:\nEnglish Language", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> English Language -> education.school_category.schools_of_this_kind -> Piers Midwinter\n# Answer:\nEnglish Language", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda\n# Answer:\nEnglish Language"], "ground_truth": ["Blackfoot Language", "English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1844", "prediction": ["# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.produced_by -> Kevin Costner -> people.person.children -> Annie Costner\n# Answer:\nKevin Costner", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.produced_by -> Kevin Costner -> people.person.children -> Cayden Wyatt Costner\n# Answer:\nKevin Costner", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.produced_by -> Kevin Costner -> film.producer.film -> Black or White\n# Answer:\nKevin Costner", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.starring -> m.0k2m4s -> film.performance.actor -> Kevin Costner\n# Answer:\nm.0k2m4s", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.produced_by -> Kevin Costner -> people.person.children -> Grace Avery Costner\n# Answer:\nKevin Costner", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.produced_by -> Kevin Costner -> common.topic.webpage -> m.07fnkx1\n# Answer:\nKevin Costner", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.produced_by -> Kevin Costner -> film.producer.film -> Dances with Wolves\n# Answer:\nKevin Costner", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.produced_by -> John Watson -> award.award_nominee.award_nominations -> m.0rfsmd4\n# Answer:\nJohn Watson", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.starring -> m.02tb0pq -> film.performance.actor -> Christian Slater\n# Answer:\nm.02tb0pq", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.produced_by -> Kevin Costner -> film.producer.film -> Hatfields & McCoys\n# Answer:\nKevin Costner"], "ground_truth": ["Kevin Costner"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1845", "prediction": ["# Reasoning Path:\nTexas -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nTexas -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nTexas -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nTexas -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nTexas -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nTexas -> location.location.time_zones -> Central Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nTexas -> government.political_district.representatives -> m.04j5vd_ -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nm.04j5vd_", "# Reasoning Path:\nTexas -> government.political_district.representatives -> m.04j5vd_ -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nm.04j5vd_", "# Reasoning Path:\nTexas -> government.political_district.representatives -> m.052p616 -> government.government_position_held.office_holder -> William Neff Patman\n# Answer:\nm.052p616", "# Reasoning Path:\nTexas -> government.political_district.representatives -> m.04j5vd_ -> government.government_position_held.office_holder -> Lyndon B. Johnson\n# Answer:\nm.04j5vd_"], "ground_truth": ["Central Time Zone", "Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1847", "prediction": ["# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> location.location.partially_contains -> Alazani\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> base.locations.continents.countries_within -> Iraq\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> location.location.partially_contains -> Aragvi River\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> location.location.partiallycontains -> m.010r1xzh\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> location.location.partially_contains -> Armenia\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> base.locations.continents.countries_within -> Syria\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> location.location.partiallycontains -> m.0113v79q\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> location.location.partiallycontains -> m.0w_08b5\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> common.topic.image -> 16 Kurdish people -> common.image.size -> m.03sgkt4\n# Answer:\n16 Kurdish people", "# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> base.locations.continents.countries_within -> Afghanistan\n# Answer:\nAsia"], "ground_truth": ["Asia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1848", "prediction": ["# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.instrumentalists -> Bix Beiderbecke\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> common.topic.notable_types -> Musical instrument\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> music.instrument.family -> Brass instrument\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.family -> Brass instrument\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.instrumentalists -> James Morrison\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.instrumentalists -> King Oliver\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> book.written_work.subjects -> Trumpeter\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> common.topic.notable_types -> Book\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.featured_artist.recordings -> Boog It -> common.topic.notable_types -> Musical Recording\n# Answer:\nBoog It", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> common.topic.notable_types -> Musical instrument\n# Answer:\nTrumpet"], "ground_truth": ["Trumpet", "Cornet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1849", "prediction": ["# Reasoning Path:\nAlyson Stoner -> film.actor.film -> m.010r560c -> film.performance.character -> Camille Gage\n# Answer:\nm.010r560c", "# Reasoning Path:\nCamp Rock -> film.film.starring -> m.03jqmzg -> film.performance.character -> Caitlyn Geller\n# Answer:\nm.03jqmzg", "# Reasoning Path:\nAlyson Stoner -> film.actor.film -> m.011shk2t -> film.performance.character -> Strawberry\n# Answer:\nm.011shk2t", "# Reasoning Path:\nAlyson Stoner -> film.actor.film -> m.03jq_g6 -> film.performance.character -> Kid Rat\n# Answer:\nm.03jq_g6", "# Reasoning Path:\nAlyson Stoner -> film.actor.film -> m.010r560c -> film.performance.film -> Step Up: All In\n# Answer:\nm.010r560c", "# Reasoning Path:\nCamp Rock -> film.film.starring -> m.04dc588 -> film.performance.character -> g.122tc1ml\n# Answer:\nm.04dc588", "# Reasoning Path:\nCamp Rock -> film.film.cinematography -> Dean Cundey -> film.actor.film -> m.0vygckj\n# Answer:\nDean Cundey", "# Reasoning Path:\nCamp Rock -> film.film.starring -> m.064ylll -> film.performance.character -> g.11dyj3mky\n# Answer:\nm.064ylll", "# Reasoning Path:\nCamp Rock -> film.film.starring -> m.03jqmzg -> film.performance.actor -> Alyson Stoner\n# Answer:\nm.03jqmzg", "# Reasoning Path:\nAlyson Stoner -> film.actor.film -> m.011shk2t -> film.performance.film -> Super Buddies\n# Answer:\nm.011shk2t"], "ground_truth": ["Caitlyn Geller"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-185", "prediction": ["# Reasoning Path:\nFrance -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Algeria\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nFrance -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Andorra\n# Answer:\nUnitary state", "# Reasoning Path:\nFrance -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Madagascar\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.11b60tv_69\n# Answer:\ng.11b60tv_69", "# Reasoning Path:\nFrance -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Angola\n# Answer:\nUnitary state", "# Reasoning Path:\nFrance -> location.statistical_region.gni_in_ppp_dollars -> g.11b60wv0lq\n# Answer:\ng.11b60wv0lq", "# Reasoning Path:\nFrance -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Italy\n# Answer:\nUnitary state", "# Reasoning Path:\nFrance -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Germany\n# Answer:\nConstitutional republic", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6gn9g\n# Answer:\ng.12tb6gn9g", "# Reasoning Path:\nFrance -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Greece\n# Answer:\nConstitutional republic"], "ground_truth": ["Constitutional republic", "Semi-presidential system", "Unitary state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1850", "prediction": ["# Reasoning Path:\nMadonna -> people.person.religion -> Jewish Renewal\n# Answer:\nJewish Renewal", "# Reasoning Path:\nMadonna -> people.person.religion -> Catholicism -> media_common.quotation_subject.quotations_about_this_subject -> Catholicism is not a soothing religion. It's a painful religion. We're all gluttons for punishment.\n# Answer:\nCatholicism", "# Reasoning Path:\nMadonna -> base.popstra.celebrity.dated -> m.064_jxy -> base.popstra.dated.participant -> Chris Rock\n# Answer:\nm.064_jxy", "# Reasoning Path:\nMadonna -> broadcast.artist.content -> .977 The '80s Channel -> common.topic.image -> x.jpg\n# Answer:\n.977 The '80s Channel", "# Reasoning Path:\nMadonna -> base.popstra.celebrity.dated -> m.064f3ln -> base.popstra.dated.participant -> Dennis Rodman\n# Answer:\nm.064f3ln", "# Reasoning Path:\nMadonna -> base.popstra.celebrity.dated -> m.064t3bj -> base.popstra.dated.participant -> Alex Rodriguez\n# Answer:\nm.064t3bj", "# Reasoning Path:\nMadonna -> broadcast.artist.content -> .977 The '80s Channel -> broadcast.content.producer -> 977 Music\n# Answer:\n.977 The '80s Channel", "# Reasoning Path:\nMadonna -> broadcast.artist.content -> .977 The Hits Channel -> broadcast.content.artist -> Baby Bash\n# Answer:\n.977 The Hits Channel", "# Reasoning Path:\nMadonna -> broadcast.artist.content -> 1.FM Absolute  90's -> common.topic.notable_types -> Broadcast Content\n# Answer:\n1.FM Absolute  90's", "# Reasoning Path:\nMadonna -> broadcast.artist.content -> .977 The Hits Channel -> broadcast.content.artist -> Beyonc\u00e9 Knowles\n# Answer:\n.977 The Hits Channel"], "ground_truth": ["Catholicism", "Jewish Renewal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1851", "prediction": ["# Reasoning Path:\nEgypt -> location.location.partially_containedby -> North Africa -> location.location.partially_contains -> Arab world\n# Answer:\nNorth Africa", "# Reasoning Path:\nEgypt -> location.location.partially_containedby -> North Africa -> location.location.partially_contains -> Middle East\n# Answer:\nNorth Africa", "# Reasoning Path:\nEgypt -> location.location.partially_containedby -> North Africa -> location.location.contains -> Sahara\n# Answer:\nNorth Africa", "# Reasoning Path:\nEgypt -> location.location.partially_containedby -> Western Asia -> location.location.events -> Middle East Theatre of World War II\n# Answer:\nWestern Asia", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Israel\n# Answer:\nArabic Language", "# Reasoning Path:\nEgypt -> location.location.partially_containedby -> North Africa -> location.location.contains -> Sudan\n# Answer:\nNorth Africa", "# Reasoning Path:\nEgypt -> location.location.partially_containedby -> Western Asia -> location.location.partially_contains -> Arab world\n# Answer:\nWestern Asia", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.writing_system -> Arabic alphabet\n# Answer:\nArabic Language", "# Reasoning Path:\nEgypt -> location.location.partially_containedby -> Western Asia -> location.location.containedby -> Asia\n# Answer:\nWestern Asia", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.dialects -> Arabic, Sudanese Spoken Language\n# Answer:\nArabic Language"], "ground_truth": ["Libya", "Sudan", "Israel"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.038461538461538464, "path_precision": 0.1, "path_recall": 0.023809523809523808, "path_ans_f1": 0.41379310344827586, "path_ans_precision": 0.3, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1853", "prediction": ["# Reasoning Path:\nRichard Hammond -> people.person.profession -> Presenter -> base.descriptive_names.names.descriptive_name -> m.010543ty\n# Answer:\nPresenter", "# Reasoning Path:\nRichard Hammond -> tv.tv_personality.tv_regular_appearances -> g.11bv36x_sm\n# Answer:\ng.11bv36x_sm", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Presenter -> base.descriptive_names.names.descriptive_name -> m.01064h0m\n# Answer:\nPresenter", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Presenter -> fictional_universe.character_occupation.characters_with_this_occupation -> Harmony Kendall\n# Answer:\nPresenter", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Television producer -> common.topic.webpage -> m.09w3s7h\n# Answer:\nTelevision producer", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Presenter -> base.descriptive_names.names.descriptive_name -> m.01066x64\n# Answer:\nPresenter", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Presenter -> people.profession.specializations -> Announcer\n# Answer:\nPresenter", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Screenwriter -> common.topic.webpage -> m.09w75x2\n# Answer:\nScreenwriter", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Presenter -> fictional_universe.character_occupation.characters_with_this_occupation -> Jackie Burkhart\n# Answer:\nPresenter", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Television producer -> common.topic.webpage -> m.09w4k2z\n# Answer:\nTelevision producer"], "ground_truth": ["Journalist", "Presenter", "Screenwriter", "Voice Actor", "Stunt Performer", "Writer", "Author", "Television producer", "Radio personality"], "ans_acc": 0.4444444444444444, "ans_hit": 1, "ans_f1": 0.5950413223140495, "ans_precission": 0.9, "ans_recall": 0.4444444444444444, "path_f1": 0.48648648648648646, "path_precision": 0.9, "path_recall": 0.3333333333333333, "path_ans_f1": 0.5950413223140495, "path_ans_precision": 0.9, "path_ans_recall": 0.4444444444444444}
{"id": "WebQTest-1854", "prediction": ["# Reasoning Path:\nLois Griffin -> film.film_character.portrayed_in_films -> m.0k3ssk -> film.performance.actor -> Alex Borstein\n# Answer:\nm.0k3ssk", "# Reasoning Path:\nLois Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntr0s -> tv.regular_tv_appearance.actor -> Alex Borstein\n# Answer:\nm.02ntr0s", "# Reasoning Path:\nLois Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntr0s -> tv.regular_tv_appearance.series -> Family Guy\n# Answer:\nm.02ntr0s", "# Reasoning Path:\nLois Griffin -> tv.tv_character.appeared_in_tv_episodes -> g.11byb56_yy\n# Answer:\ng.11byb56_yy", "# Reasoning Path:\nLois Griffin -> film.film_character.portrayed_in_films -> m.0h28yx8 -> film.performance.actor -> Alex Borstein\n# Answer:\nm.0h28yx8", "# Reasoning Path:\nLois Griffin -> film.film_character.portrayed_in_films -> m.0k3ssk -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0k3ssk", "# Reasoning Path:\nLois Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntr0s -> tv.regular_tv_appearance.seasons -> Family Guy - Season 1\n# Answer:\nm.02ntr0s", "# Reasoning Path:\nLois Griffin -> film.film_character.portrayed_in_films -> m.010x33y4 -> film.performance.actor -> Alex Borstein\n# Answer:\nm.010x33y4", "# Reasoning Path:\nLois Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntr0s -> tv.regular_tv_appearance.seasons -> Family Guy - Season 10\n# Answer:\nm.02ntr0s", "# Reasoning Path:\nLois Griffin -> film.film_character.portrayed_in_films -> m.0h28yx8 -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0h28yx8"], "ground_truth": ["Alex Borstein"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1855", "prediction": ["# Reasoning Path:\nParis -> location.location.nearby_airports -> Charles de Gaulle Airport -> travel.transport_terminus.travel_destinations_served -> m.051qtw8\n# Answer:\nCharles de Gaulle Airport", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Beauvais\u2013Till\u00e9 Airport -> common.topic.notable_types -> Transport terminus\n# Answer:\nBeauvais\u2013Till\u00e9 Airport", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Charles de Gaulle Airport -> common.topic.notable_types -> Airport\n# Answer:\nCharles de Gaulle Airport", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Paris Orly Airport -> travel.transport_terminus.travel_destinations_served -> m.051qtw8\n# Answer:\nParis Orly Airport", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Charles de Gaulle Airport -> aviation.airport.serves -> Le Royal Monceau Raffles Paris\n# Answer:\nCharles de Gaulle Airport", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Beauvais\u2013Till\u00e9 Airport -> common.topic.article -> m.03tjk9\n# Answer:\nBeauvais\u2013Till\u00e9 Airport", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Beauvais\u2013Till\u00e9 Airport -> common.topic.notable_for -> g.1256610zt\n# Answer:\nBeauvais\u2013Till\u00e9 Airport", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Charles de Gaulle Airport -> aviation.airport.serves -> Adventures by Disney - England and France Vacation\n# Answer:\nCharles de Gaulle Airport", "# Reasoning Path:\nParis -> common.topic.webpage -> m.02k8dbw -> common.webpage.category -> Topic Webpage\n# Answer:\nm.02k8dbw", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Paris Orly Airport -> film.film_location.featured_in_films -> La Jet\u00e9e\n# Answer:\nParis Orly Airport"], "ground_truth": ["Paris Orly Airport", "Charles de Gaulle Airport"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1857", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.album -> g.121_5zdm\n# Answer:\ng.121_5zdm", "# Reasoning Path:\nRihanna -> music.artist.album -> Loud -> music.album.album_content_type -> Studio album\n# Answer:\nLoud", "# Reasoning Path:\nRihanna -> music.artist.album -> Unapologetic -> music.album.release_type -> Album\n# Answer:\nUnapologetic", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\ng.11bv1mttmr", "# Reasoning Path:\nRihanna -> music.artist.album -> Loud -> music.album.genre -> Contemporary R&B\n# Answer:\nLoud", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\ng.11bv383dbd", "# Reasoning Path:\nRihanna -> music.artist.album -> Loud -> music.album.genre -> Dance-pop\n# Answer:\nLoud", "# Reasoning Path:\nRihanna -> music.artist.album -> Unapologetic -> music.album.genre -> Contemporary R&B\n# Answer:\nUnapologetic", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.063y0bl -> base.popstra.infidelity.victim -> Beyonc\u00e9 Knowles\n# Answer:\nm.063y0bl", "# Reasoning Path:\nRihanna -> music.artist.album -> Loud -> music.album.genre -> Pop music\n# Answer:\nLoud"], "ground_truth": ["Birthday Cake", "Devil in a Blue Dress", "Jump (Club Cheval Rap Remix)", "Cockiness (Love It) Remix", "Unapologetic", "Diamonds", "Rockstar 101: The Remixes", "Diamonds (remix)", "g.121xjjw_", "Talk That Talk", "Loud Tour Live at the O2", "Towards the Sun", "g.121n8wcw", "g.121_5zdm"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0.19354838709677416, "ans_precission": 0.3, "ans_recall": 0.14285714285714285, "path_f1": 0.2033898305084746, "path_precision": 0.3, "path_recall": 0.15384615384615385, "path_ans_f1": 0.19354838709677416, "path_ans_precision": 0.3, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-1858", "prediction": ["# Reasoning Path:\nDave Ramsey -> people.person.place_of_birth -> Antioch -> location.location.people_born_here -> Casey Atwood\n# Answer:\nAntioch", "# Reasoning Path:\nDave Ramsey -> people.person.places_lived -> m.03phzfq -> people.place_lived.location -> Nashville\n# Answer:\nm.03phzfq", "# Reasoning Path:\nDave Ramsey -> people.person.place_of_birth -> Antioch -> common.topic.article -> m.03gf9f\n# Answer:\nAntioch", "# Reasoning Path:\nDave Ramsey -> people.person.place_of_birth -> Antioch -> location.location.people_born_here -> Jim Turner\n# Answer:\nAntioch", "# Reasoning Path:\nDave Ramsey -> people.person.place_of_birth -> Antioch -> location.citytown.postal_codes -> 37011\n# Answer:\nAntioch", "# Reasoning Path:\nDave Ramsey -> people.person.place_of_birth -> Antioch -> location.location.people_born_here -> Keenan Reynolds\n# Answer:\nAntioch", "# Reasoning Path:\nDave Ramsey -> people.person.place_of_birth -> Antioch -> location.citytown.postal_codes -> 37013\n# Answer:\nAntioch", "# Reasoning Path:\nDave Ramsey -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nDave Ramsey -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nDave Ramsey -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nMale"], "ground_truth": ["Nashville"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-186", "prediction": ["# Reasoning Path:\nDenmark -> royalty.kingdom.rulers -> Margrethe II of Denmark -> people.family_member.family -> House of Gl\u00fccksburg\n# Answer:\nMargrethe II of Denmark", "# Reasoning Path:\nDenmark -> royalty.kingdom.rulers -> Margrethe II of Denmark -> people.person.parents -> Frederick IX of Denmark\n# Answer:\nMargrethe II of Denmark", "# Reasoning Path:\nDenmark -> royalty.kingdom.rulers -> Margrethe II of Denmark -> government.political_appointer.appointees -> m.0h5z38_\n# Answer:\nMargrethe II of Denmark", "# Reasoning Path:\nDenmark -> periodicals.newspaper_circulation_area.newspapers -> Dagbladet B\u00f8rsen -> book.newspaper.circulation_areas -> Copenhagen\n# Answer:\nDagbladet B\u00f8rsen", "# Reasoning Path:\nDenmark -> location.statistical_region.gender_balance_members_of_parliament -> g.11b71wx6lz\n# Answer:\ng.11b71wx6lz", "# Reasoning Path:\nDenmark -> location.statistical_region.gender_balance_members_of_parliament -> g.1hhc38d4d\n# Answer:\ng.1hhc38d4d", "# Reasoning Path:\nDenmark -> royalty.kingdom.rulers -> Abel, King of Denmark -> people.person.religion -> Catholicism\n# Answer:\nAbel, King of Denmark", "# Reasoning Path:\nDenmark -> periodicals.newspaper_circulation_area.newspapers -> Dagbladet B\u00f8rsen -> book.periodical.language -> Danish Language\n# Answer:\nDagbladet B\u00f8rsen", "# Reasoning Path:\nDenmark -> location.statistical_region.gender_balance_members_of_parliament -> g.1hhc3gt9z\n# Answer:\ng.1hhc3gt9z", "# Reasoning Path:\nDenmark -> royalty.kingdom.rulers -> Abel, King of Denmark -> common.topic.notable_types -> Monarch\n# Answer:\nAbel, King of Denmark"], "ground_truth": ["Margrethe II of Denmark", "Helle Thorning-Schmidt", "Margrethe Vestager"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.3157894736842105, "ans_precission": 0.3, "ans_recall": 0.3333333333333333, "path_f1": 0.24, "path_precision": 0.3, "path_recall": 0.2, "path_ans_f1": 0.3157894736842105, "path_ans_precision": 0.3, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1860", "prediction": ["# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Io -> common.topic.notable_types -> Moon\n# Answer:\nIo", "# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Ganymede -> common.topic.image -> Galilean moon Laplace resonance animation\n# Answer:\nGanymede", "# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Europa -> fictional_universe.fictional_character.gender -> Female\n# Answer:\nEuropa", "# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Ganymede -> common.topic.image -> Ganymede g1 true 2\n# Answer:\nGanymede", "# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Ganymede -> astronomy.star_system_body.star_system -> Solar System\n# Answer:\nGanymede", "# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Io -> astronomy.celestial_object.locations -> Ah Peku Patera\n# Answer:\nIo", "# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Ganymede -> common.topic.notable_types -> Moon\n# Answer:\nGanymede", "# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Europa -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Europa (Satellite)\n# Answer:\nEuropa", "# Reasoning Path:\nGalileo Galilei -> book.author.book_editions_published -> Dialogue Concerning the Two Chief World Systems - Potolemaic & Copernican -> common.topic.notable_for -> g.125dtpk3l\n# Answer:\nDialogue Concerning the Two Chief World Systems - Potolemaic & Copernican", "# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Europa -> common.topic.notable_types -> Moon\n# Answer:\nEuropa"], "ground_truth": ["Callisto", "Ganymede", "Io", "Europa"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.8181818181818182, "path_precision": 0.9, "path_recall": 0.75, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-1861", "prediction": ["# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Absolute monarchy -> government.form_of_government.countries -> Brunei\n# Answer:\nAbsolute monarchy", "# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Absolute monarchy -> common.topic.notable_types -> Form of Government\n# Answer:\nAbsolute monarchy", "# Reasoning Path:\nSaudi Arabia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60xj9xc\n# Answer:\ng.11b60xj9xc", "# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Absolute monarchy -> government.form_of_government.countries -> Oman\n# Answer:\nAbsolute monarchy", "# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Brunei\n# Answer:\nUnitary state", "# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Unitary state -> common.topic.notable_types -> Form of Government\n# Answer:\nUnitary state", "# Reasoning Path:\nSaudi Arabia -> location.statistical_region.net_migration -> g.1q5jfl07n\n# Answer:\ng.1q5jfl07n", "# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Absolute monarchy -> government.form_of_government.countries -> Ottoman Empire\n# Answer:\nAbsolute monarchy", "# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Islamic state -> government.form_of_government.countries -> Brunei\n# Answer:\nIslamic state", "# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Islamic state -> common.topic.notable_types -> Form of Government\n# Answer:\nIslamic state"], "ground_truth": ["Islamic state", "Absolute monarchy", "Unitary state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1864", "prediction": ["# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> tv.tv_character.appeared_in_tv_program -> m.03jrblb -> tv.regular_tv_appearance.actor -> Michael Weatherly\n# Answer:\nm.03jrblb", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> tv.tv_character.appeared_in_tv_program -> m.09l2mf7 -> tv.regular_tv_appearance.actor -> Michael Weatherly\n# Answer:\nm.09l2mf7", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> tv.tv_character.appeared_in_tv_program -> m.03jrblb -> tv.regular_tv_appearance.seasons -> NCIS - Season 1\n# Answer:\nm.03jrblb", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> common.topic.notable_types -> TV Character -> type.type.properties -> Appeared In TV Episode Segments\n# Answer:\nTV Character", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> common.topic.notable_types -> TV Character -> type.type.expected_by -> Character\n# Answer:\nTV Character", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> fictional_universe.fictional_character.occupation -> Special Agent -> fictional_universe.character_occupation.characters_with_this_occupation -> Agent Pleakley\n# Answer:\nSpecial Agent", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> fictional_universe.fictional_character.occupation -> Special Agent -> common.topic.article -> m.02lml8\n# Answer:\nSpecial Agent", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> tv.tv_character.appeared_in_tv_program -> m.03jrblb -> tv.regular_tv_appearance.seasons -> NCIS - Season 10\n# Answer:\nm.03jrblb", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> common.topic.notable_types -> TV Character -> type.type.properties -> Appeared In TV Episodes\n# Answer:\nTV Character", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> fictional_universe.fictional_character.occupation -> Special Agent -> people.profession.specialization_of -> Law enforcement officer\n# Answer:\nSpecial Agent"], "ground_truth": ["Michael Weatherly"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1865", "prediction": ["# Reasoning Path:\nJesus Christ -> people.person.profession -> Prophet -> base.descriptive_names.names.descriptive_name -> m.01064z57\n# Answer:\nProphet", "# Reasoning Path:\nJesus Christ -> people.person.profession -> Prophet -> book.book_subject.works -> Prophets of doom\n# Answer:\nProphet", "# Reasoning Path:\nJesus Christ -> people.person.profession -> Preacher -> common.topic.image -> Pro\n# Answer:\nPreacher", "# Reasoning Path:\nJesus Christ -> common.topic.notable_for -> g.12551f1xr\n# Answer:\ng.12551f1xr", "# Reasoning Path:\nJesus Christ -> people.person.profession -> Prophet -> base.descriptive_names.names.descriptive_name -> m.0108ccyw\n# Answer:\nProphet", "# Reasoning Path:\nJesus Christ -> people.person.profession -> Prophet -> people.profession.people_with_this_profession -> Aaron\n# Answer:\nProphet", "# Reasoning Path:\nJesus Christ -> people.person.profession -> Carpentry -> fictional_universe.character_occupation.characters_with_this_occupation -> Jesus - The Christ\n# Answer:\nCarpentry", "# Reasoning Path:\nJesus Christ -> people.person.profession -> Carpentry -> common.topic.notable_types -> Profession\n# Answer:\nCarpentry", "# Reasoning Path:\nJesus Christ -> people.person.profession -> Prophet -> base.descriptive_names.names.descriptive_name -> m.0108cd98\n# Answer:\nProphet", "# Reasoning Path:\nJesus Christ -> common.topic.notable_for -> g.1255sm1pr\n# Answer:\ng.1255sm1pr"], "ground_truth": ["Preacher", "Carpentry", "Prophet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1866", "prediction": ["# Reasoning Path:\nNew York City -> base.aareas.schema.administrative_area.administrative_parent -> New York -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> base.aareas.schema.administrative_area.administrative_parent -> New York -> location.location.containedby -> United States of America\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> base.biblioness.bibs_location.state -> New York -> location.location.containedby -> United States of America\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> base.aareas.schema.administrative_area.administrative_parent -> New York -> government.governmental_jurisdiction.government_bodies -> United States Bankruptcy Court for the Southern District of New York\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> base.aareas.schema.administrative_area.administrative_children -> Manhattan -> location.location.containedby -> New York\n# Answer:\nManhattan", "# Reasoning Path:\nNew York City -> base.biblioness.bibs_location.state -> New York -> government.governmental_jurisdiction.government_bodies -> United States Bankruptcy Court for the Southern District of New York\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> base.aareas.schema.administrative_area.administrative_children -> Manhattan -> periodicals.newspaper_circulation_area.newspapers -> Daily News\n# Answer:\nManhattan", "# Reasoning Path:\nNew York City -> base.aareas.schema.administrative_area.administrative_children -> Manhattan -> location.location.containedby -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nNew York City -> base.aareas.schema.administrative_area.administrative_children -> Manhattan -> periodicals.newspaper_circulation_area.newspapers -> Five Towns Jewish Times\n# Answer:\nManhattan", "# Reasoning Path:\nNew York City -> base.aareas.schema.administrative_area.administrative_children -> Manhattan -> location.place_with_neighborhoods.neighborhoods -> Little Italy\n# Answer:\nManhattan"], "ground_truth": ["New York"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1868", "prediction": ["# Reasoning Path:\nKnox Overstreet -> film.film_character.portrayed_in_films -> m.0jx21d -> film.performance.actor -> Josh Charles\n# Answer:\nm.0jx21d", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_for -> g.125fszykz\n# Answer:\ng.125fszykz", "# Reasoning Path:\nKnox Overstreet -> film.film_character.portrayed_in_films -> m.0jx21d -> film.performance.film -> Dead Poets Society\n# Answer:\nm.0jx21d", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_types -> Film character -> type.type.expected_by -> Character\n# Answer:\nFilm character", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_types -> Film character -> type.type.expected_by -> Character using Wham-O product\n# Answer:\nFilm character", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Fictional Character\n# Answer:\nFilm character", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_types -> Film character -> type.type.expected_by -> Killed by\n# Answer:\nFilm character", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_types -> Film character -> freebase.type_profile.kind -> Annotation\n# Answer:\nFilm character", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Topic\n# Answer:\nFilm character", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_types -> Film character -> freebase.type_profile.kind -> Portrayal\n# Answer:\nFilm character"], "ground_truth": ["Josh Charles"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-187", "prediction": ["# Reasoning Path:\nStephen Hawking -> people.person.education -> m.02kq1zs -> education.education.major_field_of_study -> Physics\n# Answer:\nm.02kq1zs", "# Reasoning Path:\nStephen Hawking -> people.person.education -> m.02kq1zs -> education.education.degree -> Bachelor of Arts\n# Answer:\nm.02kq1zs", "# Reasoning Path:\nStephen Hawking -> people.person.education -> m.03p8dsf -> education.education.institution -> Trinity Hall, Cambridge\n# Answer:\nm.03p8dsf", "# Reasoning Path:\nStephen Hawking -> people.person.education -> m.05svw_r -> education.education.institution -> St Albans School\n# Answer:\nm.05svw_r", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Astronomer -> people.profession.specialization_of -> Scientist\n# Answer:\nAstronomer", "# Reasoning Path:\nStephen Hawking -> people.person.education -> m.03p8dsf -> education.education.degree -> Doctorate\n# Answer:\nm.03p8dsf", "# Reasoning Path:\nStephen Hawking -> people.person.education -> m.05svw_r -> freebase.valuenotation.has_value -> End Date\n# Answer:\nm.05svw_r", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Astrophysicist -> people.profession.people_with_this_profession -> Adolfo Figueroa-Vi\u00f1as\n# Answer:\nAstrophysicist", "# Reasoning Path:\nStephen Hawking -> music.artist.track_contributions -> m.0rvpv1n -> music.track_contribution.track -> A Glorious Dawn\n# Answer:\nm.0rvpv1n", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Astronomer -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Astronomers\n# Answer:\nAstronomer"], "ground_truth": ["Physics"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.125, "path_precision": 0.1, "path_recall": 0.16666666666666666, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1870", "prediction": ["# Reasoning Path:\nDavid Carradine -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> California\n# Answer:\nUnited States of America", "# Reasoning Path:\nDavid Carradine -> film.person_or_entity_appearing_in_film.films -> m.0_xwg9m -> film.personal_film_appearance.film -> Jonas in the Jungle\n# Answer:\nm.0_xwg9m", "# Reasoning Path:\nDavid Carradine -> people.person.nationality -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> California\n# Answer:\nUnited States of America", "# Reasoning Path:\nDavid Carradine -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nDavid Carradine -> film.person_or_entity_appearing_in_film.films -> m.0cgnkwb -> film.personal_film_appearance.film -> Ringers: Lord of the Fans\n# Answer:\nm.0cgnkwb", "# Reasoning Path:\nDavid Carradine -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> New York City\n# Answer:\nUnited States of America", "# Reasoning Path:\nDavid Carradine -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nDavid Carradine -> film.person_or_entity_appearing_in_film.films -> m.0_xwg9m -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.0_xwg9m", "# Reasoning Path:\nDavid Carradine -> people.person.nationality -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nDavid Carradine -> film.person_or_entity_appearing_in_film.films -> m.0crrb92 -> film.personal_film_appearance.film -> Starz Inside: Unforgettably Evil\n# Answer:\nm.0crrb92"], "ground_truth": ["How to Rob a Bank", "Tropical Snow", "I Saw What You Did", "Crime Zone", "Try This One for Size", "By Dawn's Early Light", "Mr. Horn", "Warlords", "Dangerous Curves", "Martial Law", "Autumn", "Animal Instincts", "Circle of Iron", "Treasure Raiders", "Stretch", "Dark Fields", "Detention", "Nightfall", "Six Against the Rock", "Dead & Breakfast", "Children of the Corn V: Fields of Terror", "Naked Movie", "Run for Your Life", "Homo Erectus", "Brothers in Arms", "Fuego", "Safari 3000", "The Bad Seed", "The Misfit Brigade", "Dinocroc vs. Supergator", "The Donor", "Kiss of a Stranger", "Hair High", "On the Line", "Kill Bill: The Whole Bloody Affair", "Final Move", "Future Zone", "Mean Streets", "Absolute Evil", "Thunder and Lightning", "American Reel", "Capital Punishment", "The Last Sect", "High Noon, Part II: The Return of Will Kane", "Gray Lady Down", "Dune Warriors", "Hell Ride", "Americana", "Kill Bill Volume 2", "Criminal Desire", "Kandisha", "All Hell Broke Loose", "Sonny Boy", "Money to Burn", "Evil Toons", "Bala Perdida", "Night of the Templar", "Crime School", "Project Eliminator", "The New Swiss Family Robinson", "Oceans of Fire", "Kung Fu Killer", "Deathsport", "The Golden Boys", "Waxwork II: Lost in Time", "Knocking On Death's Door", "Out of the Wilderness", "Code Name Jaguar", "The Rage", "Blackout", "Trick or Treats", "Lone Wolf McQuade", "True Legend", "Son of the Dragon", "Brotherhood of the Gun", "Break", "Maybe I'll Come Home in the Spring", "Eldorado", "Bound for Glory", "Last Goodbye", "Camille", "Road of No Return", "David Carradine's AM & PM Tai Chi Workout for Beginners", "Death Race 2000", "David Carradine's Shaolin Cardio Kick Boxing Workout", "Wizards of the Lost Kingdom 2", "Guaranteed on Delivery", "The Good Guys and the Bad Guys", "Cybercity", "UnConventional", "Armed Response", "Macon County Jail", "Macho Callahan", "The Warrior and the Sorceress", "Boxcar Bertha", "Big Stan", "Roadside Prophets", "Last Hour", "Bad Cop", "An American Tail: The Treasure of Manhattan Island", "Cannonball", "Future Force", "Jealousy", "Night Rhythms", "Archie's Final Project", "Kill Zone", "Bird on a Wire", "Cloud Dancer", "Sundown: The Vampire in Retreat", "Richard III", "Behind Enemy Lines", "Double Trouble", "Max Havoc: Curse of the Dragon", "Nowhere to Run", "Permanent Vacation", "Starz Inside: Unforgettably Evil", "Fall Down Dead", "Q", "Taggart", "The Outsider", "David Carradine Kung Fu Action Masters", "Kill Bill Volume 1", "The Ultimate Enemy", "Miracle at Sage Creek", "Fast Charlie... the Moonbeam Rider", "Crank: High Voltage", "Kung Fu: The Movie", "Midnight Fear", "Last Stand at Saber River", "Karate Cop", "Balto II: Wolf Quest", "The Monster Hunter", "Being Michael Madsen", "The Serpent's Egg", "The Long Riders", "Six Days in Paradise", "Young Billy Young"], "ans_acc": 0.0072992700729927005, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.08333333333333333, "path_precision": 0.1, "path_recall": 0.07142857142857142, "path_ans_f1": 0.013605442176870748, "path_ans_precision": 0.1, "path_ans_recall": 0.0072992700729927005}
{"id": "WebQTest-1871", "prediction": ["# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 2014 Stanley Cup Finals -> sports.sports_championship_event.champion -> Los Angeles Kings\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 2014 Stanley Cup Finals -> time.event.instance_of_recurring_event -> Stanley Cup Finals\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> symbols.namesake.named_after -> Frederick Stanley, 16th Earl of Derby -> common.topic.article -> m.01q69x\n# Answer:\nFrederick Stanley, 16th Earl of Derby", "# Reasoning Path:\nStanley Cup -> time.recurring_event.current_frequency -> Yearly -> base.events.festival_event.part_of_series -> National Poetry Month\n# Answer:\nYearly", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 2014 Stanley Cup Finals -> time.event.locations -> Madison Square Garden\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1933 Stanley Cup Finals -> sports.sports_championship_event.champion -> New York Rangers\n# Answer:\n1933 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> symbols.namesake.named_after -> Frederick Stanley, 16th Earl of Derby -> symbols.name_source.namesakes -> Stanley\n# Answer:\nFrederick Stanley, 16th Earl of Derby", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1933 Stanley Cup Finals -> sports.sports_championship_event.season -> 1932\u201333 NHL season\n# Answer:\n1933 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 2014 Stanley Cup Finals -> time.event.locations -> Staples Center\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1914 Stanley Cup Finals -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1914 Stanley Cup Finals"], "ground_truth": ["Frederick Stanley, 16th Earl of Derby"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1873", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> base.aareas.schema.administrative_area.administrative_area_type -> Sovereign state\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.main_country -> China -> location.location.containedby -> Asia\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.location.containedby -> Asia\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> China -> location.location.containedby -> Asia\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.main_country -> China -> location.country.official_language -> Standard Chinese\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.main_country -> China -> location.location.containedby -> East Asia\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.main_country -> China -> location.location.partially_contains -> Black River\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> China -> location.country.official_language -> Standard Chinese\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.location.containedby -> Eurasia\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.location.contains -> Bandar Seri Begawan\n# Answer:\nBrunei"], "ground_truth": ["China"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1874", "prediction": ["# Reasoning Path:\nPortugal -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Tunisia\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nPortugal -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Algeria\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nPortugal -> location.statistical_region.places_exported_to -> m.0bhdd46 -> location.imports_and_exports.exported_to -> Libya\n# Answer:\nm.0bhdd46", "# Reasoning Path:\nPortugal -> location.country.form_of_government -> Semi-presidential system -> common.topic.notable_types -> Form of Government\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nPortugal -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Mexico\n# Answer:\nConstitutional republic", "# Reasoning Path:\nPortugal -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Egypt\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nPortugal -> location.statistical_region.places_exported_to -> m.0bhdd46 -> location.imports_and_exports.currency -> Euro\n# Answer:\nm.0bhdd46", "# Reasoning Path:\nPortugal -> location.statistical_region.cpi_inflation_rate -> g.11b60zrgdp\n# Answer:\ng.11b60zrgdp", "# Reasoning Path:\nPortugal -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Mozambique\n# Answer:\nConstitutional republic", "# Reasoning Path:\nPortugal -> location.country.form_of_government -> Democratic republic -> government.form_of_government.countries -> Chile\n# Answer:\nDemocratic republic"], "ground_truth": ["Tunisia", "Egypt", "Saudi Arabia", "Mozambique", "Morocco", "United Arab Emirates", "Angola", "Mexico", "Algeria", "Singapore", "Libya", "Jordan"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.1643835616438356, "path_precision": 0.6, "path_recall": 0.09523809523809523, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.6, "path_ans_recall": 0.5}
{"id": "WebQTest-1875", "prediction": ["# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\ng.12cp_jcjy", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\ng.11b71vdgdz", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\ng.1hhc390hy", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc39sjp\n# Answer:\ng.1hhc39sjp", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fm01\n# Answer:\ng.12tb6fm01", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc385hl\n# Answer:\ng.1hhc385hl"], "ground_truth": ["Constitutional republic", "Presidential system", "Federal republic"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1876", "prediction": ["# Reasoning Path:\nBabylon -> location.location.containedby -> Iraq -> location.country.administrative_divisions -> Iraqi Kurdistan\n# Answer:\nIraq", "# Reasoning Path:\nBabylonia -> location.country.languages_spoken -> Akkadian language -> language.human_language.language_family -> Semitic languages\n# Answer:\nAkkadian language", "# Reasoning Path:\nBabylonia -> location.country.languages_spoken -> Akkadian language -> language.human_language.main_country -> Iraq\n# Answer:\nAkkadian language", "# Reasoning Path:\nBabylonia -> location.location.containedby -> Middle East -> location.location.contains -> Oman\n# Answer:\nMiddle East", "# Reasoning Path:\nBabylonia -> location.country.languages_spoken -> Akkadian language -> language.human_language.countries_spoken_in -> Assyria\n# Answer:\nAkkadian language", "# Reasoning Path:\nBabylonia -> location.location.people_born_here -> Hillel the Elder -> people.person.gender -> Male\n# Answer:\nHillel the Elder", "# Reasoning Path:\nBabylon -> location.location.containedby -> Iraq -> common.topic.notable_types -> Country\n# Answer:\nIraq", "# Reasoning Path:\nBabylonia -> location.country.languages_spoken -> Akkadian language -> language.human_language.language_family -> Afroasiatic languages\n# Answer:\nAkkadian language", "# Reasoning Path:\nBabylon -> location.location.containedby -> Iraq -> base.locations.countries.continent -> Asia\n# Answer:\nIraq", "# Reasoning Path:\nBabylonia -> location.country.languages_spoken -> Akkadian language -> language.human_language.language_family -> East Semitic languages\n# Answer:\nAkkadian language"], "ground_truth": ["Akkadian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1877", "prediction": ["# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Eyewitness: Reports from an Art World in Crisis\n# Answer:\nModern art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nModern art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Der Blaue Reiter -> common.topic.notable_for -> g.12596yl82\n# Answer:\nDer Blaue Reiter", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.art_forms -> Painting -> base.descriptive_names.names.descriptive_name -> m.0109p390\n# Answer:\nPainting", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Origins, Imitation, Conventions\n# Answer:\nModern art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Abstract art -> visual_art.art_period_movement.associated_artists -> Adja Yunkers\n# Answer:\nAbstract art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Abstract art -> common.topic.image -> Black Square\n# Answer:\nAbstract art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Oscar Bluemner\n# Answer:\nModern art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.art_forms -> Painting -> base.descriptive_names.names.descriptive_name -> m.0109p3lg\n# Answer:\nPainting", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.art_forms -> Painting -> interests.collection_category.collectors -> m.09vb4f2\n# Answer:\nPainting"], "ground_truth": ["Expressionism", "German Expressionism", "Der Blaue Reiter", "Modern art", "Abstract art"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6461538461538462, "ans_precission": 0.7, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.6461538461538462, "path_ans_precision": 0.7, "path_ans_recall": 0.6}
{"id": "WebQTest-1878", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> American Samoa\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\ng.125_r6z32", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Germany\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Germany\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guam\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nm.010r7f0z", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Mexico\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Marshall Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> periodicals.newspaper_circulation_area.newspapers -> American Free Press -> book.periodical.language -> English Language\n# Answer:\nAmerican Free Press"], "ground_truth": ["Constitutional republic", "Presidential system", "Federal republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1879", "prediction": ["# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban peso -> common.topic.notable_for -> g.125c74q9f\n# Answer:\nCuban peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban convertible peso -> common.topic.notable_for -> g.1257mndy1\n# Answer:\nCuban convertible peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban peso -> common.topic.notable_types -> Currency\n# Answer:\nCuban peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban convertible peso -> common.topic.webpage -> m.04m29wr\n# Answer:\nCuban convertible peso", "# Reasoning Path:\nCuba -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc38hmp\n# Answer:\ng.1hhc38hmp", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban convertible peso -> common.topic.article -> m.049p6m\n# Answer:\nCuban convertible peso", "# Reasoning Path:\nCuba -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc3d68y\n# Answer:\ng.1hhc3d68y", "# Reasoning Path:\nCuba -> book.book_subject.works -> A simple Habana melody -> book.written_work.subjects -> History\n# Answer:\nA simple Habana melody", "# Reasoning Path:\nCuba -> book.book_subject.works -> A simple Habana melody -> book.written_work.subjects -> France\n# Answer:\nA simple Habana melody", "# Reasoning Path:\nCuba -> book.book_subject.works -> A simple Habana melody -> common.topic.notable_types -> Book\n# Answer:\nA simple Habana melody"], "ground_truth": ["Cuban peso", "Cuban convertible peso"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-188", "prediction": ["# Reasoning Path:\nBristol Palin -> people.person.education -> m.0cpk4bl -> education.education.institution -> West Anchorage High School\n# Answer:\nm.0cpk4bl", "# Reasoning Path:\nBristol Palin -> people.person.education -> m.0cpkbwp -> education.education.institution -> Juneau-Douglas High School\n# Answer:\nm.0cpkbwp", "# Reasoning Path:\nBristol Palin -> people.person.education -> m.0byl4zl -> education.education.institution -> Wasilla High School\n# Answer:\nm.0byl4zl", "# Reasoning Path:\nBristol Palin -> people.person.sibling_s -> m.04mvspr -> people.sibling_relationship.sibling -> Trig Palin\n# Answer:\nm.04mvspr", "# Reasoning Path:\nBristol Palin -> common.topic.webpage -> m.09wv0fg -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09wv0fg", "# Reasoning Path:\nBristol Palin -> common.topic.webpage -> m.09ww50l -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09ww50l", "# Reasoning Path:\nBristol Palin -> common.topic.webpage -> m.09wv0fg -> common.webpage.resource -> Bristol Palin and Levi Johnston cancel their engagement\n# Answer:\nm.09wv0fg", "# Reasoning Path:\nBristol Palin -> common.topic.webpage -> m.09x64tg -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09x64tg", "# Reasoning Path:\nBristol Palin -> people.person.sibling_s -> m.04tll_g -> people.sibling_relationship.sibling -> Track Palin\n# Answer:\nm.04tll_g", "# Reasoning Path:\nBristol Palin -> people.person.sibling_s -> m.04tllyz -> people.sibling_relationship.sibling -> Willow Palin\n# Answer:\nm.04tllyz"], "ground_truth": ["West Anchorage High School", "Wasilla High School", "Juneau-Douglas High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1880", "prediction": ["# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.010nnlj1 -> government.government_position_held.office_holder -> Anna Moeller\n# Answer:\nm.010nnlj1", "# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.0wzs_6j -> government.government_position_held.office_holder -> John D. Anthony\n# Answer:\nm.0wzs_6j", "# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.010nnlj1 -> government.government_position_held.jurisdiction_of_office -> Illinois\n# Answer:\nm.010nnlj1", "# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.0wzq9t2 -> government.government_position_held.office_holder -> David Harris\n# Answer:\nm.0wzq9t2", "# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.010nnlj1 -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nm.010nnlj1", "# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.0wzs_6j -> government.government_position_held.office_position_or_title -> Member of Illinois House of Representatives\n# Answer:\nm.0wzs_6j", "# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.0wzs_6j -> government.government_position_held.jurisdiction_of_office -> Illinois\n# Answer:\nm.0wzs_6j", "# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.0wzq9t2 -> government.government_position_held.basic_title -> State Representative\n# Answer:\nm.0wzq9t2", "# Reasoning Path:\nIllinois House of Representatives -> common.topic.notable_types -> Governmental Body -> freebase.type_profile.kind -> Definition\n# Answer:\nGovernmental Body", "# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.010nnlj1 -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nm.010nnlj1"], "ground_truth": ["Adam Brown", "Bernard B. Wolfe", "Ralph C. Capparelli", "John D. Anthony", "Wayne Rosenthal", "Anna Moeller", "Terry Deering", "Bradley M. Glass", "Daniel M. Pierce", "Tobias Barry", "Tom Cross", "Art Turner", "Ann Williams", "Thaddeus Jones", "Josh Harms", "Bruce L. Douglas", "Harber H. Hall", "James T. Londrigan", "David Harris", "Robert W. McCarthy"], "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.20689655172413793, "path_precision": 0.3, "path_recall": 0.15789473684210525, "path_ans_f1": 0.2, "path_ans_precision": 0.3, "path_ans_recall": 0.15}
{"id": "WebQTest-1881", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\ng.11b60n70x2", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\ng.12cp_k2s4", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ll\n# Answer:\ng.1245_22ll", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22zj\n# Answer:\ng.1245_22zj", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc378pv\n# Answer:\ng.1hhc378pv", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc37hbq\n# Answer:\ng.1hhc37hbq"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-1882", "prediction": ["# Reasoning Path:\nFranklin D. Roosevelt -> government.politician.government_positions_held -> m.0b_p9hl -> government.government_position_held.basic_title -> State Senator\n# Answer:\nm.0b_p9hl", "# Reasoning Path:\nFranklin D. Roosevelt -> government.politician.government_positions_held -> m.0b_p9hl -> government.government_position_held.office_position_or_title -> New York State Senator\n# Answer:\nm.0b_p9hl", "# Reasoning Path:\nFranklin D. Roosevelt -> government.politician.government_positions_held -> m.03q2h_9 -> government.government_position_held.basic_title -> President\n# Answer:\nm.03q2h_9", "# Reasoning Path:\nFranklin D. Roosevelt -> people.person.profession -> Politician -> freebase.type_profile.kind -> Title\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin D. Roosevelt -> government.politician.government_positions_held -> m.03q2h_9 -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nm.03q2h_9", "# Reasoning Path:\nFranklin D. Roosevelt -> government.politician.government_positions_held -> m.03q2h_j -> government.government_position_held.basic_title -> Assistant Secretary\n# Answer:\nm.03q2h_j", "# Reasoning Path:\nFranklin D. Roosevelt -> people.person.profession -> Politician -> type.type.domain -> Government\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin D. Roosevelt -> government.politician.government_positions_held -> m.03q2h_j -> government.government_position_held.office_position_or_title -> Assistant Secretary of the Navy\n# Answer:\nm.03q2h_j", "# Reasoning Path:\nFranklin D. Roosevelt -> government.politician.government_positions_held -> m.03q2h_9 -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nm.03q2h_9", "# Reasoning Path:\nFranklin D. Roosevelt -> people.person.profession -> Lawyer -> common.topic.notable_types -> Profession\n# Answer:\nLawyer"], "ground_truth": ["New York State Senator", "Assistant Secretary of the Navy", "Governor of New York"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.28571428571428575, "path_precision": 0.2, "path_recall": 0.5, "path_ans_f1": 0.30769230769230765, "path_ans_precision": 0.2, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1883", "prediction": ["# Reasoning Path:\nRobert Downey Jr. -> base.popstra.celebrity.dated -> m.065q2w4 -> base.popstra.dated.participant -> Maria Tomei\n# Answer:\nm.065q2w4", "# Reasoning Path:\nRobert Downey Jr. -> base.popstra.celebrity.dated -> m.063pkp9 -> base.popstra.dated.participant -> Farrah Fawcett\n# Answer:\nm.063pkp9", "# Reasoning Path:\nRobert Downey Jr. -> base.popstra.celebrity.canoodled -> m.0652wvp -> base.popstra.canoodled.participant -> Calista Flockhart\n# Answer:\nm.0652wvp", "# Reasoning Path:\nRobert Downey Jr. -> base.popstra.celebrity.dated -> m.065py2z -> base.popstra.dated.participant -> Uma Thurman\n# Answer:\nm.065py2z", "# Reasoning Path:\nRobert Downey Jr. -> tv.tv_actor.guest_roles -> m.040r4ks -> tv.tv_guest_role.episodes_appeared_in -> The Fat Guy Strangler\n# Answer:\nm.040r4ks", "# Reasoning Path:\nRobert Downey Jr. -> tv.tv_actor.guest_roles -> m.0bv0lvm -> tv.tv_guest_role.episodes_appeared_in -> Paula DeAnda, Robert Downey Jr.;\n# Answer:\nm.0bv0lvm", "# Reasoning Path:\nRobert Downey Jr. -> tv.tv_actor.guest_roles -> m.040r4ks -> tv.tv_guest_role.character -> Patrick Pewterschmidt\n# Answer:\nm.040r4ks", "# Reasoning Path:\nRobert Downey Jr. -> tv.tv_actor.guest_roles -> m.0bv0rg1 -> tv.tv_guest_role.episodes_appeared_in -> Show #0534\n# Answer:\nm.0bv0rg1"], "ground_truth": ["California Substance Abuse Treatment Facility and State Prison, Corcoran"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1884", "prediction": ["# Reasoning Path:\nBarack Obama -> people.person.education -> m.0nbyhqc -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nm.0nbyhqc", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgp6 -> education.education.institution -> Punahou School\n# Answer:\nm.02nqgp6", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.0nbyhqc -> education.education.institution -> Noelani Elementary School\n# Answer:\nm.0nbyhqc", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.0nbyhqc -> freebase.valuenotation.has_no_value -> Major/Field Of Study\n# Answer:\nm.0nbyhqc", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.0nbyhqc -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nm.0nbyhqc", "# Reasoning Path:\nBarack Obama -> people.person.ethnicity -> English American -> people.ethnicity.languages_spoken -> American English\n# Answer:\nEnglish American", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgp6 -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nm.02nqgp6", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.012l94sv -> education.educational_institution.students_graduates -> m.012l94s9\n# Answer:\nm.012l94sv", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.subjects -> 47 Percent\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Writer's Journey to Selma, Alabama -> book.written_work.subjects -> Selma\n# Answer:\nA Writer's Journey to Selma, Alabama"], "ground_truth": ["Juris Doctor", "Bachelor of Arts"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1885", "prediction": ["# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.deities -> Ayyappan\n# Answer:\nHinduism", "# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.deities -> Haridra Ganapati\n# Answer:\nHinduism", "# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.deities -> Rama\n# Answer:\nHinduism", "# Reasoning Path:\nShiva -> common.topic.notable_for -> g.125bspml8\n# Answer:\ng.125bspml8", "# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.texts -> Upanishads\n# Answer:\nHinduism", "# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.practices -> Astrology\n# Answer:\nHinduism", "# Reasoning Path:\nShiva -> fictional_universe.person_in_fiction.representations_in_fiction -> Bhagwan Shree Shiv -> common.topic.notable_types -> Film character\n# Answer:\nBhagwan Shree Shiv", "# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.practices -> Ayurveda\n# Answer:\nHinduism", "# Reasoning Path:\nShiva -> fictional_universe.person_in_fiction.representations_in_fiction -> Bhagwan Shree Shiv -> film.film_character.portrayed_in_films -> m.0gw2rw8\n# Answer:\nBhagwan Shree Shiv", "# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.texts -> Vachanamrut\n# Answer:\nHinduism"], "ground_truth": ["Hinduism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1886", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> medicine.medical_treatment.used_to_treat -> Presbyopia\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.image -> A bifocal corrective eyeglasses lens\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Lightning rod -> book.book_subject.works -> Stealing God's Thunder: Benjamin Franklin's Lightning Rod and the Invention of America\n# Answer:\nLightning rod", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_trwk\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> Glass\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_ty__\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nm.012zbkk5", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_wj2d\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.instrument.family -> Crystallophone\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Lightning rod -> common.topic.webpage -> m.09w0d6d\n# Answer:\nLightning rod"], "ground_truth": ["Lightning rod", "Glass harmonica", "Bifocals", "Franklin stove"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.8181818181818182, "path_precision": 0.9, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-1887", "prediction": ["# Reasoning Path:\nSwitzerland -> location.country.currency_used -> Swiss franc -> finance.currency.countries_used -> Liechtenstein\n# Answer:\nSwiss franc", "# Reasoning Path:\nSwitzerland -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> Europe\n# Answer:\nEarth", "# Reasoning Path:\nSwitzerland -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> France\n# Answer:\nEarth", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d0tc3g\n# Answer:\ng.11b6d0tc3g", "# Reasoning Path:\nSwitzerland -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> Sovereign state\n# Answer:\nEarth", "# Reasoning Path:\nSwitzerland -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> United States of America\n# Answer:\nEarth", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6p27nwl\n# Answer:\ng.11b6p27nwl", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.1hhc3d66v\n# Answer:\ng.1hhc3d66v"], "ground_truth": ["Swiss franc"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1888", "prediction": ["# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> sports.sports_league.teams -> m.0crt479\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> sports.sports_league.teams -> m.0crt4l_\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> organization.organization.child -> m.0ck2dsc\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> sports.sports_league.teams -> m.0crt47r\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> business.employer.employees -> m.010vsyz7\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> organization.organization.child -> m.0ck2ds6\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> common.topic.webpage -> m.09w8snm -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w8snm", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> organization.organization.child -> m.0zs6st6\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> business.employer.employees -> m.0cmxjcn\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> common.topic.image -> Crowd outside the 1903 World Series -> common.image.size -> m.02bpf1c\n# Answer:\nCrowd outside the 1903 World Series"], "ground_truth": ["1982 World Series"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1889", "prediction": ["# Reasoning Path:\nBill Clinton -> base.popstra.celebrity.dated -> m.065pxsm -> base.popstra.dated.participant -> Dolly Kyle Browning\n# Answer:\nm.065pxsm", "# Reasoning Path:\nBill Clinton -> government.us_president.vice_president -> Al Gore -> people.person.gender -> Male\n# Answer:\nAl Gore", "# Reasoning Path:\nBill Clinton -> people.person.children -> Chelsea Clinton -> people.person.parents -> Hillary Rodham Clinton\n# Answer:\nChelsea Clinton", "# Reasoning Path:\nBill Clinton -> government.us_president.vice_president -> Al Gore -> people.person.religion -> Baptists\n# Answer:\nAl Gore", "# Reasoning Path:\nBill Clinton -> people.person.children -> Chelsea Clinton -> people.person.children -> Charlotte Clinton Mezvinsky\n# Answer:\nChelsea Clinton", "# Reasoning Path:\nBill Clinton -> government.us_president.vice_president -> Al Gore -> people.person.religion -> Southern Baptist Convention\n# Answer:\nAl Gore", "# Reasoning Path:\nBill Clinton -> people.person.children -> Chelsea Clinton -> people.person.education -> m.02k6vqx\n# Answer:\nChelsea Clinton", "# Reasoning Path:\nBill Clinton -> people.person.children -> Chelsea Clinton -> people.person.education -> m.02wpv2h\n# Answer:\nChelsea Clinton", "# Reasoning Path:\nBill Clinton -> people.person.children -> Chelsea Clinton -> people.person.education -> m.04m0g48\n# Answer:\nChelsea Clinton"], "ground_truth": ["Al Gore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-189", "prediction": ["# Reasoning Path:\nEgypt -> location.location.containedby -> Arab world -> common.topic.subject_of -> As Egypt Howls and History Tweets\n# Answer:\nArab world", "# Reasoning Path:\nEgypt -> location.location.containedby -> Arab world -> location.location.contains -> Iraq\n# Answer:\nArab world", "# Reasoning Path:\nEgypt -> location.location.containedby -> Arab world -> periodicals.newspaper_circulation_area.newspapers -> Al-Shorouk\n# Answer:\nArab world", "# Reasoning Path:\nEgypt -> base.locations.countries.continent -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica", "# Reasoning Path:\nEgypt -> location.location.containedby -> Middle East -> location.location.contains -> Iraq\n# Answer:\nMiddle East", "# Reasoning Path:\nEgypt -> location.location.containedby -> Arab world -> location.location.contains -> Jordan\n# Answer:\nArab world", "# Reasoning Path:\nEgypt -> location.location.containedby -> Middle East -> travel.travel_destination.tour_operators -> Bunnik Tours\n# Answer:\nMiddle East", "# Reasoning Path:\nEgypt -> location.location.containedby -> Middle East -> location.location.contains -> Israel\n# Answer:\nMiddle East", "# Reasoning Path:\nEgypt -> location.location.containedby -> Middle East -> location.location.events -> Middle East Theatre of World War II\n# Answer:\nMiddle East", "# Reasoning Path:\nEgypt -> location.location.containedby -> Arab world -> location.location.contains -> Libya\n# Answer:\nArab world"], "ground_truth": ["Middle East"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1890", "prediction": ["# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.genre.artists -> Aaron Copland\n# Answer:\nArt song", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.compositional_form.compositions -> L'adieu du cavalier\n# Answer:\nArt song", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Opera -> music.compositional_form.superforms -> Vocal music\n# Answer:\nOpera", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.genre.artists -> Albert Roussel\n# Answer:\nArt song", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Chamber music -> music.genre.albums -> Acts of Beauty/Exit no Exit\n# Answer:\nChamber music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Chamber music -> music.genre.parent_genre -> Classical music\n# Answer:\nChamber music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.genre.artists -> Alexander Borodin\n# Answer:\nArt song", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.genre.albums -> 8 Lust Songs: I Sonetti Lussuriosi\n# Answer:\nArt song", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Chamber music -> common.topic.notable_types -> Musical genre\n# Answer:\nChamber music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Opera -> common.topic.notable_types -> Composition type\n# Answer:\nOpera"], "ground_truth": ["Fantasie und Fuge \u00fcber den Choral Ad nos, ad salutarem undam, S. 259: II. Adagio", "Romance oubli\u00e9e, S. 527", "Venezia e Napoli, S. 159: No. 1. Lento", "Fantasie sur l'\u00f3pera hongroise Sz\u00e9p Ilonka, S. 417", "Ave maris stella, S. 506", "Li marinari", "Schwebe, schwebe, blaues Auge, S. 305/2", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 8 in D-flat major \\\"M. K.\\\"", "Un sospiro", "Weihnachtsbaum, S. 186: Nr. 4. Adeste fideles", "Trauervorspiel und Trauermarsch, S. 206: No. 1. Trauervorspiel", "Chor\u00e4le, S. 506a: No. 5. Nun ruhen all W\u00e4lder", "Harmonies po\u00e9tiques et religieuses", "Canzone, S. 162 no. 2", "Elegy no. 2, S. 131", "Der alte Vagabund", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 5 in E major \\\"La Chasse\\\"", "Responsorien und Antiphonen, S. 30: V. In officio defunctorum", "Consolation in C-sharp minor, S. 171a no. 3", "Historische ungarische Bildnisse, S. 205a: No. 2. De\u00e1k Ferenc", "Two Hungarian Recruiting Dances, S. 241 \\\"Zum Andenken\\\": No. 1. Kinizsi n\u00f3t\u00e1ja", "Valse-Impromptu, S. 213 bis", "Salve Regina", "Sunt lacrymae rerum / En mode hongrois, S. 163 no. 5", "Le Rossignol, S. 249d", "Ave maris stella, S. 669 no. 2", "P\u00e1sztor Lakodalmas, S. 405a", "Fantasy on Themes from Mozart's Figaro and Don Giovanni", "Via Crucis, S. 504a: Vexilla regis", "Ang\u00e9lus ! Pri\u00e8re aux anges gardiens, S. 163 no. 1", "2 Polonaises, S. 223: no. 2 in E major", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 1 in G minor", "Im Rhein, im sch\u00f6nen Strome, S. 272/2", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 13 in A minor \\\"R\u00e1k\u00f3czi-Marsch\\\"", "Consolation in Des-Dur", "Vom Fels zum Meer!, S. 229", "Winzerchor aus den entfesselten Prometheus, S. 692e", "Von der Wiege bis zum Grabe, S. 107: III. Die Wiege des zukunftigen Lebens", "Comment, disaient-ils", "Entwurf der Ramann-Elegie, S. 196a", "Totentanz, S. 126: VIII. Variation VI", "2 Polonaises, S. 223: no. 1 \\\"Polonaise m\u00e9lancolique\\\" in C minor", "O Roma nobilis, S. 546a", "Biterolf und der Schmied von Ruhla", "Trois Chansons, S. 510a: No. 3. L'Esp\u00e9rance", "Die Trauer-Gondel (La lugubre gondola), S. 134", "Enfant, si j'\u00e9tais roi, S. 283/2", "Weihnachtsbaum, S. 185a: VIII. [Alt-provenzalische No\u00ebl]", "Sz\u00f3zat und Hymnus, S. 353", "Consolation in E major, S. 172 no. 6: Allegretto sempre cantabile", "Elegy no. 2, S. 197", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 11. (B\u00e9n\u00e9diction) (Pr\u00e9lude)", "Einleitung und Schlu\u00dftakte zu Tausigs 3. Valse-Caprice, S. 571a", "Introduction des Variations sur une marche du Si\u00e8ge de Corinthe, S. 421a", "Leyer und Schwert, S. 452: I. [Introduction]", "R\u00e9miniscences des Huguenots, S. 412/1", "Romancero espagnol, S. 695c: No. 3. Jota aragonesa and coda", "Album-Leaf: Serenade, S. 166g", "Schlaflos!, S. 203", "Zwei St\u00fccke aus der heiligen Elisabeth, S. 693a: No. 2. Der Sturm", "Die Loreley", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 12 in E minor \\\"Hero\u00efde \u00e9l\u00e9giaque\\\"", "Transcendental \u00c9tude No. 8", "Consolation in E major, S. 172 no. 1: Andante con moto", "R\u00e1k\u00f3czi-Marsch, S. 692d", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 21 in E minor", "Schlummerlied, S. 186/7a", "Sept variations brillantes sur un th\u00e8me de Rossini, op. 2, S. 149", "Requiem f\u00fcr die Orgel, S. 266: VII. Postludium", "Glanes de Woronince", "Album-Leaf: Adagio \u2013 religioso in C major, S. 164l", "Drei M\u00e4rsche von Franz Schubert, S. 426: No. 1. Trauermarsch (Grande Marche fun\u00e8bre)", "Liebestraum As-Dur \\\"Hohe Liebe\\\", S. 541 Nr. 1", "R\u00e9miniscences de \\\"La Juive\\\", S. 409a", "La campanella", "Helge's Treue, S. 686", "Hexam\u00e9ron, S. 392: I. Introduction", "Album-Leaf: Andante religiosamente in G major, S. 166j", "Consolation in E-Dur", "Grandes etudes de Paganini", "Das Veilchen", "Leyer und Schwert, S. 452: II. Schwertlied", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 5. Miserere", "Fanfare zur Enth\u00fcllung des Carl-Augusts Monument, S. 542b", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 18 in C-sharp minor", "Was tun?", "Weihnachtsbaum, S. 185a: VI. R\u00e9veille-Matin (Wecker)", "R\u00e1k\u00f3czi-Marsch, S. 244b", "Soir\u00e9es de Vienne, S. 427: No. 3 in E major. Allegro vivace", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 19 in F-sharp minor", "Angiolin dal biondo crin (Arranged for solo piano)", "Es rauschen die Winde, S. 294/1", "Zweite Festmarsch nach Motiven von E H z S-C-G, S. 522", "Magyar dalok, S. 242: No. 7 in E-flat major", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 20 in G minor \\\"Rumanian Rhapsody\\\"", "Gnomenreigen", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 8. Prose des morts - De profundis", "Hungarian Coronation Mass, S. 11: I. Kyrie", "Weihnachtsbaum, S. 186: Nr. 5. Scherzoso", "Magyar dalok, S. 242: No. 3 in D-flat major", "Deux Polonaises de l'oratorio St. Stanislas, S. 519: Polonaise I", "Die Zelle in Nonnenwerth, S. 534/2", "Chor\u00e4le, S. 506a: No. 4. Nun danket alle Gott", "Album-Leaf: Introduction to the Grande \u00c9tude de Paganini no. 6, S. 141/6bis", "A magyarok Istene, S. 543", "Trauervorspiel und Trauermarsch, S. 206: No. 2. Trauermarsch", "Two Hungarian Recruiting Dances, S. 241 \\\"Zum Andenken\\\": No. 2. Lass\u00fa magyar", "Hungarian Rhapsody No. 19", "Album-Leaf (Ah, vous dirai-je, maman), S. 163b", "Cantico del Sol di San Francesco d'Assisi, S. 499", "Ora pro nobis, S. 262", "Vall\u00e9e d'Obermann, S. 160 no. 6", "Heroischer Marsch im ungarischem Stil, S. 231", "Klavierst\u00fcck in F-sharp major, S. 192 no. 4. Andantino", "Ave Maria in E major, S. 182 \\\"Die Glocken von Rom\\\"", "La perla, S. 326/2", "Halloh!, Jagdchor und Steyrer aus der Oper Tony, S. 404", "Douze grandes \u00e9tudes, S. 137: No. 12 in B-flat minor (Andantino)", "R\u00e1k\u00f3czy March, S. 117", "Magyar kir\u00e1ly-dal, S. 544", "R\u00e9miniscences des Huguenots, S. 412/2", "Sonetto 47 del Petrarca, S. 161 no. 4", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 2 in E-flat major", "Pilgerchor aus Tannh\u00e4user, S. 443/2", "Fantasie und Fuge \u00fcber das Thema B-A-C-H, S. 529/2", "Valse-caprice No. 9 (Sehnsuchtswalzer), S. 427/9", "Album d'un voyageur, S. 156: III. Paraphrases: 11. Un soir dans les montagnes [de Knop] - Nocturne pastorale", "Il m'aimait tant, S. 533", "Douze grandes \u00e9tudes, S. 137: No. 3 in F major (Poco adagio)", "Ann\u00e9es de p\u00e8lerinage : Premi\u00e8re ann\u00e9e : Suisse, S. 160", "Die Loreley, S. 273/1", "Totentanz, S. 126: VI. Variation V", "R\u00e1k\u00f3czi-Marsch, S. 244c", "Trauerode (Die Todten), S. 268 no. 2", "Grande \u00e9tude de perfectionnement", "Orpheus, S. 672a", "Zigeuner-Epos, S. 695b: No. 4 in C-sharp major. Animato", "Harmonies po\u00e9tiques et religieuses, S. 154", "Weihnachtsbaum, S. 185a: XI. Ungarisch", "Rosario, S. 670: No. 3. Mysteria gloriosa", "Tarantella, S. 162 no. 3", "Orpheus", "Der n\u00e4chtliche Zug, S. 513a", "Via Crucis, S. 504a: Station XIII: J\u00e9sus est d\u00e9pos\u00e9 de la croix", "Mephisto Waltz no. 2, S. 515", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 8c. Allegro moderato", "Einzug der G\u00e4ste auf der Wartburg, S. 445 no. 1/c", "Capriccio alla turca sur des motifs de Beethoven, S. 388", "Coro di festa e marcia funebre de Don Carlos, S. 435", "Via Crucis, S. 53: Station I: Jesus wird zum Tode verdammt", "Feuille d'album, S.165", "Sonetto 104 del Petrarca", "Hungarian Rhapsody no. 13 in A minor, S. 244 no. 13", "La Lugubre gondola I, S. 200/1", "Hexam\u00e9ron, S. 365b: IV. Variation II. Moderato", "Paraphrases pour piano sur le th\u00e8me favori et oblig\u00e9: 1a. Pr\u00e9lude \u00e0 la Polka de Borodine, S. 207a", "Marche fun\u00e8bre de Dom S\u00e9bastien, S. 402", "Variationen \u00fcber das Motiv von Bach, S. 180", "Faribolo pastour, S. 236 no. 1", "Tre sonetti del Petrarca", "M\u00e9lodie polonaise, S. 249a", "Album-Leaf: Exeter Preludio, S. 164c", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 9. Andante lagrimoso", "La lugubre gondola", "Von der Wiege bis zum Grabe, S. 512: II. Der Kampf ums Dasein", "Ungarische Nationalmelodien, S. 243: No. 3. Pr\u00e9lude. Allegretto", "Der Gl\u00fcckliche", "Legend no. 2: \\\"St. Francis Walking on the Waves\\\"", "Album-Leaf, S. 167e", "Hexam\u00e9ron, S. 365b: I. Introduction. Extr\u00eamement lent", "Klavierst\u00fcck aus der Bonn Beethoven-Kantate, S. 507", "Album-Leaf: Andantino in A-flat major, S. 166p", "\u00c9l\u00e9gie sur des motifs du Prince Louis Ferdinand de Prusse, S. 168/1", "Schwanengesang und Marsch aus Hunyadi L\u00e1szl\u00f3, S. 405", "Gaudeamus igitur, S. 240/2", "Orpheus, S. 98", "L\u00e4ndler in A-flat major, S. 211", "Das Grab und die Rose", "Weihnachtsbaum, S. 185a: VII. Schlummerlied", "Mephisto Waltz no. 2, S. 111", "Festkantate zur Enth\u00fcllung des Beethoven-Denkmals in Bonn, S. 67: III. Andante mesto - Allegro maestoso - Recitativo - Largo maestoso - Allegro fuocoso", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 5 in G-flat major", "M\u00e9lodies hongroises d'apr\u00e8s Franz Schubert, S. 425: No. 3. Allegretto", "Einleitung und Coda zu Rubinsteins \u00c9tude in C-Dur, S. 554a", "Symphonisches Zwischenspiel zu \u00dcber allen Zauber Liebe, S. 497", "Hungarian Coronation Mass, S. 11: IV. Credo", "La lugubre gondola, S. 200/2", "Aux cypr\u00e8s de la Villa d'Este I : Thr\u00e9nodie, S. 163 no. 2", "Valse m\u00e9lancolique, S. 210", "Einzug der G\u00e4ste auf der Wartburg, S. 445 no. 1", "Deux marches dans le genre hongrois, S. 693: No. 2 in B-flat minor", "J'ai perdu ma force et ma vie, S. 327", "Pri\u00e8re d'un enfant \u00e0 son r\u00e9veil, S. 171c", "Album-Leaf: Tempo di marcia in E-flat major, S. 167o", "Ungarische Zigeunerweisen", "Festvorspiel, S. 226", "Consolation in E major, S. 171a no. 2: Un poco pi\u00f9 mosso", "Album-Leaf in E major (Detmold), S. 164d", "Der Hirt", "Tr\u00fcbe Wolken (Nuages gris), S. 199, R. 78", "Album-Leaf in C major, S. 167s \\\"Lyon\\\"", "Grandes \u00e9tudes de Paganini, S. 141: No. 6. Variations in A minor", "Trois morceaux suisses, S. 156a: No. 3. Ranz de ch\u00e8vres", "Album Leaf in E major (Vienna), S. 164a", "Weil noch, Sonnenstrahl", "Hexam\u00e9ron, S. 365b: IX. Finale. Molto vivace quasi prestissimo", "Paraphrase de concert sur Ernani I, S. 431a", "O lieb, so lang du lieben kannst!, S. 540a", "Historical Hungarian Portraits, S. 205: No. 1. Sz\u00e9ch\u00e9nyi Istvan", "Sardanapale", "Zigeuner-Epos, S. 695b: No. 3 in D-flat major. Sehr langsam", "Pr\u00e4ludium und Fuge \u00fcber das Motiv B.A.C.H., S. 529/1", "Bist du", "Stabat mater, S. 172b", "Ave Maria II, S. 38", "Grande \u00e9tude d'apr\u00e8s Paganini no. 2 in E-flat major, BV B 70", "Weihnachtsbaum, S. 185a: III. Die Hirten an der Krippe (In dulci jubilo)", "Zigeuner-Epos, S. 695b: No. 8 in D major. Adagio sostenuto a capriccio", "Mephisto Waltz no. 4, S. 696", "La notte, S. 516a", "Mephisto Waltz no. 3, S. 215a", "Hungarian Rhapsody No. 6", "Totentanz, S. 126: III. Variation II", "Hungarian Rhapsody no. 7 in D minor, S. 244 no. 7", "Le Triomphe fun\u00e8bre du Tasse, S. 517", "Marche fun\u00e8bre, S. 226a", "Album d'un voyageur, S. 156: I. Impressions et po\u00e9sies: 2a. Le Lac de Wallenstadt", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 8a. Andante con sentimento", "Glanes de Woronince, S. 249: No. 1. Ballade ukraine (Dumka)", "Fantaisie sur des motifs favoris de l'op\u00e9ra Somnambula de Bellini, S. 393/1", "Illustrations du Proph\u00e8te, S. 414: No. 3: Pastorale - Appel aux armes", "Festkl\u00e4nge, S. 101", "Klavierst\u00fcck in E major, S. 192 no. 1. Sehr langsam", "Einleitung und Coda zu Raffs Walzer in Des-Dur, S. 551a", "Hungarian Rhapsody no. 4 in E-flat major, S. 244 no. 4", "Historical Hungarian Portraits, S. 205: No. 7. Mosonyi Mih\u00e1ly", "Elsas Brautzug zum M\u00fcnster, S. 445 no. 2", "19 Hungarian Rhapsodies for Piano, S 244 No. 15 \\\"R\u00e1k\u00f3czy March\\\"", "Der Fischerknabe", "Ungarischer Marsch in B-flat major, S. 229a", "Concerto for Piano No. 1 in E flat major, S 124: III. Allegretto vivace", "Zigeuner-Epos, S. 695b: No. 1 in C minor. Lento", "Magyar rapsz\u00f3dia, S. 244/16/1", "Weimars Volkslied, S. 542/1", "Scherzo in G minor, S. 153", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 2 in C minor \\\"Langueur?\\\"", "Ungarische National-Melodien (Im leichten Style bearbeitet), S. 243bis: No. 2 in C major", "Totentanz, S. 126: II. Variation I", "Schuberts ungarische Melodien, S. 425a: No. 2. Marcia (Marche hongroise)", "Douze grandes \u00e9tudes, S. 137: No. 5 in B-flat major (Equalmente)", "Douze grandes \u00e9tudes, S. 137: No. 11 in D-flat major (Lento assai)", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 14 in A minor", "Deux Polonaises de l'oratorio St. Stanislas, S. 519: Polonaise II", "Album-Leaf: Andantino in E-flat, S. 163a", "Album Leaf in E-flat (Leipzig), S. 164b", "L'Hymne du Pape, S. 530", "Ungarische Nationalmelodien, S. 243: No. 1. Tempo giusto", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 7 in E major \\\"Alternative\\\"", "Historical Hungarian Portraits, S. 205: No. 6. Pet\u00f6fi Sandor", "Transcendental \u00c9tude No. 11", "Weihnachtsbaum, S. 186: Nr. 7. Schlummerlied", "Via Crucis, S. 53: Station IX: Jesus f\u00e4llt zum dritten Mal", "R\u00e9miniscences de Lucrezia Borgia, S. 400: II. Chanson \u00e0 boire (Orgie). Duo-finale", "Polonaise aus Tschaikowskys \\\"Eugen Onegin\\\", S. 429", "Der traurige M\u00f6nch", "Fantasie und Fuge \u00fcber den Choral Ad nos, ad salutarem undam, S. 259: I. Fantasie", "Trois \u00e9tudes de concert, S. 144: III. \\\"Un sospiro\\\" in D-flat major", "Album d'un voyageur, S. 156: III. Paraphrases: 12. Ranz de chevres [de F. Huber] - Allegro finale", "Cs\u00e1rd\u00e1s", "Marche militaire, S. 426a", "Orpheus, S. 511b", "Ich liebe dich, S. 542a", "Hexam\u00e9ron, S. 365b: VIIIb. Coda", "Schuberts ungarische Melodien, S. 425a: No. 3. Allegretto", "Zwei Orchesters\u00e4tze aus dem Oratorium Christus, S. 498b: Nr. 2. Die heiligen drei K\u00f6nige \u2013 Marsch", "Grande paraphrase de la Marche de Donizetti pour le Sultan Abdul-Medjid Khan, S. 403", "Soir\u00e9es de Vienne, S. 427: No. 4 in D-flat major. Andantino a capriccio", "Sonetto 123 del Petrarca, S. 161 no. 6", "R\u00e1koczy March in A minor", "Le Mal du pays, S. 160 no. 8", "Schlummerlied mit Arabesken, S. 454", "Alleluia, S. 183 no. 1", "Via Crucis, S. 53: Station II: Jesus tr\u00e4gt sein Kreuz", "Transcendental \u00c9tude No. 9", "Salve Polonia, S. 518", "Hungaria", "Comment, disaient-ils, S. 535", "Via Crucis, S. 504a: Station V: Simon le Cyr\u00e9n\u00e9en aide J\u00e9sus \u00e0 porter sa croix", "Valse-Impromptu, S. 213a", "Prol\u00e9gom\u00e8nes \u00e0 la Divina Commedia, S. 158b", "Marche hongroise, S. 425/2e", "K\u00fcnstlerfestzug zur Schillerfeier 1859, S. 520/2", "Five Hungarian Folksongs, S. 245: No. 2. M\u00e9rsek\u00e9lve. Allegretto", "Transcendental \u00c9tude No. 12", "\u00c9tude en douze exercices, S. 136: III. Allegro sempre legato", "Weinen, Klagen, Sorgen, Zagen, Pr\u00e4ludium nach Johann Sebastian Bach, S. 179", "Albumblatt in Walzerform, S. 166", "Chor\u00e4le, S. 506a: No. 9. Vexilla regis", "Hungarian Coronation Mass, S. 11: II. Gloria", "Apr\u00e9s une lecture du Dante, S. 161/7 (Ann\u00e9es de P\u00e9l\u00e8rinage II/7)", "Marche hongroise, S. 233b", "Douze grandes \u00e9tudes, S. 137: No. 9 in A-flat major (Andantino)", "Transcendental \u00c9tude No. 10", "Transcendental \u00c9tude No. 3", "Grosses Konzertsolo, S. 176", "Marche des Tcherkesses de l'op\u00e9ra Rouslan et Loudmila de Glinka, S. 406/1", "Von der Wiege bis zum Grabe, S. 107: II. Der Kampf ums Dasein", "Sancta Dorothea, S. 187", "Galop (in A minor), S. 218", "Im Rhein, im sch\u00f6nen Strome, S. 272/1", "La Campanella : Nu Rave", "Hungarian Rhapsody for Orchestra no. 1 in F minor, S. 359/1", "Introitus, S. 268 no. 1", "Album-Leaf: Magyar, S. 164e", "Comment, disaient-ils, S. 276/2", "Chapelle de Guillaume Tell, S. 160 no. 1", "Via Crucis, S. 53: Station XII: Jesus stirbt am Kreuze", "Liebestraum As-Dur \\\"Oh Lieb, so lang du lieben kannst\\\", S. 541 Nr. 3", "La Mandragore, S. 698", "Aus der Ungarischen Kr\u00f6nungsmesse, S. 501: II. Offertorium", "Eine Faust-Symphonie, S. 108: III. Mephistopheles. Allegro vivace, ironico", "\u00c9tude en douze exercices, S. 136: X. Moderato", "Bagatelle sans tonalit\u00e9", "Glanes de Woronince, S. 249: No. 3. Complainte (Dumka)", "H\u00e9ro\u00efde fun\u00e8bre", "O du mein holder Abendstern: Recitativ und Romanze aus Wagners T\u00e4nnhauser", "Weihnachtsbaum, S. 186: Nr. 3. Die Hirten an der Krippe", "Am Grabe Richard Wagners, S. 202", "L\u00e4ndler, S. 211a", "O lieb, so lang du lieben kannst, S. 298/2", "Apparitions, S. 155: No. 2. Vivamente", "Klavierst\u00fcck in F-sharp major, S. 192 no. 3. Sehr langsam", "Leyer und Schwert, S. 452: III. Gebet (vor der Schlacht)", "Mosonyis Grabgeleit, S. 194", "Oh! quand je dors, S. 536", "Symphonic poems", "Illustrations du Proph\u00e8te, S. 414: No. 2: Les Patineurs: Scherzo", "Feuille morte, S. 428", "Piano Concerto No. 3", "Weihnachtsbaum, S. 186: Nr. 9. Abendglocken", "Weihnachtsbaum, S. 186: Nr. 6. Carillon", "Romancero espagnol, S. 695c: No. 1. Introduction and Fandango with variations", "R\u00e9miniscences des Puritains, S. 390/2", "Tarantelle di bravura d\u2019apr\u00e8s la tarantelle de La muette de Portici, S. 386/2", "Consolation in D-flat major, S. 172 no. 4: Quasi adagio", "Hungarian Rhapsody no. 17 in D minor, S. 244 no. 17", "Via Crucis, S. 504a: Station XII: J\u00e9sus meurt sur la croix", "Die stille Wasserrose", "Piano Concerto No. 1", "Morceau de salon (\u00e9tude de perfectionnement), S. 142", "Variation on the March from Bellini's I Puritani", "Rondo di bravura, op. 4 no. 2, S. 152", "Valse oubli\u00e9e no. 3, S. 215 no. 3", "Hungarian Rhapsody for Orchestra no. 2 in C minor, S. 359/2", "Von der Wiege bis zum Grabe, S. 107", "Suite no. 4 in G major, op. 61 \\\"Mozartiana\\\": III. Preghiera: Andante non tanto", "Gretchen aus Faust-Symphonie, S. 513", "Via Crucis, S. 504a: Station VIII: Les Femmes de J\u00e9rusalem", "La romanesca, S. 252a/1", "Canzone napolitana, S. 248a", "Cadenza to the first movement of Beethoven's Piano Concerto no. 3, S. 389a", "Fantasie \u00fcber englische Themen, S. 694", "Zwei St\u00fccke aus dem Oratorium Christus, S. 498c: No. 1. Einleitung - Pastorale", "Historische ungarische Bildnisse, S. 205a: No. 5. V\u00f6r\u00f6smarty Mih\u00e1ly", "\u00c0 la Chapelle Sixtine, S. 461/1", "Sospiri!, S. 192 no. 5. Andante", "Tre sonetti di Petrarca", "A magyarok Istene, S. 543bis", "Gaudeamus igitur, S. 509", "Valse m\u00e9lancolique, S. 210a", "Douze grandes \u00e9tudes, S. 137: No. 6 in G minor (Largo patetico)", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 3 in E major", "Und sprich, S. 329", "B\u00fclow-Marsch, S. 230", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 1. Invocation", "Hungarian Rag", "Es war ein K\u00f6nig in Thule, S. 531 no. 4", "\u00c9tude en douze exercices, S. 136: VI. Molto agitato", "Aus der Ungarischen Kr\u00f6nungsmesse, S. 501: I. Benedictus", "Angelus!, S. 162a/2", "I' vidi in terra angelici costumi", "Sposalizio", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 4 in D-flat major \\\"Derni\u00e8re illusion\\\"", "Melodie in Dorische Tonart, S. 701d", "Album-Leaf: Magyar II in B-flat minor, S. 164e/2", "Hexam\u00e9ron, S. 392: V. Variation III di bravura - Ritornello", "2 Cs\u00e1rd\u00e1s, S. 225: No. 2. Cs\u00e1rd\u00e1s obstin\u00e9", "Resignazione, S. 187b", "Douze grandes \u00e9tudes, S. 137: No. 1 in C major (Presto)", "Waltz in E-flat major, S. 209a", "Grande Fantaisie Symphonique on themes from Berlioz's \\\"L\u00e9lio\\\" for Piano and Orchestra, S. 120", "Magyar dalok, S. 242: No. 8 in F minor", "Via Crucis, S. 53: Station VI: Sancta Veronica", "Hussitenlied, S. 234", "La Tombe et la rose, S. 539", "Weihnachtsbaum, S. 185a: IX. [Abendglocken]", "Via Crucis, S. 53: Einleitung. Vexilla regis", "Sposalizio, S. 157a", "Waldesrauschen (Forest Murmurs)", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 4. Litanies de Marie", "Historische ungarische Bildnisse, S. 205a: No. 3. Teleki L\u00e1szl\u00f3", "Reimar der Alte", "Souvenir de la fianc\u00e9e, S. 385/3", "Grande paraphrase de la Marche de Donizetti pour le Sultan Abdul-Medjid Khan, S. 403 bis", "Variations sur Le Carnaval de Venise, S. 700a", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 8b. Andante molto espressivo", "Venezia e Napoli, S. 159: No. 3. Andante placido", "Transcendental \u00c9tude No. 2", "K\u00fcnstlerfestzug zur Schillerfeier 1859, S. 520/1", "Dante Symphony", "Die Zelle in Nonnenwerth, S. 382", "Le Lac de Wallenstadt, S. 156/2a bis", "Historische ungarische Bildnisse, S. 205a: No. 6. Pet\u00f6fi S\u00e1ndor", "Variation \u00fcber einen Walzer von Diabelli, S. 147", "Concerto for Piano and Orchestra no. 1 in E-flat major, S. 124: I. Allegro maestoso", "Hungarian Rhapsody for Orchestra no. 5 in E minor, S. 359/5", "Ai No Yume", "La lugubre gondola, S. 199a/1", "B\u00e9n\u00e9diction et serment de Benvenuto Cellini, S. 396", "\u00c9tude en douze exercices, S. 136: V. Moderato", "M\u00e9lodies hongroises d'apr\u00e8s Franz Schubert, S. 425: No. 1. Andante", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 4b in E major", "Hungarian Rhapsody No. 15", "Faust Symphony", "Hungaria, S. 511e", "Fantasie und Fuge \u00fcber den Choral Ad nos, ad salutarem undam, S. 259: III. Fugue", "Lieder aus Schillers Wilhelm Tell, S. 292a: Nr. 3. Der Alpenj\u00e4ger", "Fantasie und Fuge \u00fcber den Choral Ad nos, ad salutarem undam, S. 624: III. Fugue", "Paralipom\u00e8nes \u00e0 la Divina Commedia, S. 158a", "Album-Leaf in A-flat, S. 166c", "Muttergottes-Str\u00e4usslein zum Mai-Monate, S. 316: Nr. 1. Das Veilchen", "Douze grandes \u00e9tudes, S. 137: No. 10 in F minor (Presto molto agitato)", "Feuille d'album no. 2, S. 167", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 4 in E major", "Die Lorelei, S. 531 no. 1", "Mephisto Waltz no. 3, S. 216", "Hungarian Rhapsody no. 11 in A minor, S. 244 no. 11", "Kaiser Wilhelm!, S. 197b", "Concerto sans orchestre, S. 524a", "Missa solennis zur Einweihung der Basilika in Gran, S. 9 \\\"Graner Messe\\\": I. Kyrie. Andante solenne", "Album-Leaf: Allegretto in A major, S. 167n", "Spinnerin-Lied, Transkripition aus Wagners \\\"Der fliegende Holl\u00e4nder\\\", S. 440", "Hexam\u00e9ron, S. 392: IV. Variation II. Moderato", "g.1235wfk4", "Der du von dem Himmel bist, S. 279/1", "Sunt lacrymae rerum, S. 162c", "Weihnachtsbaum, S. 186: Nr. 1. Altes Weihnacthslied (Psalite)", "Eine Symphonie zu Dantes Divina Commedia, S. 109: II. Purgatorio - Magnificat", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 1. \u00c9levez-vous, voix de mon \u00e2me", "Von der Wiege bis zum Grabe, S. 512: III. Die Wiege des zukunftigen Lebens", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 8. Miserere d'apr\u00e8s Palestrina", "Zigeuner-Epos, S. 695b: No. 11 in A minor. Lento", "Christmas Again", "Dante Sonata", "Marche hongroise, S. 425/2c", "R\u00e9miniscences de Robert le Diable, S. 413", "Benedetto sia 'l giorno", "Album Leaf in F-sharp minor, S. 163a/1", "Die Zelle in Nonnenwerth, S. 534/3", "Consolation in E major, S. 171a no. 5: Andantino", "Album-Leaf in A-flat major, S. 166l", "Grande Fantaisie sur des th\u00e8mes de Paganini, S. 700/1", "Weihnachtsbaum, S. 186: Nr. 11. Ungarisch", "Hungarian Rhapsodies", "St. Stanislaus fragment, S. 688a", "Via Crucis, S. 504a: Station III: J\u00e9sus tombe pour la premi\u00e8re fois", "Trois Chansons, S. 510a: No. 1. La Consolation", "Transcendental \u00c9tude No. 6", "Rosario, S. 670: No. 2. Mysteria dolorosa", "Fun\u00e9railles", "\u00c9tude en douze exercices, S. 136: VII. Allegretto con molta espressione", "Andante sensibilissimo, S. 701c", "Six poesies. Buch der Lieder", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 1 in E-flat major", "Via Crucis, S. 53: Station X: Jesus wird entkleidet", "Douze grandes \u00e9tudes, S. 137: No. 8 in C minor (Presto strepitoso)", "Trois morceaux suisses, S. 156a: No. 2. Un soir dans la montagne", "Elegy no. 1, S. 196", "Hungarian Rhapsody no. 3 in B-flat major, S. 244 no. 3", "Eine Faust-Symphonie, S. 108: II. Gretchen. Andante soave", "Fantasie und Fuge \u00fcber den Choral Ad nos, ad salutarem undam, S. 259", "\u00c9tude en douze exercices, S. 136: I. Allegro con fuoco", "Glanes de Woronince, S. 249: No. 2. M\u00e9lodies polonaises", "Grande fantaisie dramatique sur des th\u00e8mes de l'op\u00e9ra Les Huguenots, S. 412/3, R. 211 \\\"R\u00e9miniscences des Huguenots\\\"", "Album-Leaf in C minor (Pressburg), S. 163c", "Douze grandes \u00e9tudes, S. 137: No. 4 in D minor (Allegro patetico)", "Album-Leaf in D major, S. 164h", "Via Crucis, S. 504a: Station VII: J\u00e9sus tombe pour la seconde fois", "Magyarische Litanei", "Ungarische National-Melodie, S. 242/13 bis", "Album-Leaf: Andantino in E-flat major, S. 163a/2", "Hungarian Rhapsody no. 5 in E minor, S. 244 no. 5 \\\"H\u00e9ro\u00efde \u00e9l\u00e9giaque\\\"", "R\u00e9miniscences de Boccanegra, S. 438", "Kennst du das Land, S. 531 no. 3", "Weihnachtsbaum, S. 186: Nr. 2. O heilige Nacht!", "Canzone napolitana, S. 248", "Technische Studien, S. 146: No. 62. Spr\u00fcnge mit der Tremolo-Begleitung", "Korrekturblatt, S. 701k", "Evocation \u00e0 la Chapelle Sixtine, S. 658", "Soir\u00e9es de Vienne, S. 427: No. 9 in A-flat major. Preludio a capriccio", "Grandes \u00e9tudes de Paganini, S. 141: No. 1. Tremolo in G minor", "Weimars Volkslied", "\u00dcber allen Gipfeln ist Ruh, S. 306", "Huit variations, op. 1, S. 148", "Tre sonetti di Petrarca, S. 158: No. 2. Sonetto CIV. Pace non trovo", "Magyar dalok, S. 242: No. 11 in B-flat major", "La lugubre gondola, S. 134b", "Vexilla regis prodeunt, S. 185", "Via Crucis, S. 53: Station III: Jesus f\u00e4llt zum ersten Mal", "Ouvert\u00fcre zu Tannh\u00e4user von Richard Wagner", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 6. Hymne de l'enfant \u00e0 son r\u00e9veil", "Aus Lohengrin, S. 446: No. 2. Elsas Traum", "Feierlicher Marsch zum heiligen Gral aus Parsifal, S. 450", "Via Crucis, S. 504a: Station IX: J\u00e9sus tombe une troisi\u00e8me fois", "Consolation in E major, S. 172 no. 5: Andantino", "Album-Leaf in A minor (R\u00e1k\u00f3czi-Marsch), S. 164f", "Valse-Impromptu", "Chanson boh\u00e9mienne, S. 250 no. 2", "Romancero espagnol, S. 695c: No. 2. Elaboration of an unidentified theme", "Douze grandes \u00e9tudes, S. 137: No. 2 in A minor (Molto vivace a capriccio)", "Transcendental \u00c9tude No. 1", "Die Zelle in Nonnenwerth, S. 534/2 bis", "Die Zelle in Nonnenwerth, S. 534/1", "Chor\u00e4le, S. 506a: No. 2. Jesu Christe: Die f\u00fcnf Wunden", "Chor\u00e4le, S. 506a: No. 7. O Lamm Gottes", "Sarabande and Chaconne from Handel's Almira", "\u00c0 la Chapelle Sixtine, S. 461/2", "Kling leise, mein Lied, S. 301/1", "Huldigungsmarsch, S. 228ii", "Epithalam, S. 526", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 15 in D minor", "Lieder aus Schillers Wilhelm Tell, S. 292a: Nr. 1. Der Fischerknabe", "Wo weilt er? (Heimat)", "Tannh\u00e4user Overture", "Fantasy and Fugue on the chorale 'Ad nos ad salutarem undam'", "Douze grandes \u00e9tudes, S. 137: No. 7 in E-flat major (Allegro deciso)", "Magyar dalok, S. 242: No. 2 in C major", "Angelus! Pri\u00e8re \u00e0 l'ange gardien, S. 162a/4", "Mazeppa, S. 511c", "Prozinsky Fragment for piano, S. 701v", "Introduction et Polonaise de l'op\u00e9ra I puritani, S. 391", "Drei M\u00e4rsche von Franz Schubert, S. 426: No. 3. Grande Marche caracteristique", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 3 in A-flat minor \\\"La Campanella\\\"", "Album-Leaf: Agitato in G major, S. 167l", "Magyar dalok, S. 242: No. 9 in A minor", "Hexam\u00e9ron, S. 392: IX. Finale. Molto vivace quasi prestissimo", "Hungarian Rhapsody no. 12 in C-sharp minor, S. 244 no. 12", "Wilde Jagd: Scherzo, S. 176a", "Schlummerlied im Grabe, S. 195a", "Venezia e Napoli, S. 159: No. 4. Tarantelles napolitaines", "Historical Hungarian Portraits, S. 205: No. 5. De\u00e1k Ferenc", "Festmarsch zu S\u00e4kularfeier von Goethes Geburtstag, S. 227", "Hungarian Rhapsody no. 10 in E major, S. 244 no. 10 bis", "Hungarian Rhapsody no. 2 in C-sharp minor, S. 244 no. 2 bis", "Der tugendhafte Schreiber", "Festkantate zur Enth\u00fcllung des Beethoven-Denkmals in Bonn, S. 67: I. Maestoso - Quasi allegretto", "Zigeuner-Epos, S. 695b: No. 10 in F major. Lento", "Via Crucis, S. 504a: Station I: J\u00e9sus est condamn\u00e9 \u00e0 mort", "Der Alpenj\u00e4ger", "Consolation in E major, S. 171a no. 6: Allegretto", "Grande valse di bravura, S. 209", "Hungarian Coronation Mass, S. 11: III. Graduale (Psalm 116)", "Der blinde S\u00e4nger, S. 546", "Grosse Konzertfantasie \u00fcber spanische Weisen, S. 253", "Heinrich von Ofterdingen", "Klavierkonzert No. 1 Es-dur: III. Allegro marciale animato", "Album-Leaf: Fugue chromatique. Allegro in G minor, S. 167j", "Marche hongroise, S. 425/2b", "Festkl\u00e4nge, S. 511d", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 3. B\u00e9n\u00e9diction de Dieu dans la solitude", "Album-Leaf: Andantino in E major, S. 163d/ii", "Apparitions, S. 155: No. 3. Fantaisie sur une valse de Fran\u00e7ois Schubert", "Album-Leaf: Quasi mazurek in C major, S. 163e", "Mephisto Waltz no. 4, S. 216b", "Cadenza for \\\"Un sospiro\\\", S. 144/3", "Dumka, S. 249b", "Drei M\u00e4rsche von Franz Schubert, S. 426: No. 2. Grande Marche", "\u00c9tude en douze exercices, S. 136: IX. Allegro grazioso", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 9a. Allegretto", "Mephisto Waltz No. 1", "Responsorien und Antiphonen, S. 30: II. Feria V in coena Domini", "Valse de concert sur deux motifs de Lucia et Parisina de Donizetti, S. 214 no. 3", "Weihnachtsbaum, S. 186: Nr. 10. Ehemals", "Valse de l'op\u00e9ra \\\"Faust\\\", S. 407", "Mephisto Waltzes", "Tyrolean Melody, S. 385a", "Totentanz", "Carrousel de Madame Pelet-Narbonne, S. 214a", "Die Lorelei, S. 532", "Lieder aus Schillers Wilhelm Tell, S. 292a: Nr. 2. Der Hirt", "Le Rossignol, S. 250 no. 1", "Nocturne, S. 191/1", "Album d'un voyageur, S. 156: III. Paraphrases: 10. Ranz de vaches [de F. Huber] - Aufzug auf die Alp - Improvisata", "Litanies de Marie, S. 171e", "Album-Leaf: Larghetto in D-flat major, S. 167p", "Un portrait en musique de la Marquise de Blocqueville, S. 190: III. M\u00eame mouvement mais avec incertitude", "Marche hongroise, S. 425/2e bis", "Siegesmarsch, S. 233a", "Album Leaf in A major, S. 166k", "Via Crucis, S. 504a: Station X: J\u00e9sus est d\u00e9pouill\u00e9 de ses v\u00eatements", "Grand solo de concert, S. 365", "An Edlitam. Zur silbernen Hochzeit", "Valse oubli\u00e9e no. 4, S. 215 no. 4", "Valse m\u00e9lancolique, S. 214 no. 2", "Ce qu'on entend sur la montagne", "R\u00e9miniscences de Don Juan, S. 656", "S'il est un charmant gazon, S. 284/2", "Pastorale, S. 160 no. 3", "Album-Leaf in E-flat major, S. 167k", "Valse-caprice no. 6, S.427/6b", "Paraphrase de concert sur Ernani II, S. 432", "Album-Leaf: Preludio, S. 164j", "Canzonetta del Salvator Rosa, S. 161 no. 3", "Die Lore Ley: neue umgearbeitete Ausg.", "Adagio in C major, S. 158d", "Illustrations de l'op\u00e9ra L'Africaine, S. 415: No. 2. Marche indienne", "Hungarian Rhapsody no. 1 in C-sharp minor, S. 244 no. 1", "Chor\u00e4le, S. 506a: No. 10. Was Gott tut, das ist wohlgetan", "Drei Lieder aus Schillers Wilhelm Tell", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 6 in A minor", "Unstern! Sinistre, disastro, S. 208, R. 80", "M\u00e9lodies hongroises d'apr\u00e8s Franz Schubert, S. 425: No. 2. Marche hongroise", "Apr\u00e8s une lecture du Dante - Fantasia quasi Sonata, S. 158c", "Valse oubli\u00e9e no. 1, S. 215 no. 1", "Vergiftet sind mein Lieder, S. 289/3", "Album d'un voyageur, S. 156: I. Impressions et po\u00e9sies: 1. Lyon", "Via Crucis, S. 53: Station XI: Jesus wird ans Kreuz geschlagen", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 10. (Hymne)", "Fantaisie sur des motifs de La pastorella dell\u2019Alpi e Li marinari des Soir\u00e9es musicales, S. 423", "Les Morts, S. 516", "Five Hungarian Folksongs, S. 245: No. 5. B\u00fasongva. Lento", "Allegro di bravura, op. 4 no. 1, S. 151", "Grandes \u00e9tudes de Paganini, S. 141: No. 5. La Chasse in E major", "Angelus! Pri\u00e8re \u00e0 l'ange gardien, S. 162a/3", "Album-Leaf in G minor, S. 166l/2", "Tre sonetti di Petrarca, S. 270: I. Pace non trovo", "Weihnachtsbaum, S. 186: Nr. 8. Altes provenzalisches Weihnachtslied", "Via Crucis, S. 504a: Station XIV: J\u00e9sus est mis dans le sepulcre", "Ann\u00e9es de p\u00e8lerinage : Deuxi\u00e8me ann\u00e9e : Italie, S. 161", "Album-Leaf in G major, S. 166q", "Via Crucis, S. 53: Station XIII: Jesus wird vom Kreuz genommen", "Aus der Musik von Eduard Lassen zu Hebbels Nibelungen und Goethes Faust, S. 496: II. Faust: 1. Osterhymne", "Grande Fantaisie sur des motifs de Soir\u00e9es musicales, S. 422/1", "Drei St\u00fccke aus der Legende der heiligen Elisabeth, S. 498a: Nr. 3. Interludium", "Walhall aus Der Ring des Nibelungen, S. 449", "\u00c0 la Chapelle Sixtine, S. 360", "Album-Leaf in E major (Leipzig), S. 163d", "Jeanne d'Arc au b\u00fbcher, S. 293/3", "Ungarischer Geschwindmarsch, S. 233", "Rosario, S. 670: No. 1. Mysteria gaudiosa", "Ungarischer Sturmmarsch, S. 524", "Don Sanche", "Weimars Volkslied, S. 542/2", "Adagio non troppo, S. 151a", "Den Cypressen der Villa d'Este, S. 162b", "Deux l\u00e9gendes, S. 175: St. Fran\u00e7ois d'Assise: la pr\u00e9dication aux oiseaux", "Toccata, S. 197a", "Romance oubli\u00e9e, S. 132a", "Responsorien und Antiphonen, S. 30: III. Feria VI in Parasceve", "Hungarian Rhapsody no. 18 in F-sharp minor, S. 244 no. 18/2", "2 Cs\u00e1rd\u00e1s, S. 225: No. 1. Cs\u00e1rd\u00e1s", "Album-Leaf: Moderato in D-flat major, S. 164k", "Scherzo und Marsch, S. 177", "Les Pr\u00e9ludes, S. 511a", "Klavierst\u00fcck \u00fcber ein fremdes Thema, S. 387a", "Consolation No. 3", "Eine Faust-Symphonie, S. 108: IV. Chorus mysticus", "Concerto path\u00e9tique, S. 365a", "Fantasia quasi Concerto \\\"After Reading Dante\\\"", "Ungarische National-Melodien (Im leichten Style bearbeitet), S. 243bis: No. 1 in D major", "Marche h\u00e9ro\u00efque, S. 510", "Via Crucis, S. 53: Station IV: Jesus begegnet seiner Heiligen Mutter", "Ballade No. 1", "Album-Leaf, S. 167h", "Wieder m\u00f6cht' ich dir begegnen", "Variations on \\\"Tisz\u00e1ntuli sz\u00e9p l\u00e9any\\\", S. 384a", "Hungarian Rhapsody No. 2", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 5. Pater noster", "Wir sind nicht Mumien", "Deux \u00e9pisodes d'apres le Faust de Lenau, S. 110: I. Der n\u00e4chtliche Zug", "Hungarian Rhapsody no. 18 in F-sharp minor, S. 244 no. 18", "Trois morceaux suisses, S. 156a: No. 1. Ranz de vaches", "Romance oubli\u00e9e, S. 132c", "Konzert-Walzer \u00fcber zwei Themen aus Donizettis \\\"Lucia und Parisina\\\", S. 401", "Air cosaque, S. 249c", "R\u00e1k\u00f3czi March, S. 608", "Soir\u00e9es de Vienne, S. 427: No. 5 in G-flat major. Moderato cantabile con affetto", "Vive Henri IV, S. 239", "Eglogue, S. 160 no. 7", "Hunnenschlacht", "Valse de concert, S. 430", "Album-Leaf: Berlin Preludio, S. 164g", "S'il est un charmant gazon, S. 538", "Hungarian Rhapsody no. 18 in F-sharp minor, S. 244 no. 18/1", "Lenore, S. 346", "Via Crucis, S. 53: Station V: Simon von Kyrene hilft Jesus das Kreuz zu tragen", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 9c. Andantino con molto sentimento", "Ihr Glocken von Marling, S. 328", "Magyar dalok, S. 242: No. 1 in C minor", "Album-Leaf in A-flat (Portugal), S. 166b", "Album-Leaf: Magyar in D-flat major, S. 164e/3", "Isoldens Liebestod, S. 447", "In festo transfigurationis Domini nostri Jesu Christi, S. 188", "Mazurek, S. 166m/3", "Hungarian Coronation Mass, S. 11: V. Offertorium", "Weihnachtsbaum, S. 185a: IV. Adeste fideles (gleichsam als Marsch der heiligen drei K\u00f6nige)", "Urbi et orbi, S. 184", "God Save the Queen, S. 235", "Concerto path\u00e9tique", "Freisch\u00fctz Fantasie, S. 451", "Historische ungarische Bildnisse, S. 205a: No. 7. Mosonyi Mih\u00e1ly", "Geharnischte Lieder, S. 511: No. 3. Es rufet Gott uns mahnend", "L\u00e9gende No. 1: St Fran\u00e7ois d'Assise", "Petite Valse, S. 695d", "Ein Fichtenbaum steht einsam, S. 309", "Hungarian Rhapsody for Orchestra no. 3 in D major, S. 359/3", "Phantasiest\u00fcck \u00fcber Motive aus Rienzi, S. 439", "Concerto for Piano and Orchestra no. 1 in E-flat major, S. 124: II. Quasi adagio", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 4. Pens\u00e9e des morts", "Freudvoll und Leidvoll, S. 280/2", "Historische ungarische Bildnisse, S. 205a: No. 4. E\u00f6tv\u00f6s J\u00f3zsef", "Angiolin dal biondo crin", "Album d'un voyageur, S. 156: I. Impressions et po\u00e9sies: 2b. Au bord d'une source", "Zigeuner-Epos, S. 695b: No. 9 in E-flat major. Andante cantabile quasi adagio", "Cadenza, S. 695f", "Trois \u00e9tudes de concert, S. 144: II. \\\"La leggierezza\\\" in F minor", "Ungarische Nationalmelodien, S. 243: No. 2. Animato", "Ave Maria in D major, S. 504/1", "Am Rhein, im sch\u00f6nen Strome, S. 531 no. 2", "Prometheus", "L'Id\u00e9e fixe, S. 395", "Ave Maria (d'Arcadelt), S. 183 no. 2", "Totentanz, S. 126: I. Andante - Allegro - Allegro moderato", "Mal\u00e9diction, S. 121", "Sonata in B minor", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 7. Hymne de l'enfant \u00e0 son r\u00e9veil", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 1a in G minor", "Ihr Auge, S. 310/2", "Elegy no. 1, S. 130b", "Album-Leaf: Braunschweig Preludio, S. 166f", "L'Id\u00e9e fixe, S. 470a no. 1", "Drei St\u00fccke aus der Legende der heiligen Elisabeth, S. 498a: Nr. 1. Orchester Einleitung", "Einleitung, Fuge und Magnificat, S. 672b", "Tu es Petrus, S. 664", "Am Rhein", "Etude in Twelve Exercises", "Petite Valse favorite, S. 212a", "Klavierst\u00fcck in A-flat major, S. 189a", "Tre sonetti di Petrarca, S. 158: No. 3. Sonetto CXXIII. I' vidi in terra angelici costumi", "Transcendental \u00c9tude No. 4", "Hexam\u00e9ron, S. 392: VIII. Variation VI. Largo - Coda", "Illustrations de l'op\u00e9ra L'Africaine, S. 415: No. 1. Pri\u00e8re des matelots \\\"\u00d4 grand Saint Dominique\\\"", "Soir\u00e9es de Vienne, S. 427: No. 6 in A minor. Allegro con strepito", "Soir\u00e9es de Vienne, S. 427: No. 8 in D major. Allegro con brio", "Pri\u00e8re et Berceuse de La muette de Portici d'Auber, S. 387: Pri\u00e8re", "\u00c9tude en douze exercices, S. 136: II. Allegro con molto", "Resignazione, S. 187a", "Deux marches dans le genre hongrois, S. 693: No. 1 in D minor", "Hungarian Rhapsody no. 9 in E-flat major, S. 244 no. 9 \\\"Carnival in Pest\\\"", "Bist du, S. 277/2", "Fantasie und Fuge \u00fcber den Choral Ad nos, ad salutarem undam, S. 624: I. Fantasy", "Album d'un voyageur, S. 156: I. Impressions et po\u00e9sies: 4. Vall\u00e9e d'Obermann", "Klavierst\u00fcck in D-flat major, S. 189b", "Au bord d'une source, S. 160 no. 4 bis", "Hungarian Coronation Mass, S. 11: VII. Benedictus", "Pri\u00e8re et Berceuse de La muette de Portici d'Auber, S. 387: Berceuse", "Hungarian Coronation Mass, S. 11: VIII. Agnus Dei", "Postludium, S. 162f", "Magyar dalok, S. 242: No. 6 in G minor", "Chor\u00e4le, S. 506a: No. 6. O Haupt voll Blut und Wunden", "Totentanz, S. 126: VII. Cadenza", "Liebestraum Es-Dur \\\"Seliger Tod\\\", S. 541 Nr. 2", "Hungarian Battle March, S. 119", "Venezia e Napoli, S. 159: No. 2. Allegro", "Anfangs wollt ich fast verzagen", "Von der Wiege bis zum Grabe, S. 107: I. Die Wiege", "Feuille d\u2019album no. 1 in E major, S. 164", "Weihnachtsbaum, S. 185a: II. O heilige Nacht", "Valse oubli\u00e9e no. 2 in A-flat major, S. 215 no. 2", "Ab irato, grande \u00e9tude de perfectionnement, S. 143", "Leyer und Schwert, S. 452: IV. Lutzows wilde Jagd", "Ungarischer Marsch zur Kr\u00f6nungsfeier in Ofen-Pest am 8. Juni 1867, S. 523", "Walzer in A major, S. 208a", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 16 in E major", "Valse oubli\u00e9e no. 3, S. 215 no. 3a", "Totentanz, S. 126: IV. Variation III", "Weihnachtsbaum, S. 185a: XII. Polnisch", "Pr\u00e4ludium und Fuge \u00fcber den Namen BACH, S. 260: I. Pr\u00e4ludium", "Huldigungsmarsch, S. 228/1", "Aux anges gardiens, S. 162a/1 bis", "Slavimo Slavno Slaveni!", "Grand galop chromatique, S. 219 bis", "Responsorien und Antiphonen, S. 30: I. In nativitate Domini", "Ungarische National-Melodien (Im leichten Style bearbeitet), S. 243bis: No. 3 in B-flat major", "Seconda mazurka di Tirindelli, S. 573a", "Der 18. Psalm", "Festmarsch zur Goethe-Jubil\u00e4umfeier, S. 521", "Die Rose, S. 571", "Totentanz, S. 126: V. Variation IV", "R.W.-Venezia, S. 201", "Album d'un voyageur, S. 156: I. Impressions et po\u00e9sies: 3. Les Cloches de G*****", "Venezia e Napoli, S. 159", "Spirto gentil, S. 400a", "Liebestraum No. 3 As-dur", "Album-Leaf: Aus dem Purgatorio des Dante Sinfonie. Lamentoso in B minor, S. 166r/2", "Die drei Zigeuner", "Magyar dalok, S. 242: No. 10 in D major", "Salve Regina, S. 669 no. 1", "Consolation in E major, S. 172 no. 2: Un poco pi\u00f9 mosso", "Variations de bravoure sur des th\u00e8mes de Paganini, S. 700/2", "Der du von dem Himmel bist, S. 279/3", "Glasgow fragment, S. 701f", "Hungarian Rhapsody no. 8 in F-sharp minor, S. 244 no. 8", "Historical Hungarian Portraits, S. 205: No. 4. Teleki Laszlo", "Grand galop chromatique", "Eine Symphonie zu Dantes Divina Commedia, S. 109: I. Inferno", "Aux cypr\u00e8s de la Villa d'Este II : Thr\u00e9nodie, S. 163 no. 3", "Totentanz, S. 126: X. Allegro animato", "Mazeppa", "Piano Concerto No. 2", "Walther von der Vogelweide", "Pri\u00e8re et Berceuse de La muette de Portici d'Auber, S. 387: Introduction", "Der traurige M\u00f6nch, S. 348", "Fantasy and Fugue on the Theme B-A-C-H", "Zigeuner-Epos, S. 695b: No. 7 in A minor \\\"R\u00e1k\u00f3czi-Marsch\\\". Tempo di marcia", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 17 in A minor", "Wolfram von Eschenbach", "Marche fun\u00e8bre et Cavatine de Lucia de Lammermoor, S. 398", "Aus der Musik von Eduard Lassen zu Hebbels Nibelungen und Goethes Faust, S. 496: I. Nibelungen: 2. Bechlarn", "Album-Leaf: Lyon Pr\u00e9lude, S. 166d", "Chor\u00e4le, S. 506a: No. 8. O Traurigkeit", "Fantasie \u00fcber Themen aus Beethoven's Ruinen von Athen, S. 388b", "Album-Leaf: Andante in E-flat major, S. 167r", "Deux l\u00e9gendes, S. 175: II bis. St. Fran\u00e7ois de Paule marchant sur les flots", "Variationen \u00fcber das Motiv von Bach, S. 673", "Mazeppa, S. 138", "O du mein holder Abendstern, S. 444", "Album-Leaf (Premi\u00e8re Consolation), S. 171b", "Polnisch, S. 701g", "Three Concert \u00c9tudes", "Es war ein K\u00f6nig in Thule, S. 278/2", "Soir\u00e9es de Vienne, S. 427: No. 2 in A-flat major. Poco allegro", "Album-Leaf: Purgatorio. Andante in B minor, S. 166r/1", "En r\u00eave - Nocturne, S. 207", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 9b. Allegretto", "Pilgerchor aus Tannh\u00e4user, S. 443/1", "Consolation in D-flat major, S. 171a no. 4: Quasi adagio", "Valse, S. 210b", "Grande fantaisie de concert, S. 393/2", "Geharnischte Lieder, S. 511: No. 1. Vor der Schlacht", "Album-Leaf in A major, S. 166s", "Ballade No. 2", "Consolation in E major, S. 171a no. 1: Andante con moto", "Tre sonetti di Petrarca, S. 270: II. Benedetto sia 'l giorno", "Weihnachtsbaum, S. 185a: V. Scherzoso", "Impromptu brillant sur des th\u00e8mes de Rossini et Spontini, op. 3, S. 150", "Via Crucis, S. 53: Station VIII: Die Frauen von Jerusalem", "Grandes \u00e9tudes de Paganini, S. 141: No. 4. Arpeggio in E major", "Hungarian Rhapsody No. 13", "Anfang einer Jugendsonate, S. 692b", "Album-Leaf: Pr\u00e9lude omnitonique, S. 166e", "Grandes \u00e9tudes de Paganini, S. 141: No. 2. Octave in E-flat major", "Sunt lacrymae rerum \u2013 in ungarischen Weise, S. 162d", "Operatic aria, S. 701h/1", "Angiolin dal biondo crin, S. 269/3", "Transcendental \u00c9tude No. 5", "Magyar dalok, S. 242: No. 5 in D-flat major", "A magyarok Istene", "Ballade aus Der fliegende Holl\u00e4nder, S. 441", "Die Macht der Musik", "Hungarian Rhapsody no. 14 in F minor, S. 244 no. 14", "Von der Wiege bis zum Grabe, S. 512: I. Die Wiege", "Klavierst\u00fcck in A-flat major, S. 192 no. 2. Lento assai", "Danza sacra e duetto finale d\u2019Aida, S. 436", "Five Hungarian Folksongs, S. 245: No. 4. Kiss\u00e9 \u00e9l\u00e9nken. Vivace", "Marie-Po\u00e8me, S. 701b", "Tasso, Lamento e Trionfo", "Grande fantaisie sur la tyrolienne de l'op\u00e9ra La fianc\u00e9e de Auber, S. 385/1", "Mephisto Polka", "Aus der Musik von Eduard Lassen zu Hebbels Nibelungen und Goethes Faust, S. 496: II. Faust: 2. Hoffest: Marsch und Polonaise", "Pr\u00e4ludium und Fuge \u00fcber den Namen BACH, S. 260: II. Fuge", "Orage, S. 160 no. 5", "Illustrations du Proph\u00e8te, S. 414: No. 1. Pri\u00e8re - Hymne triomphal - Marche du sacre", "Magyar tempo, S. 241b", "Album-Leaf, S. 167d", "Aus Lohengrin, S. 446: No. 3. Lohengrins Verweis an Elsa", "Hungarian Rhapsody for Orchestra no. 4 in D minor, S. 359/4", "Hexam\u00e9ron, S. 392: VII. Variation V. Vivo e brillante - Fuocoso molto energico - Lento quasi recitativo", "Weihnachtsbaum, S. 186: Nr. 12. Polnisch", "Tarantelle di bravura d\u2019apr\u00e8s la tarantelle de La muette de Portici, S. 386/1", "Magyar dalok, S. 242: No. 4 in C-sharp major", "Apparitions, S. 155: No. 1. Senza lentezza quasi allegretto", "Weihnachtslied \\\"Christus ist geboren\\\", S. 502", "Chor\u00e4le, S. 506a: No. 1. Crux ave benedicta", "Liebestr\u00e4ume no. 2, S. 541a \\\"Gestorben war ich\\\"", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 6. Pater noster, d'apr\u00e8s la Psalmodie de l'\u00c9glise", "Historical Hungarian Portraits, S. 205: No. 3. V\u00f6r\u00f6smarty Mihaly", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 10. Cantique d'amour", "Wie entgehn der Gefahr?", "R\u00e1k\u00f3czi March, S. 242a", "Hungarian Rhapsody no. 10 in E major, S. 244 no. 10", "Au lac de Wallenstadt, S. 160 no. 2", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 7b. Lento", "Muttergottes-Str\u00e4u\u00dflein zum Maimonate", "Wer nie sein Brot mit Tr\u00e4nen a\u00df", "In Liebeslust, S. 318", "Chanson du B\u00e9arn, S. 236 no. 2", "Album-Leaf: Langsam in C-sharp minor, S. 166o", "Il penseroso, S. 157b", "Miserere du Trovatore, S. 433", "Hungarian Coronation Mass, S. 11: VI. Sanctus", "R\u00e9miniscences de Norma, S. 394", "Ann\u00e9es de p\u00e8lerinage", "Fantasie \u00fcber Motive aus Beethovens Ruinen von Athen, S. 389", "Gaudeamus igitur, S. 240/1", "Chor\u00e4le, S. 506a: No. 11. Wer nur den lieben Gott l\u00e4sst walten?", "Romance, S. 169", "Dante Symphony (arrangement for piano)", "\u00c9tude en douze exercices, S. 136: XI. Allegro grazioso", "Hyr\u0107, S. 166m/2", "Valse-caprice no. 6, S.427/6a", "Zigeuner-Epos, S. 695b: No. 5 in D flat major. Tempo giusto", "R\u00e1k\u00f3czi-Marsch, S. 244a", "Album-Leaf: Freudvoll und leidvoll, S. 166n", "Blume und Duft", "Slavimo slavno, Slaveni!, S. 503", "Trois \u00e9tudes de concert, S. 144: I. \\\"Il lamento\\\" in A-flat minor", "\u0421\u043b\u0463\u043f\u043e\u0439, S. 350", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 9. La Lampe du temple (Andante lagrimoso)", "Hungarian Rhapsody No. 1", "O sacrum convivium, S. 674a", "Marche hongroise, S425/2b", "Gondoliera, S. 162 no. 1", "Missa solennis zur Einweihung der Basilika in Gran, S. 9 \\\"Graner Messe\\\": III. Credo. Antande maestoso, risoluto", "Abschied, S. 251", "R\u00e9miniscences des Puritains, S. 390/1", "Impromptu in F-sharp major, S. 191/2, R. 59 \\\"Nocturne\\\"", "Soir\u00e9es de Vienne, S. 427: No. 1 in A-flat major. Allegretto malinconico", "Trois Chansons, S. 510a: No. 2. Avant la bataille", "Dem Andenken Pet\u0151fis", "Wiegenlied, S. 198", "Grande Fantaisie di bravura sur La Clochette de Paganini, S. 420", "Valse \u00e0 capriccio sur deux motifs de Lucia et Parisina de Donizetti, S. 401", "Grand solo de concert, S. 175a", "Sursum corda, S. 163 no. 7", "Hungarian Rhapsody for Orchestra no. 6 in D-flat major \\\"Carnival in Pest\\\", S. 359/6", "Schuberts ungarische Melodien, S. 425a: No. 1. Andante", "Five Hungarian Folksongs, S. 245: No. 3. Lassan. Andante", "Gastibelza, S. 540", "Hexam\u00e9ron, S. 365b: V. Variation III di bravura - Ritornello", "Totentanz, S. 126/1", "Rigoletto", "Weihnachtsbaum, S. 185a: X. [Ehemals]", "Transcendental \u00c9tudes", "Zigeuner-Epos, S. 695b: No. 2 in C major. Andantino", "Variation on a Waltz by Diabelli", "Liebestr\u00e4ume", "Weihnachtsbaum, S. 185a: I. Psallite - Altes Weihnachtslied", "Symphonic Poem no. 2 \\\"Tasso, Lament and Triumph\\\"", "Es muss ein Wunderbares sein, S. 314", "Totentanz, S. 126: IX. Cadenza", "La cloche sonne, S. 238", "Via Crucis, S. 504a: Station II: J\u00e9sus est charg\u00e9 de sa croix", "Berceuse, S. 174/1", "An Frau Minne", "Grande Fantaisie sur des motifs de Soir\u00e9es musicales, S. 422/2", "Ballade no. 2, S. 170a", "Romance oubli\u00e9e, S. 527 bis", "Dante fragment, S. 701e", "Einsam bin ich, nicht allein, S. 453", "Fantasie und Fuge \u00fcber den Choral Ad nos, ad salutarem undam, S. 624: II. Adagio", "Des Tages laute Stimmen schweigen", "Zwei St\u00fccke aus dem Oratorium Christus, S. 498c: No. 2. Das Wunder", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 6 in A major \\\"Attente\\\"", "Mazurka brilliante, S. 221", "Beethoven Symphonies", "Festpolonaise, S. 619a", "Les pr\u00e9ludes", "Two Concert \u00c9tudes", "Album-Leaf: Aus den Mephisto-Walzer. Episode aus Lenaus Faust - Der Tanz in der Dorfschenke, S. 167m", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 2. Hymne du matin", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 7c. Allegro pastorale", "Album-Leaf: Vivace ma non troppo in D-flat major, S. 167g", "Klavierst\u00fcck in F-sharp major, S. 193", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 22 in E flat major \\\"Pester Carneval\\\"", "Album-Leaf: Andante religioso, S. 166h", "Les Jeux d'eaux \u00e0 la Villa d'Este, S. 163 no. 4", "Christus", "Totentanz, S. 525, R. 188", "Enfant, si j'\u00e9tais roi, S. 537", "Nuages gris", "Sz\u00f3zat und Hymnus, S. 486", "Au bord d'une source", "Les Adieux, r\u00eaverie sur un motif de Rom\u00e9o et Juliette, S. 409", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 3. Hymne de la nuit", "Aus der Musik von Eduard Lassen zu Hebbels Nibelungen und Goethes Faust, S. 496: I. Nibelungen: 1. Hagen und Kriemhild", "Grosse Concert-Fantasie aus der Oper Sonnambula, S. 393/3", "C\u00e9l\u00e8bre m\u00e9lodie hongroise, S. 243a", "Oh! quand je dors, S. 282/1", "Adela\u00efde von Beethoven, S. 466: Cadenza ad libitum", "In domum Domini ibimus, S. 505", "Harmonie nach Rossini's Carit\u00e0, S. 701j", "Tre sonetti di Petrarca, S. 158: No. 1. Sonetto XLVII. Benedetto sia il giorno", "Album d'un voyageur, S. 156: I. Impressions et po\u00e9sies: 5. La chapelle de Guillaume Tell", "R\u00e9miniscences de Lucia di Lammermoor, S. 397", "Gebet, S. 265", "De Profundis, S. 121a", "San Francesco, S. 498d", "La romanesca, S. 252a/2", "Historische ungarische Bildnisse, S. 205a: No. 1. Sz\u00e9chenyi Istv\u00e1n", "A holt k\u00f6lt\u0151 szerelme, S. 349", "Pens\u00e9es \\\"Nocturne,\\\" S. 168b", "Album d'un voyageur, S. 156: I. Impressions et po\u00e9sies: 6. Psaume", "Aus Lohengrin, S. 446: No. 1. Festspiel und Brautlied", "Die Ideale", "Album-Leaf in E major, S. 167t", "Der du vom Himmel bist, S. 531 no. 5", "Cavatine de Robert le Diable, S. 412a", "Eine Faust-Symphonie, S. 108: I. Faust. Lento assai", "Puszta-Wehmut, S. 246", "Pastorale, S. 508", "Feuilles d\u2019album, S. 165", "Consolations", "Mariotte \u2013 Valse de Marie, S. 212b", "Den Schutz-Engeln, S. 162a/1", "Magnificat, S. 182a", "Album-Leaf, S. 167c", "Die Schl\u00fcsselblumen", "Album-Leaf: Schlusschor des entfesselten Prometheus. Andante solenne in D-flat major, S. 167q", "Zigeuner-Epos, S. 695b: No. 6 in G minor. Lento", "Via Crucis, S. 504a: Station VI: Sancta Veronica", "Transcendental \u00c9tude No. 7", "R\u00e9miniscences de La Scala, S. 458", "R\u00e9miniscences de Lucrezia Borgia, S. 400: I. Trio du seconde Acte", "Recueillement, S. 204", "Ave Maria (d'Arcadelt), S. 659", "Hamlet, S. 104", "Festpolonaise, S. 230a", "Fantaisie sur des motifs de l'op\u00e9ra Lucrezia Borgia de G. Donizetti, S. 399a", "Seven brilliant variations on a theme by Rossini, S. 149", "Morceau en fa majeur, S. 695", "Ebony Rhapsody", "Krakowiak, S. 166m/4", "Konzertparaphrase \u00fcber Mendelssohns Hochzeitsmarsch und Elfenreigen aus der Musik zu Shakespeares Sommernachtstraum, S. 410", "Canzonetta del Salvator Rosa, S. 157c", "Excelsior!, S. 500", "Responsorien und Antiphonen, S. 30: IV. Sabbato sancto", "\u00c9tude en douze exercices, S. 136: XII. Allegro non troppo", "Deux \u00e9pisodes d'apres le Faust de Lenau, S. 110: II. Der Tanz in der Dorfschenke (Erster Mephisto-Walzer)", "Ouvert\u00fcre zu Tannh\u00e4user, S. 442", "Rondeau fantastique sur un th\u00e8me espagnol \\\"La Contrabandista\\\", S.252", "Geharnischte Lieder, S. 511: No. 2. Nicht gezagt!", "Hexam\u00e9ron, S. 365b: VII. Variation V. Vivo e brillante - Fuocoso molto energico - Lento quasi recitativo", "Les Cloches de Gen\u00e8ve : Nocturne, S. 160 no. 9", "Album Leaf in E major, S. 166a", "Soir\u00e9es de Vienne, S. 427: No. 7 in A major. Allegro spiritoso", "Ann\u00e9es de p\u00e8lerinage : Troisi\u00e8me ann\u00e9e, S. 163", "Valse de bravoure, S. 214 no. 1", "Einleitung und Coda zu Smetanas Polka, S. 570a", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 5a in E major \\\"La Chasse\\\"", "Fantasia on Hungarian Folk Melodies for piano and orchestra, S. 123", "Galop de bal, S. 220", "Marche fun\u00e8bre, S. 163 no. 6", "Drei St\u00fccke aus der Legende der heiligen Elisabeth, S. 498a: Nr. 2. Marsch der Kreuzritter", "Ave Maria IV in G major, S. 545", "Historical Hungarian Portraits, S. 205: No. 2. E\u00f6tv\u00f6s Jozsef", "Via Crucis, S. 53: Station XIV: Jesus wird ins grab gelegt", "Wartburglieder", "Tscherkessenmarsch aus Russlan und Ludmilla, S. 406/2", "Schnitterchor aus den entfesselten Prometheus, S. 507a", "Via Crucis, S. 53: Station VII: Jesus f\u00e4llt zum zweiten Mal", "Allegro maestoso, S. 692c", "Ave Maria in D-flat major, S. 504/2", "Fantasie \u00fcber zwei Motive aus W. A. Mozarts Die Hochzeit des Figaro", "Muttergottes-Str\u00e4usslein zum Mai-Monate, S. 316: Nr. 2. Die Schl\u00fcsselblumen", "Rhapsodie espagnole", "Tre sonetti di Petrarca, S. 270: III. I' vidi in terra angelici costumi", "\u00c9l\u00e9gie sur des motifs du Prince Louis Ferdinand de Prusse, S. 168/2", "Angiolin, S. 531 no. 6", "Il penseroso, S. 161 no. 2", "Die Perle", "Am stillen Herd, S. 448", "La Marseillaise, S. 237", "Seconde marche hongroise, S. 232", "Via Crucis, S. 504a: Station IV: J\u00e9sus rencontre sa tr\u00e8s sainte m\u00e8re", "Via Crucis, S. 504a: Station XI: J\u00e9sus est attach\u00e9 \u00e0 la croix", "Five Hungarian Folksongs, S. 245: No. 1. Lassan. Lento", "Fantaisie romantique sur deux m\u00e9lodies suisses", "Des toten Dichters Liebe", "Grande fantaisie sur des th\u00e8mes de l'op\u00e9ra Niobe, S. 419", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 2. Ave Maria", "Lilie, S. 166m/1", "Cantico del sol di San Francisco d'Assisi", "Les Sab\u00e9ennes, berceuse de l\u2019op\u00e9ra La Reine de Saba, S. 408", "Hungarian Rhapsody no. 16 in A minor, S. 244 no. 16", "Pace non trovo", "Zwei Orchesters\u00e4tze aus dem Oratorium Christus, S. 498b: Nr. 1. Hirtengesang an der Krippe", "\u00c9tude en douze exercices, S. 136: VIII. Allegro con spirito", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 7a. Allegro", "Piano Concerto in E-flat major, S. 125a", "R\u00e9miniscences de Don Juan", "Klavierst\u00fcck in A-flat major, S. 189", "Festkantate zur Enth\u00fcllung des Beethoven-Denkmals in Bonn, S. 67: II. Allegro deciso", "Fantasie \u00fcber Motive aus Beethovens Ruinen von Athen, S. 122", "Chor\u00e4le, S. 506a: No. 3. Meine Seele erhebt den Herrn (Der Kirchensegen, Psalm 67)", "\u00c9tude en douze exercices, S. 136: IV. Allegro grazioso", "Zwei St\u00fccke aus der heiligen Elisabeth, S. 693a: No. 1. Das Rosenmirakel", "Album Leaf in G major (Dante-Symphony progression), S. 167f", "Wie singt die Lerche sch\u00f6n"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1891", "prediction": ["# Reasoning Path:\nRusso-Japanese War -> time.event.locations -> Korean Peninsula -> common.topic.image -> Location of the Korean Peninsula\n# Answer:\nKorean Peninsula", "# Reasoning Path:\nRusso-Japanese War -> military.military_conflict.force_strengths -> m.011pns0q -> military.force_strength.resource -> 28 cm Howitzer L/10\n# Answer:\nm.011pns0q", "# Reasoning Path:\nRusso-Japanese War -> time.event.locations -> Yellow Sea -> common.topic.article -> m.0df4r\n# Answer:\nYellow Sea", "# Reasoning Path:\nRusso-Japanese War -> time.event.includes_event -> Battle of Chemulpo Bay -> time.event.locations -> Korea\n# Answer:\nBattle of Chemulpo Bay", "# Reasoning Path:\nRusso-Japanese War -> time.event.locations -> Korean Peninsula -> common.topic.image -> Map of the Korean peninsula, as recognized by the Republic of Korea\n# Answer:\nKorean Peninsula", "# Reasoning Path:\nRusso-Japanese War -> time.event.locations -> Korean Peninsula -> common.topic.subject_of -> Crisis on the Korean peninsula : how to deal with a nuclear North Korea\n# Answer:\nKorean Peninsula", "# Reasoning Path:\nRusso-Japanese War -> time.event.locations -> Manchuria -> location.location.events -> Battle of Hsimucheng\n# Answer:\nManchuria", "# Reasoning Path:\nRusso-Japanese War -> time.event.locations -> Manchuria -> common.topic.notable_for -> g.125525748\n# Answer:\nManchuria", "# Reasoning Path:\nRusso-Japanese War -> time.event.includes_event -> Battle of Hsimucheng -> time.event.locations -> Liaoyang\n# Answer:\nBattle of Hsimucheng", "# Reasoning Path:\nRusso-Japanese War -> time.event.locations -> Korean Peninsula -> common.topic.subject_of -> Lost in Seoul: and other discoveries on the Korean peninsula\n# Answer:\nKorean Peninsula"], "ground_truth": ["Yellow Sea", "Korean Peninsula", "Manchuria"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1892", "prediction": ["# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> time.event.instance_of_recurring_event -> Stanley Cup Finals\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2012 Stanley Cup Finals -> common.topic.article -> m.0jt52q2\n# Answer:\n2012 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2012 Stanley Cup Finals -> sports.sports_championship_event.runner_up -> New Jersey Devils\n# Answer:\n2012 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.roster -> g.11b76d7nzz\n# Answer:\ng.11b76d7nzz", "# Reasoning Path:\nLos Angeles Kings -> award.award_nominee.award_nominations -> m.010_rx6h -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nm.010_rx6h", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup Finals\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2012 Stanley Cup Finals -> time.event.locations -> Prudential Center\n# Answer:\n2012 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> award.award_nominee.award_nominations -> m.010_rx6h -> award.award_nomination.award_nominee -> New York Rangers\n# Answer:\nm.010_rx6h"], "ground_truth": ["2014 Stanley Cup Finals", "2012 Stanley Cup Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1893", "prediction": ["# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\ng.12cp_k6sh", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\ng.1hhc3_4cn", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> common.topic.notable_types -> Governmental Body\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3f_h6\n# Answer:\ng.1hhc3f_h6", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> government.governmental_body.component_bodies -> House of Representatives\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> government.governmental_body.component_bodies -> Shura Council\n# Answer:\nParliament of Egypt"], "ground_truth": ["Egyptian pound"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1894", "prediction": ["# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nm.06sbpz2", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nm.06sbq0l", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.season -> 1953 Major League Baseball season\n# Answer:\nm.06sbpz2", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nm.06sbpzt", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.season -> 1954 Major League Baseball season\n# Answer:\nm.06sbpzt", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.season -> 1956 Major League Baseball Season\n# Answer:\nm.06sbq0l", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> people.place_of_interment.interred_here -> Abraham Cohn\n# Answer:\nCypress Hills Cemetery", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> common.topic.notable_types -> Cemetery\n# Answer:\nCypress Hills Cemetery", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> common.topic.article -> m.03vbmy\n# Answer:\nCypress Hills Cemetery", "# Reasoning Path:\nJackie Robinson -> influence.influence_node.influenced -> Steve Hofstetter -> people.person.profession -> Author\n# Answer:\nSteve Hofstetter"], "ground_truth": ["Montreal Royals", "Kansas City Monarchs", "Brooklyn Dodgers"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2608695652173913, "path_precision": 0.3, "path_recall": 0.23076923076923078, "path_ans_f1": 0.3157894736842105, "path_ans_precision": 0.3, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1895", "prediction": ["# Reasoning Path:\nSaint Lawrence River -> geography.river.mouth -> Gulf of Saint Lawrence -> base.scubadiving.ecoregion.contained_by -> Cold Temperate Northwest Atlantic\n# Answer:\nGulf of Saint Lawrence", "# Reasoning Path:\nSaint Lawrence River -> geography.river.mouth -> Gulf of Saint Lawrence -> base.ontologies.ontology_instance.equivalent_instances -> m.09kmh6l\n# Answer:\nGulf of Saint Lawrence", "# Reasoning Path:\nSaint Lawrence River -> location.location.partially_containedby -> United States of America -> location.location.partially_containedby -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nSaint Lawrence River -> geography.river.mouth -> Gulf of Saint Lawrence -> location.location.events -> Action of 8 June 1755\n# Answer:\nGulf of Saint Lawrence", "# Reasoning Path:\nSaint Lawrence River -> location.location.partially_containedby -> United States of America -> meteorology.cyclone_affected_area.cyclones -> Hurricane Isabel\n# Answer:\nUnited States of America", "# Reasoning Path:\nSaint Lawrence River -> geography.river.mouth -> Gulf of Saint Lawrence -> location.location.events -> Battle of the Atlantic\n# Answer:\nGulf of Saint Lawrence", "# Reasoning Path:\nSaint Lawrence River -> location.location.partially_containedby -> Canada -> base.locations.countries.continent -> North America\n# Answer:\nCanada", "# Reasoning Path:\nSaint Lawrence River -> location.location.partially_containedby -> New York -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nNew York", "# Reasoning Path:\nSaint Lawrence River -> geography.river.mouth -> Gulf of Saint Lawrence -> location.location.events -> Battle of the St. Lawrence\n# Answer:\nGulf of Saint Lawrence", "# Reasoning Path:\nSaint Lawrence River -> location.location.partially_containedby -> Canada -> location.location.partially_contains -> Saint Francis River\n# Answer:\nCanada"], "ground_truth": ["Gulf of Saint Lawrence"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1896", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> American Samoa\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\ng.125_r6z32", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guam\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Germany\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Germany\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nm.010r7f0z", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Marshall Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Mexico\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Mexico\n# Answer:\nFederal republic"], "ground_truth": ["Constitutional republic", "Presidential system", "Federal republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1897", "prediction": ["# Reasoning Path:\nAriel -> fictional_universe.fictional_character.siblings -> m.065hp2g -> fictional_universe.sibling_relationship_of_fictional_characters.siblings -> Aquata\n# Answer:\nm.065hp2g", "# Reasoning Path:\nAriel -> fictional_universe.fictional_character.parents -> The Sea King -> fictional_universe.fictional_character.children -> Aquata\n# Answer:\nThe Sea King", "# Reasoning Path:\nAriel -> fictional_universe.fictional_character.siblings -> m.065hpgl -> fictional_universe.sibling_relationship_of_fictional_characters.siblings -> Andrina\n# Answer:\nm.065hpgl", "# Reasoning Path:\nAriel -> fictional_universe.fictional_character.parents -> The Sea King -> fictional_universe.fictional_character.children -> Adella\n# Answer:\nThe Sea King", "# Reasoning Path:\nAriel -> fictional_universe.fictional_character.parents -> The Sea King -> fictional_universe.fictional_character.species -> Merfolk\n# Answer:\nThe Sea King", "# Reasoning Path:\nAriel -> fictional_universe.fictional_character.parents -> Queen Athena -> fictional_universe.fictional_character.children -> Attina\n# Answer:\nQueen Athena", "# Reasoning Path:\nAriel -> common.topic.article -> m.0cl19n\n# Answer:\nm.0cl19n", "# Reasoning Path:\nAriel -> fictional_universe.fictional_character.siblings -> m.065hpgf -> fictional_universe.sibling_relationship_of_fictional_characters.siblings -> Arista\n# Answer:\nm.065hpgf", "# Reasoning Path:\nAriel -> fictional_universe.fictional_character.parents -> The Sea King -> fictional_universe.fictional_character.children -> Alana\n# Answer:\nThe Sea King", "# Reasoning Path:\nAriel -> fictional_universe.fictional_character.parents -> The Sea King -> tv.tv_character.appeared_in_tv_program -> m.03lhbdc\n# Answer:\nThe Sea King"], "ground_truth": ["Andrina", "Attina", "Alana", "Arista", "Adella", "Aquata"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.608695652173913, "path_precision": 0.7, "path_recall": 0.5384615384615384, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1898", "prediction": ["# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> God\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> Dr. Elmer Hartman\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> fictional_universe.fictional_character_creator.fictional_characters_created -> Glenn Quagmire\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> Glenn Quagmire\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.05tw61d\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> common.topic.notable_for -> g.1255sxsvl\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> fictional_universe.fictional_character_creator.fictional_characters_created -> Stewie Griffin\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> award.award_winner.awards_won -> m.0ly_pz4\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.works_set_here -> Family Guy Video Game!\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.02kk65p\n# Answer:\nSeth MacFarlane"], "ground_truth": ["Glenn Quagmire", "Tom Tucker", "Dr. Elmer Hartman", "Carter Pewterschmidt", "Jake Tucker", "Jasper", "Brian Griffin", "Kevin Swanson", "Seamus", "God", "Kool-aid Guy", "Nate Griffin", "Mickey McFinnegan", "Peter Griffin", "Stewie Griffin"], "ans_acc": 0.26666666666666666, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25641025641025644, "path_precision": 0.5, "path_recall": 0.1724137931034483, "path_ans_f1": 0.3478260869565218, "path_ans_precision": 0.5, "path_ans_recall": 0.26666666666666666}
{"id": "WebQTest-1899", "prediction": ["# Reasoning Path:\nWilliam McKinley -> people.person.spouse_s -> m.0j4k7z3 -> people.marriage.spouse -> Ida Saxton McKinley\n# Answer:\nm.0j4k7z3", "# Reasoning Path:\nWilliam McKinley -> people.person.spouse_s -> m.0j4k7z3 -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0j4k7z3", "# Reasoning Path:\nWilliam McKinley -> people.person.children -> Ida McKinley -> people.person.parents -> Ida Saxton McKinley\n# Answer:\nIda McKinley", "# Reasoning Path:\nWilliam McKinley -> people.person.children -> Ida McKinley -> common.topic.notable_types -> Deceased Person\n# Answer:\nIda McKinley", "# Reasoning Path:\nWilliam McKinley -> people.person.children -> Katherine McKinley -> people.person.parents -> Ida Saxton McKinley\n# Answer:\nKatherine McKinley", "# Reasoning Path:\nWilliam McKinley -> people.person.children -> Ida McKinley -> common.topic.notable_for -> g.1259fcsxc\n# Answer:\nIda McKinley", "# Reasoning Path:\nWilliam McKinley -> people.person.children -> Katherine McKinley -> people.person.nationality -> United States of America\n# Answer:\nKatherine McKinley", "# Reasoning Path:\nWilliam McKinley -> people.person.children -> Katherine McKinley -> common.topic.notable_for -> g.125g47nt2\n# Answer:\nKatherine McKinley", "# Reasoning Path:\nWilliam McKinley -> book.author.works_written -> Papers, ca. 1847-1902 -> common.topic.notable_types -> Book\n# Answer:\nPapers, ca. 1847-1902", "# Reasoning Path:\nWilliam McKinley -> book.author.works_written -> Speeches And Addresses Of William McKinley -> common.topic.notable_types -> Book\n# Answer:\nSpeeches And Addresses Of William McKinley"], "ground_truth": ["Ida Saxton McKinley"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> geography.river.cities -> Denham Springs\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wg8__0 -> location.partial_containment_relationship.partially_contains -> Sabine River\n# Answer:\nm.0wg8__0", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Macon -> location.location.partially_containedby -> Arkansas\n# Answer:\nBayou Macon", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> geography.river.mouth -> Lake Maurepas\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Bartholomew -> location.location.partially_contained_by -> m.0wjpmv2\n# Answer:\nBayou Bartholomew", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Bartholomew -> common.topic.notable_for -> g.125bn2xvk\n# Answer:\nBayou Bartholomew"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-190", "prediction": ["# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> theater.theater_production_staff_role.people_who_have_had_this_role -> m.0111n9h6\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> common.topic.subject_of -> Dum-Doodles\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> theater.theater_production_staff_role.people_who_have_had_this_role -> m.0_grv_v\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Television producer -> common.topic.subjects -> Billy Sorrentino\n# Answer:\nTelevision producer", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Television producer -> base.lightweight.profession.specialization_of -> Producers and Directors\n# Answer:\nTelevision producer", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> theater.theater_production_staff_role.people_who_have_had_this_role -> m.0w3l6yj\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> base.descriptive_names.names.descriptive_name -> m.01066c2h\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Artist -> people.profession.specializations -> Illustrator\n# Answer:\nArtist", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Television producer -> common.topic.subjects -> Michael Palance\n# Answer:\nTelevision producer", "# Reasoning Path:\nTheodore Lesieg -> book.author.works_written -> ARE YOU MY MOTHER MINI PB -> common.topic.notable_for -> g.125fbm0pz\n# Answer:\nARE YOU MY MOTHER MINI PB"], "ground_truth": ["Songwriter", "Cartoonist", "Poet", "Screenwriter", "Writer", "Artist", "Television producer", "Animator", "Visual Artist", "Illustrator", "Film Producer"], "ans_acc": 0.36363636363636365, "ans_hit": 1, "ans_f1": 0.41860465116279066, "ans_precission": 0.9, "ans_recall": 0.2727272727272727, "path_f1": 0.41860465116279066, "path_precision": 0.9, "path_recall": 0.2727272727272727, "path_ans_f1": 0.5179856115107914, "path_ans_precision": 0.9, "path_ans_recall": 0.36363636363636365}
{"id": "WebQTest-1900", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.actor -> James Earl Jones\n# Answer:\nm.02nv74t", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.actor -> James Earl Jones\n# Answer:\nm.02nwtmz", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.film -> Star Wars\n# Answer:\nm.02nv74t", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Uncredited\n# Answer:\nm.02nv74t", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Voice\n# Answer:\nm.02nv74t", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.film -> Return of the Jedi\n# Answer:\nm.02nwtmz"], "ground_truth": ["David Prowse", "James Earl Jones"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3, "path_precision": 0.3, "path_recall": 0.3, "path_ans_f1": 0.37499999999999994, "path_ans_precision": 0.3, "path_ans_recall": 0.5}
{"id": "WebQTest-1901", "prediction": ["# Reasoning Path:\nKevin James -> people.person.education -> m.0cr6cc4 -> education.education.institution -> State University of New York at Cortland\n# Answer:\nm.0cr6cc4", "# Reasoning Path:\nKevin James -> people.person.education -> m.0cr6cbz -> education.education.institution -> Ward Melville High School\n# Answer:\nm.0cr6cbz", "# Reasoning Path:\nKevin James -> film.actor.film -> m.010_t38m -> film.performance.character -> Will Cooper\n# Answer:\nm.010_t38m", "# Reasoning Path:\nKevin James -> film.actor.film -> m.010x46kw -> film.performance.film -> Paul Blart: Mall Cop 2\n# Answer:\nm.010x46kw", "# Reasoning Path:\nKevin James -> film.actor.film -> m.010_t38m -> film.performance.film -> Pixels\n# Answer:\nm.010_t38m", "# Reasoning Path:\nKevin James -> film.actor.film -> m.012zk5r2 -> film.performance.film -> Hotel Transylvania 2\n# Answer:\nm.012zk5r2", "# Reasoning Path:\nKevin James -> tv.tv_actor.guest_roles -> m.0bv0l6h -> tv.tv_guest_role.episodes_appeared_in -> Team Nogueira vs Team Mir Finale\n# Answer:\nm.0bv0l6h", "# Reasoning Path:\nKevin James -> film.actor.film -> m.010x46kw -> film.performance.character -> Paul Blart\n# Answer:\nm.010x46kw", "# Reasoning Path:\nKevin James -> tv.tv_actor.guest_roles -> m.0bv13gs -> tv.tv_guest_role.episodes_appeared_in -> Episode 313\n# Answer:\nm.0bv13gs", "# Reasoning Path:\nKevin James -> tv.tv_actor.guest_roles -> m.0bv16cy -> tv.tv_guest_role.episodes_appeared_in -> 1/5/2001\n# Answer:\nm.0bv16cy"], "ground_truth": ["State University of New York at Cortland"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1902", "prediction": ["# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ng.1254zbtvv", "# Reasoning Path:\nAdolf Hitler -> people.person.place_of_birth -> Braunau am Inn -> location.location.containedby -> Austria\n# Answer:\nBraunau am Inn", "# Reasoning Path:\nAdolf Hitler -> people.person.place_of_birth -> Braunau am Inn -> common.topic.webpage -> m.03l75z9\n# Answer:\nBraunau am Inn", "# Reasoning Path:\nAdolf Hitler -> people.person.place_of_birth -> Braunau am Inn -> location.location.people_born_here -> Angela Hitler\n# Answer:\nBraunau am Inn", "# Reasoning Path:\nAdolf Hitler -> people.person.place_of_birth -> Braunau am Inn -> location.location.containedby -> Braunau am Inn District\n# Answer:\nBraunau am Inn", "# Reasoning Path:\nAdolf Hitler -> people.person.place_of_birth -> Braunau am Inn -> location.location.people_born_here -> Daniela Raschhofer\n# Answer:\nBraunau am Inn", "# Reasoning Path:\nAdolf Hitler -> people.person.place_of_birth -> Braunau am Inn -> location.location.containedby -> Upper Austria\n# Answer:\nBraunau am Inn", "# Reasoning Path:\nAdolf Hitler -> people.person.place_of_birth -> Braunau am Inn -> location.location.people_born_here -> David Schie\u00dfl\n# Answer:\nBraunau am Inn", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09wf140", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.resource -> Another Megan Fox letter -- and this one sings her praises!\n# Answer:\nm.09wf140"], "ground_truth": ["1889-04-20"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1904", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> common.topic.notable_types -> Zoo\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Men's rings\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.geolocation -> m.0clwfck\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia State Capitol -> location.location.containedby -> Georgia\n# Answer:\nGeorgia State Capitol", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> travel.transport_terminus.travel_destinations_served -> m.052zwlt\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Women's artistic team all-around\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia State Capitol -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.05h6b27\n# Answer:\nGeorgia State Capitol", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> United States of America\n# Answer:\nGeorgia Dome"], "ground_truth": ["Omni Coliseum", "Cobb Energy Performing Arts Centre", "Georgia Dome", "Atlanta Symphony Orchestra", "Center for Puppetry Arts", "Georgia State Capitol", "World of Coca-Cola", "Six Flags Over Georgia", "Jimmy Carter Library and Museum", "Six Flags White Water", "Centennial Olympic Park", "Margaret Mitchell House & Museum", "Masquerade", "Georgia Aquarium", "Atlanta Ballet", "Woodruff Arts Center", "Zoo Atlanta", "Atlanta Cyclorama & Civil War Museum", "CNN Center", "Variety Playhouse", "Atlanta History Center", "Fernbank Museum of Natural History", "Fox Theatre", "Peachtree Road Race", "Fernbank Science Center", "Atlanta Jewish Film Festival", "Hyatt Regency Atlanta", "Georgia World Congress Center", "Underground Atlanta", "Martin Luther King, Jr. National Historic Site", "Arbor Place Mall", "Turner Field", "Philips Arena", "The Tabernacle", "Atlanta Marriott Marquis", "Four Seasons Hotel Atlanta"], "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0.15254237288135591, "ans_precission": 0.9, "ans_recall": 0.08333333333333333, "path_f1": 0.15254237288135591, "path_precision": 0.9, "path_recall": 0.08333333333333333, "path_ans_f1": 0.1978021978021978, "path_ans_precision": 0.9, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-1905", "prediction": ["# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> O\u015bwi\u0119cim -> location.location.containedby -> O\u015bwi\u0119cim County\n# Answer:\nO\u015bwi\u0119cim", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> Poland -> location.location.containedby -> Europe\n# Answer:\nPoland", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> O\u015bwi\u0119cim -> location.location.containedby -> Lesser Poland\n# Answer:\nO\u015bwi\u0119cim", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> O\u015bwi\u0119cim -> sports.sports_team_location.teams -> TH Unia O\u015bwi\u0119cim\n# Answer:\nO\u015bwi\u0119cim", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> Auschwitz-Birkenau State Museum -> location.location.containedby -> Poland\n# Answer:\nAuschwitz-Birkenau State Museum", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> O\u015bwi\u0119cim -> location.location.people_born_here -> Agnieszka Wr\u00f3blewska\n# Answer:\nO\u015bwi\u0119cim", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.people_born_here -> Margherita Horowitz -> people.person.gender -> Female\n# Answer:\nMargherita Horowitz", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> O\u015bwi\u0119cim -> location.location.people_born_here -> Aneta Micha\u0142ek\n# Answer:\nO\u015bwi\u0119cim", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> Poland -> base.aareas.schema.administrative_area.administrative_children -> Greater Poland Voivodeship\n# Answer:\nPoland", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> O\u015bwi\u0119cim -> location.location.people_born_here -> Anna Jurkiewicz\n# Answer:\nO\u015bwi\u0119cim"], "ground_truth": ["O\u015bwi\u0119cim", "Auschwitz-Birkenau State Museum", "Poland"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.2, "path_recall": 0.3333333333333333, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1907", "prediction": ["# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.article -> m.04lt88\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.webpage -> m.04m2gwk\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.statistical_region.co2_emissions_per_capita -> g.1245_1qfx\n# Answer:\ng.1245_1qfx", "# Reasoning Path:\nDominican Republic -> location.statistical_region.internet_users_percent_population -> g.11b60vv5zn\n# Answer:\ng.11b60vv5zn", "# Reasoning Path:\nDominican Republic -> location.statistical_region.co2_emissions_per_capita -> g.1245_22j7\n# Answer:\ng.1245_22j7", "# Reasoning Path:\nDominican Republic -> location.statistical_region.co2_emissions_per_capita -> g.1245_4lf3\n# Answer:\ng.1245_4lf3", "# Reasoning Path:\nDominican Republic -> location.statistical_region.internet_users_percent_population -> g.1245_22jn\n# Answer:\ng.1245_22jn", "# Reasoning Path:\nDominican Republic -> location.statistical_region.internet_users_percent_population -> g.1245_m66j\n# Answer:\ng.1245_m66j"], "ground_truth": ["Dominican peso"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1909", "prediction": ["# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> media_common.netflix_genre.titles -> Adelheid\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> common.topic.notable_types -> Human Language\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> media_common.netflix_genre.titles -> All My Loved Ones\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Hungarian language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nHungarian language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Hungarian language -> common.topic.notable_types -> Human Language\n# Answer:\nHungarian language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Greek Language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nGreek Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> media_common.netflix_genre.titles -> Autumn Spring\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Hungarian language -> language.human_language.region -> Europe\n# Answer:\nHungarian language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Hungarian language -> language.human_language.countries_spoken_in -> Croatia\n# Answer:\nHungarian language"], "ground_truth": ["Hungarian language", "Slovak Language", "German Language", "Czech Language", "Rusyn Language", "Croatian language", "Ukrainian Language", "Bulgarian Language", "Russian Language", "Serbian language", "Romani language", "Polish Language", "Greek Language"], "ans_acc": 0.23076923076923078, "ans_hit": 1, "ans_f1": 0.375, "ans_precission": 1.0, "ans_recall": 0.23076923076923078, "path_f1": 0.375, "path_precision": 1.0, "path_recall": 0.23076923076923078, "path_ans_f1": 0.375, "path_ans_precision": 1.0, "path_ans_recall": 0.23076923076923078}
{"id": "WebQTest-191", "prediction": ["# Reasoning Path:\nDrew Brees -> people.person.education -> m.03gkndb -> education.education.institution -> Purdue University\n# Answer:\nm.03gkndb", "# Reasoning Path:\nDrew Brees -> people.person.education -> m.052vk4c -> education.education.institution -> Westlake High School\n# Answer:\nm.052vk4c", "# Reasoning Path:\nDrew Brees -> people.person.education -> m.0j_8p_y -> education.education.institution -> Stanford Graduate School of Business\n# Answer:\nm.0j_8p_y", "# Reasoning Path:\nDrew Brees -> people.person.education -> m.03gkndb -> education.education.major_field_of_study -> Industrial management\n# Answer:\nm.03gkndb", "# Reasoning Path:\nDrew Brees -> american_football.football_player.games -> m.07nvq27 -> american_football.player_game_statistics.team -> New Orleans Saints\n# Answer:\nm.07nvq27", "# Reasoning Path:\nDrew Brees -> people.person.parents -> Eugene Wilson Brees II -> people.person.gender -> Male\n# Answer:\nEugene Wilson Brees II", "# Reasoning Path:\nDrew Brees -> american_football.football_player.games -> m.07sh5y9 -> american_football.player_game_statistics.team -> New Orleans Saints\n# Answer:\nm.07sh5y9", "# Reasoning Path:\nDrew Brees -> american_football.football_player.games -> m.07nvq27 -> american_football.player_game_statistics.season -> 2008 NFL season\n# Answer:\nm.07nvq27", "# Reasoning Path:\nDrew Brees -> american_football.football_player.games -> m.07tdyjy -> american_football.player_game_statistics.team -> New Orleans Saints\n# Answer:\nm.07tdyjy", "# Reasoning Path:\nDrew Brees -> people.person.parents -> Eugene Wilson Brees II -> people.person.spouse_s -> m.0w44yvs\n# Answer:\nEugene Wilson Brees II"], "ground_truth": ["Purdue University", "Stanford Graduate School of Business"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1910", "prediction": ["# Reasoning Path:\nWilliam Shakespeare -> people.person.place_of_birth -> Stratford-upon-Avon -> location.location.people_born_here -> Edmund Shakespeare\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.person.place_of_birth -> Stratford-upon-Avon -> location.location.people_born_here -> Elizabeth Barnard\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.person.place_of_birth -> Stratford-upon-Avon -> travel.travel_destination.tourist_attractions -> Church of the Holy Trinity, Stratford-upon-Avon\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.deceased_person.place_of_death -> Stratford-upon-Avon -> location.location.people_born_here -> Edmund Shakespeare\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.person.place_of_birth -> Stratford-upon-Avon -> location.location.people_born_here -> Gilbert Shakespeare\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.person.place_of_birth -> Stratford-upon-Avon -> location.location.containedby -> England\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.person.place_of_birth -> Stratford-upon-Avon -> travel.travel_destination.tourist_attractions -> Royal Shakespeare Theatre\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.deceased_person.place_of_death -> Stratford-upon-Avon -> location.location.people_born_here -> Elizabeth Barnard\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.deceased_person.place_of_death -> Stratford-upon-Avon -> travel.travel_destination.tourist_attractions -> Church of the Holy Trinity, Stratford-upon-Avon\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.person.place_of_birth -> Stratford-upon-Avon -> travel.travel_destination.tourist_attractions -> Shakespeare's Birthplace\n# Answer:\nStratford-upon-Avon"], "ground_truth": ["Stratford-upon-Avon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1911", "prediction": ["# Reasoning Path:\nF. Scott Fitzgerald -> people.person.education -> m.02h52nm -> education.education.institution -> Princeton University\n# Answer:\nm.02h52nm", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.education -> m.0n0mxgb -> education.education.institution -> St. Paul Academy and Summit School\n# Answer:\nm.0n0mxgb", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.education -> m.0n0klvz -> education.education.institution -> Nardin Academy\n# Answer:\nm.0n0klvz", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.education -> m.02h52nm -> freebase.valuenotation.has_value -> Degree\n# Answer:\nm.02h52nm", "# Reasoning Path:\nF. Scott Fitzgerald -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Male\n# Answer:\nheart attack", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.education -> m.02h52nm -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nm.02h52nm", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.education -> m.0n0mxgb -> freebase.valuenotation.has_value -> Degree\n# Answer:\nm.0n0mxgb", "# Reasoning Path:\nF. Scott Fitzgerald -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.parent_disease -> Cardiovascular disease\n# Answer:\nheart attack", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.education -> m.02h52nm -> freebase.valuenotation.has_value -> Minor\n# Answer:\nm.02h52nm", "# Reasoning Path:\nF. Scott Fitzgerald -> people.deceased_person.cause_of_death -> heart attack -> people.cause_of_death.parent_cause_of_death -> Cardiovascular disease\n# Answer:\nheart attack"], "ground_truth": ["Princeton University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1912", "prediction": ["# Reasoning Path:\nJoan Crawford -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Diabetes mellitus\n# Answer:\nheart attack", "# Reasoning Path:\nJoan Crawford -> people.deceased_person.cause_of_death -> heart attack -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nheart attack", "# Reasoning Path:\nJoan Crawford -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Male\n# Answer:\nheart attack", "# Reasoning Path:\nJoan Crawford -> people.deceased_person.cause_of_death -> Pancreatic cancer -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nJoan Crawford -> film.actor.film -> m.0115tf9q -> film.performance.character -> Della Chappell\n# Answer:\nm.0115tf9q", "# Reasoning Path:\nJoan Crawford -> tv.tv_actor.guest_roles -> m.0bv_0kc -> tv.tv_guest_role.episodes_appeared_in -> Strange Witness\n# Answer:\nm.0bv_0kc", "# Reasoning Path:\nJoan Crawford -> film.actor.film -> m.0115tf9q -> film.performance.film -> Della\n# Answer:\nm.0115tf9q", "# Reasoning Path:\nJoan Crawford -> tv.tv_actor.guest_roles -> m.0bv_6zj -> tv.tv_guest_role.episodes_appeared_in -> The 20th Annual Academy Awards\n# Answer:\nm.0bv_6zj", "# Reasoning Path:\nJoan Crawford -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.icd_9_cm_classification.includes_classifications -> Malignant neoplasm of body of pancreas\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nJoan Crawford -> film.actor.film -> m.01xxsyh -> film.performance.character -> Lane Bellamy\n# Answer:\nm.01xxsyh"], "ground_truth": ["heart attack", "Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1913", "prediction": ["# Reasoning Path:\nRob Kardashian -> people.person.education -> m.0n1n2gl -> education.education.institution -> USC Marshall School of Business\n# Answer:\nm.0n1n2gl", "# Reasoning Path:\nRob Kardashian -> people.person.education -> m.0n1n2gl -> freebase.valuenotation.has_value -> Degree\n# Answer:\nm.0n1n2gl", "# Reasoning Path:\nRob Kardashian -> celebrities.celebrity.sexual_relationships -> m.0nh4s8y -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nm.0nh4s8y", "# Reasoning Path:\nRob Kardashian -> people.person.education -> m.0n1n2gl -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nm.0n1n2gl", "# Reasoning Path:\nRob Kardashian -> celebrities.celebrity.sexual_relationships -> m.0nh4s8y -> celebrities.romantic_relationship.celebrity -> Rita Ora\n# Answer:\nm.0nh4s8y", "# Reasoning Path:\nRob Kardashian -> people.person.education -> m.0n1n2gl -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nm.0n1n2gl", "# Reasoning Path:\nRob Kardashian -> people.person.sibling_s -> m.0113v0_l -> people.sibling_relationship.sibling -> Brandon Jenner\n# Answer:\nm.0113v0_l", "# Reasoning Path:\nRob Kardashian -> people.person.sibling_s -> m.04d12sk -> people.sibling_relationship.sibling -> Kim Kardashian\n# Answer:\nm.04d12sk", "# Reasoning Path:\nRob Kardashian -> people.person.sibling_s -> m.071f_w4 -> people.sibling_relationship.sibling -> Kourtney Kardashian\n# Answer:\nm.071f_w4", "# Reasoning Path:\nRob Kardashian -> people.person.sibling_s -> m.0113v0_l -> people.sibling_relationship.sibling -> Brody Jenner\n# Answer:\nm.0113v0_l"], "ground_truth": ["USC Marshall School of Business"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1914", "prediction": ["# Reasoning Path:\nAllen Iverson -> sports.sports_award_winner.awards -> m.02kbc0l -> sports.sports_award.season -> 1996\u201397 NBA season\n# Answer:\nm.02kbc0l", "# Reasoning Path:\nAllen Iverson -> sports.sports_award_winner.awards -> m.02kbc0l -> sports.sports_award.team -> Philadelphia 76ers\n# Answer:\nm.02kbc0l", "# Reasoning Path:\nAllen Iverson -> sports.sports_award_winner.awards -> m.04ynxpm -> sports.sports_award.season -> 2004\u201305 NBA season\n# Answer:\nm.04ynxpm", "# Reasoning Path:\nAllen Iverson -> sports.sports_award_winner.awards -> m.04ynxq3 -> sports.sports_award.season -> 2000\u201301 NBA season\n# Answer:\nm.04ynxq3", "# Reasoning Path:\nAllen Iverson -> basketball.basketball_player.player_statistics -> m.04qrrxn -> basketball.basketball_player_stats.season -> 2000\u201301 NBA season\n# Answer:\nm.04qrrxn", "# Reasoning Path:\nAllen Iverson -> film.person_or_entity_appearing_in_film.films -> m.01149f6y -> film.personal_film_appearance.film -> Iverson\n# Answer:\nm.01149f6y", "# Reasoning Path:\nAllen Iverson -> sports.sports_award_winner.awards -> m.04ynxpm -> sports.sports_award.award -> NBA All-Star Game Most Valuable Player Award\n# Answer:\nm.04ynxpm", "# Reasoning Path:\nAllen Iverson -> sports.sports_award_winner.awards -> m.04ynxq3 -> sports.sports_award.award -> NBA All-Star Game Most Valuable Player Award\n# Answer:\nm.04ynxq3", "# Reasoning Path:\nAllen Iverson -> basketball.basketball_player.player_statistics -> m.04qrrxn -> basketball.basketball_player_stats.team -> Philadelphia 76ers\n# Answer:\nm.04qrrxn", "# Reasoning Path:\nAllen Iverson -> basketball.basketball_player.player_statistics -> m.04qct42 -> basketball.basketball_player_stats.season -> 2002\u201303 NBA season\n# Answer:\nm.04qct42"], "ground_truth": ["2000\u201301 NBA season"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1915", "prediction": ["# Reasoning Path:\nHarry S. Truman -> people.person.education -> m.0n19ssz -> education.education.institution -> University of Missouri\u2013Kansas City School of Law\n# Answer:\nm.0n19ssz", "# Reasoning Path:\nHarry S. Truman -> people.person.education -> m.04t5ls_ -> education.education.institution -> University of Missouri\u2013Kansas City\n# Answer:\nm.04t5ls_", "# Reasoning Path:\nHarry S. Truman -> people.person.education -> m.04hc6_w -> education.education.institution -> William Chrisman High School\n# Answer:\nm.04hc6_w", "# Reasoning Path:\nHarry S. Truman -> people.person.education -> m.04t5ls_ -> education.education.major_field_of_study -> Law\n# Answer:\nm.04t5ls_", "# Reasoning Path:\nHarry S. Truman -> government.political_appointer.appointees -> m.03p40b5 -> government.government_position_held.office_holder -> J. Howard Rossbach\n# Answer:\nm.03p40b5", "# Reasoning Path:\nHarry S. Truman -> book.author.works_written -> A new era in world affairs -> common.topic.notable_types -> Book\n# Answer:\nA new era in world affairs", "# Reasoning Path:\nHarry S. Truman -> book.author.works_written -> A new era in world affairs -> common.topic.notable_for -> g.125csw7cw\n# Answer:\nA new era in world affairs", "# Reasoning Path:\nHarry S. Truman -> government.political_appointer.appointees -> m.03p40b5 -> government.government_position_held.governmental_body -> U.S. Securities and Exchange Commission\n# Answer:\nm.03p40b5", "# Reasoning Path:\nHarry S. Truman -> government.political_appointer.appointees -> m.04kqlkp -> government.government_position_held.governmental_body -> Cabinet of the United States\n# Answer:\nm.04kqlkp", "# Reasoning Path:\nHarry S. Truman -> book.author.works_written -> A new page in history -> common.topic.notable_types -> Book\n# Answer:\nA new page in history"], "ground_truth": ["University of Missouri\u2013Kansas City School of Law", "William Chrisman High School", "University of Missouri\u2013Kansas City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1916", "prediction": ["# Reasoning Path:\nUtica -> location.location.containedby -> Oneida County -> location.location.containedby -> New York\n# Answer:\nOneida County", "# Reasoning Path:\nUtica -> location.location.containedby -> Oneida County -> location.location.containedby -> Utica-Rome, NY Metropolitan Statistical Area\n# Answer:\nOneida County", "# Reasoning Path:\nUtica -> location.location.containedby -> New York -> location.location.containedby -> United States of America\n# Answer:\nNew York", "# Reasoning Path:\nUtica -> location.hud_county_place.county -> Oneida County -> location.location.containedby -> New York\n# Answer:\nOneida County", "# Reasoning Path:\nUtica -> location.location.containedby -> Oneida County -> location.us_county.hud_county_place -> Barneveld\n# Answer:\nOneida County", "# Reasoning Path:\nUtica -> location.location.containedby -> Oneida County -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nOneida County", "# Reasoning Path:\nUtica -> location.hud_county_place.county -> Oneida County -> location.location.containedby -> Utica-Rome, NY Metropolitan Statistical Area\n# Answer:\nOneida County", "# Reasoning Path:\nUtica -> location.location.containedby -> Oneida County -> location.us_county.hud_county_place -> Boonville\n# Answer:\nOneida County", "# Reasoning Path:\nUtica -> location.location.containedby -> New York -> base.aareas.schema.administrative_area.administrative_children -> Herkimer County\n# Answer:\nNew York", "# Reasoning Path:\nUtica -> location.location.containedby -> Oneida County -> location.us_county.hud_county_place -> Bridgewater\n# Answer:\nOneida County"], "ground_truth": ["Oneida County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1917", "prediction": ["# Reasoning Path:\nCharles Babbage -> law.inventor.inventions -> Analytical Engine -> common.topic.article -> m.0ng8\n# Answer:\nAnalytical Engine", "# Reasoning Path:\nCharles Babbage -> law.inventor.inventions -> Difference engine -> common.topic.image -> LondonScienceMuseumsReplicaDifferenceEngine\n# Answer:\nDifference engine", "# Reasoning Path:\nCharles Babbage -> base.argumentmaps.innovator.original_ideas -> Analytical Engine -> common.topic.article -> m.0ng8\n# Answer:\nAnalytical Engine", "# Reasoning Path:\nCharles Babbage -> law.inventor.inventions -> Difference engine -> common.topic.notable_types -> Invention\n# Answer:\nDifference engine", "# Reasoning Path:\nCharles Babbage -> law.inventor.inventions -> Difference engine -> common.topic.article -> m.029x2\n# Answer:\nDifference engine", "# Reasoning Path:\nCharles Babbage -> law.inventor.inventions -> Difference engine -> common.topic.image -> Babbage Difference Engine\n# Answer:\nDifference engine", "# Reasoning Path:\nCharles Babbage -> base.argumentmaps.innovator.original_ideas -> Analytical Engine -> common.topic.image -> AnalyticalMachine Babbage London\n# Answer:\nAnalytical Engine", "# Reasoning Path:\nCharles Babbage -> influence.influence_node.influenced_by -> Blaise Pascal -> law.inventor.inventions -> Adding machine\n# Answer:\nBlaise Pascal", "# Reasoning Path:\nCharles Babbage -> influence.influence_node.influenced_by -> Gottfried Wilhelm von Leibniz -> law.inventor.inventions -> Leibniz wheel\n# Answer:\nGottfried Wilhelm von Leibniz", "# Reasoning Path:\nCharles Babbage -> influence.influence_node.influenced_by -> Blaise Pascal -> law.inventor.inventions -> Pascal's calculator\n# Answer:\nBlaise Pascal"], "ground_truth": ["Analytical Engine", "Difference engine"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1919", "prediction": ["# Reasoning Path:\nMadagascar -> film.film.starring -> m.03ld02p -> film.performance.actor -> Sacha Baron Cohen\n# Answer:\nm.03ld02p", "# Reasoning Path:\nMadagascar -> film.film.starring -> m.03ld02p -> film.performance.special_performance_type -> Voice\n# Answer:\nm.03ld02p", "# Reasoning Path:\nMadagascar -> film.film.starring -> m.03ld02p -> film.performance.character -> Julien\n# Answer:\nm.03ld02p", "# Reasoning Path:\nMadagascar -> film.film.starring -> m.03ld02_ -> film.performance.actor -> Andy Richter\n# Answer:\nm.03ld02_", "# Reasoning Path:\nMadagascar -> film.film.rating -> PG (USA) -> film.content_rating.film_rating_system -> Motion Picture Association of America film rating system\n# Answer:\nPG (USA)", "# Reasoning Path:\nMadagascar -> film.film.starring -> m.03ld02v -> film.performance.actor -> Cedric the Entertainer\n# Answer:\nm.03ld02v", "# Reasoning Path:\nMadagascar -> film.film.starring -> m.03ld02_ -> film.performance.character -> Mort\n# Answer:\nm.03ld02_", "# Reasoning Path:\nMadagascar -> film.film.rating -> PG (USA) -> common.topic.webpage -> m.0cpryn3\n# Answer:\nPG (USA)", "# Reasoning Path:\nMadagascar -> film.film.starring -> m.03ld02v -> film.performance.special_performance_type -> Voice\n# Answer:\nm.03ld02v", "# Reasoning Path:\nMadagascar -> film.film.rating -> PG (USA) -> common.topic.article -> m.0kprrm\n# Answer:\nPG (USA)"], "ground_truth": ["Sacha Baron Cohen"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1920", "prediction": ["# Reasoning Path:\nRick Scott -> people.person.profession -> Politician -> common.topic.notable_types -> Profession\n# Answer:\nPolitician", "# Reasoning Path:\nRick Scott -> common.topic.notable_types -> Politician -> common.topic.notable_types -> Profession\n# Answer:\nPolitician", "# Reasoning Path:\nRick Scott -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdn_\n# Answer:\nPolitician", "# Reasoning Path:\nRick Scott -> people.person.profession -> Investor -> base.descriptive_names.names.descriptive_name -> m.01026l97\n# Answer:\nInvestor", "# Reasoning Path:\nRick Scott -> people.person.profession -> Investor -> common.topic.notable_for -> g.1256898vb\n# Answer:\nInvestor", "# Reasoning Path:\nRick Scott -> people.person.profession -> Businessperson -> common.topic.notable_types -> Profession\n# Answer:\nBusinessperson", "# Reasoning Path:\nRick Scott -> people.person.profession -> Politician -> people.profession.specializations -> Mayor of Auckland\n# Answer:\nPolitician", "# Reasoning Path:\nRick Scott -> people.person.profession -> Investor -> common.topic.notable_types -> Profession\n# Answer:\nInvestor", "# Reasoning Path:\nRick Scott -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdq3\n# Answer:\nPolitician", "# Reasoning Path:\nRick Scott -> people.person.profession -> Investor -> base.descriptive_names.names.descriptive_name -> m.0105gs7h\n# Answer:\nInvestor"], "ground_truth": ["Investor", "Politician", "Businessperson", "Lawyer", "Executive officer"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 1.0, "ans_recall": 0.6, "path_f1": 0.6, "path_precision": 0.6, "path_recall": 0.6, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 1.0, "path_ans_recall": 0.6}
{"id": "WebQTest-1921", "prediction": ["# Reasoning Path:\nCrimean War -> time.event.locations -> Autonomous Republic of Crimea -> base.aareas.schema.administrative_area.administrative_children -> Bakhchysarai Raion\n# Answer:\nAutonomous Republic of Crimea", "# Reasoning Path:\nCrimean War -> time.event.locations -> Black Sea -> common.topic.notable_types -> Body Of Water\n# Answer:\nBlack Sea", "# Reasoning Path:\nCrimean War -> time.event.locations -> Autonomous Republic of Crimea -> base.aareas.schema.administrative_area.administrative_children -> Bilohirsk Raion\n# Answer:\nAutonomous Republic of Crimea", "# Reasoning Path:\nCrimean War -> time.event.locations -> Autonomous Republic of Crimea -> location.location.events -> 2014\u201315 Russian military intervention in Ukraine\n# Answer:\nAutonomous Republic of Crimea", "# Reasoning Path:\nCrimean War -> time.event.locations -> Autonomous Republic of Crimea -> base.aareas.schema.administrative_area.administrative_children -> Chornomorske Raion\n# Answer:\nAutonomous Republic of Crimea", "# Reasoning Path:\nCrimean War -> time.event.locations -> Autonomous Republic of Crimea -> location.location.contains -> Ai-Petri\n# Answer:\nAutonomous Republic of Crimea", "# Reasoning Path:\nCrimean War -> time.event.locations -> Autonomous Republic of Crimea -> location.location.events -> Annexation of Crimea by the Russian Federation\n# Answer:\nAutonomous Republic of Crimea", "# Reasoning Path:\nCrimean War -> time.event.locations -> Black Sea -> location.location.events -> Action of 4 July 1773\n# Answer:\nBlack Sea", "# Reasoning Path:\nCrimean War -> time.event.locations -> Autonomous Republic of Crimea -> location.location.events -> Austro-Russian\u2013Turkish War\n# Answer:\nAutonomous Republic of Crimea", "# Reasoning Path:\nCrimean War -> time.event.locations -> Autonomous Republic of Crimea -> location.location.contains -> Alupka\n# Answer:\nAutonomous Republic of Crimea"], "ground_truth": ["Caucasus", "Black Sea", "Autonomous Republic of Crimea", "Crimea", "Balkans", "Baltic Sea", "White Sea"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 1.0, "ans_recall": 0.42857142857142855, "path_f1": 0.4444444444444445, "path_precision": 1.0, "path_recall": 0.2857142857142857, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1922", "prediction": ["# Reasoning Path:\nTheodore Roosevelt -> government.politician.government_positions_held -> m.03fx899 -> government.government_position_held.basic_title -> President\n# Answer:\nm.03fx899", "# Reasoning Path:\nTheodore Roosevelt -> government.politician.government_positions_held -> m.03fx899 -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nm.03fx899", "# Reasoning Path:\nTheodore Roosevelt -> government.politician.government_positions_held -> m.03q2hyl -> government.government_position_held.basic_title -> Assistant Secretary\n# Answer:\nm.03q2hyl", "# Reasoning Path:\nTheodore Roosevelt -> people.person.employment_history -> m.0w7nv3t -> business.employment_tenure.title -> President\n# Answer:\nm.0w7nv3t", "# Reasoning Path:\nTheodore Roosevelt -> government.politician.government_positions_held -> m.04ml2sj -> government.government_position_held.basic_title -> Governor\n# Answer:\nm.04ml2sj", "# Reasoning Path:\nTheodore Roosevelt -> people.person.employment_history -> m.0w7nv3t -> business.employment_tenure.company -> Federal government of the United States\n# Answer:\nm.0w7nv3t", "# Reasoning Path:\nTheodore Roosevelt -> government.politician.government_positions_held -> m.03q2hyl -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nm.03q2hyl", "# Reasoning Path:\nTheodore Roosevelt -> common.topic.notable_types -> Book -> freebase.type_profile.published -> Published\n# Answer:\nBook", "# Reasoning Path:\nTheodore Roosevelt -> common.topic.notable_types -> US President -> type.type.expected_by -> President\n# Answer:\nUS President", "# Reasoning Path:\nTheodore Roosevelt -> common.topic.notable_types -> Film character -> freebase.type_profile.published -> Published\n# Answer:\nFilm character"], "ground_truth": ["Member of the Legislative Assembly", "Vice President", "Governor", "Assistant Secretary", "President"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.13333333333333333, "ans_precission": 0.1, "ans_recall": 0.2, "path_f1": 0.5555555555555556, "path_precision": 0.5, "path_recall": 0.625, "path_ans_f1": 0.6, "path_ans_precision": 0.6, "path_ans_recall": 0.6}
{"id": "WebQTest-1923", "prediction": ["# Reasoning Path:\nMiley Cyrus -> people.person.parents -> Tish Cyrus -> film.producer.film -> LOL\n# Answer:\nTish Cyrus", "# Reasoning Path:\nMiley Cyrus -> people.person.parents -> Billy Ray Cyrus -> film.producer.film -> Hannah Montana: The Movie\n# Answer:\nBilly Ray Cyrus", "# Reasoning Path:\nMiley Cyrus -> people.person.parents -> Tish Cyrus -> film.producer.film -> So Undercover\n# Answer:\nTish Cyrus", "# Reasoning Path:\nMiley Cyrus -> people.person.parents -> Billy Ray Cyrus -> music.artist.album -> Ready, Set, Don't Go\n# Answer:\nBilly Ray Cyrus", "# Reasoning Path:\nMiley Cyrus -> music.artist.album -> Hannah Montana -> media_common.adapted_work.adaptations -> Hannah Montana and Miley Cyrus: Best of Both Worlds Concert\n# Answer:\nHannah Montana", "# Reasoning Path:\nMiley Cyrus -> people.person.parents -> Tish Cyrus -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nTish Cyrus", "# Reasoning Path:\nMiley Cyrus -> film.actor.film -> m.0h0_35j -> film.performance.film -> The World According to Miley Cyrus\n# Answer:\nm.0h0_35j", "# Reasoning Path:\nMiley Cyrus -> people.person.parents -> Tish Cyrus -> people.person.children -> Noah Cyrus\n# Answer:\nTish Cyrus", "# Reasoning Path:\nMiley Cyrus -> film.actor.film -> m.0w1x035 -> film.performance.film -> Rock Mafia Presents: The Big Bang\n# Answer:\nm.0w1x035", "# Reasoning Path:\nMiley Cyrus -> music.artist.album -> Hannah Montana -> tv.tv_program.country_of_origin -> United States of America\n# Answer:\nHannah Montana"], "ground_truth": ["Bolt", "The World According to Miley Cyrus", "Sonic the Hedgehog", "Family Bond", "Radio Disney Party Jams: The Concert", "Big Fish", "The Last Song", "Hannah Montana: The Movie", "Sex and the City 2", "Miley Cyrus: Tongue Tied", "High School Musical 2", "Super Rhino", "Wizards on Deck with Hannah Montana", "LOL", "So Undercover", "Hannah Montana and Miley Cyrus: Best of Both Worlds Concert", "Rock Mafia Presents: The Big Bang"], "ans_acc": 0.35294117647058826, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.22727272727272727, "path_precision": 0.5, "path_recall": 0.14705882352941177, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.6, "path_ans_recall": 0.35294117647058826}
{"id": "WebQTest-1925", "prediction": ["# Reasoning Path:\nMilk -> film.film.release_date_s -> m.0gl75y_ -> film.film_regional_release_date.film_release_distribution_medium -> Theatrical\n# Answer:\nm.0gl75y_", "# Reasoning Path:\nMilk -> freebase.valuenotation.has_no_value -> Film Series -> type.property.expected_type -> Film series\n# Answer:\nFilm Series", "# Reasoning Path:\nMilk -> film.film.release_date_s -> m.0gl75y_ -> film.film_regional_release_date.film_release_region -> United States of America\n# Answer:\nm.0gl75y_", "# Reasoning Path:\nMilk -> film.film.release_date_s -> m.0w7gvcy -> film.film_regional_release_date.film_release_region -> United States of America\n# Answer:\nm.0w7gvcy", "# Reasoning Path:\nMilk -> film.film.executive_produced_by -> Barbara A. Hall -> common.topic.notable_types -> Film producer\n# Answer:\nBarbara A. Hall", "# Reasoning Path:\nMilk -> film.film.release_date_s -> m.0w7gv4j -> film.film_regional_release_date.film_release_distribution_medium -> Theatrical\n# Answer:\nm.0w7gv4j", "# Reasoning Path:\nMilk -> freebase.valuenotation.has_no_value -> Film Series -> rdf-schema#range -> Film series\n# Answer:\nFilm Series", "# Reasoning Path:\nMilk -> freebase.valuenotation.has_no_value -> Prequel -> rdf-schema#domain -> Film\n# Answer:\nPrequel", "# Reasoning Path:\nMilk -> film.film.release_date_s -> m.0w7gv4j -> film.film_regional_release_date.film_release_region -> United States of America\n# Answer:\nm.0w7gv4j", "# Reasoning Path:\nMilk -> freebase.valuenotation.has_no_value -> Film Series -> type.property.master_property -> Films In Series\n# Answer:\nFilm Series"], "ground_truth": ["Scott Smith"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1926", "prediction": ["# Reasoning Path:\n2011 military intervention in Libya -> time.event.includes_event -> Operation Unified Protector -> time.event.locations -> Libya\n# Answer:\nOperation Unified Protector", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.includes_event -> Operation Unified Protector -> time.event.locations -> Mediterranean Sea\n# Answer:\nOperation Unified Protector", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.includes_event -> Operation Ellamy -> time.event.locations -> Libya\n# Answer:\nOperation Ellamy", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.includes_event -> Operation Unified Protector -> base.culturalevent.event.entity_involved -> United Kingdom\n# Answer:\nOperation Unified Protector", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.locations -> Libya -> location.location.events -> Operation Unified Protector\n# Answer:\nLibya", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.locations -> Libya -> location.country.official_language -> Arabic Language\n# Answer:\nLibya", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.includes_event -> Operation Unified Protector -> military.military_conflict.combatants -> m.0gv_18k\n# Answer:\nOperation Unified Protector", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.includes_event -> Operation Ellamy -> base.culturalevent.event.entity_involved -> United Kingdom\n# Answer:\nOperation Ellamy", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.locations -> Libya -> location.location.events -> 1986 United States bombing of Libya\n# Answer:\nLibya", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.includes_event -> Operation Ellamy -> common.topic.notable_types -> Military Conflict\n# Answer:\nOperation Ellamy"], "ground_truth": ["2011-03-19"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1928", "prediction": ["# Reasoning Path:\nDiana Taurasi -> sports.pro_athlete.teams -> m.0z3t_gj -> sports.sports_team_roster.team -> UMMC Ekaterinburg\n# Answer:\nm.0z3t_gj", "# Reasoning Path:\nDiana Taurasi -> sports.pro_athlete.teams -> m.0z3t_gj -> sports.sports_team_roster.position -> Guard\n# Answer:\nm.0z3t_gj", "# Reasoning Path:\nDiana Taurasi -> sports.pro_athlete.teams -> m.0kc2_tr -> sports.sports_team_roster.team -> Phoenix Mercury\n# Answer:\nm.0kc2_tr", "# Reasoning Path:\nDiana Taurasi -> base.schemastaging.athlete_extra.salary -> m.0ng6vr5 -> base.schemastaging.athlete_salary.team -> Phoenix Mercury\n# Answer:\nm.0ng6vr5", "# Reasoning Path:\nDiana Taurasi -> sports.pro_athlete.teams -> m.0z3t_gj -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0z3t_gj", "# Reasoning Path:\nDiana Taurasi -> sports.pro_athlete.teams -> m.0z3t_lb -> sports.sports_team_roster.team -> WBC Spartak Moscow Region\n# Answer:\nm.0z3t_lb", "# Reasoning Path:\nDiana Taurasi -> base.schemastaging.athlete_extra.salary -> m.0ng6vr5 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0ng6vr5", "# Reasoning Path:\nDiana Taurasi -> sports.pro_athlete.teams -> m.0kc2_tr -> sports.sports_team_roster.position -> Guard\n# Answer:\nm.0kc2_tr", "# Reasoning Path:\nDiana Taurasi -> sports.pro_athlete.teams -> m.0kc2_tr -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0kc2_tr", "# Reasoning Path:\nDiana Taurasi -> sports.pro_athlete.teams -> m.0z3t_lb -> sports.sports_team_roster.position -> Guard\n# Answer:\nm.0z3t_lb"], "ground_truth": ["Phoenix Mercury", "UMMC Ekaterinburg"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1929", "prediction": ["# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Arc de Triomphe -> architecture.structure.architectural_style -> Neoclassicism\n# Answer:\nArc de Triomphe", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> common.topic.notable_for -> g.1256sch6w\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Arc de Triomphe -> common.topic.image -> Arc Triomphe\n# Answer:\nArc de Triomphe", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Bois de Boulogne -> location.location.containedby -> France\n# Answer:\nBois de Boulogne", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> architecture.structure.architect -> Domenico da Cortona\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> common.topic.webpage -> m.02k8dbw -> common.webpage.category -> Topic Webpage\n# Answer:\nm.02k8dbw", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Arc de Triomphe -> common.topic.image -> Arc de Triomphe, Paris, 1806\n# Answer:\nArc de Triomphe", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> common.topic.image -> Chateau de Chambord\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Bois de Boulogne -> common.topic.notable_types -> Tourist attraction\n# Answer:\nBois de Boulogne", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> architecture.structure.architect -> Pierre Nepveu\n# Answer:\nCh\u00e2teau de Chambord"], "ground_truth": ["Petit Palais", "Mus\u00e9e Maillol", "\u00cele de la Cit\u00e9", "Eiffel Tower", "Place de la Concorde", "Mus\u00e9e d'Orsay", "Ch\u00e2teau de Chambord", "Centre Georges Pompidou", "Mus\u00e9e des Arts et M\u00e9tiers", "Bois de Boulogne", "Mus\u00e9e du quai Branly", "Galerie nationale du Jeu de Paume", "Parc Ast\u00e9rix", "Gare d'Orsay", "Notre Dame de Paris", "Grande Arche", "Galerie Claude Bernard", "Disneyland Park", "Caf\u00e9 Volpini", "Arc de Triomphe", "Sacr\u00e9-C\u0153ur, Paris", "Folies Berg\u00e8re", "Les Invalides", "Champs-\u00c9lys\u00e9es", "Jardin du Luxembourg", "The Louvre", "Disneyland Paris", "La Maison Rouge", "Panth\u00e9on", "Mus\u00e9e de l'Orangerie", "Verdon Gorge"], "ans_acc": 0.0967741935483871, "ans_hit": 1, "ans_f1": 0.17475728155339806, "ans_precission": 0.9, "ans_recall": 0.0967741935483871, "path_f1": 0.17475728155339806, "path_precision": 0.9, "path_recall": 0.0967741935483871, "path_ans_f1": 0.17475728155339806, "path_ans_precision": 0.9, "path_ans_recall": 0.0967741935483871}
{"id": "WebQTest-193", "prediction": ["# Reasoning Path:\nBen Roethlisberger -> people.person.education -> m.02wns4p -> education.education.institution -> Miami University\n# Answer:\nm.02wns4p", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.passing -> m.07916ct -> american_football.player_passing_statistics.season -> 2008 NFL season\n# Answer:\nm.07916ct", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.rushing -> m.07916fp -> american_football.player_rushing_statistics.season -> 2008 NFL season\n# Answer:\nm.07916fp", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.passing -> m.07916ct -> american_football.player_passing_statistics.team -> Pittsburgh Steelers\n# Answer:\nm.07916ct", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.rushing -> m.07916fp -> american_football.player_rushing_statistics.team -> Pittsburgh Steelers\n# Answer:\nm.07916fp", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.rushing -> m.07mmgmj -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\nm.07mmgmj", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.passing -> m.07mmgkg -> american_football.player_passing_statistics.team -> Pittsburgh Steelers\n# Answer:\nm.07mmgkg", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.rushing -> m.07sgw_5 -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\nm.07sgw_5", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.passing -> m.07mmgkg -> american_football.player_passing_statistics.season -> 2009 NFL season\n# Answer:\nm.07mmgkg", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.passing -> m.07sgwy5 -> american_football.player_passing_statistics.season -> 2009 NFL season\n# Answer:\nm.07sgwy5"], "ground_truth": ["Miami University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1932", "prediction": ["# Reasoning Path:\nCanada -> government.governmental_jurisdiction.governing_officials -> m.011ktc2z -> freebase.valuenotation.has_no_value -> Appointed By (if Position is Appointed)\n# Answer:\nm.011ktc2z", "# Reasoning Path:\nCanada -> government.governmental_jurisdiction.governing_officials -> m.011ktc2z -> government.government_position_held.governmental_body -> Legislative Assembly of the Northwest Territories\n# Answer:\nm.011ktc2z", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.12cp_k6ss\n# Answer:\ng.12cp_k6ss", "# Reasoning Path:\nCanada -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.12tb6g_94\n# Answer:\ng.12tb6g_94", "# Reasoning Path:\nCanada -> government.governmental_jurisdiction.governing_officials -> m.011rmy9v -> government.government_position_held.appointed_by -> Stephen Harper\n# Answer:\nm.011rmy9v", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.1hhc38hnd\n# Answer:\ng.1hhc38hnd", "# Reasoning Path:\nCanada -> government.governmental_jurisdiction.governing_officials -> m.011qftx1 -> government.government_position_held.governmental_body -> House of Commons of Canada\n# Answer:\nm.011qftx1", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.1hhc3d2ty\n# Answer:\ng.1hhc3d2ty", "# Reasoning Path:\nCanada -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.1hhc39x0w\n# Answer:\ng.1hhc39x0w", "# Reasoning Path:\nCanada -> government.governmental_jurisdiction.governing_officials -> m.011qftx1 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.011qftx1"], "ground_truth": ["House of Commons of Canada", "Parliament of Canada"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 0.1, "path_ans_recall": 0.5}
{"id": "WebQTest-1933", "prediction": ["# Reasoning Path:\nAdam Morrison -> sports.drafted_athlete.drafted -> m.04_bzh1 -> sports.sports_league_draft_pick.draft -> 2006 NBA draft\n# Answer:\nm.04_bzh1", "# Reasoning Path:\nAdam Morrison -> sports.drafted_athlete.drafted -> m.04_bzh1 -> sports.sports_league_draft_pick.team -> Charlotte Hornets\n# Answer:\nm.04_bzh1", "# Reasoning Path:\nAdam Morrison -> sports.drafted_athlete.drafted -> m.04_bzh1 -> sports.sports_league_draft_pick.school -> Gonzaga University\n# Answer:\nm.04_bzh1", "# Reasoning Path:\nAdam Morrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nAdam Morrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Alaska\n# Answer:\nUnited States of America", "# Reasoning Path:\nAdam Morrison -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nAdam Morrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Arizona\n# Answer:\nUnited States of America", "# Reasoning Path:\nAdam Morrison -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nAdam Morrison -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Alaska\n# Answer:\nUnited States of America", "# Reasoning Path:\nAdam Morrison -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Arizona\n# Answer:\nUnited States of America"], "ground_truth": ["2006 NBA draft"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1934", "prediction": ["# Reasoning Path:\nMichael Jackson -> people.person.place_of_birth -> Gary -> location.location.containedby -> United States of America\n# Answer:\nGary", "# Reasoning Path:\nMichael Jackson -> music.artist.origin -> Gary -> location.location.containedby -> United States of America\n# Answer:\nGary", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1Club.FM: 70s (Pop) -> broadcast.content.location -> Chicago\n# Answer:\n1Club.FM: 70s (Pop)", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1Club.FM: 70s (Pop) -> common.topic.image -> 1clubfm.jpg\n# Answer:\n1Club.FM: 70s (Pop)", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1Club.FM: 70s (Pop) -> common.topic.notable_types -> Broadcast Content\n# Answer:\n1Club.FM: 70s (Pop)", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> .977 The '80s Channel -> broadcast.content.genre -> 70's\n# Answer:\n.977 The '80s Channel", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> .977 The '80s Channel -> common.topic.notable_types -> Broadcast Content\n# Answer:\n.977 The '80s Channel", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1.FM Absolute  90's -> common.topic.notable_types -> Broadcast Content\n# Answer:\n1.FM Absolute  90's", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> .977 The '80s Channel -> broadcast.content.genre -> 80's\n# Answer:\n.977 The '80s Channel", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1.FM Absolute  90's -> broadcast.content.genre -> 90's\n# Answer:\n1.FM Absolute  90's"], "ground_truth": ["Gary"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1935", "prediction": ["# Reasoning Path:\nMaya Moore -> people.person.education -> m.040ggtj -> education.education.institution -> Collins Hill High School\n# Answer:\nm.040ggtj", "# Reasoning Path:\nMaya Moore -> common.topic.notable_for -> g.125cckxj9\n# Answer:\ng.125cckxj9", "# Reasoning Path:\nMaya Moore -> people.person.education -> m.04mg02t -> education.education.institution -> University of Connecticut\n# Answer:\nm.04mg02t", "# Reasoning Path:\nMaya Moore -> base.schemastaging.athlete_extra.salary -> m.0nghzz3 -> base.schemastaging.athlete_salary.team -> Minnesota Lynx\n# Answer:\nm.0nghzz3", "# Reasoning Path:\nMaya Moore -> base.schemastaging.athlete_extra.salary -> m.0nghzz3 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0nghzz3"], "ground_truth": ["University of Connecticut"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1936", "prediction": ["# Reasoning Path:\nLos Angeles Dodgers -> sports.sports_team.championships -> 1988 World Series -> time.event.locations -> Dodger Stadium\n# Answer:\n1988 World Series", "# Reasoning Path:\nLos Angeles Dodgers -> sports.sports_team.championships -> 1988 World Series -> time.event.instance_of_recurring_event -> World Series\n# Answer:\n1988 World Series", "# Reasoning Path:\nLos Angeles Dodgers -> sports.sports_team.championships -> 1959 World Series -> sports.sports_championship_event.championship -> World Series\n# Answer:\n1959 World Series", "# Reasoning Path:\nLos Angeles Dodgers -> sports.sports_team.championships -> 1988 World Series -> time.event.locations -> Oakland-Alameda County Coliseum\n# Answer:\n1988 World Series", "# Reasoning Path:\nLos Angeles Dodgers -> sports.sports_team.championships -> 1959 World Series -> sports.sports_championship_event.season -> 1959 Major League Baseball Season\n# Answer:\n1959 World Series", "# Reasoning Path:\nLos Angeles Dodgers -> sports.sports_team.championships -> 1963 World Series -> sports.sports_championship_event.runner_up -> New York Yankees\n# Answer:\n1963 World Series", "# Reasoning Path:\nLos Angeles Dodgers -> baseball.baseball_team.team_stats -> m.05n69_9 -> baseball.baseball_team_stats.season -> 1965 Major League Baseball Season\n# Answer:\nm.05n69_9", "# Reasoning Path:\nLos Angeles Dodgers -> sports.sports_team.championships -> 1959 World Series -> time.event.instance_of_recurring_event -> World Series\n# Answer:\n1959 World Series", "# Reasoning Path:\nLos Angeles Dodgers -> sports.sports_team.championships -> 1963 World Series -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1963 World Series", "# Reasoning Path:\nLos Angeles Dodgers -> sports.sports_team.championships -> 1963 World Series -> sports.sports_championship_event.season -> 1963 Major League Baseball Season\n# Answer:\n1963 World Series"], "ground_truth": ["1988 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1937", "prediction": ["# Reasoning Path:\nJames Cook -> people.person.profession -> Sailor -> fictional_universe.character_occupation.characters_with_this_occupation -> Alfred King\n# Answer:\nSailor", "# Reasoning Path:\nJames Cook -> people.person.profession -> Sailor -> base.schemastaging.context_name.pronunciation -> g.125_nnp67\n# Answer:\nSailor", "# Reasoning Path:\nJames Cook -> people.person.profession -> Sailor -> fictional_universe.character_occupation.characters_with_this_occupation -> Bussell\n# Answer:\nSailor", "# Reasoning Path:\nJames Cook -> people.person.profession -> Sailor -> base.descriptive_names.names.descriptive_name -> m.0105cyc4\n# Answer:\nSailor", "# Reasoning Path:\nJames Cook -> people.person.profession -> Explorer -> base.descriptive_names.names.descriptive_name -> m.0106bf9p\n# Answer:\nExplorer", "# Reasoning Path:\nJames Cook -> people.person.profession -> Sailor -> fictional_universe.character_occupation.characters_with_this_occupation -> Ervic\n# Answer:\nSailor", "# Reasoning Path:\nJames Cook -> people.person.profession -> Explorer -> common.topic.notable_types -> Profession\n# Answer:\nExplorer", "# Reasoning Path:\nJames Cook -> people.person.profession -> Sailor -> base.descriptive_names.names.descriptive_name -> m.0105cyz5\n# Answer:\nSailor", "# Reasoning Path:\nJames Cook -> people.person.profession -> Explorer -> base.descriptive_names.names.descriptive_name -> m.0106bhcr\n# Answer:\nExplorer", "# Reasoning Path:\nJames Cook -> people.person.profession -> Explorer -> people.profession.specializations -> Aquanaut\n# Answer:\nExplorer"], "ground_truth": ["Explorer", "Sailor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1938", "prediction": ["# Reasoning Path:\nRobin Williams -> people.person.children -> Zak Williams -> people.person.place_of_birth -> San Francisco\n# Answer:\nZak Williams", "# Reasoning Path:\nRobin Williams -> people.person.children -> Cody Alan Williams -> people.person.place_of_birth -> New York City\n# Answer:\nCody Alan Williams", "# Reasoning Path:\nRobin Williams -> people.person.children -> Zachary Pym Williams -> people.person.place_of_birth -> United States of America\n# Answer:\nZachary Pym Williams", "# Reasoning Path:\nRobin Williams -> people.person.spouse_s -> m.0h2n9t4 -> people.marriage.spouse -> Marsha Garces\n# Answer:\nm.0h2n9t4", "# Reasoning Path:\nRobin Williams -> people.person.spouse_s -> m.0h2n9t4 -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0h2n9t4", "# Reasoning Path:\nRobin Williams -> people.person.spouse_s -> m.0hykk0l -> people.marriage.spouse -> Valerie Velardi\n# Answer:\nm.0hykk0l", "# Reasoning Path:\nRobin Williams -> people.person.children -> Cody Alan Williams -> common.topic.notable_types -> Person\n# Answer:\nCody Alan Williams", "# Reasoning Path:\nRobin Williams -> people.person.children -> Zachary Pym Williams -> people.person.parents -> Valerie Velardi\n# Answer:\nZachary Pym Williams", "# Reasoning Path:\nRobin Williams -> people.person.spouse_s -> m.0h2n9t4 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.0h2n9t4", "# Reasoning Path:\nRobin Williams -> people.person.spouse_s -> m.0j11z6t -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0j11z6t"], "ground_truth": ["St. Helena"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1939", "prediction": ["# Reasoning Path:\nCanada -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nCanada -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Australia\n# Answer:\nParliamentary system", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.12cp_k6ss\n# Answer:\ng.12cp_k6ss", "# Reasoning Path:\nCanada -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Belgium\n# Answer:\nParliamentary system", "# Reasoning Path:\nCanada -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.12tb6g_94\n# Answer:\ng.12tb6g_94", "# Reasoning Path:\nCanada -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Bermuda\n# Answer:\nParliamentary system", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.1hhc38hnd\n# Answer:\ng.1hhc38hnd", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.1hhc3d2ty\n# Answer:\ng.1hhc3d2ty", "# Reasoning Path:\nCanada -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.1hhc39x0w\n# Answer:\ng.1hhc39x0w", "# Reasoning Path:\nCanada -> location.country.form_of_government -> Federal monarchy -> government.form_of_government.countries -> Australia\n# Answer:\nFederal monarchy"], "ground_truth": ["Constitutional monarchy", "Federal monarchy", "Parliamentary system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-194", "prediction": ["# Reasoning Path:\nIroquois -> people.ethnicity.geographic_distribution -> Ontario -> location.administrative_division.country -> Canada\n# Answer:\nOntario", "# Reasoning Path:\nIroquois -> people.ethnicity.geographic_distribution -> Qu\u00e9bec -> base.ontologies.ontology_instance.equivalent_instances -> m.09klhv1\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nIroquois -> people.ethnicity.included_in_group -> Native Americans in the United States -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nNative Americans in the United States", "# Reasoning Path:\nIroquois -> people.ethnicity.geographic_distribution -> Ontario -> location.location.adjoin_s -> m.02nxjf_\n# Answer:\nOntario", "# Reasoning Path:\nIroquois -> people.ethnicity.geographic_distribution -> Ontario -> government.governmental_jurisdiction.agencies -> Landlord and Tenant Board\n# Answer:\nOntario", "# Reasoning Path:\nIroquois -> people.ethnicity.included_in_group -> Indigenous peoples of the Americas -> people.ethnicity.geographic_distribution -> Central America\n# Answer:\nIndigenous peoples of the Americas", "# Reasoning Path:\nIroquois -> people.ethnicity.geographic_distribution -> Ontario -> location.location.adjoin_s -> m.02nxjfs\n# Answer:\nOntario", "# Reasoning Path:\nIroquois -> people.ethnicity.geographic_distribution -> Qu\u00e9bec -> common.topic.webpage -> m.02h755t\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nIroquois -> people.ethnicity.included_in_group -> Indigenous peoples of the Americas -> medicine.risk_factor.diseases -> Gallstone\n# Answer:\nIndigenous peoples of the Americas", "# Reasoning Path:\nIroquois -> people.ethnicity.geographic_distribution -> Ontario -> location.location.adjoin_s -> m.02wmbpt\n# Answer:\nOntario"], "ground_truth": ["Qu\u00e9bec", "Ontario"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1940", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Hinduism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> common.topic.article -> m.0jfq\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.deities -> Yahweh\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.article -> m.055v4j\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> God -> common.topic.notable_for -> g.125d2mw4b\n# Answer:\nGod", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Jainism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.image -> Ramapir01\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> religion.deity.deity_of -> Sunni Islam\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Sikhism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.image -> Ramapir22\n# Answer:\nRamdev Pir"], "ground_truth": ["Ramdev Pir", "God", "Allah"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1941", "prediction": ["# Reasoning Path:\nPeru -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Argentina\n# Answer:\nConstitutional republic", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.11b71n53cl\n# Answer:\ng.11b71n53cl", "# Reasoning Path:\nPeru -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Brazil\n# Answer:\nConstitutional republic", "# Reasoning Path:\nPeru -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Bolivia\n# Answer:\nUnitary state", "# Reasoning Path:\nPeru -> location.statistical_region.energy_use_per_capita -> g.1245_8rt3\n# Answer:\ng.1245_8rt3", "# Reasoning Path:\nPeru -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Chile\n# Answer:\nConstitutional republic", "# Reasoning Path:\nPeru -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Argentina\n# Answer:\nPresidential system", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.1hhc37ctc\n# Answer:\ng.1hhc37ctc", "# Reasoning Path:\nPeru -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Chile\n# Answer:\nUnitary state", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.1hhc38qlv\n# Answer:\ng.1hhc38qlv"], "ground_truth": ["Constitutional republic", "Presidential system", "Republic", "Unitary state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.6666666666666665, "path_precision": 0.6, "path_recall": 0.75, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1942", "prediction": ["# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Abdullah of Saudi Arabia -> people.person.sibling_s -> m.0v_bc0s\n# Answer:\nAbdullah of Saudi Arabia", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Hanan Ashrawi -> people.person.place_of_birth -> Nablus\n# Answer:\nHanan Ashrawi", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Abdullah of Saudi Arabia -> people.person.sibling_s -> m.0w2stn0\n# Answer:\nAbdullah of Saudi Arabia", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Abdullah of Saudi Arabia -> royalty.chivalric_order_member.belongs_to_order -> m.08psbkn\n# Answer:\nAbdullah of Saudi Arabia", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Hanan Ashrawi -> common.topic.notable_types -> Politician\n# Answer:\nHanan Ashrawi", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Abdullah of Saudi Arabia -> people.person.sibling_s -> m.0w2sv57\n# Answer:\nAbdullah of Saudi Arabia", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Abdullah of Saudi Arabia -> people.person.children -> Abdulaziz bin Abdullah bin Abdulaziz Al Saud\n# Answer:\nAbdullah of Saudi Arabia", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Hanan Ashrawi -> people.person.spouse_s -> m.011x04jz\n# Answer:\nHanan Ashrawi", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Menachem Begin -> book.author.works_written -> Hashk\u0323afat h\u0323ayim v\u0323e-hashk\u0323afah le\u02bcumit\n# Answer:\nMenachem Begin", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Menachem Begin -> military.military_person.service -> m.059d8ry\n# Answer:\nMenachem Begin"], "ground_truth": ["Marwan Barghouti", "Yitzhak Rabin", "Hussein of Jordan", "Yasser Arafat", "Anthony Zinni", "David Ben-Gurion", "Colin Powell", "Hanan Ashrawi", "Abdullah of Saudi Arabia", "Haj Amin al-Husseini", "Menachem Begin", "Ariel Sharon", "Ahmed Yassin", "Shimon Peres", "Nabil Shaath", "Anwar Sadat", "Mahmoud Abbas", "Ahmad Shukeiri", "Chaim Weizmann", "Ahmed Qurei", "Dalal Mughrabi"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 1.0, "ans_recall": 0.14285714285714285, "path_f1": 0.25, "path_precision": 1.0, "path_recall": 0.14285714285714285, "path_ans_f1": 0.25, "path_ans_precision": 1.0, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-1943", "prediction": ["# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Lamar Hunt -> sports.pro_athlete.teams -> m.0hsx4r3\n# Answer:\nLamar Hunt", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Lamar Hunt -> common.topic.notable_types -> American football player\n# Answer:\nLamar Hunt", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Bud Adams -> award.award_winner.awards_won -> m.0_qxnkc\n# Answer:\nBud Adams", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Bud Adams -> people.deceased_person.cause_of_death -> Natural causes\n# Answer:\nBud Adams", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Lamar Hunt -> people.person.education -> m.0hswbpc\n# Answer:\nLamar Hunt", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Bud Adams -> common.topic.article -> m.035yc8\n# Answer:\nBud Adams", "# Reasoning Path:\nAmerican Football League -> sports.sports_league.sport -> American football -> sports.sport.team_coaches -> Bob Hewko\n# Answer:\nAmerican football", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Lamar Hunt -> people.person.education -> m.0n0f2jb\n# Answer:\nLamar Hunt", "# Reasoning Path:\nAmerican Football League -> sports.sports_league.teams -> m.0crthgz -> sports.sports_league_participation.team -> Houston Oilers\n# Answer:\nm.0crthgz", "# Reasoning Path:\nAmerican Football League -> sports.sports_league.sport -> American football -> sports.sport.team_coaches -> Bobby Petrino\n# Answer:\nAmerican football"], "ground_truth": ["Lamar Hunt", "Bud Adams"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1944", "prediction": ["# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> travel.travel_destination.tourist_attractions -> Thyssen-Bornemisza Museum\n# Answer:\nMadrid", "# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> location.location.nearby_airports -> Adolfo Su\u00e1rez Madrid\u2013Barajas Airport\n# Answer:\nMadrid", "# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> travel.travel_destination.tourist_attractions -> Madrid Arena\n# Answer:\nMadrid", "# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> common.topic.webpage -> m.03l85_5\n# Answer:\nMadrid", "# Reasoning Path:\nDiego Vel\u00e1zquez -> common.topic.notable_types -> Visual Artist -> base.descriptive_names.names.descriptive_name -> m.0106_1y2\n# Answer:\nVisual Artist", "# Reasoning Path:\nDiego Vel\u00e1zquez -> common.topic.notable_types -> Visual Artist -> freebase.type_hints.included_types -> Person\n# Answer:\nVisual Artist", "# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> location.location.nearby_airports -> Cuatro Vientos Airport\n# Answer:\nMadrid", "# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> travel.travel_destination.tourist_attractions -> Almudena Cathedral\n# Answer:\nMadrid", "# Reasoning Path:\nDiego Vel\u00e1zquez -> common.topic.notable_types -> Visual Artist -> freebase.type_profile.published -> Published\n# Answer:\nVisual Artist", "# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> location.location.nearby_airports -> Getafe Air Base\n# Answer:\nMadrid"], "ground_truth": ["Madrid"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1946", "prediction": ["# Reasoning Path:\nMexico -> location.country.internet_tld -> mx\n# Answer:\nmx", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Maya, Yucat\u00e1n Language -> language.human_language.main_country -> Guatemala\n# Answer:\nMaya, Yucat\u00e1n Language", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Maya, Yucat\u00e1n Language -> common.topic.notable_types -> Human Language\n# Answer:\nMaya, Yucat\u00e1n Language", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Jakaltek language -> language.human_language.main_country -> Guatemala\n# Answer:\nJakaltek language", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Maya, Yucat\u00e1n Language -> language.human_language.language_family -> Mayan languages\n# Answer:\nMaya, Yucat\u00e1n Language", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Q'eqchi' Language -> common.topic.notable_types -> Human Language\n# Answer:\nQ'eqchi' Language", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Jakaltek language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nJakaltek language", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Q'eqchi' Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nQ'eqchi' Language"], "ground_truth": ["mx"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1947", "prediction": ["# Reasoning Path:\nSwitzerland -> location.country.currency_used -> Swiss franc -> finance.currency.countries_used -> Liechtenstein\n# Answer:\nSwiss franc", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d0tc3g\n# Answer:\ng.11b6d0tc3g", "# Reasoning Path:\nSwitzerland -> location.statistical_region.co2_emissions_per_capita -> g.1245_dgvf\n# Answer:\ng.1245_dgvf", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6p27nwl\n# Answer:\ng.11b6p27nwl", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.1hhc3d66v\n# Answer:\ng.1hhc3d66v", "# Reasoning Path:\nSwitzerland -> location.statistical_region.co2_emissions_per_capita -> g.1245_kjqq\n# Answer:\ng.1245_kjqq", "# Reasoning Path:\nSwitzerland -> location.statistical_region.co2_emissions_per_capita -> g.1245_kjqr\n# Answer:\ng.1245_kjqr"], "ground_truth": ["Swiss franc"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-195", "prediction": ["# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Fender Stratocaster -> music.guitar.guitarists -> Eric Clapton\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Fender Stratocaster -> common.topic.notable_types -> Guitar\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nGeorge Harrison -> music.group_member.instruments_played -> Bass guitar -> common.topic.notable_types -> Musical instrument\n# Answer:\nBass guitar", "# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Fender Stratocaster -> music.guitar.guitarists -> Jimi Hendrix\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Rickenbacker 360/12 -> music.guitar.guitarists -> Les Fradkin\n# Answer:\nRickenbacker 360/12", "# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Rickenbacker 360/12 -> common.topic.image -> Rickenbacker 360-12WB 12 String\n# Answer:\nRickenbacker 360/12", "# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Fender Stratocaster -> music.guitar.guitarists -> John Lennon\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Rickenbacker 360/12 -> common.topic.notable_for -> g.125cy8mvw\n# Answer:\nRickenbacker 360/12", "# Reasoning Path:\nGeorge Harrison -> music.group_member.instruments_played -> Bass guitar -> common.topic.subject_of -> Reverb\n# Answer:\nBass guitar", "# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Rickenbacker 360/12 -> music.guitar.guitarists -> The Edge\n# Answer:\nRickenbacker 360/12"], "ground_truth": ["Fender Stratocaster", "Rickenbacker 360/12"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1950", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_burial -> Arlington National Cemetery -> travel.tourist_attraction.near_travel_destination -> Arlington County\n# Answer:\nArlington National Cemetery", "# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_burial -> Arlington National Cemetery -> location.location.containedby -> Arlington County\n# Answer:\nArlington National Cemetery", "# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_burial -> Arlington National Cemetery -> base.schemastaging.context_name.pronunciation -> g.125_rp7jg\n# Answer:\nArlington National Cemetery", "# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_burial -> Arlington National Cemetery -> location.location.containedby -> Virginia\n# Answer:\nArlington National Cemetery", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr. -> symbols.name_source.namesakes -> John F. Kennedy Jr. plane crash\n# Answer:\nJohn F. Kennedy Jr.", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr. -> base.popstra.celebrity.friendship -> m.0645k94\n# Answer:\nJohn F. Kennedy Jr.", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School -> common.topic.notable_for -> g.125583hnb\n# Answer:\nBloomington Kennedy High School", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nIrish American", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> time.event.locations -> Dallas\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> common.topic.notable_types -> Event\n# Answer:\nAssassination of John F. Kennedy"], "ground_truth": ["Arlington National Cemetery"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1952", "prediction": ["# Reasoning Path:\nMuhammad in Islam -> people.person.place_of_birth -> Mecca -> location.location.containedby -> Saudi Arabia\n# Answer:\nMecca", "# Reasoning Path:\nMuhammad in Islam -> people.person.place_of_birth -> Mecca -> location.location.containedby -> Sharifate of Mecca\n# Answer:\nMecca", "# Reasoning Path:\nMuhammad in Islam -> people.person.place_of_birth -> Mecca -> location.location.contains -> Abraj Al Bait\n# Answer:\nMecca", "# Reasoning Path:\nMuhammad in Islam -> influence.influence_node.influenced -> Abd Allah ibn Abbas -> people.person.place_of_birth -> Mecca\n# Answer:\nAbd Allah ibn Abbas", "# Reasoning Path:\nMuhammad in Islam -> people.person.place_of_birth -> Mecca -> location.location.contains -> Abraj Al Bait Hajar Tower\n# Answer:\nMecca", "# Reasoning Path:\nMuhammad in Islam -> book.author.works_written -> Sayings of Mohammed -> common.topic.notable_types -> Book\n# Answer:\nSayings of Mohammed", "# Reasoning Path:\nMuhammad in Islam -> people.person.place_of_birth -> Mecca -> travel.travel_destination.tourist_attractions -> Kaaba\n# Answer:\nMecca", "# Reasoning Path:\nMuhammad in Islam -> people.person.place_of_birth -> Mecca -> location.location.contains -> Abraj Al Bait Maqam Tower\n# Answer:\nMecca", "# Reasoning Path:\nMuhammad in Islam -> people.person.place_of_birth -> Mecca -> travel.travel_destination.tourist_attractions -> Al Ja\u00e1ranah Mosque\n# Answer:\nMecca", "# Reasoning Path:\nMuhammad in Islam -> book.author.works_written -> Sayings of Mohammed -> common.topic.notable_for -> g.125gry845\n# Answer:\nSayings of Mohammed"], "ground_truth": ["Mecca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1954", "prediction": ["# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Hungarian language -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nHungarian language", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Hungarian language -> common.topic.notable_types -> Human Language\n# Answer:\nHungarian language", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Albanian language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nAlbanian language", "# Reasoning Path:\nUkraine -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6dc92bn\n# Answer:\ng.11b6dc92bn", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Hungarian language -> language.human_language.countries_spoken_in -> Croatia\n# Answer:\nHungarian language", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Albanian language -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nAlbanian language", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Romanian Language -> language.human_language.countries_spoken_in -> Romania\n# Answer:\nRomanian Language", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Romanian Language -> language.human_language.dialects -> Moldovan language\n# Answer:\nRomanian Language", "# Reasoning Path:\nUkraine -> location.country.official_language -> Ukrainian Language -> language.human_language.countries_spoken_in -> Russia\n# Answer:\nUkrainian Language"], "ground_truth": ["Hungarian language", "Romanian Language", "Albanian language", "Ukrainian Language", "Russian Language", "Tatar Language", "Moldovan language"], "ans_acc": 0.7142857142857143, "ans_hit": 1, "ans_f1": 0.6990291262135921, "ans_precission": 0.9, "ans_recall": 0.5714285714285714, "path_f1": 0.5581395348837209, "path_precision": 0.8, "path_recall": 0.42857142857142855, "path_ans_f1": 0.7964601769911505, "path_ans_precision": 0.9, "path_ans_recall": 0.7142857142857143}
{"id": "WebQTest-1955", "prediction": ["# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Making Our Democracy Work -> book.written_work.author -> Stephen Breyer\n# Answer:\nMaking Our Democracy Work", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Making Our Democracy Work -> book.written_work.original_language -> English Language\n# Answer:\nMaking Our Democracy Work", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Savannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal -> book.written_work.author -> Aberjhani\n# Answer:\nSavannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> people.appointed_role.appointment -> m.0hn35mm\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> law.court.inferior_courts -> Arkansas Supreme Court -> law.court.judges -> m.0wzgtvm\n# Answer:\nArkansas Supreme Court", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Making Our Democracy Work -> book.written_work.subjects -> United States Constitution\n# Answer:\nMaking Our Democracy Work", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Courtiers of the Marble Palace: The Rise and Influence of the Supreme Court Law Clerk -> book.written_work.subjects -> Law clerk\n# Answer:\nCourtiers of the Marble Palace: The Rise and Influence of the Supreme Court Law Clerk", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> people.appointed_role.appointment -> m.063dqnv\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> law.judicial_title.judges -> m.046x3bs\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> law.court.inferior_courts -> Arkansas Supreme Court -> law.court.legal_cases -> Howard v. Arkansas\n# Answer:\nArkansas Supreme Court"], "ground_truth": ["Antonin Scalia", "John Roberts", "Ruth Bader Ginsburg", "Anthony Kennedy", "Samuel Alito", "Stephen Breyer", "Sonia Sotomayor", "Elena Kagan", "Clarence Thomas"], "ans_acc": 0.1111111111111111, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.10526315789473685, "path_ans_precision": 0.1, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-1956", "prediction": ["# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Liechtenstein -> location.location.containedby -> Eurasia\n# Answer:\nLiechtenstein", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_area_type -> Sovereign state\n# Answer:\nLiechtenstein", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Hungary -> location.location.containedby -> Eurasia\n# Answer:\nHungary", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Liechtenstein -> location.location.containedby -> Western Europe\n# Answer:\nLiechtenstein", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Hungary -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nHungary", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Switzerland -> location.country.languages_spoken -> French\n# Answer:\nSwitzerland", "# Reasoning Path:\nEurope -> location.location.partially_contains -> Russia -> location.location.partially_contains -> Caspian Sea\n# Answer:\nRussia", "# Reasoning Path:\nEurope -> location.location.partially_contains -> Russia -> location.location.partially_contained_by -> m.0wg8l4g\n# Answer:\nRussia", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Hungary -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics\n# Answer:\nHungary", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Switzerland -> common.topic.notable_types -> Country\n# Answer:\nSwitzerland"], "ground_truth": ["Russia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1957", "prediction": ["# Reasoning Path:\nKaley Cuoco -> tv.tv_actor.starring_roles -> m.03jqc1n -> tv.regular_tv_appearance.seasons -> Charmed Season 8\n# Answer:\nm.03jqc1n", "# Reasoning Path:\nKaley Cuoco -> tv.tv_actor.starring_roles -> m.03jrq66 -> tv.regular_tv_appearance.seasons -> The Big Bang Theory - Season 6\n# Answer:\nm.03jrq66", "# Reasoning Path:\nKaley Cuoco -> tv.tv_actor.starring_roles -> m.03js5wk -> tv.regular_tv_appearance.series -> Brandy & Mr. Whiskers\n# Answer:\nm.03js5wk", "# Reasoning Path:\nKaley Cuoco -> tv.tv_actor.starring_roles -> m.03jqc1n -> tv.regular_tv_appearance.character -> Billie Jenkins\n# Answer:\nm.03jqc1n", "# Reasoning Path:\nKaley Cuoco -> tv.tv_actor.starring_roles -> m.03jrq66 -> tv.regular_tv_appearance.seasons -> The Big Bang Theory - Season 5\n# Answer:\nm.03jrq66", "# Reasoning Path:\nKaley Cuoco -> film.actor.film -> g.11b8czsxkc\n# Answer:\ng.11b8czsxkc", "# Reasoning Path:\nKaley Cuoco -> tv.tv_actor.starring_roles -> m.03jqc1n -> tv.regular_tv_appearance.series -> Charmed\n# Answer:\nm.03jqc1n", "# Reasoning Path:\nKaley Cuoco -> tv.tv_actor.starring_roles -> m.03jrq66 -> tv.regular_tv_appearance.seasons -> The Big Bang Theory - Season 1\n# Answer:\nm.03jrq66", "# Reasoning Path:\nKaley Cuoco -> tv.tv_actor.starring_roles -> m.03jrq66 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.03jrq66", "# Reasoning Path:\nKaley Cuoco -> award.award_nominee.award_nominations -> m.010wr05z -> award.award_nomination.award -> Teen Choice Award for Choice TV Actress: Comedy\n# Answer:\nm.010wr05z"], "ground_truth": ["2005-09-25"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1958", "prediction": ["# Reasoning Path:\nChris Rock -> people.person.place_of_birth -> Andrews -> location.location.people_born_here -> Dolphin D. Overton\n# Answer:\nAndrews", "# Reasoning Path:\nChris Rock -> people.person.place_of_birth -> Andrews -> location.location.containedby -> Georgetown County\n# Answer:\nAndrews", "# Reasoning Path:\nChris Rock -> people.person.place_of_birth -> Andrews -> location.location.containedby -> South Carolina\n# Answer:\nAndrews", "# Reasoning Path:\nChris Rock -> people.person.place_of_birth -> Andrews -> location.statistical_region.population -> g.11b674j202\n# Answer:\nAndrews", "# Reasoning Path:\nChris Rock -> music.featured_artist.recordings -> Everybody Hates Chris -> tv.tv_program.program_creator -> Ali LeRoi\n# Answer:\nEverybody Hates Chris", "# Reasoning Path:\nChris Rock -> people.person.place_of_birth -> Andrews -> location.location.containedby -> United States of America\n# Answer:\nAndrews", "# Reasoning Path:\nChris Rock -> people.person.place_of_birth -> Andrews -> location.statistical_region.population -> g.11x1clr4y\n# Answer:\nAndrews", "# Reasoning Path:\nChris Rock -> people.person.place_of_birth -> Andrews -> location.statistical_region.population -> g.11x1cr6w1\n# Answer:\nAndrews", "# Reasoning Path:\nChris Rock -> music.featured_artist.recordings -> My First Day -> music.recording.artist -> Prince Paul\n# Answer:\nMy First Day", "# Reasoning Path:\nChris Rock -> music.featured_artist.recordings -> Recognize -> music.recording.featured_artists -> Pharrell Williams\n# Answer:\nRecognize"], "ground_truth": ["Andrews"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-196", "prediction": ["# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> Navajo Language -> base.rosetta.languoid.document -> Navajo\n# Answer:\nNavajo Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Mandatory Palestine\n# Answer:\nEnglish Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> Navajo Language -> common.topic.notable_types -> Human Language\n# Answer:\nNavajo Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> Spanish Language -> base.rosetta.languoid.local_name -> Spanish\n# Answer:\nSpanish Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> Navajo Language -> base.rosetta.languoid.document -> Navajo Bibliography\n# Answer:\nNavajo Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> Navajo Language -> language.human_language.language_family -> Athabaskan languages\n# Answer:\nNavajo Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> English Language -> education.school_category.schools_of_this_kind -> Piers Midwinter\n# Answer:\nEnglish Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> Navajo Language -> base.rosetta.languoid.document -> Navajo Book\n# Answer:\nNavajo Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language"], "ground_truth": ["Navajo Language", "Spanish Language", "English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1960", "prediction": ["# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.championships -> 1967 Stanley Cup Finals -> sports.sports_championship_event.runner_up -> Montreal Canadiens\n# Answer:\n1967 Stanley Cup Finals", "# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.championships -> 1967 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n1967 Stanley Cup Finals", "# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.championships -> 1932 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n1932 Stanley Cup Finals", "# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.championships -> 1942 Stanley Cup Finals -> common.topic.article -> m.03cf4pr\n# Answer:\n1942 Stanley Cup Finals", "# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.championships -> 1932 Stanley Cup Finals -> common.topic.notable_for -> g.125fpr8l6\n# Answer:\n1932 Stanley Cup Finals", "# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.location -> Toronto -> travel.travel_destination.tourist_attractions -> Toronto Centre for the Arts\n# Answer:\nToronto", "# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.championships -> 1932 Stanley Cup Finals -> sports.sports_championship_event.season -> 1931\u201332 NHL season\n# Answer:\n1932 Stanley Cup Finals", "# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.championships -> 1942 Stanley Cup Finals -> sports.sports_championship_event.season -> 1941\u201342 NHL season\n# Answer:\n1942 Stanley Cup Finals", "# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.location -> Toronto -> travel.travel_destination.tourist_attractions -> Air Canada Centre\n# Answer:\nToronto", "# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.location -> Toronto -> common.topic.webpage -> m.02nc8t6\n# Answer:\nToronto"], "ground_truth": ["1967 Stanley Cup Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1961", "prediction": ["# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.region -> Africa\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> base.rosetta.languoid.parent -> Arabic Group\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Nobiin Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nNobiin Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.writing_system -> Arabic alphabet\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Domari Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nDomari Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Nobiin Language -> common.topic.notable_types -> Human Language\n# Answer:\nNobiin Language", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\ng.12cp_k6sh", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Domari Language -> common.topic.notable_types -> Human Language\n# Answer:\nDomari Language", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\ng.1hhc3_4cn", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives"], "ground_truth": ["Modern Standard Arabic"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1962", "prediction": ["# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Brazilian Portuguese -> language.language_dialect.language -> Portuguese Language\n# Answer:\nBrazilian Portuguese", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\ng.12cp_jcjy", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\ng.11b71vdgdz", "# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Portuguese Language -> language.human_language.dialects -> Brazilian Portuguese\n# Answer:\nPortuguese Language", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\ng.1hhc390hy", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc39sjp\n# Answer:\ng.1hhc39sjp", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fm01\n# Answer:\ng.12tb6fm01", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc385hl\n# Answer:\ng.1hhc385hl"], "ground_truth": ["Brazilian Portuguese", "Portuguese Language", "Italian Language"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.36363636363636365, "ans_precission": 0.25, "ans_recall": 0.6666666666666666, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 0.36363636363636365, "path_ans_precision": 0.25, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1963", "prediction": ["# Reasoning Path:\nVince Lombardi -> people.person.profession -> American football player -> common.topic.notable_types -> Profession\n# Answer:\nAmerican football player", "# Reasoning Path:\nVince Lombardi -> sports.pro_athlete.sports_played_professionally -> m.0d5jg18 -> sports.pro_sports_played.sport -> American football\n# Answer:\nm.0d5jg18", "# Reasoning Path:\nVince Lombardi -> people.person.profession -> American football player -> people.profession.specialization_of -> Athlete\n# Answer:\nAmerican football player", "# Reasoning Path:\nVince Lombardi -> people.person.profession -> Coach -> base.descriptive_names.names.descriptive_name -> m.0101cv25\n# Answer:\nCoach", "# Reasoning Path:\nVince Lombardi -> people.person.profession -> American football player -> fictional_universe.character_occupation.characters_with_this_occupation -> A. A. Ron Balakay\n# Answer:\nAmerican football player", "# Reasoning Path:\nVince Lombardi -> people.person.profession -> Coach -> base.schemastaging.sports_team_manager_position.managers -> m.01141mgz\n# Answer:\nCoach", "# Reasoning Path:\nVince Lombardi -> people.person.profession -> American football player -> people.profession.specialization_of -> Football player\n# Answer:\nAmerican football player", "# Reasoning Path:\nVince Lombardi -> people.person.profession -> Coach -> base.descriptive_names.names.descriptive_name -> m.0102zdqv\n# Answer:\nCoach", "# Reasoning Path:\nVince Lombardi -> people.person.profession -> Coach -> american_football.football_coach_position.coaches_holding_this_position -> m.0107gkfl\n# Answer:\nCoach", "# Reasoning Path:\nVince Lombardi -> people.person.profession -> American football player -> fictional_universe.character_occupation.characters_with_this_occupation -> D'Glester Hardunkichud\n# Answer:\nAmerican football player"], "ground_truth": ["Right guard"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1964", "prediction": ["# Reasoning Path:\nStanley Kowalski -> film.film_character.portrayed_in_films -> m.0jsxq4 -> film.performance.actor -> Marlon Brando\n# Answer:\nm.0jsxq4", "# Reasoning Path:\nA Streetcar Named Desire -> film.film.starring -> m.0jsxq4 -> film.performance.actor -> Marlon Brando\n# Answer:\nm.0jsxq4", "# Reasoning Path:\nStanley Kowalski -> film.film_character.portrayed_in_films -> m.0jsxq4 -> film.performance.film -> A Streetcar Named Desire\n# Answer:\nm.0jsxq4", "# Reasoning Path:\nA Streetcar Named Desire -> film.film.starring -> m.03l101c -> film.performance.actor -> Alec Baldwin\n# Answer:\nm.03l101c", "# Reasoning Path:\nStanley Kowalski -> film.film_character.portrayed_in_films -> m.03l101c -> film.performance.actor -> Alec Baldwin\n# Answer:\nm.03l101c", "# Reasoning Path:\nStanley Kowalski -> common.topic.article -> m.03bhwn\n# Answer:\nm.03bhwn", "# Reasoning Path:\nA Streetcar Named Desire -> film.film.starring -> m.0jsxq4 -> film.performance.character -> Stanley Kowalski\n# Answer:\nm.0jsxq4", "# Reasoning Path:\nA Streetcar Named Desire -> film.film.starring -> m.03l1032 -> film.performance.actor -> Treat Williams\n# Answer:\nm.03l1032", "# Reasoning Path:\nStanley Kowalski -> film.film_character.portrayed_in_films -> m.03l1032 -> film.performance.actor -> Treat Williams\n# Answer:\nm.03l1032", "# Reasoning Path:\nA Streetcar Named Desire -> award.award_nominated_work.award_nominations -> m.03mlrqc -> award.award_nomination.award -> Academy Award for Best Actor\n# Answer:\nm.03mlrqc"], "ground_truth": ["Marlon Brando"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.26666666666666666, "path_precision": 0.2, "path_recall": 0.4, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1965", "prediction": ["# Reasoning Path:\nLola Bunny -> common.topic.article -> m.0534rg\n# Answer:\nm.0534rg", "# Reasoning Path:\nLola Bunny -> film.film_character.portrayed_in_films -> m.0qst5_z -> film.performance.actor -> Britt McKillip\n# Answer:\nm.0qst5_z", "# Reasoning Path:\nLola Bunny -> film.film_character.portrayed_in_films -> m.0qst5_z -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0qst5_z", "# Reasoning Path:\nLola Bunny -> film.film_character.portrayed_in_films -> m.0y7rs3h -> film.performance.actor -> Kath Soucie\n# Answer:\nm.0y7rs3h", "# Reasoning Path:\nLola Bunny -> tv.tv_character.appeared_in_tv_program -> m.03lyj7h -> tv.regular_tv_appearance.actor -> Britt McKillip\n# Answer:\nm.03lyj7h", "# Reasoning Path:\nLola Bunny -> film.film_character.portrayed_in_films -> m.0qst5_z -> film.performance.film -> Baby Looney Tunes' Eggs-traordinary Adventure\n# Answer:\nm.0qst5_z", "# Reasoning Path:\nLola Bunny -> film.film_character.portrayed_in_films -> m.0y7rs3h -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0y7rs3h", "# Reasoning Path:\nLola Bunny -> tv.tv_character.appeared_in_tv_program -> m.03lyj7h -> tv.regular_tv_appearance.series -> Baby Looney Tunes\n# Answer:\nm.03lyj7h", "# Reasoning Path:\nLola Bunny -> tv.tv_character.appeared_in_tv_program -> m.0gl1bzt -> tv.regular_tv_appearance.actor -> Kristen Wiig\n# Answer:\nm.0gl1bzt", "# Reasoning Path:\nLola Bunny -> film.film_character.portrayed_in_films -> m.0y7rs3h -> film.performance.film -> Space Jam\n# Answer:\nm.0y7rs3h"], "ground_truth": ["Kristen Wiig", "Britt McKillip", "Kath Soucie"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1966", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 11: 1863\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin -> book.author.works_written -> The Autobiography of Charles Darwin\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Great Britain -> location.location.contains -> England\n# Answer:\nGreat Britain"], "ground_truth": ["Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Darwin Reader First Edition", "The living thoughts of Darwin", "The Descent of Man, and Selection in Relation to Sex", "From Darwin's unpublished notebooks", "Die geschlechtliche Zuchtwahl", "To the members of the Down Friendly Club", "Notebooks on transmutation of species", "Die fundamente zur entstehung der arten", "La vie et la correspondance de Charles Darwin", "The Variation of Animals and Plants under Domestication", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Kleinere geologische Abhandlungen", "Metaphysics, Materialism, & the evolution of mind", "Les moyens d'expression chez les animaux", "The Correspondence of Charles Darwin, Volume 8: 1860", "La facult\u00e9 motrice dans les plantes", "Monographs of the fossil Lepadidae and the fossil Balanidae", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The Orgin of Species", "On Natural Selection", "H.M.S. Beagle in South America", "Questions about the breeding of animals", "On the origin of species by means of natural selection", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The Formation of Vegetable Mould through the Action of Worms", "The Essential Darwin", "ontstaan der soorten door natuurlijke teeltkeus", "vari\u00eberen der huisdieren en cultuurplanten", "Diary of the voyage of H.M.S. Beagle", "The Power of Movement in Plants", "Part I: Contributions to the Theory of Natural Selection / Part II", "The collected papers of Charles Darwin", "The Correspondence of Charles Darwin, Volume 17: 1869", "Darwin-Wallace", "The foundations of the Origin of species", "On evolution", "The Expression of the Emotions in Man and Animals", "The Correspondence of Charles Darwin, Volume 15: 1867", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "Volcanic Islands", "The Correspondence of Charles Darwin, Volume 18: 1870", "Charles Darwin's marginalia", "Evolution and natural selection", "red notebook of Charles Darwin", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Beagle letters", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Darwin en Patagonia", "Darwin", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "The principal works", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "The Correspondence of Charles Darwin, Volume 10: 1862", "Darwin and Henslow", "Voyage d'un naturaliste autour du monde", "Darwin for Today", "Les mouvements et les habitudes des plantes grimpantes", "Fertilisation of Orchids", "The Correspondence of Charles Darwin, Volume 12: 1864", "monograph on the sub-class Cirripedia", "On the Movements and Habits of Climbing Plants", "Darwin's Ornithological notes", "Reise um die Welt 1831 - 36", "The Correspondence of Charles Darwin, Volume 11: 1863", "Darwin's insects", "Leben und Briefe von Charles Darwin", "The Autobiography of Charles Darwin", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Tesakneri tsagume\u030c", "Darwin's journal", "Rejse om jorden", "Darwin Darwin", "Reise eines Naturforschers um die Welt", "Cartas de Darwin 18251859", "Charles Darwin", "The Life of Erasmus Darwin", "genese\u014ds t\u014dn eid\u014dn", "A student's introduction to Charles Darwin", "The action of carbonate of ammonia on the roots of certain plants", "Evolution by natural selection", "Darwin on humus and the earthworm", "The Correspondence of Charles Darwin, Volume 9: 1861", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "On the tendency of species to form varieties", "Del Plata a Tierra del Fuego", "The voyage of Charles Darwin", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "The\u0301orie de l'e\u0301volution", "Motsa ha-minim", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "El Origin De Las Especies", "The Darwin Reader Second Edition", "Les r\u00e9cifs de corail, leur structure et leur distribution", "A Darwin Selection", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Diario del Viaje de Un Naturalista Alrededor", "Geological Observations on South America", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "Notes on the fertilization of orchids", "The Structure and Distribution of Coral Reefs", "Het uitdrukken van emoties bij mens en dier", "Works", "The Correspondence of Charles Darwin, Volume 16: 1868", "Gesammelte kleinere Schriften", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Opsht\u0323amung fun menshen", "The education of Darwin", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Darwin's notebooks on transmutation of species", "Darwinism stated by Darwin himself", "The portable Darwin", "From so simple a beginning", "The Voyage of the Beagle", "Memorias y epistolario i\u0301ntimo", "More Letters of Charles Darwin", "Darwin Compendium", "Wu zhong qi yuan", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Origins", "Charles Darwin on the routes of male humble bees", "The Different Forms of Flowers on Plants of the Same Species", "Human nature, Darwin's view", "On a remarkable bar of sandstone off Pernambuco", "The geology of the voyage of H.M.S. Beagle", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Insectivorous Plants", "Charles Darwin's natural selection", "The Correspondence of Charles Darwin, Volume 13: 1865", "Charles Darwin's letters", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "Proiskhozhdenie vidov", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "Evolution", "The Correspondence of Charles Darwin, Volume 14: 1866", "Resa kring jorden"], "ans_acc": 0.056338028169014086, "ans_hit": 1, "ans_f1": 0.027397260273972605, "ans_precission": 0.5, "ans_recall": 0.014084507042253521, "path_f1": 0.3636363636363636, "path_precision": 1.0, "path_recall": 0.2222222222222222, "path_ans_f1": 0.10666666666666667, "path_ans_precision": 1.0, "path_ans_recall": 0.056338028169014086}
{"id": "WebQTest-1967", "prediction": ["# Reasoning Path:\nSerbs -> people.ethnicity.languages_spoken -> Serbian language -> language.human_language.countries_spoken_in -> Republic of Macedonia\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbs -> people.ethnicity.languages_spoken -> Serbian language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbs -> people.ethnicity.languages_spoken -> Serbian language -> language.human_language.main_country -> Serbia\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbs -> people.ethnicity.languages_spoken -> Serbian language -> language.human_language.writing_system -> Serbian Cyrillic alphabet\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> base.rosetta.languoid.local_name -> Serbian\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbs -> people.ethnicity.languages_spoken -> Serbian language -> language.human_language.countries_spoken_in -> Bosnia and Herzegovina\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Bosnia and Herzegovina\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbs -> people.ethnicity.languages_spoken -> Serbian language -> language.human_language.writing_system -> Cyrillic script\n# Answer:\nSerbian language"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1968", "prediction": ["# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> people.cause_of_death.includes_causes_of_death -> Assassination\n# Answer:\nMurder", "# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> common.topic.webpage -> m.09w021v\n# Answer:\nMurder", "# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> people.cause_of_death.includes_causes_of_death -> Carjacking\n# Answer:\nMurder", "# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> base.fight.crime_type.crimes_of_this_type -> 101 California Street shooting\n# Answer:\nMurder", "# Reasoning Path:\nLaci Peterson -> base.popstra.celebrity.breakup -> m.0j3sj1q -> base.popstra.breakup.participant -> William Kent Gain\n# Answer:\nm.0j3sj1q", "# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> common.topic.webpage -> m.09w03yz\n# Answer:\nMurder", "# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> people.cause_of_death.includes_causes_of_death -> Honor killing\n# Answer:\nMurder", "# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> common.topic.webpage -> m.09w1yxt\n# Answer:\nMurder", "# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> base.fight.crime_type.crimes_of_this_type -> 1960s London Nude Murders\n# Answer:\nMurder", "# Reasoning Path:\nLaci Peterson -> tv.tv_program_guest.appeared_on -> m.0j6p5j6 -> tv.tv_guest_personal_appearance.episode -> Scott Peterson\n# Answer:\nm.0j6p5j6"], "ground_truth": ["Murder"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1969", "prediction": ["# Reasoning Path:\nKris Humphries -> base.schemastaging.athlete_extra.salary -> m.0j2kv6s -> base.schemastaging.athlete_salary.team -> Brooklyn Nets\n# Answer:\nm.0j2kv6s", "# Reasoning Path:\nKris Humphries -> sports.pro_athlete.teams -> m.0115v9w9 -> sports.sports_team_roster.team -> Washington Wizards\n# Answer:\nm.0115v9w9", "# Reasoning Path:\nKris Humphries -> base.schemastaging.athlete_extra.salary -> m.0j2kv6s -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0j2kv6s", "# Reasoning Path:\nKris Humphries -> base.schemastaging.athlete_extra.salary -> m.0w7q6my -> base.schemastaging.athlete_salary.team -> Boston Celtics\n# Answer:\nm.0w7q6my", "# Reasoning Path:\nKris Humphries -> sports.pro_athlete.teams -> m.0115v9w9 -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0115v9w9", "# Reasoning Path:\nKris Humphries -> sports.pro_athlete.teams -> m.0115v9w9 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0115v9w9", "# Reasoning Path:\nKris Humphries -> sports.pro_athlete.teams -> m.0j2gfzd -> sports.sports_team_roster.team -> Brooklyn Nets\n# Answer:\nm.0j2gfzd", "# Reasoning Path:\nKris Humphries -> base.schemastaging.athlete_extra.salary -> m.0w7q6my -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0w7q6my", "# Reasoning Path:\nKris Humphries -> sports.pro_athlete.teams -> m.0j2gfzd -> sports.sports_team_roster.position -> Power forward\n# Answer:\nm.0j2gfzd", "# Reasoning Path:\nKris Humphries -> sports.pro_athlete.teams -> m.0125nmsc -> sports.sports_team_roster.position -> Center\n# Answer:\nm.0125nmsc"], "ground_truth": ["Washington Wizards"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-197", "prediction": ["# Reasoning Path:\nNew England -> location.location.contains -> Hillsborough County -> location.location.containedby -> New Hampshire\n# Answer:\nHillsborough County", "# Reasoning Path:\nNew England -> location.location.contains -> Hillsborough County -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nHillsborough County", "# Reasoning Path:\nNew England -> location.location.contains -> Massachusetts -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMassachusetts", "# Reasoning Path:\nNew England -> location.location.contains -> Hillsborough County -> location.location.containedby -> North America\n# Answer:\nHillsborough County", "# Reasoning Path:\nNew England -> location.location.contains -> Hillsborough County -> location.location.containedby -> Northeastern United States\n# Answer:\nHillsborough County", "# Reasoning Path:\nNew England -> location.location.contains -> Hillsborough County -> location.statistical_region.co2_emissions_commercial -> m.045jfqg\n# Answer:\nHillsborough County", "# Reasoning Path:\nNew England -> location.location.contains -> Massachusetts -> location.location.containedby -> Contiguous United States\n# Answer:\nMassachusetts", "# Reasoning Path:\nNew England -> meteorology.cyclone_affected_area.cyclones -> Gale of 1878 -> meteorology.tropical_cyclone.affected_areas -> Bahamas\n# Answer:\nGale of 1878", "# Reasoning Path:\nNew England -> location.location.contains -> Massachusetts -> location.location.containedby -> United States of America\n# Answer:\nMassachusetts", "# Reasoning Path:\nNew England -> location.location.contains -> New Hampshire -> location.location.containedby -> Contiguous United States\n# Answer:\nNew Hampshire"], "ground_truth": ["Rhode Island", "Massachusetts", "New Hampshire", "Maine", "Vermont", "Connecticut"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.4, "ans_recall": 0.3333333333333333, "path_f1": 0.3636363636363636, "path_precision": 0.4, "path_recall": 0.3333333333333333, "path_ans_f1": 0.4, "path_ans_precision": 0.5, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1971", "prediction": ["# Reasoning Path:\nGerard Butler -> film.actor.film -> m.0jv821 -> film.performance.film -> Beowulf & Grendel\n# Answer:\nm.0jv821", "# Reasoning Path:\nGerard Butler -> film.actor.film -> m.0y50kwx -> film.performance.film -> Gods of Egypt\n# Answer:\nm.0y50kwx", "# Reasoning Path:\nGerard Butler -> award.award_nominee.award_nominations -> m.0z8k5mj -> award.award_nomination.nominated_for -> The Bounty Hunter\n# Answer:\nm.0z8k5mj", "# Reasoning Path:\nGerard Butler -> award.award_nominee.award_nominations -> m.0g8kc21 -> award.award_nomination.nominated_for -> The Bounty Hunter\n# Answer:\nm.0g8kc21", "# Reasoning Path:\nGerard Butler -> film.actor.film -> g.11b6gjwd97\n# Answer:\ng.11b6gjwd97", "# Reasoning Path:\nGerard Butler -> film.actor.film -> m.0jv821 -> film.performance.character -> Beowulf\n# Answer:\nm.0jv821", "# Reasoning Path:\nGerard Butler -> film.actor.film -> m.0y50kwx -> film.performance.character -> Set\n# Answer:\nm.0y50kwx", "# Reasoning Path:\nGerard Butler -> award.award_nominee.award_nominations -> m.0z8k5mj -> award.award_nomination.nominated_for -> The Ugly Truth\n# Answer:\nm.0z8k5mj", "# Reasoning Path:\nGerard Butler -> award.award_nominee.award_nominations -> m.0z8k5mj -> award.award_nomination.award -> Teen Choice Award for Choice Movie Actor: Romantic Comedy\n# Answer:\nm.0z8k5mj", "# Reasoning Path:\nGerard Butler -> award.award_nominee.award_nominations -> m.0z8k5mj -> award.award_nomination.ceremony -> 2010 Teen Choice Awards\n# Answer:\nm.0z8k5mj"], "ground_truth": ["Beowulf & Grendel", "How to Train Your Dragon: Legend of the Boneknapper Dragon", "Justin and the Knights of Valour", "Geostorm", "Tales of the Black Freighter", "How to Train Your Dragon", "Machine Gun Preacher", "Jewel of the Sahara", "How to Train Your Dragon 3", "Dynamo", "Dear Frankie", "Olympus Has Fallen", "Nim's Island", "Chasing Mavericks", "Timeline", "Gamer", "Shooters", "Playing for Keeps", "Fast Food", "RocknRolla", "Coriolanus", "Attila", "Law Abiding Citizen", "Harrison's Flowers", "Trailer for a Remake of Gore Vidal's Caligula", "Tomorrow Never Dies", "London Has Fallen", "How to Train Your Dragon 2", "Mrs Brown", "Reign of Fire", "The Cherry Orchard", "The Bounty Hunter", "300", "Thunder Run", "One More Kiss", "P.S. I Love You", "Butterfly on a Wheel", "Dracula 2000", "The Ugly Truth", "Movie 43", "The Phantom of the Opera", "The Game of Their Lives", "Lara Croft Tomb Raider: The Cradle of Life", "Gods of Egypt", "Wrath of Gods", "How to Train Your Dragon: Gift of the Night Fury"], "ans_acc": 0.08695652173913043, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14285714285714285, "path_precision": 0.5, "path_recall": 0.08333333333333333, "path_ans_f1": 0.14814814814814817, "path_ans_precision": 0.5, "path_ans_recall": 0.08695652173913043}
{"id": "WebQTest-1972", "prediction": ["# Reasoning Path:\nJohn Franklin -> people.person.place_of_birth -> Blue Island -> location.statistical_region.population -> g.11b66snzrw\n# Answer:\nBlue Island", "# Reasoning Path:\nJohn Franklin -> people.person.places_lived -> m.03pqb5g -> people.place_lived.location -> Blue Island\n# Answer:\nm.03pqb5g", "# Reasoning Path:\nJohn Franklin -> people.person.place_of_birth -> Blue Island -> location.hud_foreclosure_area.total_90_day_vacant_residential_addresses -> m.07fg9l6\n# Answer:\nBlue Island", "# Reasoning Path:\nJohn Franklin -> people.person.place_of_birth -> Blue Island -> location.statistical_region.population -> g.11bc88rzdw\n# Answer:\nBlue Island", "# Reasoning Path:\nJohn Franklin -> people.person.place_of_birth -> Blue Island -> location.location.people_born_here -> Anthony E. Zuiker\n# Answer:\nBlue Island", "# Reasoning Path:\nJohn Franklin -> people.person.place_of_birth -> Blue Island -> location.statistical_region.population -> g.11x1cxrps\n# Answer:\nBlue Island", "# Reasoning Path:\nJohn Franklin -> people.person.place_of_birth -> Blue Island -> location.location.people_born_here -> Bill Stapleton\n# Answer:\nBlue Island", "# Reasoning Path:\nJohn Franklin -> people.person.place_of_birth -> Blue Island -> location.location.people_born_here -> Bobby Frasor\n# Answer:\nBlue Island", "# Reasoning Path:\nJohn Franklin -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nJohn Franklin -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#domain -> Person\n# Answer:\nCountry of nationality"], "ground_truth": ["Blue Island"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1975", "prediction": ["# Reasoning Path:\nSean Connery -> film.actor.film -> g.11b7sqlhrr\n# Answer:\ng.11b7sqlhrr", "# Reasoning Path:\nSean Connery -> film.producer.film -> Looney Tunes: Back in Action -> film.film.genre -> Adventure Film\n# Answer:\nLooney Tunes: Back in Action", "# Reasoning Path:\nSean Connery -> film.producer.film -> Looney Tunes: Back in Action -> film.film.genre -> Animation\n# Answer:\nLooney Tunes: Back in Action", "# Reasoning Path:\nSean Connery -> film.producer.film -> Looney Tunes: Back in Action -> media_common.netflix_title.netflix_genres -> Family\n# Answer:\nLooney Tunes: Back in Action", "# Reasoning Path:\nSean Connery -> film.actor.film -> m.021ymvc -> film.performance.film -> Never Say Never Again\n# Answer:\nm.021ymvc", "# Reasoning Path:\nSean Connery -> film.producer.film -> Looney Tunes: Back in Action -> film.film.genre -> Comedy\n# Answer:\nLooney Tunes: Back in Action", "# Reasoning Path:\nSean Connery -> film.producer.film -> Looney Tunes: Back in Action -> film.film.language -> English Language\n# Answer:\nLooney Tunes: Back in Action", "# Reasoning Path:\nSean Connery -> film.actor.film -> m.022qj4c -> film.performance.film -> Thunderball\n# Answer:\nm.022qj4c", "# Reasoning Path:\nSean Connery -> film.producer.film -> Looney Tunes: Back in Action -> media_common.netflix_title.netflix_genres -> Family Animation\n# Answer:\nLooney Tunes: Back in Action", "# Reasoning Path:\nSean Connery -> film.producer.film -> Entrapment -> film.film.starring -> m.0g8mhpk\n# Answer:\nEntrapment"], "ground_truth": ["Sir Billi"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1976", "prediction": ["# Reasoning Path:\nDidier Drogba -> sports.pro_athlete.teams -> m.04m2qyj -> sports.sports_team_roster.team -> Galatasaray S.K.\n# Answer:\nm.04m2qyj", "# Reasoning Path:\nDidier Drogba -> sports.pro_athlete.teams -> m.04m2440 -> sports.sports_team_roster.team -> Chelsea F.C.\n# Answer:\nm.04m2440", "# Reasoning Path:\nDidier Drogba -> sports.pro_athlete.teams -> m.04m2qyj -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.04m2qyj", "# Reasoning Path:\nDidier Drogba -> sports.pro_athlete.teams -> m.011b_szr -> sports.sports_team_roster.team -> Chelsea F.C.\n# Answer:\nm.011b_szr", "# Reasoning Path:\nDidier Drogba -> sports.pro_athlete.teams -> m.04m2qyj -> sports.sports_team_roster.position -> Forward\n# Answer:\nm.04m2qyj", "# Reasoning Path:\nDidier Drogba -> sports.pro_athlete.teams -> m.011b_szr -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nm.011b_szr", "# Reasoning Path:\nDidier Drogba -> base.schemastaging.athlete_extra.salary -> m.0_lykg_ -> base.schemastaging.athlete_salary.currency -> Euro\n# Answer:\nm.0_lykg_", "# Reasoning Path:\nDidier Drogba -> sports.pro_athlete.teams -> m.011b_szr -> sports.sports_team_roster.position -> Forward\n# Answer:\nm.011b_szr", "# Reasoning Path:\nDidier Drogba -> base.schemastaging.athlete_extra.salary -> m.0s9fqn7 -> base.schemastaging.athlete_salary.currency -> Euro\n# Answer:\nm.0s9fqn7", "# Reasoning Path:\nDidier Drogba -> base.schemastaging.athlete_extra.salary -> m.0_lykg_ -> base.schemastaging.athlete_salary.team -> Galatasaray S.K.\n# Answer:\nm.0_lykg_"], "ground_truth": ["Shanghai Greenland Shenhua F.C.", "Le Mans FC", "Olympique de Marseille", "Ivory Coast national football team", "Galatasaray S.K.", "Chelsea F.C.", "En Avant de Guingamp"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.21621621621621623, "path_precision": 0.4, "path_recall": 0.14814814814814814, "path_ans_f1": 0.3333333333333333, "path_ans_precision": 0.4, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-1977", "prediction": ["# Reasoning Path:\nMichael Crabtree -> people.person.education -> m.040l5fn -> education.education.institution -> David W. Carter High School\n# Answer:\nm.040l5fn", "# Reasoning Path:\nMichael Crabtree -> people.person.education -> m.04m7z4n -> education.education.institution -> Texas Tech University\n# Answer:\nm.04m7z4n", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.games -> m.07ych9z -> american_football.player_game_statistics.season -> 2009 NFL season\n# Answer:\nm.07ych9z", "# Reasoning Path:\nMichael Crabtree -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.games -> m.07ych9z -> american_football.player_game_statistics.team -> San Francisco 49ers\n# Answer:\nm.07ych9z", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.games -> m.07z2fhs -> american_football.player_game_statistics.season -> 2009 NFL season\n# Answer:\nm.07z2fhs", "# Reasoning Path:\nMichael Crabtree -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nMichael Crabtree -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nMale", "# Reasoning Path:\nMichael Crabtree -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Acute myeloid leukemia\n# Answer:\nMale", "# Reasoning Path:\nMichael Crabtree -> people.person.gender -> Male -> biology.hybrid_parent_gender.hybrids -> m.0blp580\n# Answer:\nMale"], "ground_truth": ["Texas Tech University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1978", "prediction": ["# Reasoning Path:\nFredrik Ljungberg -> people.person.place_of_birth -> Vittsj\u00f6 -> location.location.containedby -> H\u00e4ssleholm Municipality\n# Answer:\nVittsj\u00f6", "# Reasoning Path:\nFredrik Ljungberg -> people.person.place_of_birth -> Vittsj\u00f6 -> location.location.geolocation -> m.0d33bvv\n# Answer:\nVittsj\u00f6", "# Reasoning Path:\nFredrik Ljungberg -> people.person.place_of_birth -> Vittsj\u00f6 -> location.location.containedby -> Scania\n# Answer:\nVittsj\u00f6", "# Reasoning Path:\nFredrik Ljungberg -> people.person.place_of_birth -> Vittsj\u00f6 -> location.location.people_born_here -> Albin Tingsvall\n# Answer:\nVittsj\u00f6", "# Reasoning Path:\nFredrik Ljungberg -> people.person.place_of_birth -> Vittsj\u00f6 -> location.location.containedby -> Sk\u00e5ne County\n# Answer:\nVittsj\u00f6", "# Reasoning Path:\nFredrik Ljungberg -> film.actor.film -> m.0tllb4x -> film.performance.film -> Goal II: Living the Dream\n# Answer:\nm.0tllb4x", "# Reasoning Path:\nFredrik Ljungberg -> people.person.place_of_birth -> Vittsj\u00f6 -> location.location.people_born_here -> Gustav Fridolin\n# Answer:\nVittsj\u00f6", "# Reasoning Path:\nFredrik Ljungberg -> people.person.places_lived -> m.0wkctn7 -> people.place_lived.location -> Vittsj\u00f6\n# Answer:\nm.0wkctn7", "# Reasoning Path:\nFredrik Ljungberg -> film.actor.film -> m.0tllb4x -> film.performance.special_performance_type -> Him/Herself\n# Answer:\nm.0tllb4x"], "ground_truth": ["Vittsj\u00f6"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-1979", "prediction": ["# Reasoning Path:\nMiami Dolphins -> sports.sports_team.league -> m.0crt4zr -> sports.sports_league_participation.league -> National Football League\n# Answer:\nm.0crt4zr", "# Reasoning Path:\nMiami Dolphins -> sports.sports_team.league -> m.0crtfcl -> sports.sports_league_participation.league -> AFC East\n# Answer:\nm.0crtfcl", "# Reasoning Path:\nMiami Dolphins -> sports.sports_team.league -> m.0crt9k9 -> sports.sports_league_participation.league -> American Football Conference\n# Answer:\nm.0crt9k9", "# Reasoning Path:\nMiami Dolphins -> base.schemastaging.organization_extra.contact_webpages -> m.010g8ggp -> freebase.valuenotation.is_reviewed -> Language\n# Answer:\nm.010g8ggp", "# Reasoning Path:\nMiami Dolphins -> base.schemastaging.organization_extra.contact_webpages -> m.010g8ggp -> internet.localized_uri.language -> English\n# Answer:\nm.010g8ggp", "# Reasoning Path:\nMiami Dolphins -> base.schemastaging.organization_extra.contact_webpages -> m.010g8ggp -> internet.localized_uri.location -> United States of America\n# Answer:\nm.010g8ggp", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.010hdmz0 -> sports.sports_league_draft_pick.player -> Jarvis Landry\n# Answer:\nm.010hdmz0", "# Reasoning Path:\nMiami Dolphins -> base.schemastaging.organization_extra.contact_webpages -> m.010g8ggp -> freebase.valuenotation.is_reviewed -> Location\n# Answer:\nm.010g8ggp", "# Reasoning Path:\nMiami Dolphins -> base.schemastaging.organization_extra.contact_webpages -> m.010g8ggp -> freebase.valuenotation.is_reviewed -> URI\n# Answer:\nm.010g8ggp", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.010hdmz0 -> sports.sports_league_draft_pick.school -> Louisiana State University\n# Answer:\nm.010hdmz0"], "ground_truth": ["Jamar Taylor"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1981", "prediction": ["# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.09ryhrl -> government.government_position_held.district_represented -> Province of Massachusetts Bay\n# Answer:\nm.09ryhrl", "# Reasoning Path:\nJohn Adams -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nJohn Adams -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Rhode Island\n# Answer:\nUnited States of America", "# Reasoning Path:\nJohn Adams -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.04466xb -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nm.04466xb", "# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.09ryhrl -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nm.09ryhrl", "# Reasoning Path:\nJohn Adams -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Rhode Island\n# Answer:\nUnited States of America", "# Reasoning Path:\nJohn Adams -> government.political_appointer.appointees -> m.04kq1pf -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nm.04kq1pf", "# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.04mm9px -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nm.04mm9px", "# Reasoning Path:\nJohn Adams -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Idaho\n# Answer:\nUnited States of America"], "ground_truth": ["Massachusetts"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1982", "prediction": ["# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Baroque music -> music.genre.albums -> A Bit o' This & That\n# Answer:\nBaroque music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Baroque music -> common.topic.image -> Baschenis - Musical Instruments\n# Answer:\nBaroque music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Baroque music -> music.genre.albums -> Baroque Masterpieces\n# Answer:\nBaroque music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> broadcast.genre.content -> Classica\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Baroque music -> music.genre.albums -> Baroque Rocks!\n# Answer:\nBaroque music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Baroque music -> influence.influence_node.influenced -> Les Fradkin\n# Answer:\nBaroque music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> broadcast.genre.content -> 1.FM Otto's  Baroque music\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> radio.radio_subject.programs_with_this_subject -> Adventures in Good Music\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> base.ontologies.ontology_instance.equivalent_instances -> m.07ndktx -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nm.07ndktx", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Baroque music -> influence.influence_node.influenced -> The Left Banke\n# Answer:\nBaroque music"], "ground_truth": ["Baroque music", "Classical music"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1985", "prediction": ["# Reasoning Path:\nMike Fisher -> sports.pro_athlete.teams -> m.0j20pg1 -> sports.sports_team_roster.team -> Ottawa Senators\n# Answer:\nm.0j20pg1", "# Reasoning Path:\nMike Fisher -> sports.pro_athlete.teams -> m.0j2c449 -> sports.sports_team_roster.team -> Nashville Predators\n# Answer:\nm.0j2c449", "# Reasoning Path:\nMike Fisher -> base.popstra.celebrity.canoodled -> m.063hchs -> base.popstra.canoodled.participant -> Carrie Underwood\n# Answer:\nm.063hchs", "# Reasoning Path:\nMike Fisher -> sports.pro_athlete.teams -> m.0j2c449 -> sports.sports_team_roster.position -> Centerman\n# Answer:\nm.0j2c449", "# Reasoning Path:\nMike Fisher -> people.person.profession -> Actor -> people.profession.specializations -> Audio Dramatist\n# Answer:\nActor", "# Reasoning Path:\nMike Fisher -> people.person.profession -> Actor -> people.profession.specializations -> Camgirl\n# Answer:\nActor", "# Reasoning Path:\nMike Fisher -> people.person.profession -> Actor -> music.special_music_video_performance_type.special_music_video_performances -> m.010wvs8m\n# Answer:\nActor", "# Reasoning Path:\nMike Fisher -> people.person.profession -> Ice hockey player -> base.descriptive_names.names.descriptive_name -> m.0101yndj\n# Answer:\nIce hockey player", "# Reasoning Path:\nMike Fisher -> people.person.profession -> Actor -> people.profession.specializations -> Character Actor\n# Answer:\nActor", "# Reasoning Path:\nMike Fisher -> people.person.profession -> Actor -> base.descriptive_names.names.descriptive_name -> m.01012_4j\n# Answer:\nActor"], "ground_truth": ["Nashville Predators"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1986", "prediction": ["# Reasoning Path:\nDaniel Craig -> award.award_nominee.award_nominations -> m.01140w77 -> award.award_nomination.nominated_for -> One Life\n# Answer:\nm.01140w77", "# Reasoning Path:\nDaniel Craig -> award.award_nominee.award_nominations -> m.07zmwhv -> award.award_nomination.nominated_for -> Casino Royale\n# Answer:\nm.07zmwhv", "# Reasoning Path:\nDaniel Craig -> award.award_nominee.award_nominations -> m.09zcl3b -> award.award_nomination.nominated_for -> Infamous\n# Answer:\nm.09zcl3b", "# Reasoning Path:\nDaniel Craig -> tv.tv_personality.tv_regular_appearances -> m.0zvr46v -> tv.tv_regular_personal_appearance.program -> MTV Europe Music Awards 2006\n# Answer:\nm.0zvr46v", "# Reasoning Path:\nDaniel Craig -> award.award_nominee.award_nominations -> m.01140w77 -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Narrator\n# Answer:\nm.01140w77", "# Reasoning Path:\nDaniel Craig -> award.award_nominee.award_nominations -> m.07zmwhv -> award.award_nomination.award -> BAFTA Award for Best Actor in a Leading Role\n# Answer:\nm.07zmwhv", "# Reasoning Path:\nDaniel Craig -> tv.tv_personality.tv_regular_appearances -> m.0zvr46v -> tv.tv_regular_personal_appearance.appearance_type -> Him/Herself\n# Answer:\nm.0zvr46v", "# Reasoning Path:\nDaniel Craig -> film.actor.film -> m.02h7bc8 -> film.performance.film -> The Invasion\n# Answer:\nm.02h7bc8", "# Reasoning Path:\nDaniel Craig -> award.award_nominee.award_nominations -> m.09zcl3b -> award.award_nomination.ceremony -> 22nd Independent Spirit Awards\n# Answer:\nm.09zcl3b", "# Reasoning Path:\nDaniel Craig -> film.actor.film -> m.02hw79s -> film.performance.film -> Casino Royale\n# Answer:\nm.02hw79s"], "ground_truth": ["A Kid in King Arthur's Court", "Elizabeth", "Layer Cake", "Archangel", "Skyfall", "The Adventures of Tintin: The Secret of the Unicorn", "The Trench", "Saint-Ex", "Dream House", "The Adventures of Young Indiana Jones: Daredevils of the Desert", "The Girl with the Dragon Tattoo", "Copenhagen", "The Power of One", "Enduring Love", "Sharpe's Eagle", "Hotel Splendide", "Obsession", "Kiss and Tell", "Spectre", "The Jacket", "Love Is the Devil: Study for a Portrait of Francis Bacon", "Bond 25", "Flashbacks of a Fool", "Defiance", "Sword of Honour", "Renaissance", "The Organ Grinder's Monkey", "Quantum of Solace", "Munich", "Some Voices", "Sylvia", "The Golden Compass", "The Girl Who Played with Fire", "Cowboys & Aliens", "James Bond Supports International Women's Day", "Casino Royale", "The Invasion", "Fateless", "Lara Croft: Tomb Raider", "Road to Perdition", "Infamous", "I Dreamed of Africa", "The Mother", "Love and Rage"], "ans_acc": 0.06818181818181818, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.10126582278481013, "path_precision": 0.4, "path_recall": 0.057971014492753624, "path_ans_f1": 0.11650485436893203, "path_ans_precision": 0.4, "path_ans_recall": 0.06818181818181818}
{"id": "WebQTest-1987", "prediction": ["# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> visual_art.art_period_movement.associated_artworks -> Elemental: The Power of Illuminated Love\n# Answer:\nModern art", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.art_forms -> Sculpture -> award.award_discipline.awards_in_this_discipline -> Kyoto Prize in Arts and Philosophy\n# Answer:\nSculpture", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.art_forms -> Sculpture -> common.topic.subjects -> Artist\n# Answer:\nSculpture", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> visual_art.art_period_movement.associated_artworks -> Flayed Image\n# Answer:\nModern art", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.art_forms -> Sculpture -> award.award_discipline.awards_in_this_discipline -> Wolf Prize in Arts\n# Answer:\nSculpture", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.art_forms -> Sculpture -> media_common.netflix_genre.titles -> Arts and Myths: Vol. 1\n# Answer:\nSculpture", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.associated_periods_or_movements -> Pop art -> visual_art.art_period_movement.associated_artists -> Andy Warhol\n# Answer:\nPop art", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> visual_art.art_period_movement.associated_artworks -> Golden\n# Answer:\nModern art", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nModern art", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.associated_periods_or_movements -> Pop art -> visual_art.art_period_movement.associated_artists -> Robert Rauschenberg\n# Answer:\nPop art"], "ground_truth": ["Bronze"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1988", "prediction": ["# Reasoning Path:\nJoe Biden -> people.person.education -> g.11b8cfq9yc\n# Answer:\ng.11b8cfq9yc", "# Reasoning Path:\nJoe Biden -> people.person.education -> m.03l4lnx -> education.education.major_field_of_study -> Political Science\n# Answer:\nm.03l4lnx", "# Reasoning Path:\nJoe Biden -> people.person.education -> m.03l4lnr -> education.education.institution -> Syracuse University\n# Answer:\nm.03l4lnr", "# Reasoning Path:\nJoe Biden -> common.topic.notable_for -> g.1255qxl7c\n# Answer:\ng.1255qxl7c", "# Reasoning Path:\nJoe Biden -> government.politician.government_positions_held -> m.02_x98m -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nm.02_x98m", "# Reasoning Path:\nJoe Biden -> common.topic.notable_for -> g.1259jwgq_\n# Answer:\ng.1259jwgq_", "# Reasoning Path:\nJoe Biden -> people.person.education -> m.03l4lnr -> education.education.degree -> Juris Doctor\n# Answer:\nm.03l4lnr", "# Reasoning Path:\nJoe Biden -> people.person.education -> m.03l4lnx -> education.education.major_field_of_study -> History\n# Answer:\nm.03l4lnx", "# Reasoning Path:\nJoe Biden -> government.politician.government_positions_held -> m.02_x98m -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nm.02_x98m", "# Reasoning Path:\nJoe Biden -> government.politician.government_positions_held -> m.04hzwqx -> government.government_position_held.governmental_body -> New Castle County Council\n# Answer:\nm.04hzwqx"], "ground_truth": ["History", "Political Science"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-199", "prediction": ["# Reasoning Path:\nIraq -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> United States of America\n# Answer:\nFederal republic", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Federal republic -> common.topic.notable_types -> Form of Government\n# Answer:\nFederal republic", "# Reasoning Path:\nIraq -> location.statistical_region.labor_participation_rate -> g.11b71vj1zg\n# Answer:\ng.11b71vj1zg", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Lebanon\n# Answer:\nParliamentary republic", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary republic -> common.topic.notable_types -> Form of Government\n# Answer:\nParliamentary republic", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Iran\n# Answer:\nParliamentary system", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary system -> common.topic.notable_types -> Form of Government\n# Answer:\nParliamentary system", "# Reasoning Path:\nIraq -> location.statistical_region.labor_participation_rate -> g.12cp_j7p6\n# Answer:\ng.12cp_j7p6", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Turkey\n# Answer:\nParliamentary republic", "# Reasoning Path:\nIraq -> location.statistical_region.labor_participation_rate -> g.1hhc38qnx\n# Answer:\ng.1hhc38qnx"], "ground_truth": ["Federal republic", "Republic", "Parliamentary republic", "Parliamentary system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.7241379310344827, "path_precision": 0.7, "path_recall": 0.75, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1990", "prediction": ["# Reasoning Path:\nThe Lion King -> film.film.starring -> m.0k5qnz -> film.performance.actor -> Nathan Lane\n# Answer:\nm.0k5qnz", "# Reasoning Path:\nThe Lion King -> film.film.starring -> g.11b82bfkm7\n# Answer:\ng.11b82bfkm7", "# Reasoning Path:\nThe Lion King -> film.film.starring -> m.0k5qnz -> film.performance.character -> Timon\n# Answer:\nm.0k5qnz", "# Reasoning Path:\nThe Lion King -> film.film.starring -> m.0k5qnz -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0k5qnz", "# Reasoning Path:\nThe Lion King -> film.film.starring -> m.02vc53y -> film.performance.actor -> Madge Sinclair\n# Answer:\nm.02vc53y", "# Reasoning Path:\nThe Lion King -> base.wikipedia_infobox.video_game.platforms -> Amiga\n# Answer:\nAmiga", "# Reasoning Path:\nThe Lion King -> base.wikipedia_infobox.video_game.platforms -> Game Boy\n# Answer:\nGame Boy", "# Reasoning Path:\nThe Lion King -> film.film.starring -> m.02vc53y -> film.performance.special_performance_type -> Voice\n# Answer:\nm.02vc53y"], "ground_truth": ["Nathan Lane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.125, "path_recall": 0.3333333333333333, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1991", "prediction": ["# Reasoning Path:\nOhio -> location.location.containedby -> Midwestern United States -> location.location.containedby -> United States of America\n# Answer:\nMidwestern United States", "# Reasoning Path:\nOhio -> location.location.containedby -> East North Central States -> location.location.containedby -> Midwestern United States\n# Answer:\nEast North Central States", "# Reasoning Path:\nOhio -> location.location.containedby -> Midwestern United States -> book.book_subject.works -> Caught in the Middle: America's Heartland in the Age of Globalism\n# Answer:\nMidwestern United States", "# Reasoning Path:\nOhio -> location.location.containedby -> Contiguous United States -> location.location.containedby -> Continental United States\n# Answer:\nContiguous United States", "# Reasoning Path:\nOhio -> location.location.containedby -> Contiguous United States -> location.location.containedby -> North America\n# Answer:\nContiguous United States", "# Reasoning Path:\nOhio -> location.location.containedby -> Midwestern United States -> location.location.contains -> East North Central States\n# Answer:\nMidwestern United States", "# Reasoning Path:\nOhio -> location.location.containedby -> Contiguous United States -> location.location.containedby -> United States of America\n# Answer:\nContiguous United States", "# Reasoning Path:\nOhio -> location.location.containedby -> East North Central States -> location.location.containedby -> Contiguous United States\n# Answer:\nEast North Central States", "# Reasoning Path:\nOhio -> location.location.containedby -> Contiguous United States -> base.mapcentral.fgdc_location_keyword.metadata_location_records -> Isobioclimates of the conterminous United States metadata\n# Answer:\nContiguous United States", "# Reasoning Path:\nOhio -> location.location.containedby -> East North Central States -> common.topic.notable_for -> g.1257k6kxr\n# Answer:\nEast North Central States"], "ground_truth": ["East North Central States", "Midwestern United States"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1992", "prediction": ["# Reasoning Path:\nKailua -> location.location.containedby -> Hawaii -> location.location.containedby -> Hawaiian Islands\n# Answer:\nHawaii", "# Reasoning Path:\nKailua -> location.location.containedby -> Oahu -> location.location.containedby -> Hawaii\n# Answer:\nOahu", "# Reasoning Path:\nKailua -> location.hud_foreclosure_area.ofheo_price_change -> m.07gfw9c\n# Answer:\nm.07gfw9c", "# Reasoning Path:\nKailua -> location.location.containedby -> Hawaii -> location.location.containedby -> Pacific Ocean\n# Answer:\nHawaii", "# Reasoning Path:\nKailua -> common.topic.notable_for -> g.125f1gfkx\n# Answer:\ng.125f1gfkx", "# Reasoning Path:\nKailua -> location.location.containedby -> Hawaii -> location.location.containedby -> Pacific States\n# Answer:\nHawaii", "# Reasoning Path:\nKailua -> location.location.containedby -> Oahu -> geography.island.island_group -> Hawaiian Islands\n# Answer:\nOahu", "# Reasoning Path:\nKailua -> location.location.containedby -> Hawaii County -> location.location.containedby -> Hawaii\n# Answer:\nHawaii County", "# Reasoning Path:\nKailua -> location.location.containedby -> Hawaii -> book.book_subject.works -> Anatomy of Paradise\n# Answer:\nHawaii", "# Reasoning Path:\nKailua -> location.location.containedby -> Hawaii -> location.location.events -> 1981 Hawaii International Film Festival\n# Answer:\nHawaii"], "ground_truth": ["Oahu"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1993", "prediction": ["# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Guaran\u00ed language -> common.topic.notable_types -> Human Language\n# Answer:\nGuaran\u00ed language", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Kayak II -> biology.organism.organism_type -> Horse\n# Answer:\nKayak II", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nItalian Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Italian Language -> common.topic.notable_types -> Human Language\n# Answer:\nItalian Language", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Kayak II -> base.thoroughbredracing.thoroughbred_racehorse.sex -> Stallion\n# Answer:\nKayak II", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> language.language_family.geographic_distribution -> Bolivia\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.location.partially_contains -> Alto San Juan -> location.location.partially_contained_by -> m.0wg8lvc\n# Answer:\nAlto San Juan", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nItalian Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> common.topic.notable_types -> Human Language\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> language.language_family.geographic_distribution -> Peru\n# Answer:\nQuechuan languages"], "ground_truth": ["Italian Language", "Yiddish Language", "Quechuan languages", "Guaran\u00ed language", "Spanish Language"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6461538461538462, "ans_precission": 0.7, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.6461538461538462, "path_ans_precision": 0.7, "path_ans_recall": 0.6}
{"id": "WebQTest-1994", "prediction": ["# Reasoning Path:\nWhoopi Goldberg -> award.award_winner.awards_won -> m.0k2vr3p -> award.award_honor.honored_for -> Whoopi Goldberg - Original Broadway Show Recording\n# Answer:\nm.0k2vr3p", "# Reasoning Path:\nWhoopi Goldberg -> award.award_winner.awards_won -> m.0k2vr3p -> award.award_honor.ceremony -> 28th Annual Grammy Awards\n# Answer:\nm.0k2vr3p", "# Reasoning Path:\nWhoopi Goldberg -> award.award_winner.awards_won -> m.0k2vr3p -> award.award_honor.award -> Grammy Award for Best Comedy Album\n# Answer:\nm.0k2vr3p", "# Reasoning Path:\nWhoopi Goldberg -> award.award_winner.awards_won -> m.04ktfys -> award.award_honor.honored_for -> Ghost\n# Answer:\nm.04ktfys", "# Reasoning Path:\nWhoopi Goldberg -> award.award_winner.awards_won -> m.0459r_b -> award.award_honor.award -> Mark Twain Prize for American Humor\n# Answer:\nm.0459r_b", "# Reasoning Path:\nWhoopi Goldberg -> music.artist.track -> Ball of Confusion (That's What the World Is Today) -> music.recording.releases -> Sister Act 2: Back in the Habit\n# Answer:\nBall of Confusion (That's What the World Is Today)", "# Reasoning Path:\nWhoopi Goldberg -> award.award_winner.awards_won -> m.04ktfys -> award.award_honor.ceremony -> 63rd Academy Awards\n# Answer:\nm.04ktfys", "# Reasoning Path:\nWhoopi Goldberg -> common.topic.webpage -> m.040slb7 -> common.webpage.resource -> m.0bjy8j_\n# Answer:\nm.040slb7", "# Reasoning Path:\nWhoopi Goldberg -> music.artist.track -> Ain't No Mountain High Enough -> common.topic.notable_types -> Musical Recording\n# Answer:\nAin't No Mountain High Enough", "# Reasoning Path:\nWhoopi Goldberg -> common.topic.webpage -> m.093_90w -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.093_90w"], "ground_truth": ["Grammy Award for Best Comedy Album"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1996", "prediction": ["# Reasoning Path:\nMaggie Simpson -> common.topic.notable_for -> g.1256glm_8\n# Answer:\ng.1256glm_8", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.parents -> Marge Simpson -> fictional_universe.fictional_character.character_created_by -> Matt Groening\n# Answer:\nMarge Simpson", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.parents -> Marge Simpson -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The Simpsons\n# Answer:\nMarge Simpson", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.parents -> Homer Simpson -> fictional_universe.fictional_character.character_created_by -> Matt Groening\n# Answer:\nHomer Simpson", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.parents -> Marge Simpson -> fictional_universe.fictional_character.married_to -> m.02kx7z6\n# Answer:\nMarge Simpson", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.siblings -> m.02kx80k -> fictional_universe.sibling_relationship_of_fictional_characters.siblings -> Bart Simpson\n# Answer:\nm.02kx80k", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.parents -> Homer Simpson -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The Simpsons\n# Answer:\nHomer Simpson", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.siblings -> m.02kx810 -> fictional_universe.sibling_relationship_of_fictional_characters.siblings -> Lisa Simpson\n# Answer:\nm.02kx810", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.parents -> Homer Simpson -> fictional_universe.fictional_character.medical_conditions -> Rape\n# Answer:\nHomer Simpson", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.parents -> Homer Simpson -> fictional_universe.fictional_character.medical_conditions -> Stroke\n# Answer:\nHomer Simpson"], "ground_truth": ["Nancy Cartwright"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1997", "prediction": ["# Reasoning Path:\nOwen Schmitt -> american_football.football_player.receiving -> m.0791jrl -> american_football.player_receiving_statistics.team -> Seattle Seahawks\n# Answer:\nm.0791jrl", "# Reasoning Path:\nOwen Schmitt -> common.topic.notable_for -> g.1256s0v84\n# Answer:\ng.1256s0v84", "# Reasoning Path:\nOwen Schmitt -> american_football.football_player.receiving -> m.07tfchw -> american_football.player_receiving_statistics.team -> Seattle Seahawks\n# Answer:\nm.07tfchw", "# Reasoning Path:\nOwen Schmitt -> american_football.football_player.receiving -> m.07sh8x9 -> american_football.player_receiving_statistics.team -> Seattle Seahawks\n# Answer:\nm.07sh8x9", "# Reasoning Path:\nOwen Schmitt -> american_football.football_player.receiving -> m.0791jrl -> american_football.player_receiving_statistics.season -> 2008 NFL season\n# Answer:\nm.0791jrl", "# Reasoning Path:\nOwen Schmitt -> common.topic.notable_types -> American football player -> common.topic.notable_types -> Profession\n# Answer:\nAmerican football player", "# Reasoning Path:\nOwen Schmitt -> american_football.football_player.receiving -> m.07sh8x9 -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\nm.07sh8x9", "# Reasoning Path:\nOwen Schmitt -> american_football.football_player.receiving -> m.07tfchw -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\nm.07tfchw", "# Reasoning Path:\nOwen Schmitt -> common.topic.notable_types -> American football player -> people.profession.specialization_of -> Athlete\n# Answer:\nAmerican football player", "# Reasoning Path:\nOwen Schmitt -> common.topic.notable_types -> American football player -> fictional_universe.character_occupation.characters_with_this_occupation -> A. A. Ron Balakay\n# Answer:\nAmerican football player"], "ground_truth": ["Seattle Seahawks"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.12765957446808512, "path_precision": 0.3, "path_recall": 0.08108108108108109, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1998", "prediction": ["# Reasoning Path:\nThomas Jefferson -> organization.organization_founder.organizations_founded -> Democratic-Republican Party -> common.topic.image -> Rembrandt Peale-Thomas Jefferson\n# Answer:\nDemocratic-Republican Party", "# Reasoning Path:\nThomas Jefferson -> government.politician.party -> m.03gjg7h -> government.political_party_tenure.party -> Democratic-Republican Party\n# Answer:\nm.03gjg7h", "# Reasoning Path:\nThomas Jefferson -> organization.organization_founder.organizations_founded -> Democratic-Republican Party -> organization.organization.founders -> James Madison\n# Answer:\nDemocratic-Republican Party", "# Reasoning Path:\nThomas Jefferson -> organization.organization_founder.organizations_founded -> Democratic-Republican Party -> common.topic.image -> TJeffersonrpeale\n# Answer:\nDemocratic-Republican Party", "# Reasoning Path:\nThomas Jefferson -> organization.organization_founder.organizations_founded -> Democratic-Republican Party -> government.political_party.ideology -> Classical liberalism\n# Answer:\nDemocratic-Republican Party", "# Reasoning Path:\nThomas Jefferson -> organization.organization_founder.organizations_founded -> Democratic Party -> book.book_subject.works -> The Two Americas: Our Current Political Deadlock and How to Break It\n# Answer:\nDemocratic Party", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> organization.organization_founder.organizations_founded -> Republican Party -> base.rivalries.rival.rivals -> Red states and blue states\n# Answer:\nRepublican Party", "# Reasoning Path:\nThomas Jefferson -> government.politician.party -> m.03gjg7h -> freebase.valuenotation.is_reviewed -> Party\n# Answer:\nm.03gjg7h", "# Reasoning Path:\nThomas Jefferson -> organization.organization_founder.organizations_founded -> Democratic-Republican Party -> government.political_party.ideology -> Federalism\n# Answer:\nDemocratic-Republican Party"], "ground_truth": ["Democratic-Republican Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1999", "prediction": ["# Reasoning Path:\nFrance -> location.statistical_region.religions -> m.03q9tjt -> location.religion_percentage.religion -> Catholicism\n# Answer:\nm.03q9tjt", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.11b60tv_69\n# Answer:\ng.11b60tv_69", "# Reasoning Path:\nFrance -> location.statistical_region.gni_in_ppp_dollars -> g.11b60wv0lq\n# Answer:\ng.11b60wv0lq", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6gn9g\n# Answer:\ng.12tb6gn9g", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc37cwf\n# Answer:\ng.1hhc37cwf", "# Reasoning Path:\nFrance -> location.statistical_region.gni_in_ppp_dollars -> g.1245_0_b7\n# Answer:\ng.1245_0_b7", "# Reasoning Path:\nFrance -> location.statistical_region.gni_in_ppp_dollars -> g.1245_3w4h\n# Answer:\ng.1245_3w4h"], "ground_truth": ["Catholicism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.22222222222222224, "path_precision": 0.14285714285714285, "path_recall": 0.5, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.profession -> Philosopher\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.corresponding_type -> Author\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Albert Camus -> people.person.profession -> Philosopher\n# Answer:\nAlbert Camus", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.specializations -> Author\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.specializations -> Novelist\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alvin Plantinga -> people.person.profession -> Physician\n# Answer:\nAlvin Plantinga", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Albert Camus -> people.person.profession -> Writer\n# Answer:\nAlbert Camus", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.specializations -> Poet\n# Answer:\nWriter"], "ground_truth": ["Writer", "Philosopher", "Physician"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-200", "prediction": ["# Reasoning Path:\nVietnam War -> book.written_work.subjects -> Vietnam -> book.written_work.subjects -> Asia\n# Answer:\nVietnam", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> Vietnam -> book.written_work.subjects -> Communism\n# Answer:\nVietnam", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> Vietnam -> book.written_work.subjects -> Southeast Asia\n# Answer:\nVietnam", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> Vietnam -> location.location.events -> Cold War\n# Answer:\nVietnam", "# Reasoning Path:\nVietnam War -> time.event.locations -> Laos -> location.location.containedby -> Asia\n# Answer:\nLaos", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> Asia -> base.locations.continents.countries_within -> Vietnam\n# Answer:\nAsia", "# Reasoning Path:\nVietnam War -> time.event.locations -> Cambodia -> location.location.events -> Cambodian Campaign\n# Answer:\nCambodia", "# Reasoning Path:\nVietnam War -> time.event.locations -> Cambodia -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nCambodia", "# Reasoning Path:\nVietnam War -> time.event.locations -> Vietnam -> book.written_work.subjects -> Asia\n# Answer:\nVietnam", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> Vietnam -> location.location.events -> Sino-Vietnamese War\n# Answer:\nVietnam"], "ground_truth": ["South Vietnam", "Southeast Asia", "North Vietnam", "Vietnam", "Laos", "Cambodia"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.6428571428571429, "ans_precission": 0.9, "ans_recall": 0.5, "path_f1": 0.6153846153846154, "path_precision": 0.8, "path_recall": 0.5, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-2000", "prediction": ["# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.0w2d85v -> tv.regular_tv_appearance.actor -> Ron Livingston\n# Answer:\nm.0w2d85v", "# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.025dgg3 -> tv.regular_tv_appearance.actor -> Sarah Jessica Parker\n# Answer:\nm.025dgg3", "# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.0w2d85v -> tv.regular_tv_appearance.character -> Jack Berger\n# Answer:\nm.0w2d85v", "# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.025dghk -> tv.regular_tv_appearance.actor -> Kim Cattrall\n# Answer:\nm.025dghk", "# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.025dgg3 -> tv.regular_tv_appearance.seasons -> Sex and the City - Season 1\n# Answer:\nm.025dgg3", "# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.025dgg3 -> tv.regular_tv_appearance.character -> Carrie Bradshaw\n# Answer:\nm.025dgg3", "# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.025dghk -> tv.regular_tv_appearance.seasons -> Sex and the City - Season 1\n# Answer:\nm.025dghk", "# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.025dgg3 -> tv.regular_tv_appearance.seasons -> Sex and the City - Season 6\n# Answer:\nm.025dgg3", "# Reasoning Path:\nSex and the City -> film.film.produced_by -> Darren Star -> tv.tv_writer.episodes_written -> A Melrose Place Christmas\n# Answer:\nDarren Star", "# Reasoning Path:\nSex and the City -> film.film.produced_by -> Darren Star -> film.producer.film -> Sex and the City 2\n# Answer:\nDarren Star"], "ground_truth": ["Ron Livingston"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-2001", "prediction": ["# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05l1m7b -> sports.sports_award.season -> 1988\u201389 FA Cup\n# Answer:\nm.05l1m7b", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05kkmws -> sports.sports_award.season -> 2007\u201308 FA Cup\n# Answer:\nm.05kkmws", "# Reasoning Path:\nFA Cup -> common.topic.notable_for -> g.125f13scs\n# Answer:\ng.125f13scs", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05l1m7b -> sports.sports_award.award_winner -> Liverpool F.C.\n# Answer:\nm.05l1m7b", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05l1m6n -> sports.sports_award.award_winner -> Liverpool F.C.\n# Answer:\nm.05l1m6n", "# Reasoning Path:\nFA Cup -> freebase.valuenotation.has_no_value -> Date of final occurrence -> rdf-schema#range -> Date/Time\n# Answer:\nDate of final occurrence", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05kkmws -> sports.sports_award.award_winner -> Portsmouth F.C.\n# Answer:\nm.05kkmws", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05l1m6n -> sports.sports_award.season -> 1973\u201374 FA Cup\n# Answer:\nm.05l1m6n", "# Reasoning Path:\nFA Cup -> freebase.valuenotation.has_no_value -> Date of final occurrence -> type.property.schema -> Recurring event\n# Answer:\nDate of final occurrence", "# Reasoning Path:\nFA Cup -> freebase.valuenotation.has_no_value -> Date of final occurrence -> type.property.expected_type -> Date/Time\n# Answer:\nDate of final occurrence"], "ground_truth": ["Chelsea F.C."], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-2002", "prediction": ["# Reasoning Path:\nMichael Jackson -> people.person.children -> Prince Michael Jackson II -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nPrince Michael Jackson II", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Prince Michael Jackson II -> freebase.valuenotation.has_no_value -> Profession\n# Answer:\nPrince Michael Jackson II", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.gender -> Male\n# Answer:\nMichael Joseph Jackson, Jr.", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Prince Michael Jackson II -> common.topic.notable_for -> g.125dq4cts\n# Answer:\nPrince Michael Jackson II", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Prince Michael Jackson II -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nPrince Michael Jackson II", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.nationality -> United States of America\n# Answer:\nMichael Joseph Jackson, Jr.", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Prince Michael Jackson II -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nPrince Michael Jackson II", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Paris-Michael Katherine Jackson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nParis-Michael Katherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Paris-Michael Katherine Jackson -> common.topic.article -> m.0j38d_7\n# Answer:\nParis-Michael Katherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Paris-Michael Katherine Jackson -> common.topic.subject_of -> Diamond Ranch Academy\n# Answer:\nParis-Michael Katherine Jackson"], "ground_truth": ["Prince Michael Jackson II", "Michael Joseph Jackson, Jr.", "Paris-Michael Katherine Jackson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-2003", "prediction": ["# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nm.0z83xlj", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83n09 -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nm.0z83n09", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z87qmv -> award.award_nomination.nominated_for -> Water for Elephants\n# Answer:\nm.0z87qmv", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.award_nominee -> Kristen Stewart\n# Answer:\nm.0z83xlj", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.ceremony -> 2012 Teen Choice Awards\n# Answer:\nm.0z83xlj", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83n09 -> award.award_nomination.award -> Teen Choice Award for Choice Movie Actor: Romance\n# Answer:\nm.0z83n09", "# Reasoning Path:\nRobert Pattinson -> award.award_winner.awards_won -> m.0100q0tx -> award.award_honor.honored_for -> The Twilight Saga: Breaking Dawn - Part 2\n# Answer:\nm.0100q0tx", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z87qmv -> award.award_nomination.ceremony -> 2011 Teen Choice Awards\n# Answer:\nm.0z87qmv", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z87qmv -> freebase.valuenotation.is_reviewed -> Award\n# Answer:\nm.0z87qmv", "# Reasoning Path:\nRobert Pattinson -> people.person.sibling_s -> m.07sh9v2 -> people.sibling_relationship.sibling -> Lizzy Pattinson\n# Answer:\nm.07sh9v2"], "ground_truth": ["Bel Ami", "Hold on to Me", "Little Ashes", "The Summer House", "Mission: Blacklist", "Harry Potter and the Goblet of Fire", "The Twilight Saga: Breaking Dawn - Part 1", "The Twilight Saga: New Moon", "The Twilight Saga: Breaking Dawn - Part 2", "The Bad Mother's Handbook", "The Haunted Airman", "Harry Potter and the Order of the Phoenix", "Eclipse", "Twilight", "Cosmopolis", "Remember Me", "Queen of the Desert", "Dark Kingdom: The Dragon King", "Maps to the Stars", "Vanity Fair", "The Rover", "The Childhood of a Leader", "Love & Distrust", "Unbound Captives", "How to Be", "Life", "Water for Elephants"], "ans_acc": 0.14814814814814814, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.0784313725490196, "path_precision": 0.4, "path_recall": 0.043478260869565216, "path_ans_f1": 0.21621621621621623, "path_ans_precision": 0.4, "path_ans_recall": 0.14814814814814814}
{"id": "WebQTest-2004", "prediction": ["# Reasoning Path:\nJapan -> location.location.time_zones -> Japan Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nJapan Time Zone", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\ng.1245_2h9t", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.11b71r82sc\n# Answer:\ng.11b71r82sc", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_4lv6\n# Answer:\ng.1245_4lv6", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_6fj1\n# Answer:\ng.1245_6fj1", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.12cp_jcjg\n# Answer:\ng.12cp_jcjg", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.12tb6g_8v\n# Answer:\ng.12tb6g_8v"], "ground_truth": ["Japan Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-2005", "prediction": ["# Reasoning Path:\nBob Kelso -> tv.tv_character.appeared_in_tv_program -> m.02567sb -> tv.regular_tv_appearance.actor -> Ken Jenkins\n# Answer:\nm.02567sb", "# Reasoning Path:\nBob Kelso -> tv.tv_character.appeared_in_tv_program -> m.02567sb -> tv.regular_tv_appearance.series -> Scrubs\n# Answer:\nm.02567sb", "# Reasoning Path:\nBob Kelso -> tv.tv_character.appeared_in_tv_program -> m.02567sb -> tv.regular_tv_appearance.seasons -> Scrubs - Season 1\n# Answer:\nm.02567sb", "# Reasoning Path:\nBob Kelso -> tv.tv_character.appeared_in_tv_program -> m.02567sb -> tv.regular_tv_appearance.seasons -> Scrubs - Season 2\n# Answer:\nm.02567sb", "# Reasoning Path:\nBob Kelso -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Scrubs -> award.award_nominated_work.award_nominations -> m.0lv4677\n# Answer:\nScrubs", "# Reasoning Path:\nBob Kelso -> tv.tv_character.appeared_in_tv_program -> m.02567sb -> tv.regular_tv_appearance.seasons -> Scrubs - Season 3\n# Answer:\nm.02567sb", "# Reasoning Path:\nBob Kelso -> common.topic.notable_types -> TV Character -> type.type.properties -> Appeared In TV Episode Segments\n# Answer:\nTV Character", "# Reasoning Path:\nBob Kelso -> common.topic.notable_types -> TV Character -> type.type.expected_by -> Character\n# Answer:\nTV Character", "# Reasoning Path:\nBob Kelso -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Scrubs -> award.award_nominated_work.award_nominations -> m.0lv4fdx\n# Answer:\nScrubs", "# Reasoning Path:\nBob Kelso -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Scrubs -> fictional_universe.fictional_universe.characters -> Carla Espinosa\n# Answer:\nScrubs"], "ground_truth": ["Ken Jenkins"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-2006", "prediction": ["# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Russian -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> L\n# Answer:\nRussian", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Russian -> people.ethnicity.included_in_group -> Europeans\n# Answer:\nRussian", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Russian -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Crimson Dynamo\n# Answer:\nRussian", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Russian -> people.ethnicity.included_in_group -> East Slavs\n# Answer:\nRussian", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Scandinavians -> people.ethnicity.people -> Andreas Sj\u00f6din\n# Answer:\nScandinavians", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Scandinavians -> people.ethnicity.languages_spoken -> North Germanic languages\n# Answer:\nScandinavians", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Russian -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Billy Boss\n# Answer:\nRussian", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Russian -> common.topic.image -> Danceinalanya\n# Answer:\nRussian", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Scandinavians -> common.topic.article -> m.05t0ydl\n# Answer:\nScandinavians", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Scandinavians -> people.ethnicity.people -> Christiane Seidel\n# Answer:\nScandinavians"], "ground_truth": ["Russian", "Scandinavians"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-2007", "prediction": ["# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> language.human_language.countries_spoken_in -> Czechoslovakia\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> common.topic.notable_types -> Human Language\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> media_common.netflix_genre.titles -> Adelheid\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Hungarian language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nHungarian language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Hungarian language -> common.topic.notable_types -> Human Language\n# Answer:\nHungarian language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Greek Language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nGreek Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> language.human_language.countries_spoken_in -> Croatia\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Hungarian language -> language.human_language.region -> Europe\n# Answer:\nHungarian language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> media_common.netflix_genre.titles -> All My Loved Ones\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Hungarian language -> language.human_language.countries_spoken_in -> Croatia\n# Answer:\nHungarian language"], "ground_truth": ["Hungarian language", "Slovak Language", "German Language", "Czech Language", "Rusyn Language", "Croatian language", "Ukrainian Language", "Bulgarian Language", "Russian Language", "Serbian language", "Romani language", "Polish Language", "Greek Language"], "ans_acc": 0.23076923076923078, "ans_hit": 1, "ans_f1": 0.375, "ans_precission": 1.0, "ans_recall": 0.23076923076923078, "path_f1": 0.375, "path_precision": 1.0, "path_recall": 0.23076923076923078, "path_ans_f1": 0.375, "path_ans_precision": 1.0, "path_ans_recall": 0.23076923076923078}
{"id": "WebQTest-2008", "prediction": ["# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.location.containedby -> Near East\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.location.geolocation -> m.0zwv97z\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.location.containedby -> Central Asia\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> South Africa -> location.country.languages_spoken -> English Language\n# Answer:\nSouth Africa", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> South Africa -> common.topic.notable_types -> Country\n# Answer:\nSouth Africa", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.location.containedby -> Middle East\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.country.capital -> Hamadan\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Libya -> location.location.events -> Middle East Theatre of World War II\n# Answer:\nLibya", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Libya -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nLibya", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> South Africa -> location.country.languages_spoken -> Greek Language\n# Answer:\nSouth Africa"], "ground_truth": ["Tunisia", "Saudi Arabia", "Qatar", "Syria", "Algeria", "Turkey", "Canada", "Iran", "South Yemen", "Jordan", "Iraq", "Morocco", "Tanzania", "Yemen", "United Arab Emirates", "Oman", "Libya", "Djibouti", "Kuwait", "Sudan", "Lebanon", "Mandatory Palestine", "Egypt", "Israel", "Mauritania", "Seljuk Empire", "South Africa", "Bahrain"], "ans_acc": 0.10714285714285714, "ans_hit": 1, "ans_f1": 0.19354838709677416, "ans_precission": 1.0, "ans_recall": 0.10714285714285714, "path_f1": 0.19354838709677416, "path_precision": 1.0, "path_recall": 0.10714285714285714, "path_ans_f1": 0.19354838709677416, "path_ans_precision": 1.0, "path_ans_recall": 0.10714285714285714}
{"id": "WebQTest-2009", "prediction": ["# Reasoning Path:\nTim Cook -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Alabama\n# Answer:\nMobile", "# Reasoning Path:\nTim Cook -> people.person.place_of_birth -> Mobile -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07ghzrz\n# Answer:\nMobile", "# Reasoning Path:\nTim Cook -> people.person.employment_history -> m.04gcn1r -> business.employment_tenure.company -> Intelligent Electronics\n# Answer:\nm.04gcn1r", "# Reasoning Path:\nTim Cook -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Area code 251\n# Answer:\nMobile", "# Reasoning Path:\nTim Cook -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b66g7msm\n# Answer:\nMobile", "# Reasoning Path:\nTim Cook -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Mobile County\n# Answer:\nMobile", "# Reasoning Path:\nTim Cook -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b7tn_fd2\n# Answer:\nMobile", "# Reasoning Path:\nTim Cook -> people.person.employment_history -> m.04gcn2l -> business.employment_tenure.company -> Compaq\n# Answer:\nm.04gcn2l", "# Reasoning Path:\nTim Cook -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11btt6sf_l\n# Answer:\nMobile", "# Reasoning Path:\nTim Cook -> people.person.profession -> Businessperson -> base.descriptive_names.names.descriptive_name -> m.0100z128\n# Answer:\nBusinessperson"], "ground_truth": ["Apple Inc."], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-201", "prediction": ["# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> common.topic.notable_types -> Film character\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> base.ontologies.ontology_instance.equivalent_instances -> m.07nfg4c\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> common.topic.notable_types -> US Vice President\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> people.person.sibling_s -> m.05hlvsz\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> people.person.sibling_s -> m.05px29t\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> people.person.sibling_s -> m.05px29z\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> people.person.quotations -> A strong defense is the surest way to peace. Strength makes detente attainable. Weakness invites war, as my generationmy generationknows from four very bitter experiences. Just as Americas will for peace is second to none, so will Americas strength be second to none. We cannot rely on the forbearance of others to protect this Nation. The power and diversity of the Armed Forces, active Guard and Reserve, the resolve of our fellow citizens, the flexibility in our command to navigate international waters that remain troubled are all essential to our security. -> media_common.quotation.subjects -> Uncategorised\n# Answer:\nA strong defense is the surest way to peace. Strength makes detente attainable. Weakness invites war, as my generationmy generationknows from four very bitter experiences. Just as Americas will for peace is second to none, so will Americas strength be second to none. We cannot rely on the forbearance of others to protect this Nation. The power and diversity of the Armed Forces, active Guard and Reserve, the resolve of our fellow citizens, the flexibility in our command to navigate international waters that remain troubled are all essential to our security.", "# Reasoning Path:\nGerald Ford -> government.political_appointer.appointees -> m.01xrx8h -> government.government_position_held.office_holder -> George H. W. Bush\n# Answer:\nm.01xrx8h", "# Reasoning Path:\nGerald Ford -> government.political_appointer.appointees -> m.01z0qwq -> government.government_position_held.office_holder -> Dick Cheney\n# Answer:\nm.01z0qwq", "# Reasoning Path:\nGerald Ford -> people.person.quotations -> All of us who served in one war or another know very well that all wars are the glory and the agony of the young. -> common.topic.notable_types -> Quotation\n# Answer:\nAll of us who served in one war or another know very well that all wars are the glory and the agony of the young."], "ground_truth": ["Nelson Rockefeller"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-2010", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.major_field_of_study -> Law\n# Answer:\nm.02kvkg9", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.degree -> Juris Doctor\n# Answer:\nm.02kvkg9", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkf4 -> education.education.institution -> Stanford University\n# Answer:\nm.02kvkf4", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkf4 -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nm.02kvkf4", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.0123vxrw -> education.educational_institution.students_graduates -> m.0123vxqw\n# Answer:\nm.0123vxrw", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0_gvz4k -> film.personal_film_appearance.film -> Mitt\n# Answer:\nm.0_gvz4k", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0_gvz4k -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.0_gvz4k", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0nhmmbv -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.0nhmmbv", "# Reasoning Path:\nMitt Romney -> tv.tv_actor.guest_roles -> m.0bvvym_ -> tv.tv_guest_role.episodes_appeared_in -> The Comeback of President Bush\n# Answer:\nm.0bvvym_", "# Reasoning Path:\nMitt Romney -> tv.tv_actor.guest_roles -> m.0bvw9my -> tv.tv_guest_role.episodes_appeared_in -> Mitt Romney, Ryan Sheckler, Jonas Brothers\n# Answer:\nm.0bvw9my"], "ground_truth": ["Juris Doctor", "MBA", "Bachelor of Arts"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.15384615384615383, "path_ans_precision": 0.1, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-2011", "prediction": ["# Reasoning Path:\nBob Marley -> people.deceased_person.place_of_burial -> Bob Marley Mausoleum -> travel.tourist_attraction.near_travel_destination -> Jamaica\n# Answer:\nBob Marley Mausoleum", "# Reasoning Path:\nBob Marley -> people.person.gender -> Male -> medicine.risk_factor.diseases -> heart attack\n# Answer:\nMale", "# Reasoning Path:\nBob Marley -> people.deceased_person.place_of_burial -> Bob Marley Mausoleum -> common.topic.notable_types -> Tourist attraction\n# Answer:\nBob Marley Mausoleum", "# Reasoning Path:\nBob Marley -> film.person_or_entity_appearing_in_film.films -> m.0crrdhl -> film.personal_film_appearance.film -> Life and Debt\n# Answer:\nm.0crrdhl", "# Reasoning Path:\nBob Marley -> film.person_or_entity_appearing_in_film.films -> m.0csbsw9 -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.0csbsw9", "# Reasoning Path:\nBob Marley -> film.person_or_entity_appearing_in_film.films -> m.0csbsw9 -> film.personal_film_appearance.film -> Land of Look Behind\n# Answer:\nm.0csbsw9", "# Reasoning Path:\nBob Marley -> film.person_or_entity_appearing_in_film.films -> m.0csfxxz -> film.personal_film_appearance.film -> Bob Marley: Spiritual Journey\n# Answer:\nm.0csfxxz"], "ground_truth": ["Bob Marley Mausoleum"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.2857142857142857, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.2857142857142857, "path_recall": 1.0, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 1.0}
{"id": "WebQTest-2012", "prediction": ["# Reasoning Path:\nLondon -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nGreenwich Mean Time Zone", "# Reasoning Path:\nLondon -> periodicals.newspaper_circulation_area.newspapers -> i -> book.newspaper.circulation_areas -> United Kingdom\n# Answer:\ni", "# Reasoning Path:\nLondon -> periodicals.newspaper_circulation_area.newspapers -> i -> book.periodical.frequency_or_issues_per_year -> m.0jw2n9_\n# Answer:\ni", "# Reasoning Path:\nLondon -> location.location.partially_contains -> River Lea -> location.location.containedby -> England\n# Answer:\nRiver Lea", "# Reasoning Path:\nLondon -> location.location.partially_contains -> River Lea -> geography.river.mouth -> River Thames\n# Answer:\nRiver Lea", "# Reasoning Path:\nLondon -> periodicals.newspaper_circulation_area.newspapers -> i -> common.topic.article -> m.0dk0p1s\n# Answer:\ni", "# Reasoning Path:\nLondon -> location.location.partially_contains -> River Lea -> geography.river.cities -> Luton\n# Answer:\nRiver Lea", "# Reasoning Path:\nLondon -> location.location.partially_contains -> River Lea -> location.location.containedby -> Europe\n# Answer:\nRiver Lea", "# Reasoning Path:\nLondon -> location.location.partially_contains -> River Thames -> location.location.containedby -> England\n# Answer:\nRiver Thames", "# Reasoning Path:\nLondon -> location.location.partially_contains -> River Thames -> location.location.partially_containedby -> Kent\n# Answer:\nRiver Thames"], "ground_truth": ["Greenwich Mean Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-2013", "prediction": ["# Reasoning Path:\nChicago -> travel.travel_destination.accommodation -> Hotel Sax Chicago -> common.topic.notable_for -> g.1256hnvy5\n# Answer:\nHotel Sax Chicago", "# Reasoning Path:\nChicago -> travel.travel_destination.accommodation -> Hotel Sax Chicago -> travel.accommodation.accommodation_type -> Boutique hotel\n# Answer:\nHotel Sax Chicago", "# Reasoning Path:\nChicago -> travel.travel_destination.accommodation -> Hotel Sax Chicago -> common.topic.notable_types -> Accommodation\n# Answer:\nHotel Sax Chicago", "# Reasoning Path:\nChicago -> location.statistical_region.population -> g.11b66kxd4l\n# Answer:\ng.11b66kxd4l", "# Reasoning Path:\nChicago -> location.statistical_region.population -> g.11b7vbyfz4\n# Answer:\ng.11b7vbyfz4", "# Reasoning Path:\nChicago -> location.statistical_region.population -> g.11btt77639\n# Answer:\ng.11btt77639", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> American Medical News -> common.topic.article -> m.03mf90t\n# Answer:\nAmerican Medical News", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> Arbeiter-Zeitung -> common.topic.notable_types -> Newspaper\n# Answer:\nArbeiter-Zeitung", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> American Medical News -> common.topic.notable_for -> g.1254zz3kn\n# Answer:\nAmerican Medical News", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> Chicago Daily News -> book.periodical.frequency_or_issues_per_year -> m.09r_w3m\n# Answer:\nChicago Daily News"], "ground_truth": ["Hotel Sax Chicago"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-2014", "prediction": ["# Reasoning Path:\nSheryl Crow -> medicine.notable_person_with_medical_condition.condition -> Meningioma -> base.schemastaging.context_name.pronunciation -> m.011032cv\n# Answer:\nMeningioma", "# Reasoning Path:\nSheryl Crow -> medicine.notable_person_with_medical_condition.condition -> Meningioma -> people.cause_of_death.people -> Francis Wynne Masters\n# Answer:\nMeningioma", "# Reasoning Path:\nSheryl Crow -> award.award_nominee.award_nominations -> m.0105szp3 -> award.award_nomination.award -> MTV Europe Music Award for Best Female\n# Answer:\nm.0105szp3", "# Reasoning Path:\nSheryl Crow -> music.artist.track_contributions -> m.010_wmqq -> music.track_contribution.track -> Love Is All There Is\n# Answer:\nm.010_wmqq", "# Reasoning Path:\nSheryl Crow -> award.award_nominee.award_nominations -> m.0105szp3 -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nm.0105szp3", "# Reasoning Path:\nSheryl Crow -> award.award_nominee.award_nominations -> m.05cywhs -> award.award_nomination.nominated_for -> Detours\n# Answer:\nm.05cywhs", "# Reasoning Path:\nSheryl Crow -> music.artist.track_contributions -> m.010_wmqq -> music.track_contribution.role -> Vocals\n# Answer:\nm.010_wmqq", "# Reasoning Path:\nSheryl Crow -> award.award_nominee.award_nominations -> m.0105zmr0 -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nm.0105zmr0", "# Reasoning Path:\nSheryl Crow -> music.artist.track_contributions -> m.010k1fw6 -> music.track_contribution.track -> Make It Go Away (Radiation Song)\n# Answer:\nm.010k1fw6", "# Reasoning Path:\nSheryl Crow -> award.award_nominee.award_nominations -> m.0105zmr0 -> award.award_nomination.ceremony -> 1995 MTV Europe Music Awards\n# Answer:\nm.0105zmr0"], "ground_truth": ["Meningioma"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-2015", "prediction": ["# Reasoning Path:\nArgentina -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Brazil\n# Answer:\nConstitutional republic", "# Reasoning Path:\nArgentina -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Chile\n# Answer:\nConstitutional republic", "# Reasoning Path:\nArgentina -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Brazil\n# Answer:\nFederal republic", "# Reasoning Path:\nArgentina -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Mexico\n# Answer:\nConstitutional republic", "# Reasoning Path:\nArgentina -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Mexico\n# Answer:\nFederal republic", "# Reasoning Path:\nArgentina -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> United States of America\n# Answer:\nFederal republic", "# Reasoning Path:\nArgentina -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Bolivia\n# Answer:\nPresidential system", "# Reasoning Path:\nArgentina -> location.location.partially_contains -> Alto San Juan -> location.location.partially_contained_by -> m.0wg8lvc\n# Answer:\nAlto San Juan", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Asiatic Boy -> base.thoroughbredracing.thoroughbred_racehorse.color -> Chestnut\n# Answer:\nAsiatic Boy", "# Reasoning Path:\nArgentina -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Brazil\n# Answer:\nPresidential system"], "ground_truth": ["Representative democracy", "Presidential system", "Federal republic", "Constitutional republic"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.8, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.8, "path_ans_recall": 0.75}
{"id": "WebQTest-2016", "prediction": ["# Reasoning Path:\nFabio Capello -> people.person.place_of_birth -> San Canzian d'Isonzo -> common.topic.article -> m.0fgn_w\n# Answer:\nSan Canzian d'Isonzo", "# Reasoning Path:\nFabio Capello -> common.topic.article -> m.030_x3\n# Answer:\nm.030_x3", "# Reasoning Path:\nFabio Capello -> people.person.place_of_birth -> San Canzian d'Isonzo -> common.topic.notable_types -> City/Town/Village\n# Answer:\nSan Canzian d'Isonzo", "# Reasoning Path:\nFabio Capello -> people.person.place_of_birth -> San Canzian d'Isonzo -> location.location.containedby -> Province of Gorizia\n# Answer:\nSan Canzian d'Isonzo", "# Reasoning Path:\nFabio Capello -> people.person.place_of_birth -> San Canzian d'Isonzo -> location.location.containedby -> Friuli-Venezia Giulia\n# Answer:\nSan Canzian d'Isonzo", "# Reasoning Path:\nFabio Capello -> people.person.place_of_birth -> San Canzian d'Isonzo -> location.location.containedby -> Italy\n# Answer:\nSan Canzian d'Isonzo", "# Reasoning Path:\nFabio Capello -> soccer.football_player.statistics -> m.0w957j9 -> soccer.football_player_stats.team -> Juventus F.C.\n# Answer:\nm.0w957j9", "# Reasoning Path:\nFabio Capello -> soccer.football_player.statistics -> m.0w957jt -> soccer.football_player_stats.team -> A.S. Roma\n# Answer:\nm.0w957jt", "# Reasoning Path:\nFabio Capello -> soccer.football_player.statistics -> m.0w957k7 -> soccer.football_player_stats.team -> Italy national football team\n# Answer:\nm.0w957k7"], "ground_truth": ["San Canzian d'Isonzo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-2017", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> Republican Party -> government.political_party.ideology -> Conservatism in the United States\n# Answer:\nRepublican Party", "# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> Republican Party -> organization.organization.founders -> Thomas Jefferson\n# Answer:\nRepublican Party", "# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> Illinois Republican Party -> common.topic.article -> m.0g7ly9\n# Answer:\nIllinois Republican Party", "# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> Republican Party -> organization.organization.geographic_scope -> United States of America\n# Answer:\nRepublican Party", "# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> Republican Party -> government.political_party.ideology -> Economic liberalism\n# Answer:\nRepublican Party", "# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> Freedmen's Bureau -> common.topic.notable_for -> g.1255vdyyp\n# Answer:\nFreedmen's Bureau", "# Reasoning Path:\nAbraham Lincoln -> government.politician.party -> m.03ld2ph -> government.political_party_tenure.party -> Whig Party\n# Answer:\nm.03ld2ph", "# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> Illinois Republican Party -> government.political_party.politicians_in_this_party -> m.0130lyr4\n# Answer:\nIllinois Republican Party", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pgr_5 -> people.place_lived.location -> Kentucky\n# Answer:\nm.03pgr_5", "# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> Freedmen's Bureau -> common.topic.notable_types -> Government Agency\n# Answer:\nFreedmen's Bureau"], "ground_truth": ["National Union Party", "Illinois Republican Party", "Whig Party", "Republican Party"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.6, "ans_recall": 0.5, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.7241379310344827, "path_ans_precision": 0.7, "path_ans_recall": 0.75}
{"id": "WebQTest-2018", "prediction": ["# Reasoning Path:\nJack London -> people.person.profession -> Journalist -> common.topic.notable_types -> Profession\n# Answer:\nJournalist", "# Reasoning Path:\nJack London -> people.person.profession -> Screenwriter -> freebase.equivalent_topic.equivalent_type -> TV Writer\n# Answer:\nScreenwriter", "# Reasoning Path:\nJack London -> common.topic.notable_types -> Author -> common.topic.notable_types -> Profession\n# Answer:\nAuthor", "# Reasoning Path:\nJack London -> people.person.profession -> Journalist -> people.profession.corresponding_type -> Author\n# Answer:\nJournalist", "# Reasoning Path:\nJack London -> people.person.profession -> Screenwriter -> common.topic.notable_types -> Profession\n# Answer:\nScreenwriter", "# Reasoning Path:\nJack London -> common.topic.notable_types -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor", "# Reasoning Path:\nJack London -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Edmund Grey\n# Answer:\nJournalist", "# Reasoning Path:\nJack London -> people.person.profession -> Screenwriter -> people.profession.specialization_of -> Writer\n# Answer:\nScreenwriter", "# Reasoning Path:\nJack London -> people.person.profession -> Author -> common.topic.notable_types -> Profession\n# Answer:\nAuthor", "# Reasoning Path:\nJack London -> people.person.profession -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor"], "ground_truth": ["Journalist", "Screenwriter", "Author", "Novelist", "Sailor", "Essayist"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6153846153846154, "path_precision": 0.8, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-2019", "prediction": ["# Reasoning Path:\nMorgan Freeman -> people.person.spouse_s -> m.0hj6dmb -> people.marriage.spouse -> Myrna Colley-Lee\n# Answer:\nm.0hj6dmb", "# Reasoning Path:\nMorgan Freeman -> people.person.spouse_s -> m.0hj6dq1 -> people.marriage.spouse -> Jeanette Adair Bradshaw\n# Answer:\nm.0hj6dq1", "# Reasoning Path:\nMorgan Freeman -> people.person.spouse_s -> m.0hj6dmb -> freebase.valuenotation.is_reviewed -> Spouse\n# Answer:\nm.0hj6dmb", "# Reasoning Path:\nMorgan Freeman -> people.person.spouse_s -> m.0hj6dmb -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0hj6dmb", "# Reasoning Path:\nMorgan Freeman -> people.person.spouse_s -> m.0hj6dq1 -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0hj6dq1", "# Reasoning Path:\nMorgan Freeman -> people.person.spouse_s -> m.0hj6dq1 -> freebase.valuenotation.is_reviewed -> Spouse\n# Answer:\nm.0hj6dq1", "# Reasoning Path:\nMorgan Freeman -> people.person.spouse_s -> m.0hj6dmb -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nm.0hj6dmb", "# Reasoning Path:\nMorgan Freeman -> common.topic.webpage -> m.093_l_6 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.093_l_6", "# Reasoning Path:\nMorgan Freeman -> people.person.spouse_s -> m.0hj6dmb -> freebase.valuenotation.is_reviewed -> To\n# Answer:\nm.0hj6dmb", "# Reasoning Path:\nMorgan Freeman -> common.topic.webpage -> m.093zz6z -> common.webpage.resource -> Robert Iler pleads guilty to mugging charge\n# Answer:\nm.093zz6z"], "ground_truth": ["Myrna Colley-Lee"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-202", "prediction": ["# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Finland\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> common.topic.notable_types -> Currency\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Greece\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\ng.11b71mdn0q", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\ng.12tb6gbb1", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37cxr\n# Answer:\ng.1hhc37cxr", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Nearco -> biology.organism.sex -> Male\n# Answer:\nNearco", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Ribot -> base.thoroughbredracing.thoroughbred_racehorse.sire -> Tenerani\n# Answer:\nRibot", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Ribot -> biology.organism.sex -> Male\n# Answer:\nRibot"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-2020", "prediction": ["# Reasoning Path:\nApennine Mountains -> common.topic.notable_types -> Mountain range -> freebase.type_hints.included_types -> Topic\n# Answer:\nMountain range", "# Reasoning Path:\nApennine Mountains -> location.location.containedby -> Italy -> location.location.containedby -> Europe\n# Answer:\nItaly", "# Reasoning Path:\nApennine Mountains -> common.topic.notable_types -> Mountain range -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nMountain range", "# Reasoning Path:\nApennine Mountains -> common.topic.notable_types -> Mountain range -> type.type.properties -> Mountains\n# Answer:\nMountain range", "# Reasoning Path:\nApennine Mountains -> location.location.containedby -> Italy -> location.country.languages_spoken -> Italian Language\n# Answer:\nItaly", "# Reasoning Path:\nApennine Mountains -> common.topic.notable_types -> Mountain range -> freebase.type_hints.included_types -> Geographical Feature\n# Answer:\nMountain range", "# Reasoning Path:\nApennine Mountains -> common.topic.notable_types -> Mountain range -> freebase.type_hints.included_types -> Location\n# Answer:\nMountain range", "# Reasoning Path:\nApennine Mountains -> location.location.containedby -> Italy -> location.country.first_level_divisions -> Lazio\n# Answer:\nItaly", "# Reasoning Path:\nApennine Mountains -> common.topic.notable_types -> Mountain range -> freebase.type_profile.strict_included_types -> Geographical Feature\n# Answer:\nMountain range", "# Reasoning Path:\nApennine Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope"], "ground_truth": ["Apennine Mountains"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-2021", "prediction": ["# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced -> Christopher Moore -> influence.influence_node.influenced_by -> Kurt Vonnegut\n# Answer:\nChristopher Moore", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced -> Colum McCann -> influence.influence_node.influenced_by -> William Faulkner\n# Answer:\nColum McCann", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced -> Christopher Moore -> influence.influence_node.influenced_by -> Paul Auster\n# Answer:\nChristopher Moore", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced -> Cork Graham -> influence.influence_node.influenced_by -> Ernest Hemingway\n# Answer:\nCork Graham", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced -> Christopher Moore -> book.author.book_editions_published -> A Dirty Job\n# Answer:\nChristopher Moore", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced -> Colum McCann -> influence.influence_node.influenced_by -> Don DeLillo\n# Answer:\nColum McCann", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced -> Christopher Moore -> book.author.works_written -> A Dirty Job\n# Answer:\nChristopher Moore", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced -> Colum McCann -> influence.influence_node.influenced_by -> Edna O'Brien\n# Answer:\nColum McCann", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced -> Colum McCann -> freebase.valuenotation.has_value -> Parents\n# Answer:\nColum McCann", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced -> Cork Graham -> influence.influence_node.influenced_by -> Frederick Forsyth\n# Answer:\nCork Graham"], "ground_truth": ["Thomas Malory", "William Faulkner", "Sherwood Anderson", "Robert Burns"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.14285714285714288, "path_ans_precision": 0.1, "path_ans_recall": 0.25}
{"id": "WebQTest-2022", "prediction": ["# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> African American -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nAfrican American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> African American -> people.ethnicity.languages_spoken -> American English\n# Answer:\nAfrican American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> African American -> common.topic.subject_of -> The Approaching 100th Anniversary of the Harlem Renaissance (part 2)\n# Answer:\nAfrican American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> African American -> people.ethnicity.languages_spoken -> Spanish Language\n# Answer:\nAfrican American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> African American -> common.topic.notable_types -> Ethnicity\n# Answer:\nAfrican American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> Samoan American -> people.ethnicity.people -> Afa Anoa\u02bbi\n# Answer:\nSamoan American", "# Reasoning Path:\nDwayne Johnson -> tv.tv_actor.guest_roles -> m.010b1f6p -> tv.tv_guest_role.episodes_appeared_in -> King of The Ring 1999 recording\n# Answer:\nm.010b1f6p", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> Samoan American -> common.topic.notable_for -> g.12589h52q\n# Answer:\nSamoan American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> Samoan American -> people.ethnicity.people -> Afa Anoa\u02bbi Jr.\n# Answer:\nSamoan American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> Samoan American -> people.ethnicity.languages_spoken -> American English\n# Answer:\nSamoan American"], "ground_truth": ["Samoan American", "African American"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-2023", "prediction": ["# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.headquarters -> m.04300hm -> location.mailing_address.citytown -> Brussels\n# Answer:\nm.04300hm", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.headquarters -> m.04300hm -> common.topic.notable_types -> Location\n# Answer:\nm.04300hm", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.headquarters -> m.04300hm -> common.topic.notable_for -> g.12571zl_t\n# Answer:\nm.04300hm", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> military.military_combatant.military_conflicts -> m.07vjs3f -> military.military_combatant_group.conflict -> Cold War\n# Answer:\nm.07vjs3f", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.founders -> Belgium -> location.country.capital -> Brussels\n# Answer:\nBelgium", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.founders -> Netherlands -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nNetherlands", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> military.military_combatant.military_conflicts -> m.07y5dhg -> military.military_combatant_group.conflict -> Siege of Sarajevo\n# Answer:\nm.07y5dhg", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> military.military_combatant.military_conflicts -> m.07y5bnr -> military.military_combatant_group.conflict -> Operation Enduring Freedom\n# Answer:\nm.07y5bnr", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.founders -> Canada -> base.locations.countries.continent -> North America\n# Answer:\nCanada", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.founders -> Netherlands -> organization.organization_founder.organizations_founded -> Council of Europe\n# Answer:\nNetherlands"], "ground_truth": ["Brussels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-2024", "prediction": ["# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Sharpsburg -> location.hud_foreclosure_area.ofheo_price_change -> m.07gzwmg\n# Answer:\nSharpsburg", "# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Sharpsburg -> location.hud_foreclosure_area.total_residential_addresses -> m.07gzwm1\n# Answer:\nSharpsburg", "# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Sharpsburg -> location.statistical_region.population -> g.11b66jxpbg\n# Answer:\nSharpsburg", "# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Maryland -> location.location.adjoin_s -> m.02thq3m\n# Answer:\nMaryland", "# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Maryland -> book.book_subject.works -> Antietam\n# Answer:\nMaryland", "# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Sharpsburg -> location.statistical_region.population -> g.11bc88m3lx\n# Answer:\nSharpsburg", "# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Maryland -> location.location.adjoin_s -> m.02thrj4\n# Answer:\nMaryland", "# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Sharpsburg -> location.statistical_region.population -> g.11x1cc9hr\n# Answer:\nSharpsburg", "# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Maryland -> location.location.adjoin_s -> m.02tjfv1\n# Answer:\nMaryland", "# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Maryland -> military.military_unit_place_of_origin.military_units -> 10th Regiment Maryland Volunteer Infantry\n# Answer:\nMaryland"], "ground_truth": ["Maryland"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-2025", "prediction": ["# Reasoning Path:\nZambezi -> geography.river.mouth -> Indian Ocean -> location.location.containedby -> World Ocean\n# Answer:\nIndian Ocean", "# Reasoning Path:\nZambezi -> geography.river.mouth -> Indian Ocean -> book.book_subject.works -> A fish caught in time: the search for the coelacanth\n# Answer:\nIndian Ocean", "# Reasoning Path:\nZambezi -> geography.river.mouth -> Indian Ocean -> location.location.events -> Action of 11 January 1944\n# Answer:\nIndian Ocean", "# Reasoning Path:\nZambezi -> location.location.containedby -> Africa -> base.dinosaur.dinosaur_location.dinosaur_s -> Abrictosaurus\n# Answer:\nAfrica", "# Reasoning Path:\nZambezi -> geography.river.mouth -> Indian Ocean -> book.book_subject.works -> Anthology of African and Indian Ocean Photography\n# Answer:\nIndian Ocean", "# Reasoning Path:\nZambezi -> location.location.containedby -> Africa -> common.topic.notable_types -> Continent\n# Answer:\nAfrica", "# Reasoning Path:\nZambezi -> geography.river.mouth -> Indian Ocean -> book.book_subject.works -> Boarding party\n# Answer:\nIndian Ocean", "# Reasoning Path:\nZambezi -> geography.river.mouth -> Indian Ocean -> location.location.events -> Action of 13 November 1943\n# Answer:\nIndian Ocean", "# Reasoning Path:\nZambezi -> location.location.containedby -> Africa -> base.dinosaur.dinosaur_location.dinosaur_s -> Aegyptosaurus\n# Answer:\nAfrica", "# Reasoning Path:\nZambezi -> location.location.containedby -> Africa -> base.locations.continents.countries_within -> Algeria\n# Answer:\nAfrica"], "ground_truth": ["Indian Ocean"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-2027", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nm.0w9plqp", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nm.0j2zzj_", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m -> soccer.football_player_stats.team -> LA Galaxy\n# Answer:\nm.0w8w79m", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0k4ytw5", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0j2zzj_", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.02nr829 -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nm.02nr829", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nm.0k4ytw5", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.team -> Paris Saint-Germain F.C.\n# Answer:\nm.0qzkj58", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9021c -> soccer.football_player_stats.team -> A.C. Milan\n# Answer:\nm.0w9021c", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nm.0qzkj58"], "ground_truth": ["Manchester United F.C."], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-2028", "prediction": ["# Reasoning Path:\nFrance -> government.governmental_jurisdiction.governing_officials -> m.0102_81j -> government.government_position_held.appointed_by -> Fran\u00e7ois Hollande\n# Answer:\nm.0102_81j", "# Reasoning Path:\nFrance -> government.governmental_jurisdiction.governing_officials -> m.0102_81j -> government.government_position_held.office_holder -> Michel Sapin\n# Answer:\nm.0102_81j", "# Reasoning Path:\nFrance -> government.governmental_jurisdiction.governing_officials -> m.0ngn_cw -> government.government_position_held.office_holder -> Nicolas Sarkozy\n# Answer:\nm.0ngn_cw", "# Reasoning Path:\nFrance -> government.governmental_jurisdiction.governing_officials -> m.0102_2fw -> government.government_position_held.appointed_by -> Fran\u00e7ois Hollande\n# Answer:\nm.0102_2fw", "# Reasoning Path:\nFrance -> government.governmental_jurisdiction.governing_officials -> m.0102_81j -> freebase.valuenotation.has_no_value -> District represented (if position is district-related)\n# Answer:\nm.0102_81j", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.11b60tv_69\n# Answer:\ng.11b60tv_69", "# Reasoning Path:\nFrance -> government.governmental_jurisdiction.governing_officials -> m.0ngn_cw -> government.government_position_held.office_position_or_title -> Minister of the Interior\n# Answer:\nm.0ngn_cw", "# Reasoning Path:\nFrance -> location.statistical_region.gni_in_ppp_dollars -> g.11b60wv0lq\n# Answer:\ng.11b60wv0lq", "# Reasoning Path:\nFrance -> government.governmental_jurisdiction.governing_officials -> m.0102_2fw -> government.government_position_held.governmental_body -> Government of France\n# Answer:\nm.0102_2fw", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6gn9g\n# Answer:\ng.12tb6gn9g"], "ground_truth": ["Nicolas Sarkozy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.125, "path_precision": 0.1, "path_recall": 0.16666666666666666, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-2029", "prediction": ["# Reasoning Path:\nKnossos -> location.location.containedby -> Greece -> location.location.containedby -> Europe\n# Answer:\nGreece", "# Reasoning Path:\nKnossos -> location.location.containedby -> Crete -> location.location.containedby -> Greece\n# Answer:\nCrete", "# Reasoning Path:\nKnossos -> location.location.containedby -> Greece -> location.location.containedby -> Eurasia\n# Answer:\nGreece", "# Reasoning Path:\nKnossos -> location.location.containedby -> Greece -> location.location.containedby -> Southeast Europe\n# Answer:\nGreece", "# Reasoning Path:\nKnossos -> location.location.containedby -> Greece -> location.country.administrative_divisions -> Achaea\n# Answer:\nGreece", "# Reasoning Path:\nKnossos -> location.location.containedby -> Crete -> location.location.containedby -> Mediterranean Sea\n# Answer:\nCrete", "# Reasoning Path:\nKnossos -> location.location.containedby -> Crete -> film.film_location.featured_in_films -> The 11th Day: Crete 1941\n# Answer:\nCrete", "# Reasoning Path:\nKnossos -> common.topic.article -> m.0jwy293\n# Answer:\nm.0jwy293", "# Reasoning Path:\nKnossos -> location.location.containedby -> Greece -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60nhckb\n# Answer:\nGreece", "# Reasoning Path:\nKnossos -> location.location.containedby -> Greece -> location.country.administrative_divisions -> Aetolia-Acarnania\n# Answer:\nGreece"], "ground_truth": ["Greece", "Crete"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-2030", "prediction": ["# Reasoning Path:\nArea 51 -> aviation.airport.serves -> Nevada -> location.location.containedby -> Contiguous United States\n# Answer:\nNevada", "# Reasoning Path:\nArea 51 -> aviation.airport.serves -> Nevada -> location.location.containedby -> Mountain States\n# Answer:\nNevada", "# Reasoning Path:\nArea 51 -> aviation.airport.serves -> Nevada -> location.statistical_region.religions -> m.04403h4\n# Answer:\nNevada", "# Reasoning Path:\nArea 51 -> location.location.containedby -> Nevada -> location.location.containedby -> Contiguous United States\n# Answer:\nNevada", "# Reasoning Path:\nArea 51 -> aviation.airport.serves -> Nevada -> location.location.containedby -> Southwestern United States\n# Answer:\nNevada", "# Reasoning Path:\nArea 51 -> aviation.airport.serves -> Nevada -> location.location.people_born_here -> A. J. Haglund\n# Answer:\nNevada", "# Reasoning Path:\nArea 51 -> common.topic.image -> Wfm area 51 landsat geocover 2000 -> common.image.appears_in_topic_gallery -> Groom Lake\n# Answer:\nWfm area 51 landsat geocover 2000", "# Reasoning Path:\nArea 51 -> aviation.airport.serves -> Nevada -> location.statistical_region.religions -> m.04403hg\n# Answer:\nNevada", "# Reasoning Path:\nArea 51 -> location.location.containedby -> Nevada -> location.location.containedby -> Mountain States\n# Answer:\nNevada", "# Reasoning Path:\nArea 51 -> location.location.containedby -> Nevada -> location.statistical_region.religions -> m.04403h4\n# Answer:\nNevada"], "ground_truth": ["Nevada", "Lincoln County"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6428571428571429, "ans_precission": 0.9, "ans_recall": 0.5, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.6428571428571429, "path_ans_precision": 0.9, "path_ans_recall": 0.5}
{"id": "WebQTest-2031", "prediction": ["# Reasoning Path:\nChris Riley -> people.person.place_of_birth -> Echuca -> location.location.containedby -> Australia\n# Answer:\nEchuca", "# Reasoning Path:\nChris Riley -> common.topic.notable_for -> g.1258z_d26\n# Answer:\ng.1258z_d26", "# Reasoning Path:\nChris Riley -> people.person.place_of_birth -> Echuca -> travel.travel_destination.tourist_attractions -> Billabong Ranch\n# Answer:\nEchuca", "# Reasoning Path:\nChris Riley -> people.person.place_of_birth -> Echuca -> location.location.containedby -> Victoria\n# Answer:\nEchuca", "# Reasoning Path:\nChris Riley -> people.person.place_of_birth -> Echuca -> common.topic.image -> Echuca docks Stevage\n# Answer:\nEchuca", "# Reasoning Path:\nChris Riley -> people.person.place_of_birth -> Echuca -> travel.travel_destination.tourist_attractions -> Echuca Historical Society Museum\n# Answer:\nEchuca", "# Reasoning Path:\nChris Riley -> people.person.place_of_birth -> Echuca -> travel.travel_destination.tourist_attractions -> Echuca Regional Park\n# Answer:\nEchuca", "# Reasoning Path:\nChris Riley -> people.person.place_of_birth -> Echuca -> common.topic.image -> Echuca location map in Victoria\n# Answer:\nEchuca", "# Reasoning Path:\nChris Riley -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nChris Riley -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale"], "ground_truth": ["Echuca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-204", "prediction": ["# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo -> periodicals.newspaper_circulation_area.newspapers -> Al-Ahram Weekly\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo -> common.topic.subject_of -> As Egypt Howls and History Tweets\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo -> periodicals.newspaper_circulation_area.newspapers -> Almesryoon\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.capital -> Cairo -> periodicals.newspaper_circulation_area.newspapers -> Al-Ahram Weekly\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Al Sharqia Governorate -> location.location.containedby -> North Africa\n# Answer:\nAl Sharqia Governorate", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Alexandria Governorate -> location.location.containedby -> North Africa\n# Answer:\nAlexandria Governorate", "# Reasoning Path:\nEgypt -> location.country.capital -> Cairo -> common.topic.subject_of -> As Egypt Howls and History Tweets\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.capital -> Cairo -> periodicals.newspaper_circulation_area.newspapers -> Almesryoon\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Al Sharqia Governorate -> base.aareas.schema.administrative_area.administrative_area_type -> Egyptian governorate\n# Answer:\nAl Sharqia Governorate", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Alexandria Governorate -> common.topic.notable_types -> Administrative Division\n# Answer:\nAlexandria Governorate"], "ground_truth": ["Cairo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-205", "prediction": ["# Reasoning Path:\nErnest Rutherford -> people.person.education -> m.02kq20k -> education.education.institution -> University of Canterbury\n# Answer:\nm.02kq20k", "# Reasoning Path:\nErnest Rutherford -> people.person.education -> m.02kq21s -> education.education.institution -> University of Cambridge\n# Answer:\nm.02kq21s", "# Reasoning Path:\nErnest Rutherford -> people.deceased_person.place_of_death -> Cambridge -> location.location.contains -> University of Cambridge\n# Answer:\nCambridge", "# Reasoning Path:\nErnest Rutherford -> people.person.education -> m.02wntpq -> education.education.institution -> Trinity College, Cambridge\n# Answer:\nm.02wntpq", "# Reasoning Path:\nErnest Rutherford -> people.deceased_person.place_of_death -> Cambridge -> common.topic.notable_types -> City/Town/Village\n# Answer:\nCambridge", "# Reasoning Path:\nErnest Rutherford -> base.kwebbase.kwtopic.connections_from -> ernest rutherford corresponded with albert einstein -> base.kwebbase.kwconnection.other -> Albert Einstein\n# Answer:\nernest rutherford corresponded with albert einstein", "# Reasoning Path:\nErnest Rutherford -> people.deceased_person.place_of_death -> Cambridge -> location.location.contains -> Trinity College, Cambridge\n# Answer:\nCambridge", "# Reasoning Path:\nErnest Rutherford -> people.deceased_person.place_of_death -> Cambridge -> location.location.containedby -> England\n# Answer:\nCambridge", "# Reasoning Path:\nErnest Rutherford -> base.kwebbase.kwtopic.connections_from -> ernest rutherford corresponded with enrico fermi -> base.kwebbase.kwconnection.relation -> corresponded with\n# Answer:\nernest rutherford corresponded with enrico fermi", "# Reasoning Path:\nErnest Rutherford -> people.deceased_person.place_of_death -> Cambridge -> location.location.containedby -> United Kingdom\n# Answer:\nCambridge"], "ground_truth": ["University of Cambridge", "Trinity College, Cambridge", "University of Canterbury", "Nelson College", "University of New Zealand"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.588235294117647, "path_precision": 0.5, "path_recall": 0.7142857142857143, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.5, "path_ans_recall": 0.6}
{"id": "WebQTest-206", "prediction": ["# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Shooting guard -> common.topic.image -> Basketball half-court\n# Answer:\nShooting guard", "# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Shooting guard -> sports.sports_position.sport -> Basketball\n# Answer:\nShooting guard", "# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Point guard -> sports.sports_position.sport -> Basketball\n# Answer:\nPoint guard", "# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Shooting guard -> common.topic.notable_for -> g.1255g6p7n\n# Answer:\nShooting guard", "# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Small forward -> sports.sports_position.sport -> Basketball\n# Answer:\nSmall forward", "# Reasoning Path:\nBrandon Roy -> sports.pro_athlete.sports_played_professionally -> m.0c53rsf -> sports.pro_sports_played.sport -> Basketball\n# Answer:\nm.0c53rsf", "# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Shooting guard -> common.topic.image -> Jordan by Lipofsky 16577\n# Answer:\nShooting guard", "# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Point guard -> common.topic.notable_for -> g.125c79ttb\n# Answer:\nPoint guard", "# Reasoning Path:\nBrandon Roy -> sports.drafted_athlete.drafted -> m.04_bzhf -> sports.sports_league_draft_pick.school -> University of Washington\n# Answer:\nm.04_bzhf", "# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Point guard -> common.topic.article -> m.02sg00\n# Answer:\nPoint guard"], "ground_truth": ["Point guard", "Small forward", "Shooting guard"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-207", "prediction": ["# Reasoning Path:\nHarvard University -> location.location.containedby -> Massachusetts -> location.location.containedby -> United States of America\n# Answer:\nMassachusetts", "# Reasoning Path:\nHarvard University -> location.location.containedby -> Cambridge -> location.location.containedby -> Massachusetts\n# Answer:\nCambridge", "# Reasoning Path:\nHarvard University -> location.location.containedby -> Massachusetts -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nMassachusetts", "# Reasoning Path:\nHarvard University -> location.location.containedby -> Massachusetts -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMassachusetts", "# Reasoning Path:\nHarvard University -> location.location.containedby -> Cambridge -> location.location.containedby -> United States of America\n# Answer:\nCambridge", "# Reasoning Path:\nHarvard University -> location.location.containedby -> Cambridge -> location.location.containedby -> Area codes 617 and 857\n# Answer:\nCambridge", "# Reasoning Path:\nHarvard University -> location.location.containedby -> Cambridge -> location.citytown.postal_codes -> 02138\n# Answer:\nCambridge", "# Reasoning Path:\nHarvard University -> location.location.containedby -> Cambridge -> periodicals.newspaper_circulation_area.newspapers -> Harvard Law Record\n# Answer:\nCambridge", "# Reasoning Path:\nHarvard University -> location.location.contains -> Gore Hall -> common.topic.notable_for -> g.1yw9k9gf0\n# Answer:\nGore Hall", "# Reasoning Path:\nHarvard University -> location.location.contains -> Massachusetts Hall -> location.location.containedby -> Cambridge\n# Answer:\nMassachusetts Hall"], "ground_truth": ["Cambridge", "Massachusetts", "United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7659574468085106, "ans_precission": 0.9, "ans_recall": 0.6666666666666666, "path_f1": 0.7272727272727272, "path_precision": 0.8, "path_recall": 0.6666666666666666, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-209", "prediction": ["# Reasoning Path:\nLaos -> location.location.containedby -> Asia -> travel.travel_destination.tour_operators -> Bunnik Tours\n# Answer:\nAsia", "# Reasoning Path:\nLaos -> base.locations.countries.continent -> Asia -> travel.travel_destination.tour_operators -> Bunnik Tours\n# Answer:\nAsia", "# Reasoning Path:\nLaos -> location.location.containedby -> Asia -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nAsia", "# Reasoning Path:\nLaos -> location.location.containedby -> Southeast Asia -> location.location.containedby -> Asia\n# Answer:\nSoutheast Asia", "# Reasoning Path:\nLaos -> location.location.containedby -> Eurasia -> common.topic.notable_types -> Location\n# Answer:\nEurasia", "# Reasoning Path:\nLaos -> location.statistical_region.debt_service_as_percent_of_trade_volume -> g.1hhc39wvj\n# Answer:\ng.1hhc39wvj", "# Reasoning Path:\nLaos -> location.location.containedby -> Asia -> base.locations.continents.countries_within -> Thailand\n# Answer:\nAsia", "# Reasoning Path:\nLaos -> base.locations.countries.continent -> Asia -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nAsia", "# Reasoning Path:\nLaos -> location.location.containedby -> Asia -> base.locations.continents.countries_within -> Cambodia\n# Answer:\nAsia", "# Reasoning Path:\nLaos -> location.statistical_region.debt_service_as_percent_of_trade_volume -> g.1hhc3r_nz\n# Answer:\ng.1hhc3r_nz"], "ground_truth": ["Southeast Asia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Parents\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> people.person.children -> Sean Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> freebase.valuenotation.has_value -> Parents\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> common.topic.notable_types -> Person\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> people.person.children -> Neffeteria Pugh\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan -> music.recording.featured_artists -> J. Holiday\n# Answer:\n#1 Fan", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> common.topic.notable_types -> Person\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Sal Gibson -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nSal Gibson"], "ground_truth": ["Leon Cole", "Francine Lons", "Sal Gibson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-210", "prediction": ["# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> location.location.containedby -> Los Angeles\n# Answer:\nWatts", "# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> location.location.containedby -> California\n# Answer:\nWatts", "# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> location.location.people_born_here -> Aaron Meeks\n# Answer:\nWatts", "# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> location.location.containedby -> Greater Los Angeles Area\n# Answer:\nWatts", "# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> common.topic.image -> Simon Rodia's Watts Towers.\n# Answer:\nWatts", "# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> location.location.people_born_here -> Big Jay McNeely\n# Answer:\nWatts", "# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> location.location.people_born_here -> Da Unda Dogg\n# Answer:\nWatts", "# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> common.topic.image -> Watts in 1912\n# Answer:\nWatts", "# Reasoning Path:\nTyrese Gibson -> music.composer.compositions -> How You Gonna Act Like That -> music.composition.recordings -> How You Gonna Act Lick That\n# Answer:\nHow You Gonna Act Like That", "# Reasoning Path:\nTyrese Gibson -> music.composer.compositions -> 2000 Watts -> music.album.genre -> Rhythm and blues\n# Answer:\n2000 Watts"], "ground_truth": ["Watts"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-211", "prediction": ["# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Switzerland\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics -> olympics.olympic_games.participating_countries -> Switzerland\n# Answer:\n1896 Summer Olympics", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.sports -> Ice Hockey\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Austria\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics -> olympics.olympic_games.participating_countries -> Austria\n# Answer:\n1896 Summer Olympics", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 1900 Summer Olympics -> olympics.olympic_games.participating_countries -> Switzerland\n# Answer:\n1900 Summer Olympics", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\ng.11b71mdn0q", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Australia\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics -> olympics.olympic_games.participating_countries -> Australia\n# Answer:\n1896 Summer Olympics", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics -> olympics.olympic_games.events -> Men's Long jump\n# Answer:\n1896 Summer Olympics"], "ground_truth": ["Slovenia", "France", "Austria", "Switzerland"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.06666666666666667, "path_precision": 0.5, "path_recall": 0.03571428571428571, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-212", "prediction": ["# Reasoning Path:\nColorado River -> location.location.partially_containedby -> Arizona -> common.topic.webpage -> m.09w1gvc\n# Answer:\nArizona", "# Reasoning Path:\nColorado River -> location.location.partially_containedby -> Arizona -> common.topic.webpage -> m.09w1l1s\n# Answer:\nArizona", "# Reasoning Path:\nColorado River -> location.location.partially_containedby -> Arizona -> government.governmental_jurisdiction.governing_officials -> m.0108gfng\n# Answer:\nArizona", "# Reasoning Path:\nColorado River -> location.location.partially_containedby -> Arizona -> common.topic.webpage -> m.09w2hwc\n# Answer:\nArizona", "# Reasoning Path:\nColorado River -> location.location.partially_containedby -> Arizona -> base.aliens.ufo_sighting_location.ufo_sighting_s -> Phoenix Lights\n# Answer:\nArizona", "# Reasoning Path:\nColorado River -> location.location.partially_containedby -> Colorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Cavalry Regiment\n# Answer:\nColorado", "# Reasoning Path:\nColorado River -> location.location.partially_containedby -> Arizona -> government.governmental_jurisdiction.governing_officials -> m.010dylrt\n# Answer:\nArizona", "# Reasoning Path:\nColorado River -> common.topic.image -> Colorado watershed -> common.image.size -> m.02fn6rg\n# Answer:\nColorado watershed", "# Reasoning Path:\nColorado River -> location.location.partially_containedby -> Arizona -> government.governmental_jurisdiction.governing_officials -> m.010f0rgh\n# Answer:\nArizona", "# Reasoning Path:\nColorado River -> location.location.partially_containedby -> Arizona -> base.aliens.ufo_sighting_location.ufo_sighting_s -> Travis Walton\n# Answer:\nArizona"], "ground_truth": ["Nevada", "Arizona", "California", "Colorado"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6428571428571429, "path_precision": 0.9, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-213", "prediction": ["# Reasoning Path:\nScarlett Johansson -> base.popstra.celebrity.dated -> m.065q6ym -> base.popstra.dated.participant -> Ryan Reynolds\n# Answer:\nRyan Reynolds", "# Reasoning Path:\nScarlett Johansson -> base.popstra.celebrity.dated -> m.064fp5j -> base.popstra.dated.participant -> nm1157013\n# Answer:\nm.064fp5j", "# Reasoning Path:\nScarlett Johansson -> base.popstra.celebrity.dated -> m.064jrnt -> base.popstra.dated.participant -> Josh Hartnett\n# Answer:\nm.064jrnt", "# Reasoning Path:\nScarlett Johansson -> celebrities.celebrity.sexual_relationships -> m.04xwldp -> celebrities.romantic_relationship.celebrity -> Josh Hartnett\n# Answer:\nm.04xwldp", "# Reasoning Path:\nScarlett Johansson -> celebrities.celebrity.sexual_relationships -> m.04xwldp -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nm.04xwldp", "# Reasoning Path:\nScarlett Johansson -> celebrities.celebrity.sexual_relationships -> m.07bb7c3 -> celebrities.romantic_relationship.celebrity -> Benicio del Toro\n# Answer:\nm.07bb7c3", "# Reasoning Path:\nScarlett Johansson -> celebrities.celebrity.sexual_relationships -> m.07bb7cc -> celebrities.romantic_relationship.celebrity -> Jared Leto\n# Answer:\nm.07bb7cc", "# Reasoning Path:\nScarlett Johansson -> celebrities.celebrity.sexual_relationships -> m.07bb7c3 -> celebrities.romantic_relationship.relationship_type -> Slept with\n# Answer:\nm.07bb7c3", "# Reasoning Path:\nScarlett Johansson -> common.topic.webpage -> m.05bv55w -> common.webpage.resource -> Scarlett Johansson @ Hollywood Hotties\n# Answer:\nm.05bv55w", "# Reasoning Path:\nScarlett Johansson -> celebrities.celebrity.sexual_relationships -> m.07bb7cc -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nm.07bb7cc"], "ground_truth": ["Jared Leto", "Josh Hartnett", "Romain Dauriac", "Justin Timberlake", "Patrick Wilson", "Benicio del Toro", "nm1157013", "Ryan Reynolds", "Topher Grace"], "ans_acc": 0.5555555555555556, "ans_hit": 1, "ans_f1": 0.10526315789473685, "ans_precission": 0.1, "ans_recall": 0.1111111111111111, "path_f1": 0.3870967741935483, "path_precision": 0.6, "path_recall": 0.2857142857142857, "path_ans_f1": 0.576923076923077, "path_ans_precision": 0.6, "path_ans_recall": 0.5555555555555556}
{"id": "WebQTest-214", "prediction": ["# Reasoning Path:\nRoman Empire -> location.location.partially_contained_by -> m.0ws8w7_ -> location.partial_containment_relationship.partially_contained_by -> Europe\n# Answer:\nm.0ws8w7_", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Greek Language -> language.human_language.region -> Europe\n# Answer:\nGreek Language", "# Reasoning Path:\nRoman Empire -> location.location.partially_contained_by -> m.0ws8w38 -> location.partial_containment_relationship.partially_contained_by -> North Africa\n# Answer:\nm.0ws8w38", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Latin Language -> language.human_language.region -> Europe\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> location.location.partially_contained_by -> m.0ws8wk6 -> location.partial_containment_relationship.partially_contained_by -> Middle East\n# Answer:\nm.0ws8wk6", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Greek Language -> media_common.netflix_genre.titles -> A Girl in Black\n# Answer:\nGreek Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Latin Language -> language.human_language.countries_spoken_in -> Duchy of Burgundy\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Greek Language -> media_common.netflix_genre.titles -> A Matter of Dignity\n# Answer:\nGreek Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Latin Language -> music.genre.albums -> Acustico\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> military.military_combatant.military_conflicts -> m.048zsyf -> military.military_combatant_group.conflict -> First Jewish\u2013Roman War\n# Answer:\nm.048zsyf"], "ground_truth": ["Europe", "North Africa", "Middle East"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-215", "prediction": ["# Reasoning Path:\nElvis Costello -> people.person.place_of_birth -> Paddington -> location.location.containedby -> London\n# Answer:\nPaddington", "# Reasoning Path:\nElvis Costello -> people.person.place_of_birth -> Paddington -> location.location.containedby -> United Kingdom\n# Answer:\nPaddington", "# Reasoning Path:\nElvis Costello -> tv.tv_actor.guest_roles -> m.040p_0y -> tv.tv_guest_role.episodes_appeared_in -> How I Spent My Strummer Vacation\n# Answer:\nm.040p_0y", "# Reasoning Path:\nElvis Costello -> tv.tv_actor.guest_roles -> m.0b6vqtt -> tv.tv_guest_role.episodes_appeared_in -> Kidney Now!\n# Answer:\nm.0b6vqtt", "# Reasoning Path:\nElvis Costello -> common.topic.webpage -> Discogs page -> common.webpage.category -> Topic Webpage\n# Answer:\nDiscogs page", "# Reasoning Path:\nElvis Costello -> tv.tv_actor.guest_roles -> m.04djjqv -> tv.tv_guest_role.episodes_appeared_in -> Farewell, Nervosa\n# Answer:\nm.04djjqv", "# Reasoning Path:\nElvis Costello -> common.topic.webpage -> m.03kx15c -> common.webpage.resource -> m.0blvlws\n# Answer:\nm.03kx15c", "# Reasoning Path:\nElvis Costello -> common.topic.webpage -> MusicMoz page for Elvis Costello -> common.webpage.category -> Topic Webpage\n# Answer:\nMusicMoz page for Elvis Costello", "# Reasoning Path:\nElvis Costello -> common.topic.webpage -> m.03kx15c -> common.webpage.category -> Official Website\n# Answer:\nm.03kx15c"], "ground_truth": ["Paddington"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-216", "prediction": ["# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> location.location.containedby -> Asia\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> location.location.containedby -> East Asia\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> location.location.containedby -> Eurasia\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> geography.geographical_feature.category -> Desert -> fictional_universe.fictional_setting.works_set_here -> Faun\n# Answer:\nDesert", "# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> government.governmental_jurisdiction.governing_officials -> m.010g61fb\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> location.statistical_region.external_debt_stock -> g.11b71px2lk\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> government.governmental_jurisdiction.governing_officials -> m.010g61kh\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> government.governmental_jurisdiction.governing_officials -> m.010g61rc\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> location.statistical_region.external_debt_stock -> g.1hhc390rw\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> geography.geographical_feature.category -> Desert -> common.topic.webpage -> m.09y18gb\n# Answer:\nDesert"], "ground_truth": ["Mongolia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-217", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Germany\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\ng.125_r6z32", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> American Samoa\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Germany\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> periodicals.newspaper_circulation_area.newspapers -> Draugas -> book.newspaper.circulation_areas -> Australia\n# Answer:\nDraugas", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nm.010r7f0z", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Mexico\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Mexico\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guam\n# Answer:\nPresidential system"], "ground_truth": ["Constitutional republic", "Presidential system", "Federal republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-218", "prediction": ["# Reasoning Path:\nGermany -> location.location.containedby -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nGermany -> location.location.containedby -> Eurasia\n# Answer:\nEurasia", "# Reasoning Path:\nGermany -> location.location.containedby -> Europe -> organization.organization_scope.organizations_with_this_scope -> European Union\n# Answer:\nEurope", "# Reasoning Path:\nGermany -> base.locations.countries.continent -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nGermany -> location.location.containedby -> Europe -> base.locations.continents.planet -> Earth\n# Answer:\nEurope", "# Reasoning Path:\nGermany -> location.location.containedby -> Western Europe -> location.location.containedby -> Europe\n# Answer:\nWestern Europe", "# Reasoning Path:\nGermany -> base.locations.countries.continent -> Europe -> organization.organization_scope.organizations_with_this_scope -> European Union\n# Answer:\nEurope", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\ng.11b60v9zy7", "# Reasoning Path:\nGermany -> base.locations.countries.continent -> Europe -> base.locations.continents.planet -> Earth\n# Answer:\nEurope", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fdl8\n# Answer:\ng.12tb6fdl8"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Syria\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\ng.12cp_k6sh", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> France\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\ng.1hhc3_4cn", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Portugal\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> common.topic.notable_types -> Governmental Body\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3f_h6\n# Answer:\ng.1hhc3f_h6", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nHouse of Representatives"], "ground_truth": ["Provisional government", "Semi-presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-220", "prediction": ["# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Zaza language -> common.topic.notable_types -> Human Language\n# Answer:\nZaza language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Turkish Language -> common.topic.notable_types -> Human Language\n# Answer:\nTurkish Language", "# Reasoning Path:\nTurkey -> location.statistical_region.minimum_wage -> g.11b60lkkk3\n# Answer:\ng.11b60lkkk3", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Turkish Language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nTurkish Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nArabic Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Turkish Language -> language.human_language.region -> Asia\n# Answer:\nTurkish Language", "# Reasoning Path:\nTurkey -> location.statistical_region.minimum_wage -> g.11b7gsv6fr\n# Answer:\ng.11b7gsv6fr", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Syria\n# Answer:\nArabic Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Arabic Language -> common.topic.notable_types -> Human Language\n# Answer:\nArabic Language", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.065sxsl -> military.military_combatant_group.conflict -> Korean War\n# Answer:\nm.065sxsl"], "ground_truth": ["Turkish Language", "Arabic Language", "Ladino Language", "Zaza language", "Kurdish languages"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6461538461538462, "ans_precission": 0.7, "ans_recall": 0.6, "path_f1": 0.4000000000000001, "path_precision": 0.4, "path_recall": 0.4, "path_ans_f1": 0.6461538461538462, "path_ans_precision": 0.7, "path_ans_recall": 0.6}
{"id": "WebQTest-221", "prediction": ["# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0j2mvjf -> base.schemastaging.athlete_salary.team -> New York Knicks\n# Answer:\nm.0j2mvjf", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0_qrbd1 -> base.schemastaging.athlete_salary.team -> Houston Rockets\n# Answer:\nm.0_qrbd1", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0j2mvjf -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0j2mvjf", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0ng9xr6 -> base.schemastaging.athlete_salary.team -> Houston Rockets\n# Answer:\nm.0ng9xr6", "# Reasoning Path:\nJeremy Lin -> sports.pro_athlete.teams -> m.0k6s01p -> sports.sports_team_roster.team -> New York Knicks\n# Answer:\nm.0k6s01p", "# Reasoning Path:\nJeremy Lin -> sports.pro_athlete.teams -> m.01145k7g -> sports.sports_team_roster.team -> Los Angeles Lakers\n# Answer:\nm.01145k7g", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0_qrbd1 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0_qrbd1", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0ng9xr6 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0ng9xr6", "# Reasoning Path:\nJeremy Lin -> sports.pro_athlete.teams -> m.0k6s01p -> sports.sports_team_roster.position -> Point guard\n# Answer:\nm.0k6s01p", "# Reasoning Path:\nJeremy Lin -> sports.pro_athlete.teams -> m.0j2dfqc -> sports.sports_team_roster.position -> Point guard\n# Answer:\nm.0j2dfqc"], "ground_truth": ["Los Angeles Lakers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-222", "prediction": ["# Reasoning Path:\nSeychelles -> location.location.containedby -> Africa -> base.locations.continents.countries_within -> Algeria\n# Answer:\nAfrica", "# Reasoning Path:\nSeychelles -> location.location.containedby -> Africa -> organization.organization_scope.organizations_with_this_scope -> Southern African Development Community\n# Answer:\nAfrica", "# Reasoning Path:\nSeychelles -> location.location.containedby -> Southern Sun Hotel Group -> location.location.containedby -> g.1pxpr66mx\n# Answer:\nSouthern Sun Hotel Group", "# Reasoning Path:\nSeychelles -> location.location.containedby -> Africa -> location.location.time_zones -> Seychelles Time Zone\n# Answer:\nAfrica", "# Reasoning Path:\nSeychelles -> location.location.containedby -> Africa -> base.locations.continents.countries_within -> Angola\n# Answer:\nAfrica", "# Reasoning Path:\nSeychelles -> location.statistical_region.high_tech_as_percent_of_manufactured_exports -> g.1hhc3_l38\n# Answer:\ng.1hhc3_l38", "# Reasoning Path:\nSeychelles -> location.location.containedby -> Africa -> base.locations.continents.countries_within -> Benin\n# Answer:\nAfrica", "# Reasoning Path:\nSeychelles -> base.locations.countries.continent -> Africa -> base.locations.continents.countries_within -> Algeria\n# Answer:\nAfrica", "# Reasoning Path:\nSeychelles -> base.locations.countries.continent -> Africa -> organization.organization_scope.organizations_with_this_scope -> Southern African Development Community\n# Answer:\nAfrica", "# Reasoning Path:\nSeychelles -> base.locations.countries.continent -> Africa -> location.location.time_zones -> Seychelles Time Zone\n# Answer:\nAfrica"], "ground_truth": ["Africa", "Southern Sun Hotel Group"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-224", "prediction": ["# Reasoning Path:\nWilliam Morris -> people.person.education -> m.0n1jtn3 -> education.education.institution -> University of Oxford\n# Answer:\nm.0n1jtn3", "# Reasoning Path:\nWilliam Morris -> people.person.education -> m.02wmrt8 -> education.education.institution -> Exeter College, Oxford\n# Answer:\nm.02wmrt8", "# Reasoning Path:\nWilliam Morris -> people.person.education -> m.04q1gj3 -> education.education.institution -> Marlborough College\n# Answer:\nm.04q1gj3", "# Reasoning Path:\nWilliam Morris -> base.kwebbase.kwtopic.has_sentences -> After marrying, Morris commissioned an architect friend to build him a home at Bexleyheath in Kent which became known as the Red House  because it was built of red bricks with no stucco finish. -> base.kwebbase.kwsentence.next_sentence -> The Red House was the first of this type of domestic architecture in nineteenth century England.\n# Answer:\nAfter marrying, Morris commissioned an architect friend to build him a home at Bexleyheath in Kent which became known as the Red House  because it was built of red bricks with no stucco finish.", "# Reasoning Path:\nWilliam Morris -> base.kwebbase.kwtopic.connections_from -> william morris a pal of alphonse mucha -> base.kwebbase.kwconnection.relation -> a pal of\n# Answer:\nwilliam morris a pal of alphonse mucha", "# Reasoning Path:\nWilliam Morris -> base.kwebbase.kwtopic.connections_from -> william morris a pal of edward coley burne-jones -> base.kwebbase.kwconnection.sentence -> At university Morris met the future artist Burne-Jones who became a lifelong friend.\n# Answer:\nwilliam morris a pal of edward coley burne-jones", "# Reasoning Path:\nWilliam Morris -> base.kwebbase.kwtopic.connections_from -> william morris a pal of alphonse mucha -> base.kwebbase.kwconnection.sentence -> Was a friend of Mucha and Yeats.\n# Answer:\nwilliam morris a pal of alphonse mucha", "# Reasoning Path:\nWilliam Morris -> base.kwebbase.kwtopic.connections_from -> william morris a pal of william butler yeats -> base.kwebbase.kwconnection.relation -> a pal of\n# Answer:\nwilliam morris a pal of william butler yeats", "# Reasoning Path:\nWilliam Morris -> base.kwebbase.kwtopic.has_sentences -> Among others, he was heard by the future novelist, HG Wells and Marx's daughter, Eleanor Marx. -> base.kwebbase.kwsentence.previous_sentence -> He read \\\"Das Kapital\\\" by Karl Marx  and in 1883 joined the Socialist Democratic Federation and began to tour the industrial areas of Britain, giving lectures on socialism.\n# Answer:\nAmong others, he was heard by the future novelist, HG Wells and Marx's daughter, Eleanor Marx.", "# Reasoning Path:\nWilliam Morris -> base.kwebbase.kwtopic.has_sentences -> After marrying, Morris commissioned an architect friend to build him a home at Bexleyheath in Kent which became known as the Red House  because it was built of red bricks with no stucco finish. -> base.kwebbase.kwsentence.previous_sentence -> In 1858 he published a volume of poems under the title \\\"The Defence of Guenevere and Other Poems\\\".\n# Answer:\nAfter marrying, Morris commissioned an architect friend to build him a home at Bexleyheath in Kent which became known as the Red House  because it was built of red bricks with no stucco finish."], "ground_truth": ["University of Oxford", "Exeter College, Oxford"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-225", "prediction": ["# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.containedby -> Contiguous United States\n# Answer:\nWashington", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.containedby -> Washington\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.containedby -> Northwestern United States\n# Answer:\nWashington", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.containedby -> Pacific Northwest\n# Answer:\nWashington", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Cascade Range -> location.location.containedby -> North America\n# Answer:\nCascade Range", "# Reasoning Path:\nMount St. Helens -> geography.mountain.mountain_range -> Cascade Range -> location.location.containedby -> North America\n# Answer:\nCascade Range", "# Reasoning Path:\nMount St. Helens -> common.image.size -> m.02bkwf9\n# Answer:\nm.02bkwf9", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> government.political_district.representatives -> m.04j8hdx\n# Answer:\nWashington", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.containedby -> Portland-Vancouver-Beaverton, OR-WA Metropolitan Statistical Area\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.events -> 2009 Cherry Blossom 10-Mile Run\n# Answer:\nWashington"], "ground_truth": ["Washington"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-226", "prediction": ["# Reasoning Path:\nJimmy Savile -> people.person.profession -> Presenter -> base.descriptive_names.names.descriptive_name -> m.010543ty\n# Answer:\nPresenter", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Disc jockey -> people.profession.specialization_of -> Entertainer\n# Answer:\nDisc jockey", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Presenter -> base.descriptive_names.names.descriptive_name -> m.01064h0m\n# Answer:\nPresenter", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Presenter -> fictional_universe.character_occupation.characters_with_this_occupation -> Harmony Kendall\n# Answer:\nPresenter", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Presenter -> base.descriptive_names.names.descriptive_name -> m.01066x64\n# Answer:\nPresenter", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Presenter -> people.profession.specializations -> Showman\n# Answer:\nPresenter", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Radio personality -> base.descriptive_names.names.descriptive_name -> m.010260w1\n# Answer:\nRadio personality", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Radio personality -> fictional_universe.character_occupation.characters_with_this_occupation -> Frasier Crane\n# Answer:\nRadio personality", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Presenter -> fictional_universe.character_occupation.characters_with_this_occupation -> Jackie Burkhart\n# Answer:\nPresenter", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Disc jockey -> common.topic.webpage -> m.046cmsk\n# Answer:\nDisc jockey"], "ground_truth": ["Radio personality", "Disc jockey", "Presenter"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-227", "prediction": ["# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> Ontario -> periodicals.newspaper_circulation_area.newspapers -> Sudbury Star\n# Answer:\nOntario", "# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> Ontario -> location.administrative_division.country -> Canada\n# Answer:\nOntario", "# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> Ontario -> periodicals.newspaper_circulation_area.newspapers -> Bradford West Gwillimbury Times\n# Answer:\nOntario", "# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> New York -> book.book_subject.works -> All Around the Town\n# Answer:\nNew York", "# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> Ontario -> periodicals.newspaper_circulation_area.newspapers -> Leader Spirit\n# Answer:\nOntario", "# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> Ontario -> government.governmental_jurisdiction.agencies -> Landlord and Tenant Board\n# Answer:\nOntario", "# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> New York -> location.location.geolocation -> m.046vxf5\n# Answer:\nNew York", "# Reasoning Path:\nSeneca people -> base.casinos.casino_owner.casinos_owned -> Seneca Niagara Casino & Hotel -> common.topic.notable_for -> g.1259vb0h_\n# Answer:\nSeneca Niagara Casino & Hotel", "# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> New York -> book.book_subject.works -> Amusing the Million\n# Answer:\nNew York", "# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> New York -> government.political_district.representatives -> m.011vj0lv\n# Answer:\nNew York"], "ground_truth": ["New York", "Ontario"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.events -> Assassination of Martin Luther King, Jr.\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.containedby -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> travel.tourist_attraction.near_travel_destination -> Atlanta\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nm.011j_4sh", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> location.location.geolocation -> m.0wmyhzk\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.06_41f8\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Alabama\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.011j_4sh", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0_714v2 -> film.personal_film_appearance.film -> Alice Walker: Beauty in Truth\n# Answer:\nm.0_714v2"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-231", "prediction": ["# Reasoning Path:\nMiami Marlins -> sports.sports_team.league -> m.0crt4jw -> sports.sports_league_participation.league -> Major League Baseball\n# Answer:\nm.0crt4jw", "# Reasoning Path:\nMiami Marlins -> sports.sports_team.league -> m.0crt79d -> sports.sports_league_participation.league -> National League\n# Answer:\nm.0crt79d", "# Reasoning Path:\nMiami Marlins -> sports.sports_team.league -> m.0crtdb6 -> sports.sports_league_participation.league -> National League East\n# Answer:\nm.0crtdb6", "# Reasoning Path:\nMiami Marlins -> sports.sports_team.previously_known_as -> Florida Marlins -> common.topic.notable_types -> Sports Team\n# Answer:\nFlorida Marlins", "# Reasoning Path:\nMiami Marlins -> sports.sports_team.previously_known_as -> Florida Marlins -> common.topic.notable_for -> g.1255pdp5m\n# Answer:\nFlorida Marlins", "# Reasoning Path:\nMiami Marlins -> sports.professional_sports_team.draft_picks -> m.010q_p9h -> sports.sports_league_draft_pick.player -> Tyler Kolek\n# Answer:\nm.010q_p9h", "# Reasoning Path:\nMiami Marlins -> sports.professional_sports_team.draft_picks -> m.010q_p9h -> sports.sports_league_draft_pick.draft -> 2014 Major League Baseball season\n# Answer:\nm.010q_p9h", "# Reasoning Path:\nMiami Marlins -> sports.professional_sports_team.draft_picks -> m.04vy3tq -> sports.sports_league_draft_pick.player -> Charles Johnson\n# Answer:\nm.04vy3tq", "# Reasoning Path:\nMiami Marlins -> sports.professional_sports_team.draft_picks -> m.04vy3tq -> sports.sports_league_draft_pick.school -> University of Miami\n# Answer:\nm.04vy3tq", "# Reasoning Path:\nMiami Marlins -> sports.professional_sports_team.draft_picks -> m.04vy3tq -> sports.sports_league_draft_pick.draft -> 1992 Major League Baseball Draft\n# Answer:\nm.04vy3tq"], "ground_truth": ["1994 Major League Baseball Season"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-232", "prediction": ["# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.location.containedby -> Africa\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.country.form_of_government -> Presidential system\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Gibraltar -> location.administrative_division.country -> United Kingdom\n# Answer:\nGibraltar", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Andorra -> location.location.containedby -> Eurasia\n# Answer:\nAndorra", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Gibraltar -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nGibraltar", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.country.form_of_government -> Republic\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.country.official_language -> French\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Gibraltar -> location.country.form_of_government -> Parliamentary system\n# Answer:\nGibraltar", "# Reasoning Path:\nSpanish Language -> base.rosetta.languoid.local_name -> Spanish -> base.rosetta.local_name.locale -> Andorra\n# Answer:\nSpanish", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.country.form_of_government -> Unitary state\n# Answer:\nEquatorial Guinea"], "ground_truth": ["Uruguay", "Ecuador", "Venezuela", "Honduras", "Canada", "United States of America", "Barbados", "Cura\u00e7ao", "Panama", "Argentina", "Chile", "Equatorial Guinea", "Paraguay", "Guatemala", "Belize", "Gibraltar", "Nicaragua", "Mexico", "Guyana", "Costa Rica", "Andorra", "Northern Mariana Islands", "Kingdom of Aragon", "Peru", "Spain", "Western Sahara", "Puerto Rico", "Colombia", "Bolivia", "Saint Lucia", "Vatican City", "Dominican Republic", "El Salvador", "Cuba"], "ans_acc": 0.08823529411764706, "ans_hit": 1, "ans_f1": 0.16071428571428573, "ans_precission": 0.9, "ans_recall": 0.08823529411764706, "path_f1": 0.16071428571428573, "path_precision": 0.9, "path_recall": 0.08823529411764706, "path_ans_f1": 0.1621621621621622, "path_ans_precision": 1.0, "path_ans_recall": 0.08823529411764706}
{"id": "WebQTest-233", "prediction": ["# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Grand Canyon -> travel.tourist_attraction.near_travel_destination -> Grand Canyon National Park\n# Answer:\nGrand Canyon", "# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Mesa Arts Center -> travel.tourist_attraction.near_travel_destination -> Mesa\n# Answer:\nMesa Arts Center", "# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Grand Canyon -> travel.tourist_attraction.near_travel_destination -> Lake Powell\n# Answer:\nGrand Canyon", "# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Grand Canyon -> location.location.geolocation -> m.02_ltg5\n# Answer:\nGrand Canyon", "# Reasoning Path:\nPhoenix -> education.school_mascot.school -> Fremont High School\n# Answer:\nFremont High School", "# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Grand Canyon -> location.location.contains -> Buckey O'Neill Cabin\n# Answer:\nGrand Canyon", "# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Castle Hot Springs -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.0_ly_wt\n# Answer:\nCastle Hot Springs", "# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Mesa Arts Center -> location.location.containedby -> Mesa\n# Answer:\nMesa Arts Center", "# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Castle Hot Springs -> location.location.containedby -> Arizona\n# Answer:\nCastle Hot Springs", "# Reasoning Path:\nPhoenix -> location.citytown.postal_codes -> 85001 -> location.postal_code.country -> United States of America\n# Answer:\n85001"], "ground_truth": ["Musical Instrument Museum", "Mesa Arts Center", "Phoenix Mountain Preserve", "Pueblo Grande Ruin", "Desert Botanical Garden", "Grand Canyon", "Phoenix Art Museum", "Heard Museum", "Castle Hot Springs", "Ro Ho En", "Arizona Science Center"], "ans_acc": 0.2727272727272727, "ans_hit": 1, "ans_f1": 0.4067796610169491, "ans_precission": 0.8, "ans_recall": 0.2727272727272727, "path_f1": 0.4067796610169491, "path_precision": 0.8, "path_recall": 0.2727272727272727, "path_ans_f1": 0.4067796610169491, "path_ans_precision": 0.8, "path_ans_recall": 0.2727272727272727}
{"id": "WebQTest-234", "prediction": ["# Reasoning Path:\nPanama Canal -> location.location.containedby -> Panama -> location.location.containedby -> North America\n# Answer:\nPanama", "# Reasoning Path:\nPanama Canal -> location.location.containedby -> Panama -> base.locations.countries.continent -> North America\n# Answer:\nPanama", "# Reasoning Path:\nPanama Canal -> location.location.containedby -> Panama -> location.location.containedby -> Americas\n# Answer:\nPanama", "# Reasoning Path:\nPanama Canal -> base.engineering.canal.major_cities -> Col\u00f3n -> location.location.containedby -> Panama\n# Answer:\nCol\u00f3n", "# Reasoning Path:\nPanama Canal -> location.location.containedby -> Panama -> location.location.containedby -> Central America\n# Answer:\nPanama", "# Reasoning Path:\nPanama Canal -> location.location.containedby -> Panama -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nPanama", "# Reasoning Path:\nPanama Canal -> location.location.containedby -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nPanama Canal -> location.location.containedby -> North America -> location.location.containedby -> Northern Hemisphere\n# Answer:\nNorth America", "# Reasoning Path:\nPanama Canal -> location.location.containedby -> North America -> location.location.containedby -> Western Hemisphere\n# Answer:\nNorth America", "# Reasoning Path:\nPanama Canal -> location.location.containedby -> North America -> base.locations.continents.countries_within -> Panama\n# Answer:\nNorth America"], "ground_truth": ["Panama"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-235", "prediction": ["# Reasoning Path:\nCharlie Chaplin -> people.person.profession -> Comedian -> common.topic.notable_types -> Profession\n# Answer:\nComedian", "# Reasoning Path:\nCharlie Chaplin -> people.person.profession -> Composer -> common.topic.notable_types -> Profession\n# Answer:\nComposer", "# Reasoning Path:\nCharlie Chaplin -> people.person.profession -> Film Editor -> common.topic.notable_types -> Profession\n# Answer:\nFilm Editor", "# Reasoning Path:\nCharlie Chaplin -> people.person.profession -> Composer -> people.profession.specializations -> Film Score Composer\n# Answer:\nComposer", "# Reasoning Path:\nCharlie Chaplin -> film.editor.film -> Sunnyside -> common.topic.image -> SunnysideCC\n# Answer:\nSunnyside", "# Reasoning Path:\nCharlie Chaplin -> influence.influence_node.influenced -> Benny Hill -> people.person.profession -> Comedian\n# Answer:\nBenny Hill", "# Reasoning Path:\nCharlie Chaplin -> film.editor.film -> Sunnyside -> common.topic.article -> m.07ml_2\n# Answer:\nSunnyside", "# Reasoning Path:\nCharlie Chaplin -> film.editor.film -> A Busy Day -> film.film.produced_by -> Mack Sennett\n# Answer:\nA Busy Day", "# Reasoning Path:\nCharlie Chaplin -> influence.influence_node.influenced -> Benny Hill -> people.person.profession -> Screenwriter\n# Answer:\nBenny Hill", "# Reasoning Path:\nCharlie Chaplin -> influence.influence_node.influenced -> Blake Clark -> people.person.profession -> Comedian\n# Answer:\nBlake Clark"], "ground_truth": ["Actor", "Composer", "Screenwriter", "Film director", "Film Score Composer", "Film Editor", "Comedian", "Film Producer"], "ans_acc": 0.625, "ans_hit": 1, "ans_f1": 0.38709677419354843, "ans_precission": 0.4, "ans_recall": 0.375, "path_f1": 0.38709677419354843, "path_precision": 0.4, "path_recall": 0.375, "path_ans_f1": 0.6603773584905661, "path_ans_precision": 0.7, "path_ans_recall": 0.625}
{"id": "WebQTest-237", "prediction": ["# Reasoning Path:\nFred Durst -> people.person.profession -> Musician -> fictional_universe.character_occupation.characters_with_this_occupation -> George Harrison\n# Answer:\nMusician", "# Reasoning Path:\nFred Durst -> people.person.profession -> Musician -> freebase.equivalent_topic.equivalent_type -> Musical Artist\n# Answer:\nMusician", "# Reasoning Path:\nFred Durst -> people.person.profession -> Musician -> people.profession.specializations -> Bassist\n# Answer:\nMusician", "# Reasoning Path:\nFred Durst -> people.person.profession -> Guitarist -> common.topic.notable_types -> Profession\n# Answer:\nGuitarist", "# Reasoning Path:\nFred Durst -> people.person.profession -> Singer -> base.lightweight.profession.specialization_of -> Musicians and Singers\n# Answer:\nSinger", "# Reasoning Path:\nFred Durst -> people.person.profession -> Musician -> fictional_universe.character_occupation.characters_with_this_occupation -> John Lennon\n# Answer:\nMusician", "# Reasoning Path:\nFred Durst -> people.person.profession -> Guitarist -> freebase.type_hints.included_types -> Person\n# Answer:\nGuitarist", "# Reasoning Path:\nFred Durst -> people.person.profession -> Musician -> fictional_universe.character_occupation.characters_with_this_occupation -> Paul\n# Answer:\nMusician", "# Reasoning Path:\nFred Durst -> people.person.profession -> Singer -> common.topic.subject_of -> Alan Motley\n# Answer:\nSinger", "# Reasoning Path:\nFred Durst -> music.producer.tracks_produced -> A Flat -> music.recording.tracks -> g.11b81rr9s9\n# Answer:\nA Flat"], "ground_truth": ["Musician", "Songwriter", "Actor", "Guitarist", "Singer", "Film director", "Businessperson", "Rapper"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.5294117647058825, "ans_precission": 0.9, "ans_recall": 0.375, "path_f1": 0.5294117647058825, "path_precision": 0.9, "path_recall": 0.375, "path_ans_f1": 0.5294117647058825, "path_ans_precision": 0.9, "path_ans_recall": 0.375}
{"id": "WebQTest-239", "prediction": ["# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> base.locations.countries.states_provinces_within -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Alaska\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> location.country.first_level_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> base.locations.countries.states_provinces_within -> Alaska\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Arizona\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> base.locations.countries.states_provinces_within -> Arizona\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> location.country.first_level_divisions -> Alaska\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> base.animal_synopses.animal_synopsis.height -> m.0xn6r1z -> base.qualified_values.qualified_value.unit -> Centimeter\n# Answer:\nm.0xn6r1z", "# Reasoning Path:\nAmerican Bulldog -> common.topic.image -> Saylor's Doc Holliday -> common.image.size -> m.041fr7y\n# Answer:\nSaylor's Doc Holliday"], "ground_truth": ["United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> Maryland\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime. -> media_common.quotation.subjects -> Christians and Christianity\n# Answer:\nA strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime. -> common.topic.image -> Edgar Allan Poe signature\n# Answer:\nA strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong. -> common.topic.article -> m.05chc0k\n# Answer:\nA wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong.", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong. -> media_common.quotation.subjects -> Revenge\n# Answer:\nA wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong.", "# Reasoning Path:\nEdgar Allan Poe -> film.film_story_contributor.film_story_credits -> An Evening of Edgar Allan Poe -> media_common.adaptation.adapted_from -> The Cask of Amontillado\n# Answer:\nAn Evening of Edgar Allan Poe", "# Reasoning Path:\nEdgar Allan Poe -> film.film_story_contributor.film_story_credits -> Beczka amontillado -> film.film.genre -> Short Film\n# Answer:\nBeczka amontillado", "# Reasoning Path:\nEdgar Allan Poe -> film.film_story_contributor.film_story_credits -> Beczka amontillado -> film.film.release_date_s -> m.0v4yrb9\n# Answer:\nBeczka amontillado"], "ground_truth": ["Baltimore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-240", "prediction": ["# Reasoning Path:\nAlbert Einstein -> people.person.employment_history -> m.02kq3y4 -> business.employment_tenure.company -> University of Zurich\n# Answer:\nm.02kq3y4", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k05ps4 -> education.education.institution -> University of Zurich\n# Answer:\nm.0k05ps4", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0j_z4g5 -> education.education.institution -> Luitpold Gymnasium\n# Answer:\nm.0j_z4g5", "# Reasoning Path:\nAlbert Einstein -> people.person.employment_history -> m.02kq3y4 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.02kq3y4", "# Reasoning Path:\nAlbert Einstein -> people.person.employment_history -> m.02kq4ch -> business.employment_tenure.company -> Leiden University\n# Answer:\nm.02kq4ch", "# Reasoning Path:\nAlbert Einstein -> people.person.employment_history -> m.02kq3h6 -> business.employment_tenure.company -> Charles University in Prague\n# Answer:\nm.02kq3h6", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0j_z4g5 -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nm.0j_z4g5", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k03_k5 -> education.education.institution -> ETH Zurich\n# Answer:\nm.0k03_k5", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k05ps4 -> freebase.valuenotation.has_value -> Minor\n# Answer:\nm.0k05ps4", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k05ps4 -> freebase.valuenotation.is_reviewed -> Minor\n# Answer:\nm.0k05ps4"], "ground_truth": ["ETH Zurich", "Charles University in Prague", "Leiden University", "University of Zurich", "Institute for Advanced Study"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5263157894736842, "path_precision": 0.5, "path_recall": 0.5555555555555556, "path_ans_f1": 0.6153846153846154, "path_ans_precision": 0.5, "path_ans_recall": 0.8}
{"id": "WebQTest-241", "prediction": ["# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.02kk65p\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> Brian Griffin\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.02ntr0h\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> award.award_winner.awards_won -> m.0ly_pz4\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> common.topic.notable_for -> g.1255sxsvl\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.02s9hdf\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> film.film_story_contributor.film_story_credits -> Family Guy Presents: Stewie Griffin: The Untold Story\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> God\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.works_set_here -> Family Guy Video Game!\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> award.award_winner.awards_won -> m.0sgtl8p\n# Answer:\nSeth MacFarlane"], "ground_truth": ["Seth MacFarlane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-243", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> people.person.place_of_birth -> New Haven -> base.biblioness.bibs_location.state -> Connecticut\n# Answer:\nNew Haven", "# Reasoning Path:\nGeorge W. Bush -> people.person.place_of_birth -> New Haven -> location.location.containedby -> Connecticut\n# Answer:\nNew Haven", "# Reasoning Path:\nGeorge W. Bush -> people.person.place_of_birth -> New Haven -> location.location.containedby -> United States of America\n# Answer:\nNew Haven", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointed_role -> Director of the Bureau of Counterterrorism\n# Answer:\nm.010p95d2", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pk -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.010l29pk", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointee -> Henry A. Crumpton\n# Answer:\nm.010p95d2", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1 -> people.appointment.appointed_role -> United States Ambassador to Angola\n# Answer:\nm.079pxt1", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079q3lx -> people.appointment.appointed_role -> United States Ambassador to Brazil\n# Answer:\nm.079q3lx", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pv -> film.personal_film_appearance.film -> All About Ann: Governor Richards of the Lone Star State\n# Answer:\nm.010l29pv", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1 -> people.appointment.appointee -> Dan Mozena\n# Answer:\nm.079pxt1"], "ground_truth": ["New Haven"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-245", "prediction": ["# Reasoning Path:\nMiss America -> time.recurring_event.instances -> 1928 Miss America -> time.event.locations -> Atlantic City\n# Answer:\n1928 Miss America", "# Reasoning Path:\nMiss America -> time.recurring_event.instances -> 1928 Miss America -> common.topic.notable_for -> g.125bn860g\n# Answer:\n1928 Miss America", "# Reasoning Path:\nMiss America -> time.recurring_event.instances -> 1950 Miss America -> common.topic.notable_for -> g.125ft33p8\n# Answer:\n1950 Miss America", "# Reasoning Path:\nMiss America -> time.recurring_event.instances -> 1928 Miss America -> common.topic.notable_types -> Competition\n# Answer:\n1928 Miss America", "# Reasoning Path:\nMiss America -> time.recurring_event.instances -> 1950 Miss America -> common.topic.notable_types -> Competition\n# Answer:\n1950 Miss America", "# Reasoning Path:\nMiss America -> time.recurring_event.instances -> 1950 Miss America -> common.topic.article -> m.05h9j4r\n# Answer:\n1950 Miss America", "# Reasoning Path:\nMiss America -> time.recurring_event.instances -> 1929 Miss America -> common.topic.notable_types -> Competition\n# Answer:\n1929 Miss America", "# Reasoning Path:\nMiss America -> award.recurring_competition.individual_competitions -> 1964 Miss America -> common.topic.notable_for -> g.125g_pwks\n# Answer:\n1964 Miss America", "# Reasoning Path:\nMiss America -> award.recurring_competition.individual_competitions -> 1964 Miss America -> common.topic.article -> m.0jt4fzn\n# Answer:\n1964 Miss America", "# Reasoning Path:\nMiss America -> common.topic.webpage -> m.0704g7v -> common.webpage.resource -> Miss America site\n# Answer:\nm.0704g7v"], "ground_truth": ["1921-09"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-246", "prediction": ["# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Alternating current -> common.topic.notable_for -> g.125d8rdgm\n# Answer:\nAlternating current", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Alternating current -> common.topic.image -> City lights viewed in a motion blurred exposure. The AC blinking causes the lines to be dotted rather than continuous\n# Answer:\nAlternating current", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Alternating current -> common.topic.notable_types -> Invention\n# Answer:\nAlternating current", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Neon lamp -> base.schemastaging.context_name.pronunciation -> g.125_l59sd\n# Answer:\nNeon lamp", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Wireless telegraphy -> law.invention.inventor -> Heinrich Hertz\n# Answer:\nWireless telegraphy", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Wireless telegraphy -> common.topic.notable_types -> Invention\n# Answer:\nWireless telegraphy", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Wireless telegraphy -> common.topic.image -> Hertz schematic0\n# Answer:\nWireless telegraphy", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Neon lamp -> common.topic.notable_for -> g.1256fv8fb\n# Answer:\nNeon lamp", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Wireless telegraphy -> law.invention.inventor -> Jozef Murga\u0161\n# Answer:\nWireless telegraphy", "# Reasoning Path:\nNikola Tesla -> book.author.works_written -> 'Death Ray' for Planes -> book.written_work.original_language -> English Language\n# Answer:\n'Death Ray' for Planes"], "ground_truth": ["Tesla coil", "Induction motor", "Wireless telegraphy", "Three-phase electric power", "Neon lamp", "Alternating current", "Remote control"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5806451612903225, "ans_precission": 0.9, "ans_recall": 0.42857142857142855, "path_f1": 0.5806451612903225, "path_precision": 0.9, "path_recall": 0.42857142857142855, "path_ans_f1": 0.5806451612903225, "path_ans_precision": 0.9, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-247", "prediction": ["# Reasoning Path:\nFlemish people -> people.ethnicity.geographic_distribution -> South Africa -> location.country.official_language -> English Language\n# Answer:\nSouth Africa", "# Reasoning Path:\nFlemish people -> people.ethnicity.included_in_group -> Germanic peoples -> people.ethnicity.includes_groups -> Austrians\n# Answer:\nGermanic peoples", "# Reasoning Path:\nFlemish people -> people.ethnicity.included_in_group -> Germanic peoples -> people.ethnicity.geographic_distribution -> Northern Europe\n# Answer:\nGermanic peoples", "# Reasoning Path:\nFlemish people -> people.ethnicity.geographic_distribution -> South Africa -> organization.organization_founder.organizations_founded -> UNESCO\n# Answer:\nSouth Africa", "# Reasoning Path:\nFlemish people -> people.ethnicity.geographic_distribution -> Belgium -> media_common.netflix_genre.titles -> 22nd of May\n# Answer:\nBelgium", "# Reasoning Path:\nFlemish people -> people.ethnicity.included_in_group -> Germanic peoples -> people.ethnicity.includes_groups -> British people\n# Answer:\nGermanic peoples", "# Reasoning Path:\nFlemish people -> people.ethnicity.geographic_distribution -> South Africa -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics\n# Answer:\nSouth Africa", "# Reasoning Path:\nFlemish people -> people.ethnicity.geographic_distribution -> Australia -> olympics.olympic_participating_country.olympics_participated_in -> 1900 Summer Olympics\n# Answer:\nAustralia", "# Reasoning Path:\nFlemish people -> people.ethnicity.included_in_group -> Germanic peoples -> people.ethnicity.includes_groups -> Danes\n# Answer:\nGermanic peoples", "# Reasoning Path:\nFlemish people -> people.ethnicity.geographic_distribution -> South Africa -> organization.organization_founder.organizations_founded -> World Trade Organization\n# Answer:\nSouth Africa"], "ground_truth": ["Belgium", "Brazil", "Canada", "South Africa", "France", "United States of America", "Australia"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.6, "ans_recall": 0.42857142857142855, "path_f1": 0.5, "path_precision": 0.6, "path_recall": 0.42857142857142855, "path_ans_f1": 0.5, "path_ans_precision": 0.6, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-248", "prediction": ["# Reasoning Path:\nNew York City -> location.location.containedby -> New York -> location.location.containedby -> United States of America\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> location.location.containedby -> Area code 917 -> location.location.containedby -> New York\n# Answer:\nArea code 917", "# Reasoning Path:\nNew York City -> book.book_subject.works -> Graffiti Lives: Beyond the Tag in New York\u2019s Urban Underground -> book.written_work.subjects -> Graffiti\n# Answer:\nGraffiti Lives: Beyond the Tag in New York\u2019s Urban Underground", "# Reasoning Path:\nNew York City -> location.location.containedby -> New York -> government.governmental_jurisdiction.government_bodies -> United States Bankruptcy Court for the Southern District of New York\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> location.location.containedby -> New York -> travel.travel_destination.tourist_attractions -> Ellis Island\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> location.location.containedby -> New York -> travel.travel_destination.tourist_attractions -> American Museum of Natural History\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> visual_art.art_subject.artwork_on_the_subject -> End of 14th Street Crosstown Line -> visual_art.artwork.art_subject -> Labor unrest\n# Answer:\nEnd of 14th Street Crosstown Line", "# Reasoning Path:\nNew York City -> location.location.containedby -> Area code 917 -> location.location.geolocation -> m.0wmd4bw\n# Answer:\nArea code 917", "# Reasoning Path:\nNew York City -> book.book_subject.works -> Graffiti Lives: Beyond the Tag in New York\u2019s Urban Underground -> book.written_work.subjects -> Urban art\n# Answer:\nGraffiti Lives: Beyond the Tag in New York\u2019s Urban Underground", "# Reasoning Path:\nNew York City -> book.book_subject.works -> Graffiti Lives: Beyond the Tag in New York\u2019s Urban Underground -> book.book.genre -> Non-fiction\n# Answer:\nGraffiti Lives: Beyond the Tag in New York\u2019s Urban Underground"], "ground_truth": ["New York"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-249", "prediction": ["# Reasoning Path:\nBruce Lee -> people.deceased_person.cause_of_death -> Cerebral edema -> people.cause_of_death.people -> Fyodor Cherenkov\n# Answer:\nCerebral edema", "# Reasoning Path:\nBruce Lee -> people.deceased_person.cause_of_death -> Cerebral edema -> common.topic.article -> m.0219c4\n# Answer:\nCerebral edema", "# Reasoning Path:\nBruce Lee -> people.deceased_person.cause_of_death -> Cerebral edema -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nCerebral edema", "# Reasoning Path:\nBruce Lee -> people.deceased_person.cause_of_death -> Cerebral edema -> people.cause_of_death.people -> Alan Ladd\n# Answer:\nCerebral edema", "# Reasoning Path:\nBruce Lee -> freebase.valuenotation.is_reviewed -> Martial Art(s) -> type.property.expected_type -> Martial Art\n# Answer:\nMartial Art(s)", "# Reasoning Path:\nBruce Lee -> freebase.valuenotation.is_reviewed -> Martial Art(s) -> type.property.reverse_property -> Practitioner\n# Answer:\nMartial Art(s)", "# Reasoning Path:\nBruce Lee -> people.deceased_person.cause_of_death -> Cerebral edema -> people.cause_of_death.people -> David Sharp\n# Answer:\nCerebral edema", "# Reasoning Path:\nBruce Lee -> freebase.valuenotation.is_reviewed -> Martial Art(s) -> type.property.schema -> Martial Artist\n# Answer:\nMartial Art(s)", "# Reasoning Path:\nBruce Lee -> people.person.profession -> Martial Artist -> common.topic.article -> m.0dj2pg3\n# Answer:\nMartial Artist", "# Reasoning Path:\nBruce Lee -> freebase.valuenotation.is_reviewed -> Cause of death -> rdf-schema#range -> Cause Of Death\n# Answer:\nCause of death"], "ground_truth": ["Cerebral edema"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-250", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> American Samoa\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\ng.125_r6z32", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Germany\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Germany\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guam\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> periodicals.newspaper_circulation_area.newspapers -> Draugas -> book.newspaper.circulation_areas -> Australia\n# Answer:\nDraugas", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nm.010r7f0z", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Mexico\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Marshall Islands\n# Answer:\nPresidential system"], "ground_truth": ["Constitutional republic", "Presidential system", "Federal republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-251", "prediction": ["# Reasoning Path:\nEngland -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Kingdom of England\n# Answer:\nPound sterling", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0hq0kmy\n# Answer:\nm.0hq0kmy", "# Reasoning Path:\nEngland -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Kingdom of Great Britain\n# Answer:\nPound sterling", "# Reasoning Path:\nEngland -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Northern Ireland\n# Answer:\nPound sterling", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0mv3kv1\n# Answer:\nm.0mv3kv1", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0njwhvf\n# Answer:\nm.0njwhvf", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Airedale Terrier -> biology.animal_breed.place_of_origin -> United Kingdom\n# Answer:\nAiredale Terrier", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Airedale Terrier -> common.topic.notable_types -> Animal breed\n# Answer:\nAiredale Terrier", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Airedale Terrier -> base.petbreeds.dog_breed.temperament -> Alert\n# Answer:\nAiredale Terrier", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> American Pit Bull Terrier -> biology.animal_breed.coloring -> Fawn\n# Answer:\nAmerican Pit Bull Terrier"], "ground_truth": ["Pound sterling"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-252", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> education.education.institution -> Boston Latin School\n# Answer:\nm.040vjzw", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.is_reviewed -> Institution\n# Answer:\nm.040vjzw", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nm.040vjzw", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.has_no_value -> Major/Field Of Study\n# Answer:\nm.040vjzw", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nm.012zbkk5", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nm.040vjzw", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.is_reviewed -> End Date\n# Answer:\nm.040vjzw", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.is_reviewed -> Start Date\n# Answer:\nm.040vjzw", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> BenFranklinDuplessis -> common.image.size -> m.02fglp1\n# Answer:\nBenFranklinDuplessis", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> :Library and information science\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze"], "ground_truth": ["Boston Latin School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-253", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Scotland -> base.aareas.schema.administrative_area.administrative_children -> Falkirk\n# Answer:\nScotland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> base.aareas.schema.administrative_area.administrative_children -> Anglesey\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Scotland -> location.country.first_level_divisions -> Falkirk\n# Answer:\nScotland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> base.aareas.schema.administrative_area.administrative_children -> Blaenau Gwent\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> location.country.first_level_divisions -> Anglesey\n# Answer:\nWales"], "ground_truth": ["England", "Wales", "Scotland", "Northern Ireland"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.6, "path_precision": 0.5, "path_recall": 0.75, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-254", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> base.aareas.schema.administrative_area.administrative_area_type -> Sovereign state\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.main_country -> China -> location.location.containedby -> Asia\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Malaysia -> travel.travel_destination.tour_operators -> FOX Vakanties\n# Answer:\nMalaysia", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.location.contains -> Bandar Seri Begawan\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Canada -> base.locations.countries.continent -> North America\n# Answer:\nCanada", "# Reasoning Path:\nChinese language -> language.human_language.main_country -> China -> location.country.official_language -> Standard Chinese\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.location.contains -> Brunei International Airport\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.main_country -> China -> location.location.containedby -> East Asia\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Malaysia -> base.uncommon.topic.exceptions -> City/Town/Village should pertain to Ghana, since City/Town/Village subdivides Ghanaian Municipal District.\n# Answer:\nMalaysia"], "ground_truth": ["China", "Brunei", "Vietnam", "Sh\u011bn", "Canada", "Singapore", "Malaysia"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.7272727272727273, "ans_precission": 1.0, "ans_recall": 0.5714285714285714, "path_f1": 0.5316455696202531, "path_precision": 0.7, "path_recall": 0.42857142857142855, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 1.0, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-255", "prediction": ["# Reasoning Path:\nBarack Obama -> people.person.education -> m.0nbyhqc -> education.education.institution -> Noelani Elementary School\n# Answer:\nm.0nbyhqc", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgp6 -> education.education.institution -> Punahou School\n# Answer:\nm.02nqgp6", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> education.education.institution -> Occidental College\n# Answer:\nm.02nqgpk", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.0nbyhqc -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nm.0nbyhqc", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.subjects -> Mitt Romney presidential campaign, 2012\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.0nbyhqc -> freebase.valuenotation.has_no_value -> Major/Field Of Study\n# Answer:\nm.0nbyhqc", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.0nbyhqc -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nm.0nbyhqc", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgp6 -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nm.02nqgp6", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> freebase.valuenotation.has_value -> Degree\n# Answer:\nm.02nqgpk", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Writer's Journey to Selma, Alabama -> book.written_work.subjects -> Selma\n# Answer:\nA Writer's Journey to Selma, Alabama"], "ground_truth": ["State Elementary School Menteng 01", "Occidental College", "Columbia University", "St. Francis of Assisi Catholic School", "Punahou School", "Harvard Law School", "Noelani Elementary School"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3529411764705882, "path_precision": 0.3, "path_recall": 0.42857142857142855, "path_ans_f1": 0.3529411764705882, "path_ans_precision": 0.3, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-256", "prediction": ["# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> language.human_language.countries_spoken_in -> Macau\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> common.topic.notable_types -> Human Language\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> language.human_language.countries_spoken_in -> New Zealand\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Balikun horse -> common.topic.notable_types -> Organism Classification\n# Answer:\nBalikun horse", "# Reasoning Path:\nChina -> location.location.partially_contains -> Amur River -> location.location.partially_containedby -> Russia\n# Answer:\nAmur River", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Black Telescope -> biology.animal_breed.breed_of -> Goldfish\n# Answer:\nBlack Telescope", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Bone-mouth -> common.topic.notable_types -> Organism Classification\n# Answer:\nBone-mouth", "# Reasoning Path:\nChina -> location.location.partially_contains -> Mekong -> location.location.partially_containedby -> Laos\n# Answer:\nMekong", "# Reasoning Path:\nChina -> location.location.partially_contains -> Mekong -> geography.river.basin_countries -> Laos\n# Answer:\nMekong"], "ground_truth": ["Standard Chinese"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-257", "prediction": ["# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.office_holder -> Nikita Khrushchev\n# Answer:\nm.049x6zw", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_6 -> government.government_position_held.office_holder -> Vladimir Ivashko\n# Answer:\nm.049x6_6", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.governmental_body -> Central Committee of the Communist Party of the Soviet Union\n# Answer:\nm.049x6zw", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_commander -> Nikita Khrushchev\n# Answer:\nm.0vmx6nr", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_holder -> Joseph Stalin\n# Answer:\nm.049x6_k", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_conflict -> Cuban Missile Crisis\n# Answer:\nm.0vmx6nr", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf -> military.military_command.military_commander -> Joseph Stalin\n# Answer:\nm.02h7nmf", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_6 -> government.government_position_held.office_position_or_title -> General Secretary of the Communist Party of the Soviet Union\n# Answer:\nm.049x6_6", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_position_or_title -> General Secretary of the Communist Party of the Soviet Union\n# Answer:\nm.049x6_k", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.049y3kf -> military.military_command.military_commander -> Ivan Konev\n# Answer:\nm.049y3kf"], "ground_truth": ["Leonid Brezhnev", "Nikita Khrushchev"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.28571428571428575, "path_precision": 0.2, "path_recall": 0.5, "path_ans_f1": 0.28571428571428575, "path_ans_precision": 0.2, "path_ans_recall": 0.5}
{"id": "WebQTest-258", "prediction": ["# Reasoning Path:\nAustralia -> location.statistical_region.major_exports -> m.0cnqyp9 -> location.imports_exports_by_industry.industry -> Agriculture\n# Answer:\nm.0cnqyp9", "# Reasoning Path:\nAustralia -> location.statistical_region.major_exports -> m.0cnqyp5 -> location.imports_exports_by_industry.industry -> Energy industry\n# Answer:\nm.0cnqyp5", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\ng.11b60qdnlp", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\ng.11b60vfr15", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\ng.1245_2hrg", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_55mr\n# Answer:\ng.1245_55mr", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_2hqk\n# Answer:\ng.1245_2hqk", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_6vhn\n# Answer:\ng.1245_6vhn"], "ground_truth": ["Agriculture", "Energy industry"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Fernbank Science Center -> location.location.geolocation -> m.0cltjm2\n# Answer:\nFernbank Science Center", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Fernbank Science Center -> common.topic.notable_types -> Museum\n# Answer:\nFernbank Science Center", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Fernbank Museum of Natural History -> base.movietheatres.movie_theatre.type_of_movietheatre -> IMAX\n# Answer:\nFernbank Museum of Natural History", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall -> business.shopping_center.address -> g.11b7v_3l1h\n# Answer:\nArbor Place Mall", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Fulton County Airport -> aviation.airport.serves -> Residence Inn Atlanta Cumberland\n# Answer:\nFulton County Airport", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Fernbank Museum of Natural History -> location.location.containedby -> Georgia\n# Answer:\nFernbank Museum of Natural History", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall -> business.shopping_center.address -> m.042znwp\n# Answer:\nArbor Place Mall", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall -> business.shopping_center.store -> Mandarin Express, Arbor Place Mall, Douglasville, GA\n# Answer:\nArbor Place Mall", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.03ldb41 -> common.webpage.resource -> m.0blf53m\n# Answer:\nm.03ldb41", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall -> business.shopping_center.address -> m.0ccdpz0\n# Answer:\nArbor Place Mall"], "ground_truth": ["Omni Coliseum", "Cobb Energy Performing Arts Centre", "Georgia Dome", "Atlanta Symphony Orchestra", "Center for Puppetry Arts", "Georgia State Capitol", "World of Coca-Cola", "Six Flags Over Georgia", "Jimmy Carter Library and Museum", "Six Flags White Water", "Centennial Olympic Park", "Margaret Mitchell House & Museum", "Masquerade", "Georgia Aquarium", "Atlanta Ballet", "Woodruff Arts Center", "Zoo Atlanta", "Atlanta Cyclorama & Civil War Museum", "CNN Center", "Variety Playhouse", "Atlanta History Center", "Fernbank Museum of Natural History", "Fox Theatre", "Peachtree Road Race", "Fernbank Science Center", "Atlanta Jewish Film Festival", "Hyatt Regency Atlanta", "Georgia World Congress Center", "Underground Atlanta", "Martin Luther King, Jr. National Historic Site", "Arbor Place Mall", "Turner Field", "Philips Arena", "The Tabernacle", "Atlanta Marriott Marquis", "Four Seasons Hotel Atlanta"], "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0.1509433962264151, "ans_precission": 0.8, "ans_recall": 0.08333333333333333, "path_f1": 0.1509433962264151, "path_precision": 0.8, "path_recall": 0.08333333333333333, "path_ans_f1": 0.1509433962264151, "path_ans_precision": 0.8, "path_ans_recall": 0.08333333333333333}
{"id": "WebQTest-260", "prediction": ["# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> location.location.containedby -> Mandatory Palestine\n# Answer:\nJerusalem", "# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> location.location.containedby -> Jerusalem District\n# Answer:\nJerusalem", "# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> location.location.events -> Sack of Jerusalem\n# Answer:\nJerusalem", "# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> location.location.containedby -> Dominion of Melchizedek\n# Answer:\nJerusalem", "# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> book.book_subject.works -> For the temple\n# Answer:\nJerusalem", "# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> location.location.events -> 1929 Palestine riots\n# Answer:\nJerusalem", "# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> location.location.events -> 1947 Jerusalem riots\n# Answer:\nJerusalem", "# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> book.book_subject.works -> Asher and the capmakers\n# Answer:\nJerusalem", "# Reasoning Path:\nCity of David -> common.topic.image -> JPF-Ophel - City of David -> common.image.size -> m.025dmvs\n# Answer:\nJPF-Ophel - City of David", "# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> book.book_subject.works -> Crusader Gold\n# Answer:\nJerusalem"], "ground_truth": ["Jerusalem"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-261", "prediction": ["# Reasoning Path:\nCindy Sherman -> people.person.profession -> Artist -> fictional_universe.character_occupation.characters_with_this_occupation -> Andy Warhol\n# Answer:\nArtist", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Artist -> organization.role.governors -> m.0112k4dj\n# Answer:\nArtist", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Artist -> fictional_universe.character_occupation.characters_with_this_occupation -> Byron Montgomery\n# Answer:\nArtist", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Artist -> fictional_universe.character_occupation.characters_with_this_occupation -> Cally Harper Ewing\n# Answer:\nArtist", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Artist -> people.profession.specializations -> Visual Artist\n# Answer:\nArtist", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Photographer -> base.descriptive_names.names.descriptive_name -> m.0100_vbb\n# Answer:\nPhotographer", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Film director -> freebase.type_profile.published -> Published\n# Answer:\nFilm director", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Artist -> people.profession.specializations -> Photographer\n# Answer:\nArtist", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Photographer -> common.topic.image -> FotografosByMichellZappa\n# Answer:\nPhotographer", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Photographer -> base.descriptive_names.names.descriptive_name -> m.0100_vdz\n# Answer:\nPhotographer"], "ground_truth": ["Visual Artist", "Artist", "Photographer", "Film director"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.8571428571428571, "path_precision": 1.0, "path_recall": 0.75, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-262", "prediction": ["# Reasoning Path:\nShawnee Smith -> tv.tv_actor.starring_roles -> m.0j9dzw6 -> tv.regular_tv_appearance.series -> The Stand\n# Answer:\nm.0j9dzw6", "# Reasoning Path:\nShawnee Smith -> tv.tv_actor.starring_roles -> m.05cmxdm -> tv.regular_tv_appearance.series -> Scream Queens\n# Answer:\nm.05cmxdm", "# Reasoning Path:\nShawnee Smith -> film.actor.film -> m.098d405 -> film.performance.film -> The Stand\n# Answer:\nm.098d405", "# Reasoning Path:\nShawnee Smith -> tv.tv_actor.starring_roles -> m.03jq10c -> tv.regular_tv_appearance.series -> Becker\n# Answer:\nm.03jq10c", "# Reasoning Path:\nShawnee Smith -> film.actor.film -> m.098d405 -> film.performance.character -> Julie Lawry\n# Answer:\nm.098d405", "# Reasoning Path:\nShawnee Smith -> film.actor.film -> m.02vd7w0 -> film.performance.film -> Saw II\n# Answer:\nm.02vd7w0", "# Reasoning Path:\nShawnee Smith -> film.actor.film -> m.02vc307 -> film.performance.film -> The Blob\n# Answer:\nm.02vc307", "# Reasoning Path:\nShawnee Smith -> film.actor.film -> m.02vc307 -> film.performance.character -> Meg Penny\n# Answer:\nm.02vc307", "# Reasoning Path:\nShawnee Smith -> film.actor.film -> m.02vd7w0 -> film.performance.character -> Amanda Young\n# Answer:\nm.02vd7w0", "# Reasoning Path:\nShawnee Smith -> common.topic.webpage -> m.0bnt7_g -> common.webpage.in_index -> Blissful Master Index\n# Answer:\nm.0bnt7_g"], "ground_truth": ["Scream Queens", "All is Forgiven", "The Stand", "Brand New Life", "Arsenio", "30 Days of Night: Dust to Dust", "The Tom Show", "Anger Management", "Becker"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4000000000000001, "path_precision": 0.4, "path_recall": 0.4, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.4, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-264", "prediction": ["# Reasoning Path:\nAustralia -> location.country.currency_used -> Australian dollar -> finance.currency.countries_used -> Christmas Island\n# Answer:\nAustralian dollar", "# Reasoning Path:\nAustralia -> location.country.currency_used -> Australian dollar -> common.topic.notable_types -> Currency\n# Answer:\nAustralian dollar", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\ng.11b60qdnlp", "# Reasoning Path:\nAustralia -> location.country.currency_used -> Australian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands\n# Answer:\nAustralian dollar", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\ng.11b60vfr15", "# Reasoning Path:\nAustralia -> location.country.currency_used -> Australian dollar -> finance.currency.countries_used -> Kiribati\n# Answer:\nAustralian dollar", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\ng.1245_2hrg", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_55mr\n# Answer:\ng.1245_55mr", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_2hqk\n# Answer:\ng.1245_2hqk", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_6vhn\n# Answer:\ng.1245_6vhn"], "ground_truth": ["Australian dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-265", "prediction": ["# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> common.topic.article -> m.033_5b\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Leonardo da Vinci\u2013Fiumicino Airport -> location.location.geolocation -> m.02_j8sl\n# Answer:\nLeonardo da Vinci\u2013Fiumicino Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> aviation.airport.serves -> Ciampino\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> aviation.airport.hub_for -> Aerolinee Itavia\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> aviation.airport.hub_for -> Mistral Air\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> travel.travel_destination.how_to_get_here -> m.052lyyd -> travel.transportation.mode_of_transportation -> Train\n# Answer:\nm.052lyyd", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> aviation.airport.hub_for -> Ryanair\n# Answer:\nRyanair", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Leonardo da Vinci\u2013Fiumicino Airport -> aviation.airport.hub_for -> Aermediterranea\n# Answer:\nLeonardo da Vinci\u2013Fiumicino Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Leonardo da Vinci\u2013Fiumicino Airport -> aviation.airport.focus_city_for -> Air One\n# Answer:\nLeonardo da Vinci\u2013Fiumicino Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Italian Met Office Airport -> location.location.geolocation -> m.04fngkg\n# Answer:\nItalian Met Office Airport"], "ground_truth": ["Leonardo da Vinci\u2013Fiumicino Airport", "Ciampino\u2013G. B. Pastine International Airport"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-266", "prediction": ["# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Boulevard Brewing Company -> location.location.geolocation -> m.0_8ftr0\n# Answer:\nBoulevard Brewing Company", "# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Boulevard Brewing Company -> food.brewery_brand_of_beer.beers_produced -> Boulevard Pale Ale\n# Answer:\nBoulevard Brewing Company", "# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Boulevard Brewing Company -> organization.organization.headquarters -> m.045mlhj\n# Answer:\nBoulevard Brewing Company", "# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Liberty Memorial -> location.location.containedby -> Missouri\n# Answer:\nLiberty Memorial", "# Reasoning Path:\nKansas City -> location.statistical_region.population -> g.11b66slpck\n# Answer:\ng.11b66slpck", "# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Liberty Memorial -> base.usnris.nris_listing.significance_level -> National\n# Answer:\nLiberty Memorial", "# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Worlds of Fun -> location.location.time_zones -> Central Time Zone\n# Answer:\nWorlds of Fun", "# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Worlds of Fun -> amusement_parks.park.rides -> Boomerang\n# Answer:\nWorlds of Fun", "# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Worlds of Fun -> location.location.geolocation -> m.04ksmhv\n# Answer:\nWorlds of Fun", "# Reasoning Path:\nKansas City -> location.statistical_region.population -> g.11b7tm2tlb\n# Answer:\ng.11b7tm2tlb"], "ground_truth": ["Oceans of Fun", "Ewing and Muriel Kauffman Memorial Garden", "Negro Leagues Baseball Museum", "Sprint Center", "Kansas City Zoo", "Liberty Memorial", "Hotel Phillips", "Harry S. Truman Presidential Library and Museum", "Boulevard Brewing Company", "18th and Vine-Downtown East", "Crown Center", "Deer Creek Golf Course", "Kauffman Center for the Performing Arts", "Kemper Museum of Contemporary Art", "Worlds of Fun"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.32000000000000006, "ans_precission": 0.8, "ans_recall": 0.2, "path_f1": 0.32000000000000006, "path_precision": 0.8, "path_recall": 0.2, "path_ans_f1": 0.32000000000000006, "path_ans_precision": 0.8, "path_ans_recall": 0.2}
{"id": "WebQTest-267", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.person.place_of_birth -> Atlanta -> travel.travel_destination.tourist_attractions -> Martin Luther King, Jr. National Historic Site\n# Answer:\nAtlanta", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.place_of_birth -> Atlanta -> location.location.containedby -> Georgia\n# Answer:\nAtlanta", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.place_of_birth -> Atlanta -> location.location.containedby -> United States of America\n# Answer:\nAtlanta", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nm.011j_4sh", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Montgomery\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> common.topic.article -> m.05nnh0\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.011j_4sh", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0_714v2 -> film.personal_film_appearance.film -> Alice Walker: Beauty in Truth\n# Answer:\nm.0_714v2", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0gbz10_ -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.0gbz10_", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Alabama\n# Answer:\nDexter Avenue Baptist Church"], "ground_truth": ["Atlanta"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-268", "prediction": ["# Reasoning Path:\nLeBron James -> sports.sports_award_winner.awards -> m.02kbccr -> sports.sports_award.season -> 2003\u201304 NBA season\n# Answer:\nm.02kbccr", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04qmsr4 -> basketball.basketball_player_stats.season -> 2003\u201304 NBA season\n# Answer:\nm.04qmsr4", "# Reasoning Path:\nLeBron James -> sports.sports_award_winner.awards -> m.04ynxp7 -> sports.sports_award.season -> 2007\u201308 NBA season\n# Answer:\nm.04ynxp7", "# Reasoning Path:\nLeBron James -> sports.sports_award_winner.awards -> m.02kbccr -> sports.sports_award.team -> Cleveland Cavaliers\n# Answer:\nm.02kbccr", "# Reasoning Path:\nLeBron James -> sports.sports_award_winner.awards -> m.04ynxph -> sports.sports_award.season -> 2005\u201306 NBA season\n# Answer:\nm.04ynxph", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04qmsr4 -> basketball.basketball_player_stats.team -> Cleveland Cavaliers\n# Answer:\nm.04qmsr4", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04qcjh2 -> basketball.basketball_player_stats.season -> 2005\u201306 NBA season\n# Answer:\nm.04qcjh2", "# Reasoning Path:\nLeBron James -> sports.sports_award_winner.awards -> m.04ynxp7 -> sports.sports_award.award -> NBA All-Star Game Most Valuable Player Award\n# Answer:\nm.04ynxp7", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04ql_x2 -> basketball.basketball_player_stats.season -> 2007\u201308 NBA season\n# Answer:\nm.04ql_x2", "# Reasoning Path:\nLeBron James -> award.award_nominee.award_nominations -> m.010_xz3j -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nm.010_xz3j"], "ground_truth": ["2003\u201304 NBA season"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-269", "prediction": ["# Reasoning Path:\nDelaware River -> geography.river.origin -> Mount Jefferson -> common.topic.article -> m.036r_d\n# Answer:\nMount Jefferson", "# Reasoning Path:\nDelaware River -> geography.river.origin -> West Branch Delaware River -> geography.river.origin -> Schoharie County\n# Answer:\nWest Branch Delaware River", "# Reasoning Path:\nDelaware River -> geography.river.origin -> Mount Jefferson -> location.location.geolocation -> m.02_f418\n# Answer:\nMount Jefferson", "# Reasoning Path:\nDelaware River -> geography.river.origin -> Mount Jefferson -> location.location.containedby -> Jefferson\n# Answer:\nMount Jefferson", "# Reasoning Path:\nDelaware River -> geography.river.origin -> West Branch Delaware River -> common.topic.notable_for -> g.1258hkxlv\n# Answer:\nWest Branch Delaware River", "# Reasoning Path:\nDelaware River -> common.topic.webpage -> m.09ytwnd -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09ytwnd", "# Reasoning Path:\nDelaware River -> location.location.people_born_here -> Netawatwees -> common.topic.notable_for -> g.125drclq7\n# Answer:\nNetawatwees", "# Reasoning Path:\nDelaware River -> geography.river.origin -> Mount Jefferson -> location.location.containedby -> New York\n# Answer:\nMount Jefferson", "# Reasoning Path:\nDelaware River -> geography.river.origin -> West Branch Delaware River -> common.topic.image -> Delaware headwaters map\n# Answer:\nWest Branch Delaware River", "# Reasoning Path:\nDelaware River -> location.location.people_born_here -> Netawatwees -> common.topic.notable_types -> Deceased Person\n# Answer:\nNetawatwees"], "ground_truth": ["West Branch Delaware River", "Mount Jefferson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-270", "prediction": ["# Reasoning Path:\nJapan -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Australia\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\ng.1245_2h9t", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Canada\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.11b71r82sc\n# Answer:\ng.11b71r82sc", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> New Zealand\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Italy\n# Answer:\nUnitary state", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_4lv6\n# Answer:\ng.1245_4lv6", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> New Zealand\n# Answer:\nUnitary state"], "ground_truth": ["Constitutional monarchy", "Parliamentary system", "Unitary state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-271", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.parents -> Gaskell Romney\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> Lenore Romney -> people.person.parents -> Harold A. Lafount\n# Answer:\nLenore Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.parents -> Anna Amelia Romney\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.children -> G. Scott Romney\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> Lenore Romney -> people.person.place_of_birth -> Logan\n# Answer:\nLenore Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.profession -> Businessperson\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.children -> Jane Romney\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0_gvz4k -> film.personal_film_appearance.film -> Mitt\n# Answer:\nm.0_gvz4k", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.children -> Margo Lynn Romney\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.profession -> Politician\n# Answer:\nGeorge W. Romney"], "ground_truth": ["Detroit"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-273", "prediction": ["# Reasoning Path:\nMichael Jordan -> sports.drafted_athlete.drafted -> m.02h7cgv -> sports.sports_league_draft_pick.team -> Chicago Bulls\n# Answer:\nm.02h7cgv", "# Reasoning Path:\nMichael Jordan -> common.topic.image -> Jordan Lipofsky -> common.image.appears_in_topic_gallery -> 1984 NBA draft\n# Answer:\nJordan Lipofsky", "# Reasoning Path:\nMichael Jordan -> sports.drafted_athlete.drafted -> m.02h7cgv -> sports.sports_league_draft_pick.school -> University of North Carolina at Chapel Hill\n# Answer:\nm.02h7cgv", "# Reasoning Path:\nMichael Jordan -> common.topic.image -> Jordan Lipofsky -> common.image.size -> m.029lmfv\n# Answer:\nJordan Lipofsky", "# Reasoning Path:\nMichael Jordan -> common.topic.image -> Jordan Lipofsky -> common.image.appears_in_topic_gallery -> 2009\u201310 NBA season\n# Answer:\nJordan Lipofsky", "# Reasoning Path:\nMichael Jordan -> common.topic.image -> Jordan Lipofsky -> common.image.appears_in_topic_gallery -> The Shot\n# Answer:\nJordan Lipofsky", "# Reasoning Path:\nMichael Jordan -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Esophageal cancer\n# Answer:\nMale", "# Reasoning Path:\nMichael Jordan -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Pancreatic cancer\n# Answer:\nMale", "# Reasoning Path:\nMichael Jordan -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Prostate cancer\n# Answer:\nMale"], "ground_truth": ["1984"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-276", "prediction": ["# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> Marin County -> location.location.containedby -> California\n# Answer:\nMarin County", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> Marin County -> location.location.containedby -> North Bay\n# Answer:\nMarin County", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> 94901 -> location.location.containedby -> Marin County\n# Answer:\n94901", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> Marin County -> location.location.containedby -> San Francisco Bay Area\n# Answer:\nMarin County", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> Marin County -> base.localfood.food_producing_region.seasonal_availability -> m.04_jf_1\n# Answer:\nMarin County", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> Marin County -> location.statistical_region.population -> g.11b674q3py\n# Answer:\nMarin County", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> Marin County -> base.localfood.food_producing_region.seasonal_availability -> m.04_jf_p\n# Answer:\nMarin County", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> 94901 -> location.location.geometry -> m.057gz4z\n# Answer:\n94901", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> common.topic.notable_types -> Project focus -> freebase.type_profile.strict_included_types -> Inanimate\n# Answer:\nProject focus", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> common.topic.notable_types -> Project focus -> type.type.properties -> Projects\n# Answer:\nProject focus"], "ground_truth": ["Marin County", "94901"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-277", "prediction": ["# Reasoning Path:\nEsther -> people.person.spouse_s -> m.0130wv8p -> people.marriage.spouse -> Xerxes I\n# Answer:\nm.0130wv8p", "# Reasoning Path:\nEsther -> people.person.spouse_s -> m.0130wv8p -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0130wv8p", "# Reasoning Path:\nEsther -> people.person.spouse_s -> m.012zbnxz -> people.marriage.spouse -> Ahasuerus\n# Answer:\nm.012zbnxz", "# Reasoning Path:\nEsther -> people.person.religion -> Judaism -> common.topic.notable_types -> Religion\n# Answer:\nJudaism", "# Reasoning Path:\nEsther -> people.person.spouse_s -> m.012zbnxz -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.012zbnxz", "# Reasoning Path:\nEsther -> book.book_subject.works -> Book of Esther -> book.written_work.subjects -> Ahasuerus\n# Answer:\nBook of Esther", "# Reasoning Path:\nEsther -> book.book_subject.works -> Book of Esther -> media_common.adapted_work.adaptations -> Esther and the King\n# Answer:\nBook of Esther", "# Reasoning Path:\nEsther -> book.book_subject.works -> Daily Inspiration for Finding Favor With the King -> book.book.genre -> Inspirational\n# Answer:\nDaily Inspiration for Finding Favor With the King", "# Reasoning Path:\nEsther -> book.book_subject.works -> Book of Esther -> book.written_work.subjects -> Haman\n# Answer:\nBook of Esther", "# Reasoning Path:\nEsther -> people.person.religion -> Judaism -> religion.religion.includes -> Conservative Judaism\n# Answer:\nJudaism"], "ground_truth": ["Ahasuerus", "Xerxes I"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.26666666666666666, "path_precision": 0.2, "path_recall": 0.4, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-278", "prediction": ["# Reasoning Path:\nClaire Danes -> base.popstra.celebrity.dated -> m.063t679 -> base.popstra.dated.participant -> Matt Damon\n# Answer:\nm.063t679", "# Reasoning Path:\nClaire Danes -> base.popstra.celebrity.dated -> m.063t67_ -> base.popstra.dated.participant -> Billy Crudup\n# Answer:\nm.063t67_", "# Reasoning Path:\nClaire Danes -> base.popstra.celebrity.dated -> m.063t67n -> base.popstra.dated.participant -> Hugh Dancy\n# Answer:\nm.063t67n", "# Reasoning Path:\nClaire Danes -> celebrities.celebrity.sexual_relationships -> m.03yxkqm -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nm.03yxkqm", "# Reasoning Path:\nClaire Danes -> film.actor.film -> m.012w8_c9 -> film.performance.film -> Princess Mononoke\n# Answer:\nm.012w8_c9", "# Reasoning Path:\nClaire Danes -> film.actor.film -> m.0220p8c -> film.performance.character -> Daisy\n# Answer:\nm.0220p8c", "# Reasoning Path:\nClaire Danes -> celebrities.celebrity.sexual_relationships -> m.03yxkqm -> celebrities.romantic_relationship.celebrity -> Billy Crudup\n# Answer:\nm.03yxkqm", "# Reasoning Path:\nClaire Danes -> celebrities.celebrity.sexual_relationships -> m.03yxktc -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nm.03yxktc", "# Reasoning Path:\nClaire Danes -> celebrities.celebrity.sexual_relationships -> m.03yxktn -> celebrities.romantic_relationship.celebrity -> Ben Lee\n# Answer:\nm.03yxktn", "# Reasoning Path:\nClaire Danes -> film.actor.film -> m.0220p8c -> film.performance.film -> I Love You, I Love You Not\n# Answer:\nm.0220p8c"], "ground_truth": ["Hugh Dancy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.11764705882352941, "path_precision": 0.1, "path_recall": 0.14285714285714285, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-279", "prediction": ["# Reasoning Path:\nSt. Paul -> location.location.containedby -> Russell County -> location.location.containedby -> Virginia\n# Answer:\nRussell County", "# Reasoning Path:\nSt. Paul -> location.location.containedby -> Russell County -> location.location.containedby -> United States of America\n# Answer:\nRussell County", "# Reasoning Path:\nSt. Paul -> location.location.containedby -> Wise County -> location.location.containedby -> United States of America\n# Answer:\nWise County", "# Reasoning Path:\nSt. Paul -> location.location.containedby -> Russell County -> location.administrative_division.capital -> m.0jvvsmf\n# Answer:\nRussell County", "# Reasoning Path:\nSt. Paul -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nSt. Paul -> location.citytown.postal_codes -> 24283 -> location.location.containedby -> Wise County\n# Answer:\n24283", "# Reasoning Path:\nSt. Paul -> location.statistical_region.population -> g.11b66hhtgq\n# Answer:\ng.11b66hhtgq", "# Reasoning Path:\nSt. Paul -> location.location.containedby -> Russell County -> location.location.people_born_here -> Boyd C. Fugate\n# Answer:\nRussell County", "# Reasoning Path:\nSt. Paul -> location.location.containedby -> Wise County -> location.location.containedby -> Virginia\n# Answer:\nWise County", "# Reasoning Path:\nSt. Paul -> location.location.containedby -> Wise County -> location.statistical_region.rent50_3 -> m.05ggjk4\n# Answer:\nWise County"], "ground_truth": ["Wise County", "Russell County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nm.0cr320w", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Adrienne Pickering\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> common.topic.notable_types -> City/Town/Village\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Alan Marshal\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.office_position_or_title -> Member of the Queensland Legislative Assembly\n# Answer:\nm.0cr320w", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Alison Mosely\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.contains -> Assumption College, Warwick\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr301h -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nm.0cr301h", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.size -> m.05t654b\n# Answer:\nAnna Bligh crop", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.district_represented -> Electoral district of South Brisbane\n# Answer:\nm.0cr320w"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-281", "prediction": ["# Reasoning Path:\nJapan -> sports.sports_team_location.teams -> Bridgestone-Anchor -> sports.sports_team.sport -> Cycling\n# Answer:\nBridgestone-Anchor", "# Reasoning Path:\nJapan -> sports.sports_team_location.teams -> Burning -> common.topic.notable_for -> g.1yl5j5_sh\n# Answer:\nBurning", "# Reasoning Path:\nJapan -> sports.sports_team_location.teams -> Bridgestone-Anchor -> common.topic.notable_for -> g.1ypjsqnlh\n# Answer:\nBridgestone-Anchor", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\ng.1245_2h9t", "# Reasoning Path:\nJapan -> sports.sports_team_location.teams -> Burning -> sports.sports_team.sport -> Professional wrestling\n# Answer:\nBurning", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.11b71r82sc\n# Answer:\ng.11b71r82sc", "# Reasoning Path:\nJapan -> sports.sports_team_location.teams -> Burning -> common.topic.notable_types -> Sports Team\n# Answer:\nBurning", "# Reasoning Path:\nJapan -> sports.sports_team_location.teams -> Japan men's national volleyball team -> common.topic.notable_types -> Sports Team\n# Answer:\nJapan men's national volleyball team", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_4lv6\n# Answer:\ng.1245_4lv6", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_6fj1\n# Answer:\ng.1245_6fj1"], "ground_truth": ["Burning", "Japan national handball team", "Japan national football team", "Japan men's national volleyball team", "Japan national baseball team", "Japan women's national handball team", "Bridgestone-Anchor", "Japan women's national volleyball team"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.6, "ans_recall": 0.375, "path_f1": 0.4615384615384615, "path_precision": 0.6, "path_recall": 0.375, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.6, "path_ans_recall": 0.375}
{"id": "WebQTest-282", "prediction": ["# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Azerbaijan\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> Azerbaijan\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Mongolia\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.11b60vv5th\n# Answer:\ng.11b60vv5th", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> freebase.valuenotation.is_reviewed -> Start date\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nRussia -> location.statistical_region.co2_emissions_per_capita -> g.1245_8036\n# Answer:\ng.1245_8036", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Mongolia\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> Mongolia\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.12tb6gh0b\n# Answer:\ng.12tb6gh0b"], "ground_truth": ["China", "North Korea", "Estonia", "Georgia", "Latvia", "Poland", "Mongolia", "Ukraine", "Kazakhstan", "Lithuania", "Finland", "Azerbaijan", "Norway", "Belarus"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.0425531914893617, "path_precision": 0.5, "path_recall": 0.022222222222222223, "path_ans_f1": 0.22222222222222224, "path_ans_precision": 0.5, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-284", "prediction": ["# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Indo-European languages -> language.language_family.sub_families -> Goidelic languages\n# Answer:\nIndo-European languages", "# Reasoning Path:\nLatin Language -> base.rosetta.languoid.parent -> Latino-Faliscan Group -> base.rosetta.languoid.parent -> Italic Group\n# Answer:\nLatino-Faliscan Group", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Indo-European languages -> common.topic.notable_types -> Language Family\n# Answer:\nIndo-European languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Indo-European languages -> language.language_family.sub_families -> Balto-Slavic languages\n# Answer:\nIndo-European languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Indo-European languages -> education.field_of_study.students_majoring -> m.05yz7m9\n# Answer:\nIndo-European languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> language.language_family.languages -> Aequian language\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> common.topic.image -> Iron Age Italy\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Indo-European languages -> language.language_family.sub_families -> Bihari languages\n# Answer:\nIndo-European languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> common.topic.notable_for -> g.125d12rwn\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Indo-European languages -> education.field_of_study.students_majoring -> m.0cplf2f\n# Answer:\nIndo-European languages"], "ground_truth": ["Indo-European languages", "Italic languages"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-286", "prediction": ["# Reasoning Path:\nKarl Benz -> people.person.education -> m.04hddz6 -> education.education.institution -> Karlsruhe Institute of Technology\n# Answer:\nm.04hddz6", "# Reasoning Path:\nKarl Benz -> people.person.education -> m.0n0nxj5 -> education.education.institution -> Karlsruhe Institute of Technology\n# Answer:\nm.0n0nxj5", "# Reasoning Path:\nKarl Benz -> people.person.education -> m.04hddz6 -> education.education.degree -> Bachelor of Engineering\n# Answer:\nm.04hddz6", "# Reasoning Path:\nKarl Benz -> people.person.education -> m.04hddz6 -> education.education.major_field_of_study -> Mechanical Engineering\n# Answer:\nm.04hddz6", "# Reasoning Path:\nKarl Benz -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#range -> Person\n# Answer:\nChildren", "# Reasoning Path:\nKarl Benz -> people.person.children -> Clara Benz -> common.topic.notable_types -> Deceased Person\n# Answer:\nClara Benz", "# Reasoning Path:\nKarl Benz -> people.person.children -> Ellen Benz -> common.topic.notable_for -> g.12h3244gv\n# Answer:\nEllen Benz", "# Reasoning Path:\nKarl Benz -> people.person.children -> Clara Benz -> common.topic.notable_for -> g.12h3475jj\n# Answer:\nClara Benz", "# Reasoning Path:\nKarl Benz -> people.person.children -> Eugen Benz -> common.topic.notable_types -> Person\n# Answer:\nEugen Benz", "# Reasoning Path:\nKarl Benz -> freebase.valuenotation.is_reviewed -> Children -> type.property.schema -> Person\n# Answer:\nChildren"], "ground_truth": ["Karlsruhe Institute of Technology"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-287", "prediction": ["# Reasoning Path:\nCarmen Electra -> film.actor.film -> m.0115j6sp -> film.performance.film -> Dragula\n# Answer:\nm.0115j6sp", "# Reasoning Path:\nCarmen Electra -> film.actor.film -> m.012zh0pk -> film.performance.film -> Chocolate City\n# Answer:\nm.012zh0pk", "# Reasoning Path:\nCarmen Electra -> film.actor.film -> m.02tb13p -> film.performance.film -> Full of It\n# Answer:\nm.02tb13p", "# Reasoning Path:\nCarmen Electra -> film.actor.film -> m.0115j6sp -> film.performance.character -> Jaime\n# Answer:\nm.0115j6sp", "# Reasoning Path:\nCarmen Electra -> film.actor.film -> m.02tb13p -> film.performance.special_performance_type -> Him/Herself\n# Answer:\nm.02tb13p", "# Reasoning Path:\nCarmen Electra -> tv.tv_actor.guest_roles -> m.03lkq75 -> tv.tv_guest_role.episodes_appeared_in -> Mr. Monk and the Panic Room\n# Answer:\nm.03lkq75", "# Reasoning Path:\nCarmen Electra -> base.popstra.celebrity.infidelity_victim -> m.06465lm -> base.popstra.infidelity.victim -> Dave Navarro\n# Answer:\nm.06465lm", "# Reasoning Path:\nCarmen Electra -> tv.tv_actor.guest_roles -> m.040q72p -> tv.tv_guest_role.episodes_appeared_in -> Threat Levels\n# Answer:\nm.040q72p", "# Reasoning Path:\nCarmen Electra -> tv.tv_actor.guest_roles -> m.040q68x -> tv.tv_guest_role.episodes_appeared_in -> Pilot\n# Answer:\nm.040q68x"], "ground_truth": ["Scary Movie", "Searching for Bobby D", "Date Movie", "My Boss's Daughter", "Disaster Movie", "Getting Played", "Mardi Gras: Spring Break", "Epic Movie", "The Mating Habits of the Earthbound Human", "The Back Nine", "I Want Candy", "Scary Movie 4", "Carmen Electra's Fit to Strip", "Starstruck", "Bedtime Stories", "Get Over It", "BThere Dvd Magazine", "2-Headed Shark Attack", "30 Days Until I'm Famous", "We Are Family", "Dragula", "Book of Fire", "Max Havoc: Curse of the Dragon", "Dirty Love", "American Vampire", "Lap Dance", "Perfume", "Christmas in Wonderland", "Barry Munday", "Lolo's Cafe", "Oy Vey! My Son Is Gay!!", "Cheaper by the Dozen 2", "The Chosen One: Legend of the Raven", "Good Burger", "Naked Movie", "Rent Control", "Full of It", "Meet the Spartans", "Mr. 3000", "Sol Goode", "Uptown Girls", "Starsky & Hutch", "National Lampoon's Pledge This!", "The Axe Boat", "Hot Tamale", "American Dreamz", "Lil' Pimp", "Chocolate City", "Whacked!", "Monster Island", "Baywatch: Hawaiian Wedding"], "ans_acc": 0.058823529411764705, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.07894736842105263, "path_precision": 0.3333333333333333, "path_recall": 0.04477611940298507, "path_ans_f1": 0.1, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.058823529411764705}
{"id": "WebQTest-289", "prediction": ["# Reasoning Path:\nWorld War II -> time.event.locations -> Asia -> base.locations.continents.countries_within -> Syria\n# Answer:\nAsia", "# Reasoning Path:\nWorld War II -> time.event.locations -> Asia -> base.locations.continents.countries_within -> Yemen\n# Answer:\nAsia", "# Reasoning Path:\nWorld War II -> time.event.locations -> Asia -> location.location.partially_contains -> Soviet Union\n# Answer:\nAsia", "# Reasoning Path:\nWorld War II -> time.event.locations -> Asia -> base.locations.continents.countries_within -> Brunei\n# Answer:\nAsia", "# Reasoning Path:\nWorld War II -> time.event.locations -> Asia -> location.location.events -> World War I\n# Answer:\nAsia", "# Reasoning Path:\nWorld War II -> time.event.locations -> Africa -> location.location.events -> World War I\n# Answer:\nAfrica", "# Reasoning Path:\nWorld War II -> time.event.locations -> Asia -> location.location.partially_contains -> Armenia\n# Answer:\nAsia", "# Reasoning Path:\nWorld War II -> time.event.locations -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica", "# Reasoning Path:\nWorld War II -> base.skosbase.vocabulary_equivalent_topic.narrower_concept -> World War, 1939-1945--Hostages -> base.skosbase.skos_concept.in_scheme -> Authorities Concept Scheme\n# Answer:\nWorld War, 1939-1945--Hostages", "# Reasoning Path:\nWorld War II -> base.skosbase.vocabulary_equivalent_topic.narrower_concept -> World War, 1939-1945--Hostages -> base.skosbase.skos_concept.broader_topic -> Hostage\n# Answer:\nWorld War, 1939-1945--Hostages"], "ground_truth": ["1945-09-02"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-291", "prediction": ["# Reasoning Path:\nMorgan Freeman -> people.person.education -> m.0n10623 -> education.education.institution -> Los Angeles City College\n# Answer:\nm.0n10623", "# Reasoning Path:\nMorgan Freeman -> people.person.education -> m.010665q2 -> education.education.institution -> Pasadena Playhouse\n# Answer:\nm.010665q2", "# Reasoning Path:\nMorgan Freeman -> common.topic.webpage -> m.093_l_6 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.093_l_6", "# Reasoning Path:\nMorgan Freeman -> common.topic.webpage -> m.093zz6z -> common.webpage.resource -> Robert Iler pleads guilty to mugging charge\n# Answer:\nm.093zz6z", "# Reasoning Path:\nMorgan Freeman -> tv.tv_producer.tv_episodes_produced -> m.0gfnpbk -> tv.tv_producer_episode_credit.episode -> The 16th Man\n# Answer:\nm.0gfnpbk", "# Reasoning Path:\nMorgan Freeman -> tv.tv_producer.tv_episodes_produced -> m.0j75_zn -> tv.tv_producer_episode_credit.episode -> Is There a Sixth Sense?\n# Answer:\nm.0j75_zn", "# Reasoning Path:\nMorgan Freeman -> common.topic.webpage -> m.093z_bp -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.093z_bp", "# Reasoning Path:\nMorgan Freeman -> tv.tv_producer.tv_episodes_produced -> m.0gfnpbk -> tv.tv_producer_episode_credit.role -> g.120lqbyx\n# Answer:\nm.0gfnpbk", "# Reasoning Path:\nMorgan Freeman -> common.topic.webpage -> m.093z_bp -> common.webpage.resource -> MORGAN FREEMAN\n# Answer:\nm.093z_bp", "# Reasoning Path:\nMorgan Freeman -> common.topic.webpage -> m.093zz6z -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.093zz6z"], "ground_truth": ["Pasadena Playhouse", "Los Angeles City College"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-294", "prediction": ["# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Botanist -> base.descriptive_names.names.descriptive_name -> m.0102h0md\n# Answer:\nBotanist", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Botanist -> common.topic.notable_for -> g.12568993_\n# Answer:\nBotanist", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Botanist -> common.topic.notable_types -> Profession\n# Answer:\nBotanist", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Botanist -> base.descriptive_names.names.descriptive_name -> m.0106gmq_\n# Answer:\nBotanist", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Biologist -> people.profession.specializations -> Clinical Molecular Biologist\n# Answer:\nBiologist", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Biologist -> people.profession.specialization_of -> Scientist\n# Answer:\nBiologist", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Botanist -> base.descriptive_names.names.descriptive_name -> m.0106gmzp\n# Answer:\nBotanist", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Scientist -> common.topic.subject_of -> Autobiography\n# Answer:\nScientist", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Biologist -> people.profession.specializations -> Computational Biologist\n# Answer:\nBiologist", "# Reasoning Path:\nCarl Linnaeus -> base.kwebbase.kwtopic.connections_to -> asa gray replaced system of carolus linnaeus -> base.kwebbase.kwconnection.subject -> Asa Gray\n# Answer:\nasa gray replaced system of carolus linnaeus"], "ground_truth": ["Biologist", "Scientist", "Physician", "Botanist"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.8181818181818182, "path_precision": 0.9, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-295", "prediction": ["# Reasoning Path:\nGermany -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Austria\n# Answer:\nFederal republic", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\ng.11b60v9zy7", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Austria\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_38m_\n# Answer:\ng.1245_38m_", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fdl8\n# Answer:\ng.12tb6fdl8", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Czech Republic\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc38c_4\n# Answer:\ng.1hhc38c_4", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_67jr\n# Answer:\ng.1245_67jr", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Poland\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_jlz4\n# Answer:\ng.1245_jlz4"], "ground_truth": ["Representative democracy", "Constitutional republic", "Federal republic", "Parliamentary republic"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.4, "ans_recall": 0.5, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.4, "path_ans_recall": 0.5}
{"id": "WebQTest-296", "prediction": ["# Reasoning Path:\nLibya -> location.country.form_of_government -> Provisional government -> common.topic.article -> m.02tz44\n# Answer:\nProvisional government", "# Reasoning Path:\nLibya -> location.country.form_of_government -> Provisional government -> common.topic.notable_types -> Form of Government\n# Answer:\nProvisional government", "# Reasoning Path:\nLibya -> location.statistical_region.co2_emissions_per_capita -> g.1245_1hym\n# Answer:\ng.1245_1hym", "# Reasoning Path:\nLibya -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Egypt\n# Answer:\nProvisional government", "# Reasoning Path:\nLibya -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Italy\n# Answer:\nParliamentary republic", "# Reasoning Path:\nLibya -> location.country.form_of_government -> Parliamentary republic -> common.topic.notable_types -> Form of Government\n# Answer:\nParliamentary republic", "# Reasoning Path:\nLibya -> location.statistical_region.co2_emissions_per_capita -> g.1245_2gxq\n# Answer:\ng.1245_2gxq", "# Reasoning Path:\nLibya -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> State of Palestine\n# Answer:\nProvisional government", "# Reasoning Path:\nLibya -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Poland\n# Answer:\nParliamentary republic", "# Reasoning Path:\nLibya -> location.statistical_region.co2_emissions_per_capita -> g.1245_55yg\n# Answer:\ng.1245_55yg"], "ground_truth": ["Provisional government", "Parliamentary republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-297", "prediction": ["# Reasoning Path:\nAnaheim -> location.location.time_zones -> Pacific Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nPacific Time Zone", "# Reasoning Path:\nAnaheim -> location.location.time_zones -> Pacific Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nPacific Time Zone", "# Reasoning Path:\nAnaheim -> location.location.time_zones -> Pacific Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nPacific Time Zone", "# Reasoning Path:\nAnaheim -> location.statistical_region.population -> g.11b66d_m1v\n# Answer:\ng.11b66d_m1v", "# Reasoning Path:\nAnaheim -> location.statistical_region.population -> g.11bc88q060\n# Answer:\ng.11bc88q060", "# Reasoning Path:\nAnaheim -> location.statistical_region.population -> g.11bcdlmd72\n# Answer:\ng.11bcdlmd72", "# Reasoning Path:\nAnaheim -> location.citytown.postal_codes -> 92801 -> location.location.geolocation -> m.03dnfqk\n# Answer:\n92801", "# Reasoning Path:\nAnaheim -> location.citytown.postal_codes -> 92801 -> location.location.geometry -> m.0562cjy\n# Answer:\n92801", "# Reasoning Path:\nAnaheim -> location.citytown.postal_codes -> 92801 -> common.topic.notable_types -> Postal Code\n# Answer:\n92801", "# Reasoning Path:\nAnaheim -> location.citytown.postal_codes -> 92803 -> location.postal_code.country -> United States of America\n# Answer:\n92803"], "ground_truth": ["Pacific Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-298", "prediction": ["# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1970 World Series -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1970 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1970 World Series -> sports.sports_championship_event.runner_up -> Cincinnati Reds\n# Answer:\n1970 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1970 World Series -> sports.sports_championship_event.season -> 1970 Major League Baseball Season\n# Answer:\n1970 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1983 World Series -> sports.sports_championship_event.championship -> World Series\n# Answer:\n1983 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1966 World Series -> sports.sports_championship_event.season -> 1966 Major League Baseball Season\n# Answer:\n1966 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1966 World Series -> common.topic.notable_for -> g.125br8lv_\n# Answer:\n1966 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1983 World Series -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1983 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.venue -> m.0wz1z2b -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0wz1z2b", "# Reasoning Path:\nBaltimore Orioles -> baseball.baseball_team.team_stats -> m.05n60gg -> baseball.baseball_team_stats.season -> 1954 Major League Baseball season\n# Answer:\nm.05n60gg", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.venue -> m.0wz1z2b -> sports.team_venue_relationship.venue -> Oriole Park at Camden Yards\n# Answer:\nm.0wz1z2b"], "ground_truth": ["1970 World Series", "1983 World Series", "1966 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ken Barlow\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Albert Tatlock\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> people.person.gender -> Male\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Alf Roberts\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> film.actor.film -> m.0h0_mvx\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> common.topic.notable_for -> g.1255pdp4f\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> tv.tv_genre.programs -> 100% Senorita\n# Answer:\nChivalric romance", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> common.topic.notable_types -> Multipart TV episode\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> tv.tv_genre.programs -> Alakdana\n# Answer:\nChivalric romance", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Comedy -> common.topic.subject_of -> Albrecht Behmel\n# Answer:\nComedy"], "ground_truth": ["William Roache"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-300", "prediction": ["# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> common.topic.subject_of -> Autobiography\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Philosopher -> fictional_universe.character_rank.characters_of_this_rank -> Heraclitus\n# Answer:\nPhilosopher", "# Reasoning Path:\nFrancis Bacon -> book.author.works_written -> Bacon\n# Answer:\nBacon", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> tv.tv_subject.tv_programs -> The Secret Life of Scientists & Engineers\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Statesman -> common.topic.article -> m.016fc9\n# Answer:\nStatesman", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> common.topic.subject_of -> Biography\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> base.descriptive_names.names.descriptive_name -> m.01053kvm\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Philosopher -> common.topic.notable_types -> Profession\n# Answer:\nPhilosopher", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> common.topic.subject_of -> Book\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Philosopher -> people.profession.part_of_professional_field -> Philosophy\n# Answer:\nPhilosopher"], "ground_truth": ["Philosopher", "Statesman", "Author", "Scientist", "Spy"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.7200000000000001, "ans_precission": 0.9, "ans_recall": 0.6, "path_f1": 0.8181818181818182, "path_precision": 0.9, "path_recall": 0.75, "path_ans_f1": 0.888888888888889, "path_ans_precision": 1.0, "path_ans_recall": 0.8}
{"id": "WebQTest-301", "prediction": ["# Reasoning Path:\nAmelia Earhart -> fictional_universe.fictional_character.occupation -> Pilot -> base.firsts.achievement.firsts -> m.0g8qg3r\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> fictional_universe.fictional_character.occupation -> Pilot -> base.descriptive_names.names.descriptive_name -> m.0100tslg\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> fictional_universe.fictional_character.occupation -> Pilot -> people.profession.specializations -> Aerobatic Pilot\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> fictional_universe.fictional_character.occupation -> Pilot -> people.profession.specializations -> Aircrew\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> people.person.profession -> Pilot -> base.descriptive_names.names.descriptive_name -> m.0100tslg\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> fictional_universe.fictional_character.occupation -> Pilot -> base.descriptive_names.names.descriptive_name -> m.0100tthh\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> fictional_universe.fictional_character.occupation -> Pilot -> people.profession.specializations -> Airline Pilot\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> people.person.profession -> Pilot -> base.firsts.achievement.firsts -> m.0g8qg3r\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> fictional_universe.fictional_character.occupation -> Pilot -> base.descriptive_names.names.descriptive_name -> m.0102zls6\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> people.person.profession -> Pilot -> base.descriptive_names.names.descriptive_name -> m.0100tthh\n# Answer:\nPilot"], "ground_truth": ["Writer", "Pilot"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.5833333333333334, "path_precision": 0.7, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-302", "prediction": ["# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Blue -> book.book_subject.works -> Blue: The History of a Color\n# Answer:\nBlue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Blue -> common.topic.notable_for -> g.125gl7_n4\n# Answer:\nBlue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Blue -> base.schemastaging.visual_color_extra.hsv -> m.010nmgqr\n# Answer:\nBlue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Silver -> base.schemastaging.context_name.pronunciation -> g.125_rvx57\n# Answer:\nSilver", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Silver -> common.topic.notable_types -> Color\n# Answer:\nSilver", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Navy Blue -> base.schemastaging.visual_color_extra.srgb -> m.010q_4pn\n# Answer:\nNavy Blue", "# Reasoning Path:\nDallas Cowboys -> american_football.football_team.away_games -> Dallas Cowboys at Cleveland Browns, 2008-09-07 -> common.topic.notable_types -> NFL Game\n# Answer:\nDallas Cowboys at Cleveland Browns, 2008-09-07", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Silver -> common.topic.notable_for -> g.125g6j40s\n# Answer:\nSilver", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Navy Blue -> base.schemastaging.visual_color_extra.cmyk -> m.010q_4qh\n# Answer:\nNavy Blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Navy Blue -> common.topic.notable_types -> Color\n# Answer:\nNavy Blue"], "ground_truth": ["Navy Blue", "Blue", "White", "Royal blue", "Silver"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7200000000000001, "ans_precission": 0.9, "ans_recall": 0.6, "path_f1": 0.7200000000000001, "path_precision": 0.9, "path_recall": 0.6, "path_ans_f1": 0.7200000000000001, "path_ans_precision": 0.9, "path_ans_recall": 0.6}
{"id": "WebQTest-303", "prediction": ["# Reasoning Path:\nMargaret Hoover -> people.person.education -> m.0n1gxgt -> education.education.institution -> Davidson College\n# Answer:\nm.0n1gxgt", "# Reasoning Path:\nMargaret Hoover -> people.person.education -> g.11c3kps_b6\n# Answer:\ng.11c3kps_b6", "# Reasoning Path:\nMargaret Hoover -> common.topic.notable_for -> g.125dcyfk6\n# Answer:\ng.125dcyfk6", "# Reasoning Path:\nMargaret Hoover -> people.person.nationality -> United States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nUnited States of America", "# Reasoning Path:\nMargaret Hoover -> people.person.nationality -> United States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nUnited States of America", "# Reasoning Path:\nMargaret Hoover -> people.person.nationality -> United States of America -> periodicals.newspaper_circulation_area.newspapers -> Morgen Freiheit\n# Answer:\nUnited States of America", "# Reasoning Path:\nMargaret Hoover -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Pennsylvania\n# Answer:\nUnited States of America", "# Reasoning Path:\nMargaret Hoover -> people.person.nationality -> United States of America -> periodicals.newspaper_circulation_area.newspapers -> American Free Press\n# Answer:\nUnited States of America", "# Reasoning Path:\nMargaret Hoover -> people.person.nationality -> United States of America -> periodicals.newspaper_circulation_area.newspapers -> China Daily\n# Answer:\nUnited States of America", "# Reasoning Path:\nMargaret Hoover -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Alabama\n# Answer:\nUnited States of America"], "ground_truth": ["Davidson College", "Bryn Mawr College"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 0.1, "path_ans_recall": 0.5}
{"id": "WebQTest-304", "prediction": ["# Reasoning Path:\nJamie Oliver -> people.person.children -> Petal Blossom Rainbow Oliver -> people.person.parents -> Juliette Norton\n# Answer:\nPetal Blossom Rainbow Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.children -> Daisy Boo Pamela Oliver -> people.person.parents -> Juliette Norton\n# Answer:\nDaisy Boo Pamela Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.children -> Petal Blossom Rainbow Oliver -> people.person.nationality -> United Kingdom\n# Answer:\nPetal Blossom Rainbow Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.children -> Petal Blossom Rainbow Oliver -> people.person.sibling_s -> m.0tlq4hm\n# Answer:\nPetal Blossom Rainbow Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.children -> Daisy Boo Pamela Oliver -> people.person.place_of_birth -> United Kingdom\n# Answer:\nDaisy Boo Pamela Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.children -> Buddy Bear Maurice Oliver -> people.person.gender -> Male\n# Answer:\nBuddy Bear Maurice Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.children -> Daisy Boo Pamela Oliver -> common.topic.notable_types -> Person\n# Answer:\nDaisy Boo Pamela Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.children -> Buddy Bear Maurice Oliver -> common.topic.notable_for -> g.1255j218b\n# Answer:\nBuddy Bear Maurice Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.spouse_s -> m.04t57cj -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.04t57cj", "# Reasoning Path:\nJamie Oliver -> tv.tv_producer.programs_produced -> m.0ng0wq_ -> tv.tv_producer_term.producer_type -> Executive Producer\n# Answer:\nm.0ng0wq_"], "ground_truth": ["Poppy Honey Rosie Oliver", "Daisy Boo Pamela Oliver", "Buddy Bear Maurice Oliver", "Petal Blossom Rainbow Oliver"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.8, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.8, "path_ans_recall": 0.75}
{"id": "WebQTest-305", "prediction": ["# Reasoning Path:\nBradley Walsh -> people.person.places_lived -> m.0h8jzzl -> people.place_lived.location -> Los Angeles\n# Answer:\nm.0h8jzzl", "# Reasoning Path:\nBradley Walsh -> people.person.places_lived -> m.0h8jzzr -> people.place_lived.location -> Toronto\n# Answer:\nm.0h8jzzr", "# Reasoning Path:\nBradley Walsh -> tv.tv_actor.guest_roles -> m.09p0wtd -> tv.tv_guest_role.episodes_appeared_in -> Juliette Lewis, Colin McAllister, Justin Ryan, Oasis\n# Answer:\nm.09p0wtd", "# Reasoning Path:\nBradley Walsh -> tv.tv_actor.starring_roles -> m.03jprnv -> tv.regular_tv_appearance.series -> Wheel of Fortune\n# Answer:\nm.03jprnv", "# Reasoning Path:\nBradley Walsh -> tv.tv_actor.starring_roles -> m.03js8bs -> tv.regular_tv_appearance.series -> Night and Day\n# Answer:\nm.03js8bs", "# Reasoning Path:\nBradley Walsh -> tv.tv_actor.guest_roles -> m.09p0wtk -> tv.tv_guest_role.episodes_appeared_in -> Series 4, Show 6\n# Answer:\nm.09p0wtk", "# Reasoning Path:\nBradley Walsh -> tv.tv_actor.starring_roles -> m.04dcy12 -> tv.regular_tv_appearance.series -> Torn\n# Answer:\nm.04dcy12", "# Reasoning Path:\nBradley Walsh -> tv.tv_actor.guest_roles -> m.09p0wtq -> tv.tv_guest_role.episodes_appeared_in -> ...And A Fist Full of Jack and Jills\n# Answer:\nm.09p0wtq"], "ground_truth": ["Los Angeles", "Toronto"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-306", "prediction": ["# Reasoning Path:\nInter Milan -> base.schemastaging.sports_team_extra.managers -> m.0w4j_k_ -> base.schemastaging.sports_team_manager_tenure.manager -> Walter Mazzarri\n# Answer:\nm.0w4j_k_", "# Reasoning Path:\nInter Milan -> base.schemastaging.sports_team_extra.managers -> m.0w4j_k_ -> base.schemastaging.sports_team_manager_tenure.position -> Manager\n# Answer:\nm.0w4j_k_", "# Reasoning Path:\nInter Milan -> base.schemastaging.sports_team_extra.managers -> m.0w4j_k_ -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0w4j_k_", "# Reasoning Path:\nInter Milan -> soccer.football_team.manager -> m.0w2lsb3 -> soccer.football_team_management_tenure.manager -> Walter Mazzarri\n# Answer:\nm.0w2lsb3", "# Reasoning Path:\nInter Milan -> soccer.football_team.manager -> m.0bhj5cb -> soccer.football_team_management_tenure.manager -> Jos\u00e9 Mourinho\n# Answer:\nm.0bhj5cb", "# Reasoning Path:\nInter Milan -> soccer.football_team.manager -> m.0w2lsb3 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0w2lsb3", "# Reasoning Path:\nInter Milan -> soccer.football_team.manager -> m.0gf984m -> soccer.football_team_management_tenure.manager -> Leonardo Ara\u00fajo\n# Answer:\nm.0gf984m", "# Reasoning Path:\nInter Milan -> sports.sports_team.championships -> 1964 European Cup Final -> sports.sports_championship_event.season -> 1963\u201364 European Cup\n# Answer:\n1964 European Cup Final", "# Reasoning Path:\nInter Milan -> sports.sports_team.championships -> 1964 Intercontinental Cup -> sports.sports_championship_event.championship -> Intercontinental Cup\n# Answer:\n1964 Intercontinental Cup", "# Reasoning Path:\nInter Milan -> sports.sports_team.championships -> 1964 European Cup Final -> common.topic.article -> m.02rp_5w\n# Answer:\n1964 European Cup Final"], "ground_truth": ["Walter Mazzarri"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-308", "prediction": ["# Reasoning Path:\nReese Witherspoon -> celebrities.celebrity.sexual_relationships -> m.02_90w0 -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nm.02_90w0", "# Reasoning Path:\nReese Witherspoon -> celebrities.celebrity.sexual_relationships -> m.02_90w0 -> celebrities.romantic_relationship.celebrity -> Jake Gyllenhaal\n# Answer:\nm.02_90w0", "# Reasoning Path:\nReese Witherspoon -> base.popstra.celebrity.dated -> m.065ps9j -> base.popstra.dated.participant -> Jake Gyllenhaal\n# Answer:\nm.065ps9j", "# Reasoning Path:\nReese Witherspoon -> base.popstra.celebrity.dated -> m.063h647 -> base.popstra.dated.participant -> Jake Gyllenhaal\n# Answer:\nm.063h647", "# Reasoning Path:\nReese Witherspoon -> base.popstra.celebrity.dated -> m.065pts5 -> base.popstra.dated.participant -> Jeremy Sisto\n# Answer:\nm.065pts5", "# Reasoning Path:\nReese Witherspoon -> tv.tv_program_guest.appeared_on -> m.0j7p7x1 -> tv.tv_guest_personal_appearance.episode -> Fake Collagen Doctors\n# Answer:\nm.0j7p7x1", "# Reasoning Path:\nReese Witherspoon -> tv.tv_program_guest.appeared_on -> m.0j7p7x1 -> tv.tv_guest_personal_appearance.appearance_type -> Celebrity guest\n# Answer:\nm.0j7p7x1", "# Reasoning Path:\nReese Witherspoon -> tv.tv_program_guest.appeared_on -> m.0j8g6ss -> tv.tv_guest_personal_appearance.appearance_type -> Celebrity guest\n# Answer:\nm.0j8g6ss", "# Reasoning Path:\nReese Witherspoon -> tv.tv_program_guest.appeared_on -> m.0jh_lbq -> tv.tv_guest_personal_appearance.appearance_type -> Celebrity guest\n# Answer:\nm.0jh_lbq", "# Reasoning Path:\nReese Witherspoon -> tv.tv_program_guest.appeared_on -> m.0j8g6ss -> tv.tv_guest_personal_appearance.episode -> Reese Witherspoon, Hugh Laurie, Michael Bolton\n# Answer:\nm.0j8g6ss"], "ground_truth": ["Jim Toth"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> base.biblioness.bibs_location.country -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.administrative_division.second_level_division_of -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> Scotland\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#domain -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Gender -> type.property.schema -> Person\n# Answer:\nGender"], "ground_truth": ["2005"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-310", "prediction": ["# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.image -> 10colonesbcrfront\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.webpage -> m.04m2m2k\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60wmkvh\n# Answer:\ng.11b60wmkvh", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.11b71w3_6h\n# Answer:\ng.11b71w3_6h", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_5xgy\n# Answer:\ng.1245_5xgy", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_6n42\n# Answer:\ng.1245_6n42", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.1hhc37pjz\n# Answer:\ng.1hhc37pjz", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.1hhc37x5q\n# Answer:\ng.1hhc37x5q"], "ground_truth": ["Costa Rican col\u00f3n"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-311", "prediction": ["# Reasoning Path:\nUltimate Fighting Championship -> organization.organization.headquarters -> m.03gbyq5 -> location.mailing_address.citytown -> Las Vegas\n# Answer:\nm.03gbyq5", "# Reasoning Path:\nUltimate Fighting Championship -> organization.organization.headquarters -> m.03gbyq5 -> location.mailing_address.state_province_region -> Nevada\n# Answer:\nm.03gbyq5", "# Reasoning Path:\nUltimate Fighting Championship -> organization.organization.headquarters -> m.03gbyq5 -> common.topic.notable_types -> Location\n# Answer:\nm.03gbyq5", "# Reasoning Path:\nUltimate Fighting Championship -> business.customer.supplier -> 4INFO -> organization.organization.headquarters -> m.03dk0hf\n# Answer:\n4INFO", "# Reasoning Path:\nUltimate Fighting Championship -> business.sponsor.sponsorship -> m.0_rf5gv -> business.sponsorship.sponsored_recipient -> UFC 170\n# Answer:\nm.0_rf5gv", "# Reasoning Path:\nUltimate Fighting Championship -> business.sponsor.sponsorship -> m.0jwc64x -> business.sponsorship.sponsored_recipient -> UFC 146\n# Answer:\nm.0jwc64x", "# Reasoning Path:\nUltimate Fighting Championship -> business.sponsor.sponsorship -> m.0khmvp3 -> business.sponsorship.sponsored_recipient -> UFC on Fox: Shogun vs. Vera\n# Answer:\nm.0khmvp3", "# Reasoning Path:\nUltimate Fighting Championship -> business.customer.supplier -> 4INFO -> business.employer.employees -> m.010pmfbd\n# Answer:\n4INFO", "# Reasoning Path:\nUltimate Fighting Championship -> business.customer.supplier -> 4INFO -> computer.software_developer.software -> AdHaven\n# Answer:\n4INFO", "# Reasoning Path:\nUltimate Fighting Championship -> business.customer.supplier -> 4INFO -> business.employer.employees -> m.010qbccp\n# Answer:\n4INFO"], "ground_truth": ["Las Vegas"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-312", "prediction": ["# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0hhzbby -> film.performance.actor -> Chris Pine\n# Answer:\nm.0hhzbby", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8ffv -> film.performance.actor -> William Shatner\n# Answer:\nm.02h8ffv", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0hhzbby -> film.performance.film -> Star Trek Into Darkness\n# Answer:\nm.0hhzbby", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.012vszbf -> film.performance.actor -> Chris Pine\n# Answer:\nm.012vszbf", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8ffv -> film.performance.film -> Star Trek III: The Search for Spock\n# Answer:\nm.02h8ffv", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.012vszbf -> film.performance.film -> Star Trek 3\n# Answer:\nm.012vszbf", "# Reasoning Path:\nJames T. Kirk -> common.topic.notable_types -> Film character -> freebase.type_profile.kind -> Annotation\n# Answer:\nFilm character", "# Reasoning Path:\nJames T. Kirk -> common.topic.notable_types -> Film character -> type.type.expected_by -> Character\n# Answer:\nFilm character", "# Reasoning Path:\nJames T. Kirk -> common.topic.webpage -> m.09wcz0z -> common.webpage.resource -> Wolverines! Chris Hemsworth nabs lead in 'Red Dawn'\n# Answer:\nm.09wcz0z", "# Reasoning Path:\nJames T. Kirk -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Fictional Character\n# Answer:\nFilm character"], "ground_truth": ["Vic Mignogna", "Jim Carrey", "William Shatner"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.08695652173913043, "path_precision": 0.1, "path_recall": 0.07692307692307693, "path_ans_f1": 0.15384615384615383, "path_ans_precision": 0.1, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-313", "prediction": ["# Reasoning Path:\nAlbert Pujols -> base.schemastaging.athlete_extra.salary -> m.0j2qh8m -> base.schemastaging.athlete_salary.team -> Los Angeles Angels of Anaheim\n# Answer:\nm.0j2qh8m", "# Reasoning Path:\nAlbert Pujols -> base.schemastaging.athlete_extra.salary -> m.0wdd82v -> base.schemastaging.athlete_salary.team -> Los Angeles Angels of Anaheim\n# Answer:\nm.0wdd82v", "# Reasoning Path:\nAlbert Pujols -> base.schemastaging.athlete_extra.salary -> m.0j2qh8m -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0j2qh8m", "# Reasoning Path:\nAlbert Pujols -> base.schemastaging.athlete_extra.salary -> m.0wdd82v -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0wdd82v", "# Reasoning Path:\nAlbert Pujols -> baseball.baseball_player.batting_stats -> m.06s6m0p -> baseball.batting_statistics.season -> 2001 Major League Baseball Season\n# Answer:\nm.06s6m0p", "# Reasoning Path:\nAlbert Pujols -> sports.pro_athlete.teams -> m.04xhdn1 -> sports.sports_team_roster.team -> Los Angeles Angels of Anaheim\n# Answer:\nm.04xhdn1", "# Reasoning Path:\nAlbert Pujols -> sports.pro_athlete.teams -> m.05kb0jt -> sports.sports_team_roster.team -> Scottsdale Scorpions\n# Answer:\nm.05kb0jt", "# Reasoning Path:\nAlbert Pujols -> sports.pro_athlete.teams -> m.0j2lc1l -> sports.sports_team_roster.position -> Infielder\n# Answer:\nm.0j2lc1l", "# Reasoning Path:\nAlbert Pujols -> sports.pro_athlete.teams -> m.04xhdn1 -> sports.sports_team_roster.position -> Infielder\n# Answer:\nm.04xhdn1", "# Reasoning Path:\nAlbert Pujols -> baseball.baseball_player.batting_stats -> m.06s6m0p -> baseball.batting_statistics.team -> St. Louis Cardinals\n# Answer:\nm.06s6m0p"], "ground_truth": ["Scottsdale Scorpions", "Los Angeles Angels of Anaheim"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-314", "prediction": ["# Reasoning Path:\nSweden -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Denmark\n# Answer:\nUnitary state", "# Reasoning Path:\nSweden -> location.country.form_of_government -> Unitary state -> common.topic.notable_types -> Form of Government\n# Answer:\nUnitary state", "# Reasoning Path:\nSweden -> location.country.form_of_government -> Constitutional monarchy -> common.topic.notable_types -> Form of Government\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\ng.11b60n70x2", "# Reasoning Path:\nSweden -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\ng.12cp_k2s4", "# Reasoning Path:\nSweden -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Italy\n# Answer:\nUnitary state", "# Reasoning Path:\nSweden -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Denmark\n# Answer:\nParliamentary system", "# Reasoning Path:\nSweden -> location.country.form_of_government -> Parliamentary system -> common.topic.notable_types -> Form of Government\n# Answer:\nParliamentary system", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ll\n# Answer:\ng.1245_22ll"], "ground_truth": ["Parliamentary system", "Representative democracy", "Constitutional monarchy", "Unitary state", "Hereditary monarchy"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6461538461538462, "ans_precission": 0.7, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.6461538461538462, "path_ans_precision": 0.7, "path_ans_recall": 0.6}
{"id": "WebQTest-315", "prediction": ["# Reasoning Path:\nGeorge Jones -> music.artist.origin -> Saratoga -> location.location.containedby -> Hardin County\n# Answer:\nSaratoga", "# Reasoning Path:\nGeorge Jones -> music.artist.origin -> Saratoga -> common.topic.notable_for -> g.1255l1rpv\n# Answer:\nSaratoga", "# Reasoning Path:\nGeorge Jones -> music.artist.origin -> Saratoga -> location.location.geolocation -> m.0kh6hg\n# Answer:\nSaratoga", "# Reasoning Path:\nGeorge Jones -> music.artist.origin -> Saratoga -> location.location.containedby -> Texas\n# Answer:\nSaratoga", "# Reasoning Path:\nGeorge Jones -> people.person.place_of_birth -> Saratoga -> location.location.containedby -> Hardin County\n# Answer:\nSaratoga", "# Reasoning Path:\nGeorge Jones -> people.person.place_of_birth -> Saratoga -> common.topic.notable_for -> g.1255l1rpv\n# Answer:\nSaratoga", "# Reasoning Path:\nGeorge Jones -> music.artist.origin -> Vidor -> location.location.containedby -> Orange County\n# Answer:\nVidor", "# Reasoning Path:\nGeorge Jones -> people.person.place_of_birth -> Saratoga -> location.location.geolocation -> m.0kh6hg\n# Answer:\nSaratoga", "# Reasoning Path:\nGeorge Jones -> music.artist.origin -> Vidor -> common.topic.notable_types -> City/Town/Village\n# Answer:\nVidor", "# Reasoning Path:\nGeorge Jones -> people.person.place_of_birth -> Saratoga -> location.location.containedby -> Texas\n# Answer:\nSaratoga"], "ground_truth": ["Saratoga"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-316", "prediction": ["# Reasoning Path:\nWilliam Shakespeare -> fictional_universe.fictional_character.occupation -> Playwright -> people.profession.specialization_of -> Writer\n# Answer:\nPlaywright", "# Reasoning Path:\nWilliam Shakespeare -> freebase.valuenotation.is_reviewed -> Art Form\n# Answer:\nArt Form", "# Reasoning Path:\nWilliam Shakespeare -> fictional_universe.fictional_character_creator.fictional_characters_created -> Amleto -> fictional_universe.fictional_character.gender -> Male\n# Answer:\nAmleto", "# Reasoning Path:\nWilliam Shakespeare -> fictional_universe.fictional_character.occupation -> Playwright -> rdf-schema#range -> Author\n# Answer:\nPlaywright", "# Reasoning Path:\nWilliam Shakespeare -> freebase.valuenotation.is_reviewed -> Art Subject\n# Answer:\nArt Subject", "# Reasoning Path:\nWilliam Shakespeare -> fictional_universe.fictional_character_creator.fictional_characters_created -> Fenge -> film.film_character.portrayed_in_films -> m.05ch58l\n# Answer:\nFenge", "# Reasoning Path:\nWilliam Shakespeare -> fictional_universe.fictional_character_creator.fictional_characters_created -> Amleto -> film.film_character.portrayed_in_films -> m.04j1_2s\n# Answer:\nAmleto", "# Reasoning Path:\nWilliam Shakespeare -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ancient Pistol -> fictional_universe.fictional_character.gender -> Male\n# Answer:\nAncient Pistol", "# Reasoning Path:\nWilliam Shakespeare -> fictional_universe.fictional_character.occupation -> Playwright -> people.profession.corresponding_type -> Author\n# Answer:\nPlaywright", "# Reasoning Path:\nWilliam Shakespeare -> fictional_universe.fictional_character.occupation -> Poet -> people.profession.specialization_of -> Writer\n# Answer:\nPoet"], "ground_truth": ["Playwright", "Poet", "Actor", "Author", "Lyricist"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.4000000000000001, "ans_precission": 0.4, "ans_recall": 0.4, "path_f1": 0.13333333333333333, "path_precision": 0.1, "path_recall": 0.2, "path_ans_f1": 0.48, "path_ans_precision": 0.4, "path_ans_recall": 0.6}
{"id": "WebQTest-317", "prediction": ["# Reasoning Path:\nRussia -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.countries_spoken_in -> Ukraine\n# Answer:\nUkrainian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Tsakhur Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nTsakhur Language", "# Reasoning Path:\nRussia -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Ukraine\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nUkraine -> book.book_subject.works -> Battle for the Ukraine -> book.written_work.subjects -> Russia\n# Answer:\nBattle for the Ukraine", "# Reasoning Path:\nUkraine -> book.book_subject.works -> Czars, Russia, Poland and the Ukraine, 1462-1725 -> book.written_work.subjects -> Russia\n# Answer:\nCzars, Russia, Poland and the Ukraine, 1462-1725", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Tsakhur Language -> common.topic.notable_types -> Human Language\n# Answer:\nTsakhur Language", "# Reasoning Path:\nUkraine -> book.book_subject.works -> Alicia -> book.written_work.subjects -> Asia\n# Answer:\nAlicia", "# Reasoning Path:\nUkraine -> book.book_subject.works -> Alicia -> common.topic.notable_types -> Book\n# Answer:\nAlicia", "# Reasoning Path:\nRussia -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Ukraine\n# Answer:\nConstitutional republic", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Tsakhur Language -> language.human_language.region -> Asia\n# Answer:\nTsakhur Language"], "ground_truth": ["Uzbekistan"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-318", "prediction": ["# Reasoning Path:\nSwitzerland -> location.country.currency_used -> Swiss franc -> finance.currency.countries_used -> Liechtenstein\n# Answer:\nSwiss franc", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d0tc3g\n# Answer:\ng.11b6d0tc3g", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6p27nwl\n# Answer:\ng.11b6p27nwl", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> French -> language.human_language.main_country -> France\n# Answer:\nFrench", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.1hhc3d66v\n# Answer:\ng.1hhc3d66v", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> German Language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nGerman Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> Belgium\n# Answer:\nFrench", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> German Language -> language.human_language.region -> Europe\n# Answer:\nGerman Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> Italian Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nItalian Language"], "ground_truth": ["Swiss franc"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> base.ontologies.ontology_instance.equivalent_instances -> m.09knr56 -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nm.09knr56", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc4pgdn\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.country.languages_spoken -> Malay Language\n# Answer:\nChristmas Island", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Bunyip\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gf -> common.webpage.category -> Topic Webpage\n# Answer:\nm.04lt3gf", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gl -> common.webpage.category -> Topic Webpage\n# Answer:\nm.04lt3gl", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc4y0zl\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gf -> common.webpage.resource -> m.0blsygc\n# Answer:\nm.04lt3gf", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gr -> common.webpage.resource -> m.0bm63vg\n# Answer:\nm.04lt3gr", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Yowie\n# Answer:\nAustralia"], "ground_truth": ["AUD"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-321", "prediction": ["# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.main_country -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\ng.11b60qdnlp", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> Japan\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\ng.11b60vfr15", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> New Zealand\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Indonesia\n# Answer:\nEnglish Language"], "ground_truth": ["Lojban", "English Language", "Esperanto Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-322", "prediction": ["# Reasoning Path:\nIan Somerhalder -> film.actor.film -> m.0cw0xkk -> film.performance.film -> The Sensation of Sight\n# Answer:\nm.0cw0xkk", "# Reasoning Path:\nIan Somerhalder -> film.actor.film -> m.0crv_k2 -> film.performance.film -> Lost City Raiders\n# Answer:\nm.0crv_k2", "# Reasoning Path:\nIan Somerhalder -> film.actor.film -> m.0vsjh19 -> film.performance.film -> The Anomaly\n# Answer:\nm.0vsjh19", "# Reasoning Path:\nIan Somerhalder -> film.actor.film -> m.0cw0xkk -> film.performance.character -> Drifter\n# Answer:\nm.0cw0xkk", "# Reasoning Path:\nIan Somerhalder -> film.actor.film -> m.0crv_k2 -> film.performance.character -> Jack Kubiak\n# Answer:\nm.0crv_k2", "# Reasoning Path:\nIan Somerhalder -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#range -> Person\n# Answer:\nChildren", "# Reasoning Path:\nIan Somerhalder -> film.actor.film -> m.0vsjh19 -> film.performance.character -> Harkin Langham\n# Answer:\nm.0vsjh19", "# Reasoning Path:\nIan Somerhalder -> freebase.valuenotation.is_reviewed -> Children -> type.property.schema -> Person\n# Answer:\nChildren", "# Reasoning Path:\nIan Somerhalder -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nIan Somerhalder -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#domain -> Person\n# Answer:\nChildren"], "ground_truth": ["The Old Man and the Studio", "National Lampoon's TV: The Movie", "Lost City Raiders", "Wake", "How to Make Love to a Woman", "Fireball", "Changing Hearts", "Anatomy of a Hate Crime", "Fearless", "The Anomaly", "The Lost Samaritan", "The Sensation of Sight", "Caught on Tape", "Life as a House", "In Enemy Hands", "The Rules of Attraction", "Recess", "Celebrity", "Pulse", "Time Framed", "The Tournament", "Marco Polo"], "ans_acc": 0.13636363636363635, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.3, "path_recall": 0.13043478260869565, "path_ans_f1": 0.18749999999999997, "path_ans_precision": 0.3, "path_ans_recall": 0.13636363636363635}
{"id": "WebQTest-323", "prediction": ["# Reasoning Path:\nRon Howard -> film.producer.film -> Dr. Seuss' How the Grinch Stole Christmas -> film.film.executive_produced_by -> Todd Hallowell\n# Answer:\nDr. Seuss' How the Grinch Stole Christmas", "# Reasoning Path:\nRon Howard -> film.director.film -> Dr. Seuss' How the Grinch Stole Christmas -> film.film.executive_produced_by -> Todd Hallowell\n# Answer:\nDr. Seuss' How the Grinch Stole Christmas", "# Reasoning Path:\nRon Howard -> film.director.film -> Frost/Nixon -> film.film.produced_by -> Brian Grazer\n# Answer:\nFrost/Nixon", "# Reasoning Path:\nRon Howard -> film.producer.film -> Dr. Seuss' How the Grinch Stole Christmas -> film.film.production_companies -> Image Entertainment\n# Answer:\nDr. Seuss' How the Grinch Stole Christmas", "# Reasoning Path:\nRon Howard -> film.producer.film -> Dr. Seuss' How the Grinch Stole Christmas -> film.film.film_casting_director -> Jane Jenkins\n# Answer:\nDr. Seuss' How the Grinch Stole Christmas", "# Reasoning Path:\nRon Howard -> film.producer.film -> Angels & Demons -> film.film.film_casting_director -> Jane Jenkins\n# Answer:\nAngels & Demons", "# Reasoning Path:\nRon Howard -> film.producer.film -> Dr. Seuss' How the Grinch Stole Christmas -> film.film.production_companies -> Imagine Entertainment\n# Answer:\nDr. Seuss' How the Grinch Stole Christmas", "# Reasoning Path:\nRon Howard -> film.producer.film -> A Beautiful Mind -> film.film.production_companies -> Image Entertainment\n# Answer:\nA Beautiful Mind", "# Reasoning Path:\nRon Howard -> film.director.film -> Dr. Seuss' How the Grinch Stole Christmas -> film.film.production_companies -> Image Entertainment\n# Answer:\nDr. Seuss' How the Grinch Stole Christmas", "# Reasoning Path:\nRon Howard -> film.director.film -> A Beautiful Mind -> film.film.production_companies -> Image Entertainment\n# Answer:\nA Beautiful Mind"], "ground_truth": ["Frost/Nixon", "Cinderella Man", "Backdraft", "Far and Away", "Inferno", "Angels & Demons", "The Lost Symbol", "Cocoon", "Night Shift", "Parenthood", "Rush", "The Missing", "Grand Theft Auto", "Jay-Z: Made in America", "The Dilemma", "Ransom", "Cotton Candy", "Splash", "Presidential Reunion", "Gung Ho", "Willow", "EDtv", "Apollo 13", "In the Heart of the Sea", "Dr. Seuss' How the Grinch Stole Christmas", "The Dark Tower", "The Paper", "The Da Vinci Code", "A Beautiful Mind"], "ans_acc": 0.13793103448275862, "ans_hit": 1, "ans_f1": 0.2424242424242424, "ans_precission": 1.0, "ans_recall": 0.13793103448275862, "path_f1": 0.2162162162162162, "path_precision": 0.5, "path_recall": 0.13793103448275862, "path_ans_f1": 0.2424242424242424, "path_ans_precision": 1.0, "path_ans_recall": 0.13793103448275862}
{"id": "WebQTest-326", "prediction": ["# Reasoning Path:\nGerald Ford -> people.deceased_person.cause_of_death -> Cardiovascular disease -> medicine.disease.parent_disease -> Ill-defined descriptions and complications of heart disease\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nGerald Ford -> people.deceased_person.cause_of_death -> Cardiovascular disease -> medicine.disease.medical_specialties -> Internal medicine\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nGerald Ford -> people.deceased_person.cause_of_death -> Cardiovascular disease -> base.medical_schema_staging.medical_subspecialty.parent_medical_specialty -> Internal medicine\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nGerald Ford -> people.person.quotations -> A strong defense is the surest way to peace. Strength makes detente attainable. Weakness invites war, as my generationmy generationknows from four very bitter experiences. Just as Americas will for peace is second to none, so will Americas strength be second to none. We cannot rely on the forbearance of others to protect this Nation. The power and diversity of the Armed Forces, active Guard and Reserve, the resolve of our fellow citizens, the flexibility in our command to navigate international waters that remain troubled are all essential to our security. -> media_common.quotation.subjects -> Uncategorised\n# Answer:\nA strong defense is the surest way to peace. Strength makes detente attainable. Weakness invites war, as my generationmy generationknows from four very bitter experiences. Just as Americas will for peace is second to none, so will Americas strength be second to none. We cannot rely on the forbearance of others to protect this Nation. The power and diversity of the Armed Forces, active Guard and Reserve, the resolve of our fellow citizens, the flexibility in our command to navigate international waters that remain troubled are all essential to our security.", "# Reasoning Path:\nGerald Ford -> government.political_appointer.appointees -> m.01xrx8h -> government.government_position_held.office_holder -> George H. W. Bush\n# Answer:\nm.01xrx8h", "# Reasoning Path:\nGerald Ford -> government.political_appointer.appointees -> m.01z0qwq -> government.government_position_held.office_holder -> Dick Cheney\n# Answer:\nm.01z0qwq", "# Reasoning Path:\nGerald Ford -> people.person.quotations -> All of us who served in one war or another know very well that all wars are the glory and the agony of the young. -> common.topic.notable_types -> Quotation\n# Answer:\nAll of us who served in one war or another know very well that all wars are the glory and the agony of the young.", "# Reasoning Path:\nGerald Ford -> government.political_appointer.appointees -> m.04cnx_5 -> government.government_position_held.office_holder -> Donald Rumsfeld\n# Answer:\nm.04cnx_5", "# Reasoning Path:\nGerald Ford -> people.person.quotations -> All of us who served in one war or another know very well that all wars are the glory and the agony of the young. -> common.topic.notable_for -> g.12564tpvs\n# Answer:\nAll of us who served in one war or another know very well that all wars are the glory and the agony of the young.", "# Reasoning Path:\nGerald Ford -> people.person.quotations -> America now is stumbling through the darkness of hatred and divisiveness. Our values, our principles, and our determination to succeed as a free and democratic people will give us a torch to light the way. And we will survive and become the strongernot only because of a patriotism that stands for love of country, but a patriotism that stands for love of people. -> media_common.quotation.subjects -> Uncategorised\n# Answer:\nAmerica now is stumbling through the darkness of hatred and divisiveness. Our values, our principles, and our determination to succeed as a free and democratic people will give us a torch to light the way. And we will survive and become the strongernot only because of a patriotism that stands for love of country, but a patriotism that stands for love of people."], "ground_truth": ["Cardiovascular disease"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-327", "prediction": ["# Reasoning Path:\nTurkey -> base.locations.countries.continent -> Eurasia -> base.locations.continents.countries_within -> Azerbaijan\n# Answer:\nEurasia", "# Reasoning Path:\nTurkey -> base.locations.countries.continent -> Eurasia -> base.locations.continents.countries_within -> France\n# Answer:\nEurasia", "# Reasoning Path:\nTurkey -> base.locations.countries.continent -> Eurasia -> common.topic.notable_types -> Location\n# Answer:\nEurasia", "# Reasoning Path:\nTurkey -> location.location.partially_containedby -> Asia -> location.location.partiallycontains -> m.0wg8kz1\n# Answer:\nAsia", "# Reasoning Path:\nTurkey -> location.location.partially_containedby -> Anatolia -> location.location.containedby -> Western Asia\n# Answer:\nAnatolia", "# Reasoning Path:\nTurkey -> location.location.partially_containedby -> Asia -> location.location.time_zones -> Eastern European Time Zone\n# Answer:\nAsia", "# Reasoning Path:\nTurkey -> location.location.partially_containedby -> Asia -> base.locations.continents.countries_within -> Iran\n# Answer:\nAsia", "# Reasoning Path:\nTurkey -> location.location.partially_containedby -> Anatolia -> common.topic.notable_types -> Administrative Division\n# Answer:\nAnatolia", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.065sxsl -> military.military_combatant_group.conflict -> Korean War\n# Answer:\nm.065sxsl", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.066402y -> military.military_combatant_group.conflict -> Iraqi no-fly zones\n# Answer:\nm.066402y"], "ground_truth": ["Asia", "Eurasia", "Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.631578947368421, "ans_precission": 0.6, "ans_recall": 0.6666666666666666, "path_f1": 0.631578947368421, "path_precision": 0.6, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-328", "prediction": ["# Reasoning Path:\nKevin Love -> people.person.education -> m.04ftwdc -> education.education.institution -> University of California, Los Angeles\n# Answer:\nm.04ftwdc", "# Reasoning Path:\nKevin Love -> people.person.education -> m.045z4gx -> education.education.institution -> Lake Oswego High School\n# Answer:\nm.045z4gx", "# Reasoning Path:\nKevin Love -> common.topic.notable_for -> g.125c8dfyr\n# Answer:\ng.125c8dfyr", "# Reasoning Path:\nKevin Love -> freebase.valuenotation.is_reviewed -> Height -> type.property.unit -> Meter\n# Answer:\nHeight", "# Reasoning Path:\nKevin Love -> freebase.valuenotation.is_reviewed -> Height -> type.property.schema -> Person\n# Answer:\nHeight", "# Reasoning Path:\nKevin Love -> freebase.valuenotation.is_reviewed -> Teams -> type.property.expected_type -> Sports team roster\n# Answer:\nTeams", "# Reasoning Path:\nKevin Love -> freebase.valuenotation.is_reviewed -> Height -> rdf-schema#domain -> Person\n# Answer:\nHeight", "# Reasoning Path:\nKevin Love -> freebase.valuenotation.is_reviewed -> Teams -> type.property.schema -> Athlete\n# Answer:\nTeams", "# Reasoning Path:\nKevin Love -> freebase.valuenotation.is_reviewed -> Teams -> rdf-schema#range -> Sports team roster\n# Answer:\nTeams"], "ground_truth": ["University of California, Los Angeles"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.1818181818181818, "path_precision": 0.1111111111111111, "path_recall": 0.5, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-329", "prediction": ["# Reasoning Path:\nItaly -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Brazil\n# Answer:\nConstitutional republic", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\ng.11b71mdn0q", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Germany\n# Answer:\nConstitutional republic", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Finland\n# Answer:\nParliamentary republic", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\ng.12tb6gbb1", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Japan\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37cxr\n# Answer:\ng.1hhc37cxr", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Kingdom of Italy\n# Answer:\nUnitary state"], "ground_truth": ["Constitutional republic", "Parliamentary republic", "Unitary state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\ng.11b60n70x2", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\ng.12cp_k2s4", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ll\n# Answer:\ng.1245_22ll", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22zj\n# Answer:\ng.1245_22zj", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc378pv\n# Answer:\ng.1hhc378pv", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc37hbq\n# Answer:\ng.1hhc37hbq"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-331", "prediction": ["# Reasoning Path:\nKarnak -> location.location.containedby -> Egypt -> location.location.containedby -> Arab world\n# Answer:\nEgypt", "# Reasoning Path:\nKarnak -> location.location.containedby -> Egypt -> location.location.containedby -> Middle East\n# Answer:\nEgypt", "# Reasoning Path:\nKarnak -> location.location.containedby -> Luxor Governorate -> location.location.containedby -> Egypt\n# Answer:\nLuxor Governorate", "# Reasoning Path:\nKarnak -> travel.tourist_attraction.near_travel_destination -> Luxor -> location.location.containedby -> Egypt\n# Answer:\nLuxor", "# Reasoning Path:\nKarnak -> location.location.containedby -> Egypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives\n# Answer:\nEgypt", "# Reasoning Path:\nKarnak -> location.location.people_born_here -> Yahya Taher Abdullah -> common.topic.article -> m.0gwydcg\n# Answer:\nYahya Taher Abdullah", "# Reasoning Path:\nKarnak -> location.location.containedby -> Egypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt\n# Answer:\nEgypt", "# Reasoning Path:\nKarnak -> location.location.containedby -> Egypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nEgypt", "# Reasoning Path:\nKarnak -> location.location.containedby -> Luxor Governorate -> location.location.containedby -> North Africa\n# Answer:\nLuxor Governorate", "# Reasoning Path:\nKarnak -> location.location.containedby -> Luxor Governorate -> common.topic.notable_types -> Administrative Division\n# Answer:\nLuxor Governorate"], "ground_truth": ["Egypt", "Luxor Governorate"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-333", "prediction": ["# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.0_sv_jd -> award.award_nomination.nominated_for -> High Crimes\n# Answer:\nm.0_sv_jd", "# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.0_tl4p_ -> award.award_nomination.nominated_for -> Driving Miss Daisy\n# Answer:\nm.0_tl4p_", "# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.0_sv_jd -> award.award_nomination.ceremony -> 34th NAACP Image Awards\n# Answer:\nm.0_sv_jd", "# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.0_sv_jd -> award.award_nomination.award -> NAACP Image Award for Outstanding Actor in a Motion Picture\n# Answer:\nm.0_sv_jd", "# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.021ydzf -> award.award_nomination.nominated_for -> Driving Miss Daisy\n# Answer:\nm.021ydzf", "# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.0_tl4p_ -> freebase.valuenotation.is_reviewed -> Ceremony\n# Answer:\nm.0_tl4p_", "# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.021ydzf -> award.award_nomination.ceremony -> 62nd Academy Awards\n# Answer:\nm.021ydzf", "# Reasoning Path:\nMorgan Freeman -> common.topic.webpage -> m.093_l_6 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.093_l_6", "# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.021ydzf -> award.award_nomination.award -> Academy Award for Best Actor\n# Answer:\nm.021ydzf", "# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.0_tl4p_ -> freebase.valuenotation.is_reviewed -> Year\n# Answer:\nm.0_tl4p_"], "ground_truth": ["Marie", "Johnny Handsome", "A Raisin in the Sun", "The Art of Romare Bearden", "Ted 2", "The Dark Knight", "Clean and Sober", "High Crimes", "Lean on Me", "Magnificent Desolation: Walking On The Moon 3D", "Lucy", "Batman Begins", "The Dark Knight Rises", "The Long Way Home", "m.0h0_gsx", "Levity", "Attica", "The Bonfire of the Vanities", "Nurse Betty", "Transcendence", "Harry & Son", "Dolphin Tale", "Brubaker", "Wish Wizard", "Island of Lemurs: Madagascar 3D", "The Maiden Heist", "War of the Worlds", "Outbreak", "Hard Rain", "Driving Miss Daisy", "Last Knights", "The Magic of Belle Isle", "Amistad", "America Beyond the Color Line", "The Execution of Raymond Graham", "Fight for Life", "Now You See Me: The Second Act", "Deep Impact", "The Civil War", "The Love Guru", "The Bucket List", "Under Suspicion", "For Love of Liberty: The Story of America's Black Patriots", "Death of a Prophet", "The Lego Movie", "Eyewitness", "Gone Baby Gone", "Dolphin Tale 2", "Olympus Has Fallen", "Teachers", "Unleashed", "Conan the Barbarian", "The Contract", "The Shawshank Redemption", "Clinton and Nadine", "That Was Then... This Is Now", "London Has Fallen", "The Pawnbroker", "Along Came a Spider", "m.0h0zs8c", "Robin Hood: Prince of Thieves", "RED", "Cosmic Voyage", "The Big Bounce", "The Sum of All Fears", "10 Items or Less", "The Hunting of the President", "We the People", "Glory", "Where Were You When the Lights Went Out?", "The Power of One", "Now You See Me", "Kiss the Girls", "Seven", "Unforgiven", "Oblivion", "Last Vegas", "Thick as Thieves", "Bruce Almighty", "Street Smart", "A Man Called Adam", "Invictus", "Lucky Number Slevin", "Moll Flanders", "Evan Almighty", "Million Dollar Baby", "Chain Reaction", "Guilty by Association", "Dreamcatcher", "Edison", "5 Flights Up", "Soul Brothas and Sistas: Vol. 4: Quadruple Feature", "All About Us", "Ben-Hur", "National Geographic: Inside the White House", "Heart Stopper", "Resting Place", "Feast of Love", "Wanted", "An Unfinished Life", "Roll Of Thunder, Hear My Cry"], "ans_acc": 0.019801980198019802, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.075, "path_precision": 0.3, "path_recall": 0.04285714285714286, "path_ans_f1": 0.03715170278637771, "path_ans_precision": 0.3, "path_ans_recall": 0.019801980198019802}
{"id": "WebQTest-334", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> base.popstra.celebrity.dated -> m.065pt_4 -> base.popstra.dated.participant -> Betty Grable\n# Answer:\nm.065pt_4", "# Reasoning Path:\nJohn F. Kennedy -> base.popstra.celebrity.dated -> m.065pzp2 -> base.popstra.dated.participant -> Arlene Dahl\n# Answer:\nm.065pzp2", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> Mortal Error: The Shot That Killed JFK\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nIrish American", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> common.topic.notable_types -> Event\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School -> common.topic.notable_for -> g.125583hnb\n# Answer:\nBloomington Kennedy High School", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> base.culturalevent.event.entity_involved -> Lee Harvey Oswald\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Cape Canaveral -> location.location.containedby -> Brevard County\n# Answer:\nCape Canaveral", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nIrish American", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> White people -> medicine.risk_factor.diseases -> Breast cancer\n# Answer:\nWhite people"], "ground_truth": ["Judith Exner", "Marilyn Monroe"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-335", "prediction": ["# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban peso -> common.topic.notable_for -> g.125c74q9f\n# Answer:\nCuban peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban convertible peso -> common.topic.notable_for -> g.1257mndy1\n# Answer:\nCuban convertible peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban peso -> common.topic.notable_types -> Currency\n# Answer:\nCuban peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban convertible peso -> common.topic.webpage -> m.04m29wr\n# Answer:\nCuban convertible peso", "# Reasoning Path:\nCuba -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc38hmp\n# Answer:\ng.1hhc38hmp", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban convertible peso -> common.topic.article -> m.049p6m\n# Answer:\nCuban convertible peso", "# Reasoning Path:\nCuba -> book.book_subject.works -> Farewell to the Sea -> book.written_work.original_language -> Spanish Language\n# Answer:\nFarewell to the Sea", "# Reasoning Path:\nCuba -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc3d68y\n# Answer:\ng.1hhc3d68y", "# Reasoning Path:\nCuba -> book.book_subject.works -> After Fidel: Raul Castro and the Future of Cuba's Revolution -> book.written_work.subjects -> Fidel Castro\n# Answer:\nAfter Fidel: Raul Castro and the Future of Cuba's Revolution", "# Reasoning Path:\nCuba -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc3d9ps\n# Answer:\ng.1hhc3d9ps"], "ground_truth": ["Cuban peso", "Cuban convertible peso"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-336", "prediction": ["# Reasoning Path:\nMorocco -> location.country.currency_used -> Moroccan dirham -> finance.currency.countries_used -> Western Sahara\n# Answer:\nMoroccan dirham", "# Reasoning Path:\nMorocco -> location.country.currency_used -> Moroccan dirham -> common.topic.notable_types -> Currency\n# Answer:\nMoroccan dirham", "# Reasoning Path:\nMorocco -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.1hhc385h5\n# Answer:\ng.1hhc385h5", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Marrakesh-Tensift-El Haouz -> common.topic.image -> Marrakech Tensift Al Haouz\n# Answer:\nMarrakesh-Tensift-El Haouz", "# Reasoning Path:\nMorocco -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.1hhc39ccx\n# Answer:\ng.1hhc39ccx", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Marrakesh-Tensift-El Haouz -> common.topic.article -> m.0dbm0c\n# Answer:\nMarrakesh-Tensift-El Haouz", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Chaouia-Ouardigha -> location.location.time_zones -> Western European Time Zone\n# Answer:\nChaouia-Ouardigha", "# Reasoning Path:\nMorocco -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.1hhc3_868\n# Answer:\ng.1hhc3_868", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Doukkala-Abda -> location.location.people_born_here -> Bouchaib Rmail\n# Answer:\nDoukkala-Abda", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Chaouia-Ouardigha -> common.topic.article -> m.0d1426\n# Answer:\nChaouia-Ouardigha"], "ground_truth": ["Moroccan dirham"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-337", "prediction": ["# Reasoning Path:\nMary McLeod Bethune -> people.person.profession -> g.121bkpjb\n# Answer:\ng.121bkpjb", "# Reasoning Path:\nMary McLeod Bethune -> symbols.name_source.namesakes -> Mary McLeod Bethune Council House National Historic Site -> travel.tourist_attraction.near_travel_destination -> Washington, D.C.\n# Answer:\nMary McLeod Bethune Council House National Historic Site", "# Reasoning Path:\nMary McLeod Bethune -> symbols.name_source.namesakes -> Mary McLeod Bethune Council House National Historic Site -> common.topic.notable_for -> g.1258mdtk6\n# Answer:\nMary McLeod Bethune Council House National Historic Site", "# Reasoning Path:\nMary McLeod Bethune -> symbols.name_source.namesakes -> Mary McLeod Bethune Home -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.05h5_mz\n# Answer:\nMary McLeod Bethune Home", "# Reasoning Path:\nMary McLeod Bethune -> people.person.education -> m.0n0v47n -> education.education.institution -> Barber\u2013Scotia College\n# Answer:\nm.0n0v47n", "# Reasoning Path:\nMary McLeod Bethune -> people.person.education -> m.0n1k53t -> education.education.institution -> Moody Bible Institute\n# Answer:\nm.0n1k53t", "# Reasoning Path:\nMary McLeod Bethune -> symbols.name_source.namesakes -> Bethune-Cookman University -> education.educational_institution.students_graduates -> m.02kyhrl\n# Answer:\nBethune-Cookman University", "# Reasoning Path:\nMary McLeod Bethune -> symbols.name_source.namesakes -> Bethune-Cookman University -> common.topic.notable_types -> College/University\n# Answer:\nBethune-Cookman University", "# Reasoning Path:\nMary McLeod Bethune -> symbols.name_source.namesakes -> Bethune-Cookman University -> education.educational_institution.students_graduates -> m.02kyhsm\n# Answer:\nBethune-Cookman University", "# Reasoning Path:\nMary McLeod Bethune -> symbols.name_source.namesakes -> Mary McLeod Bethune Home -> common.topic.image -> Mary mcleod bethune home\n# Answer:\nMary McLeod Bethune Home"], "ground_truth": ["Educator"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-338", "prediction": ["# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\ng.1245_2h9t", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.11b71r82sc\n# Answer:\ng.11b71r82sc", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_4lv6\n# Answer:\ng.1245_4lv6", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_6fj1\n# Answer:\ng.1245_6fj1", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.12cp_jcjg\n# Answer:\ng.12cp_jcjg", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.12tb6g_8v\n# Answer:\ng.12tb6g_8v"], "ground_truth": ["Japanese yen"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-339", "prediction": ["# Reasoning Path:\nMother Teresa -> people.person.nationality -> Ottoman Empire -> common.topic.notable_types -> Country\n# Answer:\nOttoman Empire", "# Reasoning Path:\nMother Teresa -> people.person.nationality -> India -> base.aareas.schema.administrative_area.administrative_children -> Tamil Nadu\n# Answer:\nIndia", "# Reasoning Path:\nMother Teresa -> people.person.nationality -> India -> common.topic.notable_types -> Country\n# Answer:\nIndia", "# Reasoning Path:\nMother Teresa -> base.activism.activist.area_of_activism -> Humanitarian -> common.topic.notable_for -> g.1256fnzxx\n# Answer:\nHumanitarian", "# Reasoning Path:\nMother Teresa -> people.person.nationality -> India -> location.country.languages_spoken -> English Language\n# Answer:\nIndia", "# Reasoning Path:\nMother Teresa -> people.person.nationality -> India -> base.aareas.schema.administrative_area.administrative_children -> West Bengal\n# Answer:\nIndia", "# Reasoning Path:\nMother Teresa -> base.activism.activist.area_of_activism -> Humanitarian -> common.topic.notable_types -> Profession\n# Answer:\nHumanitarian", "# Reasoning Path:\nMother Teresa -> base.activism.activist.area_of_activism -> Humanitarian -> people.profession.people_with_this_profession -> Albert Friedlander\n# Answer:\nHumanitarian", "# Reasoning Path:\nMother Teresa -> base.activism.activist.area_of_activism -> Humanitarian -> people.profession.people_with_this_profession -> Andrew Hunt\n# Answer:\nHumanitarian", "# Reasoning Path:\nMother Teresa -> film.film.release_date_s -> m.0v8_lfk -> film.film_regional_release_date.film_release_region -> United States of America\n# Answer:\nm.0v8_lfk"], "ground_truth": ["Ottoman Empire", "India"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> common.topic.notable_for -> g.125dysc88\n# Answer:\ng.125dysc88", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.team -> Carolina Panthers\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nm.0hpgnsj", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> common.topic.notable_types -> American football player\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0z23kt0 -> sports.sports_team_roster.team -> Auburn Tigers football\n# Answer:\nm.0z23kt0", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> people.person.children -> Cecil Newton\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> people.person.parents -> Jackie Newton -> common.topic.notable_for -> g.125dzwcd6\n# Answer:\nJackie Newton", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hpgnsj"], "ground_truth": ["Carolina Panthers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-340", "prediction": ["# Reasoning Path:\nRichard Pryor -> people.person.children -> Rain Pryor -> people.person.children -> Lotus Marie Pryor\n# Answer:\nRain Pryor", "# Reasoning Path:\nRichard Pryor -> common.topic.notable_for -> g.1257xht0m\n# Answer:\ng.1257xht0m", "# Reasoning Path:\nRichard Pryor -> people.person.children -> Richard Pryor Jr. -> people.person.gender -> Male\n# Answer:\nRichard Pryor Jr.", "# Reasoning Path:\nRichard Pryor -> people.person.children -> Franklin Pryor -> people.person.parents -> Geraldine Mason\n# Answer:\nFranklin Pryor", "# Reasoning Path:\nRichard Pryor -> people.person.children -> Rain Pryor -> people.person.sibling_s -> m.0t4tf6_\n# Answer:\nRain Pryor", "# Reasoning Path:\nRichard Pryor -> people.person.children -> Rain Pryor -> people.person.profession -> Actor\n# Answer:\nRain Pryor", "# Reasoning Path:\nRichard Pryor -> people.person.children -> Richard Pryor Jr. -> people.person.parents -> Patricia Price\n# Answer:\nRichard Pryor Jr.", "# Reasoning Path:\nRichard Pryor -> influence.influence_node.influenced -> Fancy Ray -> influence.influence_node.influenced_by -> James Brown\n# Answer:\nFancy Ray", "# Reasoning Path:\nRichard Pryor -> people.person.children -> Rain Pryor -> people.person.sibling_s -> m.0t4tf6v\n# Answer:\nRain Pryor", "# Reasoning Path:\nRichard Pryor -> people.person.children -> Richard Pryor Jr. -> people.person.sibling_s -> m.0t4tf8x\n# Answer:\nRichard Pryor Jr."], "ground_truth": ["Richard Pryor Jr.", "Franklin Pryor", "Steven Pryor"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.4, "ans_recall": 0.6666666666666666, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5, "path_ans_precision": 0.4, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-341", "prediction": ["# Reasoning Path:\nSpain -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Andorra\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nSpain -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Morocco\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\ng.1hhc37x4q", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60xs9d2\n# Answer:\ng.11b60xs9d2", "# Reasoning Path:\nSpain -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Belgium\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nSpain -> olympics.olympic_participating_country.olympics_participated_in -> 1900 Summer Olympics -> olympics.olympic_games.participating_countries -> France\n# Answer:\n1900 Summer Olympics", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc38hhk\n# Answer:\ng.1hhc38hhk", "# Reasoning Path:\nSpain -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Portugal\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc3dvnk\n# Answer:\ng.1hhc3dvnk", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.1hhc39shg\n# Answer:\ng.1hhc39shg"], "ground_truth": ["Morocco", "Gibraltar", "Andorra", "Portugal", "France"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.07339449541284405, "path_precision": 0.4, "path_recall": 0.04040404040404041, "path_ans_f1": 0.5333333333333333, "path_ans_precision": 0.4, "path_ans_recall": 0.8}
{"id": "WebQTest-342", "prediction": ["# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.locations.countries.continent -> North America\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.country.form_of_government -> Parliamentary system\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Gazankulu -> location.location.containedby -> South Africa\n# Answer:\nGazankulu", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Gazankulu -> common.topic.image -> SouthAfricaBantustanGazanku\n# Answer:\nGazankulu", "# Reasoning Path:\nEnglish Language -> education.school_category.schools_of_this_kind -> Piers Midwinter -> people.person.quotations -> Learn with a NEST. Please be my GUEST. I will give you my BEST,\n# Answer:\nPiers Midwinter", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> South Africa -> location.country.languages_spoken -> Arabic Language\n# Answer:\nSouth Africa", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Gazankulu -> common.topic.notable_for -> g.1255j1dmx\n# Answer:\nGazankulu", "# Reasoning Path:\nEnglish Language -> education.school_category.schools_of_this_kind -> Piers Midwinter -> common.topic.notable_types -> Organization founder\n# Answer:\nPiers Midwinter", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> South Africa -> location.country.form_of_government -> Parliamentary republic\n# Answer:\nSouth Africa"], "ground_truth": ["Swaziland", "Kenya", "Tuvalu", "New Zealand", "Gambia", "Papua New Guinea", "Kiribati", "Rwanda", "Zimbabwe", "Cook Islands", "Ghana", "Samoa", "Canada", "Territory of New Guinea", "Cayman Islands", "Vanuatu", "Nauru", "Barbados", "Marshall Islands", "Montserrat", "Tanzania", "Territory of Papua and New Guinea", "Zambia", "Uganda", "Botswana", "Cameroon", "Turks and Caicos Islands", "Hong Kong", "Guam", "Nigeria", "Republic of Ireland", "Bahamas", "Isle of Man", "Saint Vincent and the Grenadines", "United Kingdom", "Sudan", "Philippines", "Belize", "Gibraltar", "Namibia", "Singapore", "Guyana", "Malta", "Liberia", "Saint Kitts and Nevis", "India", "Lesotho", "Jersey", "Fiji", "Dominica", "Pakistan", "Puerto Rico", "Saint Lucia", "Wales", "Antigua and Barbuda", "Grenada", "Sierra Leone", "South Africa", "Bermuda"], "ans_acc": 0.03389830508474576, "ans_hit": 1, "ans_f1": 0.06349206349206349, "ans_precission": 0.5, "ans_recall": 0.03389830508474576, "path_f1": 0.06349206349206349, "path_precision": 0.5, "path_recall": 0.03389830508474576, "path_ans_f1": 0.0641711229946524, "path_ans_precision": 0.6, "path_ans_recall": 0.03389830508474576}
{"id": "WebQTest-343", "prediction": ["# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> Shanghai Cooperation Organisation -> organization.organization.founders -> Kyrgyzstan\n# Answer:\nShanghai Cooperation Organisation", "# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> Shanghai Cooperation Organisation -> organization.organization.founders -> Kazakhstan\n# Answer:\nShanghai Cooperation Organisation", "# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> UNESCO -> organization.organization.founders -> India\n# Answer:\nUNESCO", "# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> Shanghai Cooperation Organisation -> organization.membership_organization.members -> m.04dj3ty\n# Answer:\nShanghai Cooperation Organisation", "# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> Shanghai Cooperation Organisation -> organization.organization.founders -> Russia\n# Answer:\nShanghai Cooperation Organisation", "# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> BRICS -> organization.organization.founders -> Brazil\n# Answer:\nBRICS", "# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> BRICS -> common.topic.notable_types -> Organization\n# Answer:\nBRICS", "# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> UNESCO -> organization.organization.founders -> United States of America\n# Answer:\nUNESCO", "# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> UNESCO -> organization.membership_organization.members -> m.0w04xd_\n# Answer:\nUNESCO", "# Reasoning Path:\nChina -> location.location.partially_contains -> Amur River -> location.location.partially_containedby -> Russia\n# Answer:\nAmur River"], "ground_truth": ["Caribbean Development Bank", "G-20 major economies", "World Bank", "Asia-Pacific Economic Cooperation", "Asian Development Bank", "UNESCO", "United Nations", "African Development Bank", "Shanghai Cooperation Organisation"], "ans_acc": 0.2222222222222222, "ans_hit": 1, "ans_f1": 0.3373493975903614, "ans_precission": 0.7, "ans_recall": 0.2222222222222222, "path_f1": 0.3373493975903614, "path_precision": 0.7, "path_recall": 0.2222222222222222, "path_ans_f1": 0.3373493975903614, "path_ans_precision": 0.7, "path_ans_recall": 0.2222222222222222}
{"id": "WebQTest-344", "prediction": ["# Reasoning Path:\nWest St. Paul -> location.location.containedby -> Dakota County -> location.us_county.hud_county_place -> Dakota County / County Place 2703799999\n# Answer:\nDakota County", "# Reasoning Path:\nWest St. Paul -> location.location.containedby -> Dakota County -> location.us_county.hud_county_place -> Mendota\n# Answer:\nDakota County", "# Reasoning Path:\nWest St. Paul -> location.location.containedby -> Dakota County -> location.location.containedby -> Minnesota\n# Answer:\nDakota County", "# Reasoning Path:\nWest St. Paul -> location.location.containedby -> Dakota County -> location.us_county.hud_county_place -> Apple Valley\n# Answer:\nDakota County", "# Reasoning Path:\nWest St. Paul -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nWest St. Paul -> location.statistical_region.population -> g.11b66g3pg3\n# Answer:\ng.11b66g3pg3", "# Reasoning Path:\nWest St. Paul -> location.location.containedby -> Dakota County -> location.location.containedby -> Minneapolis-St. Paul-Bloomington, MN-WI Metropolitan Statistical Area\n# Answer:\nDakota County", "# Reasoning Path:\nWest St. Paul -> location.location.containedby -> Minnesota -> location.location.containedby -> Contiguous United States\n# Answer:\nMinnesota", "# Reasoning Path:\nWest St. Paul -> location.location.containedby -> Dakota County -> location.statistical_region.population -> g.11b66j64vw\n# Answer:\nDakota County", "# Reasoning Path:\nWest St. Paul -> location.hud_county_place.county -> Dakota County -> location.us_county.hud_county_place -> Dakota County / County Place 2703799999\n# Answer:\nDakota County"], "ground_truth": ["Dakota County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-346", "prediction": ["# Reasoning Path:\nThe Sixth Sense -> film.film.starring -> m.0k6gz9 -> film.performance.character -> Vincent Grey\n# Answer:\nm.0k6gz9", "# Reasoning Path:\nDonnie Wahlberg -> film.actor.film -> m.0k6gz9 -> film.performance.character -> Vincent Grey\n# Answer:\nm.0k6gz9", "# Reasoning Path:\nThe Sixth Sense -> film.film.starring -> m.0k6gz9 -> film.performance.actor -> Donnie Wahlberg\n# Answer:\nm.0k6gz9", "# Reasoning Path:\nThe Sixth Sense -> film.film.starring -> m.03jq0nf -> film.performance.character -> Kyra Collins\n# Answer:\nm.03jq0nf", "# Reasoning Path:\nThe Sixth Sense -> film.film.starring -> m.03jrcw6 -> film.performance.character -> Sean\n# Answer:\nm.03jrcw6", "# Reasoning Path:\nDonnie Wahlberg -> film.actor.film -> m.02tbg6_ -> film.performance.character -> Mr. Grey\n# Answer:\nm.02tbg6_", "# Reasoning Path:\nDonnie Wahlberg -> film.actor.film -> m.0k6gz9 -> film.performance.film -> The Sixth Sense\n# Answer:\nm.0k6gz9", "# Reasoning Path:\nDonnie Wahlberg -> film.actor.film -> m.02vc00j -> film.performance.character -> Danny Quinn\n# Answer:\nm.02vc00j", "# Reasoning Path:\nThe Sixth Sense -> film.film.starring -> m.03jq0nf -> film.performance.actor -> Mischa Barton\n# Answer:\nm.03jq0nf", "# Reasoning Path:\nDonnie Wahlberg -> people.person.place_of_birth -> Dorchester -> location.location.people_born_here -> Aaron Maund\n# Answer:\nDorchester"], "ground_truth": ["Vincent Grey"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-347", "prediction": ["# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Jean-Jacques Rousseau -> influence.influence_node.influenced -> Georg Wilhelm Friedrich Hegel\n# Answer:\nJean-Jacques Rousseau", "# Reasoning Path:\nMontesquieu -> common.topic.notable_for -> g.125fyk02t\n# Answer:\ng.125fyk02t", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Georg Wilhelm Friedrich Hegel -> base.kwebbase.kwtopic.connections_to -> arthur schopenhauer opposed views of georg wilhelm friedrich hegel\n# Answer:\nGeorg Wilhelm Friedrich Hegel", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Jeremy Bentham -> influence.influence_node.influenced -> Atilla Yayla\n# Answer:\nJeremy Bentham", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Georg Wilhelm Friedrich Hegel -> people.person.place_of_birth -> Stuttgart\n# Answer:\nGeorg Wilhelm Friedrich Hegel", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Georg Wilhelm Friedrich Hegel -> base.kwebbase.kwtopic.connections_to -> frederick wilhelm joseph von schelling a pal of georg wilhelm friedrich hegel\n# Answer:\nGeorg Wilhelm Friedrich Hegel", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Georg Wilhelm Friedrich Hegel -> base.kwebbase.kwtopic.has_sentences -> All Hegel's later work can, to a large extent, be seen as an attempt to intellectualize his conviction of the Absolute as the \\\"geist\\\" (the spirit).\n# Answer:\nGeorg Wilhelm Friedrich Hegel", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Jeremy Bentham -> influence.influence_node.influenced -> David Pearce\n# Answer:\nJeremy Bentham", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Jean-Jacques Rousseau -> influence.influence_node.influenced -> Edmund Burke\n# Answer:\nJean-Jacques Rousseau", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Georg Wilhelm Friedrich Hegel -> base.kwebbase.kwtopic.connections_to -> giuseppe piazzi opposed by georg wilhelm friedrich hegel\n# Answer:\nGeorg Wilhelm Friedrich Hegel"], "ground_truth": ["John Adams", "Jean-Jacques Rousseau", "Paul F\u00e9val, p\u00e8re", "Roberto Mangabeira Unger", "Thomas Paine", "\u00c9mile Durkheim", "Adam Smith", "Friedrich Hayek", "Isaiah Berlin", "Edmund Burke", "James Madison", "Hannah Arendt", "Edward Gibbon", "David Hume", "Clive James", "Louis Althusser", "Georg Wilhelm Friedrich Hegel", "Alexis de Tocqueville", "Thomas Jefferson", "William Blackstone", "Jeremy Bentham"], "ans_acc": 0.19047619047619047, "ans_hit": 1, "ans_f1": 0.24657534246575338, "ans_precission": 0.9, "ans_recall": 0.14285714285714285, "path_f1": 0.24657534246575338, "path_precision": 0.9, "path_recall": 0.14285714285714285, "path_ans_f1": 0.31441048034934493, "path_ans_precision": 0.9, "path_ans_recall": 0.19047619047619047}
{"id": "WebQTest-349", "prediction": ["# Reasoning Path:\nCarlos Boozer -> basketball.basketball_player.player_statistics -> m.04qgd67 -> basketball.basketball_player_stats.team -> Cleveland Cavaliers\n# Answer:\nm.04qgd67", "# Reasoning Path:\nCarlos Boozer -> basketball.basketball_player.player_statistics -> m.04qjzy0 -> basketball.basketball_player_stats.team -> Utah Jazz\n# Answer:\nm.04qjzy0", "# Reasoning Path:\nCarlos Boozer -> basketball.basketball_player.player_statistics -> m.04qht47 -> basketball.basketball_player_stats.team -> Utah Jazz\n# Answer:\nm.04qht47", "# Reasoning Path:\nCarlos Boozer -> basketball.basketball_player.player_statistics -> m.04qgd67 -> basketball.basketball_player_stats.season -> 2002\u201303 NBA season\n# Answer:\nm.04qgd67", "# Reasoning Path:\nCarlos Boozer -> basketball.basketball_player.player_statistics -> m.04qjzy0 -> basketball.basketball_player_stats.season -> 2005\u201306 NBA season\n# Answer:\nm.04qjzy0", "# Reasoning Path:\nCarlos Boozer -> base.schemastaging.athlete_extra.salary -> m.0j2hs7x -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0j2hs7x", "# Reasoning Path:\nCarlos Boozer -> basketball.basketball_player.player_statistics -> m.04qht47 -> basketball.basketball_player_stats.season -> 2007\u201308 NBA season\n# Answer:\nm.04qht47", "# Reasoning Path:\nCarlos Boozer -> base.schemastaging.athlete_extra.salary -> m.0j2hs7x -> base.schemastaging.athlete_salary.team -> Chicago Bulls\n# Answer:\nm.0j2hs7x", "# Reasoning Path:\nCarlos Boozer -> sports.pro_athlete.teams -> m.0115f60_ -> sports.sports_team_roster.team -> Los Angeles Lakers\n# Answer:\nm.0115f60_", "# Reasoning Path:\nCarlos Boozer -> sports.pro_athlete.teams -> m.0115f60_ -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0115f60_"], "ground_truth": ["Utah Jazz", "Cleveland Cavaliers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.location.containedby -> Bethesda-Frederick-Gaithersburg, MD Metropolitan Division\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.location.containedby -> Maryland\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.rent50_1 -> m.05gcgl3\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.containedby -> United States of America\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.population -> g.11b66h2c0w\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.location.containedby -> Bethesda-Frederick-Gaithersburg, MD Metropolitan Division\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.containedby -> United States, with Territories\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.statistical_region.rent50_1 -> m.05gcgl3\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.population -> g.11x1cfntl\n# Answer:\nFrederick County"], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-351", "prediction": ["# Reasoning Path:\nPortland Press Herald -> book.newspaper.circulation_areas -> Portland -> travel.travel_destination.tourist_attractions -> Portland Museum of Art\n# Answer:\nPortland", "# Reasoning Path:\nPortland Press Herald -> book.newspaper.circulation_areas -> Portland -> travel.travel_destination.tourist_attractions -> Lenny, the Chocolate Moose\n# Answer:\nPortland", "# Reasoning Path:\nPortland Press Herald -> book.newspaper.circulation_areas -> Portland -> location.location.containedby -> Maine\n# Answer:\nPortland", "# Reasoning Path:\nPortland Press Herald -> book.newspaper.circulation_areas -> Portland -> travel.travel_destination.tourist_attractions -> 7 Wonders of God Creatures\n# Answer:\nPortland", "# Reasoning Path:\nPortland Press Herald -> book.newspaper.owner -> Blethen Maine Newspapers, Inc. -> common.topic.notable_for -> g.1259lvkfc\n# Answer:\nBlethen Maine Newspapers, Inc.", "# Reasoning Path:\nPortland Press Herald -> common.topic.notable_types -> Newspaper -> freebase.type_hints.included_types -> Topic\n# Answer:\nNewspaper", "# Reasoning Path:\nPortland Press Herald -> book.newspaper.circulation_areas -> Portland -> location.location.containedby -> Cumberland County\n# Answer:\nPortland", "# Reasoning Path:\nPortland Press Herald -> book.newspaper.circulation_areas -> Southern Maine Coast -> common.topic.image -> South Coast (Maine)\n# Answer:\nSouthern Maine Coast", "# Reasoning Path:\nPortland Press Herald -> book.newspaper.circulation_areas -> Portland -> location.location.containedby -> United States of America\n# Answer:\nPortland", "# Reasoning Path:\nPortland Press Herald -> book.newspaper.circulation_areas -> Portland -> location.statistical_region.population -> g.11b66kbfvh\n# Answer:\nPortland"], "ground_truth": ["Blethen Maine Newspapers, Inc."], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-352", "prediction": ["# Reasoning Path:\nAlbert Speer -> architecture.architect.structures_designed -> Reich Chancellery -> location.location.containedby -> Berlin\n# Answer:\nReich Chancellery", "# Reasoning Path:\nAlbert Speer -> architecture.architect.structures_designed -> Reich Chancellery -> architecture.structure.architectural_style -> Nazi architecture\n# Answer:\nReich Chancellery", "# Reasoning Path:\nAlbert Speer -> architecture.architect.structures_designed -> Olympiastadion -> common.topic.image -> 2009-04-07 Berlin Olympiastadion 539\n# Answer:\nOlympiastadion", "# Reasoning Path:\nAlbert Speer -> architecture.architect.structures_designed -> Reich Chancellery -> common.topic.image -> 1895 reichskanzlerpalais\n# Answer:\nReich Chancellery", "# Reasoning Path:\nAlbert Speer -> architecture.architect.structures_designed -> Olympiastadion -> common.topic.notable_types -> Olympic venue\n# Answer:\nOlympiastadion", "# Reasoning Path:\nAlbert Speer -> architecture.architect.structures_designed -> Deutsches Stadion -> common.topic.image -> Deutsches stadion nuremberg model\n# Answer:\nDeutsches Stadion", "# Reasoning Path:\nAlbert Speer -> architecture.architect.structures_designed -> Deutsches Stadion -> common.topic.notable_types -> Building\n# Answer:\nDeutsches Stadion", "# Reasoning Path:\nAlbert Speer -> people.person.children -> Albert Speer, Jr. -> architecture.architect.structures_designed -> Saoud bin Abdulrahman Stadium\n# Answer:\nAlbert Speer, Jr.", "# Reasoning Path:\nAlbert Speer -> architecture.architect.structures_designed -> Olympiastadion -> common.topic.image -> Olympicstadium2\n# Answer:\nOlympiastadion", "# Reasoning Path:\nAlbert Speer -> people.person.parents -> Albert Friedrich Speer -> common.topic.notable_types -> Deceased Person\n# Answer:\nAlbert Friedrich Speer"], "ground_truth": ["Deutsches Stadion", "Volkshalle", "Reich Chancellery", "Olympiastadion"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.8, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.8, "path_ans_recall": 0.75}
{"id": "WebQTest-354", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> Pastel Art -> visual_art.art_period_movement.associated_artists -> Edgar Degas\n# Answer:\nPastel Art", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> Pastel Art -> common.topic.notable_for -> g.11b75r8htr\n# Answer:\nPastel Art", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Raphael and his age\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> Pastel Art -> visual_art.art_period_movement.associated_artists -> Maurice Quentin de La Tour\n# Answer:\nPastel Art", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> Pastel Art -> visual_art.art_period_movement.associated_artworks -> Henry Dawkins\n# Answer:\nPastel Art", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> Pastel Art -> visual_art.art_period_movement.associated_artists -> Rosalba Carriera\n# Answer:\nPastel Art", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> High Renaissance -> visual_art.art_period_movement.associated_artists -> Sala\u00ec\n# Answer:\nHigh Renaissance", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> High Renaissance -> base.schemastaging.context_name.pronunciation -> g.125_krwf6\n# Answer:\nHigh Renaissance", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> people.profession.people_with_this_profession -> Sandro Botticelli\n# Answer:\nPainting"], "ground_truth": ["High Renaissance", "Pastel Art"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-356", "prediction": ["# Reasoning Path:\nArlington -> location.location.nearby_airports -> Arlington Municipal Airport -> aviation.airport.serves -> TownePlace Suites Arlington Near Six Flags\n# Answer:\nArlington Municipal Airport", "# Reasoning Path:\nArlington -> location.location.nearby_airports -> Arlington Municipal Airport -> common.topic.article -> m.02pq2dc\n# Answer:\nArlington Municipal Airport", "# Reasoning Path:\nArlington -> location.statistical_region.population -> g.11b66bywm8\n# Answer:\ng.11b66bywm8", "# Reasoning Path:\nArlington -> location.location.nearby_airports -> Arlington Municipal Airport -> location.location.containedby -> 76018\n# Answer:\nArlington Municipal Airport", "# Reasoning Path:\nArlington -> location.statistical_region.population -> g.11b7td7_9_\n# Answer:\ng.11b7td7_9_", "# Reasoning Path:\nArlington -> location.location.nearby_airports -> Arlington Municipal Airport -> location.location.containedby -> Tarrant County\n# Answer:\nArlington Municipal Airport", "# Reasoning Path:\nArlington -> location.statistical_region.population -> g.11b7tm_xv7\n# Answer:\ng.11b7tm_xv7", "# Reasoning Path:\nArlington -> location.location.contains -> 22202 -> location.postal_code.country -> United States of America\n# Answer:\n22202", "# Reasoning Path:\nArlington -> location.location.contains -> 22202 -> location.location.geometry -> m.056zbj1\n# Answer:\n22202", "# Reasoning Path:\nArlington -> location.location.contains -> AMC The Parks At Arlington 18 -> architecture.building.occupant -> m.0pdlnd9\n# Answer:\nAMC The Parks At Arlington 18"], "ground_truth": ["Arlington Municipal Airport"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-359", "prediction": ["# Reasoning Path:\nEmile Heskey -> soccer.football_player.statistics -> m.0w90221 -> soccer.football_player_stats.team -> Aston Villa F.C.\n# Answer:\nm.0w90221", "# Reasoning Path:\nEmile Heskey -> sports.pro_athlete.teams -> m.05ckn9m -> sports.sports_team_roster.team -> Aston Villa F.C.\n# Answer:\nm.05ckn9m", "# Reasoning Path:\nEmile Heskey -> soccer.football_player.statistics -> m.0w8wcgz -> soccer.football_player_stats.team -> Wigan Athletic F.C.\n# Answer:\nm.0w8wcgz", "# Reasoning Path:\nEmile Heskey -> sports.pro_athlete.teams -> m.05ckn9m -> sports.sports_team_roster.position -> Forward\n# Answer:\nm.05ckn9m", "# Reasoning Path:\nEmile Heskey -> sports.pro_athlete.teams -> m.04m0_rf -> sports.sports_team_roster.team -> Wigan Athletic F.C.\n# Answer:\nm.04m0_rf", "# Reasoning Path:\nEmile Heskey -> soccer.football_player.statistics -> m.0w9mysy -> soccer.football_player_stats.team -> Newcastle Jets FC\n# Answer:\nm.0w9mysy", "# Reasoning Path:\nEmile Heskey -> sports.pro_athlete.teams -> m.0n3jm1r -> sports.sports_team_roster.team -> Newcastle Jets FC\n# Answer:\nm.0n3jm1r", "# Reasoning Path:\nEmile Heskey -> base.schemastaging.athlete_extra.salary -> m.0kdmv43 -> base.schemastaging.athlete_salary.team -> Aston Villa F.C.\n# Answer:\nm.0kdmv43", "# Reasoning Path:\nEmile Heskey -> sports.pro_athlete.teams -> m.04m0_rf -> sports.sports_team_roster.position -> Forward\n# Answer:\nm.04m0_rf", "# Reasoning Path:\nEmile Heskey -> base.schemastaging.athlete_extra.salary -> m.0kdmv43 -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nm.0kdmv43"], "ground_truth": ["Aston Villa F.C.", "Newcastle Jets FC"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.588235294117647, "path_precision": 0.5, "path_recall": 0.7142857142857143, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nm.0lwxmy1", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> University of Alabama School of Law\n# Answer:\nm.0n1l46h", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.place_of_birth -> Los Angeles\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138 -> education.education.institution -> University of Alabama\n# Answer:\nm.04hx138", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.gender -> Female\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_types -> Book Edition\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_for -> g.125920htw\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Matar Un Ruisenor -> book.book_edition.book -> To Kill a Mockingbird\n# Answer:\nMatar Un Ruisenor", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nHarper Seven Beckham"], "ground_truth": ["Monroe County High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-360", "prediction": ["# Reasoning Path:\nHirohito -> people.person.place_of_birth -> Aoyama, Minato, Tokyo -> location.location.containedby -> Tokyo\n# Answer:\nAoyama, Minato, Tokyo", "# Reasoning Path:\nHirohito -> people.deceased_person.place_of_death -> Tokyo -> location.location.containedby -> Japan\n# Answer:\nTokyo", "# Reasoning Path:\nHirohito -> people.person.place_of_birth -> Aoyama, Minato, Tokyo -> location.neighborhood.neighborhood_of -> Tokyo\n# Answer:\nAoyama, Minato, Tokyo", "# Reasoning Path:\nHirohito -> people.person.place_of_birth -> Aoyama, Minato, Tokyo -> location.location.people_born_here -> Junichi Sasai\n# Answer:\nAoyama, Minato, Tokyo", "# Reasoning Path:\nHirohito -> people.deceased_person.place_of_death -> Tokyo -> base.aareas.schema.administrative_area.capital -> Shinjuku\n# Answer:\nTokyo", "# Reasoning Path:\nHirohito -> people.person.place_of_birth -> Aoyama, Minato, Tokyo -> location.location.people_born_here -> Kenichi Enomoto\n# Answer:\nAoyama, Minato, Tokyo", "# Reasoning Path:\nHirohito -> people.deceased_person.place_of_death -> Tokyo -> travel.travel_destination.local_transportation -> East Japan Railway Company\n# Answer:\nTokyo", "# Reasoning Path:\nHirohito -> people.person.place_of_birth -> Aoyama, Minato, Tokyo -> location.location.people_born_here -> Seinosuke Nakajima\n# Answer:\nAoyama, Minato, Tokyo", "# Reasoning Path:\nHirohito -> people.person.sibling_s -> m.0k6lh0d -> people.sibling_relationship.sibling -> Nobuhito, Prince Takamatsu\n# Answer:\nm.0k6lh0d", "# Reasoning Path:\nHirohito -> people.deceased_person.place_of_death -> Tokyo -> travel.travel_destination.local_transportation -> Tokyo Metro\n# Answer:\nTokyo"], "ground_truth": ["Aoyama, Minato, Tokyo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-361", "prediction": ["# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Sch\u00f6nbrunn Palace -> film.film_location.featured_in_films -> Ecstasy\n# Answer:\nSch\u00f6nbrunn Palace", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Sch\u00f6nbrunn Palace -> location.location.geolocation -> m.02_s799\n# Answer:\nSch\u00f6nbrunn Palace", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Sch\u00f6nbrunn Palace -> travel.travel_destination.tour_operators -> Adventures by Disney - Austria, Germany, Czech Republic Vacation\n# Answer:\nSch\u00f6nbrunn Palace", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> St. Peter's Church -> location.location.containedby -> Austria\n# Answer:\nSt. Peter's Church", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Albertina -> architecture.building.building_function -> Palace\n# Answer:\nAlbertina", "# Reasoning Path:\nVienna -> government.governmental_jurisdiction.governing_officials -> m.09c1mpv -> government.government_position_held.office_position_or_title -> Reich Governor\n# Answer:\nm.09c1mpv", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> St. Peter's Church -> common.topic.image -> Peterskirche Vienna Sept 2006 001\n# Answer:\nSt. Peter's Church", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Albertina -> architecture.building.building_function -> Museum\n# Answer:\nAlbertina", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Albertina -> common.topic.webpage -> m.03l3z9k\n# Answer:\nAlbertina", "# Reasoning Path:\nVienna -> government.governmental_jurisdiction.governing_officials -> m.09c1mpv -> government.government_position_held.appointed_by -> Adolf Hitler\n# Answer:\nm.09c1mpv"], "ground_truth": ["Schottenkirche, Vienna", "Karlskirche, Vienna", "Sch\u00f6nbrunn Palace", "Leopold Museum", "St. Peter's Church", "Kunsthistorisches Museum", "Belvedere, Vienna", "Rock im Park", "Naturhistorisches Museum", "Capuchin Church, Vienna", "Albertina", "Jesuit Church, Vienna", "Liechtenstein Museum", "Haus der Musik", "Augustinian Church, Vienna", "Schloss Hof", "Prater", "Vienna State Opera", "Millennium Tower", "Hofburg Palace", "mumok", "Minoritenkirche", "St. Stephen's Cathedral, Vienna", "Rathaus, Vienna", "Vienna International Centre", "Imperial Crypt", "Vienna Observatory", "Sigmund Freud Museum", "Museum of Military History, Vienna"], "ans_acc": 0.10344827586206896, "ans_hit": 1, "ans_f1": 0.183206106870229, "ans_precission": 0.8, "ans_recall": 0.10344827586206896, "path_f1": 0.1889763779527559, "path_precision": 0.8, "path_recall": 0.10714285714285714, "path_ans_f1": 0.183206106870229, "path_ans_precision": 0.8, "path_ans_recall": 0.10344827586206896}
{"id": "WebQTest-362", "prediction": ["# Reasoning Path:\nIreland -> location.location.events -> 1999 Cricket World Cup -> time.event.locations -> England\n# Answer:\n1999 Cricket World Cup", "# Reasoning Path:\nIreland -> location.location.events -> 1999 Cricket World Cup -> time.event.next_in_series -> 2003 Cricket World Cup\n# Answer:\n1999 Cricket World Cup", "# Reasoning Path:\nIreland -> location.location.events -> 1999 Cricket World Cup -> time.event.locations -> Wales\n# Answer:\n1999 Cricket World Cup", "# Reasoning Path:\nIreland -> location.location.events -> 1999 Cricket World Cup -> common.topic.image -> Cricket World Cup trophy 2\n# Answer:\n1999 Cricket World Cup", "# Reasoning Path:\nIreland -> location.location.events -> 2008 Nicky Rackard Cup -> common.topic.article -> m.03hgw2x\n# Answer:\n2008 Nicky Rackard Cup", "# Reasoning Path:\nIreland -> location.location.events -> 1999 Cricket World Cup -> time.event.locations -> Netherlands\n# Answer:\n1999 Cricket World Cup", "# Reasoning Path:\nIreland -> location.location.events -> 2008 Nicky Rackard Cup -> common.topic.notable_for -> g.1ylqcrxvw\n# Answer:\n2008 Nicky Rackard Cup", "# Reasoning Path:\nIreland -> organization.organization_scope.organizations_with_this_scope -> 32 County Sovereignty Movement -> organization.organization.founders -> Bernadette Sands McKevitt\n# Answer:\n32 County Sovereignty Movement", "# Reasoning Path:\nIreland -> location.location.events -> 2008 Nicky Rackard Cup -> time.event.instance_of_recurring_event -> Nicky Rackard Cup\n# Answer:\n2008 Nicky Rackard Cup", "# Reasoning Path:\nIreland -> organization.organization_scope.organizations_with_this_scope -> Amalgamated Society of Carpenters and Joiners -> organization.organization.geographic_scope -> Australia\n# Answer:\nAmalgamated Society of Carpenters and Joiners"], "ground_truth": ["Desmond Rebellions", "Guerrilla phase of the Irish Civil War", "Irish War of Independence", "Battle of Ros-Mhic-Thri\u00fain", "Irish Rebellion of 1641", "Irish Free State offensive", "Siege of Wexford", "Siege of Smerwick", "Sack of Dun Gallimhe", "Bruce campaign in Ireland", "Battle of Glentaisie", "Siege of Waterford", "Battle of the Curragh", "Norman invasion of Ireland", "Irish Civil War", "Battle of Knockdoe", "Irish Rebellion of 1798", "Exp\u00e9dition d'Irlande", "Cromwellian conquest of Ireland", "Planned French invasion of Britain", "Battle of Belahoe", "Second Desmond Rebellion", "Wars of the Three Kingdoms", "Williamite War in Ireland", "Irish Confederate Wars", "Anglo-Spanish War", "Nine Years' War", "Battle of Tochar Cruachain-Bri-Ele", "Siege of Drogheda", "Battle of Affane", "Jacobite risings"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-365", "prediction": ["# Reasoning Path:\nPeoria -> travel.travel_destination.tourist_attractions -> Dozer Park -> common.topic.notable_types -> Sports Facility\n# Answer:\nDozer Park", "# Reasoning Path:\nPeoria -> travel.travel_destination.tourist_attractions -> Wildlife Prairie State Park -> location.location.geolocation -> m.0cq_w11\n# Answer:\nWildlife Prairie State Park", "# Reasoning Path:\nPeoria -> location.location.contains -> John C. Flanagan House Museum -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.05h82bq\n# Answer:\nJohn C. Flanagan House Museum", "# Reasoning Path:\nPeoria -> travel.travel_destination.tourist_attractions -> John C. Flanagan House Museum -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.05h82bq\n# Answer:\nJohn C. Flanagan House Museum", "# Reasoning Path:\nPeoria -> location.location.contains -> John C. Flanagan House Museum -> location.location.containedby -> Illinois\n# Answer:\nJohn C. Flanagan House Museum", "# Reasoning Path:\nPeoria -> location.statistical_region.population -> g.11b66dqm1b\n# Answer:\ng.11b66dqm1b", "# Reasoning Path:\nPeoria -> travel.travel_destination.tourist_attractions -> Dozer Park -> location.location.containedby -> 61602\n# Answer:\nDozer Park", "# Reasoning Path:\nPeoria -> travel.travel_destination.tourist_attractions -> Wildlife Prairie State Park -> common.topic.notable_for -> g.125f33mst\n# Answer:\nWildlife Prairie State Park", "# Reasoning Path:\nPeoria -> travel.travel_destination.tourist_attractions -> John C. Flanagan House Museum -> location.location.containedby -> Illinois\n# Answer:\nJohn C. Flanagan House Museum", "# Reasoning Path:\nPeoria -> location.location.contains -> John C. Flanagan House Museum -> base.usnris.nris_listing.significance_level -> State\n# Answer:\nJohn C. Flanagan House Museum"], "ground_truth": ["WeaverRidge Golf Club", "Lakeview Museum of Arts and Sciences", "Peoria Civic Center", "Par-A-Dice Hotel and Casino", "Wildlife Prairie State Park", "John C. Flanagan House Museum", "George L. Luthy Memorial Botanical Garden", "Peoria Zoo", "Dozer Park", "Heart of Illinois Fair"], "ans_acc": 0.3, "ans_hit": 1, "ans_f1": 0.45000000000000007, "ans_precission": 0.9, "ans_recall": 0.3, "path_f1": 0.26666666666666666, "path_precision": 0.4, "path_recall": 0.2, "path_ans_f1": 0.45000000000000007, "path_ans_precision": 0.9, "path_ans_recall": 0.3}
{"id": "WebQTest-366", "prediction": ["# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> British Sign Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nBritish Sign Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Irish -> language.human_language.main_country -> Republic of Ireland\n# Answer:\nIrish", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> British Sign Language -> common.topic.notable_types -> Human Language\n# Answer:\nBritish Sign Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> British Sign Language -> language.human_language.region -> Europe\n# Answer:\nBritish Sign Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.countries_spoken_in -> Scotland\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Irish -> language.human_language.countries_spoken_in -> Northern Ireland\n# Answer:\nIrish", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.language_family -> Indo-European languages\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nUnited Kingdom -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nm.0ghc35h", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.countries_spoken_in -> Kingdom of Great Britain\n# Answer:\nScottish Gaelic"], "ground_truth": ["Scottish Gaelic", "Esperanto Language", "British Sign Language", "Welsh Language", "English Language", "Cornish Language", "Scots Language", "Irish"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6428571428571429, "ans_precission": 0.9, "ans_recall": 0.5, "path_f1": 0.6428571428571429, "path_precision": 0.9, "path_recall": 0.5, "path_ans_f1": 0.6428571428571429, "path_ans_precision": 0.9, "path_ans_recall": 0.5}
{"id": "WebQTest-367", "prediction": ["# Reasoning Path:\nElvis Presley -> people.person.profession -> Musician -> freebase.equivalent_topic.equivalent_type -> Musical Artist\n# Answer:\nMusician", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Soldier\n# Answer:\nSoldier", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Musician -> people.profession.specializations -> Singer\n# Answer:\nMusician", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Musician -> common.topic.subject_of -> Stephen Melton\n# Answer:\nMusician", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Singer -> common.topic.subject_of -> Alan Motley\n# Answer:\nSinger", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Singer -> people.profession.specializations -> Singer-songwriter\n# Answer:\nSinger", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Singer -> people.profession.specialization_of -> Musician\n# Answer:\nSinger", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Musician -> people.profession.specializations -> Singer-songwriter\n# Answer:\nMusician", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Singer -> common.topic.subject_of -> Justin Bieber\n# Answer:\nSinger", "# Reasoning Path:\nElvis Presley -> common.topic.webpage -> MusicMoz page for Elvis Presley -> common.webpage.category -> Topic Webpage\n# Answer:\nMusicMoz page for Elvis Presley"], "ground_truth": ["Musician", "Singer", "Soldier", "Actor"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> book.book_subject.works -> Sagebrush country -> common.topic.notable_types -> Book\n# Answer:\nSagebrush country", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.010flwmg", "# Reasoning Path:\nUtah -> book.book_subject.works -> Sagebrush country -> common.topic.notable_for -> g.125c_wfqj\n# Answer:\nSagebrush country", "# Reasoning Path:\nUtah -> book.book_subject.works -> Blossoms of faith -> common.topic.notable_for -> g.125fjpyr7\n# Answer:\nBlossoms of faith", "# Reasoning Path:\nUtah -> book.book_subject.works -> Sagebrush country -> book.book.editions -> Sagebrush country: land and the American West\n# Answer:\nSagebrush country", "# Reasoning Path:\nUtah -> book.book_subject.works -> Bound for Canaan -> book.written_work.author -> Margaret Blair Young\n# Answer:\nBound for Canaan", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> government.government_position_held.office_position_or_title -> Lieutenant Governor of Utah\n# Answer:\nm.010flwmg"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-370", "prediction": ["# Reasoning Path:\nNetherlands -> location.country.languages_spoken -> Dutch Language -> language.human_language.main_country -> Cura\u00e7ao\n# Answer:\nDutch Language", "# Reasoning Path:\nNetherlands -> location.country.official_language -> Dutch Language -> language.human_language.main_country -> Cura\u00e7ao\n# Answer:\nDutch Language", "# Reasoning Path:\nNetherlands -> location.country.languages_spoken -> Frisian languages -> language.human_language.region -> Friesland\n# Answer:\nFrisian languages", "# Reasoning Path:\nNetherlands -> location.country.languages_spoken -> Frisian languages -> language.human_language.language_family -> Germanic languages\n# Answer:\nFrisian languages", "# Reasoning Path:\nNetherlands -> location.statistical_region.gni_in_ppp_dollars -> g.11b60prvm_\n# Answer:\ng.11b60prvm_", "# Reasoning Path:\nNetherlands -> location.country.languages_spoken -> Frisian languages -> language.human_language.language_family -> Indo-European languages\n# Answer:\nFrisian languages", "# Reasoning Path:\nNetherlands -> location.country.languages_spoken -> Dutch Language -> media_common.netflix_genre.titles -> Antonia's Line\n# Answer:\nDutch Language", "# Reasoning Path:\nNetherlands -> location.country.official_language -> Dutch Language -> media_common.netflix_genre.titles -> Antonia's Line\n# Answer:\nDutch Language", "# Reasoning Path:\nNetherlands -> location.statistical_region.gni_in_ppp_dollars -> g.1245_55zz\n# Answer:\ng.1245_55zz", "# Reasoning Path:\nNetherlands -> location.country.languages_spoken -> Frisian languages -> language.human_language.language_family -> West Germanic languages\n# Answer:\nFrisian languages"], "ground_truth": ["West Flemish", "Frisian languages", "Dutch Language"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7272727272727272, "ans_precission": 0.8, "ans_recall": 0.6666666666666666, "path_f1": 0.631578947368421, "path_precision": 0.6, "path_recall": 0.6666666666666666, "path_ans_f1": 0.7272727272727272, "path_ans_precision": 0.8, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-371", "prediction": ["# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.main_country -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.region -> Southeast Europe\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Italy\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Greek Language -> media_common.netflix_genre.titles -> A Girl in Black\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Greek Language -> language.human_language.region -> Europe\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> media_common.netflix_genre.titles -> A Girl in Black\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> language.human_language.region -> Europe\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Montenegro\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Greek Language -> media_common.netflix_genre.titles -> A Matter of Dignity\n# Answer:\nGreek Language"], "ground_truth": ["Greek Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-374", "prediction": ["# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Collage -> visual_art.visual_art_form.artists -> Aberjhani\n# Answer:\nCollage", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Collage -> visual_art.visual_art_form.artists -> Adam Pendleton\n# Answer:\nCollage", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Eric Ravilious\n# Answer:\nDrawing", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Drawing -> business.consumer_product.brand -> Eyecandyair\n# Answer:\nDrawing", "# Reasoning Path:\nHenri Matisse -> book.book_subject.works -> Matisse -> common.topic.notable_for -> g.125dhx5l1\n# Answer:\nMatisse", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Collage -> visual_art.visual_art_form.artists -> Aleksei Kruchenykh\n# Answer:\nCollage", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Collage -> media_common.literary_genre.books_in_this_genre -> Atentat la bunele tabieturi. H\u00e2rtii lipite. Frontispiciu poema Lumina \u00een relief\n# Answer:\nCollage", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Printmaking -> visual_art.visual_art_form.artworks -> Dam\n# Answer:\nPrintmaking", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Henry Moore\n# Answer:\nDrawing", "# Reasoning Path:\nHenri Matisse -> book.book_subject.works -> Matisse -> book.written_work.author -> Pierre Schneider\n# Answer:\nMatisse"], "ground_truth": ["Drawing", "Collage", "Printmaking", "Sculpture", "Painting"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6857142857142857, "ans_precission": 0.8, "ans_recall": 0.6, "path_f1": 0.6857142857142857, "path_precision": 0.8, "path_recall": 0.6, "path_ans_f1": 0.6857142857142857, "path_ans_precision": 0.8, "path_ans_recall": 0.6}
{"id": "WebQTest-375", "prediction": ["# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> book.written_work.author -> Adelaide Anne Procter\n# Answer:\nA House to Let", "# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> book.written_work.author -> Elizabeth Gaskell\n# Answer:\nA House to Let", "# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> common.topic.notable_for -> g.1255hfpzv\n# Answer:\nA House to Let", "# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> book.written_work.author -> Wilkie Collins\n# Answer:\nA House to Let", "# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> common.topic.article -> m.0273w68\n# Answer:\nA House to Let", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Fereydoon Motamed -> influence.influence_node.influenced_by -> Edgar Allan Poe\n# Answer:\nFereydoon Motamed", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Finnegan 'Finn' Bell -> film.film_character.portrayed_in_films -> m.010hszpw\n# Answer:\nFinnegan 'Finn' Bell", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Finnegan 'Finn' Bell -> tv.tv_character.appeared_in_tv_program -> m.0gbd7_9\n# Answer:\nFinnegan 'Finn' Bell", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Alfred Lammle -> fictional_universe.fictional_character.occupation -> Adventurer\n# Answer:\nAlfred Lammle", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Fereydoon Motamed -> influence.influence_node.influenced_by -> Jean Piaget\n# Answer:\nFereydoon Motamed"], "ground_truth": ["Oliver Twist"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-376", "prediction": ["# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Evil Under the Sun -> film.film.written_by -> Anthony Schaffer\n# Answer:\nEvil Under the Sun", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Agatha Christie's Miss Marple: 4:50 from Paddington -> film.film.edited_by -> Bernard Ashby\n# Answer:\nAgatha Christie's Miss Marple: 4:50 from Paddington", "# Reasoning Path:\nAgatha Christie -> book.book_subject.works -> Agatha Christie and the Eleven Missing Days -> book.book.editions -> Agatha Christie and the eleven missing days\n# Answer:\nAgatha Christie and the Eleven Missing Days", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Evil Under the Sun -> film.film.language -> English Language\n# Answer:\nEvil Under the Sun", "# Reasoning Path:\nAgatha Christie -> common.topic.webpage -> m.03lzz8x -> common.webpage.resource -> m.0bl41lc\n# Answer:\nm.03lzz8x", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Evil Under the Sun -> film.film.written_by -> Barry Sandler\n# Answer:\nEvil Under the Sun", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Evil Under the Sun -> film.film.distributors -> m.059rh54\n# Answer:\nEvil Under the Sun", "# Reasoning Path:\nAgatha Christie -> book.book_subject.works -> Agatha Christie and the Eleven Missing Days -> common.topic.article -> m.0746q5c\n# Answer:\nAgatha Christie and the Eleven Missing Days", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Evil Under the Sun -> film.film.language -> French\n# Answer:\nEvil Under the Sun", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Agatha Christie's Miss Marple: 4:50 from Paddington -> film.film.runtime -> m.0ghfzm6\n# Answer:\nAgatha Christie's Miss Marple: 4:50 from Paddington"], "ground_truth": ["Partners in crime", "A Caribbean Mystery (BBC Radio Collection)", "Poems", "A Pocket Full of Rye (Agatha Christie Collection)", "Mystery of the Blue Train", "Towards Zero.", "The Big Four (Hercule Poirot Mysteries)", "Sleeping Murder (Miss Marple Mysteries)", "Death in the clouds", "The Big Four (Poirot)", "Sad Cypress", "Partners in Crime", "Poirot Investigates", "The Secret Adversary (Classic Books on Cassettes Collection)", "The Sittaford Mystery (Agatha Christie Signature Edition)", "Five Little Pigs.", "A Pocket Full of Rye (Miss Marple Mysteries)", "Murder Is Easy (Agatha Christie Collection)", "The pale horse", "Murder on the Links (BBC Audio Crime)", "The big four", "Unfinished Portrait", "They Do It With Mirrors (Agatha Christie Audio Mystery)", "The Golden Ball and Other Stories (G. K. Hall (Large Print))", "The Adventure of the Christmas Pudding (The Crime Club)", "Death in the Clouds", "Labours of Hercules", "The secret of Chimneys", "Problem at Pollensa Bay", "Burden", "Evil Under the Sun (Poirot)", "The Mysterious Affair at Styles (Hercule Poirot Mysteries (Audio))", "Towards Zero (Agatha Christie Collection)", "And Then There Were None (Agatha Christie Collection)", "Hercule Poirot\u2019s Christmas", "By the Pricking of My Thumbs", "Endless night", "The seven dials mystery", "They do it with mirrors", "Postern of Fate", "One, Two, Buckle My Shoe (Poirot)", "Nemesis (Miss Marple)", "Death on the Nile (Agatha Christie Audio Mystery)", "Lord Edgware dies.", "Crooked House (Agatha Christie Collection)", "Murder in the mews.", "The Hollow (Winterbrook Edition)", "A Murder is Announced", "Clocks", "And Then There Were None (St. Martin's True Crime Library)", "By the Pricking of My Thumbs (Tommy & Tuppence Chronology)", "Appointment With Death", "The Pale Horse (St. Martin's Minotaur Mysteries)", "The Murder on the Links", "Sleeping Murder (The Agatha Christie Mystery Collection)", "The Murder of Roger Ackroyd (Hercule Poirot Mystery)", "Crooked House (Winterbrook Edition)", "Endless night.", "Death In The Clouds", "The Seven Dials mystery", "The adventure of the Christmas pudding, and a selection of entr\u00e9es", "Come tell me how you live", "Unfinished Portrait (Westmacott)", "Poirot Investigates (Hercule Poirot Mysteries (Audio))", "The Hollow (Ulverscroft Large Print)", "The adventure of the Christmas pudding and a selection of entre\u0301es.", "4.50 from Paddington (Miss Marple Mysteries (Audio))", "A Murder Is Announced (BBC Radio Collection)", "The Body In The Library (A Miss Marple Mystery)", "Cat Among the Pigeons (Hercule Poirot Mysteries (Paperback))", "Death Comes As the End (Mystery Masters)", "a Pocket full of Rye", "They Do It with Mirrors (Agatha Christie Collection)", "They do it with Mirrors", "Ordeal By Innocence", "Taken at the Flood (Ulverscroft Large Print)", "Death comes as the end", "Curtain (Ulverscroft Large Print Ser.)", "Man in the Brown Suit", "A CARIBBEAN MYSTERY", "The Secret of Chimneys (Ulverscroft Mystery)", "The Pale Horse (Agatha Christie Collection)", "One, Two, Buckle My Shoe (Agatha Christie Mysteries Collection (Paperback))", "And Then There Were None Book (Detective English Readers)", "Passenger to Frankfurt (Winterbrook Edition)", "Absent in the Spring (Westmacott)", "Unexpected Guest", "Towards zero", "Peril at end house.", "CLOCKS", "And Then There Were None (Agatha Christie Audio Mystery)", "The Golden Ball and Other Stories", "Murder is easy", "Dead man's folly", "The mysterious affair at Styles", "The Secret Adversary (Tommy & Tuppence Chronology)", "Evil under the sun", "The Pale Horse", "And Then There Were None (Audio Editions Mystery Masters)", "Sparkling cyanide.", "After the Funeral (Hercule Poirot Mysteries (Paperback))", "The secret of chimneys.", "Poirot Investigates (G. K. Hall (Large Print))", "Death Comes As the End (The Agatha Christie Mystery Collection)", "Curtain (G. K. Hall (Large Print))", "The Secret Adversary (Agatha Christie Audio Mystery)", "The Man in the Brown Suit (Agatha Christie Mysteries Collection)", "Murder On The Links", "Lord Edgware Dies (Hercule Poirot Mysteries)", "Death in the Clouds (Agatha Christie Mysteries Collection (Paperback))", "Third Girl", "The Big Four (Agatha Christie Collection)", "The Mysterious Affair at Styles (Large Print Edition)", "Third Girl (Hercule Poirot Mysteries (Paperback))", "Come, tell me how you live", "The Sittaford Mystery", "Secret Adversary (The Agatha Christie Mystery Collection)", "Poirot Investigates (Hercule Poirot Mysteries (Paperback))", "Cat among the pigeons.", "Pale horse", "A Caribbean Mystery (Audio Editions)", "While the light lasts and other stories", "Third Girl (Poirot)", "Cat Among the Pigeons (Hercule Poirot Mysteries)", "Taken at the Flood (Hercule Poirot)", "Curtain (The Agatha Christie mystery collection)", "The Labours of Hercules", "Curtain (Hercule Poirot Mysteries)", "Parker Pyne Investigates", "Three Act Tragedy", "By the Pricking of My Thumbs (Agatha Christie Audio Mystery)", "Partners in Crime (The Agatha Christie Mystery Collection)", "The murder at the vicarage", "The Listerdale Mystery (Agatha Christie Collection)", "Sleeping murder", "Hickory Dickory Dock (Poirot)", "By The Pricking Of My Thumbs (Tommy and Tuppence Mysteries)", "Towards Zero (The Agatha Christie Mystery Collection)", "Murder on the links (The Agatha Christie mystery collection)", "The Sittaford Mystery (Mystery Masters Series)", "The Mysterious Mr. Quin", "The Secret Adversary (Large Print Edition)", "Problem At Pollensa Bay and Seven Other Mysteries", "Death in the Clouds (Hercule Poirot)", "The Unexpected Guest", "Cards on the Table", "The murder on the links", "Elephants can remember", "The regatta mystery and other stories", "Three act tragedy", "The murder on the links.", "Five Little Pigs (Agatha Christie Mysteries Collection)", "The crooked house", "Murder at the Vicarage (G. K. Hall (Large Print))", "Murder at the Vicarage (Agatha Christie Mysteries Collection)", "4.50 From Paddington", "Peril at End House (Hercule Poirot Mysteries)", "Peril at End House", "The Mysterious Mr.Quin (Agatha Christie Signature Edition)", "By the Pricking of my Thumbs", "Elephants Can Remember (The Agatha Christie Mystery Collection)", "The Sittaford Mystery (St. Martin's Minotaur Mysteries)", "Ordeal by Innocence (Agatha Christie Collection)", "A Murder Is Announced (Agatha Christie Collection)", "The Mysterious Mr Quin", "And then there were none", "Cards on the Table (A Hercule Poirot Mystery)", "A Cat Among the Pigeons (Agatha Christie Collection)", "Mysterious Affair at Styles/Cassettes (1362)", "The thirteen problems", "A Caribbean Mystery (Miss Marple Mysteries)", "Postern of Fate (Agatha Christie Audio Mystery)", "A Pocket Full of Rye (Miss Marple Mysteries (Audio))", "Curtain (Poirot)", "Nemesis (Miss Marple Mysteries)", "Poirot investigates.", "The Secret Adversary", "Elephants Can Remember (Poirot)", "One, Two, Buckle My Shoe", "Destination Unknown (Agatha Christie Collection)", "Lord Edgware Dies (BBC Audio Crime)", "Body in the Library (Miss Marple Mysteries (Paperback))", "A Murder Is Announced", "A Murder Is Announced (Miss Marple)", "Destination unknown (Agatha Christie mystery collection)", "Murder Is Easy (St. Martin's Minotaur Mysteries)", "The Sittaford Mystery (BBC Radio Collection)", "The Secret Of Chimneys (St. Martin's Minotaur Mysteries)", "A Pocket Full of Rye (Miss Marple Mysteries (Paperback))", "Three Act Tragedy (Poirot)", "Lord Edgware dies", "Nemesis (Cover to Cover Classics)", "The Murder of Roger Ackroyd", "Sparkling Cyanide (Agatha Christie Signature Edition)", "Crooked house", "Three ACT Tragedy", "Postern of Fate (G K Hall Large Print Book Series)", "The Mysterious Affair at Styles (Dodo Press)", "Elephants Can Remember (Hercule Poirot Mysteries (Audio))", "The Murder on the Links (Hercule Poirot Mysteries)", "The mysterious Mr. Quin", "The Adventure of the Christmas Pudding", "Peril at End House (The Agatha Christie Mystery Collection) (The Agatha Christie Mystery Collection)", "Towards Zero (Audio Editions Mystery Masters)", "One, two, buckle my shoe", "Five little pigs.", "They Came to Baghdad (The Agatha Christie Mystery Collection)", "Star over Bethlehem and other stories by Agatha Christie Mallowan", "Murder at the Vicarage", "Three Act Tragedy (BBC Radio Collection)", "After the Funeral (BBC Audio Crime)", "Murder is Easy (St. Martin's Minotaur Mysteries)", "Problem at Pollensa Bay and 7 Other Mysteries", "Postern of fate", "Secret Adversary", "Cards on the Table (Agatha Christie Audio Mystery)", "Appointment With Death (G K Hall Large Print Book Series)", "Death on the Nile (Hercule Poirot Mysteries)", "Golden Ball and Other Stories (Agatha Christie Mysteries Collection)", "Towards Zero (Agatha Christie Mysteries Collection (Paperback))", "Ordeal by Innocence (Signature Editions)", "The unexpected guest", "The mysterious Mr Quin", "Five Little Pigs (Hercule Poirot Mysteries)", "Sad Cypress (BBC Radio Collection)", "Three Act Tragedy (Hercule Poirot Mysteries)", "Sittaford Mystery (St. Martin's Minotaur Mysteries)", "Murder In Mesopotamia (Agatha Christie Mystery Collection) Bantam Blue Leatherette Edition", "Sparkling Cyanide (Variant Title = Remembered Death)", "Hickory Dickory Dock", "Taken at the Flood (Agatha Christie Collection)", "Taken at the Flood (Hercule Poirot Mysteries (Paperback))", "Taken at the Flood", "The Clocks (A Hercule Poirot Murder Mystery)", "Appointment with Death (Dell; 10246)", "The Clocks", "Cards on the Table (BBC Radio Collection)", "The Clocks (Poirot)", "Crooked House", "They Do It with Mirrors (Miss Marple Mysteries (Audio Partners))", "Murder in the Mews (Poirot)", "Peril At End House", "Poirot Investigates (Poirot)", "The body in the library", "Pale Horse", "PASSENGER TO FRANKFURT.", "The Mysterious Affair at Styles, Large-Print Edition", "The Rose and the Yew Tree", "The Body in the Library (BBC Audio Crime)", "They Do It with Mirrors", "Dumb witness.", "The Murder At the Vicarage", "Hickory dickory dock.", "Cards on the Table (Agatha Christie Mysteries Collection (Paperback))", "They came to Baghdad", "The Body in the Library (Miss Marple)", "Come, Tell Me How You Live (Common Reader Editions: Rediscoveries: LONDON)", "Man in the Brown Suit (Ulverscroft Mystery)", "The Big Four.", "Elephants Can Remember (The Christie Collection)", "CROOKED HOUSE", "Sad cypress.", "Cards on the table.", "The Pale Horse (The Agatha Christie mystery collection)", "Murder at the Vicarage (Agatha Christie Audio Mystery)", "Peril at End House (Poirot)", "Death on the Nile (Hercule Poirot Mysteries (Paperback))", "Poems (The Agatha Christie mystery collection)", "Death on the Nile", "The Man in the Brown Suit", "Death on the Nile (BBC Radio Collection)", "Partners in Crime (Tommy & Tuppence Chronology)", "The Seven Dials mystery.", "The Man in the Brown Suit (Mystery Masters)", "By the pricking of my thumbs", "After The Funeral", "The Listerdale mystery.", "Three blind mice and other stories (Agatha Christie Mystery Collection)", "Taken at the Flood (aka There is a Tide...)", "Rose and the Yew Tree", "After the Funeral (Hercule Poirot)", "The A. B. C. Murders", "Murder in Mesopotamia.", "The Hollow (Hercule Poirot)", "The Secret Adversary (G K Hall's Agatha Christie Series)", "Partners in Crime (Listen for Pleasure)", "Poirot Investigates (Hercule Poirot Mysteries)", "A Pocket Full of Rye (BBC Radio Collection: Crimes and Thrillers)", "Sparkling cyanide", "Cat among the pigeons", "Partners in Crime (Vol. 1 Finessing the King, Vol 2 The Crackler, Vol 3 The Unbreakable Alibi)", "The Mystery of the Blue Train (Poirot)", "A Murder Is Announced (Miss Marple Mysteries (Paperback))", "A Caribbean Mystery (Greenway E.)", "Witness for the prosecution, and other stories", "The Mysterious Mr.Quin", "Sleeping murder.", "Lord Edgware Dies (Agatha Christie Collection S.)", "And Then There Were None (The Christie Collection)", "The Murder at the Vicarage (BBC Full Cast Dramatization)", "Clocks (Hc Collection)", "THE MYSTERY OF THE BLUE TRAIN.", "The Sittaford Mystery (BBC Radio Presents)", "The Body in the Library (BBC Radio Collection)", "The Adventure of the Christmas Pudding and Other Stories", "A Pocket Full Of Rye", "Murder at the vicarage.", "The Hollow.", "The Listerdale mystery", "Murder On the Links", "The Big Four", "Elephants can remember.", "Death on the Nile (Hercule Poirot)", "The Adventure of the Christmas Pudding (Poirot)", "The Burden", "The Sittaford Mystery (Agatha Christie Collection)", "Cards on the table", "They Came to Baghdad (Mystery Masters)", "Appointment with Death (BBC Radio Collection)", "Mystery of the blue train", "One, Two Buckle My Shoe (BBC Radio Collection)", "The Big Four (G K Hall Large Print Book Series)", "The Under Dog and Other Stories", "Murder in Mesopotamia (Hercule Poirot)", "The Witness for the Prosecution and Other Stories (Mystery Masters Series)", "The murder of Roger Ackroyd", "A Murder Is Announced (G. K. Hall (Large Print))", "Passenger to Frankfurt (The Christie Collection)", "The hound of death", "Parker Pyne Investigates (Agatha Christie Collection)", "The A.B.C. Murders (The Christie Collection)", "Sad Cypress (Hercule Poirot)", "Secret of Chimneys", "Murder in Mesopotamia", "The Man In The Brown Suit (Agatha Christie Mystery Collection) Bantam Blue Leatherette Edition", "Crooked house.", "Double Sin and Other Stories", "POIROT INVESTIGATES (Hercule Poirot Mysteries (Paperback))", "Partners in crime.", "Appointment with Death (Hercule Poirot)", "Secret Adversary (Unabridged Classics)", "Murder in Mesopotamia (Poirot)", "Star over Bethlehem, and other stories (The Agatha Christie mystery collection)", "Five little pigs (Agatha Christie Mystery Collection)", "Five Little Pigs (Poirot)", "The Regatta Mystery", "One,two, buckle my shoe", "Destination Unknown (St. Martin's Minotaur Mysteries)", "Curtain", "Murder at the Vicarage (Miss Marple Mysteries)", "The Clocks (Agatha Christie Collection)", "Dumb Witness", "Hickory dickory dock", "Ordeal by Innocence", "Pale Horse (R)", "Cat Among the Pigeons (Ulverscroft Mystery)", "Mystery Of The Blue Train / The Listerdale Mystery / Murder At The Vicarage", "Death Comes as the End", "Thirteen Problems", "Destination Unknown", "The Mysterious Affair At Styles (Classic Books on Cassettes Collection) (Classic Books on Cassettes Collection)", "4.50 from Paddington (Agatha Christie Collection)", "The Murder on the Links (Hercule Poirot)", "The Mysterious Mr. Quin (Paperback)) (Hercule Poirot Mysteries (Paperback))", "A Caribbean Mystery (BBC Radio Presents: An Audio Dramatization)", "Nemesis (BBC Radio Collection)", "The witness for the prosecution and other stories", "Murder in the Mews (Ulverscroft Large Print)", "4.50 from Paddington (BBC Audio Crime)", "Evil Under the Sun (Agatha Christie Audio Mystery)", "Man in the Brown Suit (St. Martin's Minotaur Mysteries)", "They Do It With Mirrors (Miss Marple Mysteries)", "The Hound of Death (Agatha Christie Collection)", "The A.B.C. Murders", "Appointment With Death (Hercule Poirot Mysteries)", "Passenger to Frankfurt", "The big four.", "4.50 from Paddington (Miss Marple)", "The Hollow (Agatha Christie Collection)", "4.50 from Paddington (Agatha Christie Signature Edition)", "Taken At The Flood (A Hercule Poirot Mystery)", "The hollow", "Murder on the links", "The Secret of Chimneys", "Three blind mice and other stories", "Sad cypress", "Nemesis (G. K. Hall (Large Print))", "Seven Dials Mystery (Agatha Christie Audio Mystery)", "The Murder at the Vicarage (Miss Marple Mysteries)", "The labours of Hercules", "4.50 from Paddington", "Hallowe'en party", "And Then There Were None (Mystery Masters)", "A pocket full of rye", "Murder in Mesopotamia (BBC Radio Collection)", "Appointment with Death", "Death in the Clouds (Agatha Christie Collection)", "Partners in Crime (Agatha Christie Mysteries Collection)", "Murder on the Links", "The Listerdale Mystery", "The murder at the vicarage.", "Endless Night (Agatha Christie Collection)", "The Unexpected Guest (Acting Edition)", "The harlequin tea set and other stories", "Hickory Dickory Dock (Agatha Christie Collection)", "Towards zero.", "Mrs. McGinty's dead", "After the Funeral", "Murder in the Mews", "The Adventure of the Christmas Pudding and The Mystery of the Spanish Chest", "Passenger to Frankfurt.", "Sparkling Cyanide (Mystery Masters Series)", "They Came to Baghdad (Agatha Christie Collection)", "A Murder is Announced (Miss Marple Mysteries)", "The Sittaford Mystery (Agatha Christie Mystery Collection)", "Sleeping Murder (Large Print Ed)", "Secret of Chimneys (St. Martin's Minotaur Mysteries)", "Appointment with death.", "The Hollow (A Hercule Poirot Novel)", "They Do It with Mirrors (BBC Radio Collection)", "Cat Among the Pigeons ( A Hercule Poirot Mystery)", "Murder Is Easy (Audio Editions Mystery Masters)", "Ordeal by innocence", "Evil Under the Sun (Agatha Christie Collection)", "CURTAIN", "The Seven Dials Mystery (Agatha Christie Signature Edition)", "The Mystery of the Blue Train (Hercule Poirot Mysteries)", "The Body in the Library (Miss Marple Mysteries)", "Evil Under the Sun", "Evil Under the Sun (G. K. Hall (Large Print))", "At Bertram's Hotel", "Towards Zero", "Murder in the mews", "Five Little Pigs", "Sittaford Mystery", "Taken at the Flood (Radio Collection)", "They came to Baghdad.", "The secret of chimneys", "The Sittaford mystery", "Sad Cypress (Hercule Poirot Mysteries)", "Five Little Pigs (The Christie Collection)", "Crooked House (Agatha Christie Audio Mystery)", "Witness for the Prosecution and Other Stories (G. K. Hall (Large Print))", "The Clocks (Hercule Poirot Mysteries)", "Murder of Roger Ackroyd", "Lord Edgware Dies (BBC Radio Collection)", "Endless Night", "Third girl", "Seven Dials Mystery", "Third Girl (Agatha Christie Collection)", "Appointment with Death (The Christie Collection)", "Towards Zero (St. Martin's Minotaur Mysteries)", "The Witness for the Prosecution and Other Stories", "Murder in Mesopotamia (BBC Radio Presents)", "Nemesis", "Murder at the Vicarage (Miss Marple Mystery Series)", "Sleeping Murder (Miss Marple)", "The mysterious affair at Styles.", "Sleeping Murder", "Sparkling Cyanide (St. Martin's Minotaur Mysteries)", "Sad Cypress (Poirot)", "Partners in Crime (Agatha Christie Collection)", "The secret adversary", "Mysterious Affair at Styles", "The murder of Roger Ackroyd.", "The Murder of Roger Ackroyd (Hercule Poirot Mysteries)", "The mysterious Mr. Quin.", "The A.B.C. murders", "Double sin and other stories", "The \\\"Mysterious Mr Quin\\\"", "A Pocket Full of Rye", "Death Comes As the End", "Taken at the flood", "4.50 from Paddington (BBC Radio Collection)", "Evil Under the Sun (BBC Radio Collection)", "The Hound of Death", "The Secret of Chimneys (Mystery Masters Series)", "The Labours of Hercules (Poirot)", "Sparkling Cyanide", "Endless Night (Ulverscroft Large Print)", "Murder at the Vicarage (Miss Marple Mysteries (Paperback))", "Five little pigs", "Sparkling Cyanide (Agatha Christie Collection)", "After the Funeral (Hercule Poirot Mysteries)", "Lord Edgware Dies", "The Man in the Brown Suit (Agatha Christie Collection)", "Cat Among the Pigeons (Poirot)", "The adventure of the Christmas pudding", "Destination unknown", "A Pocket Full of Rye (Miss Marple)", "Murder in Mesopotamia (Agatha Christie Audio Mystery)", "They Came to Baghdad (Dell Book)", "Sleeping Murder (Miss Marple Mysteries (Paperback))", "A Pocket Full of Rye (A Jane Marple Murder Mystery)", "Sparkling Cyanide (Audio Editions Mystery Masters)", "The Unexpected Guest (BBC Radio Collection)", "A Caribbean Mystery", "The Man In The Brown Suit", "Death in the Clouds (Agatha Christie Audio Mystery)", "The Man In The Brown Suit (Classic Books on Cassettes Collection)", "The Murder at the Vicarage", "After the Funeral (The Christie Collection)", "Big Four (Ulverscroft Large Print)", "The under dog and other stories", "The Seven Dials Mystery (Agatha Christie Mystery Collection) Bantam Blue Leatherette Edition", "A Caribbean Mystery (G. K. Hall (Large Print))", "One, Two, Buckle My Shoe (Ulverscroft Mystery)", "The Murder of Roger Ackroyd. (Lernmaterialien)", "Death on the Nile.", "Peril at End House (BBC Radio Collection)", "The Secret Adversary (Dodo Press)", "Dumb Witness (Poirot)", "Five Little Pigs (Also published as Murder In Retrospect)", "Absent in the Spring", "Murder is easy.", "They Came to Baghdad", "Elephants Can Remember (Agatha Christie Mysteries Collection)", "They Do It With Mirrors", "Destination Unknown (Signature Editions)", "Lord Edgware Dies (Poirot)", "The Body in the Library (BBC Radio Collection: Crimes and Thrillers)", "Death in the Clouds (BBC Radio Collection)", "Cat among the Pigeons", "The Murder of Roger Ackroyd (Poirot)", "The Mysterious Mr. Quin (Ulverscroft Large Print)", "Appointment with death", "The Mystery of the Blue Train", "The Labours of Hercules (Hercule Poirot Mysteries)", "Partners in Crime (Tommy and Tuppence Mysteries)", "The mystery of the blue train", "Problem at Pollensa Bay & Other Stories", "Evil Under the Sun (Hercule Poirot Mysteries)", "Cards on the Table (Hercule Poirot Mysteries)", "Taken at the Flood (Poirot)", "By the Pricking of My Thumbs (Tommy and Tuppence Mysteries (Paperback))", "The Hollow (Poirot)", "Death Comes as the End (Agatha Christie Collection)", "And Then There Were None (The Agatha Christie Mystery Collection)", "Parker Pyne investigates", "Peril at End House.", "4.50 from Paddington.", "The Seven Dials Mystery", "Elephants Can Remember (Hercule Poirot Mysteries)", "Clocks (Ulverscroft Large Print)", "Cat Among the Pigeons", "Murder at the Vicarage (BBC Radio Presents)", "Murder in the Mews (Agatha Christie Signature Edition)", "A Pocket Full of Rye (BBC Audio Crime)", "The Under Dog and Other Stories (G. K. Hall (Large Print))", "Murder on the Orient Express", "The Hollow", "PASSENGER TO FRANKFURT", "Dumb witness", "The Mysterious Affair at Styles", "The Thirteen Problems (Miss Marple)", "Destination Unknown (Mystery Masters Series)", "By the Pricking of My Thumbs (Tommy and Tuppence Mysteries)", "The Man in the Brown Suit (St. Martin's Minotaur Mysteries)", "And Then There Were None", "Hickory Dickory Dock (Hercule Poirot Mysteries (Paperback))", "Cards on the table (Agatha Christie mystery collection)", "The Body in the Library", "Cards on the Table (Mystery Masters)", "The pale horse.", "The Rose and the Yew Tree (Westmacott)", "Double Sin and other stories", "The Murder at the Vicarage (Miss Marple)", "Murder Is Easy", "Poirot investigates", "They Came to Baghdad (G. K. Hall (Large Print))", "The Thirteen Problems", "Postern of Fate (Tommy & Tuppence Chronology)", "Death on the Nile (Hercule Poirot Mysteries (Audio))", "Elephants Can Remember", "The golden ball and other stories", "Big Four", "The Thirteen Problems (Miss Marple Mysteries)", "And Then There Were None (Ulverscroft Large Print)", "A Murder Is Announced (Miss Marple Mysteries)", "A Caribbean Mystery (Miss Marple)", "Death On The Nile (Hercule Poirot Mysteries (Paperback))", "The Murder at the Vicarage (Acting Edition)", "A murder is announced", "Crooked House (Minotaur Mysteries)", "They Do It with Mirrors (Miss Marple)", "Body in the library", "Murder On The Links (Classic Books on Cassettes Collection)", "Cards on the Table (Hercule Poirot)", "Ordeal by Innocence (Agatha Christie Mysteries Collection)", "After the funeral.", "Death on the Nile (The Christie Collection)", "Appointment with Death (Hercule Poirot Mysteries)", "The Murder of Roger Ackroyd (Agatha Christie Audio Mystery)", "Come, Tell Me How You Live", "Third Girl (Hercule Poirot Mysteries)", "Evil under the sun.", "Evil under the Sun (Hercule Poirot Mysteries)", "Murder in Mesopotamia (Hercule Poirot Mysteries)", "The Secret of Chimneys (Agatha Christie Signature Edition)", "After the Funeral (Agatha Christie Collection)"], "ans_acc": 0.01, "ans_hit": 1, "ans_f1": 0.019718309859154928, "ans_precission": 0.7, "ans_recall": 0.01, "path_f1": 0.04721030042918455, "path_precision": 0.5, "path_recall": 0.024774774774774775, "path_ans_f1": 0.019718309859154928, "path_ans_precision": 0.7, "path_ans_recall": 0.01}
{"id": "WebQTest-377", "prediction": ["# Reasoning Path:\nMichael Jordan -> people.person.education -> m.0jwk2zd -> education.education.institution -> Emsley A. Laney High School\n# Answer:\nm.0jwk2zd", "# Reasoning Path:\nMichael Jordan -> people.person.education -> m.04g36q3 -> education.education.institution -> University of North Carolina at Chapel Hill\n# Answer:\nm.04g36q3", "# Reasoning Path:\nMichael Jordan -> sports.drafted_athlete.drafted -> m.02h7cgv -> sports.sports_league_draft_pick.team -> Chicago Bulls\n# Answer:\nm.02h7cgv", "# Reasoning Path:\nMichael Jordan -> people.person.education -> m.0jwk2zd -> freebase.valuenotation.has_value -> End Date\n# Answer:\nm.0jwk2zd", "# Reasoning Path:\nMichael Jordan -> sports.drafted_athlete.drafted -> m.02h7cgv -> sports.sports_league_draft_pick.school -> University of North Carolina at Chapel Hill\n# Answer:\nm.02h7cgv", "# Reasoning Path:\nMichael Jordan -> people.person.education -> m.0jwk2zd -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nm.0jwk2zd", "# Reasoning Path:\nMichael Jordan -> people.person.education -> m.04g36q3 -> education.education.major_field_of_study -> Cultural geography\n# Answer:\nm.04g36q3", "# Reasoning Path:\nMichael Jordan -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Esophageal cancer\n# Answer:\nMale", "# Reasoning Path:\nMichael Jordan -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Pancreatic cancer\n# Answer:\nMale", "# Reasoning Path:\nMichael Jordan -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Prostate cancer\n# Answer:\nMale"], "ground_truth": ["University of North Carolina at Chapel Hill", "Emsley A. Laney High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-379", "prediction": ["# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.article -> m.02qnd1f\n# Answer:\nBallistic trauma", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nBallistic trauma", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Suicide -> common.topic.notable_types -> Cause Of Death\n# Answer:\nSuicide", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ng.1254zbtvv", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.notable_for -> g.12558k_cd\n# Answer:\nBallistic trauma", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> base.aareas.schema.administrative_area.administrative_parent -> Germany\n# Answer:\nBerlin", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> film.film_location.featured_in_films -> Valkyrie\n# Answer:\nBerlin", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> film.film_location.featured_in_films -> Inglourious Basterds\n# Answer:\nBerlin"], "ground_truth": ["1945-04-30"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> government.political_appointer.appointees -> m.09k8753 -> government.government_position_held.office_holder -> Rob Portman\n# Answer:\nm.09k8753", "# Reasoning Path:\nGeorge W. Bush -> government.political_appointer.appointees -> m.010g1fx8 -> government.government_position_held.office_holder -> Michael Mukasey\n# Answer:\nm.010g1fx8", "# Reasoning Path:\nGeorge W. Bush -> government.political_appointer.appointees -> m.010g1fx8 -> government.government_position_held.basic_title -> Attorney general\n# Answer:\nm.010g1fx8", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointed_role -> Director of the Bureau of Counterterrorism\n# Answer:\nm.010p95d2", "# Reasoning Path:\nGeorge W. Bush -> government.political_appointer.appointees -> m.09k8753 -> government.government_position_held.governmental_body -> Executive Office of the President of the United States\n# Answer:\nm.09k8753", "# Reasoning Path:\nGeorge W. Bush -> government.political_appointer.appointees -> m.011qhxxc -> government.government_position_held.office_holder -> Sean Stackley\n# Answer:\nm.011qhxxc", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pk -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.010l29pk", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointee -> Henry A. Crumpton\n# Answer:\nm.010p95d2", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1 -> people.appointment.appointed_role -> United States Ambassador to Angola\n# Answer:\nm.079pxt1", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079q3lx -> people.appointment.appointed_role -> United States Ambassador to Brazil\n# Answer:\nm.079q3lx"], "ground_truth": ["John Kerry", "Gene Amondson", "Michael Peroutka", "Ralph Nader"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-380", "prediction": ["# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.containedby -> Washington\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Cascade Range -> location.location.containedby -> North America\n# Answer:\nCascade Range", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.containedby -> Portland-Vancouver-Beaverton, OR-WA Metropolitan Statistical Area\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.containedby -> Contiguous United States\n# Answer:\nWashington", "# Reasoning Path:\nMount St. Helens -> geography.mountain.mountain_range -> Cascade Range -> location.location.containedby -> North America\n# Answer:\nCascade Range", "# Reasoning Path:\nMount St. Helens -> common.image.size -> m.02bkwf9\n# Answer:\nm.02bkwf9", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.containedby -> Northwestern United States\n# Answer:\nWashington", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.contains -> Moffetts Hot Springs\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.partially_contains -> Columbia River\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.containedby -> Pacific Northwest\n# Answer:\nWashington"], "ground_truth": ["Washington", "Cascade Range", "North America", "United States of America", "Skamania County"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.7200000000000001, "ans_precission": 0.9, "ans_recall": 0.6, "path_f1": 0.6857142857142857, "path_precision": 0.8, "path_recall": 0.6, "path_ans_f1": 0.8470588235294118, "path_ans_precision": 0.9, "path_ans_recall": 0.8}
{"id": "WebQTest-381", "prediction": ["# Reasoning Path:\nNew York City -> book.book_subject.works -> Graffiti Lives: Beyond the Tag in New York\u2019s Urban Underground -> book.written_work.subjects -> Graffiti\n# Answer:\nGraffiti Lives: Beyond the Tag in New York\u2019s Urban Underground", "# Reasoning Path:\nNew York City -> book.book_subject.works -> Graffiti Lives: Beyond the Tag in New York\u2019s Urban Underground -> book.written_work.subjects -> Urban art\n# Answer:\nGraffiti Lives: Beyond the Tag in New York\u2019s Urban Underground", "# Reasoning Path:\nNew York City -> book.book_subject.works -> Graffiti Lives: Beyond the Tag in New York\u2019s Urban Underground -> book.book.genre -> Non-fiction\n# Answer:\nGraffiti Lives: Beyond the Tag in New York\u2019s Urban Underground", "# Reasoning Path:\nNew York City -> book.book_subject.works -> A Christmas Caroline -> book.written_work.subjects -> Christmas\n# Answer:\nA Christmas Caroline", "# Reasoning Path:\nNew York City -> visual_art.art_subject.artwork_on_the_subject -> End of 14th Street Crosstown Line -> visual_art.artwork.art_subject -> Labor unrest\n# Answer:\nEnd of 14th Street Crosstown Line", "# Reasoning Path:\nNew York City -> location.statistical_region.population -> g.11b674hm47\n# Answer:\ng.11b674hm47", "# Reasoning Path:\nNew York City -> book.book_subject.works -> Graffiti Lives: Beyond the Tag in New York\u2019s Urban Underground -> common.topic.image -> graffiti.jpg\n# Answer:\nGraffiti Lives: Beyond the Tag in New York\u2019s Urban Underground", "# Reasoning Path:\nNew York City -> location.statistical_region.population -> g.11b7tlk4sq\n# Answer:\ng.11b7tlk4sq", "# Reasoning Path:\nNew York City -> book.book_subject.works -> Art Deco New York -> book.written_work.subjects -> Art\n# Answer:\nArt Deco New York", "# Reasoning Path:\nNew York City -> book.book_subject.works -> A Christmas Caroline -> book.book.genre -> Fiction\n# Answer:\nA Christmas Caroline"], "ground_truth": ["Flight of the Conchords", "Trial Heat", "The Stand", "Both Sides"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-382", "prediction": ["# Reasoning Path:\nTupac Shakur -> book.author.works_written -> The Rose That Grew from Concrete -> common.topic.notable_for -> g.1254y3l6t\n# Answer:\nThe Rose That Grew from Concrete", "# Reasoning Path:\nTupac Shakur -> book.author.works_written -> The Rose That Grew from Concrete -> common.topic.notable_for -> g.125cjn5d9\n# Answer:\nThe Rose That Grew from Concrete", "# Reasoning Path:\nTupac Shakur -> book.author.works_written -> The Tupac Shakur Collection -> common.topic.notable_types -> Book\n# Answer:\nThe Tupac Shakur Collection", "# Reasoning Path:\nTupac Shakur -> book.author.book_editions_published -> The Rose That Grew from Concrete -> common.topic.notable_for -> g.1254y3l6t\n# Answer:\nThe Rose That Grew from Concrete", "# Reasoning Path:\nTupac Shakur -> book.author.works_written -> The Tupac Shakur Collection -> book.book.genre -> Music\n# Answer:\nThe Tupac Shakur Collection", "# Reasoning Path:\nTupac Shakur -> common.topic.image -> Tupac -> book.book_edition.publisher -> Atria Books\n# Answer:\nTupac", "# Reasoning Path:\nTupac Shakur -> common.topic.image -> Meagainsttheworld -> common.image.size -> m.07xyxx2\n# Answer:\nMeagainsttheworld", "# Reasoning Path:\nTupac Shakur -> common.topic.image -> Tupac -> book.book_edition.number_of_pages -> m.04xl38s\n# Answer:\nTupac", "# Reasoning Path:\nTupac Shakur -> book.author.book_editions_published -> The Rose That Grew from Concrete -> common.topic.notable_for -> g.125cjn5d9\n# Answer:\nThe Rose That Grew from Concrete", "# Reasoning Path:\nTupac Shakur -> common.topic.image -> Tupac -> book.book_edition.publisher -> Pocket Books\n# Answer:\nTupac"], "ground_truth": ["The Rose That Grew from Concrete", "The rose that grew from concrete", "Tupac"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-383", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.country.official_language -> Malay Language\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.country.languages_spoken -> Malay Language\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Malaysia -> location.country.languages_spoken -> Fuzhou dialect\n# Answer:\nMalaysia", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.country.languages_spoken -> English Language\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> base.aareas.schema.administrative_area.administrative_area_type -> Sovereign state\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Malaysia -> location.country.languages_spoken -> Standard Chinese\n# Answer:\nMalaysia", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Canada -> location.country.official_language -> English Language\n# Answer:\nCanada", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Malaysia -> location.country.languages_spoken -> English Language\n# Answer:\nMalaysia", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.country.languages_spoken -> Nepali Language\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Malaysia -> travel.travel_destination.tour_operators -> FOX Vakanties\n# Answer:\nMalaysia"], "ground_truth": ["Simplified Chinese character", "Traditional Chinese characters", "'Phags-pa script", "Chinese characters", "N\u00fcshu script"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-384", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.notable_types -> Profession\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.article -> m.0knj9\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Bard\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Poet\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.profession -> Writer\n# Answer:\nAnne Bront\u00eb", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.image -> Hope-coventina01a\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Poet -> media_common.quotation_subject.quotations_about_this_subject -> As imagination bodies forth The forms of things unknown, the poet's pen Turns them to shapes\n# Answer:\nPoet", "# Reasoning Path:\nRobert Burns -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Cardiovascular disease\n# Answer:\nMale", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> book.book_subject.works -> Authors Frequently Mentioned on the Web\n# Answer:\nWriter"], "ground_truth": ["Writer", "Author", "Bard", "Poet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.8, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-385", "prediction": ["# Reasoning Path:\nWilliam McKinley -> people.deceased_person.place_of_death -> Buffalo -> location.location.containedby -> Erie County\n# Answer:\nBuffalo", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.place_of_death -> Buffalo -> base.biblioness.bibs_location.state -> New York\n# Answer:\nBuffalo", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.cause_of_death -> Assassination -> people.cause_of_death.includes_causes_of_death -> Assassination by firearm\n# Answer:\nAssassination", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.place_of_death -> Buffalo -> location.location.containedby -> Area code 716\n# Answer:\nBuffalo", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.place_of_death -> Buffalo -> location.location.containedby -> New York\n# Answer:\nBuffalo", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.place_of_death -> Buffalo -> base.wikipedia_infobox.settlement.area_code -> Area code 716\n# Answer:\nBuffalo", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.cause_of_death -> Assassination -> people.cause_of_death.includes_causes_of_death -> Assassination by 'Suicide'\n# Answer:\nAssassination", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.cause_of_death -> Assassination -> base.fight.crime_type.crimes_of_this_type -> Assassination of Yitzhak Rabin\n# Answer:\nAssassination", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.cause_of_death -> Firearm -> people.cause_of_death.includes_causes_of_death -> Assassination by firearm\n# Answer:\nFirearm", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.cause_of_death -> Assassination -> people.cause_of_death.includes_causes_of_death -> Assassination by 'Accident'\n# Answer:\nAssassination"], "ground_truth": ["1901-09-14"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-386", "prediction": ["# Reasoning Path:\nLake Merritt -> geography.lake.basin_countries -> United States of America -> location.country.administrative_divisions -> Washington\n# Answer:\nUnited States of America", "# Reasoning Path:\nLake Merritt -> geography.lake.basin_countries -> United States of America -> base.locations.countries.states_provinces_within -> Virginia\n# Answer:\nUnited States of America", "# Reasoning Path:\nLake Merritt -> geography.lake.basin_countries -> United States of America -> location.country.administrative_divisions -> Mississippi\n# Answer:\nUnited States of America", "# Reasoning Path:\nLake Merritt -> geography.lake.basin_countries -> United States of America -> base.locations.countries.states_provinces_within -> Oregon\n# Answer:\nUnited States of America", "# Reasoning Path:\nLake Merritt -> geography.lake.basin_countries -> United States of America -> location.country.administrative_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nLake Merritt -> geography.lake.basin_countries -> United States of America -> location.country.first_level_divisions -> Washington\n# Answer:\nUnited States of America", "# Reasoning Path:\nLake Merritt -> common.topic.notable_types -> Lake -> freebase.documented_object.documentation -> m.021y5b6\n# Answer:\nLake", "# Reasoning Path:\nLake Merritt -> geography.lake.basin_countries -> United States of America -> base.locations.countries.states_provinces_within -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nLake Merritt -> location.location.containedby -> Oakland -> location.hud_county_place.county -> Alameda County\n# Answer:\nOakland", "# Reasoning Path:\nLake Merritt -> common.topic.notable_types -> Lake -> freebase.type_profile.kind -> Definition\n# Answer:\nLake"], "ground_truth": ["3.048"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-388", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.holidays -> Barat Night -> common.topic.notable_types -> Holiday\n# Answer:\nBarat Night", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Barat Night -> common.topic.notable_for -> g.1z2spz3qy\n# Answer:\nBarat Night", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Eid al-Fitr -> time.holiday.day_of_year -> 1 Shawwal\n# Answer:\nEid al-Fitr", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Eid al-Fitr -> freebase.valuenotation.is_reviewed -> Featured in religions\n# Answer:\nEid al-Fitr", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> base.schemastaging.context_name.pronunciation -> g.125_r5my9\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Mourning of Muharram -> common.topic.image -> Azakhana wazeer un nisa\n# Answer:\nMourning of Muharram", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> common.topic.notable_for -> g.125cc3_fl\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Mourning of Muharram -> common.topic.article -> m.03clhwk\n# Answer:\nMourning of Muharram", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Eid al-Fitr -> common.topic.image -> Dublin Mosque at Eid ul-Fitr 2005.\n# Answer:\nEid al-Fitr", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Mourning of Muharram -> common.topic.image -> Shi'a Muslims in Bahrain strike their chests during the Remembrance of Muharram.\n# Answer:\nMourning of Muharram"], "ground_truth": ["Mawlid", "Day of Ashura", "Eid al-Adha", "Islamic New Year", "Mourning of Muharram", "Eid-Ul-Adha Eve", "Day of Arafat", "Laylat al-Qadr", "Night of Wishes", "Eid al-Fitr Eve", "Ramadan", "Lailat al Mi'raj", "Barat Night", "Eid al-Fitr", "Nuzul Al-Quran"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.32000000000000006, "ans_precission": 0.8, "ans_recall": 0.2, "path_f1": 0.32000000000000006, "path_precision": 0.8, "path_recall": 0.2, "path_ans_f1": 0.32000000000000006, "path_ans_precision": 0.8, "path_ans_recall": 0.2}
{"id": "WebQTest-389", "prediction": ["# Reasoning Path:\nClaude Debussy -> music.artist.genre -> Art song -> music.genre.artists -> Aaron Copland\n# Answer:\nArt song", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> Art song -> music.genre.artists -> Albert Roussel\n# Answer:\nArt song", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> Art song -> music.compositional_form.compositions -> Beau soir\n# Answer:\nArt song", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> Art song -> music.genre.artists -> Alexander Borodin\n# Answer:\nArt song", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> Art song -> music.genre.albums -> 8 Lust Songs: I Sonetti Lussuriosi\n# Answer:\nArt song", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> Art song -> music.compositional_form.compositions -> Danse macabre\n# Answer:\nArt song", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> 20th-century classical music -> music.genre.albums -> Double Sextet/2x5\n# Answer:\n20th-century classical music", "# Reasoning Path:\nClaude Debussy -> people.person.profession -> Composer -> type.type.expected_by -> Music\n# Answer:\nComposer", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> Art song -> music.compositional_form.compositions -> Deux po\u00e8mes de Lord Byron\n# Answer:\nArt song", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> Art song -> music.genre.albums -> Acts of Beauty/Exit no Exit\n# Answer:\nArt song"], "ground_truth": ["Ballet", "French opera", "20th-century classical music", "Classical music", "Art song", "Incidental music"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6428571428571429, "ans_precission": 0.9, "ans_recall": 0.5, "path_f1": 0.4337349397590361, "path_precision": 0.9, "path_recall": 0.2857142857142857, "path_ans_f1": 0.6428571428571429, "path_ans_precision": 0.9, "path_ans_recall": 0.5}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.spouse -> Ayaan Hirsi Ali\n# Answer:\nm.0j4jq57", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0j4jq57", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.0j4jq57", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> people.marriage.spouse -> Sue Douglas\n# Answer:\nm.0pdtjg5", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> freebase.valuenotation.has_value -> To\n# Answer:\nm.0j4jq57", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.0pdtjg5", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0pdtjg5", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0pdthbn -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.0pdthbn", "# Reasoning Path:\nNiall Ferguson -> book.author.works_written -> Civilization: The West and the Rest -> common.topic.notable_types -> Book\n# Answer:\nCivilization: The West and the Rest", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0h4nmc7 -> film.personal_film_appearance.film -> The Ascent of Money\n# Answer:\nm.0h4nmc7"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-390", "prediction": ["# Reasoning Path:\nStanley Cup Finals -> sports.sports_championship.events -> 2014 Stanley Cup Finals -> sports.sports_championship_event.runner_up -> New York Rangers\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup Finals -> sports.sports_championship.events -> 2014 Stanley Cup Finals -> sports.sports_championship_event.season -> 2013\u201314 NHL season\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup Finals -> time.recurring_event.instances -> 2014 Stanley Cup Finals -> sports.sports_championship_event.runner_up -> New York Rangers\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup Finals -> sports.sports_championship.events -> 2014 Stanley Cup Finals -> sports.sports_championship_event.champion -> Los Angeles Kings\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup Finals -> time.recurring_event.instances -> 2014 Stanley Cup Finals -> sports.sports_championship_event.season -> 2013\u201314 NHL season\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup Finals -> time.recurring_event.instances -> 2014 Stanley Cup Finals -> sports.sports_championship_event.champion -> Los Angeles Kings\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup Finals -> common.topic.notable_types -> Sports League Championship -> type.type.properties -> Events\n# Answer:\nSports League Championship", "# Reasoning Path:\nStanley Cup Finals -> time.recurring_event.instances -> 2009 Stanley Cup Finals -> sports.sports_championship_event.champion -> Pittsburgh Penguins\n# Answer:\n2009 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup Finals -> common.topic.notable_types -> Sports League Championship -> freebase.type_profile.equivalent_topic -> Championship\n# Answer:\nSports League Championship", "# Reasoning Path:\nStanley Cup Finals -> time.recurring_event.instances -> 2009 Stanley Cup Finals -> common.topic.notable_for -> g.1258dchbl\n# Answer:\n2009 Stanley Cup Finals"], "ground_truth": ["1967 Stanley Cup Finals"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-392", "prediction": ["# Reasoning Path:\nHank Baskett -> american_football.football_player.receiving -> m.09grk6x -> american_football.player_receiving_statistics.team -> Philadelphia Eagles\n# Answer:\nm.09grk6x", "# Reasoning Path:\nHank Baskett -> american_football.football_player.receiving -> m.09rm09r -> american_football.player_receiving_statistics.team -> Indianapolis Colts\n# Answer:\nm.09rm09r", "# Reasoning Path:\nHank Baskett -> american_football.football_player.receiving -> m.09tcnhz -> american_football.player_receiving_statistics.team -> Philadelphia Eagles\n# Answer:\nm.09tcnhz", "# Reasoning Path:\nHank Baskett -> american_football.football_player.receiving -> m.09grk6x -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\nm.09grk6x", "# Reasoning Path:\nHank Baskett -> american_football.football_player.receiving -> m.09rm09r -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\nm.09rm09r", "# Reasoning Path:\nHank Baskett -> people.person.profession -> American football player -> common.topic.notable_types -> Profession\n# Answer:\nAmerican football player", "# Reasoning Path:\nHank Baskett -> american_football.football_player.receiving -> m.09tcnhz -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\nm.09tcnhz", "# Reasoning Path:\nHank Baskett -> people.person.profession -> Football player -> freebase.type_profile.kind -> Title\n# Answer:\nFootball player", "# Reasoning Path:\nHank Baskett -> sports.pro_athlete.teams -> m.01205rsm -> sports.sports_team_roster.team -> Minnesota Vikings\n# Answer:\nm.01205rsm", "# Reasoning Path:\nHank Baskett -> sports.pro_athlete.teams -> m.064w5kf -> sports.sports_team_roster.position -> Wide receiver\n# Answer:\nm.064w5kf"], "ground_truth": ["Minnesota Vikings", "Philadelphia Eagles", "Indianapolis Colts"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.10526315789473685, "path_precision": 0.4, "path_recall": 0.06060606060606061, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-393", "prediction": ["# Reasoning Path:\nParaguay -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Argentina\n# Answer:\nSpanish Language", "# Reasoning Path:\nParaguay -> location.country.languages_spoken -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nParaguay -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nParaguay -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Bolivia\n# Answer:\nSpanish Language", "# Reasoning Path:\nParaguay -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.countries_spoken_in -> Ukraine\n# Answer:\nUkrainian Language", "# Reasoning Path:\nParaguay -> location.country.official_language -> Paraguayan Guaran\u00ed -> language.human_language.region -> Americas\n# Answer:\nParaguayan Guaran\u00ed", "# Reasoning Path:\nParaguay -> location.statistical_region.cpi_inflation_rate -> g.11b60sd4vw\n# Answer:\ng.11b60sd4vw", "# Reasoning Path:\nParaguay -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Chile\n# Answer:\nSpanish Language", "# Reasoning Path:\nParaguay -> location.country.official_language -> Paraguayan Guaran\u00ed -> base.rosetta.languoid.parent -> Guarani I Group\n# Answer:\nParaguayan Guaran\u00ed", "# Reasoning Path:\nParaguay -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.main_country -> Ukraine\n# Answer:\nUkrainian Language"], "ground_truth": ["Spanish Language", "Paraguayan Guaran\u00ed"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-395", "prediction": ["# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.location -> Pittsburgh -> organization.organization_scope.organizations_with_this_scope -> Kerr Communications\n# Answer:\nPittsburgh", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.location -> Pittsburgh -> location.location.containedby -> Pennsylvania\n# Answer:\nPittsburgh", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.arena_stadium -> Pitt Stadium -> location.location.containedby -> 15213\n# Answer:\nPitt Stadium", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.arena_stadium -> Forbes Field -> sports.sports_facility.teams -> Pittsburgh Phantoms\n# Answer:\nForbes Field", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.arena_stadium -> Pitt Stadium -> common.topic.article -> m.038_tx\n# Answer:\nPitt Stadium", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.location -> Pittsburgh -> location.location.containedby -> Allegheny County\n# Answer:\nPittsburgh", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.arena_stadium -> Pitt Stadium -> location.location.containedby -> Allegheny County\n# Answer:\nPitt Stadium", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.arena_stadium -> Forbes Field -> location.location.containedby -> Pittsburgh\n# Answer:\nForbes Field", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.location -> Pittsburgh -> location.location.containedby -> Area code 412\n# Answer:\nPittsburgh", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.arena_stadium -> Heinz Field -> location.location.containedby -> Pittsburgh\n# Answer:\nHeinz Field"], "ground_truth": ["Pittsburgh"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-396", "prediction": ["# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncxj8d -> education.education.institution -> Trinity College, Cambridge\n# Answer:\nm.0ncxj8d", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.02kq1mm -> education.education.institution -> Trinity College, Cambridge\n# Answer:\nm.02kq1mm", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncl4g6 -> education.education.institution -> The King's School, Grantham\n# Answer:\nm.0ncl4g6", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncxj8d -> education.education.degree -> Master of Arts\n# Answer:\nm.0ncxj8d", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.02kq1mm -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nm.02kq1mm", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncl4g6 -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nm.0ncl4g6", "# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> Newton -> astronomy.extraterrestrial_location.geolocation -> m.0w2sw5y\n# Answer:\nNewton", "# Reasoning Path:\nIsaac Newton -> base.kwebbase.kwtopic.connections_to -> abraham de moivre studied work of isaac newton -> base.kwebbase.kwconnection.subject -> Abraham de Moivre\n# Answer:\nabraham de moivre studied work of isaac newton", "# Reasoning Path:\nIsaac Newton -> base.kwebbase.kwtopic.connections_to -> anton van leeuwenhoek royal society member with isaac newton -> base.kwebbase.kwconnection.sentence -> In 1680 Leeuwenhoek was elected a full member of the Royal Society, joining Newton, Hooke, Oldenburg, Boyle, Wren,  Edmond Halley, and other scientific luminaries of his day, who exchanged letters with him, suggesting subjects for his research.\n# Answer:\nanton van leeuwenhoek royal society member with isaac newton", "# Reasoning Path:\nIsaac Newton -> base.kwebbase.kwtopic.connections_to -> abraham de moivre studied work of isaac newton -> base.kwebbase.kwconnection.sentence -> On his release in 1686, he fled to England, where he made a modest living tutoring in mathematics, and continued to study recent mathematical texts, the most notable being Newton's 'Principia Mathematica.' In 1692 he met the astronomer Halley who was so impressed by him that he read his first paper (on Newton's calculus), to the Royal Society in 1695.\n# Answer:\nabraham de moivre studied work of isaac newton"], "ground_truth": ["Trinity College, Cambridge"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-397", "prediction": ["# Reasoning Path:\nHarvard University -> education.educational_institution.colors -> Crimson -> common.topic.notable_types -> Color\n# Answer:\nCrimson", "# Reasoning Path:\nHarvard University -> education.educational_institution.colors -> Crimson -> common.topic.image -> Carminic acid structure\n# Answer:\nCrimson", "# Reasoning Path:\nHarvard University -> education.educational_institution.colors -> Crimson -> base.roses.rose_color.roses_of_this_color -> Rosa 'Chevy Chase'\n# Answer:\nCrimson", "# Reasoning Path:\nHarvard University -> location.location.contains -> Gore Hall -> common.topic.notable_for -> g.1yw9k9gf0\n# Answer:\nGore Hall", "# Reasoning Path:\nHarvard University -> location.location.contains -> Massachusetts Hall -> location.location.containedby -> Massachusetts\n# Answer:\nMassachusetts Hall", "# Reasoning Path:\nHarvard University -> education.educational_institution.colors -> Crimson -> base.roses.rose_color.roses_of_this_color -> Rosa 'Chrysler Imperial'\n# Answer:\nCrimson", "# Reasoning Path:\nHarvard University -> location.location.contains -> Gore Hall -> freebase.valuenotation.is_reviewed -> Architect\n# Answer:\nGore Hall", "# Reasoning Path:\nHarvard University -> organization.endowed_organization.endowment -> m.0ct1ryp -> measurement_unit.dated_money_value.currency -> United States Dollar\n# Answer:\nm.0ct1ryp", "# Reasoning Path:\nHarvard University -> education.educational_institution.colors -> Crimson -> base.roses.rose_color.roses_of_this_color -> Rosa 'Crimson Glory'\n# Answer:\nCrimson", "# Reasoning Path:\nHarvard University -> organization.endowed_organization.endowment -> m.0znsncs -> measurement_unit.dated_money_value.currency -> United States Dollar\n# Answer:\nm.0znsncs"], "ground_truth": ["Crimson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-398", "prediction": ["# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Colorado\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> Colorado -> location.location.containedby -> United States of America\n# Answer:\nColorado", "# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> Colorado -> location.location.containedby -> Contiguous United States\n# Answer:\nColorado", "# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Colorado\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> Colorado -> location.location.containedby -> Mountain States\n# Answer:\nColorado", "# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> Colorado Springs -> location.location.containedby -> Colorado\n# Answer:\nColorado Springs", "# Reasoning Path:\nUniversity of the Rockies -> organization.organization.headquarters -> m.0cnpmv9 -> common.topic.notable_types -> Location\n# Answer:\nm.0cnpmv9", "# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> Colorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Cavalry Regiment\n# Answer:\nColorado", "# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Alabama\n# Answer:\nUnited States of America"], "ground_truth": ["Colorado Springs", "Colorado", "United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-399", "prediction": ["# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Breast cancer -> medicine.disease.notable_people_with_this_condition -> Amanda Mealing\n# Answer:\nBreast cancer", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Breast cancer -> medicine.disease.notable_people_with_this_condition -> Amy Mickelson\n# Answer:\nBreast cancer", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Breast cancer -> medicine.disease.includes_diseases -> Inflammatory breast cancer\n# Answer:\nBreast cancer", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Myelodysplastic syndrome -> people.cause_of_death.people -> Charles Smart\n# Answer:\nMyelodysplastic syndrome", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Myelodysplastic syndrome -> medicine.disease.medical_specialties -> Hematology\n# Answer:\nMyelodysplastic syndrome", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Breast cancer -> medicine.disease.notable_people_with_this_condition -> Amy Robach\n# Answer:\nBreast cancer", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Breast cancer -> common.topic.webpage -> m.04mwg6d\n# Answer:\nBreast cancer", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Myelodysplastic syndrome -> base.schemastaging.context_name.pronunciation -> g.125_pcw3q\n# Answer:\nMyelodysplastic syndrome", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Breast cancer -> medicine.disease.includes_diseases -> HER2-Negative Breast Cancer\n# Answer:\nBreast cancer", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Myelodysplastic syndrome -> people.cause_of_death.people -> Dave Madden\n# Answer:\nMyelodysplastic syndrome"], "ground_truth": ["Breast cancer", "Myelodysplastic syndrome"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-400", "prediction": ["# Reasoning Path:\nByzantine Empire -> location.location.containedby -> Near East -> location.location.containedby -> Western Asia\n# Answer:\nNear East", "# Reasoning Path:\nByzantine Empire -> location.location.containedby -> Southern Europe -> location.location.containedby -> Europe\n# Answer:\nSouthern Europe", "# Reasoning Path:\nByzantine Empire -> location.location.containedby -> Near East -> location.location.contains -> Anatolia\n# Answer:\nNear East", "# Reasoning Path:\nByzantine Empire -> location.country.form_of_government -> Autocracy -> government.form_of_government.countries -> Roman Empire\n# Answer:\nAutocracy", "# Reasoning Path:\nByzantine Empire -> location.location.containedby -> Near East -> location.location.contains -> Arminiya\n# Answer:\nNear East", "# Reasoning Path:\nByzantine Empire -> location.location.containedby -> Near East -> location.location.events -> Arab\u2013Israeli conflict\n# Answer:\nNear East", "# Reasoning Path:\nByzantine Empire -> location.location.events -> Battle of Akroinon -> common.topic.article -> m.0cghj9\n# Answer:\nBattle of Akroinon", "# Reasoning Path:\nByzantine Empire -> location.location.containedby -> Southern Europe -> location.location.events -> Mediterranean and Middle East theatre of World War II\n# Answer:\nSouthern Europe", "# Reasoning Path:\nByzantine Empire -> location.country.form_of_government -> Absolute monarchy -> common.topic.image -> Cardin Le Bret (1558-1665) jeden z teoretyk\u00f3w absolutyzmu z ojcem\n# Answer:\nAbsolute monarchy", "# Reasoning Path:\nByzantine Empire -> location.location.containedby -> Near East -> location.location.contains -> Levant\n# Answer:\nNear East"], "ground_truth": ["Constantinople"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-401", "prediction": ["# Reasoning Path:\nEdward Scissorhands -> film.film_character.portrayed_in_films -> m.0jxqvs -> film.performance.actor -> Johnny Depp\n# Answer:\nm.0jxqvs"], "ground_truth": ["Johnny Depp"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-402", "prediction": ["# Reasoning Path:\nSlipknot -> music.musical_group.member -> m.010ky_w5 -> music.group_membership.role -> Drums\n# Answer:\nm.010ky_w5", "# Reasoning Path:\nJoey Jordison -> music.group_member.membership -> m.010ky_w5 -> music.group_membership.role -> Drums\n# Answer:\nm.010ky_w5", "# Reasoning Path:\nSlipknot -> music.musical_group.member -> g.11b7w8xg0_\n# Answer:\ng.11b7w8xg0_", "# Reasoning Path:\nSlipknot -> music.musical_group.member -> m.010ky_w5 -> music.group_membership.member -> Joey Jordison\n# Answer:\nm.010ky_w5", "# Reasoning Path:\nSlipknot -> music.musical_group.member -> g.11btvgynpj\n# Answer:\ng.11btvgynpj", "# Reasoning Path:\nSlipknot -> music.musical_group.member -> m.010ky_w5 -> freebase.valuenotation.is_reviewed -> Group\n# Answer:\nm.010ky_w5", "# Reasoning Path:\nJoey Jordison -> music.artist.track_contributions -> m.0fc0681 -> music.track_contribution.role -> Drums\n# Answer:\nm.0fc0681", "# Reasoning Path:\nJoey Jordison -> music.artist.track_contributions -> m.0dyl34p -> music.track_contribution.role -> Drums\n# Answer:\nm.0dyl34p", "# Reasoning Path:\nJoey Jordison -> music.group_member.membership -> m.010ky_w5 -> freebase.valuenotation.is_reviewed -> Group\n# Answer:\nm.010ky_w5", "# Reasoning Path:\nSlipknot -> music.musical_group.member -> m.010ky_w5 -> freebase.valuenotation.is_reviewed -> Member\n# Answer:\nm.010ky_w5"], "ground_truth": ["Drums"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-403", "prediction": ["# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Ladino Language -> common.topic.notable_types -> Human Language\n# Answer:\nLadino Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nArabic Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Ladino Language -> language.human_language.region -> Asia\n# Answer:\nLadino Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Kurdish languages -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nKurdish languages", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Arabic Language -> common.topic.notable_types -> Human Language\n# Answer:\nArabic Language", "# Reasoning Path:\nTurkey -> location.statistical_region.minimum_wage -> g.11b60lkkk3\n# Answer:\ng.11b60lkkk3", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Kurdish languages -> common.topic.notable_types -> Human Language\n# Answer:\nKurdish languages", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Algeria\n# Answer:\nArabic Language", "# Reasoning Path:\nTurkey -> location.statistical_region.minimum_wage -> g.11b7gsv6fr\n# Answer:\ng.11b7gsv6fr", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Iran\n# Answer:\nArabic Language"], "ground_truth": ["Turkish Language", "Arabic Language", "Ladino Language", "Zaza language", "Kurdish languages"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6857142857142857, "ans_precission": 0.8, "ans_recall": 0.6, "path_f1": 0.6857142857142857, "path_precision": 0.8, "path_recall": 0.6, "path_ans_f1": 0.6857142857142857, "path_ans_precision": 0.8, "path_ans_recall": 0.6}
{"id": "WebQTest-404", "prediction": ["# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04qmsr4 -> basketball.basketball_player_stats.team -> Cleveland Cavaliers\n# Answer:\nm.04qmsr4", "# Reasoning Path:\nLeBron James -> sports.pro_athlete.teams -> m.011461z_ -> sports.sports_team_roster.team -> Cleveland Cavaliers\n# Answer:\nm.011461z_", "# Reasoning Path:\nLeBron James -> sports.pro_athlete.teams -> m.02398d8 -> sports.sports_team_roster.team -> Miami Heat\n# Answer:\nm.02398d8", "# Reasoning Path:\nLeBron James -> sports.pro_athlete.teams -> m.012z6mb1 -> sports.sports_team_roster.position -> Point forward\n# Answer:\nm.012z6mb1", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04qmsr4 -> basketball.basketball_player_stats.season -> 2003\u201304 NBA season\n# Answer:\nm.04qmsr4", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04qcjh2 -> basketball.basketball_player_stats.team -> Cleveland Cavaliers\n# Answer:\nm.04qcjh2", "# Reasoning Path:\nLeBron James -> sports.pro_athlete.teams -> m.011461z_ -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.011461z_", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04ql_x2 -> basketball.basketball_player_stats.team -> Cleveland Cavaliers\n# Answer:\nm.04ql_x2", "# Reasoning Path:\nLeBron James -> sports.pro_athlete.teams -> m.02398d8 -> freebase.valuenotation.is_reviewed -> Player\n# Answer:\nm.02398d8", "# Reasoning Path:\nLeBron James -> award.award_nominee.award_nominations -> m.010_xz3j -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nm.010_xz3j"], "ground_truth": ["Cleveland Cavaliers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4210526315789474, "path_precision": 0.4, "path_recall": 0.4444444444444444, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-405", "prediction": ["# Reasoning Path:\nBattle of Gettysburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> military.military_combatant.military_commanders -> m.048z_8v\n# Answer:\nConfederate States of America", "# Reasoning Path:\nBattle of Gettysburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> book.book_subject.works -> Fighting for the Confederacy\n# Answer:\nConfederate States of America", "# Reasoning Path:\nBattle of Gettysburg -> base.culturalevent.event.entity_involved -> George Meade -> common.topic.article -> m.012pmt\n# Answer:\nGeorge Meade", "# Reasoning Path:\nBattle of Gettysburg -> military.military_conflict.military_personnel_involved -> George Meade -> common.topic.article -> m.012pmt\n# Answer:\nGeorge Meade", "# Reasoning Path:\nBattle of Gettysburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> military.military_combatant.military_commanders -> m.049y2m5\n# Answer:\nConfederate States of America", "# Reasoning Path:\nBattle of Gettysburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> military.military_combatant.military_commanders -> m.049y2qw\n# Answer:\nConfederate States of America", "# Reasoning Path:\nBattle of Gettysburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> book.book_subject.works -> Apostles of Disunion: Southern Secession Commissioners and the Causes of the Civil War\n# Answer:\nConfederate States of America", "# Reasoning Path:\nBattle of Gettysburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> location.country.capital -> Danville\n# Answer:\nConfederate States of America", "# Reasoning Path:\nBattle of Gettysburg -> base.culturalevent.event.entity_involved -> George Meade -> military.military_person.participated_in_conflicts -> American Civil War\n# Answer:\nGeorge Meade", "# Reasoning Path:\nBattle of Gettysburg -> military.military_conflict.military_personnel_involved -> George Meade -> military.military_person.participated_in_conflicts -> American Civil War\n# Answer:\nGeorge Meade"], "ground_truth": ["George Meade", "Robert E. Lee", "Union", "Confederate States of America", "United States of America"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 1.0, "ans_recall": 0.4, "path_f1": 0.03902439024390244, "path_precision": 0.8, "path_recall": 0.02, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 1.0, "path_ans_recall": 0.6}
{"id": "WebQTest-406", "prediction": ["# Reasoning Path:\nSan Francisco Giants -> baseball.baseball_team.current_manager -> Bruce Bochy -> baseball.baseball_manager.former_teams_managed -> m.0j7zl02\n# Answer:\nBruce Bochy", "# Reasoning Path:\nSan Francisco Giants -> baseball.baseball_team.current_manager -> Bruce Bochy -> baseball.baseball_player.position_s -> Manager\n# Answer:\nBruce Bochy", "# Reasoning Path:\nSan Francisco Giants -> baseball.baseball_team.current_manager -> Bruce Bochy -> people.person.nationality -> France\n# Answer:\nBruce Bochy", "# Reasoning Path:\nSan Francisco Giants -> baseball.baseball_team.current_manager -> Bruce Bochy -> people.person.nationality -> United States of America\n# Answer:\nBruce Bochy", "# Reasoning Path:\nSan Francisco Giants -> baseball.baseball_team.team_stats -> m.05n69_j -> baseball.baseball_team_stats.season -> 1965 Major League Baseball Season\n# Answer:\nm.05n69_j", "# Reasoning Path:\nSan Francisco Giants -> baseball.baseball_team.current_manager -> Bruce Bochy -> baseball.baseball_player.position_s -> Catcher\n# Answer:\nBruce Bochy", "# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.league -> m.0crt4b6 -> sports.sports_league_participation.league -> Major League Baseball\n# Answer:\nm.0crt4b6", "# Reasoning Path:\nSan Francisco Giants -> baseball.baseball_team.team_stats -> m.05n69my -> baseball.baseball_team_stats.season -> 1959 Major League Baseball Season\n# Answer:\nm.05n69my", "# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.league -> m.0crt7c2 -> sports.sports_league_participation.league -> National League\n# Answer:\nm.0crt7c2", "# Reasoning Path:\nSan Francisco Giants -> baseball.baseball_team.team_stats -> m.05n69l5 -> baseball.baseball_team_stats.season -> 1958 Major League Baseball Season\n# Answer:\nm.05n69l5"], "ground_truth": ["Ron Wotus", "Tim Flannery", "Mark Gardner", "Carney Lansford", "Roberto Kelly", "Dave Righetti", "Bill Hayes"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-408", "prediction": ["# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Bud Adams -> award.award_winner.awards_won -> m.0_qxnkc\n# Answer:\nBud Adams", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Bud Adams -> people.deceased_person.cause_of_death -> Natural causes\n# Answer:\nBud Adams", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Lamar Hunt -> people.person.parents -> H. L. Hunt\n# Answer:\nLamar Hunt", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Bud Adams -> common.topic.article -> m.035yc8\n# Answer:\nBud Adams", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Lamar Hunt -> sports.pro_athlete.teams -> m.0hsx4r3\n# Answer:\nLamar Hunt", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Lamar Hunt -> common.topic.notable_types -> American football player\n# Answer:\nLamar Hunt", "# Reasoning Path:\nAmerican Football League -> sports.sports_league.sport -> American football -> sports.sport.team_coaches -> Bob Hewko\n# Answer:\nAmerican football", "# Reasoning Path:\nAmerican Football League -> sports.sports_league.sport -> American football -> sports.sport.team_coaches -> Bobby Petrino\n# Answer:\nAmerican football", "# Reasoning Path:\nAmerican Football League -> sports.sports_league.teams -> m.0crthm0 -> sports.sports_league_participation.team -> Oakland Raiders\n# Answer:\nm.0crthm0", "# Reasoning Path:\nAmerican Football League -> sports.sports_league.sport -> American football -> sports.sport.team_coaches -> Charles Spearman\n# Answer:\nAmerican football"], "ground_truth": ["Lamar Hunt", "Bud Adams"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-409", "prediction": ["# Reasoning Path:\nGiuliana Rancic -> people.person.place_of_birth -> Naples -> travel.travel_destination.tourist_attractions -> Cappella Sansevero\n# Answer:\nNaples", "# Reasoning Path:\nGiuliana Rancic -> people.person.place_of_birth -> Naples -> travel.travel_destination.tourist_attractions -> Mount Vesuvius\n# Answer:\nNaples", "# Reasoning Path:\nGiuliana Rancic -> people.person.place_of_birth -> Naples -> location.location.events -> 1997 Napoli Film Festival\n# Answer:\nNaples", "# Reasoning Path:\nGiuliana Rancic -> base.popstra.celebrity.dated -> m.065p_0q -> base.popstra.dated.participant -> Jerry O'Connell\n# Answer:\nm.065p_0q", "# Reasoning Path:\nGiuliana Rancic -> people.person.place_of_birth -> Naples -> travel.travel_destination.tourist_attractions -> Museo di Capodimonte\n# Answer:\nNaples", "# Reasoning Path:\nGiuliana Rancic -> people.person.place_of_birth -> Naples -> location.statistical_region.population -> g.11b7t8558d\n# Answer:\nNaples", "# Reasoning Path:\nGiuliana Rancic -> film.person_or_entity_appearing_in_film.films -> m.0w17pwb -> film.personal_film_appearance.type_of_appearance -> Host\n# Answer:\nm.0w17pwb", "# Reasoning Path:\nGiuliana Rancic -> people.person.place_of_birth -> Naples -> location.location.events -> 1998 Napoli Film Festival\n# Answer:\nNaples", "# Reasoning Path:\nGiuliana Rancic -> people.person.place_of_birth -> Naples -> location.location.events -> 1999 Napoli Film Festival\n# Answer:\nNaples", "# Reasoning Path:\nGiuliana Rancic -> people.person.place_of_birth -> Naples -> location.statistical_region.population -> g.11btt4_t0l\n# Answer:\nNaples"], "ground_truth": ["Naples"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> base.aareas.schema.administrative_area.administrative_parent -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.containedby -> Americas\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Pacific Ocean\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> common.topic.article -> m.03660h\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Pacific Ocean -> location.location.containedby -> World Ocean\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.primarily_containedby -> South America\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Baltra Island -> location.location.containedby -> Santa Cruz Canton, Ecuador\n# Answer:\nBaltra Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.partially_containedby -> Pacific Ocean\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Floreana Island -> symbols.namesake.named_after -> Juan Jos\u00e9 Flores\n# Answer:\nFloreana Island"], "ground_truth": ["Pacific Ocean", "Gal\u00e1pagos Province", "Ecuador"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-410", "prediction": ["# Reasoning Path:\nUkraine -> location.country.capital -> Kiev -> travel.travel_destination.tourist_attractions -> Kiev Pechersk Lavra\n# Answer:\nKiev", "# Reasoning Path:\nUkraine -> location.country.capital -> Kiev -> sports.sports_team_location.teams -> BC Budivelnyk\n# Answer:\nKiev", "# Reasoning Path:\nUkraine -> location.country.capital -> Kiev -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nKiev", "# Reasoning Path:\nUkraine -> base.aareas.schema.administrative_area.administrative_children -> Cherkasy Oblast -> location.location.contains -> Kaniv\n# Answer:\nCherkasy Oblast", "# Reasoning Path:\nUkraine -> location.country.capital -> Kiev -> travel.travel_destination.tourist_attractions -> St Andrew's Church, Kiev\n# Answer:\nKiev", "# Reasoning Path:\nUkraine -> base.aareas.schema.administrative_area.administrative_children -> Cherkasy Oblast -> base.aareas.schema.administrative_area.administrative_area_type -> Ukrainian oblast\n# Answer:\nCherkasy Oblast", "# Reasoning Path:\nUkraine -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6dc92bn\n# Answer:\ng.11b6dc92bn", "# Reasoning Path:\nUkraine -> base.aareas.schema.administrative_area.administrative_children -> Cherkasy Oblast -> common.topic.notable_types -> Ukraine Oblast\n# Answer:\nCherkasy Oblast", "# Reasoning Path:\nUkraine -> base.aareas.schema.administrative_area.administrative_children -> Kiev -> travel.travel_destination.tourist_attractions -> Kiev Pechersk Lavra\n# Answer:\nKiev", "# Reasoning Path:\nUkraine -> base.aareas.schema.administrative_area.administrative_children -> Kiev -> sports.sports_team_location.teams -> BC Budivelnyk\n# Answer:\nKiev"], "ground_truth": ["Kiev"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-414", "prediction": ["# Reasoning Path:\nBrian Dawkins -> base.schemastaging.athlete_extra.salary -> m.0j3nt3p -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nm.0j3nt3p", "# Reasoning Path:\nBrian Dawkins -> base.schemastaging.athlete_extra.salary -> m.0j3nt3p -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0j3nt3p", "# Reasoning Path:\nBrian Dawkins -> common.image.size -> m.09hv3d2\n# Answer:\nm.09hv3d2", "# Reasoning Path:\nBrian Dawkins -> award.award_nominee.award_nominations -> m.0_qyjj3 -> freebase.valuenotation.has_no_value -> Ceremony\n# Answer:\nm.0_qyjj3", "# Reasoning Path:\nBrian Dawkins -> award.award_nominee.award_nominations -> m.0_qyjj3 -> award.award_nomination.award -> Walter Payton NFL Man of the Year Award\n# Answer:\nm.0_qyjj3", "# Reasoning Path:\nBrian Dawkins -> award.award_nominee.award_nominations -> m.0_qyjj3 -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nm.0_qyjj3"], "ground_truth": ["Denver Broncos"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.08333333333333333, "path_precision": 0.16666666666666666, "path_recall": 0.05555555555555555, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-415", "prediction": ["# Reasoning Path:\nTupac Shakur -> film.actor.film -> m.0j_81z -> film.performance.film -> Juice\n# Answer:\nm.0j_81z", "# Reasoning Path:\nTupac Shakur -> film.actor.film -> m.02vb3h0 -> film.performance.film -> Poetic Justice\n# Answer:\nm.02vb3h0", "# Reasoning Path:\nTupac Shakur -> film.actor.film -> m.0j_81z -> film.performance.character -> Bishop\n# Answer:\nm.0j_81z", "# Reasoning Path:\nTupac Shakur -> film.actor.film -> m.02vcpnk -> film.performance.film -> Bullet\n# Answer:\nm.02vcpnk", "# Reasoning Path:\nTupac Shakur -> music.featured_artist.recordings -> Fatha Figga -> music.recording.canonical_version -> The Fatha Figga\n# Answer:\nFatha Figga", "# Reasoning Path:\nTupac Shakur -> music.producer.releases_produced -> Gang Related -> common.topic.notable_types -> Film\n# Answer:\nGang Related", "# Reasoning Path:\nTupac Shakur -> film.actor.film -> m.02vb3h0 -> film.performance.character -> Lucky\n# Answer:\nm.02vb3h0", "# Reasoning Path:\nTupac Shakur -> music.featured_artist.recordings -> 4 tha Hustlas -> common.topic.notable_types -> Musical Recording\n# Answer:\n4 tha Hustlas", "# Reasoning Path:\nTupac Shakur -> music.producer.releases_produced -> Gang Related -> film.film.language -> English Language\n# Answer:\nGang Related", "# Reasoning Path:\nTupac Shakur -> music.producer.releases_produced -> 15 Years on Death Row -> common.topic.notable_for -> g.1256xn5kv\n# Answer:\n15 Years on Death Row"], "ground_truth": ["Poetic Justice", "Juice", "Murder Was the Case", "Above the Rim", "Gridlock'd", "Gang Related", "Nothing but Trouble", "Bullet"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.15384615384615385, "ans_precission": 0.2, "ans_recall": 0.125, "path_f1": 0.34285714285714286, "path_precision": 0.4, "path_recall": 0.3, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-416", "prediction": ["# Reasoning Path:\nChina -> location.country.currency_used -> Renminbi -> common.topic.notable_types -> Currency\n# Answer:\nRenminbi", "# Reasoning Path:\nChina -> location.location.partially_contains -> Amur River -> location.location.partially_containedby -> Russia\n# Answer:\nAmur River", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Balikun horse -> common.topic.notable_types -> Organism Classification\n# Answer:\nBalikun horse", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Black Telescope -> biology.animal_breed.breed_of -> Goldfish\n# Answer:\nBlack Telescope", "# Reasoning Path:\nChina -> location.location.partially_contains -> Arun River, China-Nepal -> location.location.partially_containedby -> Nepal\n# Answer:\nArun River, China-Nepal", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Bone-mouth -> common.topic.notable_types -> Organism Classification\n# Answer:\nBone-mouth", "# Reasoning Path:\nChina -> location.location.partially_contains -> Argun River -> common.topic.notable_types -> River\n# Answer:\nArgun River", "# Reasoning Path:\nChina -> location.location.partially_contains -> Amur River -> common.topic.notable_types -> River\n# Answer:\nAmur River", "# Reasoning Path:\nChina -> location.location.partially_contains -> Arun River, China-Nepal -> geography.river.basin_countries -> Nepal\n# Answer:\nArun River, China-Nepal", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Black Telescope -> biology.organism_classification.rank -> Breed\n# Answer:\nBlack Telescope"], "ground_truth": ["Renminbi"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-417", "prediction": ["# Reasoning Path:\nKenneth Parcell -> tv.tv_character.appeared_in_tv_program -> m.03hbzrb -> tv.regular_tv_appearance.actor -> Jack McBrayer\n# Answer:\nm.03hbzrb", "# Reasoning Path:\nKenneth Parcell -> fictional_universe.fictional_character.character_created_by -> Tina Fey -> fictional_universe.fictional_character_creator.fictional_characters_created -> Jack Donaghy\n# Answer:\nTina Fey", "# Reasoning Path:\nKenneth Parcell -> fictional_universe.fictional_character.character_created_by -> Tina Fey -> people.person.profession -> Screenwriter\n# Answer:\nTina Fey", "# Reasoning Path:\nKenneth Parcell -> tv.tv_character.appeared_in_tv_program -> m.03hbzrb -> tv.regular_tv_appearance.seasons -> 30 Rock - Season 1\n# Answer:\nm.03hbzrb", "# Reasoning Path:\nKenneth Parcell -> fictional_universe.fictional_character.character_created_by -> Tina Fey -> fictional_universe.fictional_character_creator.fictional_characters_created -> Cerie\n# Answer:\nTina Fey", "# Reasoning Path:\nKenneth Parcell -> fictional_universe.fictional_character.character_created_by -> Tina Fey -> people.person.profession -> Television producer\n# Answer:\nTina Fey", "# Reasoning Path:\nKenneth Parcell -> tv.tv_character.appeared_in_tv_program -> m.03hbzrb -> tv.regular_tv_appearance.seasons -> 30 Rock - Season 2\n# Answer:\nm.03hbzrb", "# Reasoning Path:\nKenneth Parcell -> fictional_universe.fictional_character.character_created_by -> Tina Fey -> fictional_universe.fictional_character_creator.fictional_characters_created -> Danny Baker\n# Answer:\nTina Fey", "# Reasoning Path:\nKenneth Parcell -> fictional_universe.fictional_character.character_created_by -> Tina Fey -> people.person.profession -> Actor\n# Answer:\nTina Fey", "# Reasoning Path:\nKenneth Parcell -> tv.tv_character.appeared_in_tv_program -> m.03hbzrb -> tv.regular_tv_appearance.seasons -> 30 Rock - Season 3\n# Answer:\nm.03hbzrb"], "ground_truth": ["Jack McBrayer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-418", "prediction": ["# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.division -> AFC South -> common.topic.article -> m.068g_7\n# Answer:\nAFC South", "# Reasoning Path:\nIndianapolis Colts -> sports.sports_team.league -> m.0crt4sl -> sports.sports_league_participation.league -> National Football League\n# Answer:\nm.0crt4sl", "# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.division -> AFC South -> american_football.football_division.teams -> Tennessee Titans\n# Answer:\nAFC South", "# Reasoning Path:\nIndianapolis Colts -> sports.sports_team.league -> m.0crt9j5 -> sports.sports_league_participation.league -> American Football Conference\n# Answer:\nm.0crt9j5", "# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.division -> AFC South -> sports.sports_league.teams -> m.0crtf9v\n# Answer:\nAFC South", "# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.division -> AFC South -> american_football.football_division.teams -> Houston Texans\n# Answer:\nAFC South", "# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.division -> AFC South -> american_football.football_division.teams -> Jacksonville Jaguars\n# Answer:\nAFC South", "# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.division -> AFC South -> sports.sports_league.teams -> m.0crtfb5\n# Answer:\nAFC South", "# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.division -> AFC South -> sports.sports_league.teams -> m.0crtfbg\n# Answer:\nAFC South", "# Reasoning Path:\nIndianapolis Colts -> sports.sports_team.league -> m.0crtfbg -> sports.sports_league_participation.league -> AFC South\n# Answer:\nm.0crtfbg"], "ground_truth": ["AFC South", "National Football League", "American Football Conference"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.45161290322580644, "ans_precission": 0.7, "ans_recall": 0.3333333333333333, "path_f1": 0.6153846153846154, "path_precision": 0.8, "path_recall": 0.5, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-419", "prediction": ["# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.location.people_born_here -> Afonso Mendes de Melo\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> g.11btv6zfvr\n# Answer:\ng.11btv6zfvr", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.location.people_born_here -> Afonso Pires Gato\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> government.governmental_jurisdiction.governing_officials -> m.04_9736\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.location.people_born_here -> Ant\u00f3nio Lu\u00eds de Sousa, 2nd Marquis of Minas\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.country.capital -> Coimbra\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> symbols.name_source.namesakes -> Magellan -> common.topic.article -> m.01m0j_\n# Answer:\nMagellan", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> government.governmental_jurisdiction.governing_officials -> m.04_975f\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> government.governmental_jurisdiction.governing_officials -> m.0c0mt_c\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.country.capital -> Lisbon\n# Answer:\nKingdom of Portugal"], "ground_truth": ["Kingdom of Portugal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> Somebody to Love\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.role -> Vocals\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> Eenie Meenie -> music.composition.composer -> Benny Blanco\n# Answer:\nEenie Meenie", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> Eenie Meenie -> music.recording.artist -> Sean Kingston\n# Answer:\nEenie Meenie", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> Eenie Meenie -> music.composition.composer -> Sean Kingston\n# Answer:\nEenie Meenie", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.featured_artists -> Ludacris\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.artist -> Ludacris\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> music.composition.composer -> Andre Harris\n# Answer:\nAll Bad"], "ground_truth": ["Roller Coaster", "Wait for a Minute", "Right Here", "Never Let You Go", "Beauty And A Beat", "Live My Life", "As Long as You Love Me", "Pray", "Die in Your Arms", "Home to Mama", "#thatPower", "All Bad", "Recovery", "Eenie Meenie", "Lolly", "PYD", "Change Me", "First Dance", "Bad Day", "Thought Of You", "Confident", "Boyfriend", "Heartbreaker", "All Around The World", "All That Matters", "Never Say Never", "Baby", "Somebody to Love", "Turn to You (Mother's Day Dedication)", "Hold Tight", "Bigger"], "ans_acc": 0.12903225806451613, "ans_hit": 1, "ans_f1": 0.16666666666666666, "ans_precission": 0.6, "ans_recall": 0.0967741935483871, "path_f1": 0.05825242718446601, "path_precision": 0.3, "path_recall": 0.03225806451612903, "path_ans_f1": 0.2178988326848249, "path_ans_precision": 0.7, "path_ans_recall": 0.12903225806451613}
{"id": "WebQTest-420", "prediction": ["# Reasoning Path:\nPerpignan -> location.location.containedby -> Pyr\u00e9n\u00e9es-Orientales -> location.location.containedby -> Languedoc-Roussillon\n# Answer:\nPyr\u00e9n\u00e9es-Orientales", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Languedoc-Roussillon -> location.location.containedby -> France\n# Answer:\nLanguedoc-Roussillon", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Languedoc-Roussillon -> location.location.containedby -> Metropolitan France\n# Answer:\nLanguedoc-Roussillon", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Languedoc-Roussillon -> location.location.people_born_here -> Miguel Lluch\n# Answer:\nLanguedoc-Roussillon", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Languedoc-Roussillon -> location.location.containedby -> Pyrenees\u2013Mediterranean Euroregion\n# Answer:\nLanguedoc-Roussillon", "# Reasoning Path:\nPerpignan -> location.location.containedby -> France -> location.location.containedby -> Europe\n# Answer:\nFrance", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Pyr\u00e9n\u00e9es-Orientales -> location.location.containedby -> France\n# Answer:\nPyr\u00e9n\u00e9es-Orientales", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Pyr\u00e9n\u00e9es-Orientales -> common.topic.image -> Location of Pyr\u00e9n\u00e9es-Orientales in France\n# Answer:\nPyr\u00e9n\u00e9es-Orientales", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Languedoc-Roussillon -> common.topic.image -> Flag of Languedoc-Roussillon\n# Answer:\nLanguedoc-Roussillon", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Pyr\u00e9n\u00e9es-Orientales -> location.location.containedby -> Languedoc-Roussillon-Midi-Pyr\u00e9n\u00e9es\n# Answer:\nPyr\u00e9n\u00e9es-Orientales"], "ground_truth": ["Pyr\u00e9n\u00e9es-Orientales", "Languedoc-Roussillon", "France"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-421", "prediction": ["# Reasoning Path:\nKim Richards -> people.person.spouse_s -> m.0j4kh7g -> people.marriage.spouse -> G. Monty Brinson\n# Answer:\nm.0j4kh7g", "# Reasoning Path:\nKim Richards -> people.person.spouse_s -> m.0j4kh7g -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0j4kh7g", "# Reasoning Path:\nKim Richards -> people.person.parents -> Ken Richards -> people.person.spouse_s -> m.0j4js1t\n# Answer:\nKen Richards", "# Reasoning Path:\nKim Richards -> people.person.spouse_s -> m.0j4kh7g -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.0j4kh7g", "# Reasoning Path:\nKim Richards -> people.person.spouse_s -> m.0kv3rtm -> people.marriage.spouse -> John Jackson\n# Answer:\nm.0kv3rtm", "# Reasoning Path:\nKim Richards -> people.person.spouse_s -> m.0swszf2 -> people.marriage.spouse -> Gregg Davis\n# Answer:\nm.0swszf2", "# Reasoning Path:\nKim Richards -> people.person.parents -> Kathleen Richards -> people.person.spouse_s -> m.0j4js1t\n# Answer:\nKathleen Richards", "# Reasoning Path:\nKim Richards -> people.person.spouse_s -> m.0kv3rtm -> people.marriage.type_of_union -> Domestic partnership\n# Answer:\nm.0kv3rtm", "# Reasoning Path:\nKim Richards -> people.person.parents -> Ken Richards -> people.person.children -> Kyle Richards\n# Answer:\nKen Richards", "# Reasoning Path:\nKim Richards -> tv.tv_actor.starring_roles -> m.0bngb3y -> tv.regular_tv_appearance.character -> Ruthie Alder\n# Answer:\nm.0bngb3y"], "ground_truth": ["G. Monty Brinson", "Gregg Davis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.26666666666666666, "path_precision": 0.2, "path_recall": 0.4, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-422", "prediction": ["# Reasoning Path:\nBosnia and Herzegovina -> location.location.events -> Bosnian War -> time.event.includes_event -> Operation Storm\n# Answer:\nBosnian War", "# Reasoning Path:\nBosnia and Herzegovina -> location.location.events -> Bosnian War -> time.event.includes_event -> Ahatovi\u0107i massacre\n# Answer:\nBosnian War", "# Reasoning Path:\nBosnia and Herzegovina -> location.location.events -> Bosnian War -> military.military_conflict.military_personnel_involved -> Ratko Mladi\u0107\n# Answer:\nBosnian War", "# Reasoning Path:\nBosnia and Herzegovina -> location.location.events -> Bosnian War -> time.event.includes_event -> Battle of Kupres\n# Answer:\nBosnian War", "# Reasoning Path:\nBosnia and Herzegovina -> location.location.events -> Bosnian War -> base.culturalevent.event.entity_involved -> Alija Izetbegovi\u0107\n# Answer:\nBosnian War", "# Reasoning Path:\nBosnia and Herzegovina -> location.location.events -> Bosnian Genocide -> common.topic.notable_for -> g.12h2v8hs9\n# Answer:\nBosnian Genocide", "# Reasoning Path:\nBosnia and Herzegovina -> location.location.events -> Bosnian War -> military.military_conflict.military_personnel_involved -> Janko Bobetko\n# Answer:\nBosnian War", "# Reasoning Path:\nBosnia and Herzegovina -> location.location.events -> Operation Deliberate Force -> military.military_conflict.military_personnel_involved -> Andrew Hall\n# Answer:\nOperation Deliberate Force", "# Reasoning Path:\nBosnia and Herzegovina -> location.statistical_region.health_expenditure_as_percent_of_gdp -> g.12cp_j3km\n# Answer:\ng.12cp_j3km", "# Reasoning Path:\nBosnia and Herzegovina -> location.location.events -> Bosnian War -> base.culturalevent.event.entity_involved -> Fikret Abdi\u0107\n# Answer:\nBosnian War"], "ground_truth": ["Operation Neretva '93", "Bosnian War", "Operation Corridor 92", "Operation Una", "Massacre in Grabovica", "Operation Sana", "Vi\u0161egrad massacres", "Operation Storm", "Glogova massacre", "Operation Deliberate Force", "La\u0161va Valley ethnic cleansing", "Battle of Kupres", "Operation Miracle", "Ahatovi\u0107i massacre", "Ahmi\u0107i massacre", "Operation Deny Flight", "Sjeverin massacre", "Operation Summer '95", "\u010cemerno massacre", "Stupni Do massacre", "NATO intervention in Bosnia and Herzegovina", "Operation Mistral 2", "Operation Southern Move", "Bosnian Genocide", "Operation Spider", "\u0160trpci massacre", "Sovi\u0107i massacre", "Operation Jackal", "Croat\u2013Bosniak War", "Yugoslav Wars", "Fo\u010da massacres", "Operation Winter '94", "Battle of Hasselt", "Operation Tiger"], "ans_acc": 0.17647058823529413, "ans_hit": 1, "ans_f1": 0.16071428571428573, "ans_precission": 0.9, "ans_recall": 0.08823529411764706, "path_f1": 0.1565217391304348, "path_precision": 0.9, "path_recall": 0.08571428571428572, "path_ans_f1": 0.2950819672131148, "path_ans_precision": 0.9, "path_ans_recall": 0.17647058823529413}
{"id": "WebQTest-423", "prediction": ["# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> Super Bowl X -> time.event.locations -> Miami Orange Bowl\n# Answer:\nSuper Bowl X", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> 2011 AFC Championship Game -> common.topic.notable_for -> g.1z2srjz6h\n# Answer:\n2011 AFC Championship Game", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> Super Bowl X -> time.event.instance_of_recurring_event -> Super Bowl\n# Answer:\nSuper Bowl X", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> 2009 AFC Championship Game -> time.event.instance_of_recurring_event -> AFC Championship Game\n# Answer:\n2009 AFC Championship Game", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> 2011 AFC Championship Game -> time.event.locations -> Heinz Field\n# Answer:\n2011 AFC Championship Game", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> 2009 AFC Championship Game -> sports.sports_championship_event.runner_up -> Baltimore Ravens\n# Answer:\n2009 AFC Championship Game", "# Reasoning Path:\nPittsburgh Steelers -> american_football.football_team.current_head_coach -> Mike Tomlin -> people.person.place_of_birth -> Hampton\n# Answer:\nMike Tomlin", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> 2009 AFC Championship Game -> common.topic.notable_for -> g.1z2st10mn\n# Answer:\n2009 AFC Championship Game", "# Reasoning Path:\nPittsburgh Steelers -> common.topic.webpage -> m.03nbxbp -> common.webpage.category -> Topic Webpage\n# Answer:\nm.03nbxbp", "# Reasoning Path:\nPittsburgh Steelers -> american_football.football_team.current_head_coach -> Mike Tomlin -> tv.tv_personality.tv_regular_appearances -> m.0wysfgg\n# Answer:\nMike Tomlin"], "ground_truth": ["Super Bowl XLIII"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-425", "prediction": ["# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.locations.countries.continent -> North America\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.country.form_of_government -> Parliamentary system\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> South Africa -> location.country.languages_spoken -> Arabic Language\n# Answer:\nSouth Africa", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> New Zealand -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nNew Zealand", "# Reasoning Path:\nEnglish Language -> education.school_category.schools_of_this_kind -> Piers Midwinter -> people.person.quotations -> Learn with a NEST. Please be my GUEST. I will give you my BEST,\n# Answer:\nPiers Midwinter", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> South Africa -> location.country.form_of_government -> Parliamentary republic\n# Answer:\nSouth Africa", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> New Zealand -> location.country.languages_spoken -> Standard Chinese\n# Answer:\nNew Zealand", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> South Africa -> location.country.official_language -> Afrikaans Language\n# Answer:\nSouth Africa", "# Reasoning Path:\nEnglish Language -> education.school_category.schools_of_this_kind -> Piers Midwinter -> common.topic.notable_types -> Organization founder\n# Answer:\nPiers Midwinter"], "ground_truth": ["Japan", "Qatar", "Kiribati", "Rwanda", "Samoa", "Zimbabwe", "Australia", "Jordan", "Bangladesh", "Turks and Caicos Islands", "State of Palestine", "Saint Vincent and the Grenadines", "United Kingdom", "Sudan", "Philippines", "Kingdom of Great Britain", "Malta", "Mandatory Palestine", "India", "Lesotho", "Wales", "South Africa", "Bermuda", "Kenya", "Ghana", "England", "Territory of New Guinea", "Malaysia", "South Yemen", "Barbados", "Cura\u00e7ao", "Bahamas", "Transkei", "Singapore", "Saint Kitts and Nevis", "Liberia", "Israel", "Puerto Rico", "Antigua and Barbuda", "Grenada", "Republic of Ireland", "Bonaire", "Swaziland", "Sri Lanka", "Timor-Leste", "Cook Islands", "Vanuatu", "Nauru", "Cyprus", "Marshall Islands", "China", "Montserrat", "Territory of Papua and New Guinea", "Uganda", "Gazankulu", "Indonesia", "Guam", "Dominica", "Pakistan", "Ethiopia", "Saint Lucia", "Tuvalu", "New Zealand", "Tokelau", "Honduras", "Papua New Guinea", "Gambia", "Canada", "Cayman Islands", "United States of America", "Tanzania", "Zambia", "Botswana", "Cameroon", "Hong Kong", "Nigeria", "Isle of Man", "Belize", "Gibraltar", "Namibia", "Laos", "Guyana", "Jersey", "Fiji", "Brunei", "Vatican City", "Sierra Leone"], "ans_acc": 0.034482758620689655, "ans_hit": 1, "ans_f1": 0.06611570247933884, "ans_precission": 0.8, "ans_recall": 0.034482758620689655, "path_f1": 0.06611570247933884, "path_precision": 0.8, "path_recall": 0.034482758620689655, "path_ans_f1": 0.06611570247933884, "path_ans_precision": 0.8, "path_ans_recall": 0.034482758620689655}
{"id": "WebQTest-426", "prediction": ["# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Giza -> travel.travel_destination.tourist_attractions -> Giza Plateau\n# Answer:\nGiza", "# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Giza -> travel.travel_destination.tourist_attractions -> Great Pyramid of Giza\n# Answer:\nGiza", "# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Giza -> location.location.containedby -> Giza Governorate\n# Answer:\nGiza", "# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Egypt -> location.location.containedby -> Arab world\n# Answer:\nEgypt", "# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Giza -> travel.travel_destination.tourist_attractions -> Al Amir Perfume Palace\n# Answer:\nGiza", "# Reasoning Path:\nGiza Necropolis -> film.film_location.featured_in_films -> Samsara -> film.film.featured_film_locations -> Egypt\n# Answer:\nSamsara", "# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Giza -> location.location.containedby -> Egypt\n# Answer:\nGiza", "# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Egypt -> location.location.containedby -> Middle East\n# Answer:\nEgypt", "# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Giza -> location.location.people_born_here -> Abdel Halim Ali\n# Answer:\nGiza", "# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Egypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives\n# Answer:\nEgypt"], "ground_truth": ["Egypt", "Giza"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-427", "prediction": ["# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79701 -> location.location.contains -> Bank of America Building\n# Answer:\n79701", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79701 -> common.topic.notable_for -> g.1256hz0nz\n# Answer:\n79701", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79705 -> location.postal_code.country -> United States of America\n# Answer:\n79705", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79701 -> common.topic.notable_types -> Postal Code\n# Answer:\n79701", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79702 -> common.topic.notable_for -> g.1257xzgff\n# Answer:\n79702", "# Reasoning Path:\nMidland -> location.statistical_region.population -> g.11b66cwzp7\n# Answer:\ng.11b66cwzp7", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79701 -> location.location.contains -> Centennial Tower\n# Answer:\n79701", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79705 -> common.topic.notable_for -> g.1256cn_z1\n# Answer:\n79705", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79701 -> location.location.contains -> Petroleum Building\n# Answer:\n79701", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79705 -> common.topic.notable_types -> Postal Code\n# Answer:\n79705"], "ground_truth": ["79704", "79705", "79706", "79702", "79708", "79712", "79701", "79710", "79703", "79707", "79711"], "ans_acc": 0.2727272727272727, "ans_hit": 1, "ans_f1": 0.41860465116279066, "ans_precission": 0.9, "ans_recall": 0.2727272727272727, "path_f1": 0.41860465116279066, "path_precision": 0.9, "path_recall": 0.2727272727272727, "path_ans_f1": 0.41860465116279066, "path_ans_precision": 0.9, "path_ans_recall": 0.2727272727272727}
{"id": "WebQTest-428", "prediction": ["# Reasoning Path:\nMidwestern United States -> location.location.contains -> Council Grove -> location.location.contains -> Butler Community College, Council Grove\n# Answer:\nCouncil Grove", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Council Grove -> location.location.contains -> Council Grove Historic District\n# Answer:\nCouncil Grove", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Council Grove -> location.location.containedby -> Kansas\n# Answer:\nCouncil Grove", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Missouri -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Council Grove -> location.location.contains -> Council Grove Missouri, Kansas and Texas Depot\n# Answer:\nCouncil Grove", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Council Grove -> location.location.containedby -> Morris County\n# Answer:\nCouncil Grove", "# Reasoning Path:\nMidwestern United States -> book.book_subject.works -> Caught in the Middle: America's Heartland in the Age of Globalism -> common.topic.article -> m.063wp6w\n# Answer:\nCaught in the Middle: America's Heartland in the Age of Globalism", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Council Grove -> location.location.containedby -> United States of America\n# Answer:\nCouncil Grove", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Council Grove -> location.statistical_region.population -> g.11b66ff3qx\n# Answer:\nCouncil Grove", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Missouri -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nMissouri"], "ground_truth": ["Minnesota", "Indiana", "Missouri", "Wisconsin", "Iowa", "Kansas", "West North Central States", "Nebraska", "South Dakota", "North Dakota", "Michigan", "Avenue of the Saints", "East North Central States", "Illinois", "Council Grove", "Ohio"], "ans_acc": 0.1875, "ans_hit": 1, "ans_f1": 0.21951219512195125, "ans_precission": 0.9, "ans_recall": 0.125, "path_f1": 0.21951219512195125, "path_precision": 0.9, "path_recall": 0.125, "path_ans_f1": 0.31034482758620696, "path_ans_precision": 0.9, "path_ans_recall": 0.1875}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Publisher -> common.topic.notable_types -> Editor title\n# Answer:\nPublisher", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Acetylene Lamp\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> common.topic.notable_types -> Profession\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> people.profession.corresponding_type -> Author\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Ali G\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> base.descriptive_names.names.descriptive_name -> m.0105cq_d\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> type.type.expected_by -> politician\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> symbols.name_source.namesakes -> Champs-\u00c9lys\u00e9es \u2013 Clemenceau -> common.topic.notable_types -> Transit Stop\n# Answer:\nChamps-\u00c9lys\u00e9es \u2013 Clemenceau", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> April Carver\n# Answer:\nJournalist"], "ground_truth": ["Journalist", "Writer", "Publisher", "Statesman", "Physician"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.48, "ans_precission": 0.6, "ans_recall": 0.4, "path_f1": 0.48, "path_precision": 0.6, "path_recall": 0.4, "path_ans_f1": 0.48, "path_ans_precision": 0.6, "path_ans_recall": 0.4}
{"id": "WebQTest-430", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.role -> Vocals\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> Somebody to Love\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0115qhzk -> award.award_honor.award -> MTV Europe Music Voices Award\n# Answer:\nm.0115qhzk", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber's Believe -> film.film.personal_appearances -> m.0101fs_z\n# Answer:\nJustin Bieber's Believe", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> freebase.valuenotation.is_reviewed -> Award category\n# Answer:\nm.010lkp2z", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> award.award_honor.ceremony -> 2014 Billboard Music Awards\n# Answer:\nm.010lkp2z", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> award.award_honor.award -> Billboard Music Award for Top Social Artist\n# Answer:\nm.010lkp2z", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber's Believe -> film.film.personal_appearances -> m.0101fsyr\n# Answer:\nJustin Bieber's Believe"], "ground_truth": ["Vocals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-431", "prediction": ["# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.main_country -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> New Zealand\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\ng.11b60qdnlp", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\ng.11b60vfr15", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> Japan\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Indonesia\n# Answer:\nEnglish Language"], "ground_truth": ["Lojban", "English Language", "Esperanto Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-432", "prediction": ["# Reasoning Path:\nAnnie Oakley -> people.person.parents -> Jacob Moses -> people.person.spouse_s -> m.0j55wzy\n# Answer:\nJacob Moses", "# Reasoning Path:\nAnnie Oakley -> people.person.parents -> Jacob Moses -> common.topic.notable_for -> g.125bnxfjq\n# Answer:\nJacob Moses", "# Reasoning Path:\nAnnie Oakley -> people.person.spouse_s -> m.0j4kg9x -> people.marriage.spouse -> Frank E. Butler\n# Answer:\nm.0j4kg9x", "# Reasoning Path:\nAnnie Oakley -> people.person.parents -> Jacob Moses -> common.topic.notable_types -> Deceased Person\n# Answer:\nJacob Moses", "# Reasoning Path:\nAnnie Oakley -> people.person.parents -> Susan Wise Moses -> people.person.spouse_s -> m.0j55wzy\n# Answer:\nSusan Wise Moses", "# Reasoning Path:\nAnnie Oakley -> people.person.spouse_s -> m.0j4kg9x -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0j4kg9x", "# Reasoning Path:\nAnnie Oakley -> people.person.parents -> Susan Wise Moses -> people.person.gender -> Female\n# Answer:\nSusan Wise Moses"], "ground_truth": ["Frank E. Butler"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-433", "prediction": ["# Reasoning Path:\nAlexander Luthor -> film.film_character.portrayed_in_films -> m.0k3zb8 -> film.performance.actor -> Kevin Spacey\n# Answer:\nm.0k3zb8", "# Reasoning Path:\nAlexander Luthor -> film.film_character.portrayed_in_films -> m.07ss97y -> film.performance.actor -> Chris Noth\n# Answer:\nm.07ss97y", "# Reasoning Path:\nAlexander Luthor -> film.film_character.portrayed_in_films -> m.0k3z7_ -> film.performance.actor -> Gene Hackman\n# Answer:\nm.0k3z7_", "# Reasoning Path:\nAlexander Luthor -> film.film_character.portrayed_in_films -> m.0k3zb8 -> film.performance.film -> Superman Returns\n# Answer:\nm.0k3zb8", "# Reasoning Path:\nAlexander Luthor -> film.film_character.portrayed_in_films -> m.07ss97y -> film.performance.special_performance_type -> Voice\n# Answer:\nm.07ss97y", "# Reasoning Path:\nAlexander Luthor -> fictional_universe.fictional_character.romantically_involved_with -> m.09ds17f -> fictional_universe.romantic_involvement.partner -> Lana Lang\n# Answer:\nm.09ds17f", "# Reasoning Path:\nAlexander Luthor -> base.fictionaluniverse.fictional_killer.characters_killed -> Lionel Luthor -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Metropolis\n# Answer:\nLionel Luthor", "# Reasoning Path:\nAlexander Luthor -> film.film_character.portrayed_in_films -> m.0k3z7_ -> film.performance.film -> Superman IV: The Quest for Peace\n# Answer:\nm.0k3z7_", "# Reasoning Path:\nAlexander Luthor -> base.fictionaluniverse.fictional_killer.characters_killed -> Lionel Luthor -> fictional_universe.fictional_character.character_created_by -> Alfred Gough\n# Answer:\nLionel Luthor", "# Reasoning Path:\nAlexander Luthor -> base.fictionaluniverse.fictional_killer.characters_killed -> Lionel Luthor -> fictional_universe.fictional_character.character_created_by -> Miles Millar\n# Answer:\nLionel Luthor"], "ground_truth": ["James Marsters", "Chris Noth", "Jesse Eisenberg", "Powers Boothe", "William Hootkins", "Anthony LaPaglia", "Kevin Spacey", "Gene Hackman", "Clancy Brown"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2608695652173913, "path_precision": 0.3, "path_recall": 0.23076923076923078, "path_ans_f1": 0.3157894736842105, "path_ans_precision": 0.3, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-434", "prediction": ["# Reasoning Path:\nZambezi -> geography.river.origin -> Mwinilunga -> location.location.containedby -> Zambia\n# Answer:\nMwinilunga", "# Reasoning Path:\nZambezi -> geography.river.origin -> Mwinilunga -> common.topic.notable_types -> Location\n# Answer:\nMwinilunga", "# Reasoning Path:\nZambezi -> geography.river.origin -> Mwinilunga -> common.topic.article -> m.07cdmx\n# Answer:\nMwinilunga", "# Reasoning Path:\nZambezi -> geography.river.cities -> Sesheke -> location.location.containedby -> Zambia\n# Answer:\nSesheke", "# Reasoning Path:\nZambezi -> geography.river.cities -> Mongu -> location.location.containedby -> Zambia\n# Answer:\nMongu", "# Reasoning Path:\nZambezi -> geography.river.cities -> Mongu -> location.statistical_region.population -> g.11bc87rxtx\n# Answer:\nMongu", "# Reasoning Path:\nZambezi -> geography.river.cities -> Sesheke -> common.topic.image -> Location of Sesheke in Zambia\n# Answer:\nSesheke", "# Reasoning Path:\nZambezi -> geography.river.cities -> Kariba, Zimbabwe -> location.location.nearby_airports -> Kariba Airport\n# Answer:\nKariba, Zimbabwe", "# Reasoning Path:\nZambezi -> geography.river.basin_countries -> Mozambique -> location.country.languages_spoken -> Portuguese Language\n# Answer:\nMozambique", "# Reasoning Path:\nZambezi -> geography.river.cities -> Sesheke -> common.topic.notable_for -> g.125614dst\n# Answer:\nSesheke"], "ground_truth": ["Mwinilunga"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-436", "prediction": ["# Reasoning Path:\nNigeria -> location.location.time_zones -> West Africa Time Zone\n# Answer:\nWest Africa Time Zone", "# Reasoning Path:\nNigeria -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Sierra Leone\n# Answer:\nEnglish Language", "# Reasoning Path:\nNigeria -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Liberia\n# Answer:\nEnglish Language", "# Reasoning Path:\nNigeria -> location.country.official_language -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_0sgz\n# Answer:\ng.1245_0sgz", "# Reasoning Path:\nNigeria -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Cameroon\n# Answer:\nEnglish Language", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_22x5\n# Answer:\ng.1245_22x5", "# Reasoning Path:\nNigeria -> location.country.official_language -> English Language -> language.human_language.main_country -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_2gxr\n# Answer:\ng.1245_2gxr", "# Reasoning Path:\nNigeria -> location.country.official_language -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language"], "ground_truth": ["West Africa Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-437", "prediction": ["# Reasoning Path:\nJenni Rivera -> people.deceased_person.place_of_death -> Iturbide -> location.location.geolocation -> m.0wmwfp5\n# Answer:\nIturbide", "# Reasoning Path:\nJenni Rivera -> people.deceased_person.place_of_death -> Iturbide -> common.topic.notable_for -> g.126t0vkgr\n# Answer:\nIturbide", "# Reasoning Path:\nJenni Rivera -> people.deceased_person.place_of_death -> Iturbide -> location.administrative_division.second_level_division_of -> Mexico\n# Answer:\nIturbide", "# Reasoning Path:\nJenni Rivera -> music.artist.album -> Jenni -> common.topic.notable_for -> g.125b9dwrj\n# Answer:\nJenni", "# Reasoning Path:\nJenni Rivera -> music.artist.album -> Jenni -> music.album.genre -> Banda\n# Answer:\nJenni", "# Reasoning Path:\nJenni Rivera -> music.artist.album -> 1969 - Siempre, En Vivo Desde Monterrey, Parte 1 -> music.album.genre -> Latin music\n# Answer:\n1969 - Siempre, En Vivo Desde Monterrey, Parte 1", "# Reasoning Path:\nJenni Rivera -> common.topic.webpage -> m.04q59kl -> common.webpage.resource -> m.0bjwwyj\n# Answer:\nm.04q59kl", "# Reasoning Path:\nJenni Rivera -> common.topic.webpage -> m.05nq1j8 -> common.webpage.resource -> m.0bkghgr\n# Answer:\nm.05nq1j8", "# Reasoning Path:\nJenni Rivera -> music.artist.album -> Jenni -> music.album.genre -> Pop music\n# Answer:\nJenni", "# Reasoning Path:\nJenni Rivera -> common.topic.webpage -> m.04q59kl -> common.webpage.category -> Topic Webpage\n# Answer:\nm.04q59kl"], "ground_truth": ["Iturbide"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-438", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkf4 -> education.education.institution -> Stanford University\n# Answer:\nm.02kvkf4", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.institution -> Harvard Law School\n# Answer:\nm.02kvkg9", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkf4 -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nm.02kvkf4", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.major_field_of_study -> Law\n# Answer:\nm.02kvkg9", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.0123vxrw -> education.educational_institution.students_graduates -> m.0123vxqw\n# Answer:\nm.0123vxrw", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0_gvz4k -> film.personal_film_appearance.film -> Mitt\n# Answer:\nm.0_gvz4k", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0_gvz4k -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.0_gvz4k", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0nhmmbv -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.0nhmmbv", "# Reasoning Path:\nMitt Romney -> tv.tv_actor.guest_roles -> m.0bvvym_ -> tv.tv_guest_role.episodes_appeared_in -> The Comeback of President Bush\n# Answer:\nm.0bvvym_", "# Reasoning Path:\nMitt Romney -> tv.tv_actor.guest_roles -> m.0bvw9my -> tv.tv_guest_role.episodes_appeared_in -> Mitt Romney, Ryan Sheckler, Jonas Brothers\n# Answer:\nm.0bvw9my"], "ground_truth": ["Brigham Young University"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-439", "prediction": ["# Reasoning Path:\nSouth Dakota -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nSouth Dakota -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nSouth Dakota -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nSouth Dakota -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nSouth Dakota -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nSouth Dakota -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nSouth Dakota -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nSouth Dakota -> government.governmental_jurisdiction.agencies -> Consulate General of India, Chicago -> government.government_agency.jurisdiction -> Illinois\n# Answer:\nConsulate General of India, Chicago", "# Reasoning Path:\nSouth Dakota -> government.governmental_jurisdiction.governing_officials -> m.010hbn99 -> government.government_position_held.basic_title -> Governor\n# Answer:\nm.010hbn99", "# Reasoning Path:\nSouth Dakota -> government.governmental_jurisdiction.governing_officials -> m.010hbnc6 -> government.government_position_held.basic_title -> Governor\n# Answer:\nm.010hbnc6"], "ground_truth": ["Central Time Zone", "Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nm.0hz8v06", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.Kind_of_symbol -> State flower\n# Answer:\nm.0hz8v06", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.symbol -> Petrified wood\n# Answer:\nm.04l0cxp", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz -> location.location_symbol_relationship.symbol -> Mountain tree frog\n# Answer:\nm.04st6lz", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.Kind_of_symbol -> State fossil\n# Answer:\nm.04l0cxp", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w1gvc", "# Reasoning Path:\nArizona -> location.statistical_region.religions -> m.04403h9 -> location.religion_percentage.religion -> Pentecostalism\n# Answer:\nm.04403h9", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.resource -> ATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!\n# Answer:\nm.09w1gvc", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1l1s -> common.webpage.resource -> ATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!\n# Answer:\nm.09w1l1s", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz -> location.location_symbol_relationship.Kind_of_symbol -> State Amphibian\n# Answer:\nm.04st6lz"], "ground_truth": ["Saguaro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-440", "prediction": ["# Reasoning Path:\nJason Mraz -> music.artist.origin -> Mechanicsville -> location.location.containedby -> Virginia\n# Answer:\nMechanicsville", "# Reasoning Path:\nJason Mraz -> music.artist.origin -> Mechanicsville -> location.location.geolocation -> m.0khpj5\n# Answer:\nMechanicsville", "# Reasoning Path:\nJason Mraz -> music.artist.origin -> California -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nCalifornia", "# Reasoning Path:\nJason Mraz -> music.artist.origin -> California -> location.administrative_division.country -> United States of America\n# Answer:\nCalifornia", "# Reasoning Path:\nJason Mraz -> people.person.place_of_birth -> Mechanicsville -> location.location.containedby -> Virginia\n# Answer:\nMechanicsville", "# Reasoning Path:\nJason Mraz -> music.artist.origin -> Mechanicsville -> location.location.containedby -> Hanover County\n# Answer:\nMechanicsville", "# Reasoning Path:\nJason Mraz -> music.artist.origin -> Mechanicsville -> location.location.people_born_here -> Billy Parker\n# Answer:\nMechanicsville", "# Reasoning Path:\nJason Mraz -> music.artist.origin -> California -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nCalifornia", "# Reasoning Path:\nJason Mraz -> people.person.place_of_birth -> Mechanicsville -> location.location.people_born_here -> Billy Parker\n# Answer:\nMechanicsville", "# Reasoning Path:\nJason Mraz -> people.person.place_of_birth -> Mechanicsville -> location.location.geolocation -> m.0khpj5\n# Answer:\nMechanicsville"], "ground_truth": ["Mechanicsville", "California"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-441", "prediction": ["# Reasoning Path:\nJohn Bonham -> people.deceased_person.cause_of_death -> Inhalation of vomit -> people.cause_of_death.people -> Jimi Hendrix\n# Answer:\nInhalation of vomit", "# Reasoning Path:\nJohn Bonham -> people.deceased_person.cause_of_death -> Inhalation of vomit -> people.cause_of_death.people -> Bon Scott\n# Answer:\nInhalation of vomit", "# Reasoning Path:\nJohn Bonham -> people.deceased_person.cause_of_death -> Inhalation of vomit -> people.cause_of_death.parent_cause_of_death -> Pulmonary aspiration\n# Answer:\nInhalation of vomit", "# Reasoning Path:\nJohn Bonham -> people.deceased_person.cause_of_death -> Inhalation of vomit -> people.cause_of_death.people -> Crash Holly\n# Answer:\nInhalation of vomit", "# Reasoning Path:\nJohn Bonham -> people.deceased_person.cause_of_death -> Inhalation of vomit -> common.topic.notable_for -> g.1255jz6dx\n# Answer:\nInhalation of vomit", "# Reasoning Path:\nJohn Bonham -> music.artist.track -> Dixie Fried -> music.recording.artist -> Albert Lee\n# Answer:\nDixie Fried", "# Reasoning Path:\nJohn Bonham -> common.topic.webpage -> m.03l65bd -> common.webpage.resource -> Led Zeppelin Website\n# Answer:\nm.03l65bd", "# Reasoning Path:\nJohn Bonham -> music.artist.track -> Dixie Fried -> music.recording.artist -> Jimmy Page\n# Answer:\nDixie Fried", "# Reasoning Path:\nJohn Bonham -> music.artist.track -> Dixie Fried -> music.recording.tracks -> m.0v3jj5f\n# Answer:\nDixie Fried", "# Reasoning Path:\nJohn Bonham -> music.artist.track -> 'Cause I Love You -> common.topic.notable_for -> g.125dz2vd4\n# Answer:\n'Cause I Love You"], "ground_truth": ["Inhalation of vomit"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-442", "prediction": ["# Reasoning Path:\nWalmart -> business.business_operation.industry -> Retail -> common.topic.subjects -> Furniture Choice\n# Answer:\nRetail", "# Reasoning Path:\nWalmart -> business.business_operation.industry -> Retail -> common.topic.article -> m.0191_h\n# Answer:\nRetail", "# Reasoning Path:\nWalmart -> business.business_operation.industry -> Retail -> common.topic.subjects -> Chemist Warehouse\n# Answer:\nRetail", "# Reasoning Path:\nWalmart -> organization.organization.sectors -> Retail -> common.topic.subjects -> Furniture Choice\n# Answer:\nRetail", "# Reasoning Path:\nWalmart -> organization.organization.sectors -> Retail -> common.topic.article -> m.0191_h\n# Answer:\nRetail", "# Reasoning Path:\nWalmart -> business.business_operation.industry -> Retail -> common.topic.subjects -> Cigg-e\n# Answer:\nRetail", "# Reasoning Path:\nWalmart -> business.business_operation.industry -> Retail -> common.topic.subject_of -> Andis\n# Answer:\nRetail", "# Reasoning Path:\nWalmart -> business.business_operation.industry -> Department store -> business.industry.child_industry -> Department Stores (except Discount Department Stores)\n# Answer:\nDepartment store", "# Reasoning Path:\nWalmart -> business.business_operation.industry -> Variety Stores -> business.industry.parent_industry -> All Other General Merchandise Stores\n# Answer:\nVariety Stores", "# Reasoning Path:\nWalmart -> business.business_operation.industry -> Department store -> common.topic.image -> Interior of a typical Macy's department store\n# Answer:\nDepartment store"], "ground_truth": ["Department store", "Retail", "Variety Stores"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-443", "prediction": ["# Reasoning Path:\nTexarkana, Arkansas -> location.location.containedby -> Arkansas -> location.location.containedby -> United States of America\n# Answer:\nArkansas", "# Reasoning Path:\nTexarkana, Arkansas -> location.hud_county_place.county -> Miller County -> location.location.containedby -> Arkansas\n# Answer:\nMiller County", "# Reasoning Path:\nTexarkana, Arkansas -> location.location.containedby -> Arkansas -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nArkansas", "# Reasoning Path:\nTexarkana, Arkansas -> location.location.containedby -> Miller County -> location.location.containedby -> Arkansas\n# Answer:\nMiller County", "# Reasoning Path:\nTexarkana, Arkansas -> location.location.containedby -> Arkansas -> location.administrative_division.country -> United States of America\n# Answer:\nArkansas", "# Reasoning Path:\nTexarkana, Arkansas -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Arkansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nTexarkana, Arkansas -> location.hud_county_place.county -> Miller County -> location.location.containedby -> Texarkana, TX-Texarkana, AR Metropolitan Statistical Area\n# Answer:\nMiller County", "# Reasoning Path:\nTexarkana, Arkansas -> location.location.people_born_here -> BJ Davis -> common.topic.notable_types -> Award Winner\n# Answer:\nBJ Davis", "# Reasoning Path:\nTexarkana, Arkansas -> location.location.containedby -> Miller County -> location.location.containedby -> Texarkana, TX-Texarkana, AR Metropolitan Statistical Area\n# Answer:\nMiller County", "# Reasoning Path:\nTexarkana, Arkansas -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Arkansas\n# Answer:\nUnited States of America"], "ground_truth": ["Miller County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-446", "prediction": ["# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Classical music -> music.genre.subgenre -> Chamber music\n# Answer:\nClassical music", "# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Classical music -> music.genre.subgenre -> Baroque music\n# Answer:\nClassical music", "# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Classical music -> broadcast.genre.content -> 1.FM Otto's  Baroque music\n# Answer:\nClassical music", "# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Classical music -> music.genre.subgenre -> Pop music\n# Answer:\nClassical music", "# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Classical music -> book.book_subject.works -> M\u00e9moires\n# Answer:\nClassical music", "# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Classical music -> broadcast.genre.content -> 1.FM Otto's Classical channel\n# Answer:\nClassical music", "# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Opera -> broadcast.genre.content -> 1.FM Otto's Opera House\n# Answer:\nOpera", "# Reasoning Path:\nLudwig van Beethoven -> book.book_subject.works -> Beethoven -> common.image.appears_in_topic_gallery -> Joseph Karl Stieler\n# Answer:\nBeethoven", "# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Classical music -> broadcast.genre.content -> 1.FM Otto's Opera House\n# Answer:\nClassical music", "# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Classical music -> book.book_subject.works -> The Classical Style: Haydn, Mozart, Beethoven\n# Answer:\nClassical music"], "ground_truth": ["Opera", "Classical music"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-447", "prediction": ["# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> The Long Winter -> book.written_work.next_in_series -> Little Town on the Prairie\n# Answer:\nThe Long Winter", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> The Long Winter -> book.written_work.previous_in_series -> By the Shores of Silver Lake\n# Answer:\nThe Long Winter", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> Little House -> common.topic.notable_types -> Book\n# Answer:\nLittle House", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> Hard Times on the Prairie -> common.topic.notable_for -> g.1255l13vx\n# Answer:\nHard Times on the Prairie", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> Little House -> common.topic.notable_for -> g.12552mbzr\n# Answer:\nLittle House", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> Hard Times on the Prairie -> book.book.editions -> Hard Times on the Prairie (Little House Chapter Books)\n# Answer:\nHard Times on the Prairie", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> Hard Times on the Prairie -> book.written_work.subjects -> 19th century\n# Answer:\nHard Times on the Prairie", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> Little House -> book.book.editions -> Little House (9 Books, Boxed Set)\n# Answer:\nLittle House", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> Hard Times on the Prairie -> book.written_work.subjects -> Kansas\n# Answer:\nHard Times on the Prairie", "# Reasoning Path:\nLaura Ingalls Wilder -> film.film_story_contributor.film_story_credits -> Bless All the Dear Children -> film.film.film_set_decoration_by -> Donald E. Webb\n# Answer:\nBless All the Dear Children"], "ground_truth": ["The first four years.", "A FAMILY COLLECTION", "These Happy Golden Years (Little House (Original Series Paperback))", "The First Four Years (Little House Books)", "Little House the Laura Years Boxed Set", "Little Town on the Prairie (Little House)", "Orillas del r\u00edo Plum", "Going West", "Long Winter (Little House (Original Series Paperback))", "Going West (My First Little House Books)", "Little House On The Prairie CD (Little House the Laura Years)", "Sugar Snow (My First Little House Books (Sagebrush))", "My Little House Chapter Book Collection", "Little House Friends (Little House Chapter Books/the Laura Years, 9)", "By the Shores of Silver Lake (Little House)", "Little House in the Ozarks: The Rediscovered Writings", "Farmer Boy Days (Little House Chapter Book)", "Farmer Boy Days (Little House Chapter Books)", "The Long Winter (Little House (Original Series Paperback))", "Little House On The Prairie (Little House the Laura Years)", "Little House Friends (Little House Chapter Book)", "West from Home", "A little house sampler", "Sugar Snow (My First Little House Books)", "On the Banks of Plum Creek (One Cassette)", "Going to Town", "A Little House Collection", "A Little house traveler", "Little House in the Big Woods (Isis Large Print for Children Windrush)", "Christmas in the Big Woods (Little House)", "Christmas Stories (Little House Chapter Books/the Laura Years, 10)", "The Long Winter (Little House Books)", "Prairie Day", "Laura Helps Pa", "Winter Days in the Big Woods", "My Little House Book of Animals", "Deer in the Wood", "My Little House Songbook (My First Little House Books)", "1998 Laura Ingalls Wilder Country Engagement Book", "My Little House Songbook (My First Little House Books Series)", "Dance at Grandpa's (My First Little House)", "A Little Prairie House", "On the Banks of Plum Creek (Little House the Laura Years)", "Laura's Early Years Collection", "Little House Parties (Little House Chapter Books/the Laura Years, 14)", "Laura's Garden", "A Little Prairie House (My First Little House Books)", "Farmer Boy Days", "Little House in the Big Woods (Little House)", "The First Four Years (Little House the Laura Years)", "Little House on the Prairie (Little Brown Notebook Series)", "Pioneer Sisters (Little House Chapter Books/the Laura Years, 2)", "Saving graces", "A Little House Sampler", "First Four Years (Little House (Original Series Paperback))", "Going to Town (My First Little House Books)", "Little House in the Big Woods (Little House (Original Series Paperback))", "Little House Farm Days (Little House Chapter Books/the Laura Years, 7)", "The Deer in the Wood (My First Little House Books (Sagebrush))", "These Happy Golden Years (Little House)", "By the Shores of Silver Lake CD (Little House)", "These Happy Golden Years (Little House (HarperTrophy))", "Little Town on the Prairie", "Bedtime for Laura", "A Little House Traveler", "Little House in the Big Woods", "Little House on the Prairie NW 247", "These Happy Golden Years CD", "Caroline and Her Sister", "Going to Town (My First Little House)", "The First Four Years (Little House)", "My Little House Birthday Book", "On the way home", "On the Way Home", "The first four years", "Laura & Mr. Edwards (Little House Chapter Book)", "Little house on the prairie.", "Little House on the Prairie Tie-in Edition (Little House)", "Little House Friends", "School Days", "Animal Adventures", "My Little House Diary", "Little House on the Prairie (Little House (HarperTrophy))", "Little town on the prairie", "These Happy Golden Years", "Little house on the prairie", "The Deer in the Wood (Little House)", "These happy golden years", "Words from a fearless heart", "Winter on the Farm (My First Little House Books (Sagebrush))", "By the Shores of Silver Lake (Little House Books)", "Summertime in the Big Woods", "Little House Sisters", "The Deer in the Wood (My First Little House Books)", "My Little House 123", "Little House in the Big Woods (Little House Books)", "Little House On The Prairie Low Price CD", "Prairie Day (My First Little House)", "The Little House Baby Book", "My Book of Little House Paper Dolls", "The First Four Years (Little House (Original Series Library))", "Hard Times on the Prairie (Little House Chapter Books)", "Little House On The Prairie (Little House (Original Series Paperback))", "Dear Laura", "Long Winter (Little House (HarperTrophy))", "Little House Friends (Little House Chapter Books)", "Little House Farm Days (Little House Chapter Books)", "Little House Parties", "Laura Ingalls Wilder's prairie wisdom", "Christmas in the Big Woods (My First Little House Books)", "The long winter", "Farmer Boy Days (Little House Chapter Books/the Laura Years, 6)", "Going to town", "My Little House Songbook (My First Little House Books, No 1)", "The Complete Little House on the Prairie", "Little House Parties (Little House Chapter Books)", "My Book of Little House Christmas Paper Dolls: Christmas on the Prairie", "Sugar Snow (My First Little House)", "Little House Parties (Little House Chapter Book)", "Writings to young women from Laura Ingalls Wilder", "Going West (My First Little House Books (Sagebrush))", "The First Four Years CD (Little House the Laura Years)", "My Little House Book of Memories", "Dear Laura: Letters from Children to Laura Ingalls Wilder", "Little House In The Big Woods CD (Little House the Laura Years)", "The Adventures of Laura and Jack (A Little House Chapter Book) (A Little house chapter book)", "The deer in the wood (My first Little house books)", "Christmas Stories (Little House Chapter Books)", "Going West (My First Little Houe Books)", "A little house treasury", "Dance at Grandpa's", "On the banks of Plum Creek", "Little House on the Prairie", "My Little House Book of Family", "Little House on the Prairie (Little House)", "Farmer Boy (Little House Books)", "The adventures of Laura and Jack", "Farmer boy", "Little Town On The Prairie (Little House (Original Series Paperback))", "Pioneer Sisters", "The Little House Collection", "Christmas in the Big Woods", "These Happy Golden Years (Little House on the Prairie)", "Little House on the Prairie (Classic Mammoth)", "Animal Adventures (Little House Chapter Books)", "A Day on the Prairie", "Long Winter", "By the shores of Silver Lake", "Santa Comes to Little House", "Little house in the big woods", "Little House", "Hello, Laura!", "A Farmer Boy", "Farmer Boy (Little House (Original Series Paperback))", "Farmer Boy (Little House)", "On the Banks of Plum Creek", "Little House in the Big Woods 75th Anniversary Edition (Little House)", "Laura & Nellie (Little House Chapter Book)", "Winter on the Farm (My First Little House Books)", "My Little House Songbook", "The Deer in the Wood", "By the Shores of Silver Lake", "These Happy Golden Years (Little House Books)", "A Little Prairie House (Little House)", "Country Fair", "Little House in the Big Woods (Classic Mammoth)", "Little Town on the Prairie (Little House Books)", "Hard Times on the Prairie", "Little House on the Prairie Boxed Set ((9 Books) Little House On the Prairie; Farmer Boy; On the Banks of Plum Creek; the Long Winter; These Happy Golden Years; the First Four Years; By the Shores of Silver Lake; Little House In the Big Woods; Little Town On the Prairie)", "Laura's Little House", "Little House on the Prairie Book and Charm (Charming Classics)", "Largo Invierno", "The Long Winter", "By the Shores of Silver Lake (Little House (Original Series Paperback))", "Little House In The Big Woods Unabr CD Low Price (Little House the Laura Years)", "First Four Years (Little House (HarperTrophy))", "Winter on the Farm (My First Little House)", "Little House On The Prairie", "Christmas Stories (Little House Chapter Book)", "Animal Adventures (Little House Chapter Books/the Laura Years, 3)", "Laura & Mr. Edwards", "School Days (Laura (Econo-Clad))", "These Happy Golden Years (Laura Years)", "Hard Times on the Prairie (Little House Chapter Books/the Laura Years, 8)", "Pioneer Sisters (Little House Chapter Book)", "Santa comes to little house", "A Little house reader", "Going West (My First Little House)", "g.122chq7m", "Christmas Stories", "The Little House Collection Box Set (Full Color) (Little House)", "West from home", "Farmer Boy", "County Fair (My First Little House)", "A Farmer Boy Birthday", "Little House (9 Books, Boxed Set)", "g.11b60fcs_w", "The First Four Years", "Little House Farm Days", "A Farmer Boy Birthday (My First Little House Books)", "Little House on the Prairie (Little House Books)", "Laura's Christmas", "County Fair (My First Little House Books (Sagebrush))", "On the Banks of Plum Creek (Little House)", "School Days (Little House Chapter Book)", "Sugar Snow", "On the Banks of Plum Creek (Little House (Original Series Paperback))", "A Little House Reader", "School Days (Little House Chapter Books/the Laura Years, 4)", "A Little House Birthday", "Little House in the Big Woods Book and Charm (Charming Classics)", "The Long Winter (Little House)", "Little House in the Big Woods.", "Winter on the Farm", "Pioneer Sisters (Laura (Econo-Clad))", "Little Town on the Prairie CD", "On the Banks of Plum Creek CD (Little House the Laura Years)", "Laura's Pa", "Laura & Nellie", "On the Banks of Plum Creek (Classic Mammoth)", "The Little House on the Prairie", "School days (Little house chapter book)"], "ans_acc": 0.04954954954954955, "ans_hit": 1, "ans_f1": 0.052478134110787174, "ans_precission": 0.9, "ans_recall": 0.02702702702702703, "path_f1": 0.055848588271796465, "path_precision": 0.9, "path_recall": 0.02881844380403458, "path_ans_f1": 0.09392789373814042, "path_ans_precision": 0.9, "path_ans_recall": 0.04954954954954955}
{"id": "WebQTest-448", "prediction": ["# Reasoning Path:\nItaly -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Greece\n# Answer:\nConstitutional republic", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\ng.11b71mdn0q", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Brazil\n# Answer:\nConstitutional republic", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Finland\n# Answer:\nParliamentary republic", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\ng.12tb6gbb1", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Japan\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37cxr\n# Answer:\ng.1hhc37cxr", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Kingdom of Italy\n# Answer:\nUnitary state"], "ground_truth": ["Constitutional republic", "Parliamentary republic", "Unitary state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-449", "prediction": ["# Reasoning Path:\nJohn Irving -> book.author.works_written -> A Prayer for Owen Meany -> book.written_work.previous_in_series -> The Cider House Rules\n# Answer:\nA Prayer for Owen Meany", "# Reasoning Path:\nJohn Irving -> book.author.works_written -> A Prayer for Owen Meany -> book.written_work.next_in_series -> A Son of the Circus\n# Answer:\nA Prayer for Owen Meany", "# Reasoning Path:\nJohn Irving -> book.author.book_editions_published -> A Prayer for Owen Meany -> book.written_work.previous_in_series -> The Cider House Rules\n# Answer:\nA Prayer for Owen Meany", "# Reasoning Path:\nJohn Irving -> book.author.works_written -> A Prayer for Owen Meany -> book.book.characters -> Owen Meany\n# Answer:\nA Prayer for Owen Meany", "# Reasoning Path:\nJohn Irving -> book.author.works_written -> The imaginary girlfriend -> common.topic.notable_for -> g.125925tg1\n# Answer:\nThe imaginary girlfriend", "# Reasoning Path:\nJohn Irving -> book.author.book_editions_published -> A Prayer for Owen Meany -> book.written_work.next_in_series -> A Son of the Circus\n# Answer:\nA Prayer for Owen Meany", "# Reasoning Path:\nJohn Irving -> book.author.works_written -> A Widow for One Year -> book.book_edition.place_of_publication -> New York City\n# Answer:\nA Widow for One Year", "# Reasoning Path:\nJohn Irving -> book.author.works_written -> The imaginary girlfriend -> common.topic.notable_types -> Book\n# Answer:\nThe imaginary girlfriend", "# Reasoning Path:\nJohn Irving -> book.author.book_editions_published -> A Prayer for Owen Meany -> book.book.characters -> Owen Meany\n# Answer:\nA Prayer for Owen Meany", "# Reasoning Path:\nJohn Irving -> book.author.works_written -> A Prayer for Owen Meany -> book.book.characters -> Dan Needham\n# Answer:\nA Prayer for Owen Meany"], "ground_truth": ["Last Night in Twisted River", "The Hotel New Hampshire", "The Cider House Rules", "3 by Irving", "Water Method Man", "A Sound Like Someone Trying Not to Make a Sound", "In One Person", "Until I Find You", "Trying to Save Piggy Sneed", "A Son of the Circus", "A Widow for One Year", "The World According to Garp", "The imaginary girlfriend", "The Fourth Hand", "The 158-Pound Marriage", "Setting Free the Bears", "A Prayer for Owen Meany"], "ans_acc": 0.29411764705882354, "ans_hit": 1, "ans_f1": 0.3, "ans_precission": 1.0, "ans_recall": 0.17647058823529413, "path_f1": 0.2727272727272727, "path_precision": 0.6, "path_recall": 0.17647058823529413, "path_ans_f1": 0.45454545454545453, "path_ans_precision": 1.0, "path_ans_recall": 0.29411764705882354}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.location.contains -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> base.aareas.schema.administrative_area.administrative_parent -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\ng.11bv1mttmr", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.country.administrative_divisions -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> common.topic.notable_types -> Country\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\ng.11bv383dbd", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.063y0bl -> base.popstra.infidelity.victim -> Beyonc\u00e9 Knowles\n# Answer:\nm.063y0bl", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> m.010_ydxv -> music.track_contribution.track -> If I Never See Your Face Again\n# Answer:\nm.010_ydxv"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-450", "prediction": ["# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.official_symbols -> m.04st848 -> location.location_symbol_relationship.symbol -> Nene\n# Answer:\nm.04st848", "# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.official_symbols -> m.04stk8g -> location.location_symbol_relationship.symbol -> Reef triggerfish\n# Answer:\nm.04stk8g", "# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.official_symbols -> m.04stpzt -> location.location_symbol_relationship.symbol -> Candlenut\n# Answer:\nm.04stpzt", "# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.official_symbols -> m.04st848 -> location.location_symbol_relationship.Kind_of_symbol -> State bird\n# Answer:\nm.04st848", "# Reasoning Path:\nHawaii -> book.book_subject.works -> Fires of Eden -> book.book.editions -> Fuegos del Eden, Los\n# Answer:\nFires of Eden", "# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.official_symbols -> m.04stk8g -> location.location_symbol_relationship.Kind_of_symbol -> State Fish\n# Answer:\nm.04stk8g", "# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.official_symbols -> m.04stpzt -> location.location_symbol_relationship.Kind_of_symbol -> State tree\n# Answer:\nm.04stpzt", "# Reasoning Path:\nHawaii -> book.book_subject.works -> Fires of Eden -> book.book.genre -> Fiction\n# Answer:\nFires of Eden", "# Reasoning Path:\nHawaii -> base.popstra.location.vacationers -> m.063hnlk -> base.popstra.vacation_choice.vacationer -> Arnold Schwarzenegger\n# Answer:\nm.063hnlk", "# Reasoning Path:\nHawaii -> book.book_subject.works -> Fires of Eden -> book.book.editions -> Fires of Eden.\n# Answer:\nFires of Eden"], "ground_truth": ["Hawaiian hibiscus"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-451", "prediction": ["# Reasoning Path:\nAlice Stokes Paul -> base.activism.activist.area_of_activism -> Women's suffrage -> common.topic.image -> Feminist Suffrage Parade in New York City, 1912\n# Answer:\nWomen's suffrage", "# Reasoning Path:\nAlice Stokes Paul -> base.activism.activist.area_of_activism -> Women's suffrage -> tv.tv_subject.tv_episodes -> Sufferin' Until Sufferage\n# Answer:\nWomen's suffrage", "# Reasoning Path:\nAlice Stokes Paul -> people.person.profession -> Author -> common.topic.notable_types -> Profession\n# Answer:\nAuthor", "# Reasoning Path:\nAlice Stokes Paul -> common.topic.notable_types -> Organization founder -> freebase.type_profile.kind -> Role\n# Answer:\nOrganization founder", "# Reasoning Path:\nAlice Stokes Paul -> base.activism.activist.area_of_activism -> Women's suffrage -> common.topic.image -> Rose-Sanderson-Votes-for-Women\n# Answer:\nWomen's suffrage", "# Reasoning Path:\nAlice Stokes Paul -> base.activism.activist.area_of_activism -> Women's suffrage -> common.topic.image -> The argument over women's rights in Victoria was lampooned in this Melbourne Punch cartoon of 1887\n# Answer:\nWomen's suffrage", "# Reasoning Path:\nAlice Stokes Paul -> base.activism.activist.area_of_activism -> Women's suffrage -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Taking Liberties: The struggle for Britain's freedom and rights\n# Answer:\nWomen's suffrage", "# Reasoning Path:\nAlice Stokes Paul -> common.topic.notable_types -> Organization founder -> common.topic.article -> m.02_77m6\n# Answer:\nOrganization founder", "# Reasoning Path:\nAlice Stokes Paul -> people.person.profession -> Author -> projects.project_role.projects -> m.0w4hkq8\n# Answer:\nAuthor", "# Reasoning Path:\nAlice Stokes Paul -> common.topic.notable_types -> Organization founder -> type.type.expected_by -> Founders\n# Answer:\nOrganization founder"], "ground_truth": ["Organization founder"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-453", "prediction": ["# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.main_country -> Jamaica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Jamaican Creole English Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Jamaican Creole English Language -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60wmkvh\n# Answer:\ng.11b60wmkvh", "# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Bribri Language -> base.rosetta.languoid.parent -> Talamanca Group\n# Answer:\nBribri Language", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.11b71w3_6h\n# Answer:\ng.11b71w3_6h", "# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Bribri Language -> common.topic.notable_for -> g.125cn8jpy\n# Answer:\nBribri Language", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_5xgy\n# Answer:\ng.1245_5xgy", "# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Spanish Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nSpanish Language"], "ground_truth": ["Spanish Language", "Jamaican Creole English Language", "Bribri Language", "Limonese Creole"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7241379310344827, "ans_precission": 0.7, "ans_recall": 0.75, "path_f1": 0.7241379310344827, "path_precision": 0.7, "path_recall": 0.75, "path_ans_f1": 0.7241379310344827, "path_ans_precision": 0.7, "path_ans_recall": 0.75}
{"id": "WebQTest-454", "prediction": ["# Reasoning Path:\nHank Marvin -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.geolocation -> m.0239t8w\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nHank Marvin -> people.person.place_of_birth -> Newcastle upon Tyne -> base.ontologies.ontology_instance.equivalent_instances -> m.09kmh29\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nHank Marvin -> music.lyricist.lyrics_written -> Throw Down a Line -> music.composition.recordings -> Throw Down A Line\n# Answer:\nThrow Down a Line", "# Reasoning Path:\nHank Marvin -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.events -> 2003 AV Festival\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nHank Marvin -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.events -> 2006 AV Festival\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nHank Marvin -> music.lyricist.lyrics_written -> Throw Down a Line -> music.single.versions -> Throw Down A Line\n# Answer:\nThrow Down a Line", "# Reasoning Path:\nHank Marvin -> music.lyricist.lyrics_written -> Throw Down a Line -> common.topic.notable_for -> g.12q4p8xlj\n# Answer:\nThrow Down a Line", "# Reasoning Path:\nHank Marvin -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.events -> 2008 AV Festival\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nHank Marvin -> music.artist.contribution -> m.01tnlpq -> music.recording_contribution.album -> Simply Shadows\n# Answer:\nm.01tnlpq", "# Reasoning Path:\nHank Marvin -> music.artist.contribution -> m.01tnlpx -> music.recording_contribution.album -> Moonlight Shadows\n# Answer:\nm.01tnlpx"], "ground_truth": ["Newcastle upon Tyne"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-455", "prediction": ["# Reasoning Path:\nChipper Jones -> baseball.baseball_player.batting_stats -> m.06r0l_7 -> baseball.batting_statistics.season -> 2004 Major League Baseball season\n# Answer:\nm.06r0l_7", "# Reasoning Path:\nChipper Jones -> baseball.baseball_player.batting_stats -> m.06r0lj8 -> baseball.batting_statistics.season -> 1993 Major League Baseball Season\n# Answer:\nm.06r0lj8", "# Reasoning Path:\nChipper Jones -> baseball.baseball_player.batting_stats -> m.06r0lky -> baseball.batting_statistics.season -> 1995 Major League Baseball Season\n# Answer:\nm.06r0lky", "# Reasoning Path:\nChipper Jones -> baseball.baseball_player.bats -> Ambidexterity -> common.topic.image -> PhilMickelsonTPC18thTee\n# Answer:\nAmbidexterity", "# Reasoning Path:\nChipper Jones -> baseball.baseball_player.batting_stats -> m.06r0lj8 -> baseball.batting_statistics.team -> Atlanta Braves\n# Answer:\nm.06r0lj8", "# Reasoning Path:\nChipper Jones -> award.award_nominee.award_nominations -> m.0sgpm44 -> award.award_nomination.award -> Kids\u2019 Choice Award for Favorite MVP\n# Answer:\nm.0sgpm44", "# Reasoning Path:\nChipper Jones -> baseball.baseball_player.batting_stats -> m.06r0l_7 -> baseball.batting_statistics.team -> Atlanta Braves\n# Answer:\nm.06r0l_7", "# Reasoning Path:\nChipper Jones -> baseball.baseball_player.batting_stats -> m.06r0lky -> baseball.batting_statistics.team -> Atlanta Braves\n# Answer:\nm.06r0lky", "# Reasoning Path:\nChipper Jones -> baseball.baseball_player.bats -> Ambidexterity -> common.topic.article -> m.03bwzdk\n# Answer:\nAmbidexterity", "# Reasoning Path:\nChipper Jones -> award.award_nominee.award_nominations -> m.0sgpm44 -> award.award_nomination.ceremony -> 2000 Kids' Choice Awards\n# Answer:\nm.0sgpm44"], "ground_truth": ["1990 Major League Baseball Draft"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-456", "prediction": ["# Reasoning Path:\nStar Trek -> fictional_universe.fictional_universe.works_set_here -> Star Trek VI: The Undiscovered Country -> film.film.prequel -> Star Trek V: The Final Frontier\n# Answer:\nStar Trek VI: The Undiscovered Country", "# Reasoning Path:\nWinona Ryder -> common.image.size -> m.04sl_lr\n# Answer:\nm.04sl_lr", "# Reasoning Path:\nStar Trek -> fictional_universe.fictional_universe.works_set_here -> Star Trek VI: The Undiscovered Country -> film.film.written_by -> Gene Roddenberry\n# Answer:\nStar Trek VI: The Undiscovered Country", "# Reasoning Path:\nStar Trek -> fictional_universe.fictional_universe.works_set_here -> Star Trek V: The Final Frontier -> media_common.netflix_title.netflix_genres -> Action Sci-Fi & Fantasy\n# Answer:\nStar Trek V: The Final Frontier", "# Reasoning Path:\nStar Trek -> fictional_universe.fictional_universe.works_set_here -> Star Trek V: The Final Frontier -> film.film.production_companies -> Paramount Pictures\n# Answer:\nStar Trek V: The Final Frontier", "# Reasoning Path:\nStar Trek -> fictional_universe.fictional_universe.works_set_here -> Star Trek VI: The Undiscovered Country -> media_common.netflix_title.netflix_genres -> Action Sci-Fi & Fantasy\n# Answer:\nStar Trek VI: The Undiscovered Country", "# Reasoning Path:\nStar Trek -> fictional_universe.fictional_universe.works_set_here -> Star Trek V: The Final Frontier -> film.film.story_by -> Gene Roddenberry\n# Answer:\nStar Trek V: The Final Frontier", "# Reasoning Path:\nStar Trek -> fictional_universe.fictional_universe.works_set_here -> Star Trek VI: The Undiscovered Country -> film.film.written_by -> Leonard Nimoy\n# Answer:\nStar Trek VI: The Undiscovered Country", "# Reasoning Path:\nStar Trek -> fictional_universe.fictional_universe.works_set_here -> Star Trek V: The Final Frontier -> media_common.netflix_title.netflix_genres -> Sci-Fi & Fantasy\n# Answer:\nStar Trek V: The Final Frontier", "# Reasoning Path:\nWinona Ryder -> film.actor.film -> m.010fw41j -> film.performance.film -> Turks & Caicos\n# Answer:\nm.010fw41j"], "ground_truth": ["Amanda Grayson"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-459", "prediction": ["# Reasoning Path:\nJovan Belcher -> people.deceased_person.place_of_death -> Kansas City -> location.location.containedby -> Missouri\n# Answer:\nKansas City", "# Reasoning Path:\nJovan Belcher -> people.deceased_person.place_of_death -> Kansas City -> location.hud_county_place.county -> Platte County\n# Answer:\nKansas City", "# Reasoning Path:\nJovan Belcher -> people.deceased_person.place_of_death -> Kansas City -> location.location.containedby -> United States of America\n# Answer:\nKansas City", "# Reasoning Path:\nJovan Belcher -> people.deceased_person.place_of_death -> Kansas City -> travel.travel_destination.tourist_attractions -> Liberty Memorial\n# Answer:\nKansas City", "# Reasoning Path:\nJovan Belcher -> people.deceased_person.place_of_death -> Kansas City -> travel.travel_destination.tourist_attractions -> 18th and Vine-Downtown East\n# Answer:\nKansas City", "# Reasoning Path:\nJovan Belcher -> people.person.gender -> Male -> medicine.risk_factor.diseases -> heart attack\n# Answer:\nMale", "# Reasoning Path:\nJovan Belcher -> people.deceased_person.place_of_death -> Kansas City -> travel.travel_destination.tourist_attractions -> Boulevard Brewing Company\n# Answer:\nKansas City", "# Reasoning Path:\nJovan Belcher -> american_football.football_player.games -> m.07sg_19 -> american_football.player_game_statistics.team -> Kansas City Chiefs\n# Answer:\nm.07sg_19", "# Reasoning Path:\nJovan Belcher -> american_football.football_player.games -> m.07tc4tg -> american_football.player_game_statistics.team -> Kansas City Chiefs\n# Answer:\nm.07tc4tg", "# Reasoning Path:\nJovan Belcher -> american_football.football_player.games -> m.07sg_19 -> american_football.player_game_statistics.season -> 2009 NFL season\n# Answer:\nm.07sg_19"], "ground_truth": ["Kansas City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Illinois\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Indiana\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\nm.02bk8zt", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> 222px-Harrison_inauguration1841.jpg\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Iowa\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Virginia\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Indiana\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.03qtjkt\n# Answer:\nm.03qtjkt"], "ground_truth": ["1841-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-461", "prediction": ["# Reasoning Path:\nClay Matthews III -> people.person.education -> m.0hprg7d -> education.education.institution -> University of Southern California\n# Answer:\nm.0hprg7d", "# Reasoning Path:\nClay Matthews III -> people.person.education -> m.0n0g06t -> education.education.institution -> Agoura High School\n# Answer:\nm.0n0g06t", "# Reasoning Path:\nClay Matthews III -> people.person.place_of_birth -> Northridge -> common.topic.image -> Northridge Shopping District\n# Answer:\nNorthridge", "# Reasoning Path:\nClay Matthews III -> american_football.football_player.forty_yard_dash_time -> m.0hprhl_\n# Answer:\nm.0hprhl_", "# Reasoning Path:\nClay Matthews III -> people.person.place_of_birth -> Northridge -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNorthridge", "# Reasoning Path:\nClay Matthews III -> american_football.football_player.forty_yard_dash_time -> m.0wkcwx3\n# Answer:\nm.0wkcwx3", "# Reasoning Path:\nClay Matthews III -> people.person.place_of_birth -> Northridge -> location.location.people_born_here -> Adam Fortin\n# Answer:\nNorthridge", "# Reasoning Path:\nClay Matthews III -> people.person.place_of_birth -> Northridge -> location.location.people_born_here -> Alphonse Philippe Mouzon\n# Answer:\nNorthridge", "# Reasoning Path:\nClay Matthews III -> people.person.place_of_birth -> Northridge -> location.location.people_born_here -> Alyssa Diaz\n# Answer:\nNorthridge"], "ground_truth": ["University of Southern California", "Agoura High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-462", "prediction": ["# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.statistical_region.population -> g.11b66fk05m\n# Answer:\ng.11b66fk05m", "# Reasoning Path:\nHouston -> location.statistical_region.population -> g.11b7tcbqts\n# Answer:\ng.11b7tcbqts", "# Reasoning Path:\nHouston -> location.place_with_neighborhoods.neighborhoods -> Aldine -> location.location.time_zones -> Central Time Zone\n# Answer:\nAldine", "# Reasoning Path:\nHouston -> location.place_with_neighborhoods.neighborhoods -> Addicks -> location.location.containedby -> Greater Houston\n# Answer:\nAddicks", "# Reasoning Path:\nHouston -> location.place_with_neighborhoods.neighborhoods -> Addicks -> location.location.people_born_here -> Jesse Lee Turner\n# Answer:\nAddicks", "# Reasoning Path:\nHouston -> location.place_with_neighborhoods.neighborhoods -> Afton Oaks -> common.topic.image -> AftonOaksHouston\n# Answer:\nAfton Oaks"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-463", "prediction": ["# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.0k3r3v -> film.performance.actor -> Carrie Fisher\n# Answer:\nm.0k3r3v", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.01xpnt9 -> film.performance.actor -> Carrie Fisher\n# Answer:\nm.01xpnt9", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.0k3r24 -> film.performance.actor -> Carrie Fisher\n# Answer:\nm.0k3r24", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.0k3r3v -> film.performance.film -> Return of the Jedi\n# Answer:\nm.0k3r3v", "# Reasoning Path:\nLeia Organa -> tv.tv_character.appeared_in_tv_program -> m.0gx626p -> tv.regular_tv_appearance.actor -> Carrie Fisher\n# Answer:\nm.0gx626p", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.01xpnt9 -> film.performance.film -> Star Wars Episode V: The Empire Strikes Back\n# Answer:\nm.01xpnt9", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.0k3r24 -> film.performance.film -> Star Wars\n# Answer:\nm.0k3r24", "# Reasoning Path:\nLeia Organa -> tv.tv_character.appeared_in_tv_program -> m.0h1255z -> tv.regular_tv_appearance.actor -> Amy Smart\n# Answer:\nm.0h1255z", "# Reasoning Path:\nLeia Organa -> tv.tv_character.appeared_in_tv_program -> m.0gx626p -> tv.regular_tv_appearance.series -> The Making of Star Wars\n# Answer:\nm.0gx626p", "# Reasoning Path:\nLeia Organa -> book.book_character.appears_in_book -> Fury -> book.written_work.previous_in_series -> Inferno\n# Answer:\nFury"], "ground_truth": ["Carrie Fisher"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-464", "prediction": ["# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Balikun horse -> common.topic.notable_types -> Organism Classification\n# Answer:\nBalikun horse", "# Reasoning Path:\nChina -> location.location.partially_contains -> Amur River -> location.location.partially_containedby -> Russia\n# Answer:\nAmur River", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Black Telescope -> biology.animal_breed.breed_of -> Goldfish\n# Answer:\nBlack Telescope", "# Reasoning Path:\nChina -> location.location.partially_contains -> Argun River -> location.location.partially_containedby -> Russia\n# Answer:\nArgun River", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Bone-mouth -> common.topic.notable_types -> Organism Classification\n# Answer:\nBone-mouth", "# Reasoning Path:\nChina -> location.location.partially_contains -> Arun River, China-Nepal -> location.location.partially_containedby -> Nepal\n# Answer:\nArun River, China-Nepal", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Black Telescope -> biology.organism_classification.rank -> Breed\n# Answer:\nBlack Telescope", "# Reasoning Path:\nChina -> location.location.partially_contains -> Amur River -> common.topic.notable_types -> River\n# Answer:\nAmur River", "# Reasoning Path:\nChina -> location.location.partially_contains -> Amur River -> geography.river.origin -> Argun River\n# Answer:\nAmur River", "# Reasoning Path:\nChina -> location.location.partially_contains -> Argun River -> common.topic.notable_types -> River\n# Answer:\nArgun River"], "ground_truth": ["Communist state", "Socialist state", "Single-party state"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-465", "prediction": ["# Reasoning Path:\nNew York City -> book.book_subject.works -> Graffiti Lives: Beyond the Tag in New York\u2019s Urban Underground -> book.written_work.subjects -> Graffiti\n# Answer:\nGraffiti Lives: Beyond the Tag in New York\u2019s Urban Underground", "# Reasoning Path:\nNew York City -> book.book_subject.works -> Graffiti Lives: Beyond the Tag in New York\u2019s Urban Underground -> book.written_work.subjects -> Urban art\n# Answer:\nGraffiti Lives: Beyond the Tag in New York\u2019s Urban Underground", "# Reasoning Path:\nNew York City -> book.book_subject.works -> Graffiti Lives: Beyond the Tag in New York\u2019s Urban Underground -> book.book.genre -> Non-fiction\n# Answer:\nGraffiti Lives: Beyond the Tag in New York\u2019s Urban Underground", "# Reasoning Path:\nNew York City -> book.book_subject.works -> A Christmas Caroline -> book.written_work.subjects -> Christmas\n# Answer:\nA Christmas Caroline", "# Reasoning Path:\nNew York City -> visual_art.art_subject.artwork_on_the_subject -> End of 14th Street Crosstown Line -> visual_art.artwork.art_subject -> Labor unrest\n# Answer:\nEnd of 14th Street Crosstown Line", "# Reasoning Path:\nNew York City -> location.statistical_region.population -> g.11b674hm47\n# Answer:\ng.11b674hm47", "# Reasoning Path:\nNew York City -> book.book_subject.works -> Graffiti Lives: Beyond the Tag in New York\u2019s Urban Underground -> common.topic.image -> graffiti.jpg\n# Answer:\nGraffiti Lives: Beyond the Tag in New York\u2019s Urban Underground", "# Reasoning Path:\nNew York City -> location.statistical_region.population -> g.11b7tlk4sq\n# Answer:\ng.11b7tlk4sq", "# Reasoning Path:\nNew York City -> book.book_subject.works -> Art Deco New York -> book.written_work.subjects -> Art\n# Answer:\nArt Deco New York", "# Reasoning Path:\nNew York City -> book.book_subject.works -> A Christmas Caroline -> book.book.genre -> Fiction\n# Answer:\nA Christmas Caroline"], "ground_truth": ["Newark Liberty International Airport", "LaGuardia Airport", "John F. Kennedy International Airport"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-466", "prediction": ["# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\ng.12cp_jcjy", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\ng.1hhc390hy", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\ng.11b71vdgdz", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc39sjp\n# Answer:\ng.1hhc39sjp", "# Reasoning Path:\nBrazil -> location.statistical_region.gni_in_ppp_dollars -> g.11b60s1wwg\n# Answer:\ng.11b60s1wwg", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fm01\n# Answer:\ng.12tb6fm01", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc385hl\n# Answer:\ng.1hhc385hl", "# Reasoning Path:\nBrazil -> location.statistical_region.gni_in_ppp_dollars -> g.1245_0s4c\n# Answer:\ng.1245_0s4c", "# Reasoning Path:\nBrazil -> location.statistical_region.gni_in_ppp_dollars -> g.1245_1jc5\n# Answer:\ng.1245_1jc5"], "ground_truth": ["Brazilian Portuguese", "Portuguese Language", "Italian Language"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-467", "prediction": ["# Reasoning Path:\nObi-Wan Kenobi -> film.film_character.portrayed_in_films -> m.0k3qy3 -> film.performance.actor -> Ewan McGregor\n# Answer:\nm.0k3qy3", "# Reasoning Path:\nObi-Wan Kenobi -> film.film_character.portrayed_in_films -> m.0k3r05 -> film.performance.actor -> Ewan McGregor\n# Answer:\nm.0k3r05", "# Reasoning Path:\nObi-Wan Kenobi -> film.film_character.portrayed_in_films -> m.0k3qzt -> film.performance.actor -> Ewan McGregor\n# Answer:\nm.0k3qzt", "# Reasoning Path:\nObi-Wan Kenobi -> common.topic.notable_for -> g.1258f4t33\n# Answer:\ng.1258f4t33", "# Reasoning Path:\nObi-Wan Kenobi -> film.film_character.portrayed_in_films -> m.0k3qy3 -> film.performance.film -> Star Wars Episode I: The Phantom Menace\n# Answer:\nm.0k3qy3", "# Reasoning Path:\nObi-Wan Kenobi -> film.film_character.portrayed_in_films -> m.0k3r05 -> film.performance.film -> Star Wars Episode III: Revenge of the Sith\n# Answer:\nm.0k3r05", "# Reasoning Path:\nObi-Wan Kenobi -> film.film_character.portrayed_in_films -> m.0k3qzt -> film.performance.film -> Star Wars Episode II: Attack of the Clones\n# Answer:\nm.0k3qzt", "# Reasoning Path:\nObi-Wan Kenobi -> common.topic.webpage -> m.09xlg53 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09xlg53", "# Reasoning Path:\nObi-Wan Kenobi -> common.topic.webpage -> m.09xlg53 -> common.webpage.resource -> Trend Watch: Who has the best celebrity beard?\n# Answer:\nm.09xlg53", "# Reasoning Path:\nObi-Wan Kenobi -> common.topic.webpage -> m.09xsm5h -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09xsm5h"], "ground_truth": ["Ewan McGregor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-468", "prediction": ["# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.location.containedby -> California\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.location.containedby -> Los Angeles County\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.location.containedby -> United States of America\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.location.people_born_here -> Ace Mask\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.location.people_born_here -> Art Olivier\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.statistical_region.population -> g.11b66hc7d0\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> common.topic.webpage -> m.09xxg8l -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09xxg8l", "# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.location.people_born_here -> Big Van Vader\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.statistical_region.population -> g.11bymm5pgf\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> common.topic.webpage -> m.09ykq8v -> common.webpage.resource -> What's your favorite tennis-related movie moment?\n# Answer:\nm.09ykq8v"], "ground_truth": ["Lynwood"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-469", "prediction": ["# Reasoning Path:\nPanama -> location.country.currency_used -> Panamanian balboa -> common.topic.image -> \u00bd balboa(front)\n# Answer:\nPanamanian balboa", "# Reasoning Path:\nPanama -> location.country.currency_used -> Panamanian balboa -> common.topic.article -> m.0200cx\n# Answer:\nPanamanian balboa", "# Reasoning Path:\nPanama -> location.statistical_region.official_development_assistance -> g.12cp_k2q2\n# Answer:\ng.12cp_k2q2", "# Reasoning Path:\nPanama -> location.country.currency_used -> United States Dollar -> finance.currency.countries_formerly_used -> Cuba\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPanama -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> Ecuador\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPanama -> location.country.currency_used -> United States Dollar -> base.coinsdaily.coin_type.country -> United States of America\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPanama -> location.statistical_region.official_development_assistance -> g.1hhc37pjb\n# Answer:\ng.1hhc37pjb", "# Reasoning Path:\nPanama -> location.statistical_region.official_development_assistance -> g.1hhc37pjc\n# Answer:\ng.1hhc37pjc", "# Reasoning Path:\nPanama -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> El Salvador\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPanama -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> Puerto Rico\n# Answer:\nUnited States Dollar"], "ground_truth": ["United States Dollar", "Panamanian balboa"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> common.topic.notable_types -> Musical Recording\n# Answer:\nMusical Recording", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> common.topic.image -> Ryan Braun\n# Answer:\nRyan Braun", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> California\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> common.topic.notable_types -> TV Episode\n# Answer:\nTV Episode", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> Los Angeles\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Alan Muraoka\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> Los Angeles County\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Chris Snail\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> common.topic.notable_types -> TV Program -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nTV Program", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.03lppm1 -> common.webpage.resource -> m.0bjrztw\n# Answer:\nm.03lppm1"], "ground_truth": ["Mission Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-470", "prediction": ["# Reasoning Path:\nZambezi -> geography.river.origin -> Mwinilunga -> location.location.containedby -> Zambia\n# Answer:\nMwinilunga", "# Reasoning Path:\nZambezi -> geography.river.origin -> Mwinilunga -> common.topic.notable_types -> Location\n# Answer:\nMwinilunga", "# Reasoning Path:\nZambezi -> geography.river.origin -> Mwinilunga -> common.topic.article -> m.07cdmx\n# Answer:\nMwinilunga", "# Reasoning Path:\nZambezi -> geography.river.cities -> Sesheke -> location.location.containedby -> Zambia\n# Answer:\nSesheke", "# Reasoning Path:\nZambezi -> geography.river.cities -> Mongu -> location.location.containedby -> Zambia\n# Answer:\nMongu", "# Reasoning Path:\nZambezi -> geography.river.cities -> Mongu -> location.statistical_region.population -> g.11bc87rxtx\n# Answer:\nMongu", "# Reasoning Path:\nZambezi -> geography.river.cities -> Sesheke -> common.topic.image -> Location of Sesheke in Zambia\n# Answer:\nSesheke", "# Reasoning Path:\nZambezi -> geography.river.cities -> Kariba, Zimbabwe -> location.location.nearby_airports -> Kariba Airport\n# Answer:\nKariba, Zimbabwe", "# Reasoning Path:\nZambezi -> geography.river.basin_countries -> Mozambique -> location.country.languages_spoken -> Portuguese Language\n# Answer:\nMozambique", "# Reasoning Path:\nZambezi -> geography.river.cities -> Sesheke -> common.topic.notable_for -> g.125614dst\n# Answer:\nSesheke"], "ground_truth": ["Mwinilunga"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-472", "prediction": ["# Reasoning Path:\nRonaldo -> sports.pro_athlete.teams -> m.052b2p8 -> sports.sports_team_roster.team -> Sport Club Corinthians Paulista\n# Answer:\nm.052b2p8", "# Reasoning Path:\nRonaldo -> sports.pro_athlete.teams -> m.0nh6z4r -> sports.sports_team_roster.team -> Cruzeiro Esporte Clube\n# Answer:\nm.0nh6z4r", "# Reasoning Path:\nRonaldo -> sports.pro_athlete.teams -> m.0nh6z71 -> sports.sports_team_roster.team -> PSV Eindhoven\n# Answer:\nm.0nh6z71", "# Reasoning Path:\nRonaldo -> soccer.football_player.goals_scored -> m.0bg7169 -> soccer.football_goal.point_awarded_to -> Brazil national football team\n# Answer:\nm.0bg7169", "# Reasoning Path:\nRonaldo -> sports.pro_athlete.teams -> m.052b2p8 -> sports.sports_team_roster.position -> Forward\n# Answer:\nm.052b2p8", "# Reasoning Path:\nRonaldo -> sports.pro_athlete.teams -> m.0nh6z4r -> sports.sports_team_roster.position -> Forward\n# Answer:\nm.0nh6z4r", "# Reasoning Path:\nRonaldo -> soccer.football_player.goals_scored -> m.0bg7169 -> soccer.football_goal.match -> 2002 FIFA World Cup Final\n# Answer:\nm.0bg7169", "# Reasoning Path:\nRonaldo -> soccer.football_player.goals_scored -> m.0bg716j -> soccer.football_goal.point_awarded_to -> Brazil national football team\n# Answer:\nm.0bg716j", "# Reasoning Path:\nRonaldo -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#range -> Person\n# Answer:\nChildren", "# Reasoning Path:\nRonaldo -> soccer.football_player.goals_scored -> m.0bg716j -> soccer.football_goal.match -> 2002 FIFA World Cup Final\n# Answer:\nm.0bg716j"], "ground_truth": ["Real Madrid C.F.", "Brazil national football team"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.2, "path_recall": 0.3333333333333333, "path_ans_f1": 0.28571428571428575, "path_ans_precision": 0.2, "path_ans_recall": 0.5}
{"id": "WebQTest-473", "prediction": ["# Reasoning Path:\nRobin Williams -> people.person.places_lived -> m.0bh5kjj -> people.place_lived.location -> San Francisco\n# Answer:\nm.0bh5kjj", "# Reasoning Path:\nRobin Williams -> people.person.places_lived -> m.03pqrmt -> people.place_lived.location -> Bloomfield Hills\n# Answer:\nm.03pqrmt", "# Reasoning Path:\nRobin Williams -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> San Francisco\n# Answer:\nUnited States of America", "# Reasoning Path:\nRobin Williams -> people.person.nationality -> United States of America -> common.topic.notable_types -> Country\n# Answer:\nUnited States of America", "# Reasoning Path:\nRobin Williams -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> California\n# Answer:\nUnited States of America", "# Reasoning Path:\nRobin Williams -> people.person.children -> Zak Williams -> people.person.place_of_birth -> San Francisco\n# Answer:\nZak Williams", "# Reasoning Path:\nRobin Williams -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Chicago\n# Answer:\nUnited States of America", "# Reasoning Path:\nRobin Williams -> people.person.children -> Cody Alan Williams -> people.person.nationality -> United States of America\n# Answer:\nCody Alan Williams", "# Reasoning Path:\nRobin Williams -> people.person.children -> Cody Alan Williams -> people.person.place_of_birth -> New York City\n# Answer:\nCody Alan Williams", "# Reasoning Path:\nRobin Williams -> people.person.children -> Zachary Pym Williams -> people.person.nationality -> United States of America\n# Answer:\nZachary Pym Williams"], "ground_truth": ["San Francisco", "Bloomfield Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5333333333333333, "path_precision": 0.4, "path_recall": 0.8, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-474", "prediction": ["# Reasoning Path:\nJacob Black -> film.film_character.portrayed_in_films -> m.04z4zmg -> film.performance.actor -> Taylor Lautner\n# Answer:\nm.04z4zmg", "# Reasoning Path:\nTwilight -> film.film.starring -> m.04m6l9_ -> film.performance.actor -> Kristen Stewart\n# Answer:\nm.04m6l9_", "# Reasoning Path:\nJacob Black -> film.film_character.portrayed_in_films -> m.0b68zn5 -> film.performance.actor -> Taylor Lautner\n# Answer:\nm.0b68zn5", "# Reasoning Path:\nJacob Black -> film.film_character.portrayed_in_films -> m.04z4zmg -> film.performance.film -> Twilight\n# Answer:\nm.04z4zmg", "# Reasoning Path:\nTwilight -> award.award_winning_work.awards_won -> m.0nfjtkv -> award.award_honor.award_winner -> Taylor Lautner\n# Answer:\nm.0nfjtkv", "# Reasoning Path:\nTwilight -> film.film.starring -> m.04m6lb4 -> film.performance.actor -> Robert Pattinson\n# Answer:\nm.04m6lb4", "# Reasoning Path:\nTwilight -> film.film.starring -> m.04z4zl_ -> film.performance.actor -> Elizabeth Reaser\n# Answer:\nm.04z4zl_", "# Reasoning Path:\nJacob Black -> film.film_character.portrayed_in_films -> m.05tflhw -> film.performance.actor -> Taylor Lautner\n# Answer:\nm.05tflhw", "# Reasoning Path:\nTwilight -> film.film.starring -> m.04m6l9_ -> film.performance.character -> Bella Swan\n# Answer:\nm.04m6l9_", "# Reasoning Path:\nTwilight -> award.award_winning_work.awards_won -> m.0nfjtkv -> award.award_honor.ceremony -> 2009 Teen Choice Awards\n# Answer:\nm.0nfjtkv"], "ground_truth": ["Taylor Lautner"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4000000000000001, "path_precision": 0.4, "path_recall": 0.4, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-477", "prediction": ["# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Aeroparque Jorge Newbery -> aviation.airport.hub_for -> Aerol\u00edneas Argentinas\n# Answer:\nAeroparque Jorge Newbery", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Don Torcuato Airport -> location.location.containedby -> Argentina\n# Answer:\nDon Torcuato Airport", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Aeroparque Jorge Newbery -> aviation.airport.hub_for -> AirPampas\n# Answer:\nAeroparque Jorge Newbery", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Aeroparque Jorge Newbery -> aviation.airport.focus_city_for -> LAN Airlines\n# Answer:\nAeroparque Jorge Newbery", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Ministro Pistarini International Airport -> common.topic.webpage -> m.03l9zc6\n# Answer:\nMinistro Pistarini International Airport", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Ministro Pistarini International Airport -> common.topic.article -> m.01ky3n\n# Answer:\nMinistro Pistarini International Airport", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Aeroparque Jorge Newbery -> aviation.airport.hub_for -> Austral L\u00edneas A\u00e9reas\n# Answer:\nAeroparque Jorge Newbery", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Aeroparque Jorge Newbery -> location.location.containedby -> Argentina\n# Answer:\nAeroparque Jorge Newbery", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Ministro Pistarini International Airport -> location.location.events -> Ezeiza massacre\n# Answer:\nMinistro Pistarini International Airport", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Don Torcuato Airport -> common.topic.notable_for -> g.12565x5lm\n# Answer:\nDon Torcuato Airport"], "ground_truth": ["Ministro Pistarini International Airport", "Aeroparque Jorge Newbery", "Don Torcuato Airport"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-479", "prediction": ["# Reasoning Path:\nAdrian Peterson -> sports.pro_athlete.teams -> m.03gkk7c -> sports.sports_team_roster.team -> Minnesota Vikings\n# Answer:\nm.03gkk7c", "# Reasoning Path:\nAdrian Peterson -> sports.pro_athlete.teams -> m.03gkk7c -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.03gkk7c", "# Reasoning Path:\nAdrian Peterson -> sports.pro_athlete.teams -> m.03gkk7c -> sports.sports_team_roster.position -> Running back\n# Answer:\nm.03gkk7c", "# Reasoning Path:\nAdrian Peterson -> award.award_nominee.award_nominations -> m.0_qwmrv -> award.award_nomination.award -> NFL Play of the Year Award\n# Answer:\nm.0_qwmrv", "# Reasoning Path:\nAdrian Peterson -> award.award_winner.awards_won -> m.04kt42p -> award.award_honor.award -> National Football League Rookie of the Year Award\n# Answer:\nm.04kt42p", "# Reasoning Path:\nAdrian Peterson -> award.award_nominee.award_nominations -> m.0_qwmrv -> freebase.valuenotation.has_value -> Nominated work\n# Answer:\nm.0_qwmrv", "# Reasoning Path:\nAdrian Peterson -> award.award_nominee.award_nominations -> m.0_qxjbh -> award.award_nomination.ceremony -> 2nd Annual NFL Honors\n# Answer:\nm.0_qxjbh", "# Reasoning Path:\nAdrian Peterson -> award.award_nominee.award_nominations -> m.0_qwmrv -> award.award_nomination.ceremony -> 3rd Annual NFL Honors\n# Answer:\nm.0_qwmrv", "# Reasoning Path:\nAdrian Peterson -> award.award_nominee.award_nominations -> m.0_qxkv3 -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nm.0_qxkv3", "# Reasoning Path:\nAdrian Peterson -> award.award_winner.awards_won -> m.0_qxb1r -> award.award_honor.ceremony -> 2nd Annual NFL Honors\n# Answer:\nm.0_qxb1r"], "ground_truth": ["University of Oklahoma"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.citytown -> Suwon\n# Answer:\nm.03l9ynf", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.administrative_division.country -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.country -> South Korea\n# Answer:\nm.03l9ynf", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> base.aareas.schema.administrative_area.administrative_parent -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.postal_code -> 443-742\n# Answer:\nm.03l9ynf", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.containedby -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Mahaka Media\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Textron\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Time Warner\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> common.topic.article -> m.0h6dr\n# Answer:\nConglomerate"], "ground_truth": ["Suwon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-480", "prediction": ["# Reasoning Path:\nPennsylvania -> symbols.namesake.named_after -> William Penn -> people.person.place_of_birth -> Bristol\n# Answer:\nWilliam Penn", "# Reasoning Path:\nPennsylvania -> base.aareas.schema.administrative_area.administrative_children -> Meadville -> location.location.containedby -> Crawford County\n# Answer:\nMeadville", "# Reasoning Path:\nPennsylvania -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.country.first_level_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nPennsylvania -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.country.first_level_divisions -> Arizona\n# Answer:\nUnited States of America", "# Reasoning Path:\nPennsylvania -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nPennsylvania -> symbols.namesake.named_after -> William Penn -> people.person.parents -> Giles Penn\n# Answer:\nWilliam Penn", "# Reasoning Path:\nPennsylvania -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.country.first_level_divisions -> Arkansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nPennsylvania -> base.aareas.schema.administrative_area.administrative_children -> Meadville -> location.hud_county_place.county -> Crawford County\n# Answer:\nMeadville", "# Reasoning Path:\nPennsylvania -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Arizona\n# Answer:\nUnited States of America", "# Reasoning Path:\nPennsylvania -> symbols.namesake.named_after -> William Penn -> people.person.parents -> Joan Gilbert\n# Answer:\nWilliam Penn"], "ground_truth": ["William Penn"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-481", "prediction": ["# Reasoning Path:\nGermany -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\ng.11b60v9zy7", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_38m_\n# Answer:\ng.1245_38m_", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fdl8\n# Answer:\ng.12tb6fdl8", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc38c_4\n# Answer:\ng.1hhc38c_4", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_67jr\n# Answer:\ng.1245_67jr", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_jlz4\n# Answer:\ng.1245_jlz4"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-482", "prediction": ["# Reasoning Path:\nBatman -> film.film.starring -> m.02tb1p0 -> film.performance.actor -> Jack Palance\n# Answer:\nm.02tb1p0", "# Reasoning Path:\nBatman -> film.film.starring -> m.02wk6dq -> film.performance.actor -> Michael Keaton\n# Answer:\nm.02wk6dq", "# Reasoning Path:\nBatman -> film.film.film_series -> Batman Film Series -> common.topic.article -> m.02rx1t3\n# Answer:\nBatman Film Series", "# Reasoning Path:\nBatman -> film.film.other_crew -> m.0v9503y -> film.film_crew_gig.crewmember -> Lynda Armstrong\n# Answer:\nm.0v9503y", "# Reasoning Path:\nBatman -> film.film.other_crew -> m.0v9506s -> film.film_crew_gig.crewmember -> Nick Dudman\n# Answer:\nm.0v9506s", "# Reasoning Path:\nBatman -> film.film.starring -> m.02tb1p0 -> film.performance.character -> g.1232h_22\n# Answer:\nm.02tb1p0", "# Reasoning Path:\nBatman -> film.film.starring -> m.02wk8_m -> film.performance.character -> Joker\n# Answer:\nm.02wk8_m", "# Reasoning Path:\nBatman -> film.film.film_series -> Batman Film Series -> award.award_nominated_work.award_nominations -> m.0ndvr43\n# Answer:\nBatman Film Series", "# Reasoning Path:\nBatman -> film.film.other_crew -> m.0v9503y -> film.film_crew_gig.film_crew_role -> Makeup Artist\n# Answer:\nm.0v9503y", "# Reasoning Path:\nBatman -> film.film.other_crew -> m.0v9508w -> film.film_crew_gig.film_crew_role -> Chief Makeup Artist\n# Answer:\nm.0v9508w"], "ground_truth": ["Matthew Wagner", "Carlos Alazraqui", "Tom Hardy", "Danny Trejo"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-484", "prediction": ["# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.main_country -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> New Zealand\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\ng.11b60qdnlp", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\ng.11b60vfr15", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> Japan\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\ng.1245_2hrg"], "ground_truth": ["Lojban", "English Language", "Esperanto Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-487", "prediction": ["# Reasoning Path:\nVeronica Lake -> people.person.spouse_s -> m.03lf38m -> people.marriage.spouse -> John S. Detlie\n# Answer:\nm.03lf38m", "# Reasoning Path:\nVeronica Lake -> people.person.spouse_s -> m.0h2qr4m -> people.marriage.spouse -> Joseph A. McCarthy\n# Answer:\nm.0h2qr4m", "# Reasoning Path:\nVeronica Lake -> people.person.children -> Andre Michael De Toth III -> people.person.parents -> Andr\u00e9 de Toth\n# Answer:\nAndre Michael De Toth III", "# Reasoning Path:\nVeronica Lake -> people.person.spouse_s -> m.03lf38m -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.03lf38m", "# Reasoning Path:\nVeronica Lake -> people.person.spouse_s -> m.02kkn6n -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.02kkn6n", "# Reasoning Path:\nVeronica Lake -> people.person.parents -> Constance Charlotta Trimble -> people.person.spouse_s -> m.0ztrgpz\n# Answer:\nConstance Charlotta Trimble", "# Reasoning Path:\nVeronica Lake -> people.person.parents -> Harry E. Ockelman -> people.person.spouse_s -> m.0ztrgpz\n# Answer:\nHarry E. Ockelman", "# Reasoning Path:\nVeronica Lake -> people.person.spouse_s -> m.03lf38m -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.03lf38m", "# Reasoning Path:\nVeronica Lake -> people.person.spouse_s -> m.02kkn6n -> people.marriage.spouse -> Andr\u00e9 de Toth\n# Answer:\nm.02kkn6n", "# Reasoning Path:\nVeronica Lake -> people.person.parents -> Constance Charlotta Trimble -> people.person.spouse_s -> m.0ztrgwd\n# Answer:\nConstance Charlotta Trimble"], "ground_truth": ["Robert Carleton-Munro", "Andr\u00e9 de Toth", "John S. Detlie", "Joseph A. McCarthy"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.5217391304347827, "path_ans_precision": 0.4, "path_ans_recall": 0.75}
{"id": "WebQTest-488", "prediction": ["# Reasoning Path:\nFrancis Drake -> people.person.spouse_s -> m.0k1nmz1 -> people.marriage.spouse -> Mary Newman\n# Answer:\nm.0k1nmz1", "# Reasoning Path:\nFrancis Drake -> people.person.spouse_s -> m.0k1nmz1 -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0k1nmz1", "# Reasoning Path:\nFrancis Drake -> people.person.spouse_s -> m.0k1nmzh -> people.marriage.spouse -> Elizabeth Sydenham\n# Answer:\nm.0k1nmzh", "# Reasoning Path:\nFrancis Drake -> people.person.spouse_s -> m.0k1nmzh -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0k1nmzh", "# Reasoning Path:\nFrancis Drake -> military.military_commander.military_commands -> m.05sp345 -> military.military_command.military_conflict -> Battle of Gravelines (1588)\n# Answer:\nm.05sp345", "# Reasoning Path:\nFrancis Drake -> base.kwebbase.kwtopic.has_sentences -> After a daring raid on the mule train carrying treasure overland from Panama to Nombre de Dios,  Drake felt rich enough to return home. -> base.kwebbase.kwsentence.previous_sentence -> The pinnaces played a vital part in ferrying raiding parties and their plunder on and off the South American mainland.\n# Answer:\nAfter a daring raid on the mule train carrying treasure overland from Panama to Nombre de Dios,  Drake felt rich enough to return home.", "# Reasoning Path:\nFrancis Drake -> base.kwebbase.kwtopic.has_sentences -> After a hostile reception in the Caroline Islands they reached the Moluccas where the Sultan agreed to trade with England and allowed Drake to sail off with a profitable cargo: six tons of cloves. -> base.kwebbase.kwsentence.next_sentence -> \\\"The Golden Hind\\\" survived running aground on a coral reef and Drake visited Java before crossing the Indian Ocean and round the Cape of Good Hope into the Atlantic.\n# Answer:\nAfter a hostile reception in the Caroline Islands they reached the Moluccas where the Sultan agreed to trade with England and allowed Drake to sail off with a profitable cargo: six tons of cloves.", "# Reasoning Path:\nFrancis Drake -> base.kwebbase.kwtopic.has_sentences -> After a daring raid on the mule train carrying treasure overland from Panama to Nombre de Dios,  Drake felt rich enough to return home. -> base.kwebbase.kwsentence.next_sentence -> His ships were loaded up with enough gold to crew and equip thirty warships and he flew the English flag on his mainmast as he sailed (out of gun range) past Cartagena.\n# Answer:\nAfter a daring raid on the mule train carrying treasure overland from Panama to Nombre de Dios,  Drake felt rich enough to return home.", "# Reasoning Path:\nFrancis Drake -> military.military_commander.military_commands -> m.05t66k0 -> military.military_command.military_conflict -> Anglo-Spanish War\n# Answer:\nm.05t66k0", "# Reasoning Path:\nFrancis Drake -> base.kwebbase.kwtopic.has_sentences -> After a hostile reception in the Caroline Islands they reached the Moluccas where the Sultan agreed to trade with England and allowed Drake to sail off with a profitable cargo: six tons of cloves. -> base.kwebbase.kwsentence.previous_sentence -> He did not see land until the end of September but it had rained nearly every day and his men had been able to catch fish.\n# Answer:\nAfter a hostile reception in the Caroline Islands they reached the Moluccas where the Sultan agreed to trade with England and allowed Drake to sail off with a profitable cargo: six tons of cloves."], "ground_truth": ["Elizabeth Sydenham", "Mary Newman"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-489", "prediction": ["# Reasoning Path:\nOne for the Money -> film.film.starring -> m.0gv_fdr -> film.performance.character -> Stephanie Plum\n# Answer:\nm.0gv_fdr", "# Reasoning Path:\nOne for the Money -> film.film.starring -> m.0gv_fdr -> film.performance.actor -> Katherine Heigl\n# Answer:\nm.0gv_fdr", "# Reasoning Path:\nOne for the Money -> film.film.starring -> m.0gv_r2n -> film.performance.character -> Vinnie Plum\n# Answer:\nm.0gv_r2n", "# Reasoning Path:\nOne for the Money -> film.film.starring -> m.0gvwwl8 -> film.performance.actor -> Daniel Sunjata\n# Answer:\nm.0gvwwl8", "# Reasoning Path:\nOne for the Money -> film.film.starring -> m.0gv_r2n -> film.performance.actor -> Patrick Fischler\n# Answer:\nm.0gv_r2n", "# Reasoning Path:\nOne for the Money -> film.film.starring -> m.0gvwwl8 -> film.performance.character -> Ranger\n# Answer:\nm.0gvwwl8", "# Reasoning Path:\nOne for the Money -> film.film.other_crew -> m.0gwdhg0 -> film.film_crew_gig.film_crew_role -> Key Makeup Artist\n# Answer:\nm.0gwdhg0", "# Reasoning Path:\nOne for the Money -> film.film.other_crew -> m.0gwdhg0 -> film.film_crew_gig.crewmember -> Steven E. Anderson\n# Answer:\nm.0gwdhg0", "# Reasoning Path:\nOne for the Money -> film.film.other_crew -> m.0gwdhg8 -> film.film_crew_gig.film_crew_role -> Key Hair Stylist\n# Answer:\nm.0gwdhg8", "# Reasoning Path:\nOne for the Money -> film.film.release_date_s -> m.0jsmys9 -> film.film_regional_release_date.film_release_region -> Canada\n# Answer:\nm.0jsmys9"], "ground_truth": ["Katherine Heigl"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> book.book_subject.works -> Beside Still Waters\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> common.topic.notable_for -> g.1q69mrtxz\n# Answer:\nEntering Heaven alive", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> base.ontologies.ontology_instance.equivalent_instances -> m.07nfc1g\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.beliefs -> End time\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> book.book_subject.works -> I, the Sun\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> religion.belief.belief_of -> Christianity\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> book.book_subject.works -> Moses and Monotheism\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> common.topic.article -> m.02wvcg8\n# Answer:\nEntering Heaven alive", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> \u1e6c\u016bb\u0101 -> common.topic.notable_for -> g.1q6hmhsk5\n# Answer:\n\u1e6c\u016bb\u0101", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> religion.belief.belief_of -> Conservative Judaism\n# Answer:\nMonotheism"], "ground_truth": ["Predestination in Islam", "Islamic holy books", "God in Islam", "Qiyamah", "Prophets in Islam", "\u1e6c\u016bb\u0101", "Islamic view of angels", "Tawhid", "Monotheism", "Masih ad-Dajjal", "Mahdi", "Sharia", "Entering Heaven alive"], "ans_acc": 0.23076923076923078, "ans_hit": 1, "ans_f1": 0.3673469387755102, "ans_precission": 0.9, "ans_recall": 0.23076923076923078, "path_f1": 0.3673469387755102, "path_precision": 0.9, "path_recall": 0.23076923076923078, "path_ans_f1": 0.3673469387755102, "path_ans_precision": 0.9, "path_ans_recall": 0.23076923076923078}
{"id": "WebQTest-490", "prediction": ["# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Greek Language -> language.human_language.writing_system -> Greek alphabet\n# Answer:\nGreek Language", "# Reasoning Path:\nRoman Empire -> location.country.official_language -> Latin Language -> language.human_language.countries_spoken_in -> Duchy of Burgundy\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Greek Language -> media_common.netflix_genre.titles -> A Girl in Black\n# Answer:\nGreek Language", "# Reasoning Path:\nRoman Empire -> location.country.official_language -> Latin Language -> language.human_language.countries_spoken_in -> Kingdom of Aragon\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> location.country.official_language -> Latin Language -> music.genre.albums -> Acustico\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Greek Language -> media_common.netflix_genre.titles -> A Matter of Dignity\n# Answer:\nGreek Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Latin Language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> location.country.official_language -> Latin Language -> language.human_language.countries_spoken_in -> Republic of Genoa\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> location.country.official_language -> Latin Language -> music.genre.artists -> Alex Carta\u00f1\u00e1\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Latin Language -> language.human_language.countries_spoken_in -> Duchy of Burgundy\n# Answer:\nLatin Language"], "ground_truth": ["Latin Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-491", "prediction": ["# Reasoning Path:\nRingo Starr -> music.artist.contribution -> m.0vv1_m9 -> music.recording_contribution.album -> Cloud Nine\n# Answer:\nm.0vv1_m9", "# Reasoning Path:\nRingo Starr -> people.person.profession -> Actor -> people.profession.specializations -> Narrator\n# Answer:\nActor", "# Reasoning Path:\nRingo Starr -> music.artist.contribution -> m.0vv1lmv -> music.recording_contribution.album -> Is This What You Want?\n# Answer:\nm.0vv1lmv", "# Reasoning Path:\nRingo Starr -> music.artist.contribution -> m.0vv1l5w -> music.recording_contribution.album -> Wonderwall Music\n# Answer:\nm.0vv1l5w", "# Reasoning Path:\nRingo Starr -> people.person.profession -> Actor -> common.topic.subjects -> Michael Palance\n# Answer:\nActor", "# Reasoning Path:\nRingo Starr -> people.person.profession -> Cinematographer -> common.topic.notable_types -> Profession\n# Answer:\nCinematographer", "# Reasoning Path:\nRingo Starr -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nRingo Starr -> people.person.profession -> Composer -> common.topic.notable_types -> Profession\n# Answer:\nComposer", "# Reasoning Path:\nRingo Starr -> people.person.parents -> Elsie Starkey -> people.person.gender -> Female\n# Answer:\nElsie Starkey", "# Reasoning Path:\nRingo Starr -> people.person.parents -> Richard Starkey -> common.topic.notable_for -> g.125h3_b65\n# Answer:\nRichard Starkey"], "ground_truth": ["Everlasting Love", "Rory and the Hurricanes", "Drowning in the Sea of Love", "English Garden / I Really Love Her", "In My Car", "Devil Woman", "Act Naturally", "Some People", "Trippin' on My Own Tears", "Red and Black Blues", "Love Don't Last Long", "Boys", "Vertical Man", "Matchbox", "In a Heartbeat", "A Dose of Rock 'N' Roll", "Love Me Do", "Missouri Loves Company", "Out on the Streets", "Iko Iko", "I'll Be Fine Anywhere", "I Don't Believe You", "I Wouldn't Have You Any Other Way", "Spooky Wierdness", "Sure To Fall", "(It's All Down to) Good Night Vienna", "If It's Love That You Want", "Slow Down", "Liverpool 8", "Night and Day", "What Goes Around", "For Love", "Alibi", "Oh My Lord", "Step Lightly", "I'm the Greatest", "Y Not", "Drumming Is My Madness", "Logical Song", "One", "Who Needs a Heart", "Six O'Clock (extended version)", "Cryin'", "Can She Do It Like She Dances", "Six O'Clock", "Tango All Night", "Your Sixteen", "Husbands and Wives", "Stardust", "The End", "I'll Still Love You", "With a Little Help From My Friends (reprise)", "Don't Go Where the Road Don't Go", "Have You Seen My Baby (Hold On)", "Running Free", "The Christmas Dance", "Gone Are the Days", "Occapella", "All in the Name of Love", "After All These Years", "Husbands And Wives", "Simple Love Song", "You Don't Know Me at All", "No No Song", "Love Is", "Picture Show Life", "Free Drinks", "White Christmas", "King of Broken Hearts", "Dead Giveaway", "I Think Therefore I Rock 'n Roll", "You're Sixteen (You're Beautiful and You're Mine)", "Blindman", "Scouse the Mouse", "Las Brisas", "Photograph", "You Know It Makes Sense", "Love First, Ask Questions Later", "No One to Blame", "Brandy", "Nashville Jam", "Tonight", "You Bring the Party Down", "Don't Be Cruel", "Tuff Love", "Going Down", "Me and You", "All by Myself", "The Little Drummer Boy", "Easy for Me", "The Turnaround", "Old Time Relovin'", "Don't Know a Thing About Love", "A Mouse Like Me", "Lay Down Your Arms (feat. Stevie Nicks)", "I'm a Fool to Care", "Spooky Weirdness", "Snookeroo", "Lucky Man", "Cookin' (In the Kitchen of Love)", "Heart on My Sleeve", "Silent Homecoming", "Living in a Pet Shop", "OO-WEE", "Where Did Our Love Go", "Easy For Me", "Only You (And You Alone)", "Fading In Fading Out", "Wrong All the Time", "Have You Seen My Baby", "Only You (and You Alone)", "Bye Bye Blackbird", "Love Is a Many Splendoured Thing", "You and Me (Babe)", "Be My Baby", "I Wanna Be Your Man", "Weight of the World", "Don\u2019t Hang Up", "I'm Yours", "I've Got Blisters...", "Hand Gun Promos", "Fading in Fading Out", "Sweet Little Sixteen", "I Still Love Rock 'n' Roll", "I Really Love Her", "It's No Secret", "S.O.S.", "Give Me Back the Beat", "Honey Don't", "Call Me", "Wine, Women, and Loud Happy Songs", "Let the Rest of the World Go By", "$15 Draw", "Hopeless", "Wonderful", "Nice Way", "Hard Times", "I'd Be Talking All the Time", "Octopus's Garden", "Elizabeth Reigns", "Oo-Wee", "Anthem", "Eye to Eye", "You\u2019re Sixteen", "All the Young Dudes", "Rock Island Line", "A Dose of Rock 'n' Roll", "You Always Hurt the One You Love", "I Was Walkin'", "Coochy Coochy", "Mystery of the Night", "No No Song/Skokiaan", "Boat Ride", "Samba", "Fastest Growing Heartache In The West", "Imagine Me There", "Have I Told You Lately That I Love You?", "You Can't Fight Lightning", "Satisfied", "Out On The Streets", "Memphis in Your Mind", "Goodnight Vienna", "Give a Little Bit", "With a Little Help From My Friends", "Caterwaul", "Gave It All Up", "Hey Baby", "Walk With You", "Never Without You", "Private Property", "Let Love Lead", "Can't Do It Wrong", "Sneaking Sally Through the Alley", "Love Bizarre", "Write One for Me", "Choose Love", "Peace Dream", "Wings", "Bamboula", "La De Da", "Who's Your Daddy", "Sunshine Life for Me (Sail Away Raymond)", "She's About a Mover", "Beaucoups of Blues", "Everyone Wins", "It Don't Come Easy", "You're Sixteen", "This Be Called a Song", "What in the... World", "Glamorous Life", "Sentimental Journey", "Down and Out", "Give It a Try", "With a Little Help From My Friends / It Don't Come Easy", "Fiddle About", "What Love Wants to Be", "Yellow Submarine", "As Far as We Can Go (original version)", "Not Looking Back", "Goodnight Vienna (reprise)", "I'd be Talking all the Time", "Only You", "Runaways", "Gypsies In Flight", "Stop and Take the Time to Smell the Roses (Original Vocal Version)", "Without Understanding", "Early 1970", "Island in the Sun", "I Keep Forgettin'", "Instant Amnesia", "Blue Christmas", "You Belong to Me", "Pax Um Biscum (Peace Be With You)", "Christmas Eve", "Rudolph the Red\u2010Nosed Reindeer", "Don't Pass Me By", "The No-No Song", "Karn Evil 9", "With a Little Help From My Friends (live)", "(It's All Down to) Good Night Vienna (single version)", "Right Side of the Road", "Postcards From Paradise", "Don't Hang Up", "In Liverpool", "Sure to Fall (In Love With You)", "Lady Gaye", "Pure Gold", "Attention", "Six O\u2019Clock", "Stop and Take the Time to Smell the Roses", "Back Off Boogaloo", "Scouse's Dream", "Six O'Clock (Extended Version)", "Drowning In The Sea Of Love", "Pasodobles", "Everybody's in a Hurry but Me", "Bad Boy", "Confirmation", "Think It Over", "Fill in the Blanks", "Oh My My", "Christmas Time Is Here Again", "Bridges", "Lipstick Traces", "Gypsies in Flight", "I Know a Place", "The Other Side of Liverpool", "Golden Blunders", "As Far as We Can Go", "Dear Santa", "Come on Christmas, Christmas Come On", "The No No Song", "Mindfield", "Puppet", "Fading in and Fading Out", "Without Her", "A Man Like Me", "Pinocchio Medley (\\\"Do You See the Noses Growing?\\\"): Desolation Theme / When You Wish Upon a Star", "Dream", "Wrack My Brain", "Monkey See, Monkey Do", "I Wanna Be Santa Claus", "Wake Up", "Take the Long Way Home", "Think About You", "Now That She's Gone Away", "Spooky Weirdiness", "Blue, Turning Grey Over You", "Whispering Grass (Don't Tell the Trees)", "Drift Away", "Time", "Tommy's Holiday Camp", "Hard to Be True", "The Really 'Serious' Introduction", "Woman of the Night", "Touch and Go", "Waiting", "Harry's Song", "All By Myself", "R U Ready", "Fastest Growing Heartache in the West", "Winter Wonderland", "Wine, Women and Loud Happy Songs", "Loser's Lounge", "You Never Know"], "ans_acc": 0.0035087719298245615, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.006779661016949153, "path_ans_precision": 0.1, "path_ans_recall": 0.0035087719298245615}
{"id": "WebQTest-492", "prediction": ["# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> travel.travel_destination.tourist_attractions -> Ear of Dionysius\n# Answer:\nSyracuse", "# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> location.location.containedby -> Sicily\n# Answer:\nSyracuse", "# Reasoning Path:\nSaint Lucy -> common.topic.notable_types -> Deceased Person -> type.type.properties -> Cause of death\n# Answer:\nDeceased Person", "# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> travel.travel_destination.tourist_attractions -> Ancient theatre of Taormina\n# Answer:\nSyracuse", "# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> location.location.containedby -> Italy\n# Answer:\nSyracuse", "# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> travel.travel_destination.tourist_attractions -> Arethuse\n# Answer:\nSyracuse", "# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> location.location.events -> Action of 26 June 1625\n# Answer:\nSyracuse", "# Reasoning Path:\nSaint Lucy -> common.topic.notable_types -> Deceased Person -> type.type.properties -> Date of burial\n# Answer:\nDeceased Person", "# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> location.location.containedby -> Province of Syracuse\n# Answer:\nSyracuse", "# Reasoning Path:\nSaint Lucy -> common.topic.notable_types -> Deceased Person -> type.type.properties -> Date of cremation\n# Answer:\nDeceased Person"], "ground_truth": ["Syracuse"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-493", "prediction": ["# Reasoning Path:\nAustralia -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\ng.11b60qdnlp", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Canada\n# Answer:\nParliamentary system", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\ng.11b60vfr15", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Federal monarchy -> government.form_of_government.countries -> Canada\n# Answer:\nFederal monarchy", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\ng.1245_2hrg", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Japan\n# Answer:\nParliamentary system", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_55mr\n# Answer:\ng.1245_55mr", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_2hqk\n# Answer:\ng.1245_2hqk", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> New Caledonia\n# Answer:\nParliamentary system"], "ground_truth": ["Constitutional monarchy", "Federal monarchy", "Parliamentary system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-497", "prediction": ["# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> people.person.parents -> Edward VII\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> people.person.parents -> Alexandra of Denmark\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> people.deceased_person.cause_of_death -> Euthanasia\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> Mary of Teck -> people.person.parents -> Francis, Duke of Teck\n# Answer:\nMary of Teck", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> freebase.valuenotation.is_reviewed -> Children\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> Mary of Teck -> people.person.parents -> Princess Mary Adelaide of Cambridge\n# Answer:\nMary of Teck", "# Reasoning Path:\nGeorge VI -> people.person.parents -> Mary of Teck -> music.artist.track -> A Message to the Children of the Empire\n# Answer:\nMary of Teck", "# Reasoning Path:\nGeorge VI -> royalty.noble_person.titles -> m.04_b_2d -> royalty.noble_title_tenure.noble_title -> Duke of York\n# Answer:\nm.04_b_2d", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nGeorge V"], "ground_truth": ["George V"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-500", "prediction": ["# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08691 -> location.location.containedby -> Mercer County\n# Answer:\n08691", "# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08691 -> location.location.contains -> Trenton\u2013Robbinsville Airport\n# Answer:\n08691", "# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08690 -> common.topic.notable_types -> Postal Code\n# Answer:\n08690", "# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08691 -> common.topic.notable_for -> g.125dz6_5q\n# Answer:\n08691", "# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08601 -> location.postal_code.country -> United States of America\n# Answer:\n08601", "# Reasoning Path:\nTrenton -> location.statistical_region.population -> g.11b66l5kmz\n# Answer:\ng.11b66l5kmz", "# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08690 -> location.location.containedby -> Mercer County\n# Answer:\n08690", "# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08690 -> common.topic.notable_for -> g.1257fk1dk\n# Answer:\n08690", "# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08601 -> common.topic.notable_types -> Postal Code\n# Answer:\n08601", "# Reasoning Path:\nTrenton -> location.statistical_region.population -> g.11bymn745l\n# Answer:\ng.11bymn745l"], "ground_truth": ["08629", "08618", "08607", "08619", "08605", "08625", "08606", "08628", "08650", "08620", "08608", "08602", "08641", "08601", "08611", "08638", "08691", "08604", "08690", "08610", "08645", "08646", "08603", "08647", "08666", "08695", "08609"], "ans_acc": 0.1111111111111111, "ans_hit": 1, "ans_f1": 0.19512195121951217, "ans_precission": 0.8, "ans_recall": 0.1111111111111111, "path_f1": 0.19512195121951217, "path_precision": 0.8, "path_recall": 0.1111111111111111, "path_ans_f1": 0.19512195121951217, "path_ans_precision": 0.8, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-501", "prediction": ["# Reasoning Path:\nAshley Greene -> film.actor.film -> m.0gbwhrn -> film.performance.film -> Shrink\n# Answer:\nm.0gbwhrn", "# Reasoning Path:\nAshley Greene -> film.actor.film -> m.05kdryk -> film.performance.film -> Radio Free Albemuth\n# Answer:\nm.05kdryk", "# Reasoning Path:\nAshley Greene -> film.actor.film -> m.0gbwhs7 -> film.performance.film -> Otis\n# Answer:\nm.0gbwhs7", "# Reasoning Path:\nAshley Greene -> film.actor.film -> m.0gbwhrn -> film.performance.character -> Missy\n# Answer:\nm.0gbwhrn", "# Reasoning Path:\nAshley Greene -> film.actor.film -> m.05kdryk -> film.performance.character -> Rhonda\n# Answer:\nm.05kdryk", "# Reasoning Path:\nAshley Greene -> people.person.places_lived -> m.05hv4jz -> people.place_lived.location -> Los Angeles\n# Answer:\nm.05hv4jz", "# Reasoning Path:\nAshley Greene -> film.actor.film -> m.0gbwhs7 -> film.performance.character -> Kim #4\n# Answer:\nm.0gbwhs7", "# Reasoning Path:\nAshley Greene -> people.person.places_lived -> m.05k64xr -> people.place_lived.location -> Middleburg\n# Answer:\nm.05k64xr", "# Reasoning Path:\nAshley Greene -> common.topic.image -> 390px-AshleyGreene.jpg -> common.image.size -> m.03sw_5s\n# Answer:\n390px-AshleyGreene.jpg", "# Reasoning Path:\nAshley Greene -> people.person.places_lived -> m.05hv4wx -> people.place_lived.location -> Jacksonville\n# Answer:\nm.05hv4wx"], "ground_truth": ["The Apparition", "Skateland", "Urge", "Burying the Ex", "The Twilight Saga: Breaking Dawn - Part 1", "Americana", "The Twilight Saga: New Moon", "The Twilight Saga: Breaking Dawn - Part 2", "The Boom Boom Room", "Otis", "Butter", "LOL", "Eclipse", "Radio Free Albemuth", "Twilight", "King of California", "A Warrior's Heart", "Staten Island Summer", "Wish I Was Here", "Random", "Shrink", "Summer's Blood", "CBGB"], "ans_acc": 0.13043478260869565, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14634146341463414, "path_precision": 0.3, "path_recall": 0.0967741935483871, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.3, "path_ans_recall": 0.13043478260869565}
{"id": "WebQTest-502", "prediction": ["# Reasoning Path:\nAl-Qaeda -> film.film_subject.films -> Restrepo -> film.film.subjects -> Taliban insurgency\n# Answer:\nRestrepo", "# Reasoning Path:\nAl-Qaeda -> base.terrorism.terrorist_organization.involved_in_attacks -> September 11 attacks -> base.schemastaging.context_name.pronunciation -> g.125_njzfs\n# Answer:\nSeptember 11 attacks", "# Reasoning Path:\nAl-Qaeda -> base.terrorism.terrorist_organization.involved_in_attacks -> September 11 attacks -> base.crime.crime.persons_accused -> m.05bnm31\n# Answer:\nSeptember 11 attacks", "# Reasoning Path:\nAl-Qaeda -> film.film_subject.films -> Restrepo -> film.film.subjects -> Juan Restrepo\n# Answer:\nRestrepo", "# Reasoning Path:\nAl-Qaeda -> base.terrorism.terrorist_organization.involved_in_attacks -> September 11 attacks -> film.film_subject.films -> United 93\n# Answer:\nSeptember 11 attacks", "# Reasoning Path:\nAl-Qaeda -> base.terrorism.terrorist_organization.involved_in_attacks -> September 11 attacks -> film.film_subject.films -> 11'09\\\"01 September 11\n# Answer:\n11'09\\\"01 September 11", "# Reasoning Path:\nAl-Qaeda -> film.film_subject.films -> Restrepo -> film.film.subjects -> War in Afghanistan\n# Answer:\nRestrepo", "# Reasoning Path:\nAl-Qaeda -> film.film_subject.films -> Restrepo -> film.film.release_date_s -> m.0gl84h1\n# Answer:\nRestrepo", "# Reasoning Path:\nAl-Qaeda -> base.terrorism.terrorist_organization.involved_in_attacks -> September 11 attacks -> film.film_subject.films -> 9/11: The Falling Man\n# Answer:\nSeptember 11 attacks", "# Reasoning Path:\nAl-Qaeda -> film.film_subject.films -> Restrepo -> media_common.netflix_title.netflix_genres -> Documentary film\n# Answer:\nRestrepo"], "ground_truth": ["Taliban insurgency", "September 11 attacks"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.4, "ans_recall": 0.5, "path_f1": 0.5454545454545454, "path_precision": 0.6, "path_recall": 0.5, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-503", "prediction": ["# Reasoning Path:\nSwitzerland -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> Belgium\n# Answer:\nFrench", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> French -> language.human_language.main_country -> France\n# Answer:\nFrench", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> Italian Language -> language.human_language.main_country -> Italy\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> Burkina Faso\n# Answer:\nFrench", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> German Language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nGerman Language", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d0tc3g\n# Answer:\ng.11b6d0tc3g", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> German Language -> language.human_language.region -> Europe\n# Answer:\nGerman Language", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> French -> language.human_language.countries_spoken_in -> Belgium\n# Answer:\nFrench"], "ground_truth": ["German Language", "Italian Language", "Romansh language", "French"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.5454545454545454, "path_precision": 0.6, "path_recall": 0.5, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-504", "prediction": ["# Reasoning Path:\nPhilip the Apostle -> people.deceased_person.cause_of_death -> Crucifixion -> people.cause_of_death.people -> Jesus Christ\n# Answer:\nCrucifixion", "# Reasoning Path:\nPhilip the Apostle -> people.deceased_person.cause_of_death -> Crucifixion -> people.cause_of_death.people -> Andrew the Apostle\n# Answer:\nCrucifixion", "# Reasoning Path:\nPhilip the Apostle -> people.deceased_person.cause_of_death -> Crucifixion -> people.cause_of_death.people -> Artayctes\n# Answer:\nCrucifixion", "# Reasoning Path:\nPhilip the Apostle -> people.deceased_person.cause_of_death -> Crucifixion -> base.ontologies.ontology_instance.equivalent_instances -> m.09klpz1\n# Answer:\nCrucifixion", "# Reasoning Path:\nPhilip the Apostle -> people.deceased_person.cause_of_death -> Crucifixion -> book.book_subject.works -> Adding Cross to crown\n# Answer:\nCrucifixion", "# Reasoning Path:\nPhilip the Apostle -> symbols.name_source.namesakes -> St Philip's Cathedral, Birmingham -> common.topic.notable_for -> g.1255b7f1h\n# Answer:\nSt Philip's Cathedral, Birmingham", "# Reasoning Path:\nPhilip the Apostle -> base.saints.saint.venerated_in -> Christianity -> religion.religion.deities -> Jesus Christ\n# Answer:\nChristianity", "# Reasoning Path:\nPhilip the Apostle -> base.saints.saint.venerated_in -> Islam -> religion.religion.deities -> God\n# Answer:\nIslam", "# Reasoning Path:\nPhilip the Apostle -> people.deceased_person.cause_of_death -> Crucifixion -> book.book_subject.works -> Cross\n# Answer:\nCrucifixion", "# Reasoning Path:\nPhilip the Apostle -> symbols.name_source.namesakes -> St Philip's Church, Alderley Edge -> common.topic.webpage -> m.07sp4sy\n# Answer:\nSt Philip's Church, Alderley Edge"], "ground_truth": ["Apostle Philip"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.0, "path_ans_precision": 0.0, "path_ans_recall": 1.0}
{"id": "WebQTest-506", "prediction": ["# Reasoning Path:\nEngland -> location.location.time_zones -> Greenwich Mean Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nGreenwich Mean Time Zone", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0hq0kmy\n# Answer:\nm.0hq0kmy", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0mv3kv1\n# Answer:\nm.0mv3kv1", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0njwhvf\n# Answer:\nm.0njwhvf", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Airedale Terrier -> biology.animal_breed.place_of_origin -> United Kingdom\n# Answer:\nAiredale Terrier", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Airedale Terrier -> common.topic.notable_types -> Animal breed\n# Answer:\nAiredale Terrier", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Airedale Terrier -> base.petbreeds.dog_breed.temperament -> Alert\n# Answer:\nAiredale Terrier", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> American Pit Bull Terrier -> biology.animal_breed.coloring -> Fawn\n# Answer:\nAmerican Pit Bull Terrier", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Bandog -> base.petbreeds.dog_breed.color -> Fawn\n# Answer:\nBandog", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Bandog -> biology.animal_breed.breed_of -> Dog\n# Answer:\nBandog"], "ground_truth": ["Greenwich Mean Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-508", "prediction": ["# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Revaluation of the Turkish Lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nRevaluation of the Turkish Lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.065sxsl -> military.military_combatant_group.conflict -> Korean War\n# Answer:\nm.065sxsl", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.066402y -> military.military_combatant_group.conflict -> Iraqi no-fly zones\n# Answer:\nm.066402y"], "ground_truth": ["Turkish lira"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-509", "prediction": ["# Reasoning Path:\nNigeria -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Sudan\n# Answer:\nFederal republic", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_0sgz\n# Answer:\ng.1245_0sgz", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> United States of America\n# Answer:\nFederal republic", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Chad\n# Answer:\nPresidential system", "# Reasoning Path:\nNigeria -> location.statistical_region.external_debt_stock -> g.11b71rqxzt\n# Answer:\ng.11b71rqxzt", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_22x5\n# Answer:\ng.1245_22x5", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guinea\n# Answer:\nPresidential system", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_2gxr\n# Answer:\ng.1245_2gxr", "# Reasoning Path:\nNigeria -> location.statistical_region.external_debt_stock -> g.1hhc38d1f\n# Answer:\ng.1hhc38d1f", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Kenya\n# Answer:\nPresidential system"], "ground_truth": ["Presidential system", "Federal republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.actor -> Jamie Dornan\n# Answer:\nm.0ydn3r2", "# Reasoning Path:\nChristian Grey -> common.topic.notable_for -> g.125czvn3w\n# Answer:\ng.125czvn3w", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.film -> Fifty Shades of Grey\n# Answer:\nm.0ydn3r2", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Males\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> common.topic.notable_for -> g.1255p_jgc\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Acute myeloid leukemia\n# Answer:\nMale"], "ground_truth": ["Jamie Dornan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-512", "prediction": ["# Reasoning Path:\nMary Shelley -> people.person.profession -> Biographer -> people.profession.specialization_of -> Writer\n# Answer:\nBiographer", "# Reasoning Path:\nMary Shelley -> people.person.profession -> Biographer -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nBiographer", "# Reasoning Path:\nMary Shelley -> people.person.profession -> Editor -> people.profession.corresponding_type -> Author\n# Answer:\nEditor", "# Reasoning Path:\nMary Shelley -> people.person.profession -> Biographer -> common.topic.notable_for -> g.1259rk51h\n# Answer:\nBiographer", "# Reasoning Path:\nMary Shelley -> people.person.profession -> Editor -> common.topic.notable_types -> Profession\n# Answer:\nEditor", "# Reasoning Path:\nMary Shelley -> people.person.profession -> Editor -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nEditor", "# Reasoning Path:\nMary Shelley -> people.person.profession -> Travel writer -> people.profession.people_with_this_profession -> Andrew McCarthy\n# Answer:\nTravel writer", "# Reasoning Path:\nMary Shelley -> people.person.profession -> Travel writer -> people.profession.specialization_of -> Writer\n# Answer:\nTravel writer", "# Reasoning Path:\nMary Shelley -> influence.influence_node.influenced_by -> Jean-Jacques Rousseau -> people.person.profession -> Writer\n# Answer:\nJean-Jacques Rousseau", "# Reasoning Path:\nMary Shelley -> people.person.profession -> Travel writer -> common.topic.notable_types -> Profession\n# Answer:\nTravel writer"], "ground_truth": ["Playwright", "Travel writer", "Writer", "Author", "Novelist", "Biographer", "Essayist", "Editor"], "ans_acc": 0.625, "ans_hit": 1, "ans_f1": 0.6428571428571429, "ans_precission": 0.9, "ans_recall": 0.5, "path_f1": 0.5294117647058825, "path_precision": 0.9, "path_recall": 0.375, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 1.0, "path_ans_recall": 0.625}
{"id": "WebQTest-513", "prediction": ["# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.main_country -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> New Zealand\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\ng.11b60qdnlp", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\ng.11b60vfr15", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> Japan\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Indonesia\n# Answer:\nEnglish Language"], "ground_truth": ["Lojban", "English Language", "Esperanto Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-514", "prediction": ["# Reasoning Path:\nDenmark -> location.location.containedby -> Eurasia -> base.locations.continents.countries_within -> France\n# Answer:\nEurasia", "# Reasoning Path:\nDenmark -> location.location.containedby -> Eurasia -> base.locations.continents.countries_within -> Turkey\n# Answer:\nEurasia", "# Reasoning Path:\nDenmark -> location.location.containedby -> Eurasia -> location.location.events -> World War II\n# Answer:\nEurasia", "# Reasoning Path:\nDenmark -> location.location.containedby -> Eurasia -> common.topic.notable_types -> Location\n# Answer:\nEurasia", "# Reasoning Path:\nDenmark -> periodicals.newspaper_circulation_area.newspapers -> Dagbladet B\u00f8rsen -> book.newspaper.circulation_areas -> Copenhagen\n# Answer:\nDagbladet B\u00f8rsen", "# Reasoning Path:\nDenmark -> location.statistical_region.gender_balance_members_of_parliament -> g.11b71wx6lz\n# Answer:\ng.11b71wx6lz", "# Reasoning Path:\nDenmark -> location.statistical_region.gender_balance_members_of_parliament -> g.1hhc38d4d\n# Answer:\ng.1hhc38d4d", "# Reasoning Path:\nDenmark -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Austria\n# Answer:\nEurope", "# Reasoning Path:\nDenmark -> location.statistical_region.gender_balance_members_of_parliament -> g.1hhc3gt9z\n# Answer:\ng.1hhc3gt9z", "# Reasoning Path:\nDenmark -> location.location.containedby -> Kingdom of Denmark -> location.location.partially_containedby -> Europe\n# Answer:\nKingdom of Denmark"], "ground_truth": ["Nordic countries", "Scandinavia"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-515", "prediction": ["# Reasoning Path:\nTheodore Roosevelt -> people.person.places_lived -> m.0108qyxb -> people.place_lived.location -> Washington, D.C.\n# Answer:\nm.0108qyxb", "# Reasoning Path:\nTheodore Roosevelt -> people.person.places_lived -> m.04hxhjj -> people.place_lived.location -> New York City\n# Answer:\nm.04hxhjj", "# Reasoning Path:\nTheodore Roosevelt -> people.person.children -> Archibald Roosevelt -> people.person.children -> Archibald Bulloch Roosevelt Jr.\n# Answer:\nArchibald Roosevelt", "# Reasoning Path:\nTheodore Roosevelt -> symbols.name_source.namesakes -> Roosevelt -> location.location.containedby -> Washington\n# Answer:\nRoosevelt", "# Reasoning Path:\nTheodore Roosevelt -> people.person.children -> Archibald Roosevelt -> people.person.places_lived -> m.03pq5fw\n# Answer:\nArchibald Roosevelt", "# Reasoning Path:\nTheodore Roosevelt -> people.person.children -> Alice Roosevelt Longworth -> people.person.places_lived -> m.04ysvz_\n# Answer:\nAlice Roosevelt Longworth", "# Reasoning Path:\nTheodore Roosevelt -> people.person.children -> Ethel Roosevelt Derby -> people.person.place_of_birth -> Oyster Bay\n# Answer:\nEthel Roosevelt Derby", "# Reasoning Path:\nTheodore Roosevelt -> people.person.children -> Alice Roosevelt Longworth -> people.person.place_of_birth -> New York City\n# Answer:\nAlice Roosevelt Longworth", "# Reasoning Path:\nTheodore Roosevelt -> symbols.name_source.namesakes -> Roosevelt -> common.topic.article -> m.02gt_g\n# Answer:\nRoosevelt", "# Reasoning Path:\nTheodore Roosevelt -> people.person.children -> Archibald Roosevelt -> people.person.parents -> Edith Roosevelt\n# Answer:\nArchibald Roosevelt"], "ground_truth": ["New York City", "Washington, D.C."], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.21428571428571427, "path_precision": 0.3, "path_recall": 0.16666666666666666, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-516", "prediction": ["# Reasoning Path:\nMalaysia -> location.country.form_of_government -> Constitutional monarchy -> common.topic.notable_types -> Form of Government\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nMalaysia -> location.country.form_of_government -> Elective monarchy -> common.topic.notable_types -> Form of Government\n# Answer:\nElective monarchy", "# Reasoning Path:\nMalaysia -> location.statistical_region.size_of_armed_forces -> g.12cp_jm8t\n# Answer:\ng.12cp_jm8t", "# Reasoning Path:\nMalaysia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Australia\n# Answer:\nParliamentary system", "# Reasoning Path:\nMalaysia -> location.country.form_of_government -> Parliamentary system -> common.topic.notable_types -> Form of Government\n# Answer:\nParliamentary system", "# Reasoning Path:\nMalaysia -> location.statistical_region.gdp_nominal_per_capita -> g.11b60thpy3\n# Answer:\ng.11b60thpy3", "# Reasoning Path:\nMalaysia -> location.country.form_of_government -> Elective monarchy -> government.form_of_government.countries -> United Arab Emirates\n# Answer:\nElective monarchy", "# Reasoning Path:\nMalaysia -> location.statistical_region.size_of_armed_forces -> g.1hhc37x7v\n# Answer:\ng.1hhc37x7v", "# Reasoning Path:\nMalaysia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Cambodia\n# Answer:\nParliamentary system", "# Reasoning Path:\nMalaysia -> location.statistical_region.size_of_armed_forces -> g.1hhc39wxf\n# Answer:\ng.1hhc39wxf"], "ground_truth": ["Elective monarchy", "Constitutional monarchy", "Democracy", "Parliamentary system"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.6666666666666665, "ans_precission": 0.6, "ans_recall": 0.75, "path_f1": 0.6666666666666665, "path_precision": 0.6, "path_recall": 0.75, "path_ans_f1": 0.6666666666666665, "path_ans_precision": 0.6, "path_ans_recall": 0.75}
{"id": "WebQTest-517", "prediction": ["# Reasoning Path:\nCanada -> location.country.languages_spoken -> Abenaki language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nAbenaki language", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> English Language -> language.human_language.dialects -> Canadian English\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nLojban", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> Abenaki language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nAbenaki language", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.12cp_k6ss\n# Answer:\ng.12cp_k6ss", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> Lojban -> common.topic.notable_types -> Human Language\n# Answer:\nLojban", "# Reasoning Path:\nCanada -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.12tb6g_94\n# Answer:\ng.12tb6g_94", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> Lojban -> language.human_language.region -> Americas\n# Answer:\nLojban", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language"], "ground_truth": ["Korean Language", "Romanian Language", "Arabic Language", "Portuguese Language", "Punjabi language", "Inuinnaqtun", "Greek Language", "Gwich'in Language", "Tamil Language", "Abenaki language", "Vietnamese Language", "Spanish Language", "Polish Language", "Persian Language", "Gujarati Language", "Canadian English", "Urdu Language", "Inuktitut", "English Language", "Slavey language", "French", "Dogrib Language", "Tagalog language", "German Language", "Chipewyan Language", "Cree language", "Chinese language", "Russian Language", "Lojban", "Upper Midwest American English", "Western Canadian Inuktitut Language"], "ans_acc": 0.12903225806451613, "ans_hit": 1, "ans_f1": 0.1726618705035971, "ans_precission": 0.8, "ans_recall": 0.0967741935483871, "path_f1": 0.1726618705035971, "path_precision": 0.8, "path_recall": 0.0967741935483871, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.8, "path_ans_recall": 0.12903225806451613}
{"id": "WebQTest-518", "prediction": ["# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.image -> 10colonesbcrfront\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.webpage -> m.04m2m2k\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60wmkvh\n# Answer:\ng.11b60wmkvh", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.11b71w3_6h\n# Answer:\ng.11b71w3_6h", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_5xgy\n# Answer:\ng.1245_5xgy", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_6n42\n# Answer:\ng.1245_6n42", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.1hhc37pjz\n# Answer:\ng.1hhc37pjz", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.1hhc37x5q\n# Answer:\ng.1hhc37x5q"], "ground_truth": ["Costa Rican col\u00f3n"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-519", "prediction": ["# Reasoning Path:\nNashville -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nNashville -> location.location.time_zones -> Central Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_mmrcv\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nNashville -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nNashville -> common.topic.webpage -> m.03l8rmb -> common.webpage.resource -> m.0bkjqzl\n# Answer:\nm.03l8rmb", "# Reasoning Path:\nNashville -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nNashville -> location.location.events -> 2007 Fylmz Festival -> time.event.instance_of_recurring_event -> Fylmz Festival\n# Answer:\n2007 Fylmz Festival", "# Reasoning Path:\nNashville -> common.topic.webpage -> m.03l8rmb -> common.webpage.category -> Official Website\n# Answer:\nm.03l8rmb", "# Reasoning Path:\nNashville -> common.topic.webpage -> m.09w5qrw -> common.webpage.resource -> Jessica Simpson goes a little bit country\n# Answer:\nm.09w5qrw", "# Reasoning Path:\nNashville -> location.location.events -> 2007 Fylmz Festival -> film.film_festival_event.festival -> Fylmz Festival\n# Answer:\n2007 Fylmz Festival", "# Reasoning Path:\nNashville -> common.topic.webpage -> m.09w5qrw -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w5qrw"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Anton Chekhov\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> people.cause_of_death.parent_cause_of_death -> Infectious disease\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Albert Camus\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Antibiotics\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Franz Kafka\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Isoniazid\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion. -> common.topic.notable_types -> Quotation\n# Answer:\nA dirty joke is a sort of mental rebellion.", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Pyrazinamide\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Fyodor Dostoyevsky -> people.deceased_person.cause_of_death -> Epilepsy\n# Answer:\nFyodor Dostoyevsky", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion. -> common.topic.notable_for -> g.12599cgh4\n# Answer:\nA dirty joke is a sort of mental rebellion."], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-521", "prediction": ["# Reasoning Path:\ng.125_cxx77\n# Answer:\ng.125_cxx77"], "ground_truth": ["Ted Bracewell"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-522", "prediction": ["# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> base.aareas.schema.administrative_area.administrative_parent -> Germany\n# Answer:\nBerlin", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ng.1254zbtvv", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> film.film_location.featured_in_films -> Inglourious Basterds\n# Answer:\nBerlin", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> film.film_location.featured_in_films -> Valkyrie\n# Answer:\nBerlin", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09wf140", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.resource -> Another Megan Fox letter -- and this one sings her praises!\n# Answer:\nm.09wf140", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wgn1j -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09wgn1j", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wgn1j -> common.webpage.resource -> Charles Manson: 40 years later, the movie about him you have to see\n# Answer:\nm.09wgn1j", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wqfb3 -> common.webpage.resource -> Erykah Badu teases 'New Amerykah Part Two' with trippy, funky clip\n# Answer:\nm.09wqfb3", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wqfb3 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09wqfb3"], "ground_truth": ["Berlin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-523", "prediction": ["# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Russia -> location.location.partially_contains -> Caspian Sea\n# Answer:\nRussia", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Russia -> location.location.partially_contained_by -> m.0wg8l4g\n# Answer:\nRussia", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Russia -> location.location.partially_contains -> Caucasus Mountains\n# Answer:\nRussia", "# Reasoning Path:\nEurope -> location.location.partially_contains -> Russia -> location.location.partially_contains -> Caspian Sea\n# Answer:\nRussia", "# Reasoning Path:\nEurope -> location.location.partially_contains -> Russia -> location.location.partially_contained_by -> m.0wg8l4g\n# Answer:\nRussia", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Russia -> location.location.partially_contains -> Diklosmta\n# Answer:\nRussia", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Russia -> location.country.administrative_divisions -> Bashkortostan\n# Answer:\nRussia", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Albania -> location.location.containedby -> Eurasia\n# Answer:\nAlbania", "# Reasoning Path:\nEurope -> location.location.partially_contains -> Russia -> location.location.partially_contains -> Caucasus Mountains\n# Answer:\nRussia", "# Reasoning Path:\nEurope -> location.location.partially_contains -> Soviet Union -> location.location.partially_containedby -> Asia\n# Answer:\nSoviet Union"], "ground_truth": ["Russia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-524", "prediction": ["# Reasoning Path:\nGulf War -> time.event.locations -> Arabian Peninsula -> location.location.contains -> Saudi Arabia\n# Answer:\nArabian Peninsula", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> royalty.kingdom.rulers -> Abdullah of Saudi Arabia\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.locations -> Arabian Peninsula -> location.location.contains -> Kuwait\n# Answer:\nArabian Peninsula", "# Reasoning Path:\nGulf War -> time.event.locations -> Arabian Peninsula -> location.location.people_born_here -> Ka'b bin Zuhayr\n# Answer:\nArabian Peninsula", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> location.country.official_language -> Arabic Language\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.includes_event -> 1991 uprisings in Iraq -> time.event.locations -> Iraq\n# Answer:\n1991 uprisings in Iraq", "# Reasoning Path:\nGulf War -> time.event.locations -> Arabian Peninsula -> location.location.contains -> Bahrain\n# Answer:\nArabian Peninsula", "# Reasoning Path:\nGulf War -> time.event.locations -> Arabian Peninsula -> location.location.events -> Arabian Peninsula Massacre\n# Answer:\nArabian Peninsula", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> base.locations.countries.continent -> Asia\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.locations -> Kuwait -> location.location.events -> Air engagements of the Gulf War\n# Answer:\nKuwait"], "ground_truth": ["Saudi Arabia", "Kuwait", "Arabian Peninsula", "Israel", "Persian Gulf", "Iraq"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 1.0, "ans_recall": 0.6666666666666666, "path_f1": 0.6428571428571429, "path_precision": 0.9, "path_recall": 0.5, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-525", "prediction": ["# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> government.government_position_held.office_holder -> Tim Kaine\n# Answer:\nm.0r99pcd", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcw0 -> government.government_position_held.office_holder -> Jim Webb\n# Answer:\nm.05kfcw0", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcm1 -> government.government_position_held.office_holder -> Mark Warner\n# Answer:\nm.05kfcm1", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0r99pcd", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nm.0r99pcd", "# Reasoning Path:\nVirginia -> location.location.containedby -> Charles Irving Thornton Tombstone -> location.location.geolocation -> m.0zjvs72\n# Answer:\nCharles Irving Thornton Tombstone", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcm1 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.05kfcm1", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcw0 -> government.government_position_held.legislative_sessions -> 110th United States Congress\n# Answer:\nm.05kfcw0", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcm1 -> government.government_position_held.legislative_sessions -> 111th United States Congress\n# Answer:\nm.05kfcm1", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcw0 -> government.government_position_held.legislative_sessions -> 111th United States Congress\n# Answer:\nm.05kfcw0"], "ground_truth": ["Jim Webb", "Mark Warner", "Tim Kaine"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-526", "prediction": ["# Reasoning Path:\nGeorge Clinton -> music.featured_artist.recordings -> Mothership Reconnection (Daft Punk remix) -> music.recording.artist -> Funkadellic\n# Answer:\nMothership Reconnection (Daft Punk remix)", "# Reasoning Path:\nGeorge Clinton -> music.featured_artist.recordings -> Mothership Reconnection (Daft Punk remix) -> music.recording.featured_artists -> Funkadellic\n# Answer:\nMothership Reconnection (Daft Punk remix)", "# Reasoning Path:\nGeorge Clinton -> music.featured_artist.recordings -> Black People -> music.recording.artist -> Funkadellic\n# Answer:\nBlack People", "# Reasoning Path:\nGeorge Clinton -> music.featured_artist.recordings -> Mothership Reconnection (Daft Punk remix) -> music.recording.artist -> Scott Grooves\n# Answer:\nMothership Reconnection (Daft Punk remix)", "# Reasoning Path:\nGeorge Clinton -> music.featured_artist.recordings -> Mothership Reconnection (Daft Punk remix) -> common.topic.notable_types -> Musical Recording\n# Answer:\nMothership Reconnection (Daft Punk remix)", "# Reasoning Path:\nGeorge Clinton -> music.featured_artist.recordings -> All I Came to Do -> music.recording.artist -> Messy Marv\n# Answer:\nAll I Came to Do", "# Reasoning Path:\nGeorge Clinton -> music.producer.tracks_produced -> Funkentelechy -> music.recording.artist -> Parliament\n# Answer:\nFunkentelechy", "# Reasoning Path:\nGeorge Clinton -> music.featured_artist.recordings -> Mothership Reconnection (Daft Punk remix) -> music.recording.featured_artists -> Parliament\n# Answer:\nMothership Reconnection (Daft Punk remix)", "# Reasoning Path:\nGeorge Clinton -> music.artist.album -> George Clinton Family Series, Part 1: Go Fer Yer Funk -> music.album.artist -> Funkadellic\n# Answer:\nGeorge Clinton Family Series, Part 1: Go Fer Yer Funk", "# Reasoning Path:\nGeorge Clinton -> music.featured_artist.recordings -> Black People -> common.topic.notable_for -> g.125ffndps\n# Answer:\nBlack People"], "ground_truth": ["Parliament-Funkadelic", "Dolby's Cube", "Funkadellic", "The Parliaments", "Parliament", "George Clinton & Bootsy Collins"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.10752688172043011, "path_precision": 0.5, "path_recall": 0.060240963855421686, "path_ans_f1": 0.42857142857142855, "path_ans_precision": 0.6, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-527", "prediction": ["# Reasoning Path:\nSlovakia -> location.location.partially_contains -> Bieszczady Mountains -> location.location.partially_containedby -> Poland\n# Answer:\nBieszczady Mountains", "# Reasoning Path:\nSlovakia -> location.location.partially_contains -> Bieszczady Mountains -> geography.mountain_range.mountains -> Tarnica\n# Answer:\nBieszczady Mountains", "# Reasoning Path:\nSlovakia -> location.location.partially_contains -> Bieszczady Mountains -> location.location.partially_contained_by -> m.0wg8tgd\n# Answer:\nBieszczady Mountains", "# Reasoning Path:\nSlovakia -> location.location.partially_contains -> Bieszczady Mountains -> location.location.partially_containedby -> Ukraine\n# Answer:\nBieszczady Mountains", "# Reasoning Path:\nSlovakia -> location.location.partially_contains -> Babia G\u00f3ra -> location.location.partially_containedby -> Poland\n# Answer:\nBabia G\u00f3ra", "# Reasoning Path:\nSlovakia -> location.statistical_region.co2_emissions_per_capita -> g.1245_9wdf\n# Answer:\ng.1245_9wdf", "# Reasoning Path:\nSlovakia -> location.location.partially_contains -> Beskids -> location.location.partially_containedby -> Poland\n# Answer:\nBeskids", "# Reasoning Path:\nSlovakia -> location.statistical_region.co2_emissions_per_capita -> g.1245_fzcr\n# Answer:\ng.1245_fzcr", "# Reasoning Path:\nSlovakia -> location.location.containedby -> Eurasia -> base.locations.continents.countries_within -> Cyprus\n# Answer:\nEurasia", "# Reasoning Path:\nSlovakia -> location.location.containedby -> Eurasia -> location.location.events -> World War II\n# Answer:\nEurasia"], "ground_truth": ["Czech Republic", "Hungary", "Poland", "Ukraine", "Austria"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.06504065040650406, "path_precision": 0.4, "path_recall": 0.035398230088495575, "path_ans_f1": 0.4000000000000001, "path_ans_precision": 0.4, "path_ans_recall": 0.4}
{"id": "WebQTest-528", "prediction": ["# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Lawyer -> common.topic.subjects -> Sally & Fitch LLP\n# Answer:\nLawyer", "# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Lawyer -> people.profession.specializations -> Criminal defense lawyer\n# Answer:\nLawyer", "# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Lawyer -> common.topic.subjects -> Accident Advice Helpline\n# Answer:\nLawyer", "# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Lawyer -> common.topic.subject_of -> Accident Advice Helpline\n# Answer:\nLawyer", "# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Lawyer -> common.topic.subjects -> Brian White & Associates, P.C.\n# Answer:\nLawyer", "# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Laity -> people.profession.specializations -> Minister\n# Answer:\nLaity", "# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Lawyer -> common.topic.subject_of -> Adam Greenman Law\n# Answer:\nLawyer", "# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdn_\n# Answer:\nPolitician", "# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Laity -> common.topic.article -> m.01zmq7\n# Answer:\nLaity", "# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Lawyer -> common.topic.subject_of -> Axess Law\n# Answer:\nLawyer"], "ground_truth": ["US President"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-529", "prediction": ["# Reasoning Path:\nPaula Deen -> people.person.education -> m.0n1jznt -> education.education.institution -> Albany High School\n# Answer:\nm.0n1jznt", "# Reasoning Path:\nPaula Deen -> base.schemastaging.person_extra.net_worth -> m.0j8mg8t -> measurement_unit.dated_money_value.source -> celebritynetworth.com\n# Answer:\nm.0j8mg8t", "# Reasoning Path:\nPaula Deen -> people.person.parents -> Corrie A. Paul -> common.topic.notable_types -> Deceased Person\n# Answer:\nCorrie A. Paul", "# Reasoning Path:\nPaula Deen -> base.schemastaging.person_extra.net_worth -> m.0j8mg8t -> measurement_unit.dated_money_value.currency -> United States Dollar\n# Answer:\nm.0j8mg8t", "# Reasoning Path:\nPaula Deen -> people.person.parents -> Corrie A. Paul -> common.topic.notable_for -> g.125d2hh2z\n# Answer:\nCorrie A. Paul", "# Reasoning Path:\nPaula Deen -> people.person.parents -> Earl Wayne Hiers, Sr -> people.person.gender -> Male\n# Answer:\nEarl Wayne Hiers, Sr", "# Reasoning Path:\nPaula Deen -> people.person.parents -> Earl Wayne Hiers, Sr -> common.topic.notable_types -> Deceased Person\n# Answer:\nEarl Wayne Hiers, Sr"], "ground_truth": ["Albany High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-530", "prediction": ["# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> location.location.containedby -> England\n# Answer:\nSalisbury", "# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> location.location.containedby -> Wiltshire\n# Answer:\nSalisbury", "# Reasoning Path:\nRobert Irvine -> common.topic.article -> m.0272y30\n# Answer:\nm.0272y30", "# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> travel.travel_destination.tourist_attractions -> Fisherton Mill\n# Answer:\nSalisbury", "# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> location.location.containedby -> United Kingdom\n# Answer:\nSalisbury", "# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> common.topic.image -> Aston, Flintshire (United Kingdom)\n# Answer:\nSalisbury", "# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> travel.travel_destination.tourist_attractions -> Guildhall Salisbury\n# Answer:\nSalisbury", "# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> travel.travel_destination.tourist_attractions -> Mompesson House\n# Answer:\nSalisbury", "# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> common.topic.image -> Salisbury Cathedral\n# Answer:\nSalisbury", "# Reasoning Path:\nRobert Irvine -> people.person.nationality -> United Kingdom -> periodicals.newspaper_circulation_area.newspapers -> Financial Times\n# Answer:\nUnited Kingdom"], "ground_truth": ["Salisbury"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-531", "prediction": ["# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Disco -> music.genre.parent_genre -> Soul music\n# Answer:\nDisco", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Contemporary R&B -> common.topic.notable_types -> Musical genre\n# Answer:\nContemporary R&B", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Contemporary R&B -> broadcast.genre.content -> 1Club.FM: 80s (Pop)\n# Answer:\nContemporary R&B", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Contemporary R&B -> music.genre.parent_genre -> Rhythm and blues\n# Answer:\nContemporary R&B", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Contemporary R&B -> broadcast.genre.content -> 1Club.FM: Jammin' Oldies\n# Answer:\nContemporary R&B", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Disco -> music.genre.subgenre -> House music\n# Answer:\nDisco", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Dance music -> broadcast.genre.content -> 1.FM Absolute  90's\n# Answer:\nDance music", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Disco -> broadcast.genre.content -> 1.FM Disco Ball\n# Answer:\nDisco", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Contemporary R&B -> broadcast.genre.content -> 1Club.FM: V101\n# Answer:\nContemporary R&B", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Dance music -> broadcast.genre.content -> 1.FM Euro 80's\n# Answer:\nDance music"], "ground_truth": ["Funk", "Adult contemporary music", "Pop rock", "Contemporary R&B", "New jack swing", "Soul music", "Rock music", "Dance-pop", "Rhythm and blues", "Pop music", "Dance music", "Post-disco", "Disco", "Electronic music", "Motown Sound"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 1.0, "ans_recall": 0.2, "path_f1": 0.33333333333333337, "path_precision": 1.0, "path_recall": 0.2, "path_ans_f1": 0.5, "path_ans_precision": 1.0, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-532", "prediction": ["# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nm.0j2zzj_", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nm.0k4ytw5", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9pdz2 -> soccer.football_player_stats.team -> A.C. Milan\n# Answer:\nm.0w9pdz2", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nm.0w9plqp", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0j2zzj_", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m -> soccer.football_player_stats.team -> LA Galaxy\n# Answer:\nm.0w8w79m", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0k4ytw5", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.team -> Paris Saint-Germain F.C.\n# Answer:\nm.0qzkj58", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nm.0qzkj58", "# Reasoning Path:\nDavid Beckham -> tv.tv_actor.guest_roles -> m.0_z851f -> tv.tv_guest_role.special_performance_type -> Him/Herself\n# Answer:\nm.0_z851f"], "ground_truth": ["Football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-533", "prediction": ["# Reasoning Path:\nJulian Lennon -> people.person.profession -> Guitarist -> freebase.type_hints.included_types -> Person\n# Answer:\nGuitarist", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Singer -> base.lightweight.profession.specialization_of -> Musicians and Singers\n# Answer:\nSinger", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Guitarist -> freebase.type_hints.included_types -> Topic\n# Answer:\nGuitarist", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Photographer -> base.descriptive_names.names.descriptive_name -> m.0100_vbb\n# Answer:\nPhotographer", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Guitarist -> people.profession.specializations -> Bassist\n# Answer:\nGuitarist", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Photographer -> common.topic.image -> FotografosByMichellZappa\n# Answer:\nPhotographer", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Guitarist -> people.profession.specializations -> Classical guitarists\n# Answer:\nGuitarist", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Photographer -> base.descriptive_names.names.descriptive_name -> m.0100_vdz\n# Answer:\nPhotographer", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Photographer -> common.topic.webpage -> m.09wdkl6\n# Answer:\nPhotographer", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Guitarist -> fictional_universe.character_occupation.characters_with_this_occupation -> David St. Hubbins\n# Answer:\nGuitarist"], "ground_truth": ["Musician", "Record producer", "Actor", "Guitarist", "Photographer", "Singer", "Writer", "Singer-songwriter", "Film Score Composer"], "ans_acc": 0.4444444444444444, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 1.0, "ans_recall": 0.3333333333333333, "path_f1": 0.5, "path_precision": 1.0, "path_recall": 0.3333333333333333, "path_ans_f1": 0.6153846153846153, "path_ans_precision": 1.0, "path_ans_recall": 0.4444444444444444}
{"id": "WebQTest-534", "prediction": ["# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Bernie Brewer -> common.topic.image -> Bernie Brewer (left) in his dugout at Miller Park.\n# Answer:\nBernie Brewer", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Bernie Brewer -> common.topic.notable_types -> Mascot\n# Answer:\nBernie Brewer", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Hank -> common.topic.notable_types -> Mascot\n# Answer:\nHank", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Bernie Brewer -> common.topic.image -> Bernie brewer in dugout\n# Answer:\nBernie Brewer", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Bernie Brewer -> common.topic.image -> Bernie-Brewer.jpg\n# Answer:\nBernie Brewer", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Hank -> common.topic.notable_for -> g.11b5ltzrvv\n# Answer:\nHank", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Bonnie Brewer -> common.topic.article -> m.0blvgh\n# Answer:\nBonnie Brewer", "# Reasoning Path:\nMilwaukee Brewers -> base.schemastaging.organization_extra.contact_webpages -> m.010dwkgw -> internet.localized_uri.location -> United States of America\n# Answer:\nm.010dwkgw", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Bonnie Brewer -> common.topic.notable_for -> g.1259jd77m\n# Answer:\nBonnie Brewer", "# Reasoning Path:\nMilwaukee Brewers -> base.schemastaging.organization_extra.contact_webpages -> m.010dwkgw -> internet.localized_uri.language -> English\n# Answer:\nm.010dwkgw"], "ground_truth": ["Bonnie Brewer", "Bernie Brewer", "Hank"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-535", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Missouri -> location.location.containedby -> Contiguous United States\n# Answer:\nMissouri", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_children -> Missouri -> location.location.containedby -> Contiguous United States\n# Answer:\nMissouri", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_children -> Nebraska -> common.topic.notable_types -> US State\n# Answer:\nNebraska", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\ng.125_r6z32", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Missouri -> location.location.containedby -> United States, with Territories\n# Answer:\nMissouri", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Texas -> base.aareas.schema.administrative_area.administrative_area_type -> U.S. state\n# Answer:\nTexas", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_children -> Missouri -> location.location.containedby -> United States, with Territories\n# Answer:\nMissouri", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Texas -> location.location.partially_contains -> Rio Grande\n# Answer:\nTexas", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nm.010r7f0z", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_children -> Texas -> base.aareas.schema.administrative_area.administrative_area_type -> U.S. state\n# Answer:\nTexas"], "ground_truth": ["Texas"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-537", "prediction": ["# Reasoning Path:\nCharlie Hunnam -> film.actor.film -> m.0113ygm8 -> film.performance.film -> Crimson Peak\n# Answer:\nm.0113ygm8", "# Reasoning Path:\nCharlie Hunnam -> film.actor.film -> m.0h0yd7r -> film.performance.film -> Pacific Rim\n# Answer:\nm.0h0yd7r", "# Reasoning Path:\nCharlie Hunnam -> film.actor.film -> m.0gxp6xq -> film.performance.film -> The Ledge\n# Answer:\nm.0gxp6xq", "# Reasoning Path:\nCharlie Hunnam -> film.actor.film -> m.0113ygm8 -> film.performance.character -> Dr. Alan McMichael\n# Answer:\nm.0113ygm8", "# Reasoning Path:\nCharlie Hunnam -> film.actor.film -> m.0h0yd7r -> film.performance.character -> Raleigh Becket\n# Answer:\nm.0h0yd7r", "# Reasoning Path:\nCharlie Hunnam -> film.actor.film -> m.0gxp6xq -> film.performance.character -> Gavin Nichols\n# Answer:\nm.0gxp6xq", "# Reasoning Path:\nCharlie Hunnam -> common.topic.webpage -> m.094hqqj -> common.webpage.resource -> Television\n# Answer:\nm.094hqqj", "# Reasoning Path:\nCharlie Hunnam -> common.topic.webpage -> m.094n9d5 -> common.webpage.resource -> R. Kelly faces police probe over underage sex video\n# Answer:\nm.094n9d5", "# Reasoning Path:\nCharlie Hunnam -> people.person.parents -> Jane Hunnam -> people.person.children -> William Hunnam\n# Answer:\nJane Hunnam", "# Reasoning Path:\nCharlie Hunnam -> common.topic.webpage -> m.094hqqj -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.094hqqj"], "ground_truth": ["Pacific Rim", "Deadfall", "Cold Mountain", "Abandon", "Knights of the Roundtable: King Arthur", "Green Street", "Whatever Happened to Harold Smith?", "The Ledge", "Crimson Peak", "Frankie Go Boom", "Nicholas Nickleby", "Children of Men"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.28571428571428564, "path_precision": 0.3, "path_recall": 0.2727272727272727, "path_ans_f1": 0.2727272727272727, "path_ans_precision": 0.3, "path_ans_recall": 0.25}
{"id": "WebQTest-538", "prediction": ["# Reasoning Path:\nTaylor Swift -> film.actor.film -> m.09kkm44 -> film.performance.film -> Valentine's Day\n# Answer:\nm.09kkm44", "# Reasoning Path:\nTaylor Swift -> film.actor.film -> m.0gwnx7m -> film.performance.film -> The Lorax\n# Answer:\nm.0gwnx7m", "# Reasoning Path:\nTaylor Swift -> film.actor.film -> m.09kkm44 -> film.performance.character -> Felicia\n# Answer:\nm.09kkm44", "# Reasoning Path:\nTaylor Swift -> film.actor.film -> m.0y4q1x7 -> film.performance.film -> The Giver\n# Answer:\nm.0y4q1x7", "# Reasoning Path:\nTaylor Swift -> film.actor.film -> m.0gwnx7m -> film.performance.character -> Audrey\n# Answer:\nm.0gwnx7m", "# Reasoning Path:\nTaylor Swift -> music.artist.album -> Fifteen -> common.topic.notable_types -> Composition\n# Answer:\nFifteen", "# Reasoning Path:\nTaylor Swift -> film.actor.film -> m.0y4q1x7 -> film.performance.character -> Rosemary\n# Answer:\nm.0y4q1x7", "# Reasoning Path:\nTaylor Swift -> base.popstra.celebrity.friendship -> m.06461gz -> base.popstra.friendship.participant -> Selena Gomez\n# Answer:\nm.06461gz", "# Reasoning Path:\nTaylor Swift -> film.actor.film -> m.0y4q1x7 -> freebase.valuenotation.is_reviewed -> Actor\n# Answer:\nm.0y4q1x7", "# Reasoning Path:\nTaylor Swift -> music.artist.album -> 1989 -> common.topic.notable_types -> Musical Album\n# Answer:\n1989"], "ground_truth": ["The Lorax", "Valentine's Day", "The Giver", "Hannah Montana: The Movie", "Jonas Brothers: The Concert Experience"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333326, "path_precision": 0.3, "path_recall": 0.375, "path_ans_f1": 0.4, "path_ans_precision": 0.3, "path_ans_recall": 0.6}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nm.08_ksdj", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.office_position_or_title -> Chancellor of Germany\n# Answer:\nm.08_ksdj", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nm.0pz073c", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ng.1254zbtvv", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> organization.organization.geographic_scope -> Germany\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> base.schemastaging.context_name.pronunciation -> g.125_pt37m\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.place_founded -> Nazi Germany\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.office_position_or_title -> F\u00fchrer\n# Answer:\nm.0pz073c", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.basic_title -> F\u00fchrer\n# Answer:\nm.0pz073c", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.founders -> Heinrich Himmler\n# Answer:\nWaffen-SS"], "ground_truth": ["Nazi Germany"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.28571428571428564, "path_precision": 0.3, "path_recall": 0.2727272727272727, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-540", "prediction": ["# Reasoning Path:\nBrett Favre -> people.person.education -> m.03gkqk_ -> education.education.institution -> University of Southern Mississippi\n# Answer:\nm.03gkqk_", "# Reasoning Path:\nBrett Favre -> people.person.religion -> Catholicism -> base.popstra.religion.membership -> m.064bnd1\n# Answer:\nCatholicism", "# Reasoning Path:\nBrett Favre -> common.topic.image -> Brett Favre as Jet1-edit -> common.image.size -> m.04y2174\n# Answer:\nBrett Favre as Jet1-edit", "# Reasoning Path:\nBrett Favre -> common.topic.image -> BrettFavre -> common.image.size -> m.029jwyn\n# Answer:\nBrettFavre", "# Reasoning Path:\nBrett Favre -> common.topic.image -> Packvbears -> common.image.size -> m.0kjks2\n# Answer:\nPackvbears", "# Reasoning Path:\nBrett Favre -> common.topic.image -> BrettFavre -> common.image.appears_in_topic_gallery -> Athletes who should hang it up\n# Answer:\nBrettFavre", "# Reasoning Path:\nBrett Favre -> common.topic.image -> Packvbears -> common.image.appears_in_topic_gallery -> Bears\u2013Packers rivalry\n# Answer:\nPackvbears"], "ground_truth": ["University of Southern Mississippi"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.22222222222222224, "path_precision": 0.14285714285714285, "path_recall": 0.5, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-542", "prediction": ["# Reasoning Path:\nAlex Chilton -> people.deceased_person.place_of_death -> New Orleans -> common.topic.subject_of -> Sweet Magnolia Tours\n# Answer:\nNew Orleans", "# Reasoning Path:\nAlex Chilton -> people.deceased_person.place_of_death -> New Orleans -> base.biblioness.bibs_location.state -> Louisiana\n# Answer:\nNew Orleans", "# Reasoning Path:\nAlex Chilton -> people.person.parents -> Sidney Chilton -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nSidney Chilton", "# Reasoning Path:\nAlex Chilton -> people.person.parents -> Sidney Chilton -> people.person.gender -> Male\n# Answer:\nSidney Chilton", "# Reasoning Path:\nAlex Chilton -> music.artist.album -> 19 Years: A Collection of Alex Chilton -> common.topic.notable_for -> g.1256t_zr4\n# Answer:\n19 Years: A Collection of Alex Chilton", "# Reasoning Path:\nAlex Chilton -> music.artist.album -> 1970 -> common.topic.notable_types -> Musical Album\n# Answer:\n1970", "# Reasoning Path:\nAlex Chilton -> music.artist.album -> 19 Years: A Collection of Alex Chilton -> music.album.album_content_type -> Compilation album\n# Answer:\n19 Years: A Collection of Alex Chilton", "# Reasoning Path:\nAlex Chilton -> people.person.parents -> Sidney Chilton -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nSidney Chilton", "# Reasoning Path:\nAlex Chilton -> music.artist.album -> 1970 -> common.topic.notable_for -> g.1258ysr17\n# Answer:\n1970", "# Reasoning Path:\nAlex Chilton -> people.person.parents -> Sidney Chilton -> freebase.valuenotation.has_value -> Parents\n# Answer:\nSidney Chilton"], "ground_truth": ["New Orleans"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-543", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.02wp75f -> education.education.institution -> Boston University\n# Answer:\nm.02wp75f", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.04hddst -> education.education.institution -> Morehouse College\n# Answer:\nm.04hddst", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> education.education.institution -> Crozer Theological Seminary\n# Answer:\nm.0dh6jzz", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.02wp75f -> education.education.degree -> PhD\n# Answer:\nm.02wp75f", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.04hddst -> education.education.degree -> Bachelor of Arts\n# Answer:\nm.04hddst", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> education.education.degree -> Bachelor of Divinity\n# Answer:\nm.0dh6jzz", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nm.0dh6jzz", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nm.011j_4sh", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> common.topic.article -> m.05nnh0\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> freebase.valuenotation.has_value -> Minor\n# Answer:\nm.0dh6jzz"], "ground_truth": ["Crozer Theological Seminary", "Morehouse College", "Boston University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-545", "prediction": ["# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.cause_of_death -> Anal cancer -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nAnal cancer", "# Reasoning Path:\nFarrah Fawcett -> common.topic.notable_for -> g.125f8rws1\n# Answer:\ng.125f8rws1", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.cause_of_death -> Anal cancer -> medicine.disease.parent_disease -> Cancer\n# Answer:\nAnal cancer", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.cause_of_death -> Anal cancer -> common.topic.webpage -> m.09w_c2s\n# Answer:\nAnal cancer", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.cause_of_death -> Anal cancer -> common.topic.webpage -> m.09wx5h4\n# Answer:\nAnal cancer", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.cause_of_death -> Anal cancer -> common.topic.webpage -> m.09wzc81\n# Answer:\nAnal cancer", "# Reasoning Path:\nFarrah Fawcett -> common.topic.webpage -> m.0773vxj -> common.webpage.category -> Topic Webpage\n# Answer:\nm.0773vxj", "# Reasoning Path:\nFarrah Fawcett -> common.topic.webpage -> m.08wwplb -> common.webpage.resource -> Farrah Fawcett: 16 highlights\n# Answer:\nm.08wwplb", "# Reasoning Path:\nFarrah Fawcett -> common.topic.webpage -> m.0773vxj -> common.webpage.resource -> TV and Movie Trivia Tribute\n# Answer:\nm.0773vxj", "# Reasoning Path:\nFarrah Fawcett -> common.topic.webpage -> m.094039t -> common.webpage.resource -> '70s Comebacks of the Week\n# Answer:\nm.094039t"], "ground_truth": ["Anal cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-546", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> common.topic.notable_types -> Person\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.nationality -> United States of America\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Esm\u00e9 Annabelle Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nEsm\u00e9 Annabelle Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.spouse -> Tracy Pollan\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.location_of_ceremony -> Arlington\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.profession -> Actor\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.sibling_s -> m.0j217jw\n# Answer:\nSam Michael Fox"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-547", "prediction": ["# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.image -> 10colonesbcrfront\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.webpage -> m.04m2m2k\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60wmkvh\n# Answer:\ng.11b60wmkvh", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.11b71w3_6h\n# Answer:\ng.11b71w3_6h", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_5xgy\n# Answer:\ng.1245_5xgy", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_6n42\n# Answer:\ng.1245_6n42", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.1hhc37pjz\n# Answer:\ng.1hhc37pjz", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.1hhc37x5q\n# Answer:\ng.1hhc37x5q"], "ground_truth": ["Costa Rican col\u00f3n"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-548", "prediction": ["# Reasoning Path:\nJoe Walsh -> music.group_member.instruments_played -> Bass guitar -> common.topic.webpage -> m.09wnh17\n# Answer:\nBass guitar", "# Reasoning Path:\nJoe Walsh -> music.group_member.instruments_played -> Bass guitar -> common.topic.subject_of -> Reverb\n# Answer:\nBass guitar", "# Reasoning Path:\nJoe Walsh -> music.group_member.instruments_played -> Guitar -> music.instrument.variation -> Bass guitar\n# Answer:\nGuitar", "# Reasoning Path:\nJoe Walsh -> music.group_member.instruments_played -> Bass guitar -> education.field_of_study.students_majoring -> m.0cn9pt_\n# Answer:\nBass guitar", "# Reasoning Path:\nJoe Walsh -> music.group_member.instruments_played -> Guitar -> music.instrument.variation -> Fretless guitar\n# Answer:\nGuitar", "# Reasoning Path:\nJoe Walsh -> music.guitarist.guitars_played -> Fender Stratocaster -> music.guitar.guitarists -> George Harrison\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nJoe Walsh -> music.guitarist.guitars_played -> Fender Stratocaster -> common.topic.image -> Fender strat\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nJoe Walsh -> music.guitarist.guitars_played -> Fender Stratocaster -> music.guitar.guitarists -> David Gilmour\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nJoe Walsh -> music.guitarist.guitars_played -> Fender Stratocaster -> symbols.namesake.named_after -> Leo Fender\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nJoe Walsh -> music.group_member.instruments_played -> Guitar -> common.topic.subject_of -> Luke Marchie\n# Answer:\nGuitar"], "ground_truth": ["Fender Stratocaster"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-549", "prediction": ["# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0d6v -> base.schemastaging.phone_sandbox.service_language -> Spanish Language\n# Answer:\nm.010d0d6v", "# Reasoning Path:\nEuropean Union -> base.nations.supranational_body.member -> France -> location.country.official_language -> French\n# Answer:\nFrance", "# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0d6v -> base.schemastaging.phone_sandbox.service_location -> Earth\n# Answer:\nm.010d0d6v", "# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0fvn -> base.schemastaging.phone_sandbox.service_language -> French\n# Answer:\nm.010d0fvn", "# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0d6v -> base.schemastaging.phone_sandbox.service_language -> English Language\n# Answer:\nm.010d0d6v", "# Reasoning Path:\nEuropean Union -> base.nations.supranational_body.member -> France -> location.country.languages_spoken -> French\n# Answer:\nFrance", "# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0fvn -> base.schemastaging.phone_sandbox.service_language -> English Language\n# Answer:\nm.010d0fvn", "# Reasoning Path:\nEuropean Union -> location.statistical_region.broadband_penetration_rate -> g.1245_6ndt\n# Answer:\ng.1245_6ndt", "# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0d6v -> base.schemastaging.phone_sandbox.service_language -> French\n# Answer:\nm.010d0d6v", "# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0d6v -> freebase.valuenotation.has_no_value -> Saturday closing time\n# Answer:\nm.010d0d6v"], "ground_truth": ["German Language", "Portuguese Language", "Italian Language", "English Language", "Dutch Language", "French", "Spanish Language", "Polish Language", "Greek Language", "Swedish Language"], "ans_acc": 0.3, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.47619047619047616, "path_precision": 0.5, "path_recall": 0.45454545454545453, "path_ans_f1": 0.42, "path_ans_precision": 0.7, "path_ans_recall": 0.3}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> common.topic.notable_types -> Profession\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.specialization_of -> Musician\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.track_contributions -> g.11b7_lvdf2\n# Answer:\ng.11b7_lvdf2", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.specialization_of -> Musician\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> http://www.discogs.com/artist/Michael+Bubl%E9 -> common.webpage.category -> Topic Webpage\n# Answer:\nhttp://www.discogs.com/artist/Michael+Bubl%E9", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.03lpqjt -> common.webpage.category -> Official Website\n# Answer:\nm.03lpqjt", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> common.topic.notable_types -> Profession\n# Answer:\nSongwriter"], "ground_truth": ["Songwriter", "Singer", "Actor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-550", "prediction": ["# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.starring_roles -> m.04f2t4z -> tv.regular_tv_appearance.series -> Reality Check\n# Answer:\nReality Check", "# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.starring_roles -> m.0bmltz0 -> tv.regular_tv_appearance.series -> Gladiators 2000\n# Answer:\nGladiators 2000", "# Reasoning Path:\nRyan Seacrest -> tv.tv_producer.programs_produced -> g.11byrxg698\n# Answer:\ng.11byrxg698", "# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.starring_roles -> m.04f2t4z -> tv.regular_tv_appearance.character -> Jack Craft\n# Answer:\nm.04f2t4z", "# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.guest_roles -> m.0bgmqnf -> tv.tv_guest_role.episodes_appeared_in -> 5/21/04\n# Answer:\nm.0bgmqnf", "# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.starring_roles -> m.0bmqp0y -> tv.regular_tv_appearance.series -> Click\n# Answer:\nm.0bmqp0y", "# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.starring_roles -> m.0bmltz0 -> tv.regular_tv_appearance.special_performance_type -> Him/Herself\n# Answer:\nm.0bmltz0", "# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.guest_roles -> m.0bgn4x7 -> tv.tv_guest_role.episodes_appeared_in -> The 59th Annual Emmy Awards\n# Answer:\nm.0bgn4x7", "# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.starring_roles -> m.0bmqp0y -> tv.regular_tv_appearance.special_performance_type -> Him/Herself\n# Answer:\nm.0bmqp0y", "# Reasoning Path:\nRyan Seacrest -> tv.tv_producer.programs_produced -> m.04f890x -> tv.tv_producer_term.program -> Denise Richards: It's Complicated\n# Answer:\nDenise Richards: It's Complicated"], "ground_truth": ["American Top 40", "On Air with Ryan Seacrest"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-552", "prediction": ["# Reasoning Path:\nItaly -> location.country.official_language -> Italian Language -> language.human_language.main_country -> Slovenia\n# Answer:\nItalian Language", "# Reasoning Path:\nItaly -> location.country.official_language -> Italian Language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nItalian Language", "# Reasoning Path:\nItaly -> location.country.languages_spoken -> Italian Language -> language.human_language.main_country -> Slovenia\n# Answer:\nItalian Language", "# Reasoning Path:\nItaly -> location.country.official_language -> Italian Language -> language.human_language.countries_spoken_in -> Slovenia\n# Answer:\nItalian Language", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\ng.11b71mdn0q", "# Reasoning Path:\nItaly -> location.country.official_language -> Italian Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nItalian Language", "# Reasoning Path:\nItaly -> location.country.languages_spoken -> Italian Language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nItalian Language", "# Reasoning Path:\nItaly -> location.country.languages_spoken -> Austrian German -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nAustrian German", "# Reasoning Path:\nItaly -> location.country.official_language -> Italian Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nItalian Language", "# Reasoning Path:\nItaly -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Slovenia\n# Answer:\nItalian Language"], "ground_truth": ["Italian Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-554", "prediction": ["# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Andorra\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Greece\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Belgium\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\ng.1hhc37x4q", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60xs9d2\n# Answer:\ng.11b60xs9d2", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc38hhk\n# Answer:\ng.1hhc38hhk", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc3dvnk\n# Answer:\ng.1hhc3dvnk", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.1hhc39shg\n# Answer:\ng.1hhc39shg", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.1hhc3h715\n# Answer:\ng.1hhc3h715"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-555", "prediction": ["# Reasoning Path:\nDrew Barrymore -> base.popstra.celebrity.substance_abuse -> m.063gck7 -> base.popstra.substance_abuse.substance -> alcohol and drugs\n# Answer:\nm.063gck7", "# Reasoning Path:\nDrew Barrymore -> film.director.film -> Whip It -> film.film.genre -> Comedy\n# Answer:\nWhip It", "# Reasoning Path:\nDrew Barrymore -> film.director.film -> Whip It -> film.film.language -> English Language\n# Answer:\nWhip It", "# Reasoning Path:\nDrew Barrymore -> film.director.film -> Whip It -> film.film.genre -> Drama\n# Answer:\nWhip It", "# Reasoning Path:\nDrew Barrymore -> film.director.film -> Whip It -> film.film.genre -> Sports films\n# Answer:\nWhip It", "# Reasoning Path:\nDrew Barrymore -> film.director.film -> Whip It -> media_common.netflix_title.netflix_genres -> Comedies\n# Answer:\nWhip It", "# Reasoning Path:\nDrew Barrymore -> film.producer.films_executive_produced -> Donnie Darko -> media_common.netflix_title.netflix_genres -> Drama\n# Answer:\nDonnie Darko", "# Reasoning Path:\nDrew Barrymore -> film.producer.films_executive_produced -> Donnie Darko -> film.film.genre -> Drama\n# Answer:\nDonnie Darko", "# Reasoning Path:\nDrew Barrymore -> film.director.film -> Whip It -> media_common.netflix_title.netflix_genres -> Comedy\n# Answer:\nWhip It", "# Reasoning Path:\nDrew Barrymore -> film.producer.films_executive_produced -> Donnie Darko -> media_common.netflix_title.netflix_genres -> Independent Dramas\n# Answer:\nDonnie Darko"], "ground_truth": ["Alcoholic beverage", "Cocaine"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-556", "prediction": ["# Reasoning Path:\nSt George's Day -> time.holiday.day_of_year -> April 23 -> common.topic.notable_types -> Day Of Year\n# Answer:\nApril 23", "# Reasoning Path:\nSt George's Day -> time.holiday.day_of_year -> April 23 -> common.topic.article -> m.0tj3\n# Answer:\nApril 23", "# Reasoning Path:\nSt George's Day -> time.holiday.day_of_year -> April 23 -> time.day_of_year.holidays -> All Ukrainian Day of psychologist\n# Answer:\nApril 23", "# Reasoning Path:\nSt George's Day -> time.holiday.day_of_year -> April 23 -> time.day_of_year.holidays -> National Cherry Cheesecake Day\n# Answer:\nApril 23", "# Reasoning Path:\nSt George's Day -> time.holiday.featured_in_religions -> Christianity -> religion.religion.deities -> God\n# Answer:\nChristianity", "# Reasoning Path:\nSt George's Day -> time.holiday.day_of_year -> April 23 -> time.day_of_year.holidays -> National Sovereignty and Children's Day\n# Answer:\nApril 23", "# Reasoning Path:\nSt George's Day -> common.topic.image -> Shakespeare -> common.image.appears_in_topic_gallery -> ...O brave new world, That has such people in't!\n# Answer:\nShakespeare", "# Reasoning Path:\nSt George's Day -> common.topic.image -> Shakespeare -> common.image.size -> m.02bk77b\n# Answer:\nShakespeare", "# Reasoning Path:\nSt George's Day -> time.holiday.featured_in_religions -> Christianity -> religion.religion.deities -> Holy Spirit\n# Answer:\nChristianity", "# Reasoning Path:\nSt George's Day -> time.holiday.featured_in_religions -> Christianity -> book.periodical_subject.periodicals -> 7ball\n# Answer:\nChristianity"], "ground_truth": ["April 23"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-557", "prediction": ["# Reasoning Path:\nRoman Empire -> location.country.form_of_government -> Autocracy -> government.form_of_government.countries -> Byzantine Empire\n# Answer:\nAutocracy", "# Reasoning Path:\nRoman Empire -> location.country.form_of_government -> Autocracy -> common.topic.notable_for -> g.125b8tl9d\n# Answer:\nAutocracy", "# Reasoning Path:\nRoman Empire -> location.country.form_of_government -> Autocracy -> common.topic.notable_types -> Form of Government\n# Answer:\nAutocracy", "# Reasoning Path:\nRoman Empire -> book.book_subject.works -> Memoirs of Hadrian -> book.written_work.subjects -> Hadrian\n# Answer:\nMemoirs of Hadrian", "# Reasoning Path:\nRoman Empire -> location.country.form_of_government -> Autocracy -> government.form_of_government.countries -> Congress Poland\n# Answer:\nAutocracy", "# Reasoning Path:\nRoman Empire -> location.country.form_of_government -> Autocracy -> government.form_of_government.countries -> Francoist Spain\n# Answer:\nAutocracy", "# Reasoning Path:\nRoman Empire -> book.book_subject.works -> Memoirs of Hadrian -> book.book.editions -> Memoirs of Hadrian: And Reflections on the Composition of Memoirs of Hadrian\n# Answer:\nMemoirs of Hadrian", "# Reasoning Path:\nRoman Empire -> military.military_combatant.military_conflicts -> m.048zsyf -> military.military_combatant_group.conflict -> First Jewish\u2013Roman War\n# Answer:\nm.048zsyf", "# Reasoning Path:\nRoman Empire -> military.military_combatant.military_conflicts -> m.048zt3v -> military.military_combatant_group.conflict -> Bar Kokhba revolt\n# Answer:\nm.048zt3v", "# Reasoning Path:\nRoman Empire -> book.book_subject.works -> Memoirs of Hadrian -> common.topic.notable_types -> Book\n# Answer:\nMemoirs of Hadrian"], "ground_truth": ["Autocracy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-559", "prediction": ["# Reasoning Path:\nRobin Gibb -> music.group_member.instruments_played -> Piano -> common.topic.notable_types -> Musical instrument\n# Answer:\nPiano", "# Reasoning Path:\nRobin Gibb -> music.group_member.instruments_played -> Violin -> common.topic.notable_types -> Musical instrument\n# Answer:\nViolin", "# Reasoning Path:\nRobin Gibb -> music.artist.track_contributions -> m.0110195t -> music.track_contribution.track -> Islands in the Stream\n# Answer:\nm.0110195t", "# Reasoning Path:\nRobin Gibb -> music.artist.track_contributions -> m.0110253_ -> music.track_contribution.role -> Vocals\n# Answer:\nm.0110253_", "# Reasoning Path:\nRobin Gibb -> music.artist.track_contributions -> m.0110195t -> music.track_contribution.role -> Vocals\n# Answer:\nm.0110195t", "# Reasoning Path:\nRobin Gibb -> music.artist.label -> Festival Records -> common.topic.article -> m.04w833\n# Answer:\nFestival Records", "# Reasoning Path:\nRobin Gibb -> music.artist.label -> Festival Records -> common.topic.notable_types -> Record label\n# Answer:\nFestival Records", "# Reasoning Path:\nRobin Gibb -> music.artist.track_contributions -> m.0110253_ -> music.track_contribution.track -> Don't Cry Alone\n# Answer:\nm.0110253_", "# Reasoning Path:\nRobin Gibb -> music.artist.track_contributions -> m.013885yn -> music.track_contribution.track -> Edge of the Universe\n# Answer:\nm.013885yn", "# Reasoning Path:\nRobin Gibb -> music.artist.label -> ATCO Records -> common.topic.webpage -> m.04lsq5b\n# Answer:\nATCO Records"], "ground_truth": ["Piano", "Harmonica", "Drum machine", "Violin", "Vocals"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.26666666666666666, "ans_precission": 0.2, "ans_recall": 0.4, "path_f1": 0.14035087719298245, "path_precision": 0.4, "path_recall": 0.0851063829787234, "path_ans_f1": 0.48, "path_ans_precision": 0.4, "path_ans_recall": 0.6}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.statistical_region.co2_emissions_mobile -> m.045l26t\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> location.location.containedby -> United States of America\n# Answer:\nKansas", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.statistical_region.co2_emissions_mobile -> m.045l26t\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111 -> location.location.containedby -> Wyandotte County\n# Answer:\n66111", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.us_county.hud_county_place -> Kansas City\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.location.containedby -> Kansas City, MO-KS Metropolitan Statistical Area\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.statistical_region.population -> g.11b674pwdp\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nKansas", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.statistical_region.population -> g.11b674pwdp\n# Answer:\nWyandotte County"], "ground_truth": ["Wyandotte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-560", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.actor -> James Earl Jones\n# Answer:\nm.03jr0x9", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zsqt -> film.performance.actor -> David Prowse\n# Answer:\nm.0j7zsqt", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.special_performance_type -> Voice\n# Answer:\nm.03jr0x9", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.film -> The Benchwarmers\n# Answer:\nm.03jr0x9", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zsqt -> film.performance.film -> The Making of Star Wars\n# Answer:\nm.0j7zsqt", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.part_of_series -> Jedi Quest\n# Answer:\nPath to Truth"], "ground_truth": ["Matt Lanter"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-561", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.04j60k7 -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nm.04j60k7", "# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.0bqspr2 -> government.government_position_held.office_position_or_title -> Member of Illinois House of Representatives\n# Answer:\nm.0bqspr2", "# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.04j60k7 -> government.government_position_held.governmental_body -> United States House of Representatives\n# Answer:\nm.04j60k7", "# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.0bqspr2 -> government.government_position_held.jurisdiction_of_office -> Illinois\n# Answer:\nm.0bqspr2", "# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.0446bdb -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nm.0446bdb", "# Reasoning Path:\nAbraham Lincoln -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.0bqspr2 -> government.government_position_held.governmental_body -> Illinois House of Representatives\n# Answer:\nm.0bqspr2", "# Reasoning Path:\nAbraham Lincoln -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> California\n# Answer:\nUnited States of America", "# Reasoning Path:\nAbraham Lincoln -> people.person.nationality -> United States of America -> location.location.time_zones -> Central Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.0446bdb -> government.government_position_held.basic_title -> President\n# Answer:\nm.0446bdb"], "ground_truth": ["Abraham Lincoln 1861 presidential inauguration", "Abraham Lincoln 1865 presidential inauguration"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-562", "prediction": ["# Reasoning Path:\nTexas Rangers -> sports.sports_team.arena_stadium -> Rangers Ballpark in Arlington -> location.location.geolocation -> m.02_t4f8\n# Answer:\nRangers Ballpark in Arlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.arena_stadium -> Rangers Ballpark in Arlington -> location.location.containedby -> Arlington\n# Answer:\nRangers Ballpark in Arlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.arena_stadium -> Rangers Ballpark in Arlington -> business.sponsored_recipient.sponsorship -> m.060crqp\n# Answer:\nRangers Ballpark in Arlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.location -> Arlington -> location.statistical_region.population -> g.11b66bywm8\n# Answer:\nArlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.location -> Arlington -> location.location.contains -> Rangers Ballpark in Arlington\n# Answer:\nArlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.arena_stadium -> Rangers Ballpark in Arlington -> location.location.containedby -> Texas\n# Answer:\nRangers Ballpark in Arlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.location -> Arlington -> location.metropolitan_area_anchor.metropolitan_area -> Fort Worth-Arlington, TX Metropolitan Division\n# Answer:\nArlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.location -> Arlington -> location.statistical_region.population -> g.11b7td7_9_\n# Answer:\nArlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.location -> Arlington -> location.statistical_region.population -> g.11b7tm_xv7\n# Answer:\nArlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.location -> Arlington -> location.location.contains -> 22202\n# Answer:\nArlington"], "ground_truth": ["Rangers Ballpark in Arlington"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-564", "prediction": ["# Reasoning Path:\nArkansas State Capitol -> location.location.containedby -> Arkansas -> base.aareas.schema.administrative_area.capital -> Little Rock\n# Answer:\nArkansas", "# Reasoning Path:\nArkansas State Capitol -> location.location.containedby -> Arkansas -> base.aareas.schema.administrative_area.administrative_children -> White County\n# Answer:\nArkansas", "# Reasoning Path:\nArkansas State Capitol -> location.location.containedby -> Arkansas -> base.aareas.schema.administrative_area.administrative_children -> Arkansas County\n# Answer:\nArkansas", "# Reasoning Path:\nArkansas State Capitol -> location.location.containedby -> Little Rock -> location.location.containedby -> Arkansas\n# Answer:\nLittle Rock", "# Reasoning Path:\nArkansas State Capitol -> common.topic.image -> The Capitol Building -> common.image.appears_in_topic_gallery -> Arkansas General Assembly\n# Answer:\nThe Capitol Building", "# Reasoning Path:\nArkansas State Capitol -> location.location.containedby -> Arkansas -> base.aareas.schema.administrative_area.administrative_children -> Ashley County\n# Answer:\nArkansas", "# Reasoning Path:\nArkansas State Capitol -> location.location.containedby -> Arkansas -> location.location.containedby -> Contiguous United States\n# Answer:\nArkansas", "# Reasoning Path:\nArkansas State Capitol -> location.location.containedby -> Arkansas -> location.location.containedby -> United States of America\n# Answer:\nArkansas", "# Reasoning Path:\nArkansas State Capitol -> common.image.appears_in_topic_gallery -> George R. Mann -> people.person.profession -> Architect\n# Answer:\nGeorge R. Mann", "# Reasoning Path:\nArkansas State Capitol -> location.location.containedby -> Little Rock -> location.location.containedby -> Area code 501\n# Answer:\nLittle Rock"], "ground_truth": ["Little Rock"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-567", "prediction": ["# Reasoning Path:\nNogales -> location.citytown.postal_codes -> 85621 -> location.location.containedby -> Santa Cruz County\n# Answer:\n85621", "# Reasoning Path:\nNogales -> location.citytown.postal_codes -> 85621 -> location.location.contains -> Lourdes Catholic School\n# Answer:\n85621", "# Reasoning Path:\nNogales -> location.hud_foreclosure_area.ofheo_price_change -> m.07g9y5n\n# Answer:\nm.07g9y5n", "# Reasoning Path:\nNogales -> location.citytown.postal_codes -> 85621 -> location.postal_code.country -> United States of America\n# Answer:\n85621", "# Reasoning Path:\nNogales -> location.statistical_region.population -> g.11b66h2c6g\n# Answer:\ng.11b66h2c6g", "# Reasoning Path:\nNogales -> location.citytown.postal_codes -> 85628 -> location.postal_code.country -> United States of America\n# Answer:\n85628", "# Reasoning Path:\nNogales -> location.citytown.postal_codes -> 85662 -> common.topic.notable_for -> g.125d1ds3d\n# Answer:\n85662", "# Reasoning Path:\nNogales -> location.statistical_region.population -> g.11bymgs1xf\n# Answer:\ng.11bymgs1xf", "# Reasoning Path:\nNogales -> location.citytown.postal_codes -> 85628 -> common.topic.notable_for -> g.125614f5r\n# Answer:\n85628", "# Reasoning Path:\nNogales -> location.statistical_region.population -> g.11x1d4n5r\n# Answer:\ng.11x1d4n5r"], "ground_truth": ["85628", "85662", "85621"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-568", "prediction": ["# Reasoning Path:\nMichael Vick -> people.person.education -> m.0hpcsnj -> education.education.institution -> Virginia Tech\n# Answer:\nm.0hpcsnj", "# Reasoning Path:\nMichael Vick -> award.award_winner.awards_won -> m.0731jvs -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nm.0731jvs", "# Reasoning Path:\nMichael Vick -> people.person.education -> m.0n1k_2y -> education.education.institution -> Warwick High School\n# Answer:\nm.0n1k_2y", "# Reasoning Path:\nMichael Vick -> common.topic.image -> VICKpb crop -> common.image.size -> m.03sv987\n# Answer:\nVICKpb crop", "# Reasoning Path:\nMichael Vick -> award.award_winner.awards_won -> m.0731k7n -> award.award_honor.award -> Galloping Gobbler\n# Answer:\nm.0731k7n", "# Reasoning Path:\nMichael Vick -> common.topic.image -> Vick 6 -> common.image.size -> m.0bct9py\n# Answer:\nVick 6", "# Reasoning Path:\nMichael Vick -> award.award_winner.awards_won -> m.0731jvs -> award.award_honor.award -> Best NFL Player ESPY Award\n# Answer:\nm.0731jvs", "# Reasoning Path:\nMichael Vick -> award.award_winner.awards_won -> m.0_qxy0s -> award.award_honor.award -> NFL Honors - AP Comeback Player of the Year\n# Answer:\nm.0_qxy0s", "# Reasoning Path:\nMichael Vick -> award.award_winner.awards_won -> m.0_qxy0s -> freebase.valuenotation.has_no_value -> Ceremony\n# Answer:\nm.0_qxy0s", "# Reasoning Path:\nMichael Vick -> award.award_winner.awards_won -> m.0_qxy0s -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nm.0_qxy0s"], "ground_truth": ["2009"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-570", "prediction": ["# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Lawyer -> common.topic.subjects -> Sally & Fitch LLP\n# Answer:\nLawyer", "# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Lawyer -> common.topic.subjects -> Accident Advice Helpline\n# Answer:\nLawyer", "# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Judge -> common.topic.notable_types -> Profession\n# Answer:\nJudge", "# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Lawyer -> common.topic.subjects -> Brian White & Associates, P.C.\n# Answer:\nLawyer", "# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Lawyer -> people.profession.specializations -> Jurist\n# Answer:\nLawyer", "# Reasoning Path:\nWilliam Howard Taft -> base.yalebase.secret_society_member.belonged_to -> m.046xl1n -> base.yalebase.secret_society_membership.society -> Skull and Bones\n# Answer:\nm.046xl1n", "# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Jurist -> people.profession.specialization_of -> Lawyer\n# Answer:\nJurist", "# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Lawyer -> common.topic.subject_of -> Sally & Fitch LLP\n# Answer:\nLawyer", "# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Lawyer -> people.profession.specializations -> Criminal defense lawyer\n# Answer:\nLawyer", "# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Jurist -> common.topic.article -> m.0dl0h\n# Answer:\nJurist"], "ground_truth": ["Lawyer", "Judge", "Jurist"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-572", "prediction": ["# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> travel.travel_destination.tourist_attractions -> Falls of Foyers\n# Answer:\nInverness", "# Reasoning Path:\nRobin Cook -> common.topic.notable_for -> g.125bt8v5y\n# Answer:\ng.125bt8v5y", "# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> location.location.containedby -> Inverness-shire\n# Answer:\nInverness", "# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> location.location.containedby -> Scotland\n# Answer:\nInverness", "# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> location.statistical_region.population -> m.0cl9fgl\n# Answer:\nInverness", "# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> travel.travel_destination.tourist_attractions -> Aigas Field Centre\n# Answer:\nInverness", "# Reasoning Path:\nRobin Cook -> tv.tv_actor.guest_roles -> m.09nbpck -> tv.tv_guest_role.episodes_appeared_in -> 06 January 2002\n# Answer:\nm.09nbpck", "# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> location.location.containedby -> United Kingdom\n# Answer:\nInverness", "# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> travel.travel_destination.tourist_attractions -> Beauly Priory\n# Answer:\nInverness", "# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> location.statistical_region.population -> m.0cl9fgq\n# Answer:\nInverness"], "ground_truth": ["Inverness"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-573", "prediction": ["# Reasoning Path:\nMelba Pattillo Beals -> people.person.place_of_birth -> Little Rock -> base.biblioness.bibs_location.state -> Arkansas\n# Answer:\nLittle Rock", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.places_lived -> m.03pds23 -> people.place_lived.location -> Arkansas\n# Answer:\nm.03pds23", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.place_of_birth -> Little Rock -> location.citytown.postal_codes -> 72201\n# Answer:\nLittle Rock", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.place_of_birth -> Little Rock -> location.location.events -> 2007 Little Rock Film Festival\n# Answer:\nLittle Rock", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.place_of_birth -> Little Rock -> location.citytown.postal_codes -> 72202\n# Answer:\nLittle Rock", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.place_of_birth -> Little Rock -> location.citytown.postal_codes -> 72203\n# Answer:\nLittle Rock", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.place_of_birth -> Little Rock -> location.location.events -> 2008 Little Rock Film Festival\n# Answer:\nLittle Rock", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.education -> m.0n0d4k7 -> education.education.institution -> San Francisco State University\n# Answer:\nm.0n0d4k7", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.education -> m.0n1jwpr -> education.education.institution -> Columbia University\n# Answer:\nm.0n1jwpr", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.place_of_birth -> Little Rock -> location.location.events -> 2009 Little Rock Film Festival\n# Answer:\nLittle Rock"], "ground_truth": ["Arkansas"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-575", "prediction": ["# Reasoning Path:\nLouisiana -> government.political_district.representatives -> m.0bws06r -> government.government_position_held.office_holder -> Huey Long\n# Answer:\nm.0bws06r", "# Reasoning Path:\nLouisiana -> government.political_district.representatives -> m.0bws0xr -> government.government_position_held.office_holder -> Edward Livingston\n# Answer:\nm.0bws0xr", "# Reasoning Path:\nLouisiana -> government.political_district.representatives -> m.0n9d2xf -> government.government_position_held.office_holder -> Robert C. Nicholas\n# Answer:\nm.0n9d2xf", "# Reasoning Path:\nLouisiana -> government.political_district.representatives -> m.0bws06r -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nm.0bws06r", "# Reasoning Path:\nLouisiana -> government.political_district.representatives -> m.0n9d2xf -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nm.0n9d2xf", "# Reasoning Path:\nLouisiana -> government.governmental_jurisdiction.governing_officials -> m.09tmlgw -> government.government_position_held.office_holder -> Huey Long\n# Answer:\nm.09tmlgw", "# Reasoning Path:\nLouisiana -> government.political_district.representatives -> m.0n9d2xf -> government.government_position_held.basic_title -> Senator\n# Answer:\nm.0n9d2xf", "# Reasoning Path:\nLouisiana -> government.political_district.representatives -> m.0bws0xr -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nm.0bws0xr", "# Reasoning Path:\nLouisiana -> government.governmental_jurisdiction.governing_officials -> m.010f0pjr -> government.government_position_held.office_holder -> Scott Angelle\n# Answer:\nm.010f0pjr", "# Reasoning Path:\nLouisiana -> government.governmental_jurisdiction.governing_officials -> m.010f0pn6 -> government.government_position_held.office_holder -> Jay Dardenne\n# Answer:\nm.010f0pn6"], "ground_truth": ["George A. Waggaman", "Jeremiah B. Howell", "William C. C. Claiborne", "Russell B. Long", "Alexandre Mouton", "Charles Dominique Joseph Bouligny", "Thomas Posey", "Eligius Fromentin", "Allan B. Magruder", "David Vitter", "Mary Landrieu", "Robert C. Nicholas", "James Brown", "Huey Long", "Judah P. Benjamin", "Henry Johnson", "Edward Douglass White", "Alexander Porter", "Pierre Soul\u00e9", "Jean Noel Destr\u00e9han", "John Slidell", "John Breaux", "Charles Magill Conrad", "Josiah S. Johnston", "Alexander Barrow", "Edward Livingston"], "ans_acc": 0.11538461538461539, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.19047619047619047, "path_precision": 0.4, "path_recall": 0.125, "path_ans_f1": 0.1791044776119403, "path_ans_precision": 0.4, "path_ans_recall": 0.11538461538461539}
{"id": "WebQTest-576", "prediction": ["# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.location.contains -> Belize District\n# Answer:\nBelize", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.location.contains -> Cayo District\n# Answer:\nBelize", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.location.contains -> Corozal District\n# Answer:\nBelize", "# Reasoning Path:\nCentral America -> location.location.contains -> Panama -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nPanama", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.location.containedby -> Americas\n# Answer:\nBelize", "# Reasoning Path:\nCentral America -> location.location.contains -> Honduras -> location.location.containedby -> Americas\n# Answer:\nHonduras", "# Reasoning Path:\nCentral America -> location.location.contains -> Honduras -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nHonduras", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.country.administrative_divisions -> Belize District\n# Answer:\nBelize", "# Reasoning Path:\nCentral America -> location.location.contains -> Panama -> location.location.partiallycontains -> m.0wg9hnr\n# Answer:\nPanama", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.location.containedby -> Latin America\n# Answer:\nBelize"], "ground_truth": ["Guatemala", "Gran Colombia", "Belize", "Honduras", "Panama", "Costa Rica", "El Salvador"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 1.0, "ans_recall": 0.42857142857142855, "path_f1": 0.6, "path_precision": 1.0, "path_recall": 0.42857142857142855, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-577", "prediction": ["# Reasoning Path:\nDerek Fisher -> people.person.education -> m.030qjb6 -> education.education.institution -> University of Arkansas at Little Rock\n# Answer:\nm.030qjb6", "# Reasoning Path:\nDerek Fisher -> people.person.education -> m.040jfy4 -> education.education.institution -> Parkview Arts and Science Magnet High School\n# Answer:\nm.040jfy4", "# Reasoning Path:\nDerek Fisher -> people.person.education -> m.0sw01cy -> education.education.institution -> University of Arkansas\n# Answer:\nm.0sw01cy", "# Reasoning Path:\nDerek Fisher -> basketball.basketball_player.player_statistics -> m.04q9tdk -> basketball.basketball_player_stats.team -> Golden State Warriors\n# Answer:\nm.04q9tdk", "# Reasoning Path:\nDerek Fisher -> basketball.basketball_player.player_statistics -> m.04qc3sv -> basketball.basketball_player_stats.season -> 2000\u201301 NBA season\n# Answer:\nm.04qc3sv", "# Reasoning Path:\nDerek Fisher -> basketball.basketball_player.player_statistics -> m.04q9tdk -> basketball.basketball_player_stats.season -> 2004\u201305 NBA season\n# Answer:\nm.04q9tdk", "# Reasoning Path:\nDerek Fisher -> basketball.basketball_player.player_statistics -> m.04qhk3g -> basketball.basketball_player_stats.season -> 2007\u201308 NBA season\n# Answer:\nm.04qhk3g", "# Reasoning Path:\nDerek Fisher -> tv.tv_actor.guest_roles -> m.09nch_2 -> tv.tv_guest_role.episodes_appeared_in -> 6/23/04\n# Answer:\nm.09nch_2", "# Reasoning Path:\nDerek Fisher -> tv.tv_actor.guest_roles -> m.09nch_7 -> tv.tv_guest_role.episodes_appeared_in -> Derek Fisher, Mindless Self Indulgence\n# Answer:\nm.09nch_7", "# Reasoning Path:\nDerek Fisher -> basketball.basketball_player.player_statistics -> m.04qc3sv -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nm.04qc3sv"], "ground_truth": ["University of Arkansas", "University of Arkansas at Little Rock"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-578", "prediction": ["# Reasoning Path:\nKobe Bryant -> sports.drafted_athlete.drafted -> m.02h7cfz -> sports.sports_league_draft_pick.draft -> 1996 NBA draft\n# Answer:\nm.02h7cfz", "# Reasoning Path:\nKobe Bryant -> sports.drafted_athlete.drafted -> m.02h7cfz -> sports.sports_league_draft_pick.team -> Charlotte Hornets\n# Answer:\nm.02h7cfz", "# Reasoning Path:\nKobe Bryant -> sports.drafted_athlete.drafted -> m.02h7cfz -> sports.sports_league_draft_pick.school -> Lower Merion High School\n# Answer:\nm.02h7cfz", "# Reasoning Path:\nKobe Bryant -> basketball.basketball_player.player_statistics -> m.04qb_g9 -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nm.04qb_g9", "# Reasoning Path:\nKobe Bryant -> basketball.basketball_player.player_statistics -> m.04qb_g9 -> basketball.basketball_player_stats.season -> 2006\u201307 NBA season\n# Answer:\nm.04qb_g9", "# Reasoning Path:\nKobe Bryant -> basketball.basketball_player.player_statistics -> m.04qbwp0 -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nm.04qbwp0", "# Reasoning Path:\nKobe Bryant -> basketball.basketball_player.player_statistics -> m.04qcpcj -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nm.04qcpcj", "# Reasoning Path:\nKobe Bryant -> people.person.places_lived -> m.03pp_xg -> people.place_lived.location -> Newport Beach\n# Answer:\nm.03pp_xg", "# Reasoning Path:\nKobe Bryant -> basketball.basketball_player.player_statistics -> m.04qbwp0 -> basketball.basketball_player_stats.season -> 1997\u201398 NBA season\n# Answer:\nm.04qbwp0", "# Reasoning Path:\nKobe Bryant -> basketball.basketball_player.player_statistics -> m.04qcpcj -> basketball.basketball_player_stats.season -> 2005\u201306 NBA season\n# Answer:\nm.04qcpcj"], "ground_truth": ["Charlotte Hornets"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-579", "prediction": ["# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.location -> Charlotte -> sports.sports_team_location.teams -> Charlotte 49ers men's basketball\n# Answer:\nCharlotte", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.arena_stadium -> The Time Warner Cable Arena -> location.location.containedby -> Charlotte\n# Answer:\nThe Time Warner Cable Arena", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.location -> Charlotte -> sports.sports_team_location.teams -> Carolina Panthers\n# Answer:\nCharlotte", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.location -> Charlotte -> common.topic.webpage -> m.03lnb3b\n# Answer:\nCharlotte", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.location -> Charlotte -> sports.sports_team_location.teams -> Charlotte Hounds\n# Answer:\nCharlotte", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.location -> Charlotte -> location.statistical_region.population -> g.11b674q2v0\n# Answer:\nCharlotte", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.location -> Charlotte -> common.topic.webpage -> m.07ldsgk\n# Answer:\nCharlotte", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.arena_stadium -> The Time Warner Cable Arena -> location.location.events -> 2008 NCAA Men's Division I Basketball Tournament\n# Answer:\nThe Time Warner Cable Arena", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.location -> Charlotte -> common.topic.webpage -> m.09w7_w9\n# Answer:\nCharlotte", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.location -> Charlotte -> location.statistical_region.population -> g.11b7tmvwpg\n# Answer:\nCharlotte"], "ground_truth": ["Charlotte"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h -> sports.sports_team_roster.team -> UCLA Bruins football\n# Answer:\nm.0hpgh_h", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nm.06sbpz2", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0gggrzr -> sports.sports_team_roster.team -> Brooklyn Dodgers\n# Answer:\nm.0gggrzr", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbp_k -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nm.06sbp_k", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgj2z -> sports.sports_team_roster.team -> Los Angeles Bulldogs\n# Answer:\nm.0hpgj2z", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hpgh_h", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.season -> 1953 Major League Baseball season\n# Answer:\nm.06sbpz2", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h -> freebase.valuenotation.has_value -> Position\n# Answer:\nm.0hpgh_h", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0gggrzr -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nm.0gggrzr", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nm.06sbpzt"], "ground_truth": ["UCLA Bruins football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-580", "prediction": ["# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Libya\n# Answer:\nArabic Language", "# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> Arabic Language -> common.topic.notable_types -> Human Language\n# Answer:\nArabic Language", "# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> Arabic Language -> language.human_language.main_country -> Tunisia\n# Answer:\nArabic Language", "# Reasoning Path:\nAlgeria -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b71sdykm\n# Answer:\ng.11b71sdykm", "# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Mauritania\n# Answer:\nArabic Language", "# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> Proto-Berber language -> common.topic.article -> m.05zyx79\n# Answer:\nProto-Berber language", "# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Morocco\n# Answer:\nArabic Language", "# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> Proto-Berber language -> common.topic.notable_types -> Human Language\n# Answer:\nProto-Berber language", "# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> French -> language.human_language.region -> Europe\n# Answer:\nFrench"], "ground_truth": ["Western Berber languages", "Northern Berber languages", "Arabic Language", "Numidian language", "French", "Proto-Berber language", "Guanche Language"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5806451612903225, "ans_precission": 0.9, "ans_recall": 0.42857142857142855, "path_f1": 0.5806451612903225, "path_precision": 0.9, "path_recall": 0.42857142857142855, "path_ans_f1": 0.5806451612903225, "path_ans_precision": 0.9, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-581", "prediction": ["# Reasoning Path:\nAnne Boleyn -> common.topic.notable_types -> Noble person -> freebase.type_hints.included_types -> Person\n# Answer:\nNoble person", "# Reasoning Path:\nAnne Boleyn -> film.film_subject.films -> The Other Boleyn Girl -> film.film.featured_film_locations -> Elstree Studios\n# Answer:\nThe Other Boleyn Girl", "# Reasoning Path:\nAnne Boleyn -> common.topic.notable_types -> Noble person -> freebase.type_hints.included_types -> Topic\n# Answer:\nNoble person", "# Reasoning Path:\nAnne Boleyn -> common.topic.notable_types -> Noble person -> base.descriptive_names.names.descriptive_name -> m.012b_j83\n# Answer:\nNoble person", "# Reasoning Path:\nAnne Boleyn -> common.topic.notable_types -> Noble person -> freebase.type_profile.strict_included_types -> Person\n# Answer:\nNoble person", "# Reasoning Path:\nAnne Boleyn -> common.topic.notable_types -> Film -> type.type.properties -> Directed by\n# Answer:\nFilm", "# Reasoning Path:\nAnne Boleyn -> common.topic.notable_types -> Film -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nFilm", "# Reasoning Path:\nAnne Boleyn -> common.topic.notable_types -> Play -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nPlay", "# Reasoning Path:\nAnne Boleyn -> people.family_member.family -> Tudor dynasty -> people.family.members -> Eleanor Clifford, Countess of Cumberland\n# Answer:\nTudor dynasty", "# Reasoning Path:\nAnne Boleyn -> film.film_subject.films -> The Other Boleyn Girl -> film.film.starring -> m.02vbnc0\n# Answer:\nThe Other Boleyn Girl"], "ground_truth": ["Tower of London"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-582", "prediction": ["# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Daniel Faraday\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Emmett Brown\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.0101ddrk\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Mathematician -> base.descriptive_names.names.descriptive_name -> m.01066gkh\n# Answer:\nMathematician", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> J. Robert Oppenheimer\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> common.topic.image -> Albert Einstein 1947\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Astrologer -> base.descriptive_names.names.descriptive_name -> m.01067_zc\n# Answer:\nAstrologer", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Astrologer -> people.profession.specialization_of -> Prophet\n# Answer:\nAstrologer", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.01066smw\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Mathematician -> base.descriptive_names.names.descriptive_name -> m.010f3g_r\n# Answer:\nMathematician"], "ground_truth": ["Physicist", "Astronomer", "Astrologer", "Scientist", "Mathematician"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 1.0, "ans_recall": 0.6, "path_f1": 0.7499999999999999, "path_precision": 1.0, "path_recall": 0.6, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 1.0, "path_ans_recall": 0.6}
{"id": "WebQTest-583", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.02wp75f -> education.education.institution -> Boston University\n# Answer:\nm.02wp75f", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> education.education.institution -> Crozer Theological Seminary\n# Answer:\nm.0dh6jzz", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.04hddst -> education.education.institution -> Morehouse College\n# Answer:\nm.04hddst", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.02wp75f -> education.education.degree -> PhD\n# Answer:\nm.02wp75f", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> education.education.degree -> Bachelor of Divinity\n# Answer:\nm.0dh6jzz", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.04hddst -> education.education.degree -> Bachelor of Arts\n# Answer:\nm.04hddst", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nm.0dh6jzz", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nm.011j_4sh", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> freebase.valuenotation.has_value -> Minor\n# Answer:\nm.0dh6jzz", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> common.topic.article -> m.05nnh0\n# Answer:\nDexter Avenue Baptist Church"], "ground_truth": ["Boston University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-584", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Abhiyaza\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_for -> g.12582gf_5\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_types -> City/Town/Village\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Nanda\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> R\u0101hula\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism -> religion.religion.practices -> Astrology\n# Answer:\nBuddhism", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> location.location.geolocation -> m.0wmmlzp\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Tooth Relic Temple and Museum -> common.topic.webpage -> m.0gb1hxk\n# Answer:\nBuddha Tooth Relic Temple and Museum", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism -> religion.religion.practices -> Buddhist meditation\n# Answer:\nBuddhism", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism -> book.book_subject.works -> Fusang or the Discovery of America by Chinese Buddhist Priests in the Fifth Century\n# Answer:\nBuddhism"], "ground_truth": ["Kapilavastu"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-585", "prediction": ["# Reasoning Path:\nHenry VIII of England -> royalty.monarch.kingdom -> England -> location.country.currency_used -> Pound sterling\n# Answer:\nEngland", "# Reasoning Path:\nHenry VIII of England -> royalty.monarch.royal_line -> Tudor dynasty -> people.family.members -> Edmund Tudor, Duke of Somerset\n# Answer:\nTudor dynasty", "# Reasoning Path:\nHenry VIII of England -> royalty.monarch.kingdom -> England -> location.country.languages_spoken -> Cornish Language\n# Answer:\nEngland", "# Reasoning Path:\nHenry VIII of England -> royalty.monarch.royal_line -> Tudor dynasty -> people.family.country -> Principality of Wales\n# Answer:\nTudor dynasty", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.nationality -> England\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> royalty.monarch.kingdom -> England -> base.aareas.schema.administrative_area.administrative_children -> North East England\n# Answer:\nEngland", "# Reasoning Path:\nHenry VIII of England -> royalty.monarch.royal_line -> Tudor dynasty -> people.family.founder -> Henry VII of England\n# Answer:\nTudor dynasty", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> common.topic.article -> m.02sx6l\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> royalty.monarch.kingdom -> England -> location.country.languages_spoken -> English Language\n# Answer:\nEngland", "# Reasoning Path:\nHenry VIII of England -> royalty.monarch.royal_line -> Tudor dynasty -> people.family.country -> Kingdom of England\n# Answer:\nTudor dynasty"], "ground_truth": ["England"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-586", "prediction": ["# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Leukemia -> people.cause_of_death.parent_cause_of_death -> Hibakusha\n# Answer:\nLeukemia", "# Reasoning Path:\nLarry Hagman -> film.actor.film -> g.11b7563h44\n# Answer:\ng.11b7563h44", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Leukemia -> people.cause_of_death.parent_cause_of_death -> Cancer\n# Answer:\nLeukemia", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nCancer", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.medical_specialties -> Oncology\n# Answer:\nCancer", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Leukemia -> medicine.disease.symptoms -> Leukemia cutis\n# Answer:\nLeukemia", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Leukemia -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nLeukemia", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Cancer -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nCancer", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Leukemia -> medicine.disease.symptoms -> Arthritis\n# Answer:\nLeukemia", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.treatments -> Radiation therapy\n# Answer:\nCancer"], "ground_truth": ["Head and neck cancer", "Leukemia"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-588", "prediction": ["# Reasoning Path:\nRobert E. Lee -> military.military_commander.military_commands -> m.04fv8sm -> military.military_command.military_conflict -> Battle of Appomattox Court House\n# Answer:\nm.04fv8sm", "# Reasoning Path:\nRobert E. Lee -> military.military_commander.military_commands -> m.0h1l91r -> military.military_command.military_conflict -> Western Virginia Campaign\n# Answer:\nm.0h1l91r", "# Reasoning Path:\nRobert E. Lee -> military.military_commander.military_commands -> m.04yxf4y -> military.military_command.military_conflict -> Northern Virginia Campaign\n# Answer:\nm.04yxf4y", "# Reasoning Path:\nRobert E. Lee -> people.person.sibling_s -> m.0w4gc9q -> people.sibling_relationship.sibling -> Sydney Smith Lee\n# Answer:\nm.0w4gc9q", "# Reasoning Path:\nRobert E. Lee -> military.military_commander.military_commands -> m.04yxf4y -> military.military_command.military_combatant -> Confederate States of America\n# Answer:\nm.04yxf4y", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> General Lee's Army -> book.written_work.subjects -> American Civil War\n# Answer:\nGeneral Lee's Army", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> General Lee's Army -> book.book.editions -> General Lee's Army: From Victory to Collapse\n# Answer:\nGeneral Lee's Army", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> Robert E. Lee's Civil War -> book.written_work.subjects -> American Civil War\n# Answer:\nRobert E. Lee's Civil War", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> General Lee's Army -> book.written_work.author -> Joseph Glatthaar\n# Answer:\nGeneral Lee's Army", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> General Lee's Army -> book.written_work.subjects -> Army of Northern Virginia\n# Answer:\nGeneral Lee's Army"], "ground_truth": ["Gettysburg Campaign", "John Brown's raid on Harpers Ferry", "Third Battle of Petersburg", "Siege of Petersburg", "Second Battle of Deep Bottom", "Battle of Mine Run", "Battle of Fredericksburg", "Battle of South Mountain", "Battle of Frederick", "Battle of Cold Harbor", "Battle of Antietam", "American Civil War", "Battle of the Wilderness", "Second Battle of Rappahannock Station", "Battle of Malvern Hill", "Battle of Williamsport", "Maryland Campaign", "Battle of Gettysburg", "Mexican\u2013American War", "Battle of Darbytown and New Market Roads", "Seven Days Battles", "Richmond in the American Civil War", "Battle of Franklin's Crossing", "Battle of Totopotomoy Creek", "Siege of Fort Pulaski", "Overland Campaign", "Battle of Fort Stedman", "Battle of Spotsylvania Court House", "Western Virginia Campaign", "Battle of Cumberland Church", "Stoneman's 1863 Raid", "Battle of the Crater", "Second Battle of Bull Run", "Battle of Chaffin's Farm", "Battle of White Oak Road", "Northern Virginia Campaign", "Battle of Beaver Dam Creek", "Battle of Gaines's Mill", "Battle of Glendale", "Battle of Cheat Mountain", "Battle of Oak Grove", "Battle of Appomattox Court House", "Second Battle of Petersburg", "Battle of Chancellorsville"], "ans_acc": 0.09090909090909091, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.11111111111111109, "path_precision": 0.3, "path_recall": 0.06818181818181818, "path_ans_f1": 0.15384615384615385, "path_ans_precision": 0.5, "path_ans_recall": 0.09090909090909091}
{"id": "WebQTest-589", "prediction": ["# Reasoning Path:\nRussia -> location.country.languages_spoken -> Adyghe Language -> language.human_language.language_family -> North Caucasian languages\n# Answer:\nAdyghe Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Abaza Language -> language.human_language.region -> Europe\n# Answer:\nAbaza Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Adyghe Language -> language.human_language.language_family -> Northwest Caucasian languages\n# Answer:\nAdyghe Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> language.human_language.countries_spoken_in -> Belarus\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> common.topic.notable_types -> Human Language\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.11b60vv5th\n# Answer:\ng.11b60vv5th", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Osetin Language -> language.human_language.main_country -> Georgia\n# Answer:\nOsetin Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> language.human_language.region -> Eurasia\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> language.human_language.countries_spoken_in -> Georgia\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Osetin Language -> common.topic.notable_types -> Human Language\n# Answer:\nOsetin Language"], "ground_truth": ["Russian Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> people.person.place_of_birth -> Ossining\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.story_by -> Harold Gray -> comic_strips.comic_strip_creator.comic_strips_written -> m.0gwbjrm\n# Answer:\nHarold Gray", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> award.award_nominee.award_nominations -> m.010bvypw\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> award.award_winner.awards_won -> m.0_5xrrh\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.release_date_s -> m.0jsmxbk -> film.film_regional_release_date.film_release_region -> United States of America\n# Answer:\nm.0jsmxbk", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> award.award_nominee.award_nominations -> m.010by0k1\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.story_by -> Harold Gray -> people.person.nationality -> United States of America\n# Answer:\nHarold Gray", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Choreographer\n# Answer:\nChoreographer", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> award.award_nominee.award_nominations -> m.010d5qmn\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> award.award_winner.awards_won -> m.0_g8mdk\n# Answer:\nThomas Meehan"], "ground_truth": ["1976-08-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-590", "prediction": ["# Reasoning Path:\nLibya -> government.governmental_jurisdiction.governing_officials -> m.0105y7bt -> government.government_position_held.office_holder -> Abdullah al-Thani\n# Answer:\nm.0105y7bt", "# Reasoning Path:\nLibya -> government.governmental_jurisdiction.governing_officials -> m.010gfn62 -> government.government_position_held.office_holder -> Ahmed Maiteeq\n# Answer:\nm.010gfn62", "# Reasoning Path:\nLibya -> government.governmental_jurisdiction.governing_officials -> m.011n4mp5 -> government.government_position_held.office_holder -> Abdullah al-Thani\n# Answer:\nm.011n4mp5", "# Reasoning Path:\nLibya -> location.location.events -> Factional violence in Libya -> military.military_conflict.military_personnel_involved -> Khalifa Haftar\n# Answer:\nFactional violence in Libya", "# Reasoning Path:\nLibya -> location.statistical_region.co2_emissions_per_capita -> g.1245_1hym\n# Answer:\ng.1245_1hym", "# Reasoning Path:\nLibya -> government.governmental_jurisdiction.governing_officials -> m.0105y7bt -> government.government_position_held.office_position_or_title -> Prime Minister of Libya\n# Answer:\nm.0105y7bt", "# Reasoning Path:\nLibya -> location.statistical_region.co2_emissions_per_capita -> g.1245_2gxq\n# Answer:\ng.1245_2gxq", "# Reasoning Path:\nLibya -> government.governmental_jurisdiction.governing_officials -> m.010gfn62 -> government.government_position_held.office_position_or_title -> Prime Minister of Libya\n# Answer:\nm.010gfn62", "# Reasoning Path:\nLibya -> government.governmental_jurisdiction.governing_officials -> m.011n4mp5 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.011n4mp5", "# Reasoning Path:\nLibya -> location.location.events -> Factional violence in Libya -> time.event.includes_event -> 2012 Bani Walid uprising\n# Answer:\nFactional violence in Libya"], "ground_truth": ["Abdullah al-Thani"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-592", "prediction": ["# Reasoning Path:\nShays' Rebellion -> time.event.locations -> Massachusetts -> location.location.events -> 2014 Boston Tornado\n# Answer:\nMassachusetts", "# Reasoning Path:\nShays' Rebellion -> time.event.locations -> Massachusetts -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMassachusetts", "# Reasoning Path:\nShays' Rebellion -> time.event.locations -> Massachusetts -> location.location.events -> Actions of 5\u20136 May 1945\n# Answer:\nMassachusetts", "# Reasoning Path:\nShays' Rebellion -> base.schemastaging.context_name.pronunciation -> g.125_lt26z\n# Answer:\ng.125_lt26z", "# Reasoning Path:\nShays' Rebellion -> time.event.locations -> Massachusetts -> location.location.events -> Chappaquiddick Incident\n# Answer:\nMassachusetts", "# Reasoning Path:\nShays' Rebellion -> common.topic.notable_types -> Event -> freebase.type_profile.published -> Published\n# Answer:\nEvent", "# Reasoning Path:\nShays' Rebellion -> time.event.locations -> Massachusetts -> symbols.name_source.namesakes -> 4547 Massachusetts\n# Answer:\nMassachusetts", "# Reasoning Path:\nShays' Rebellion -> time.event.locations -> Massachusetts -> symbols.name_source.namesakes -> USS Massachusetts\n# Answer:\nMassachusetts", "# Reasoning Path:\nShays' Rebellion -> common.topic.notable_types -> Event -> type.type.expected_by -> event\n# Answer:\nEvent", "# Reasoning Path:\nShays' Rebellion -> common.topic.notable_types -> Event -> type.type.properties -> Comment\n# Answer:\nEvent"], "ground_truth": ["1786"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-593", "prediction": ["# Reasoning Path:\nMontel Williams -> common.topic.article -> m.018pv0\n# Answer:\nm.018pv0", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> medicine.disease.symptoms -> Fasciculation\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> people.cause_of_death.people -> Frieda Inescort\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> medicine.disease.notable_people_with_this_condition -> Frieda Inescort\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> medicine.disease.symptoms -> Amnesia\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> medicine.disease.symptoms -> Anxiety\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> people.cause_of_death.people -> Anne Volant Rowling\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> medicine.disease.notable_people_with_this_condition -> Fiona Mactaggart\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> people.cause_of_death.people -> Annette Funicello\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> medicine.disease.notable_people_with_this_condition -> Aaron Solowoniuk\n# Answer:\nMultiple sclerosis"], "ground_truth": ["Multiple sclerosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-595", "prediction": ["# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.article -> m.0642vqz\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> sports.sports_championship_event.season -> 2000 NFL season\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> sports.sports_championship_event.season -> 2012 NFL season\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> 2013 AFC Championship Game -> time.event.instance_of_recurring_event -> AFC Championship Game\n# Answer:\n2013 AFC Championship Game", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> common.topic.notable_types -> Super bowl\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> film.film_subject.films -> The Band That Wouldn't Die -> film.film.music -> Marcello Zavros\n# Answer:\nThe Band That Wouldn't Die", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> time.event.locations -> Raymond James Stadium\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> 2013 AFC Championship Game -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n2013 AFC Championship Game", "# Reasoning Path:\nBaltimore Ravens -> film.film_subject.films -> The Band That Wouldn't Die -> common.topic.image -> Baltimore Ravens Marching Band\n# Answer:\nThe Band That Wouldn't Die"], "ground_truth": ["Super Bowl XLVII"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-596", "prediction": ["# Reasoning Path:\nKansas City Royals -> sports.sports_team.location -> Kansas City -> location.location.containedby -> Missouri\n# Answer:\nKansas City", "# Reasoning Path:\nKansas City Royals -> organization.organization.leadership -> m.0z45pkh -> organization.leadership.person -> David D. Glass\n# Answer:\nm.0z45pkh", "# Reasoning Path:\nKansas City Royals -> sports.sports_team.location -> Kansas City -> location.location.containedby -> United States of America\n# Answer:\nKansas City", "# Reasoning Path:\nKansas City Royals -> sports.sports_team.location -> Kansas City -> travel.travel_destination.tourist_attractions -> 18th and Vine-Downtown East\n# Answer:\nKansas City", "# Reasoning Path:\nKansas City Royals -> sports.sports_team.location -> Kansas City -> location.place_with_neighborhoods.neighborhoods -> 18th and Vine-Downtown East\n# Answer:\nKansas City", "# Reasoning Path:\nKansas City Royals -> organization.organization.leadership -> m.0z45pkh -> freebase.valuenotation.has_value -> From\n# Answer:\nm.0z45pkh", "# Reasoning Path:\nKansas City Royals -> sports.sports_team.location -> Kansas City -> travel.travel_destination.tourist_attractions -> Crown Center\n# Answer:\nKansas City", "# Reasoning Path:\nKansas City Royals -> sports.sports_team.league -> m.0crt4ht -> sports.sports_league_participation.league -> Major League Baseball\n# Answer:\nm.0crt4ht", "# Reasoning Path:\nKansas City Royals -> sports.sports_team.league -> m.0crtd4s -> sports.sports_league_participation.league -> American League Central\n# Answer:\nm.0crtd4s", "# Reasoning Path:\nKansas City Royals -> organization.organization.leadership -> m.0z45pkh -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0z45pkh"], "ground_truth": ["Kansas City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-597", "prediction": ["# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Folk rock -> music.genre.parent_genre -> Folk music\n# Answer:\nFolk rock", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Soul music -> music.genre.subgenre -> Blue-eyed soul\n# Answer:\nSoul music", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Soul music -> music.genre.parent_genre -> Blues\n# Answer:\nSoul music", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Blues -> music.genre.parent_genre -> Folk music\n# Answer:\nBlues", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Soul music -> music.genre.subgenre -> acoustic pop\n# Answer:\nSoul music", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Soul music -> broadcast.genre.content -> ACB Radio Interactive\n# Answer:\nSoul music", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Folk rock -> music.genre.parent_genre -> Rock music\n# Answer:\nFolk rock", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Folk rock -> music.genre.subgenre -> Jam band\n# Answer:\nFolk rock", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Folk rock -> common.topic.notable_types -> Musical genre\n# Answer:\nFolk rock", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Soul music -> broadcast.genre.content -> SoulfulSmoothJazz.com\n# Answer:\nSoul music"], "ground_truth": ["Folk music", "Soul rock", "Rock music", "Pop rock", "Soul music", "Alternative rock", "Blue-eyed soul", "Blues rock", "Country", "Pop music", "Indie rock", "Soft rock", "Acoustic music", "Folk rock", "Country rock", "Blues"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.3157894736842105, "ans_precission": 1.0, "ans_recall": 0.1875, "path_f1": 0.3157894736842105, "path_precision": 1.0, "path_recall": 0.1875, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 1.0, "path_ans_recall": 0.375}
{"id": "WebQTest-599", "prediction": ["# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Mimeograph -> freebase.valuenotation.is_reviewed -> Inventor\n# Answer:\nMimeograph", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Carbon microphone -> common.topic.article -> m.0d9fl1\n# Answer:\nCarbon microphone", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Mimeograph -> common.topic.notable_types -> Invention\n# Answer:\nMimeograph", "# Reasoning Path:\nThomas Edison -> common.image.size -> m.01xjmzn\n# Answer:\nm.01xjmzn", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Carbon microphone -> common.topic.notable_types -> Invention\n# Answer:\nCarbon microphone", "# Reasoning Path:\nThomas Edison -> common.image.size -> m.02bjm0y\n# Answer:\nm.02bjm0y", "# Reasoning Path:\nThomas Edison -> common.image.size -> m.02ctc4t\n# Answer:\nm.02ctc4t", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Electric Power Distribution -> business.industry.parent_industry -> Electric Power Transmission, Control, and Distribution\n# Answer:\nElectric Power Distribution", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Electric Power Distribution -> common.topic.image -> 11kV/400V-230V transformer in an older suburb of Wellington, New Zealand\n# Answer:\nElectric Power Distribution", "# Reasoning Path:\nThomas Edison -> film.producer.film -> Annie Oakley -> film.film.produced_by -> William Kennedy Dickson\n# Answer:\nAnnie Oakley"], "ground_truth": ["Mimeograph", "Carbon microphone", "Movie camera", "Phonograph cylinder", "Electric Power Distribution", "Quadruplex telegraph", "Phonograph", "Incandescent light bulb"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.6, "ans_recall": 0.375, "path_f1": 0.4615384615384615, "path_precision": 0.6, "path_recall": 0.375, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.6, "path_ans_recall": 0.375}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Alabama\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> United States of America\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Area code 251\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07ghzrz\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b66g7msm\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.education -> m.03gkqtp -> education.education.institution -> Louisiana State University\n# Answer:\nm.03gkqtp", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b7tn_fd2\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11btt6sf_l\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07919ln -> american_football.player_passing_statistics.team -> Oakland Raiders\n# Answer:\nm.07919ln", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07919ln -> american_football.player_passing_statistics.season -> 2008 NFL season\n# Answer:\nm.07919ln"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> base.aareas.schema.administrative_area.administrative_parent -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.administrative_division.country -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Anna Roosevelt Halsted -> people.deceased_person.cause_of_death -> Laryngeal cancer\n# Answer:\nAnna Roosevelt Halsted", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Elliott Roosevelt -> people.deceased_person.place_of_death -> Scottsdale\n# Answer:\nElliott Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Autobiography of Eleanor Roosevelt -> common.topic.notable_types -> Book Edition\n# Answer:\nAutobiography of Eleanor Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Courage in a dangerous world -> book.book_edition.isbn -> 9780231111812\n# Answer:\nCourage in a dangerous world", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Autobiography of Eleanor Roosevelt -> common.topic.notable_for -> g.125b58fny\n# Answer:\nAutobiography of Eleanor Roosevelt"], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-600", "prediction": ["# Reasoning Path:\nNovak Djokovic -> people.person.place_of_birth -> Belgrade -> periodicals.newspaper_circulation_area.newspapers -> Danas\n# Answer:\nBelgrade", "# Reasoning Path:\nNovak Djokovic -> people.person.place_of_birth -> Belgrade -> location.location.contains -> Belgrade Nikola Tesla Airport\n# Answer:\nBelgrade", "# Reasoning Path:\nNovak Djokovic -> people.person.place_of_birth -> Belgrade -> base.biblioness.bibs_location.country -> Yugoslavia\n# Answer:\nBelgrade", "# Reasoning Path:\nNovak Djokovic -> people.person.place_of_birth -> Belgrade -> periodicals.newspaper_circulation_area.newspapers -> Politika\n# Answer:\nBelgrade", "# Reasoning Path:\nNovak Djokovic -> people.person.place_of_birth -> Belgrade -> periodicals.newspaper_circulation_area.newspapers -> Start\n# Answer:\nBelgrade", "# Reasoning Path:\nNovak Djokovic -> people.person.nationality -> Serbia -> sports.sports_team_location.teams -> KK Crvena zvezda\n# Answer:\nSerbia", "# Reasoning Path:\nNovak Djokovic -> people.person.nationality -> Serbia -> sports.sports_team_location.teams -> KK Mega Basket\n# Answer:\nSerbia", "# Reasoning Path:\nNovak Djokovic -> people.person.nationality -> Serbia -> location.location.containedby -> Europe\n# Answer:\nSerbia", "# Reasoning Path:\nNovak Djokovic -> people.person.nationality -> Serbia -> sports.sports_team_location.teams -> KK Partizan\n# Answer:\nSerbia", "# Reasoning Path:\nNovak Djokovic -> people.person.nationality -> Serbia -> sports.sport_country.athletic_performances -> m.0fp9_n9\n# Answer:\nSerbia"], "ground_truth": ["Serbia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-602", "prediction": ["# Reasoning Path:\nNew England -> location.location.containedby -> Eastern United States -> location.location.contains -> Appalachia\n# Answer:\nEastern United States", "# Reasoning Path:\nNew England -> location.location.contains -> Massachusetts -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMassachusetts", "# Reasoning Path:\nNew England -> location.location.containedby -> Eastern United States -> location.location.contains -> Appalachian National Scenic Trail\n# Answer:\nEastern United States", "# Reasoning Path:\nNew England -> location.location.containedby -> Northeastern United States -> location.location.contains -> Hillsborough County\n# Answer:\nNortheastern United States", "# Reasoning Path:\nNew England -> location.location.contains -> Massachusetts -> location.location.containedby -> Contiguous United States\n# Answer:\nMassachusetts", "# Reasoning Path:\nNew England -> location.location.contains -> New Hampshire -> location.location.containedby -> Contiguous United States\n# Answer:\nNew Hampshire", "# Reasoning Path:\nNew England -> location.location.containedby -> Eastern United States -> location.location.contains -> Indiana\n# Answer:\nEastern United States", "# Reasoning Path:\nNew England -> location.location.containedby -> Eastern United States -> location.location.adjoin_s -> m.0h2t9bz\n# Answer:\nEastern United States", "# Reasoning Path:\nNew England -> location.location.containedby -> Eastern United States -> meteorology.cyclone_affected_area.cyclones -> 1915 Galveston hurricane\n# Answer:\nEastern United States", "# Reasoning Path:\nNew England -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Massachusetts\n# Answer:\nUnited States of America"], "ground_truth": ["Northeastern United States"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-603", "prediction": ["# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Revaluation of the Turkish Lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nRevaluation of the Turkish Lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.065sxsl -> military.military_combatant_group.conflict -> Korean War\n# Answer:\nm.065sxsl", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.066402y -> military.military_combatant_group.conflict -> Iraqi no-fly zones\n# Answer:\nm.066402y"], "ground_truth": ["Turkish lira"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-604", "prediction": ["# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Acid rock -> common.topic.notable_for -> g.1257tm1jy\n# Answer:\nAcid rock", "# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Funk rock -> music.genre.parent_genre -> Rock music\n# Answer:\nFunk rock", "# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Acid rock -> common.topic.notable_types -> Musical genre\n# Answer:\nAcid rock", "# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Acid rock -> common.topic.webpage -> m.09xphgr\n# Answer:\nAcid rock", "# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Funk rock -> common.topic.notable_types -> Musical genre\n# Answer:\nFunk rock", "# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Blues -> music.genre.subgenre -> Garage rock\n# Answer:\nBlues", "# Reasoning Path:\nJimi Hendrix -> music.artist.label -> Barclay Records -> music.record_label.artist -> The Jimi Hendrix Experience\n# Answer:\nBarclay Records", "# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Funk rock -> music.genre.subgenre -> Dance-punk\n# Answer:\nFunk rock", "# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Blues -> music.genre.subgenre -> Progressive rock\n# Answer:\nBlues", "# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Blues -> broadcast.genre.content -> 1.FM Blues\n# Answer:\nBlues"], "ground_truth": ["Rock music", "Funk rock", "Blues rock", "Heavy metal", "Rhythm and blues", "Progressive rock", "Acid rock", "Blues", "Experimental rock", "Psychedelic rock", "Hard rock", "Psychedelia"], "ans_acc": 0.4166666666666667, "ans_hit": 1, "ans_f1": 0.391304347826087, "ans_precission": 0.9, "ans_recall": 0.25, "path_f1": 0.391304347826087, "path_precision": 0.9, "path_recall": 0.25, "path_ans_f1": 0.569620253164557, "path_ans_precision": 0.9, "path_ans_recall": 0.4166666666666667}
{"id": "WebQTest-605", "prediction": ["# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgmw -> education.education.degree -> Bachelor of Arts\n# Answer:\nm.02nqgmw", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgmw -> education.education.major_field_of_study -> Political Science\n# Answer:\nm.02nqgmw", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgp6 -> education.education.institution -> Punahou School\n# Answer:\nm.02nqgp6", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> freebase.valuenotation.has_value -> Degree\n# Answer:\nm.02nqgpk", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> education.education.institution -> Occidental College\n# Answer:\nm.02nqgpk", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nm.02nqgpk", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.subjects -> Mitt Romney presidential campaign, 2012\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Bound Man -> book.written_work.subjects -> African-American studies\n# Answer:\nA Bound Man", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> freebase.valuenotation.has_value -> Minor\n# Answer:\nm.02nqgpk", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Writer's Journey to Selma, Alabama -> book.written_work.subjects -> Selma\n# Answer:\nA Writer's Journey to Selma, Alabama"], "ground_truth": ["Juris Doctor", "Bachelor of Arts"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 0.1, "path_ans_recall": 0.5}
{"id": "WebQTest-606", "prediction": ["# Reasoning Path:\nAlexander Graham Bell -> people.person.sibling_s -> m.0w17rt4 -> people.sibling_relationship.sibling -> Edward Charles Bell\n# Answer:\nm.0w17rt4", "# Reasoning Path:\nAlexander Graham Bell -> people.person.children -> Edward Bell -> people.person.parents -> Mabel Gardiner Hubbard\n# Answer:\nEdward Bell", "# Reasoning Path:\nAlexander Graham Bell -> people.person.sibling_s -> m.0k257qs -> people.sibling_relationship.sibling -> Melville James Bell\n# Answer:\nm.0k257qs", "# Reasoning Path:\nAlexander Graham Bell -> people.person.children -> Elsie Bell -> people.person.sibling_s -> m.0w17p51\n# Answer:\nElsie Bell", "# Reasoning Path:\nAlexander Graham Bell -> people.person.children -> Edward Bell -> people.person.sibling_s -> m.0w17pb4\n# Answer:\nEdward Bell", "# Reasoning Path:\nAlexander Graham Bell -> people.person.children -> Elsie Bell -> people.person.parents -> Mabel Gardiner Hubbard\n# Answer:\nElsie Bell", "# Reasoning Path:\nAlexander Graham Bell -> people.person.children -> Marian Hubbard Bell -> people.person.sibling_s -> m.0w17p51\n# Answer:\nMarian Hubbard Bell", "# Reasoning Path:\nAlexander Graham Bell -> people.person.children -> Marian Hubbard Bell -> people.person.parents -> Mabel Gardiner Hubbard\n# Answer:\nMarian Hubbard Bell", "# Reasoning Path:\nAlexander Graham Bell -> people.person.children -> Edward Bell -> people.person.sibling_s -> m.0w1c1b0\n# Answer:\nEdward Bell", "# Reasoning Path:\nAlexander Graham Bell -> people.person.children -> Elsie Bell -> people.person.sibling_s -> m.0w17pb4\n# Answer:\nElsie Bell"], "ground_truth": ["Edward Charles Bell", "Melville James Bell"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.2, "path_recall": 0.3333333333333333, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-608", "prediction": ["# Reasoning Path:\nLiverpool F.C. -> sports.sports_team.championships -> 2012 Football League Cup Final -> sports.sports_championship_event.championship -> Football League Cup\n# Answer:\n2012 Football League Cup Final", "# Reasoning Path:\nLiverpool F.C. -> sports.sports_team.championships -> 2012 Football League Cup Final -> freebase.valuenotation.has_no_value -> Comment\n# Answer:\n2012 Football League Cup Final", "# Reasoning Path:\nLiverpool F.C. -> sports.sports_team.championships -> 2012 Football League Cup Final -> soccer.football_match.held_at -> Wembley Stadium\n# Answer:\n2012 Football League Cup Final", "# Reasoning Path:\nLiverpool F.C. -> sports.sports_team.championships -> 1964\u201365 FA Cup -> sports.sports_league_season.awards -> m.05l1m64\n# Answer:\n1964\u201365 FA Cup", "# Reasoning Path:\nLiverpool F.C. -> business.employer.employees -> m.05l1p44 -> business.employment_tenure.title -> Assistant manager\n# Answer:\nm.05l1p44", "# Reasoning Path:\nLiverpool F.C. -> sports.sports_team.championships -> 1964\u201365 FA Cup -> sports.sports_league_season.championship -> 1965 FA Cup Final\n# Answer:\n1964\u201365 FA Cup", "# Reasoning Path:\nLiverpool F.C. -> sports.sports_team.championships -> 1965 FA Cup Final -> soccer.football_match.teams -> Leeds United F.C.\n# Answer:\n1965 FA Cup Final", "# Reasoning Path:\nLiverpool F.C. -> sports.sports_team.championships -> 1965 FA Cup Final -> sports.sports_championship_event.championship -> FA Cup\n# Answer:\n1965 FA Cup Final", "# Reasoning Path:\nLiverpool F.C. -> business.employer.employees -> m.05l1p44 -> business.employment_tenure.person -> Colin Pascoe\n# Answer:\nm.05l1p44", "# Reasoning Path:\nLiverpool F.C. -> business.employer.employees -> m.05l1p4d -> business.employment_tenure.person -> Gary Ablett\n# Answer:\nm.05l1p4d"], "ground_truth": ["2012 Football League Cup Final"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-609", "prediction": ["# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.regular_cast -> m.02t8rjb -> tv.regular_tv_appearance.actor -> Bill Daily\n# Answer:\nm.02t8rjb", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.regular_cast -> m.02t90y8 -> tv.regular_tv_appearance.actor -> Hayden Rorke\n# Answer:\nm.02t90y8", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.regular_cast -> m.02t8sfq -> tv.regular_tv_appearance.actor -> Larry Hagman\n# Answer:\nm.02t8sfq", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.regular_cast -> m.02t8rjb -> tv.regular_tv_appearance.character -> Roger Healey\n# Answer:\nm.02t8rjb", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.theme_song -> Jeannie -> tv.tv_program.regular_cast -> m.03l6nh7\n# Answer:\nJeannie", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.regular_cast -> m.02t8rjb -> tv.regular_tv_appearance.seasons -> I Dream of Jeannie, Season 1\n# Answer:\nm.02t8rjb", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.tv_producer -> m.05kk2dn -> tv.tv_producer_term.producer -> Sidney Sheldon\n# Answer:\nm.05kk2dn", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.regular_cast -> m.02t8sfq -> tv.regular_tv_appearance.character -> Tony Nelson\n# Answer:\nm.02t8sfq", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.theme_song -> Jeannie -> tv.tv_program.regular_cast -> m.03l6nhd\n# Answer:\nJeannie", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.regular_cast -> m.02t8rjb -> tv.regular_tv_appearance.seasons -> I Dream of Jeannie, Season 2\n# Answer:\nm.02t8rjb"], "ground_truth": ["Barbara Eden"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf2_w -> location.religion_percentage.religion -> Islam\n# Answer:\nm.03xf2_w", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.064szjw -> location.religion_percentage.religion -> Hinduism\n# Answer:\nm.064szjw", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60ptk2z\n# Answer:\ng.11b60ptk2z", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf301 -> location.religion_percentage.religion -> Protestantism\n# Answer:\nm.03xf301", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.12cp_j7n1\n# Answer:\ng.12cp_j7n1", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_4m6h\n# Answer:\ng.1245_4m6h", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_67l9\n# Answer:\ng.1245_67l9", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.1hhc378kt\n# Answer:\ng.1hhc378kt", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.1hhc38d0l\n# Answer:\ng.1hhc38d0l"], "ground_truth": ["Catholicism", "Islam", "Protestantism", "Hinduism"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.42857142857142855, "path_precision": 0.3333333333333333, "path_recall": 0.6, "path_ans_f1": 0.46153846153846156, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.75}
{"id": "WebQTest-611", "prediction": ["# Reasoning Path:\nSaint Peter -> people.deceased_person.place_of_death -> Rome -> location.location.time_zones -> Central European Time Zone\n# Answer:\nRome", "# Reasoning Path:\nSaint Peter -> people.deceased_person.place_of_death -> Rome -> base.biblioness.bibs_location.country -> Italy\n# Answer:\nRome", "# Reasoning Path:\nSaint Peter -> people.deceased_person.place_of_death -> Rome -> travel.travel_destination.tourist_attractions -> Sistine Chapel\n# Answer:\nRome", "# Reasoning Path:\nSaint Peter -> people.deceased_person.place_of_death -> Rome -> travel.travel_destination.tourist_attractions -> St. Peter's Basilica\n# Answer:\nRome", "# Reasoning Path:\nSaint Peter -> people.deceased_person.place_of_death -> Rome -> travel.travel_destination.tourist_attractions -> Santa Maria sopra Minerva\n# Answer:\nRome", "# Reasoning Path:\nSaint Peter -> symbols.name_source.namesakes -> Bodrum Castle -> common.topic.article -> m.04x3dl\n# Answer:\nBodrum Castle", "# Reasoning Path:\nSaint Peter -> symbols.name_source.namesakes -> Bodrum Castle -> travel.tourist_attraction.near_travel_destination -> Bodrum\n# Answer:\nBodrum Castle", "# Reasoning Path:\nSaint Peter -> symbols.name_source.namesakes -> Cathedral of Saint Peter -> architecture.structure.architectural_style -> Gothic Revival architecture\n# Answer:\nCathedral of Saint Peter", "# Reasoning Path:\nSaint Peter -> visual_art.art_subject.artwork_on_the_subject -> Deliverance of Saint Peter -> common.topic.notable_for -> g.125gc_v9s\n# Answer:\nDeliverance of Saint Peter", "# Reasoning Path:\nSaint Peter -> visual_art.art_subject.artwork_on_the_subject -> The Crucifixion of St. Peter -> visual_art.artwork.art_subject -> Crucifixion\n# Answer:\nThe Crucifixion of St. Peter"], "ground_truth": ["Rome"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-612", "prediction": ["# Reasoning Path:\nJeremy Shockey -> american_football.football_player.receiving -> m.0791gjj -> american_football.player_receiving_statistics.team -> New Orleans Saints\n# Answer:\nm.0791gjj", "# Reasoning Path:\nJeremy Shockey -> american_football.football_player.receiving -> m.07mmpkl -> american_football.player_receiving_statistics.team -> New Orleans Saints\n# Answer:\nm.07mmpkl", "# Reasoning Path:\nJeremy Shockey -> american_football.football_player.receiving -> m.07sh69_ -> american_football.player_receiving_statistics.team -> New Orleans Saints\n# Answer:\nm.07sh69_", "# Reasoning Path:\nJeremy Shockey -> american_football.football_player.receiving -> m.0791gjj -> american_football.player_receiving_statistics.season -> 2008 NFL season\n# Answer:\nm.0791gjj", "# Reasoning Path:\nJeremy Shockey -> american_football.football_player.receiving -> m.07mmpkl -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\nm.07mmpkl", "# Reasoning Path:\nJeremy Shockey -> sports.pro_athlete.teams -> m.0hqff4t -> sports.sports_team_roster.team -> New Orleans Saints\n# Answer:\nm.0hqff4t", "# Reasoning Path:\nJeremy Shockey -> american_football.football_player.games -> m.07nvpw4 -> american_football.player_game_statistics.season -> 2008 NFL season\n# Answer:\nm.07nvpw4", "# Reasoning Path:\nJeremy Shockey -> sports.pro_athlete.teams -> m.0hqff4t -> sports.sports_team_roster.position -> Tight end\n# Answer:\nm.0hqff4t", "# Reasoning Path:\nJeremy Shockey -> american_football.football_player.receiving -> m.07sh69_ -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\nm.07sh69_", "# Reasoning Path:\nJeremy Shockey -> sports.pro_athlete.teams -> m.0hqff53 -> sports.sports_team_roster.team -> New York Giants\n# Answer:\nm.0hqff53"], "ground_truth": ["Carolina Panthers"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-613", "prediction": ["# Reasoning Path:\nDeion Sanders -> people.person.education -> m.02kygcm -> education.education.institution -> Florida State University\n# Answer:\nm.02kygcm", "# Reasoning Path:\nDeion Sanders -> people.person.education -> m.04_7nf0 -> education.education.institution -> North Fort Myers High School\n# Answer:\nm.04_7nf0", "# Reasoning Path:\nDeion Sanders -> base.popstra.celebrity.operation -> m.064ffy8 -> base.popstra.operation.procedure -> Crescent Court\n# Answer:\nm.064ffy8", "# Reasoning Path:\nDeion Sanders -> baseball.baseball_player.position_s -> Center fielder -> sports.sports_position.players -> g.1q4kq6m3k\n# Answer:\nCenter fielder", "# Reasoning Path:\nDeion Sanders -> baseball.baseball_player.position_s -> Center fielder -> common.topic.article -> m.02sg4k\n# Answer:\nCenter fielder", "# Reasoning Path:\nDeion Sanders -> baseball.baseball_player.position_s -> Center fielder -> sports.sports_position.sport -> Baseball\n# Answer:\nCenter fielder", "# Reasoning Path:\nDeion Sanders -> baseball.baseball_player.position_s -> Outfielder -> common.topic.notable_types -> Baseball Position\n# Answer:\nOutfielder", "# Reasoning Path:\nDeion Sanders -> baseball.baseball_player.position_s -> Center fielder -> sports.sports_position.players -> g.1yx1wbd16\n# Answer:\nCenter fielder", "# Reasoning Path:\nDeion Sanders -> baseball.baseball_player.position_s -> Left fielder -> sports.sports_position.players -> g.1q4kq6m4l\n# Answer:\nLeft fielder", "# Reasoning Path:\nDeion Sanders -> baseball.baseball_player.position_s -> Left fielder -> base.schemastaging.context_name.pronunciation -> g.125_lm1h5\n# Answer:\nLeft fielder"], "ground_truth": ["Florida State University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-614", "prediction": ["# Reasoning Path:\nBattle of the Alamo -> time.event.locations -> San Antonio -> location.location.time_zones -> Central Time Zone\n# Answer:\nSan Antonio", "# Reasoning Path:\nBattle of the Alamo -> time.event.locations -> San Antonio -> location.location.events -> Fiesta San Antonio 2011\n# Answer:\nSan Antonio", "# Reasoning Path:\nBattle of the Alamo -> time.event.locations -> San Antonio -> location.location.events -> Fiesta San Antonio 2013\n# Answer:\nSan Antonio", "# Reasoning Path:\nBattle of the Alamo -> time.event.locations -> San Antonio -> location.location.events -> 1983 Newbery-Caldecott-Wilder Awards\n# Answer:\nSan Antonio", "# Reasoning Path:\nBattle of the Alamo -> time.event.locations -> San Antonio -> location.citytown.postal_codes -> 78201\n# Answer:\nSan Antonio", "# Reasoning Path:\nBattle of the Alamo -> military.military_conflict.military_personnel_involved -> Manuel Fern\u00e1ndez Castrill\u00f3n -> common.topic.notable_types -> Military Person\n# Answer:\nManuel Fern\u00e1ndez Castrill\u00f3n", "# Reasoning Path:\nBattle of the Alamo -> time.event.locations -> San Antonio -> location.citytown.postal_codes -> 78202\n# Answer:\nSan Antonio", "# Reasoning Path:\nBattle of the Alamo -> common.topic.image -> 1854 Alamo -> common.image.size -> m.05l90gw\n# Answer:\n1854 Alamo", "# Reasoning Path:\nBattle of the Alamo -> military.military_conflict.military_personnel_involved -> Manuel Fern\u00e1ndez Castrill\u00f3n -> military.military_person.participated_in_conflicts -> Battle of San Jacinto\n# Answer:\nManuel Fern\u00e1ndez Castrill\u00f3n", "# Reasoning Path:\nBattle of the Alamo -> military.military_conflict.military_personnel_involved -> Mart\u00edn Perfecto de Cos -> military.military_person.participated_in_conflicts -> Battle of San Jacinto\n# Answer:\nMart\u00edn Perfecto de Cos"], "ground_truth": ["1836-02-23"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-615", "prediction": ["# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> common.topic.article -> m.02sx6l\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.spouse_s -> m.03mlj1z\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> common.topic.article -> m.0hn9rcf\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.place_of_birth -> Blackmore\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Edward VI of England -> people.person.religion -> Protestantism\n# Answer:\nEdward VI of England", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> people.person.nationality -> Kingdom of England\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> royalty.noble_person.titles -> m.0hqncyh\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> symbols.name_source.namesakes -> Henry FitzAlan, 19th Earl of Arundel -> people.person.children -> Henry Lord Maltravers\n# Answer:\nHenry FitzAlan, 19th Earl of Arundel", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Edward VI of England -> people.deceased_person.place_of_burial -> Henry VII Chapel\n# Answer:\nEdward VI of England", "# Reasoning Path:\nHenry VIII of England -> symbols.name_source.namesakes -> Henry FitzAlan, 19th Earl of Arundel -> people.person.children -> Jane Lumley\n# Answer:\nHenry FitzAlan, 19th Earl of Arundel"], "ground_truth": ["Henry, Duke of Cornwall", "Edward VI of England", "Mary I of England", "Henry FitzRoy, 1st Duke of Richmond and Somerset", "Elizabeth I of England"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6857142857142857, "ans_precission": 0.8, "ans_recall": 0.6, "path_f1": 0.6857142857142857, "path_precision": 0.8, "path_recall": 0.6, "path_ans_f1": 0.6857142857142857, "path_ans_precision": 0.8, "path_ans_recall": 0.6}
{"id": "WebQTest-616", "prediction": ["# Reasoning Path:\nElizabeth II -> people.person.children -> Prince Edward, Earl of Wessex -> people.person.children -> James, Viscount Severn\n# Answer:\nPrince Edward, Earl of Wessex", "# Reasoning Path:\nElizabeth II -> people.person.children -> Prince Edward, Earl of Wessex -> people.person.spouse_s -> m.0j4k1bf\n# Answer:\nPrince Edward, Earl of Wessex", "# Reasoning Path:\nElizabeth II -> people.person.children -> Prince Edward, Earl of Wessex -> people.person.children -> Lady Louise Windsor\n# Answer:\nPrince Edward, Earl of Wessex", "# Reasoning Path:\nElizabeth II -> people.person.spouse_s -> m.028zms8 -> people.marriage.spouse -> Prince Philip, Duke of Edinburgh\n# Answer:\nm.028zms8", "# Reasoning Path:\nElizabeth II -> people.person.children -> Prince Edward, Earl of Wessex -> people.person.parents -> Prince Philip, Duke of Edinburgh\n# Answer:\nPrince Edward, Earl of Wessex", "# Reasoning Path:\nElizabeth II -> people.person.children -> Anne, Princess Royal -> people.person.children -> Peter Phillips\n# Answer:\nAnne, Princess Royal", "# Reasoning Path:\nElizabeth II -> people.person.spouse_s -> m.028zms8 -> people.marriage.location_of_ceremony -> Westminster Abbey\n# Answer:\nm.028zms8", "# Reasoning Path:\nElizabeth II -> people.person.nationality -> United Kingdom -> royalty.kingdom.rulers -> Edward VIII of the United Kingdom\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nElizabeth II -> people.person.children -> Anne, Princess Royal -> people.person.children -> Zara Phillips\n# Answer:\nAnne, Princess Royal", "# Reasoning Path:\nElizabeth II -> people.person.children -> Charles, Prince of Wales -> people.person.children -> Prince Harry\n# Answer:\nCharles, Prince of Wales"], "ground_truth": ["Lilibet", "Elizabeth II"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-617", "prediction": ["# Reasoning Path:\nTim Tebow -> people.person.place_of_birth -> Makati -> location.location.containedby -> Philippines\n# Answer:\nMakati", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> book.written_work.author -> Nathan Whitaker\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> people.person.place_of_birth -> Makati -> location.location.people_born_here -> Adolf Alix, Jr.\n# Answer:\nMakati", "# Reasoning Path:\nTim Tebow -> people.person.place_of_birth -> Makati -> location.location.containedby -> Metro Manila\n# Answer:\nMakati", "# Reasoning Path:\nTim Tebow -> people.person.place_of_birth -> Makati -> location.location.people_born_here -> Albie Casi\u00f1o\n# Answer:\nMakati", "# Reasoning Path:\nTim Tebow -> people.person.place_of_birth -> Makati -> location.location.contains -> Alphaland Makati Tower\n# Answer:\nMakati", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.notable_types -> Book\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> people.person.place_of_birth -> Makati -> location.location.people_born_here -> Alex Compton\n# Answer:\nMakati", "# Reasoning Path:\nTim Tebow -> people.person.place_of_birth -> Makati -> location.location.contains -> Asia Pacific College\n# Answer:\nMakati", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.article -> m.0j4d5g4\n# Answer:\nThrough My Eyes"], "ground_truth": ["Makati"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-618", "prediction": ["# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Queen Anne's County -> location.location.containedby -> Baltimore-Towson, MD Metropolitan Statistical Area\n# Answer:\nQueen Anne's County", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Queen Anne's County -> common.topic.article -> m.0g83b\n# Answer:\nQueen Anne's County", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Queen Anne's County -> location.statistical_region.population -> g.11b66hy55n\n# Answer:\nQueen Anne's County", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Queen Anne's County -> location.location.containedby -> Maryland\n# Answer:\nQueen Anne's County", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Anne Arundel County -> location.location.containedby -> Baltimore-Towson, MD Metropolitan Statistical Area\n# Answer:\nAnne Arundel County", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Anne Arundel County -> location.statistical_region.co2_emissions_residential -> m.045k33p\n# Answer:\nAnne Arundel County", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.body_of_water_spanned -> Chesapeake Bay -> location.location.containedby -> North America\n# Answer:\nChesapeake Bay", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Queen Anne's County -> location.statistical_region.population -> g.11x1cgn71\n# Answer:\nQueen Anne's County", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Anne Arundel County -> location.statistical_region.population -> g.11b66cbs7h\n# Answer:\nAnne Arundel County", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Queen Anne's County -> location.statistical_region.population -> g.11x1cvdsr\n# Answer:\nQueen Anne's County"], "ground_truth": ["Anne Arundel County", "Queen Anne's County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-619", "prediction": ["# Reasoning Path:\nThe Big Valley -> tv.tv_program.regular_cast -> m.04d576g -> tv.regular_tv_appearance.character -> Victoria Barkley\n# Answer:\nm.04d576g", "# Reasoning Path:\nThe Big Valley -> award.award_nominated_work.award_nominations -> m.07tws31 -> award.award_nomination.award_nominee -> Barbara Stanwyck\n# Answer:\nm.07tws31", "# Reasoning Path:\nThe Big Valley -> tv.tv_program.regular_cast -> m.04d576g -> tv.regular_tv_appearance.actor -> Barbara Stanwyck\n# Answer:\nm.04d576g", "# Reasoning Path:\nThe Big Valley -> tv.tv_program.regular_cast -> m.04d576m -> tv.regular_tv_appearance.actor -> Linda Evans\n# Answer:\nm.04d576m", "# Reasoning Path:\nThe Big Valley -> award.award_nominated_work.award_nominations -> m.07tws78 -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Lead Actress in a Drama Series\n# Answer:\nm.07tws78", "# Reasoning Path:\nThe Big Valley -> tv.tv_program.regular_cast -> m.04d576s -> tv.regular_tv_appearance.character -> Heath Barkley\n# Answer:\nm.04d576s", "# Reasoning Path:\nThe Big Valley -> award.award_nominated_work.award_nominations -> m.07tws31 -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Lead Actress in a Drama Series\n# Answer:\nm.07tws31", "# Reasoning Path:\nThe Big Valley -> tv.tv_program.regular_cast -> m.04d576m -> tv.regular_tv_appearance.character -> Audra Barkley\n# Answer:\nm.04d576m", "# Reasoning Path:\nThe Big Valley -> award.award_nominated_work.award_nominations -> m.07twsc9 -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Lead Actress in a Drama Series\n# Answer:\nm.07twsc9", "# Reasoning Path:\nThe Big Valley -> award.award_nominated_work.award_nominations -> m.07tws78 -> award.award_nomination.ceremony -> 19th Primetime Emmy Awards\n# Answer:\nm.07tws78"], "ground_truth": ["Barbara Stanwyck"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.1904761904761905, "path_precision": 0.2, "path_recall": 0.18181818181818182, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Countryway Gunshop\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Projectile weapon\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.image -> HK USP 45\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.location.containedby -> Missouri\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Springfield Armory, Inc.\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> base.weapons.weapon.subtypes -> Handgun\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.image -> M&Prevolver\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> base.fight.crime_type.crimes_of_this_type -> Assassination of Yitzhak Rabin\n# Answer:\nAssassination", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.location.containedby -> Buchanan County\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.statistical_region.population -> g.11b66mljjm\n# Answer:\nSaint Joseph"], "ground_truth": ["Firearm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-620", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> people.person.place_of_birth -> Brookline -> travel.travel_destination.tourist_attractions -> John Fitzgerald Kennedy National Historic Site\n# Answer:\nBrookline", "# Reasoning Path:\nJohn F. Kennedy -> base.inaugurations.inauguration_speaker.inauguration -> John F. Kennedy 1961 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nJohn F. Kennedy 1961 presidential inauguration", "# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_death -> Dallas -> travel.travel_destination.tourist_attractions -> Dealey Plaza\n# Answer:\nDallas", "# Reasoning Path:\nJohn F. Kennedy -> people.person.place_of_birth -> Brookline -> business.business_location.parent_company -> 16 Handles brookline\n# Answer:\nBrookline", "# Reasoning Path:\nJohn F. Kennedy -> base.inaugurations.inauguration_speaker.inauguration -> John F. Kennedy 1961 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nJohn F. Kennedy 1961 presidential inauguration", "# Reasoning Path:\nJohn F. Kennedy -> base.inaugurations.inauguration_speaker.inauguration -> John F. Kennedy 1961 presidential inauguration -> base.inaugurations.inauguration.inaugural_address -> Inaugural address of John F. Kennedy\n# Answer:\nJohn F. Kennedy 1961 presidential inauguration", "# Reasoning Path:\nJohn F. Kennedy -> people.person.place_of_birth -> Brookline -> location.location.containedby -> Massachusetts\n# Answer:\nBrookline", "# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_death -> Dallas -> location.location.containedby -> United States of America\n# Answer:\nDallas", "# Reasoning Path:\nJohn F. Kennedy -> people.person.place_of_birth -> Brookline -> location.location.containedby -> United States of America\n# Answer:\nBrookline", "# Reasoning Path:\nJohn F. Kennedy -> base.inaugurations.inauguration_speaker.inauguration -> John F. Kennedy 1961 presidential inauguration -> base.culturalevent.event.entity_involved -> Earl Warren\n# Answer:\nJohn F. Kennedy 1961 presidential inauguration"], "ground_truth": ["United States Capitol", "Washington, D.C."], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.23529411764705882, "path_precision": 0.2, "path_recall": 0.2857142857142857, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-621", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Andreas Bielau\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Andreas Diebitz\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> olympics.olympic_participating_country.athletes -> m.04dq7vv\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Arkadii Dragomoshchenko\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics\n# Answer:\nAustria", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Liechtenstein\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> olympics.olympic_participating_country.athletes -> m.04dq8sf\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Eschen\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.country.first_level_divisions -> Balzers\n# Answer:\nLiechtenstein"], "ground_truth": ["Belgium", "Germany", "Luxembourg", "East Germany", "Austria", "Switzerland", "Liechtenstein"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.6990291262135921, "ans_precission": 0.9, "ans_recall": 0.5714285714285714, "path_f1": 0.5806451612903225, "path_precision": 0.9, "path_recall": 0.42857142857142855, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 1.0, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-622", "prediction": ["# Reasoning Path:\nTaliban -> common.topic.article -> m.07jqt\n# Answer:\nm.07jqt", "# Reasoning Path:\nTaliban -> organization.organization.geographic_scope -> Afghanistan -> base.aareas.schema.administrative_area.administrative_children -> Maidan Wardak Province\n# Answer:\nAfghanistan", "# Reasoning Path:\nTaliban -> organization.organization.geographic_scope -> Afghanistan -> location.country.first_level_divisions -> Maidan Wardak Province\n# Answer:\nAfghanistan", "# Reasoning Path:\nTaliban -> military.military_combatant.military_conflicts -> m.03z97l9 -> military.military_combatant_group.conflict -> War on Terror\n# Answer:\nm.03z97l9", "# Reasoning Path:\nTaliban -> organization.organization.geographic_scope -> Afghanistan -> location.country.first_level_divisions -> Badakhshan Province\n# Answer:\nAfghanistan", "# Reasoning Path:\nTaliban -> organization.organization.geographic_scope -> Afghanistan -> location.country.first_level_divisions -> Badghis Province\n# Answer:\nAfghanistan", "# Reasoning Path:\nTaliban -> organization.organization.geographic_scope -> Afghanistan -> base.aareas.schema.administrative_area.administrative_children -> Badakhshan Province\n# Answer:\nAfghanistan", "# Reasoning Path:\nTaliban -> organization.organization.geographic_scope -> Afghanistan -> location.country.administrative_divisions -> Maidan Wardak Province\n# Answer:\nAfghanistan", "# Reasoning Path:\nTaliban -> military.military_combatant.military_conflicts -> m.04y_7_c -> military.military_combatant_group.conflict -> 2001 uprising in Herat\n# Answer:\nm.04y_7_c", "# Reasoning Path:\nTaliban -> organization.organization.geographic_scope -> Afghanistan -> base.aareas.schema.administrative_area.administrative_children -> Badghis Province\n# Answer:\nAfghanistan"], "ground_truth": ["Afghanistan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-624", "prediction": ["# Reasoning Path:\nElena Gilbert -> tv.tv_character.appeared_in_tv_program -> m.0790rw5 -> tv.regular_tv_appearance.actor -> Nina Dobrev\n# Answer:\nm.0790rw5", "# Reasoning Path:\nElena Gilbert -> tv.tv_character.appeared_in_tv_program -> m.0790rw5 -> tv.regular_tv_appearance.series -> The Vampire Diaries\n# Answer:\nm.0790rw5", "# Reasoning Path:\nElena Gilbert -> book.book_character.appears_in_book -> The Fury -> book.book.characters -> Damon Salvatore\n# Answer:\nThe Fury", "# Reasoning Path:\nElena Gilbert -> fictional_universe.fictional_character.species -> Homo sapiens -> common.topic.webpage -> m.09w06xc\n# Answer:\nHomo sapiens", "# Reasoning Path:\nElena Gilbert -> book.book_character.appears_in_book -> Dark Reunion -> book.written_work.original_language -> English Language\n# Answer:\nDark Reunion", "# Reasoning Path:\nElena Gilbert -> book.book_character.appears_in_book -> The Fury -> book.written_work.original_language -> English Language\n# Answer:\nThe Fury", "# Reasoning Path:\nElena Gilbert -> book.book_character.appears_in_book -> The Return: Nightfall -> book.book.characters -> Damon Salvatore\n# Answer:\nThe Return: Nightfall", "# Reasoning Path:\nElena Gilbert -> fictional_universe.fictional_character.species -> Homo sapiens -> common.topic.webpage -> m.09w0bxp\n# Answer:\nHomo sapiens", "# Reasoning Path:\nElena Gilbert -> fictional_universe.fictional_character.species -> Homo sapiens -> fictional_universe.character_species.found_in_fictional_universe -> Anitaverse\n# Answer:\nHomo sapiens", "# Reasoning Path:\nElena Gilbert -> book.book_character.appears_in_book -> Dark Reunion -> book.book.editions -> The Vampire Diaries Volume IV: Dark Reunion\n# Answer:\nDark Reunion"], "ground_truth": ["Nina Dobrev"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-626", "prediction": ["# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Missouri -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Missouri -> location.location.containedby -> Contiguous United States\n# Answer:\nMissouri", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Missouri -> location.location.containedby -> Midwestern United States\n# Answer:\nMissouri", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Iowa -> government.political_district.representatives -> m.05kfggp\n# Answer:\nIowa", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Missouri -> location.location.partially_contains -> Mississippi River\n# Answer:\nMissouri", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Missouri -> location.location.containedby -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Missouri -> location.location.partially_contains -> Black River\n# Answer:\nMissouri", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Iowa -> government.political_district.representatives -> m.05kfgz4\n# Answer:\nIowa", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Iowa -> government.governmental_jurisdiction.official_symbols -> m.04st6mb\n# Answer:\nIowa", "# Reasoning Path:\nMissouri River -> book.book_subject.works -> Bucking the sun -> book.book.editions -> Bucking the Sun\n# Answer:\nBucking the sun"], "ground_truth": ["Missouri", "Iowa", "Nebraska", "North Dakota", "South Dakota", "Kansas", "Montana"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.4337349397590361, "ans_precission": 0.9, "ans_recall": 0.2857142857142857, "path_f1": 0.4337349397590361, "path_precision": 0.9, "path_recall": 0.2857142857142857, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 1.0, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-627", "prediction": ["# Reasoning Path:\nTaylor Lautner -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Michigan\n# Answer:\nUnited States of America", "# Reasoning Path:\nTaylor Lautner -> people.person.ethnicity -> French American -> people.ethnicity.included_in_group -> European American\n# Answer:\nFrench American", "# Reasoning Path:\nTaylor Lautner -> people.person.place_of_birth -> Grand Rapids -> location.location.containedby -> United States of America\n# Answer:\nGrand Rapids", "# Reasoning Path:\nTaylor Lautner -> people.person.ethnicity -> French American -> common.topic.webpage -> m.04m3l_f\n# Answer:\nFrench American", "# Reasoning Path:\nTaylor Lautner -> people.person.place_of_birth -> Grand Rapids -> location.location.containedby -> Michigan\n# Answer:\nGrand Rapids", "# Reasoning Path:\nTaylor Lautner -> people.person.ethnicity -> French American -> dining.cuisine.restaurant -> Bistro 330\n# Answer:\nFrench American", "# Reasoning Path:\nTaylor Lautner -> people.person.place_of_birth -> Grand Rapids -> travel.travel_destination.tourist_attractions -> DeVos Place Convention Center\n# Answer:\nGrand Rapids", "# Reasoning Path:\nTaylor Lautner -> people.person.ethnicity -> Indigenous peoples of the Americas -> common.topic.subject_of -> Tribal Gathering\n# Answer:\nIndigenous peoples of the Americas", "# Reasoning Path:\nTaylor Lautner -> people.person.place_of_birth -> Grand Rapids -> location.location.containedby -> Kent County\n# Answer:\nGrand Rapids", "# Reasoning Path:\nTaylor Lautner -> people.person.place_of_birth -> Grand Rapids -> location.location.contains -> DeVos Place Convention Center\n# Answer:\nGrand Rapids"], "ground_truth": ["United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-628", "prediction": ["# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.official_symbols -> m.0462792 -> location.location_symbol_relationship.symbol -> California grizzly bear\n# Answer:\nm.0462792", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.official_symbols -> m.046279k -> location.location_symbol_relationship.symbol -> I Love You, California\n# Answer:\nm.046279k", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.official_symbols -> m.046279y -> location.location_symbol_relationship.symbol -> Square dance\n# Answer:\nm.046279y", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.official_symbols -> m.0462792 -> location.location_symbol_relationship.Kind_of_symbol -> State animal\n# Answer:\nm.0462792", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.official_symbols -> m.046279k -> location.location_symbol_relationship.Kind_of_symbol -> State song\n# Answer:\nm.046279k", "# Reasoning Path:\nCalifornia -> location.location.events -> 1988 Wine Country Film Festival -> time.event.locations -> Petaluma\n# Answer:\n1988 Wine Country Film Festival", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.official_symbols -> m.046279y -> location.location_symbol_relationship.Kind_of_symbol -> State folk dance\n# Answer:\nm.046279y", "# Reasoning Path:\nCalifornia -> location.location.events -> 1987 Wine Country Film Festival -> freebase.valuenotation.has_value -> Closing date\n# Answer:\n1987 Wine Country Film Festival", "# Reasoning Path:\nCalifornia -> location.location.events -> 1987 Wine Country Film Festival -> time.event.locations -> Sonoma Valley\n# Answer:\n1987 Wine Country Film Festival", "# Reasoning Path:\nCalifornia -> film.film_location.featured_in_films -> Crazy in Alabama -> film.film.language -> English Language\n# Answer:\nCrazy in Alabama"], "ground_truth": ["California grizzly bear"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-629", "prediction": ["# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Athletic Shoes & Apparel -> business.industry.companies -> LaCrosse Footwear\n# Answer:\nAthletic Shoes & Apparel", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Athletic Shoes & Apparel -> business.industry.companies -> Crocs\n# Answer:\nAthletic Shoes & Apparel", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Athletic Shoes & Apparel -> common.topic.notable_types -> Industry\n# Answer:\nAthletic Shoes & Apparel", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Footwear -> business.industry.companies -> New Balance\n# Answer:\nFootwear", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Athletic Shoes & Apparel -> business.industry.companies -> Deckers Outdoor Corporation\n# Answer:\nAthletic Shoes & Apparel", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Athletic Shoes & Apparel -> business.industry.parent_industry -> Footwear Manufacturing\n# Answer:\nAthletic Shoes & Apparel", "# Reasoning Path:\nNike, Inc. -> organization.organization.founders -> Bill Bowerman -> common.topic.notable_types -> Organization founder\n# Answer:\nBill Bowerman", "# Reasoning Path:\nNike, Inc. -> organization.organization.founders -> Phil Knight -> organization.organization_founder.organizations_founded -> Laika\n# Answer:\nPhil Knight", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Footwear -> fashion.garment.more_specialized_forms -> Shoe\n# Answer:\nFootwear", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Sports equipment -> business.industry.child_industry -> Fabricated Rubber Products, NEC (wet suits)\n# Answer:\nSports equipment"], "ground_truth": ["NIKE Waffle", "NIKE Poland Sp.zo.o", "NIKE Australia Pty. Ltd", "NIKE (Switzerland) GmbH", "NIKE International Ltd", "NIKE Hong Kong Ltd", "NIKE de Mexico S de R.L. de C.V.", "Umbro.com", "Cole Haan Japan", "Manchester United Merchandising Ltd", "Umbro International JV", "NIKE Retail LLC", "NIKE USA Inc", "American NIKE SL", "Converse Canada Holding B.V.", "NIKE Pegasus", "Umbro Schweiz Ltd", "NIKE International LLC", "NIKE International Holding B.V.", "Triax Insurance Inc", "Umbro Ltd", "Converse Hong Kong Holding Co Ltd", "NIKE Servicios de Mexico S. de R.L. de C.V.", "NIKE France SAS.", "Umbro Finance Ltd", "Savier Inc", "NIKE Italy S.R.L.", "Converse Footwear Technical Service (Zhongshan) Co Ltd", "NIKE Lavadome", "NIKE (Suzhou) Sports Co Ltd", "NIKE Dunk Holding B.V.", "PT Hurley Indonesia", "BRS NIKE Taiwan Inc", "NIKE Retail Services Inc", "NIKE Europe Holding B.V.", "NIKE Licenciamentos do Brasil Ltda", "NIKE Chile B.V.", "NIKE Deutschland GmbH", "Bragano Trading S.r.l.", "Umbro JV Ltd", "NIKE Zoom LLC", "NIKE European Operations Netherlands B.V.", "NIKE Argentina Srl", "Juventus Merchandising S.r.l.", "NIKE Retail B.V.", "NIKE Global Holding B.V.", "Cole Haan Co Store", "Umbro HK Ltd", "Converse Canada Corp", "NIKE Trading Co B.V.", "Nike Vision", "NIKE International Holding Inc", "Hurley999 UK Ltd", "NIKE Logistics Yugen Kaisha", "Converse Netherlands B.V.", "Hurley Australia Pty Ltd", "Hurley International Holding B.V.", "NIKE IHM Inc", "NIKE BH B.V.", "NIKE Mexico Holdings LLC", "NIKE South Africa Ltd", "NIKE Vision Timing & Techlab LP", "Converse Hong Kong Ltd", "NIKE Canada Holding B.V.", "Umbro Sportwear Ltd", "Umbro International Ltd", "Twin Dragons Holding B.V.", "NIKE Retail Poland sp. z o. o.", "NIKE TN Inc", "Umbro", "Exeter Brands Group LLC", "NIKE UK Holding B.V.", "NIKE Russia LLC", "Hurley International", "NIKE Group Holding B.V.", "PMG International Ltd", "NIKE Hellas EPE", "NIKE Jump Ltd", "Twin Dragons Global Ltd", "NIKE Huarache", "Converse Sporting Goods (China) Co Ltd", "Hurley 999 SL", "PT NIKE Indonesia", "Converse Trading Co B.V.", "NIKE Laser Holding B.V.", "Converse", "NIKE (UK) Ltd", "NIKE India Private Ltd", "NIKE de Chile Ltda", "NIKE Denmark ApS", "NIKE Max LLC", "NIKE NZ Holding B.V.", "NIKE SINGAPORE PTE LTD", "NIKE Global Trading PTE. LTD", "Futbol Club Barcelona SL", "NIKE Offshore Holding B.V.", "NIKE India Holding B.V.", "NIKE Finland OY", "NIKE Sourcing India Private Ltd", "Exeter Hong Kong Ltd", "NIKE Sports (China) Co Ltd", "NIKE Holding LLC", "Yugen Kaisha Hurley Japan", "NIKE Tailwind", "Nike Brand Kitchen", "NIKE Galaxy Holding B.V.", "NIKE do Brasil Comercio e Participacoes Ltda", "NIKE Australia Holding B.V.", "NIKE Global Services PTE. LTD", "NIKE Asia Holding B.V.", "NIKE Flight", "Umbro Asia Sourcing Ltd", "NIKE (Thailand) Ltd", "Umbro Licensing Ltd", "NIKE Suzhou Holding HK Ltd", "NIKE SALES (MALAYSIA) SDN. BHD.", "NIKE South Africa Holdings LLC", "Converse (Asia Pacific) Ltd", "NIKE China Holding HK Ltd", "NIKE 360 Holding B.V.", "Umbro International Holdings Ltd", "NIKE Canada Corp", "Cole Haan Hong Kong Ltd", "NIKE Vietnam Co", "NIKE GmbH", "USISL Inc", "NIKE New Zealand Co", "NIKE Finance Ltd", "NIKE CA LLC", "NIKE Vapor Ltd", "NIKE Japan Corp", "NIKE Philippines Inc", "NIKE Israel Ltd", "NIKE Africa Ltd", "NIKE Sweden AB", "Cole Haan", "NIKE Cortez", "NIKE Sports Korea Co Ltd", "Umbro Worldwide Ltd", "Nike Golf"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> common.topic.article -> m.03mpv\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.nationality -> United States of America\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> people.person.gender -> Male\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.profession -> Politician\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> common.topic.notable_types -> US President\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pgr_5 -> people.place_lived.location -> Kentucky\n# Answer:\nm.03pgr_5", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pn4x_ -> people.place_lived.location -> Springfield\n# Answer:\nm.03pn4x_", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.04hdfss -> people.place_lived.location -> Illinois\n# Answer:\nm.04hdfss"], "ground_truth": ["Hannibal Hamlin", "Andrew Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-630", "prediction": ["# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Belize\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Chile\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Achawa language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nAchawa language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> Belize\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.statistical_region.gdp_nominal_per_capita -> g.11b60ywwvv\n# Answer:\ng.11b60ywwvv", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Cuba\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language"], "ground_truth": ["Ponares Language", "Yucuna Language", "Macaguaje Language", "Ticuna language", "Carijona Language", "Andaqui Language", "Desano Language", "Inga Language", "Coyaima Language", "Tunebo, Western Language", "Guahibo language", "Cocama language", "Macagu\u00e1n Language", "Guayabero Language", "Anserma Language", "Tomedes Language", "Romani, Vlax Language", "Hupd\u00eb Language", "Ocaina Language", "Piapoco Language", "Providencia Sign Language", "Catio language", "Quechua, Napo Lowland Language", "Natagaimas Language", "Tanimuca-Retuar\u00e3 Language", "Cagua Language", "P\u00e1ez language", "Kogi Language", "Tinigua language", "Malayo Language", "Kuna, Border Language", "Piratapuyo Language", "Puinave Language", "Baudo language", "Siona Language", "Tuyuca language", "Tunebo, Barro Negro Language", "Curripaco Language", "Waimaj\u00e3 Language", "Guambiano Language", "Cubeo Language", "Macuna Language", "Palenquero Language", "Chipiajes Language", "Guanano Language", "Totoro Language", "Spanish Language", "Tunebo, Angosturas Language", "Minica Huitoto", "Coxima Language", "Piaroa Language", "Bora Language", "Bar\u00ed Language", "Cumeral Language", "Runa Language", "Barasana Language", "Cof\u00e1n Language", "Muinane Language", "Inga, Jungle Language", "Yukpa Language", "Andoque Language", "Islander Creole English", "Murui Huitoto language", "Nukak language", "Carabayo Language", "Arhuaco Language", "Playero language", "Nheengatu language", "Tunebo, Central Language", "Koreguaje Language", "Cuiba language", "Colombian Sign Language", "Tama Language", "Tucano Language", "Nonuya language", "Wayuu Language", "Cams\u00e1 Language", "Omejes Language", "Siriano Language", "Achawa language", "Ember\u00e1, Northern Language", "Cabiyar\u00ed Language", "Pijao Language", "S\u00e1liba Language", "Awa-Cuaiquer Language", "Uwa language"], "ans_acc": 0.023255813953488372, "ans_hit": 1, "ans_f1": 0.04534005037783375, "ans_precission": 0.9, "ans_recall": 0.023255813953488372, "path_f1": 0.045283018867924525, "path_precision": 0.6, "path_recall": 0.023529411764705882, "path_ans_f1": 0.04534005037783375, "path_ans_precision": 0.9, "path_ans_recall": 0.023255813953488372}
{"id": "WebQTest-631", "prediction": ["# Reasoning Path:\nRay Allen -> basketball.basketball_player.player_statistics -> m.04qbrjm -> basketball.basketball_player_stats.season -> 2006\u201307 NBA season\n# Answer:\nm.04qbrjm", "# Reasoning Path:\nRay Allen -> basketball.basketball_player.player_statistics -> m.04qcnpp -> basketball.basketball_player_stats.season -> 2002\u201303 NBA season\n# Answer:\nm.04qcnpp", "# Reasoning Path:\nRay Allen -> people.person.sibling_s -> m.0j2h3vr -> people.sibling_relationship.sibling -> Kristie Raye\n# Answer:\nm.0j2h3vr", "# Reasoning Path:\nRay Allen -> basketball.basketball_player.player_statistics -> m.04qbrjm -> basketball.basketball_player_stats.team -> Seattle Supersonics\n# Answer:\nm.04qbrjm", "# Reasoning Path:\nRay Allen -> basketball.basketball_player.player_statistics -> m.04qdv8z -> basketball.basketball_player_stats.season -> 1997\u201398 NBA season\n# Answer:\nm.04qdv8z", "# Reasoning Path:\nRay Allen -> sports.pro_athlete.teams -> m.05cf4bc -> sports.sports_team_roster.team -> Connecticut Huskies men's basketball\n# Answer:\nm.05cf4bc", "# Reasoning Path:\nRay Allen -> basketball.basketball_player.player_statistics -> m.04qcnpp -> basketball.basketball_player_stats.team -> Seattle Supersonics\n# Answer:\nm.04qcnpp", "# Reasoning Path:\nRay Allen -> people.person.sibling_s -> m.0ztqw9x -> people.sibling_relationship.sibling -> John Allen\n# Answer:\nm.0ztqw9x", "# Reasoning Path:\nRay Allen -> basketball.basketball_player.player_statistics -> m.04qdv8z -> basketball.basketball_player_stats.team -> Milwaukee Bucks\n# Answer:\nm.04qdv8z", "# Reasoning Path:\nRay Allen -> people.person.sibling_s -> m.0ztqwb8 -> people.sibling_relationship.sibling -> Kim Allen\n# Answer:\nm.0ztqwb8"], "ground_truth": ["1996"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-633", "prediction": ["# Reasoning Path:\nClaire Danes -> people.person.profession -> Actor -> common.topic.subjects -> Michael Palance\n# Answer:\nActor", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Television producer -> common.topic.subject_of -> Billy Sorrentino\n# Answer:\nTelevision producer", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Television producer -> common.topic.notable_types -> Profession\n# Answer:\nTelevision producer", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Television producer -> common.topic.subjects -> Billy Sorrentino\n# Answer:\nTelevision producer", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Actor -> people.profession.specializations -> Voice Actor\n# Answer:\nActor", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Television producer -> common.topic.subject_of -> Michael Palance\n# Answer:\nTelevision producer", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Television producer -> common.topic.subjects -> Michael Palance\n# Answer:\nTelevision producer", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Voice Actor -> film.film_job.films_with_this_crew_job -> m.0y8734m\n# Answer:\nVoice Actor", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Voice Actor -> common.topic.notable_types -> Profession\n# Answer:\nVoice Actor"], "ground_truth": ["Voice Actor", "Television producer", "Actor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-634", "prediction": ["# Reasoning Path:\nSan Antonio -> location.location.containedby -> Bexar County -> location.location.containedby -> San Antonio, TX Metropolitan Statistical Area\n# Answer:\nBexar County", "# Reasoning Path:\nSan Antonio -> location.location.containedby -> Texas -> location.location.containedby -> United States of America\n# Answer:\nTexas", "# Reasoning Path:\nSan Antonio -> location.location.containedby -> Bexar County -> location.location.containedby -> Texas\n# Answer:\nBexar County", "# Reasoning Path:\nSan Antonio -> location.location.containedby -> Bexar County -> location.hud_foreclosure_area.ofheo_price_change -> m.07cs268\n# Answer:\nBexar County", "# Reasoning Path:\nSan Antonio -> location.hud_county_place.county -> Comal County -> location.location.containedby -> Edwards Plateau\n# Answer:\nComal County", "# Reasoning Path:\nSan Antonio -> location.location.containedby -> Bexar County -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nBexar County", "# Reasoning Path:\nSan Antonio -> location.hud_county_place.county -> Comal County -> location.location.containedby -> San Antonio, TX Metropolitan Statistical Area\n# Answer:\nComal County", "# Reasoning Path:\nSan Antonio -> location.hud_county_place.county -> Comal County -> base.aareas.schema.administrative_area.administrative_area_type -> U.S. county\n# Answer:\nComal County", "# Reasoning Path:\nSan Antonio -> location.location.containedby -> Texas -> military.military_combatant.military_conflicts -> m.08crx2r\n# Answer:\nTexas", "# Reasoning Path:\nSan Antonio -> location.location.containedby -> Area code 210 -> location.location.contains -> Von Ormy\n# Answer:\nArea code 210"], "ground_truth": ["Medina County", "Bexar County", "Comal County"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.6829268292682926, "ans_precission": 0.7, "ans_recall": 0.6666666666666666, "path_f1": 0.3636363636363636, "path_precision": 0.4, "path_recall": 0.3333333333333333, "path_ans_f1": 0.6829268292682926, "path_ans_precision": 0.7, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-635", "prediction": ["# Reasoning Path:\nMatt Stone -> film.actor.film -> m.0bthgvx -> film.performance.character -> Wizard of Oz\n# Answer:\nm.0bthgvx", "# Reasoning Path:\nMatt Stone -> fictional_universe.fictional_character_creator.fictional_characters_created -> Mr. Garrison -> fictional_universe.fictional_character.occupation -> Assistant\n# Answer:\nMr. Garrison", "# Reasoning Path:\nMatt Stone -> fictional_universe.fictional_character_creator.fictional_characters_created -> Bebe Stevens -> film.film_character.portrayed_in_films -> m.0bthj21\n# Answer:\nBebe Stevens", "# Reasoning Path:\nMatt Stone -> fictional_universe.fictional_character_creator.fictional_characters_created -> Mr. Garrison -> common.topic.article -> m.02p1d82\n# Answer:\nMr. Garrison", "# Reasoning Path:\nMatt Stone -> fictional_universe.fictional_character_creator.fictional_characters_created -> Mr. Garrison -> fictional_universe.fictional_character.occupation -> Teacher\n# Answer:\nMr. Garrison", "# Reasoning Path:\nMatt Stone -> fictional_universe.fictional_character_creator.fictional_characters_created -> Mr. Garrison -> fictional_universe.fictional_character.gender -> Female\n# Answer:\nMr. Garrison", "# Reasoning Path:\nMatt Stone -> fictional_universe.fictional_character_creator.fictional_characters_created -> Bebe Stevens -> fictional_universe.fictional_character.character_created_by -> Trey Parker\n# Answer:\nBebe Stevens", "# Reasoning Path:\nMatt Stone -> film.actor.film -> m.0bthgvx -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0bthgvx", "# Reasoning Path:\nMatt Stone -> film.actor.film -> m.02t9_gm -> film.performance.character -> Dave the Lighting Guy\n# Answer:\nm.02t9_gm", "# Reasoning Path:\nMatt Stone -> fictional_universe.fictional_character_creator.fictional_characters_created -> Bebe Stevens -> tv.tv_character.appeared_in_tv_program -> m.03lc46p\n# Answer:\nBebe Stevens"], "ground_truth": ["Doug Remer", "Terrance Henry Stoot", "Hermaphrodite", "Danny Glover", "Ticket Taker", "Bill Gates", "Jimbo Kearn", "Taper Guy", "Ethan Hawke", "Dave the Lighting Guy", "Former Columbine Student", "Saddam Hussein", "George Clooney", "Kyle Broflovski", "Man toasting drinks at MCA outdoor bar", "Gerald Broflovski", "James Humphries", "Kenny McCormick", "Chris", "Kyle / Kenny / Jesus", "Wizard of Oz"], "ans_acc": 0.09523809523809523, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.11764705882352941, "path_precision": 0.2, "path_recall": 0.08333333333333333, "path_ans_f1": 0.12903225806451613, "path_ans_precision": 0.2, "path_ans_recall": 0.09523809523809523}
{"id": "WebQTest-636", "prediction": ["# Reasoning Path:\nMichigan -> location.location.time_zones -> UTC\u221205:00 -> common.topic.notable_types -> Time Zone\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Eastern Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Eastern Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nMichigan -> government.governmental_jurisdiction.governing_officials -> m.010f1h5h -> government.government_position_held.basic_title -> Attorney general\n# Answer:\nm.010f1h5h", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Eastern Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nMichigan -> government.governmental_jurisdiction.governing_officials -> m.010f1h5h -> government.government_position_held.office_position_or_title -> Michigan Attorney General\n# Answer:\nm.010f1h5h"], "ground_truth": ["UTC\u221205:00", "Eastern Time Zone", "Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-637", "prediction": ["# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Azerbaijan People's Government\n# Answer:\nSocialist state", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Socialist state -> common.topic.notable_types -> Form of Government\n# Answer:\nSocialist state", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Socialist state -> common.topic.image -> Socialist states all\n# Answer:\nSocialist state", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Bukharan People's Soviet Republic\n# Answer:\nSocialist state", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Albania\n# Answer:\nParliamentary republic", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Parliamentary republic -> common.topic.article -> m.0glnt1\n# Answer:\nParliamentary republic", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> China\n# Answer:\nSocialist state", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Parliamentary republic -> common.topic.image -> Form of government parliamentary\n# Answer:\nParliamentary republic", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Communist state -> common.topic.image -> Communist States\n# Answer:\nCommunist state", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Austria\n# Answer:\nParliamentary republic"], "ground_truth": ["Socialist state", "Communist state", "Parliamentary republic", "Soviet democracy"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.8571428571428571, "path_precision": 1.0, "path_recall": 0.75, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-638", "prediction": ["# Reasoning Path:\nJapan -> location.country.languages_spoken -> Korean Language -> base.skosbase.skos_concept.in_scheme -> Authorities Concept Scheme\n# Answer:\nKorean Language", "# Reasoning Path:\nJapan -> location.country.languages_spoken -> Korean Language -> language.human_language.region -> Asia\n# Answer:\nKorean Language", "# Reasoning Path:\nJapan -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEsperanto Language", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\ng.1245_2h9t", "# Reasoning Path:\nJapan -> location.country.languages_spoken -> Korean Language -> base.skosbase.skos_concept.in_scheme -> Topical Terms Concept Scheme\n# Answer:\nKorean Language", "# Reasoning Path:\nJapan -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.11b71r82sc\n# Answer:\ng.11b71r82sc", "# Reasoning Path:\nJapan -> location.country.languages_spoken -> Korean Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nKorean Language", "# Reasoning Path:\nJapan -> location.country.languages_spoken -> English Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nJapan -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> China\n# Answer:\nEsperanto Language"], "ground_truth": ["Korean Language", "Japanese Language", "Esperanto Language", "English Language", "Ainu Language"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6857142857142857, "ans_precission": 0.8, "ans_recall": 0.6, "path_f1": 0.6857142857142857, "path_precision": 0.8, "path_recall": 0.6, "path_ans_f1": 0.6857142857142857, "path_ans_precision": 0.8, "path_ans_recall": 0.6}
{"id": "WebQTest-639", "prediction": ["# Reasoning Path:\nDennis Rodman -> sports.pro_athlete.teams -> m.0_qwx1g -> sports.sports_team_roster.team -> Chicago Bulls\n# Answer:\nm.0_qwx1g", "# Reasoning Path:\nDennis Rodman -> sports.pro_athlete.teams -> m.0_qwyt6 -> sports.sports_team_roster.team -> San Antonio Spurs\n# Answer:\nm.0_qwyt6", "# Reasoning Path:\nDennis Rodman -> sports.pro_athlete.teams -> m.0_qx2_s -> sports.sports_team_roster.team -> Detroit Pistons\n# Answer:\nm.0_qx2_s", "# Reasoning Path:\nDennis Rodman -> sports.pro_athlete.teams -> m.0_qwx1g -> sports.sports_team_roster.position -> Power forward\n# Answer:\nm.0_qwx1g", "# Reasoning Path:\nDennis Rodman -> people.person.profession -> Athlete -> common.topic.notable_types -> Profession\n# Answer:\nAthlete", "# Reasoning Path:\nDennis Rodman -> sports.pro_athlete.teams -> m.0_qwyt6 -> sports.sports_team_roster.position -> Power forward\n# Answer:\nm.0_qwyt6", "# Reasoning Path:\nDennis Rodman -> award.award_winner.awards_won -> m.059pgqh -> award.award_honor.award -> Razzie Award for Worst Supporting Actor\n# Answer:\nm.059pgqh", "# Reasoning Path:\nDennis Rodman -> people.person.profession -> Athlete -> people.profession.specializations -> Basketball player\n# Answer:\nAthlete", "# Reasoning Path:\nDennis Rodman -> sports.pro_athlete.teams -> m.0_qx2_s -> sports.sports_team_roster.position -> Power forward\n# Answer:\nm.0_qx2_s", "# Reasoning Path:\nDennis Rodman -> award.award_winner.awards_won -> m.059pgqh -> award.award_honor.ceremony -> 18th Golden Raspberry Awards\n# Answer:\nm.059pgqh"], "ground_truth": ["1995"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Abel Magwitch -> book.book_character.appears_in_book -> Great Expectations\n# Answer:\nAbel Magwitch", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Finnegan 'Finn' Bell -> film.film_character.portrayed_in_films -> m.010hszpw\n# Answer:\nFinnegan 'Finn' Bell", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Fereydoon Motamed -> influence.influence_node.influenced_by -> Edgar Allan Poe\n# Answer:\nFereydoon Motamed", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Finnegan 'Finn' Bell -> tv.tv_character.appeared_in_tv_program -> m.0gbd7_9\n# Answer:\nFinnegan 'Finn' Bell", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Alfred Lammle -> fictional_universe.fictional_character.occupation -> Adventurer\n# Answer:\nAlfred Lammle", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Finnegan 'Finn' Bell -> film.film_character.portrayed_in_films -> m.011lv6qv\n# Answer:\nFinnegan 'Finn' Bell", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Fereydoon Motamed -> influence.influence_node.influenced_by -> Jean Piaget\n# Answer:\nFereydoon Motamed", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Fereydoon Motamed -> people.person.gender -> Male\n# Answer:\nFereydoon Motamed", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced_by -> H. G. Wells\n# Answer:\nAldous Huxley", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Finnegan 'Finn' Bell -> film.film_character.portrayed_in_films -> m.011lv9kw\n# Answer:\nFinnegan 'Finn' Bell"], "ground_truth": ["A Christmas Carol (Great Stories)", "A Tale of Two Cities (Collector's Library)", "A Christmas Carol (Dramascripts Classic Texts)", "The Old Curiosity Shop", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "A Christmas Carol (Oxford Bookworms Library)", "A Tale of Two Cities (The Classic Collection)", "A Christmas Carol (Scholastic Classics)", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "A Tale of Two Cities (Prentice Hall Science)", "A Christmas Carol (Everyman's Library Children's Classics)", "A Christmas Carol (Soundings)", "A Tale of Two Cities", "Our mutual friend", "Great expectations", "A Tale of Two Cities (Longman Fiction)", "A Tale of Two Cities (Courage Literary Classics)", "A Tale of Two Cities (Illustrated Classics)", "David Copperfield.", "A Tale of Two Cities (Oxford Playscripts)", "A Tale of Two Cities (Classics Illustrated)", "Hard times", "A Christmas Carol (Ladybird Children's Classics)", "A Tale of Two Cities (Dramascripts S.)", "A Tale of Two Cities (Everyman's Library Classics)", "A Tale of Two Cities (Student's Novels)", "A Christmas Carol (New Longman Literature)", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "A Christmas Carol (Clear Print)", "A Christmas Carol (Take Part)", "A Tale of Two Cities (Everyman Paperbacks)", "A Tale of Two Cities (Ladybird Children's Classics)", "A Tale of Two Cities (Classic Fiction)", "A Tale of Two Cities (BBC Audio Series)", "A TALE OF TWO CITIES", "A Christmas Carol (Aladdin Classics)", "A Tale of Two Cities (Penguin Popular Classics)", "A Tale of Two Cities (Piccolo Books)", "A Tale of Two Cities (Saddleback Classics)", "A Tale of Two Cities (Ultimate Classics)", "Dombey and Son.", "A Christmas Carol (Acting Edition)", "A Christmas Carol (Watermill Classic)", "A Tale of Two Cities (Barnes & Noble Classics Series)", "A Christmas Carol (Classic Collection)", "A Christmas Carol (Family Classics)", "A Christmas Carol (Classics for Young Adults and Adults)", "A Christmas Carol (Reissue)", "A Christmas Carol", "Martin Chuzzlewit", "Oliver Twist", "A Tale Of Two Cities (Adult Classics)", "A Christmas Carol (Puffin Choice)", "A Tale of Two Cities (Clear Print)", "A Tale of Two Cities (Paperback Classics)", "A Christmas Carol (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Pacemaker Classics)", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "A Christmas Carol (Ladybird Classics)", "A Christmas Carol. (Lernmaterialien)", "The mystery of Edwin Drood", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "A Christmas Carol (Illustrated Classics)", "A Tale of Two Cities (Soundings)", "A Christmas Carol (Tor Classics)", "A Christmas Carol (Children's Theatre Playscript)", "A Christmas Carol (Watermill Classics)", "A Tale of Two Cities (Signet Classics)", "A Tale of Two Cities (Large Print Edition)", "The cricket on the hearth", "A Tale of Two Cities (Collected Works of Charles Dickens)", "A Tale of Two Cities (Bantam Classic)", "A Tale of Two Cities (Oxford Bookworms Library)", "A Christmas Carol (Read & Listen Books)", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "The old curiosity shop.", "A Christmas Carol (Nelson Graded Readers)", "A Christmas Carol (Radio Theatre)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "A Christmas Carol (Bantam Classic)", "A Christmas Carol (Thornes Classic Novels)", "A Tale of Two Cities (Classic Literature with Classical Music)", "A Christmas Carol (Penguin Student Editions)", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "A Christmas Carol (Chrysalis Children's Classics Series)", "A Christmas Carol (Gollancz Children's Classics)", "A Christmas Carol (Through the Magic Window Series)", "Great Expectations", "Great expectations.", "A Tale of Two Cities (Cover to Cover Classics)", "Our mutual friend.", "A Tale of Two Cities (Cyber Classics)", "A Christmas Carol (Classic, Picture, Ladybird)", "A Tale of Two Cities (The Greatest Historical Novels)", "A Tale of Two Cities (Penguin Readers, Level 5)", "A Christmas Carol (Pacemaker Classic)", "A Tale of Two Cities (40th Anniversary Edition)", "A Tale of Two Cities (Isis Clear Type Classic)", "A Christmas Carol (Whole Story)", "A Christmas Carol (Classic Fiction)", "A Tale of Two Cities (Progressive English)", "A Tale of Two Cities (Konemann Classics)", "A Tale of Two Cities (Wordsworth Classics)", "A Christmas Carol (Penguin Readers, Level 2)", "A Tale of Two Cities (Puffin Classics)", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "The Pickwick Papers", "A Tale of Two Cities (Illustrated Junior Library)", "A Tale of Two Cities (Bookcassette(r) Edition)", "Little Dorrit", "A Christmas Carol (Classics Illustrated)", "The old curiosity shop", "David Copperfield", "A Tale Of Two Cities (Adult Classics in Audio)", "A Christmas Carol (Children's Classics)", "Bleak House", "A Christmas Carol (Saddleback Classics)", "A Tale of Two Cities (Penguin Classics)", "A Tale of Two Cities (Naxos AudioBooks)", "A Tale of Two Cities (Everyman's Library (Paper))", "A Christmas Carol (Cp 1135)", "A Tale of Two Cities (Macmillan Students' Novels)", "A Christmas Carol (Enriched Classics)", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "A Tale of Two Cities (Enriched Classic)", "A Tale of Two Cities (Classic Retelling)", "A Tale of Two Cities (Cassette (1 Hr).)", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Tale of Two Cities (Adopted Classic)", "Great Expectations.", "A Christmas Carol (Apple Classics)", "A Tale of Two Cities (Compact English Classics)", "A Tale of Two Cities (10 Cassettes)", "A Christmas Carol (Large Print)", "A Christmas Carol (Pacemaker Classics)", "A Tale of Two Cities (Dover Thrift Editions)", "A Christmas Carol (The Kennett Library)", "A Christmas Carol (Value Books)", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "Bleak House.", "Bleak house", "A Tale of Two Cities (Silver Classics)", "A Tale of Two Cities (Masterworks)", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "A Tale of Two Cities (Classics Illustrated Notes)", "The Pickwick papers", "Sketches by Boz", "The Mystery of Edwin Drood", "A Tale of Two Cities (Tor Classics)", "A Tale of Two Cities (Dramatized)", "A Christmas Carol (Dramascripts)", "A Christmas Carol (Audio Editions)", "A Tale of Two Cities (Acting Edition)", "Dombey and Son", "A Christmas Carol (R)", "A Tale of Two Cities (Simple English)", "A Christmas Carol (Young Reading Series 2)", "A Tale of Two Cities (Dodo Press)", "A CHRISTMAS CAROL", "A Tale of Two Cities (Longman Classics, Stage 2)", "A Christmas Carol (Puffin Classics)", "The life and adventures of Nicholas Nickleby", "A Christmas Carol (Green Integer, 50)", "A Christmas Carol (Cover to Cover)", "Dombey and son", "A Christmas Carol (Limited Editions)", "A Christmas Carol (Usborne Young Reading)", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Unabridged Classics)"], "ans_acc": 0.023668639053254437, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.00588235294117647, "path_precision": 0.1, "path_recall": 0.0030303030303030303, "path_ans_f1": 0.03827751196172249, "path_ans_precision": 0.1, "path_ans_recall": 0.023668639053254437}
{"id": "WebQTest-640", "prediction": ["# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Tentmaker -> common.topic.notable_types -> Profession\n# Answer:\nTentmaker", "# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Tentmaker -> common.topic.notable_for -> g.1254xhyd0\n# Answer:\nTentmaker", "# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Missionary -> media_common.quotation_subject.quotations_about_this_subject -> As each Sister is to become a Co-Worker of Christ in the slums, each ought to understand what God and the Missionaries of Charity expect from her. Let Christ radiate and live his life in her and through her in the slums. Let the poor, seeing her, be drawn to Christ and invite him to enter their homes and their lives. Let the sick and suffering find in her a real angel of comfort and consolation. Let the little ones of the streets cling to her because she reminds them of him, the friend of the little ones.\n# Answer:\nMissionary", "# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Missionary -> base.ontologies.ontology_instance.equivalent_instances -> m.07ngj4q\n# Answer:\nMissionary", "# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Missionary -> base.schemastaging.context_name.pronunciation -> g.125_p_yp_\n# Answer:\nMissionary", "# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Writer -> people.profession.specializations -> Author\n# Answer:\nWriter", "# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Missionary -> media_common.quotation_subject.quotations_about_this_subject -> Firm, faithful, and devoted, full of energy and zeal, and truth, he labors for his race; he clears their painful way to improvement; he hews down like a giant the prejudices of creed and caste that encumber it. He may be stern; he may be exacting; he may be ambitious yet; but his is the sternness of the warrior Greatheart, who guards his pilgrim convoy from the onslaught of Apollyon. His is the exaction of the apostle, who speaks but for Christ, when he says, Whosoever will come after Me, let him deny himself, and take up his cross and follow Me. His is the ambition of the high master-spirit, which aims to fill a place in the first rank of those who are redeemed from the earth -- who stand without fault before the throne of God, who share the last mighty victories of the Lamb, who are called, and chosen, and faithful.\n# Answer:\nMissionary", "# Reasoning Path:\nPaul the Apostle -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Writer -> common.topic.article -> m.0cbdb\n# Answer:\nWriter", "# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Missionary -> media_common.quotation_subject.quotations_about_this_subject -> Go practice if you please with men and women: leave a child alone for Christ's particular love's sake!\n# Answer:\nMissionary"], "ground_truth": ["Writer", "Tentmaker", "Missionary", "Prophet"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.8181818181818182, "path_precision": 0.9, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-642", "prediction": ["# Reasoning Path:\nVin Scully -> people.person.education -> m.02wnq86 -> education.education.institution -> Fordham University\n# Answer:\nm.02wnq86", "# Reasoning Path:\nVin Scully -> people.person.education -> m.0svtrlm -> education.education.institution -> Fordham Preparatory School\n# Answer:\nm.0svtrlm", "# Reasoning Path:\nVin Scully -> common.topic.image -> Scully pub -> common.image.size -> m.029sr_k\n# Answer:\nScully pub", "# Reasoning Path:\nVin Scully -> film.actor.film -> m.0gdlst9 -> film.performance.film -> Play Ball!: Official Rules & Regulations\n# Answer:\nm.0gdlst9", "# Reasoning Path:\nVin Scully -> common.topic.image -> VinScully0308 -> common.image.appears_in_topic_gallery -> Los Angeles Dodgers Radio Network\n# Answer:\nVinScully0308", "# Reasoning Path:\nVin Scully -> film.actor.film -> m.0gdlwjf -> film.performance.film -> For Love of the Game\n# Answer:\nm.0gdlwjf", "# Reasoning Path:\nVin Scully -> common.topic.image -> VinScully0308 -> common.image.size -> m.03t8x2n\n# Answer:\nVinScully0308", "# Reasoning Path:\nVin Scully -> film.actor.film -> m.0h11hq9 -> film.performance.film -> Bluetopia: The L.A. Dodgers Movie\n# Answer:\nm.0h11hq9", "# Reasoning Path:\nVin Scully -> film.actor.film -> m.0gdlwjf -> film.performance.special_performance_type -> Him/Herself\n# Answer:\nm.0gdlwjf"], "ground_truth": ["Fordham University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-643", "prediction": ["# Reasoning Path:\nLeia Organa -> fictional_universe.fictional_character.place_of_birth -> g.120mc0gk\n# Answer:\ng.120mc0gk", "# Reasoning Path:\nLeia Organa -> fictional_universe.fictional_character.places_lived -> Alderaan -> fictional_universe.fictional_setting.characters_that_have_lived_here -> Breha Organa\n# Answer:\nAlderaan", "# Reasoning Path:\nLeia Organa -> fictional_universe.fictional_character.places_lived -> Alderaan -> fictional_universe.fictional_setting.setting_type -> Planet\n# Answer:\nAlderaan", "# Reasoning Path:\nLeia Organa -> fictional_universe.fictional_character.places_lived -> Alderaan -> common.topic.notable_for -> g.1254xtwfh\n# Answer:\nAlderaan", "# Reasoning Path:\nLeia Organa -> fictional_universe.fictional_character.places_lived -> Alderaan -> fictional_universe.fictional_setting.characters_that_have_lived_here -> g.1237vxbv\n# Answer:\nAlderaan", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.010wvf3v -> film.performance.film -> Robot Chicken: Star Wars Episode III\n# Answer:\nm.010wvf3v", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.010wvf3v -> film.performance.actor -> Eden Espinosa\n# Answer:\nm.010wvf3v", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.0k2h24 -> film.performance.film -> Return of the Ewok\n# Answer:\nm.0k2h24", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.01xpnt9 -> film.performance.actor -> Carrie Fisher\n# Answer:\nm.01xpnt9", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.01xpnt9 -> film.performance.film -> Star Wars Episode V: The Empire Strikes Back\n# Answer:\nm.01xpnt9"], "ground_truth": ["Alderaan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-644", "prediction": ["# Reasoning Path:\nEl Salvador -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Venezuela\n# Answer:\nConstitutional republic", "# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> base.culturalevent.event.entity_involved -> Osm\u00edn Aguirre y Salinas\n# Answer:\n1932 Salvadoran peasant massacre", "# Reasoning Path:\nEl Salvador -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Costa Rica\n# Answer:\nConstitutional republic", "# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> base.culturalevent.event.entity_involved -> Maximiliano Hern\u00e1ndez Mart\u00ednez\n# Answer:\n1932 Salvadoran peasant massacre", "# Reasoning Path:\nEl Salvador -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Venezuela\n# Answer:\nPresidential system", "# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> common.topic.notable_for -> g.125btm68w\n# Answer:\n1932 Salvadoran peasant massacre", "# Reasoning Path:\nEl Salvador -> location.statistical_region.poverty_rate_2dollars_per_day -> g.11b6c_pzzl\n# Answer:\ng.11b6c_pzzl", "# Reasoning Path:\nEl Salvador -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Colombia\n# Answer:\nConstitutional republic", "# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> base.culturalevent.event.entity_involved -> Armed Forces of El Salvador\n# Answer:\n1932 Salvadoran peasant massacre", "# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> common.topic.article -> m.0bjpf4\n# Answer:\n1932 Salvadoran peasant massacre"], "ground_truth": ["Robert Renderos", "Wilfredo Iraheta", "Mauricio Alvarenga", "Francisca Gonz\u00e1lez", "Keoki", "Jose B. Gonzalez", "Marlon Menj\u00edvar", "Tom\u00e1s Medina", "Rene Moran", "Takeshi Fujiwara", "Jos\u00e9 Francisco Valiente", "Jose Orlando Martinez", "Sarah Ramos", "Patricia Chica", "Gualberto Fern\u00e1ndez", "Pedro Chavarria", "Armando Chac\u00f3n", "Carlos Linares", "Jos\u00e9 Castellanos Contreras", "Joel Aguilar", "Jorge B\u00facaro", "Elmer Acevedo", "Norman Quijano", "Jose Solis", "Damaris Qu\u00e9les", "Malin Arvidsson", "Victor Manuel Ochoa", "Eduardo \\\"Volkswagen\\\" Hern\u00e1ndez", "Arturo Rivera y Damas", "Victor Lopez", "Salvador Castaneda Castro", "Papa A.P.", "Rub\u00e9n Zamora", "Elena Diaz", "\u00c1ngel Orellana", "Jos\u00e9 Luis Rugamas", "Richard Oriani", "Am\u00e9rico Gonz\u00e1lez", "Genaro Serme\u00f1o", "Roberto Rivas", "Fausto Omar V\u00e1squez", "Consuelo de Saint Exup\u00e9ry", "Eva Dimas", "William L\u00f3pez", "Eduardo Hern\u00e1ndez", "Melvin Barrera", "Mario Montoya", "Miguel Cruz", "Ra\u00fal Cicero", "Emilio Guardado", "Andr\u00e9s Eduardo Men\u00e9ndez", "Francisco Gavidia", "Ricardo L\u00f3pez Tenorio", "Mauricio Alfaro", "Julio Adalberto Rivera Carballo", "Johnny Lopez", "Milton Palacios", "Camilo Minero", "William Armando", "Doroteo Vasconcelos", "Miguel Angel Deras", "Xenia Estrada", "Francisco Due\u00f1as", "Prudencia Ayala", "Paula Heredia", "Arturo Armando Molina", "Jos\u00e9 Manfredi Portillo", "Rafael Campo", "Mario Wilfredo Contreras", "Mauricio Alonso Rodr\u00edguez", "Manuel Enrique Araujo", "Jos\u00e9 Mar\u00eda Ca\u00f1as", "Roberto Carlos Martinez", "Juan Ram\u00f3n S\u00e1nchez", "V\u00edctor Ram\u00edrez", "\u00d3scar Antonio Ulloa", "Enrique \u00c1lvarez C\u00f3rdova", "Alfredo Ruano", "Gerardo Barrios", "Nicolas F. Shi", "Francisco Men\u00e9ndez", "Miguel Ca\u00f1izalez", "Juan Rafael Bustillo", "Jos\u00e9 Inocencio Alas", "Selvin Gonz\u00e1lez", "Isa\u00edas Choto", "Francisco Funes", "Ana Maria de Martinez", "Alexander Campos", "Carlos Barrios", "Jorge Rivera", "Pedro Geoffroy Rivas", "Santiago \\\"Jimmy\\\" Mellado", "Erwin McManus", "Claudia Lars", "Steve Montenegro", "Rafael Menj\u00edvar Ochoa", "Jaime Portillo", "William Renderos Iraheta", "Pedro Jos\u00e9 Escal\u00f3n", "F\u00e9lix Pineda", "Bobby Rivas", "Diego Vel\u00e1zquez", "Ernesto Aparicio", "Alexander M\u00e9ndoza", "Bernard Lewinsky", "Guillermo Garc\u00eda", "Ruben Cedillos", "Jorge Mel\u00e9ndez", "g.11b8058v7j", "Saturnino Osorio", "Ana Sol Gutierrez", "Laura Molina", "Edwin Ramos", "DJ Quest", "Rutilio Grande"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-645", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Raphael and his age\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Leonardo da Vinci and the Art of Sculpture: Inspiration and Invention\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> people.profession.people_with_this_profession -> Sandro Botticelli\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> visual_art.visual_artist.art_forms -> Fresco\n# Answer:\nAntonio da Correggio", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi -> visual_art.artwork.art_form -> Painting\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> common.topic.subject_of -> BRS Custom Painting\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> influence.influence_node.influenced_by -> Melozzo da Forl\u00ec\n# Answer:\nAntonio da Correggio", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Bernardino Luini -> visual_art.visual_artist.art_forms -> Painting\n# Answer:\nBernardino Luini", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi -> visual_art.artwork.locations -> m.0pcqbm3\n# Answer:\nAdoration of the Magi"], "ground_truth": ["Drawing", "Sculpture", "Painting"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-646", "prediction": ["# Reasoning Path:\nMary Bell -> common.topic.notable_for -> g.125bm4y8j\n# Answer:\ng.125bm4y8j", "# Reasoning Path:\nMary Bell -> people.person.place_of_birth -> Newcastle upon Tyne -> base.aareas.schema.administrative_area.administrative_area_type -> English metropolitan borough city\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nMary Bell -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.containedby -> Tyne and Wear\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nMary Bell -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.containedby -> England\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nMary Bell -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.contains -> Basil's Kitchen\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nMary Bell -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.containedby -> North East England\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nMary Bell -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.contains -> Blakelaw and North Fenham\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nMary Bell -> book.book_subject.works -> Cries Unheard: Why Children Kill - The Story of Mary Bell -> book.book.genre -> Non-fiction\n# Answer:\nCries Unheard: Why Children Kill - The Story of Mary Bell", "# Reasoning Path:\nMary Bell -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.contains -> Brunswick\n# Answer:\nNewcastle upon Tyne"], "ground_truth": ["Newcastle upon Tyne"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-648", "prediction": ["# Reasoning Path:\nKimberly Williams-Paisley -> people.person.spouse_s -> m.02kkn0v -> people.marriage.spouse -> Brad Paisley\n# Answer:\nm.02kkn0v", "# Reasoning Path:\nKimberly Williams-Paisley -> people.person.parents -> Gurney Williams -> people.person.spouse_s -> m.0h2h0f_\n# Answer:\nGurney Williams", "# Reasoning Path:\nKimberly Williams-Paisley -> people.person.spouse_s -> m.02kkn0v -> people.marriage.location_of_ceremony -> Pepperdine University\n# Answer:\nm.02kkn0v", "# Reasoning Path:\nKimberly Williams-Paisley -> people.person.spouse_s -> m.02kkn0v -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.02kkn0v", "# Reasoning Path:\nKimberly Williams-Paisley -> people.person.parents -> Linda Barbara Williams -> people.person.spouse_s -> m.0h2h0f_\n# Answer:\nLinda Barbara Williams", "# Reasoning Path:\nKimberly Williams-Paisley -> people.person.parents -> Gurney Williams -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nGurney Williams", "# Reasoning Path:\nKimberly Williams-Paisley -> people.person.parents -> Gurney Williams -> people.person.profession -> Journalist\n# Answer:\nGurney Williams", "# Reasoning Path:\nKimberly Williams-Paisley -> common.topic.webpage -> m.062tr2x -> common.webpage.category -> Official Website\n# Answer:\nm.062tr2x", "# Reasoning Path:\nKimberly Williams-Paisley -> people.person.parents -> Gurney Williams -> freebase.valuenotation.has_value -> Parents\n# Answer:\nGurney Williams", "# Reasoning Path:\nKimberly Williams-Paisley -> people.person.parents -> Gurney Williams -> freebase.valuenotation.has_value -> Place of birth\n# Answer:\nGurney Williams"], "ground_truth": ["Brad Paisley"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-649", "prediction": ["# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0hnjpjf -> award.award_nomination.nominated_for -> Crazy, Stupid, Love.\n# Answer:\nm.0hnjpjf", "# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0z83ls_ -> award.award_nomination.nominated_for -> Crazy, Stupid, Love.\n# Answer:\nm.0z83ls_", "# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0hn5tnq -> award.award_nomination.nominated_for -> Drive\n# Answer:\nm.0hn5tnq", "# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0hnjpjf -> award.award_nomination.ceremony -> 69th Golden Globe Awards\n# Answer:\nm.0hnjpjf", "# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0z83ls_ -> award.award_nomination.award -> Teen Choice Award for Choice Movie Actor: Comedy\n# Answer:\nm.0z83ls_", "# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0z83ls_ -> award.award_nomination.ceremony -> 2012 Teen Choice Awards\n# Answer:\nm.0z83ls_", "# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0hn5tnq -> award.award_nomination.ceremony -> 17th Critics' Choice Awards\n# Answer:\nm.0hn5tnq", "# Reasoning Path:\nRyan Gosling -> award.award_winner.awards_won -> m.0111jbyr -> award.award_honor.award -> Best Foreign Actor\n# Answer:\nm.0111jbyr", "# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0hn5tnq -> award.award_nomination.award -> Critics' Choice Movie Award for Best Actor\n# Answer:\nm.0hn5tnq", "# Reasoning Path:\nRyan Gosling -> award.award_winner.awards_won -> m.0111jbyr -> award.award_honor.honored_for -> The Believer\n# Answer:\nm.0111jbyr"], "ground_truth": ["I'm Still Here: Real Diaries of Young People Who Lived During the Holocaust", "The Unbelievables", "The Believer", "Half Nelson", "Weightless", "The Place Beyond the Pines", "The Notebook", "Drunk History Christmas - Feat. Ryan Gosling, Jim Carrey & Eva Mendes", "All Good Things", "Crazy, Stupid, Love.", "Only God Forgives", "The United States of Leland", "Touch of Evil", "Remember the Titans", "Blue Valentine", "The Slaughter Rule", "Lars and the Real Girl", "Murder by Numbers", "The Nice Guys", "Gangster Squad", "The Big Short", "Fracture", "The Ides of March", "Logan's Run", "Drive", "Stay", "Nothing Too Good for a Cowboy"], "ans_acc": 0.1111111111111111, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.0879120879120879, "path_precision": 0.4, "path_recall": 0.04938271604938271, "path_ans_f1": 0.1739130434782609, "path_ans_precision": 0.4, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.office_holder -> Nikita Khrushchev\n# Answer:\nm.049x6zw", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_6 -> government.government_position_held.office_holder -> Vladimir Ivashko\n# Answer:\nm.049x6_6", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_commander -> Nikita Khrushchev\n# Answer:\nm.0vmx6nr", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.governmental_body -> Central Committee of the Communist Party of the Soviet Union\n# Answer:\nm.049x6zw", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_conflict -> Cuban Missile Crisis\n# Answer:\nm.0vmx6nr", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_holder -> Joseph Stalin\n# Answer:\nm.049x6_k", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf -> military.military_command.military_commander -> Joseph Stalin\n# Answer:\nm.02h7nmf", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_6 -> government.government_position_held.office_position_or_title -> General Secretary of the Communist Party of the Soviet Union\n# Answer:\nm.049x6_6", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_position_or_title -> General Secretary of the Communist Party of the Soviet Union\n# Answer:\nm.049x6_k", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.049y3kf -> military.military_command.military_commander -> Ivan Konev\n# Answer:\nm.049y3kf"], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-650", "prediction": ["# Reasoning Path:\nJulia Gillard -> government.politician.party -> m.046vpjr -> government.political_party_tenure.party -> Australian Labor Party\n# Answer:\nm.046vpjr", "# Reasoning Path:\nJulia Gillard -> government.politician.party -> m.0lr1xnw -> government.political_party_tenure.party -> Queensland Labor Party\n# Answer:\nm.0lr1xnw", "# Reasoning Path:\nJulia Gillard -> common.topic.article -> m.02kx9y\n# Answer:\nm.02kx9y", "# Reasoning Path:\nJulia Gillard -> people.person.parents -> John Gillard -> people.deceased_person.place_of_death -> Adelaide\n# Answer:\nJohn Gillard", "# Reasoning Path:\nJulia Gillard -> people.person.parents -> John Gillard -> people.person.children -> Alison Gillard\n# Answer:\nJohn Gillard", "# Reasoning Path:\nJulia Gillard -> people.person.parents -> Moira Gillard -> people.person.children -> Alison Gillard\n# Answer:\nMoira Gillard", "# Reasoning Path:\nJulia Gillard -> people.person.parents -> John Gillard -> people.person.profession -> Registered nurse\n# Answer:\nJohn Gillard", "# Reasoning Path:\nJulia Gillard -> people.person.parents -> Moira Gillard -> people.person.nationality -> Australia\n# Answer:\nMoira Gillard", "# Reasoning Path:\nJulia Gillard -> people.person.parents -> Moira Gillard -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nMoira Gillard", "# Reasoning Path:\nJulia Gillard -> people.person.parents -> Moira Gillard -> freebase.valuenotation.has_value -> Parents\n# Answer:\nMoira Gillard"], "ground_truth": ["Australian Labor Party", "Queensland Labor Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-651", "prediction": ["# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\ng.11b60v9zy7", "# Reasoning Path:\nGermany -> location.location.partially_contains -> Funtenseetauern -> location.location.partially_containedby -> Austria\n# Answer:\nFuntenseetauern", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fdl8\n# Answer:\ng.12tb6fdl8", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_38m_\n# Answer:\ng.1245_38m_", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc38c_4\n# Answer:\ng.1hhc38c_4", "# Reasoning Path:\nGermany -> location.location.partially_contains -> Fellhorn -> location.location.partially_containedby -> Austria\n# Answer:\nFellhorn", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_67jr\n# Answer:\ng.1245_67jr", "# Reasoning Path:\nGermany -> location.location.partially_contains -> Funtenseetauern -> location.location.partially_contained_by -> m.0x175dk\n# Answer:\nFuntenseetauern", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_jlz4\n# Answer:\ng.1245_jlz4", "# Reasoning Path:\nGermany -> location.location.partially_contains -> Funtenseetauern -> location.location.containedby -> Europe\n# Answer:\nFuntenseetauern"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-652", "prediction": ["# Reasoning Path:\nSelena Gomez -> people.person.ethnicity -> Italian American -> common.topic.notable_types -> Ethnicity\n# Answer:\nItalian American", "# Reasoning Path:\nSelena Gomez -> people.person.ethnicity -> Italian American -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Alex Russo\n# Answer:\nItalian American", "# Reasoning Path:\nSelena Gomez -> people.person.ethnicity -> Italian American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nItalian American", "# Reasoning Path:\nSelena Gomez -> people.person.ethnicity -> Mexicans -> people.ethnicity.geographic_distribution -> Mexico\n# Answer:\nMexicans", "# Reasoning Path:\nSelena Gomez -> people.person.ethnicity -> Mexicans -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nMexicans", "# Reasoning Path:\nSelena Gomez -> music.artist.album -> Another Cinderella Story EP -> common.topic.notable_for -> g.1ydnq94r6\n# Answer:\nAnother Cinderella Story EP", "# Reasoning Path:\nSelena Gomez -> award.award_nominee.award_nominations -> m.010wr51v -> award.award_nomination.award -> Teen Choice Award for Choice Smile\n# Answer:\nm.010wr51v", "# Reasoning Path:\nSelena Gomez -> people.person.ethnicity -> Mexicans -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nMexicans", "# Reasoning Path:\nSelena Gomez -> music.artist.album -> Another Cinderella Story EP -> music.album.releases -> Another Cinderella Story\n# Answer:\nAnother Cinderella Story EP", "# Reasoning Path:\nSelena Gomez -> music.artist.album -> Cruella de Vil -> freebase.valuenotation.has_value -> Album content type\n# Answer:\nCruella de Vil"], "ground_truth": ["Mexicans", "Italian American"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-653", "prediction": ["# Reasoning Path:\nDenise van Outen -> people.person.spouse_s -> m.062thbp -> people.marriage.spouse -> Lee Mead\n# Answer:\nm.062thbp", "# Reasoning Path:\nDenise van Outen -> people.person.spouse_s -> m.062thbp -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.062thbp", "# Reasoning Path:\nDenise van Outen -> people.person.spouse_s -> m.062thbp -> people.marriage.location_of_ceremony -> Seychelles\n# Answer:\nm.062thbp", "# Reasoning Path:\nDenise van Outen -> music.artist.track -> Tyler King -> music.recording.releases -> Tell Me on a Sunday\n# Answer:\nTyler King", "# Reasoning Path:\nDenise van Outen -> base.popstra.celebrity.dated -> m.065pw0h -> base.popstra.dated.participant -> Nick Moran\n# Answer:\nm.065pw0h", "# Reasoning Path:\nDenise van Outen -> base.popstra.celebrity.dated -> m.065pybf -> base.popstra.dated.participant -> Richard Traviss\n# Answer:\nm.065pybf", "# Reasoning Path:\nDenise van Outen -> music.artist.track -> Tyler King -> music.recording.artist -> Andrew Lloyd Webber\n# Answer:\nTyler King", "# Reasoning Path:\nDenise van Outen -> base.popstra.celebrity.dated -> m.065pzwy -> base.popstra.dated.participant -> James Lance\n# Answer:\nm.065pzwy", "# Reasoning Path:\nDenise van Outen -> music.artist.track -> Capped Teeth and Caesar Salad -> music.recording.contributions -> m.0njmjp8\n# Answer:\nCapped Teeth and Caesar Salad", "# Reasoning Path:\nDenise van Outen -> music.artist.track -> Can't Take My Eyes Off You -> music.recording.artist -> Andy Williams\n# Answer:\nCan't Take My Eyes Off You"], "ground_truth": ["Lee Mead"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-654", "prediction": ["# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\ng.12cp_k6sh", "# Reasoning Path:\nEgypt -> location.statistical_region.gdp_real -> g.11b60s4lvy\n# Answer:\ng.11b60s4lvy", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> common.topic.notable_types -> Governmental Body\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\ng.1hhc3_4cn", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3f_h6\n# Answer:\ng.1hhc3f_h6", "# Reasoning Path:\nEgypt -> location.statistical_region.gdp_real -> g.12tb6hbyz\n# Answer:\ng.12tb6hbyz", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> location.statistical_region.gdp_real -> g.1hhc37cv3\n# Answer:\ng.1hhc37cv3", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> government.governmental_body.component_bodies -> House of Representatives\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nHouse of Representatives"], "ground_truth": ["Egyptian pound"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-655", "prediction": ["# Reasoning Path:\nWyoming -> location.citytown.postal_codes -> 49509 -> location.location.geolocation -> m.03ds4y_\n# Answer:\n49509", "# Reasoning Path:\nWyoming -> location.citytown.postal_codes -> 49509 -> common.topic.notable_for -> g.125dngrry\n# Answer:\n49509", "# Reasoning Path:\nWyoming -> location.citytown.postal_codes -> 49519 -> common.topic.notable_types -> Postal Code\n# Answer:\n49519", "# Reasoning Path:\nWyoming -> location.citytown.postal_codes -> 49509 -> location.location.containedby -> Kent County\n# Answer:\n49509", "# Reasoning Path:\nWyoming -> location.statistical_region.population -> g.11b674hfwb\n# Answer:\ng.11b674hfwb", "# Reasoning Path:\nWyoming -> location.citytown.postal_codes -> 49519 -> common.topic.notable_for -> g.1256qhz91\n# Answer:\n49519", "# Reasoning Path:\nWyoming -> location.citytown.postal_codes -> 49519 -> location.postal_code.country -> United States of America\n# Answer:\n49519", "# Reasoning Path:\nWyoming -> location.statistical_region.population -> g.11bymkw_85\n# Answer:\ng.11bymkw_85", "# Reasoning Path:\nWyoming -> location.statistical_region.population -> g.11x1chmfg\n# Answer:\ng.11x1chmfg", "# Reasoning Path:\nWyoming -> location.location.containedby -> Kent County -> location.statistical_region.population -> g.11b66bsm4_\n# Answer:\nKent County"], "ground_truth": ["49509", "49519"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-656", "prediction": ["# Reasoning Path:\nCosta Rica -> government.governmental_jurisdiction.governing_officials -> m.0j5qnx9 -> government.government_position_held.office_holder -> Laura Chinchilla\n# Answer:\nm.0j5qnx9", "# Reasoning Path:\nCosta Rica -> government.governmental_jurisdiction.governing_officials -> m.010g6lmd -> government.government_position_held.office_holder -> \u00d3scar Arias\n# Answer:\nm.010g6lmd", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60wmkvh\n# Answer:\ng.11b60wmkvh", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.11b71w3_6h\n# Answer:\ng.11b71w3_6h", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_5xgy\n# Answer:\ng.1245_5xgy", "# Reasoning Path:\nCosta Rica -> government.governmental_jurisdiction.governing_officials -> m.0j5qnx9 -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nm.0j5qnx9", "# Reasoning Path:\nCosta Rica -> government.governmental_jurisdiction.governing_officials -> m.0105mtsf -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nm.0105mtsf", "# Reasoning Path:\nCosta Rica -> government.governmental_jurisdiction.governing_officials -> m.0105mtsf -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0105mtsf", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_6n42\n# Answer:\ng.1245_6n42", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.1hhc37pjz\n# Answer:\ng.1hhc37pjz"], "ground_truth": ["Laura Chinchilla"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-657", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.events -> Assassination of Martin Luther King, Jr.\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.containedby -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> travel.tourist_attraction.near_travel_destination -> Atlanta\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> location.location.geolocation -> m.0wmyhzk\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.06_41f8\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Montgomery\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> common.topic.article -> m.05nnh0\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Alabama\n# Answer:\nDexter Avenue Baptist Church"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-658", "prediction": ["# Reasoning Path:\nGospel of John -> book.written_work.author -> John the Evangelist -> symbols.name_source.namesakes -> Anglican Church of St. John the Evangelist\n# Answer:\nJohn the Evangelist", "# Reasoning Path:\nGospel of John -> common.topic.notable_for -> g.125brlxm_\n# Answer:\ng.125brlxm_", "# Reasoning Path:\nGospel of John -> book.written_work.subjects -> Jesus Christ -> people.person.ethnicity -> Jewish people\n# Answer:\nJesus Christ", "# Reasoning Path:\nGospel of John -> book.written_work.author -> John the Evangelist -> symbols.name_source.namesakes -> Cathedral of St. John the Evangelist\n# Answer:\nJohn the Evangelist", "# Reasoning Path:\nGospel of John -> book.written_work.author -> John the Evangelist -> common.topic.image -> John-the-evangelist-francesco-furini\n# Answer:\nJohn the Evangelist", "# Reasoning Path:\nGospel of John -> book.written_work.author -> John the Evangelist -> symbols.name_source.namesakes -> Church of St John the Evangelist, Poulton-le-Fylde\n# Answer:\nJohn the Evangelist", "# Reasoning Path:\nGospel of John -> book.written_work.author -> John the Evangelist -> base.saints.saint.venerated_in -> Anglican Communion\n# Answer:\nJohn the Evangelist", "# Reasoning Path:\nGospel of John -> book.written_work.author -> John the Evangelist -> common.topic.image -> JohnEvangelistReni\n# Answer:\nJohn the Evangelist", "# Reasoning Path:\nGospel of John -> book.written_work.subjects -> Jesus Christ -> common.topic.webpage -> m.09w3_0l\n# Answer:\nJesus Christ", "# Reasoning Path:\nGospel of John -> book.written_work.author -> John the Evangelist -> common.topic.image -> St John the Evangelist, imagined by Jacopo Pontormo, ca 1525 (Santa Felicita, Florence)\n# Answer:\nJohn the Evangelist"], "ground_truth": ["John the Evangelist"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-659", "prediction": ["# Reasoning Path:\nUnited Kingdom -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nUnited Kingdom -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Republic of Ireland\n# Answer:\nUnitary state", "# Reasoning Path:\nUnited Kingdom -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Angola\n# Answer:\nUnitary state", "# Reasoning Path:\nUnited Kingdom -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Anguilla\n# Answer:\nParliamentary system", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nm.0ghc35h", "# Reasoning Path:\nUnited Kingdom -> location.statistical_region.long_term_unemployment_rate -> g.12cp_jvpx\n# Answer:\ng.12cp_jvpx", "# Reasoning Path:\nUnited Kingdom -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nUnited Kingdom -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Antigua and Barbuda\n# Answer:\nParliamentary system", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.arrested_person -> Gary Glitter\n# Answer:\nm.0ghc35h", "# Reasoning Path:\nUnited Kingdom -> location.statistical_region.long_term_unemployment_rate -> g.1hhc390lx\n# Answer:\ng.1hhc390lx"], "ground_truth": ["Constitutional monarchy", "Parliamentary system", "Unitary state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.target_of_exchange -> m.03x8_16\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> common.topic.article -> m.09nqr\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.target_of_exchange -> m.04kq24y\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> Federated States of Micronesia\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.statistical_region.gdp_deflator_change -> g.11b6ddwl64\n# Answer:\ng.11b6ddwl64", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> American Samoa\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Frontpoint Security Solutions -> organization.organization.geographic_scope -> United States of America\n# Answer:\nFrontpoint Security Solutions", "# Reasoning Path:\nPuerto Rico -> location.statistical_region.gdp_deflator_change -> g.124x8g_1y\n# Answer:\ng.124x8g_1y", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> Bonaire\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> business.business_operation.industry -> Real Estate Investment Trust\n# Answer:\nDDR Corp."], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-661", "prediction": ["# Reasoning Path:\nSamir Nasri -> base.schemastaging.athlete_extra.salary -> m.0j6tdy1 -> base.schemastaging.athlete_salary.team -> Manchester City F.C.\n# Answer:\nm.0j6tdy1", "# Reasoning Path:\nSamir Nasri -> base.schemastaging.athlete_extra.salary -> m.0j6tdy1 -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nm.0j6tdy1", "# Reasoning Path:\nSamir Nasri -> sports.pro_athlete.teams -> m.04mb0rc -> sports.sports_team_roster.team -> Arsenal F.C.\n# Answer:\nm.04mb0rc", "# Reasoning Path:\nSamir Nasri -> sports.pro_athlete.teams -> m.04mb0rc -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nm.04mb0rc", "# Reasoning Path:\nSamir Nasri -> sports.pro_athlete.teams -> m.0j52lrl -> sports.sports_team_roster.team -> France national football team\n# Answer:\nm.0j52lrl", "# Reasoning Path:\nSamir Nasri -> sports.pro_athlete.teams -> m.0j6tdwm -> sports.sports_team_roster.team -> Manchester City F.C.\n# Answer:\nm.0j6tdwm", "# Reasoning Path:\nSamir Nasri -> people.person.sibling_s -> m.0j52sl5 -> people.sibling_relationship.sibling -> Malik Nasri\n# Answer:\nm.0j52sl5", "# Reasoning Path:\nSamir Nasri -> sports.pro_athlete.teams -> m.0j52lrl -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nm.0j52lrl", "# Reasoning Path:\nSamir Nasri -> sports.pro_athlete.teams -> m.0j52lrl -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0j52lrl", "# Reasoning Path:\nSamir Nasri -> sports.pro_athlete.teams -> m.0j6tdwm -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nm.0j6tdwm"], "ground_truth": ["Olympique de Marseille"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-662", "prediction": ["# Reasoning Path:\nCzech Republic -> location.country.currency_used -> Czech koruna -> common.topic.notable_types -> Currency\n# Answer:\nCzech koruna", "# Reasoning Path:\nCzech Republic -> media_common.netflix_genre.titles -> Fruit of Paradise -> film.film.country -> Czechoslovakia\n# Answer:\nFruit of Paradise", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> Beskids -> location.location.partially_contained_by -> m.0wg8sdh\n# Answer:\nBeskids", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> B\u00f3br -> geography.river.mouth -> Oder\n# Answer:\nB\u00f3br", "# Reasoning Path:\nCzech Republic -> media_common.netflix_genre.titles -> Adelheid -> film.film.country -> Czechoslovakia\n# Answer:\nAdelheid", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> Beskids -> location.location.partially_containedby -> Slovakia\n# Answer:\nBeskids", "# Reasoning Path:\nCzech Republic -> media_common.netflix_genre.titles -> Fruit of Paradise -> media_common.netflix_title.netflix_genres -> Drama\n# Answer:\nFruit of Paradise", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> Beskids -> location.location.partially_containedby -> Poland\n# Answer:\nBeskids", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> B\u00f3br -> location.location.partially_containedby -> Poland\n# Answer:\nB\u00f3br", "# Reasoning Path:\nCzech Republic -> media_common.netflix_genre.titles -> Fruit of Paradise -> common.topic.notable_types -> Film\n# Answer:\nFruit of Paradise"], "ground_truth": ["Czech koruna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-663", "prediction": ["# Reasoning Path:\nWilliam Penn -> people.person.place_of_birth -> Bristol -> location.location.containedby -> England\n# Answer:\nBristol", "# Reasoning Path:\nWilliam Penn -> people.person.place_of_birth -> Bristol -> location.location.containedby -> United Kingdom\n# Answer:\nBristol", "# Reasoning Path:\nWilliam Penn -> people.person.parents -> Giles Penn\n# Answer:\nGiles Penn", "# Reasoning Path:\nWilliam Penn -> people.person.place_of_birth -> Bristol -> base.biblioness.bibs_location.country -> United Kingdom\n# Answer:\nBristol", "# Reasoning Path:\nWilliam Penn -> people.person.place_of_birth -> London -> location.location.containedby -> England\n# Answer:\nLondon", "# Reasoning Path:\nWilliam Penn -> symbols.name_source.namesakes -> Penn -> location.statistical_region.population -> m.0cpktdq\n# Answer:\nPenn", "# Reasoning Path:\nWilliam Penn -> people.person.parents -> Joan Gilbert\n# Answer:\nJoan Gilbert", "# Reasoning Path:\nWilliam Penn -> people.person.place_of_birth -> London -> base.biblioness.bibs_location.country -> United Kingdom\n# Answer:\nLondon", "# Reasoning Path:\nWilliam Penn -> people.person.place_of_birth -> London -> location.location.containedby -> Great Britain\n# Answer:\nLondon", "# Reasoning Path:\nWilliam Penn -> symbols.name_source.namesakes -> Penn School Historic District -> location.location.containedby -> Beaufort County\n# Answer:\nPenn School Historic District"], "ground_truth": ["1672-04-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-664", "prediction": ["# Reasoning Path:\nFirefox -> computer.software.software_genre -> File Transfer Protocol -> computer.software_genre.software_in_genre -> 3D-FTP\n# Answer:\nFile Transfer Protocol", "# Reasoning Path:\nFirefox -> computer.web_browser.extensions -> Adblock Plus -> computer.software.developer -> Wladimir Palant\n# Answer:\nAdblock Plus", "# Reasoning Path:\nFirefox -> computer.software.software_genre -> File Transfer Protocol -> base.schemastaging.context_name.pronunciation -> g.125_qdwb5\n# Answer:\nFile Transfer Protocol", "# Reasoning Path:\nFirefox -> computer.software.software_genre -> Web browser -> base.ontologies.ontology_instance.equivalent_instances -> m.07ng00y\n# Answer:\nWeb browser", "# Reasoning Path:\nFirefox -> computer.web_browser.extensions -> Firebug -> computer.software.developer -> g.11bc13vmc7\n# Answer:\nFirebug", "# Reasoning Path:\nFirefox -> common.topic.image -> Firefox 2.0 -> common.image.size -> m.02hzhbc\n# Answer:\nFirefox 2.0", "# Reasoning Path:\nFirefox -> computer.software.software_genre -> File Transfer Protocol -> computer.software_genre.software_in_genre -> BitKinex\n# Answer:\nFile Transfer Protocol", "# Reasoning Path:\nFirefox -> computer.software.software_genre -> File Transfer Protocol -> computer.internet_protocol.software_used_by -> Backup4all\n# Answer:\nFile Transfer Protocol", "# Reasoning Path:\nFirefox -> computer.software.software_genre -> News aggregator -> internet.website_category.sites -> Fark\n# Answer:\nNews aggregator", "# Reasoning Path:\nFirefox -> computer.software.software_genre -> News aggregator -> common.topic.subject_of -> AdHaven\n# Answer:\nNews aggregator"], "ground_truth": ["XBL", "XUL", "Cascading Style Sheets", "JavaScript", "C++"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.28571428571428575, "ans_precission": 0.5, "ans_recall": 0.2, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 1.0, "path_ans_recall": 0.2}
{"id": "WebQTest-665", "prediction": ["# Reasoning Path:\nNew Zealand -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nNew Zealand -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nNew Zealand -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Japan\n# Answer:\nUnitary state", "# Reasoning Path:\nNew Zealand -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Netherlands\n# Answer:\nUnitary state", "# Reasoning Path:\nNew Zealand -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Australia\n# Answer:\nParliamentary system", "# Reasoning Path:\nNew Zealand -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Canada\n# Answer:\nParliamentary system", "# Reasoning Path:\nNew Zealand -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Greece\n# Answer:\nParliamentary system", "# Reasoning Path:\nNew Zealand -> olympics.olympic_participating_country.medals_won -> m.04gdvs5 -> olympics.olympic_medal_honor.medal -> Gold medal\n# Answer:\nm.04gdvs5", "# Reasoning Path:\nNew Zealand -> film.film_location.featured_in_films -> Mee-Shee: The Water Giant -> film.film.language -> English Language\n# Answer:\nMee-Shee: The Water Giant", "# Reasoning Path:\nNew Zealand -> film.film_location.featured_in_films -> Mee-Shee: The Water Giant -> media_common.netflix_title.netflix_genres -> Family\n# Answer:\nMee-Shee: The Water Giant"], "ground_truth": ["Constitutional monarchy", "Parliamentary system", "Unitary state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-666", "prediction": ["# Reasoning Path:\nIsthmus of Panama -> location.location.containedby -> South America -> location.location.containedby -> Americas\n# Answer:\nSouth America", "# Reasoning Path:\nIsthmus of Panama -> common.topic.article -> m.04zwg3\n# Answer:\nm.04zwg3", "# Reasoning Path:\nIsthmus of Panama -> location.location.containedby -> South America -> location.location.containedby -> DVD Region 4\n# Answer:\nSouth America", "# Reasoning Path:\nIsthmus of Panama -> location.location.containedby -> South America -> location.location.containedby -> Western Hemisphere\n# Answer:\nSouth America", "# Reasoning Path:\nIsthmus of Panama -> location.location.containedby -> South America -> location.location.events -> 2009 flu pandemic\n# Answer:\nSouth America", "# Reasoning Path:\nIsthmus of Panama -> common.topic.image -> Pm-map -> common.image.size -> m.02bh8qh\n# Answer:\nPm-map", "# Reasoning Path:\nIsthmus of Panama -> location.location.containedby -> South America -> location.location.events -> 2013 Infinite 1st World Tour\n# Answer:\nSouth America", "# Reasoning Path:\nIsthmus of Panama -> location.location.containedby -> South America -> book.book_subject.works -> Dioses Y Legiones\n# Answer:\nSouth America", "# Reasoning Path:\nIsthmus of Panama -> location.location.containedby -> South America -> location.location.events -> 2016 Copa Libertadores\n# Answer:\nSouth America", "# Reasoning Path:\nIsthmus of Panama -> location.location.containedby -> South America -> book.book_subject.works -> A Brazilian Alphabet for the Younger Reader\n# Answer:\nSouth America"], "ground_truth": ["South America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-667", "prediction": ["# Reasoning Path:\nRepublic of Ireland -> location.country.capital -> Dublin -> location.location.containedby -> Ireland\n# Answer:\nDublin", "# Reasoning Path:\nRepublic of Ireland -> location.country.capital -> Dublin -> base.biblioness.bibs_location.country -> Ireland\n# Answer:\nDublin", "# Reasoning Path:\nRepublic of Ireland -> location.country.capital -> Dublin -> location.location.time_zones -> Irish Time Zone\n# Answer:\nDublin", "# Reasoning Path:\nRepublic of Ireland -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdmsw\n# Answer:\ng.11b71mdmsw", "# Reasoning Path:\nRepublic of Ireland -> location.statistical_region.co2_emissions_per_capita -> g.1245_0s2z\n# Answer:\ng.1245_0s2z", "# Reasoning Path:\nRepublic of Ireland -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fm09\n# Answer:\ng.12tb6fm09", "# Reasoning Path:\nRepublic of Ireland -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37x9c\n# Answer:\ng.1hhc37x9c", "# Reasoning Path:\nRepublic of Ireland -> location.statistical_region.co2_emissions_per_capita -> g.1245_0sgs\n# Answer:\ng.1245_0sgs", "# Reasoning Path:\nRepublic of Ireland -> location.statistical_region.co2_emissions_per_capita -> g.1245_1hyj\n# Answer:\ng.1245_1hyj"], "ground_truth": ["Dublin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-668", "prediction": ["# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0z83n2p -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nm.0z83n2p", "# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nm.0z83xlj", "# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0sgl9q8 -> award.award_nomination.nominated_for -> The Twilight Saga: New Moon\n# Answer:\nm.0sgl9q8", "# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0z83n2p -> award.award_nomination.ceremony -> 2012 Teen Choice Awards\n# Answer:\nm.0z83n2p", "# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.award_nominee -> Robert Pattinson\n# Answer:\nm.0z83xlj", "# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.ceremony -> 2012 Teen Choice Awards\n# Answer:\nm.0z83xlj", "# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0sgl9q8 -> award.award_nomination.award_nominee -> Taylor Lautner\n# Answer:\nm.0sgl9q8", "# Reasoning Path:\nKristen Stewart -> base.popstra.celebrity.dated -> m.064tjrc -> base.popstra.dated.participant -> Michael Angarano\n# Answer:\nm.064tjrc", "# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0sgl9q8 -> award.award_nomination.award -> Blimp Award for Cutest Couple\n# Answer:\nm.0sgl9q8", "# Reasoning Path:\nKristen Stewart -> base.popstra.celebrity.bought -> m.064tkql -> base.popstra.product_choice.product -> Chrysler White Aspen\n# Answer:\nm.064tkql"], "ground_truth": ["Jumper", "The Cake Eaters", "Cutlass", "Still Alice", "Zathura", "The Big Shoe", "Anesthesia", "Into the Wild", "Snow White and the Huntsman", "The Runaways", "Camp X-Ray", "Billy Lynn's Long Halftime Walk", "The Twilight Saga: Breaking Dawn - Part 1", "Cold Creek Manor", "The Safety of Objects", "Adventureland", "Panic Room", "Clouds of Sils Maria", "Equals", "On the Road", "The Twilight Saga: New Moon", "The Twilight Saga: Breaking Dawn - Part 2", "Speak", "Fierce People", "Eclipse", "Twilight", "Catch That Kid", "The Flintstones in Viva Rock Vegas", "The Thirteenth Year", "American Ultra", "The Yellow Handkerchief", "What Just Happened", "Snow White and the Huntsman 2", "The Messengers", "Welcome to the Rileys", "Undertow", "In the Land of Women"], "ans_acc": 0.08108108108108109, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.04477611940298508, "path_precision": 0.3, "path_recall": 0.024193548387096774, "path_ans_f1": 0.12765957446808512, "path_ans_precision": 0.3, "path_ans_recall": 0.08108108108108109}
{"id": "WebQTest-669", "prediction": ["# Reasoning Path:\nJosef Mengele -> base.activism.activist.area_of_activism -> Nazism -> base.activism.activism_issue.supporting_political_parties -> Nazi Party\n# Answer:\nNazism", "# Reasoning Path:\nJosef Mengele -> people.person.profession -> Physician -> media_common.quotation_subject.quotations_about_this_subject -> A doctor, like anyone else who has to deal with human beings, each of them unique, cannot be a scientist; he is either, like the surgeon, a craftsman, or, like the physician and the psychologist, an artist. This means that in order to be a good doctor a man must also have a good character, that is to say, whatever weaknesses and foibles he may have, he must love his fellow human beings in the concrete and desire their good before his own.\n# Answer:\nPhysician", "# Reasoning Path:\nJosef Mengele -> base.activism.activist.area_of_activism -> Nazism -> base.activism.activism_issue.activist_organizations -> Nazi Party\n# Answer:\nNazism", "# Reasoning Path:\nJosef Mengele -> people.person.profession -> Physician -> media_common.quotation_subject.quotations_about_this_subject -> A patient going to a doctor for his first visit was asked, And whom did you consult before coming to me? Only the village druggist, was the answer. And what sort of foolish advice did that numbskull give you? asked the doctor, his tone and manner denoting his contempt for the advice of the layman. Oh, replied his patient, with no malice aforethought, he told me to come and see you.\n# Answer:\nPhysician", "# Reasoning Path:\nJosef Mengele -> people.person.profession -> Physician -> people.profession.specializations -> Anesthesiologist\n# Answer:\nPhysician", "# Reasoning Path:\nJosef Mengele -> base.activism.activist.area_of_activism -> Nazism -> base.activism.activism_issue.supporting_political_parties -> American Nazi Party\n# Answer:\nNazism", "# Reasoning Path:\nJosef Mengele -> people.person.profession -> Physician -> media_common.quotation_subject.quotations_about_this_subject -> A skilful leech is better far, than half a hundred men of war.\n# Answer:\nPhysician", "# Reasoning Path:\nJosef Mengele -> people.person.profession -> Physician -> base.descriptive_names.names.descriptive_name -> m.0105bg71\n# Answer:\nPhysician", "# Reasoning Path:\nJosef Mengele -> base.activism.activist.area_of_activism -> Nazism -> base.activism.activism_issue.supporting_political_parties -> Aryan League\n# Answer:\nNazism", "# Reasoning Path:\nJosef Mengele -> people.person.profession -> Physician -> people.profession.specializations -> Army Doctor\n# Answer:\nPhysician"], "ground_truth": ["Physician"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Methotrexate\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Erlotinib\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Adjuvant\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Epilepsy\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.symptoms -> Sputum\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> common.topic.webpage -> m.09v_zjv\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.symptoms -> Air crescent sign\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Dementia\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease.medical_specialties -> Neurology\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.symptoms -> Blumer's shelf\n# Answer:\nLung cancer"], "ground_truth": ["Brain tumor", "Lung cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-670", "prediction": ["# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.region -> Africa\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> base.rosetta.languoid.parent -> Arabic Group\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Nobiin Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nNobiin Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.writing_system -> Arabic alphabet\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Bedawi Arabic -> language.human_language.countries_spoken_in -> Jordan\n# Answer:\nBedawi Arabic", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Nobiin Language -> common.topic.notable_types -> Human Language\n# Answer:\nNobiin Language", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\ng.12cp_k6sh", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Bedawi Arabic -> language.human_language.countries_spoken_in -> Syria\n# Answer:\nBedawi Arabic", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Bedawi Arabic -> language.language_dialect.language -> Arabic Language\n# Answer:\nBedawi Arabic", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Bedawi Arabic -> language.human_language.language_family -> Afroasiatic languages\n# Answer:\nBedawi Arabic"], "ground_truth": ["Modern Standard Arabic"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-671", "prediction": ["# Reasoning Path:\nHawaii -> government.political_district.representatives -> m.030qq8b -> government.government_position_held.office_holder -> Daniel Kahikina Akaka\n# Answer:\nm.030qq8b", "# Reasoning Path:\nHawaii -> government.political_district.representatives -> m.030r1fw -> government.government_position_held.office_holder -> Daniel Inouye\n# Answer:\nm.030r1fw", "# Reasoning Path:\nHawaii -> government.political_district.representatives -> m.09dm85m -> government.government_position_held.office_holder -> Hiram Fong\n# Answer:\nm.09dm85m", "# Reasoning Path:\nHawaii -> government.political_district.representatives -> m.030qq8b -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nm.030qq8b", "# Reasoning Path:\nHawaii -> government.political_district.representatives -> m.030qq8b -> freebase.valuenotation.is_reviewed -> To\n# Answer:\nm.030qq8b", "# Reasoning Path:\nHawaii -> government.political_district.representatives -> m.030r1fw -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nm.030r1fw", "# Reasoning Path:\nHawaii -> government.political_district.representatives -> m.09dm85m -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nm.09dm85m", "# Reasoning Path:\nHawaii -> government.political_district.representatives -> m.030qq8b -> freebase.valuenotation.is_reviewed -> Appointed By (if Position is Appointed)\n# Answer:\nm.030qq8b", "# Reasoning Path:\nHawaii -> government.political_district.representatives -> m.030r1fw -> government.government_position_held.legislative_sessions -> 100th United States Congress\n# Answer:\nm.030r1fw", "# Reasoning Path:\nHawaii -> book.book_subject.works -> Anatomy of Paradise -> book.written_work.subjects -> Anthropology\n# Answer:\nAnatomy of Paradise"], "ground_truth": ["Mazie Hirono"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-672", "prediction": ["# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Azerbaijan People's Government\n# Answer:\nSocialist state", "# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Socialist state -> common.topic.notable_types -> Form of Government\n# Answer:\nSocialist state", "# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Socialist state -> common.topic.image -> Socialist states all\n# Answer:\nSocialist state", "# Reasoning Path:\nNorth Korea -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f719\n# Answer:\ng.12tb6f719", "# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Bukharan People's Soviet Republic\n# Answer:\nSocialist state", "# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Juche -> government.political_ideology.political_parties -> Anti-Imperialist National Democratic Front\n# Answer:\nJuche", "# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Juche -> common.topic.notable_types -> Political ideology\n# Answer:\nJuche", "# Reasoning Path:\nNorth Korea -> location.statistical_region.electricity_consumption_per_capita -> g.1245_0sg2\n# Answer:\ng.1245_0sg2", "# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> China\n# Answer:\nSocialist state", "# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Juche -> common.topic.notable_for -> g.125bc6qfr\n# Answer:\nJuche"], "ground_truth": ["Juche", "Socialist state", "Single-party state"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7272727272727272, "ans_precission": 0.8, "ans_recall": 0.6666666666666666, "path_f1": 0.7272727272727272, "path_precision": 0.8, "path_recall": 0.6666666666666666, "path_ans_f1": 0.7272727272727272, "path_ans_precision": 0.8, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-673", "prediction": ["# Reasoning Path:\nRobert Kardashian -> people.person.education -> m.0glw5zj -> education.education.institution -> University of San Diego School of Law\n# Answer:\nm.0glw5zj", "# Reasoning Path:\nRobert Kardashian -> people.person.education -> m.0646hdy -> education.education.institution -> University of Southern California\n# Answer:\nm.0646hdy", "# Reasoning Path:\nRobert Kardashian -> people.person.education -> m.04hwvgz -> education.education.institution -> University of San Diego\n# Answer:\nm.04hwvgz", "# Reasoning Path:\nRobert Kardashian -> people.person.children -> Khlo\u00e9 Kardashian -> people.person.education -> m.0n5my85\n# Answer:\nKhlo\u00e9 Kardashian", "# Reasoning Path:\nRobert Kardashian -> people.person.children -> Kim Kardashian -> people.person.education -> m.0gx8_97\n# Answer:\nKim Kardashian", "# Reasoning Path:\nRobert Kardashian -> people.person.profession -> Businessperson -> base.descriptive_names.names.descriptive_name -> m.0100z128\n# Answer:\nBusinessperson", "# Reasoning Path:\nRobert Kardashian -> people.person.profession -> Businessperson -> base.descriptive_names.names.descriptive_name -> m.0105_bmw\n# Answer:\nBusinessperson", "# Reasoning Path:\nRobert Kardashian -> people.person.profession -> Businessperson -> people.profession.specializations -> Actuary\n# Answer:\nBusinessperson", "# Reasoning Path:\nRobert Kardashian -> people.person.profession -> Lawyer -> base.descriptive_names.names.descriptive_name -> m.0101pjk9\n# Answer:\nLawyer", "# Reasoning Path:\nRobert Kardashian -> people.person.profession -> Businessperson -> base.descriptive_names.names.descriptive_name -> m.0105_bpb\n# Answer:\nBusinessperson"], "ground_truth": ["University of San Diego", "University of Southern California", "University of San Diego School of Law"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-675", "prediction": ["# Reasoning Path:\nSt. James City -> location.location.containedby -> Lee County -> location.location.containedby -> Florida\n# Answer:\nLee County", "# Reasoning Path:\nSt. James City -> location.location.containedby -> Lee County -> location.location.containedby -> South Florida water management district\n# Answer:\nLee County", "# Reasoning Path:\nSt. James City -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nSt. James City -> location.hud_county_place.county -> Lee County -> location.location.containedby -> Florida\n# Answer:\nLee County", "# Reasoning Path:\nSt. James City -> location.location.containedby -> Lee County -> location.statistical_region.population -> g.11b66l1tmq\n# Answer:\nLee County", "# Reasoning Path:\nSt. James City -> location.location.containedby -> Florida -> location.location.containedby -> Contiguous United States\n# Answer:\nFlorida", "# Reasoning Path:\nSt. James City -> location.hud_county_place.county -> Lee County -> location.us_county.county_seat -> Fort Myers\n# Answer:\nLee County", "# Reasoning Path:\nSt. James City -> location.location.containedby -> Lee County -> location.us_county.hud_county_place -> Alva\n# Answer:\nLee County", "# Reasoning Path:\nSt. James City -> location.hud_county_place.county -> Lee County -> location.location.containedby -> South Florida water management district\n# Answer:\nLee County", "# Reasoning Path:\nSt. James City -> common.topic.image -> Location of St. James City, Florida -> common.image.size -> m.0kl4r6\n# Answer:\nLocation of St. James City, Florida"], "ground_truth": ["Lee County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-676", "prediction": ["# Reasoning Path:\nAkkadian Empire -> location.location.contains -> Akkad -> location.location.geolocation -> m.0wmpxx7\n# Answer:\nAkkad", "# Reasoning Path:\nAkkadian Empire -> location.location.containedby -> Iraq -> location.location.containedby -> Arab world\n# Answer:\nIraq", "# Reasoning Path:\nAkkadian Empire -> location.location.contains -> Akkad -> common.topic.notable_types -> City/Town/Village\n# Answer:\nAkkad", "# Reasoning Path:\nAkkadian Empire -> location.location.containedby -> Iraq -> location.location.containedby -> Asia\n# Answer:\nIraq", "# Reasoning Path:\nAkkadian Empire -> location.location.containedby -> Baghdad Governorate -> location.location.containedby -> Iraq\n# Answer:\nBaghdad Governorate", "# Reasoning Path:\nAkkadian Empire -> location.location.containedby -> Iraq -> location.location.containedby -> Eurasia\n# Answer:\nIraq", "# Reasoning Path:\nAkkadian Empire -> common.topic.notable_types -> Country -> freebase.type_profile.strict_included_types -> Location\n# Answer:\nCountry", "# Reasoning Path:\nAkkadian Empire -> location.location.containedby -> Iraq -> location.country.administrative_divisions -> Al Anbar Governorate\n# Answer:\nIraq", "# Reasoning Path:\nAkkadian Empire -> location.location.containedby -> Iraq -> location.statistical_region.energy_use_per_capita -> g.1245_4lds\n# Answer:\nIraq", "# Reasoning Path:\nAkkadian Empire -> location.location.containedby -> Iraq -> location.country.administrative_divisions -> Al Diwaniyah Governorate\n# Answer:\nIraq"], "ground_truth": ["Akkad"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-677", "prediction": ["# Reasoning Path:\nOlympic Games -> common.topic.notable_types -> Website Category -> common.topic.article -> m.02ht13p\n# Answer:\nWebsite Category", "# Reasoning Path:\nOlympic Games -> common.topic.notable_types -> Website Category -> freebase.type_profile.kind -> Classification\n# Answer:\nWebsite Category", "# Reasoning Path:\nOlympic Games -> common.topic.notable_types -> Website Category -> type.type.properties -> Websites of this genre\n# Answer:\nWebsite Category", "# Reasoning Path:\nOlympic Games -> time.recurring_event.current_frequency -> Quadrennial -> common.topic.notable_types -> Unit of frequency\n# Answer:\nQuadrennial", "# Reasoning Path:\nOlympic Games -> time.recurring_event.current_frequency -> Quadrennial -> common.topic.notable_for -> g.125d3_wnj\n# Answer:\nQuadrennial", "# Reasoning Path:\nOlympic Games -> time.recurring_event.current_frequency -> Quadrennial -> freebase.unit_profile.dimension -> Time\n# Answer:\nQuadrennial", "# Reasoning Path:\nOlympic Games -> book.book_subject.works -> All That Glitters Is Not Gold: The Olympic Game -> book.written_work.author -> William O. Johnson Jr.\n# Answer:\nAll That Glitters Is Not Gold: The Olympic Game", "# Reasoning Path:\nOlympic Games -> book.book_subject.works -> Olympic dreams -> book.book.genre -> Fiction\n# Answer:\nOlympic dreams", "# Reasoning Path:\nOlympic Games -> book.book_subject.works -> Berlin Games -> book.written_work.subjects -> Europe\n# Answer:\nBerlin Games", "# Reasoning Path:\nOlympic Games -> book.book_subject.works -> Berlin Games -> common.topic.notable_types -> Book\n# Answer:\nBerlin Games"], "ground_truth": ["Quadrennial"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-678", "prediction": ["# Reasoning Path:\nGuyana -> location.country.form_of_government -> Semi-presidential system -> common.topic.image -> States with semi-presidential systems are shown in yellow\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nGuyana -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Algeria\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nGuyana -> location.country.form_of_government -> Semi-presidential system -> base.schemastaging.context_name.pronunciation -> g.125_l584v\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nGuyana -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Burkina Faso\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nGuyana -> location.statistical_region.gni_in_ppp_dollars -> g.11b60z05j3\n# Answer:\ng.11b60z05j3", "# Reasoning Path:\nGuyana -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Cuba\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nGuyana -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Ghana\n# Answer:\nUnitary state", "# Reasoning Path:\nGuyana -> location.country.form_of_government -> Unitary state -> common.topic.notable_types -> Form of Government\n# Answer:\nUnitary state", "# Reasoning Path:\nGuyana -> location.country.form_of_government -> Republic -> common.topic.image -> Giovanni Battista Tiepolo 080\n# Answer:\nRepublic", "# Reasoning Path:\nGuyana -> location.statistical_region.gni_in_ppp_dollars -> g.1245_7c__\n# Answer:\ng.1245_7c__"], "ground_truth": ["Republic", "Semi-presidential system", "Unitary state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.02h9cb0 -> tv.regular_tv_appearance.actor -> David Hasselhoff\n# Answer:\nm.02h9cb0", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4lk -> tv.regular_tv_appearance.actor -> Patricia McPherson\n# Answer:\nm.03lj4lk", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 3\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 4\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 0 -> tv.tv_series_season.episodes -> Behind the Scenes\n# Answer:\nKnight Rider - Season 0", "# Reasoning Path:\nKnight Rider -> common.topic.webpage -> m.09w0_mw -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w0_mw", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.02h9cb0 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 3\n# Answer:\nm.02h9cb0", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 0 -> tv.tv_series_season.episodes -> Blueprints Gallery Feature\n# Answer:\nKnight Rider - Season 0"], "ground_truth": ["William Daniels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-680", "prediction": ["# Reasoning Path:\nSergio Ag\u00fcero -> sports.pro_athlete.teams -> m.0j3_f1z -> sports.sports_team_roster.team -> Manchester City F.C.\n# Answer:\nm.0j3_f1z", "# Reasoning Path:\nSergio Ag\u00fcero -> sports.pro_athlete.teams -> m.04m4tkx -> sports.sports_team_roster.team -> Atl\u00e9tico Madrid\n# Answer:\nm.04m4tkx", "# Reasoning Path:\nSergio Ag\u00fcero -> soccer.football_player.statistics -> m.0w9d95t -> soccer.football_player_stats.team -> Manchester City F.C.\n# Answer:\nm.0w9d95t", "# Reasoning Path:\nSergio Ag\u00fcero -> sports.pro_athlete.teams -> m.0j3_f1z -> sports.sports_team_roster.position -> Forward\n# Answer:\nm.0j3_f1z", "# Reasoning Path:\nSergio Ag\u00fcero -> sports.pro_athlete.teams -> m.0j3_2g3 -> sports.sports_team_roster.team -> Argentina national football team\n# Answer:\nm.0j3_2g3", "# Reasoning Path:\nSergio Ag\u00fcero -> soccer.football_player.statistics -> m.0w9d7pl -> soccer.football_player_stats.team -> Argentina national football team\n# Answer:\nm.0w9d7pl", "# Reasoning Path:\nSergio Ag\u00fcero -> sports.pro_athlete.teams -> m.04m4tkx -> sports.sports_team_roster.position -> Forward\n# Answer:\nm.04m4tkx", "# Reasoning Path:\nSergio Ag\u00fcero -> sports.pro_athlete.teams -> m.0j3_2g3 -> sports.sports_team_roster.position -> Forward\n# Answer:\nm.0j3_2g3", "# Reasoning Path:\nSergio Ag\u00fcero -> soccer.football_player.matches_played -> m.0g5nq_z -> soccer.football_player_match_participation.team -> Argentina national football team\n# Answer:\nm.0g5nq_z", "# Reasoning Path:\nSergio Ag\u00fcero -> soccer.football_player.statistics -> m.0w8x3j2 -> soccer.football_player_stats.team -> Atl\u00e9tico Madrid\n# Answer:\nm.0w8x3j2"], "ground_truth": ["Atl\u00e9tico Madrid", "Club Atl\u00e9tico Independiente", "Manchester City F.C.", "Argentina national football team"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.48275862068965514, "path_precision": 0.7, "path_recall": 0.3684210526315789, "path_ans_f1": 0.7241379310344827, "path_ans_precision": 0.7, "path_ans_recall": 0.75}
{"id": "WebQTest-681", "prediction": ["# Reasoning Path:\nEuropean Union -> government.governmental_jurisdiction.governing_officials -> m.0g5k82m -> government.government_position_held.office_holder -> Jerzy Buzek\n# Answer:\nm.0g5k82m", "# Reasoning Path:\nEuropean Union -> government.governmental_jurisdiction.governing_officials -> m.0g5k82w -> government.government_position_held.office_holder -> Jerzy Buzek\n# Answer:\nm.0g5k82w", "# Reasoning Path:\nEuropean Union -> government.governmental_jurisdiction.governing_officials -> m.0g5k82m -> government.government_position_held.office_position_or_title -> President of the European Parliament\n# Answer:\nm.0g5k82m", "# Reasoning Path:\nEuropean Union -> government.governmental_jurisdiction.governing_officials -> m.010p8k3t -> government.government_position_held.office_holder -> Nick Griffin\n# Answer:\nm.010p8k3t", "# Reasoning Path:\nEuropean Union -> location.statistical_region.broadband_penetration_rate -> g.1245_6ndt\n# Answer:\ng.1245_6ndt", "# Reasoning Path:\nEuropean Union -> government.governmental_jurisdiction.governing_officials -> m.0g5k82w -> government.government_position_held.district_represented -> Silesian\n# Answer:\nm.0g5k82w", "# Reasoning Path:\nEuropean Union -> government.governmental_jurisdiction.governing_officials -> m.0g5k82w -> government.government_position_held.governmental_body -> European Parliament\n# Answer:\nm.0g5k82w", "# Reasoning Path:\nEuropean Union -> location.statistical_region.broadband_penetration_rate -> g.1245yvl64\n# Answer:\ng.1245yvl64", "# Reasoning Path:\nEuropean Union -> location.statistical_region.broadband_penetration_rate -> g.1245ywrjj\n# Answer:\ng.1245ywrjj", "# Reasoning Path:\nEuropean Union -> government.governmental_jurisdiction.governing_officials -> m.010p8k3t -> government.government_position_held.governmental_body -> European Parliament\n# Answer:\nm.010p8k3t"], "ground_truth": ["Jerzy Buzek"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-683", "prediction": ["# Reasoning Path:\nMiss USA -> time.recurring_event.instances -> Miss USA 2004 -> award.competition.competitors -> Amanda Pennekamp\n# Answer:\nMiss USA 2004", "# Reasoning Path:\nMiss USA -> time.recurring_event.instances -> Miss USA 2009 -> award.competition.competitors -> Alicia-Monique Blanco\n# Answer:\nMiss USA 2009", "# Reasoning Path:\nMiss USA -> time.recurring_event.instances -> Miss USA 2004 -> award.competition.competitors -> Danielle Demski\n# Answer:\nMiss USA 2004", "# Reasoning Path:\nMiss USA -> time.recurring_event.instances -> Miss USA 2004 -> award.competition.competitors -> Ellen Chapman\n# Answer:\nMiss USA 2004", "# Reasoning Path:\nMiss USA -> time.recurring_event.instances -> Miss USA 2009 -> award.competition.competitors -> Alysha Castonguay\n# Answer:\nMiss USA 2009", "# Reasoning Path:\nMiss USA -> time.recurring_event.instances -> Miss USA 1952 -> award.competition.type_of_competition -> Beauty Pageant\n# Answer:\nMiss USA 1952", "# Reasoning Path:\nMiss USA -> award.recurring_competition.individual_competitions -> Miss USA 2001 -> award.competition.competitors -> Jennifer Glover\n# Answer:\nMiss USA 2001", "# Reasoning Path:\nMiss USA -> award.recurring_competition.individual_competitions -> Miss USA 2001 -> common.topic.notable_for -> g.1256p97kg\n# Answer:\nMiss USA 2001", "# Reasoning Path:\nMiss USA -> time.recurring_event.instances -> Miss USA 2009 -> award.competition.competitors -> Ashley Underwood\n# Answer:\nMiss USA 2009", "# Reasoning Path:\nMiss USA -> time.recurring_event.instances -> Miss USA 2009 -> award.competition.winner -> Kristen Dalton\n# Answer:\nMiss USA 2009"], "ground_truth": ["Miss USA 1993"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-684", "prediction": ["# Reasoning Path:\nNigeria -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Sudan\n# Answer:\nFederal republic", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_0sgz\n# Answer:\ng.1245_0sgz", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> United States of America\n# Answer:\nFederal republic", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Chad\n# Answer:\nPresidential system", "# Reasoning Path:\nNigeria -> location.statistical_region.external_debt_stock -> g.11b71rqxzt\n# Answer:\ng.11b71rqxzt", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_22x5\n# Answer:\ng.1245_22x5", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guinea\n# Answer:\nPresidential system", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_2gxr\n# Answer:\ng.1245_2gxr", "# Reasoning Path:\nNigeria -> location.statistical_region.external_debt_stock -> g.1hhc38d1f\n# Answer:\ng.1hhc38d1f", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Kenya\n# Answer:\nPresidential system"], "ground_truth": ["Presidential system", "Federal republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-685", "prediction": ["# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> location.place_with_neighborhoods.neighborhoods -> Bath Beach\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> location.place_with_neighborhoods.neighborhoods -> Bay Ridge\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> travel.travel_destination.tourist_attractions -> Brooklyn Botanic Garden\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> people.person.places_lived -> m.04hv_n0 -> people.place_lived.location -> Brooklyn\n# Answer:\nm.04hv_n0", "# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> location.place_with_neighborhoods.neighborhoods -> Bedford\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> film.film_location.featured_in_films -> A Tree Grows in Brooklyn\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> travel.travel_destination.tourist_attractions -> Brooklyn Bridge\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> travel.travel_destination.tourist_attractions -> Brooklyn Historical Society Building\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> film.film_location.featured_in_films -> Annie Hall\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> book.author.works_written -> Barefoot Contessa Back to Basics -> common.topic.notable_types -> Book\n# Answer:\nBarefoot Contessa Back to Basics"], "ground_truth": ["Brooklyn"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-686", "prediction": ["# Reasoning Path:\nAtlanta Braves -> sports.sports_team.championships -> 1995 World Series -> sports.sports_championship_event.season -> 1995 Major League Baseball Season\n# Answer:\n1995 World Series", "# Reasoning Path:\nAtlanta Braves -> sports.sports_team.championships -> 1995 World Series -> common.topic.article -> m.04j717\n# Answer:\n1995 World Series", "# Reasoning Path:\nAtlanta Braves -> sports.sports_team.championships -> 1995 World Series -> common.topic.notable_for -> g.1257282kq\n# Answer:\n1995 World Series", "# Reasoning Path:\nAtlanta Braves -> sports.professional_sports_team.owner_s -> John C. Malone -> organization.organization_founder.organizations_founded -> Liberty Media Corporation\n# Answer:\nJohn C. Malone", "# Reasoning Path:\nAtlanta Braves -> sports.professional_sports_team.draft_picks -> m.04vw__5 -> sports.sports_league_draft_pick.draft -> 1982 Major League Baseball Draft\n# Answer:\nm.04vw__5", "# Reasoning Path:\nAtlanta Braves -> sports.professional_sports_team.draft_picks -> m.04vw_lk -> sports.sports_league_draft_pick.draft -> 1981 Major League Baseball Draft\n# Answer:\nm.04vw_lk", "# Reasoning Path:\nAtlanta Braves -> sports.professional_sports_team.draft_picks -> m.04vw__5 -> sports.sports_league_draft_pick.player -> Duane Ward\n# Answer:\nm.04vw__5", "# Reasoning Path:\nAtlanta Braves -> sports.professional_sports_team.draft_picks -> m.04vws5t -> sports.sports_league_draft_pick.draft -> 1965 Major League Baseball Draft\n# Answer:\nm.04vws5t", "# Reasoning Path:\nAtlanta Braves -> sports.professional_sports_team.owner_s -> Liberty Media Corporation -> internet.website_owner.websites_owned_new -> m.0rzl6cf\n# Answer:\nLiberty Media Corporation", "# Reasoning Path:\nAtlanta Braves -> sports.professional_sports_team.draft_picks -> m.04vw_lk -> sports.sports_league_draft_pick.player -> Jason Grimsley\n# Answer:\nm.04vw_lk"], "ground_truth": ["1995 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-688", "prediction": ["# Reasoning Path:\nIran -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nArabic Language", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Arabic Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nArabic Language", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Arabic Language -> common.topic.notable_types -> Human Language\n# Answer:\nArabic Language", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Kurdish languages -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nKurdish languages", "# Reasoning Path:\nIran -> location.country.official_language -> Persian Language -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal_per_capita -> g.11b60rrlgr\n# Answer:\ng.11b60rrlgr", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Gilaki Language -> language.human_language.region -> Asia\n# Answer:\nGilaki Language", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Kurdish languages -> common.topic.notable_types -> Human Language\n# Answer:\nKurdish languages", "# Reasoning Path:\nIran -> location.country.official_language -> Persian Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Kurdish languages -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nKurdish languages"], "ground_truth": ["Balochi language", "Azerbaijani language", "Gilaki Language", "Khalaj, Turkic Language", "Arabic Language", "Persian Language", "Mazanderani Language", "Luri language", "Kumzari Language", "Assyrian Neo-Aramaic Language", "Talysh language", "Armenian Language", "Khorasani Turkish Language", "Pashto language", "Turkmen Language", "Afshar language", "Qashqa'i Language", "Kurdish languages"], "ans_acc": 0.2222222222222222, "ans_hit": 1, "ans_f1": 0.3564356435643564, "ans_precission": 0.9, "ans_recall": 0.2222222222222222, "path_f1": 0.3564356435643564, "path_precision": 0.9, "path_recall": 0.2222222222222222, "path_ans_f1": 0.3564356435643564, "path_ans_precision": 0.9, "path_ans_recall": 0.2222222222222222}
{"id": "WebQTest-689", "prediction": ["# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> Andorra\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.region -> Europe\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> France\n# Answer:\nEarth", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> Gibraltar\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> United States of America\n# Answer:\nEarth", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\ng.1hhc37x4q", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> City\n# Answer:\nEarth", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc38hhk\n# Answer:\ng.1hhc38hhk", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> Africa\n# Answer:\nEarth"], "ground_truth": ["Spanish Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.location.containedby -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.location.containedby -> Tennessee\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County -> location.location.containedby -> Tennessee\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.country -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.location.containedby -> Nashville-Davidson--Murfreesboro--Franklin, TN Metropolitan Statistical Area\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County -> location.us_county.county_seat -> Franklin\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11b66dwnl4\n# Answer:\ng.11b66dwnl4", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.statistical_region.co2_emissions_total -> m.045hnmm\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County -> location.location.containedby -> Nashville-Davidson--Murfreesboro--Franklin, TN Metropolitan Statistical Area\n# Answer:\nWilliamson County"], "ground_truth": ["Williamson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-690", "prediction": ["# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Piano -> education.field_of_study.students_majoring -> m.02wmsgv\n# Answer:\nPiano", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Piano -> interests.collectable_item.in_collections -> m.0104pjzp\n# Answer:\nPiano", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Piano -> education.field_of_study.students_majoring -> m.04hm3p5\n# Answer:\nPiano", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Jazz piano -> music.performance_role.regular_performances -> m.0z84xb_\n# Answer:\nJazz piano", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Jazz piano -> common.topic.article -> m.06ch5b\n# Answer:\nJazz piano", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Piano -> education.field_of_study.students_majoring -> m.04hwb7q\n# Answer:\nPiano", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Piano -> music.performance_role.guest_performances -> m.012lfl80\n# Answer:\nPiano", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Jazz piano -> common.topic.webpage -> m.09xz26q\n# Answer:\nJazz piano", "# Reasoning Path:\nScott Joplin -> book.author.works_written -> Ragtime -> common.topic.notable_types -> Book\n# Answer:\nRagtime", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Jazz piano -> music.performance_role.regular_performances -> m.0z84xd1\n# Answer:\nJazz piano"], "ground_truth": ["Piano", "Jazz piano"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-691", "prediction": ["# Reasoning Path:\nMiley Cyrus -> people.person.education -> m.0h2yk16 -> education.education.institution -> Heritage Elementary School\n# Answer:\nm.0h2yk16", "# Reasoning Path:\nMiley Cyrus -> people.person.education -> m.0h2yk16 -> freebase.valuenotation.has_value -> Degree\n# Answer:\nm.0h2yk16", "# Reasoning Path:\nMiley Cyrus -> music.artist.album -> Hannah Montana -> tv.tv_program.regular_cast -> m.010w0vzd\n# Answer:\nHannah Montana", "# Reasoning Path:\nMiley Cyrus -> people.person.education -> m.0h2yk16 -> freebase.valuenotation.has_value -> End Date\n# Answer:\nm.0h2yk16", "# Reasoning Path:\nMiley Cyrus -> music.featured_artist.recordings -> A Day in the Life -> music.recording.featured_artists -> New Fumes\n# Answer:\nA Day in the Life", "# Reasoning Path:\nMiley Cyrus -> people.person.education -> m.0h2yk16 -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nm.0h2yk16", "# Reasoning Path:\nMiley Cyrus -> music.artist.album -> Hannah Montana -> tv.tv_program.regular_cast -> m.0gkd0gh\n# Answer:\nHannah Montana", "# Reasoning Path:\nMiley Cyrus -> music.artist.album -> Hannah Montana -> tv.tv_program.country_of_origin -> United States of America\n# Answer:\nHannah Montana", "# Reasoning Path:\nMiley Cyrus -> music.featured_artist.recordings -> 23 -> music.recording.featured_artists -> Juicy J\n# Answer:\n23", "# Reasoning Path:\nMiley Cyrus -> music.featured_artist.recordings -> 23 -> music.composition.language -> English Language\n# Answer:\n23"], "ground_truth": ["Heritage Elementary School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-693", "prediction": ["# Reasoning Path:\nAfghanistan -> location.country.official_language -> Farsi, Eastern Language -> common.topic.image -> Dari lang\n# Answer:\nFarsi, Eastern Language", "# Reasoning Path:\nAfghanistan -> location.country.official_language -> Farsi, Eastern Language -> base.rosetta.languoid.local_name -> Farsi, Eastern\n# Answer:\nFarsi, Eastern Language", "# Reasoning Path:\nAfghanistan -> location.country.official_language -> Farsi, Eastern Language -> base.rosetta.languoid.parent -> Persian Group\n# Answer:\nFarsi, Eastern Language", "# Reasoning Path:\nAfghanistan -> location.statistical_region.cpi_inflation_rate -> g.11b60q43bt\n# Answer:\ng.11b60q43bt", "# Reasoning Path:\nAfghanistan -> location.country.official_language -> Pashto language -> language.human_language.countries_spoken_in -> Iran\n# Answer:\nPashto language", "# Reasoning Path:\nAfghanistan -> location.statistical_region.time_required_to_start_a_business -> g.11b6p6pyh9\n# Answer:\ng.11b6p6pyh9", "# Reasoning Path:\nAfghanistan -> location.country.official_language -> Pashto language -> common.topic.notable_types -> Human Language\n# Answer:\nPashto language", "# Reasoning Path:\nAfghanistan -> location.statistical_region.cpi_inflation_rate -> g.12tb6gn6w\n# Answer:\ng.12tb6gn6w", "# Reasoning Path:\nAfghanistan -> location.country.official_language -> Pashto language -> language.human_language.countries_spoken_in -> Pakistan\n# Answer:\nPashto language", "# Reasoning Path:\nAfghanistan -> location.country.official_language -> Pashto language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nPashto language"], "ground_truth": ["Pashto language", "Farsi, Eastern Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-694", "prediction": ["# Reasoning Path:\nPrime Minister of Jamaica -> government.government_office_or_title.office_holders -> m.010fp6mv -> government.government_position_held.office_holder -> Alexander Bustamante\n# Answer:\nm.010fp6mv", "# Reasoning Path:\nPrime Minister of Jamaica -> common.topic.article -> m.01sm6d\n# Answer:\nm.01sm6d", "# Reasoning Path:\nPrime Minister of Jamaica -> government.government_office_or_title.office_holders -> m.010fp7qb -> government.government_position_held.office_holder -> Donald Sangster\n# Answer:\nm.010fp7qb", "# Reasoning Path:\nPrime Minister of Jamaica -> government.government_office_or_title.office_holders -> m.010fp8pl -> government.government_position_held.office_holder -> Hugh Shearer\n# Answer:\nm.010fp8pl", "# Reasoning Path:\nPrime Minister of Jamaica -> government.government_office_or_title.office_holders -> m.010fp6mv -> government.government_position_held.jurisdiction_of_office -> Jamaica\n# Answer:\nm.010fp6mv", "# Reasoning Path:\nPrime Minister of Jamaica -> government.government_office_or_title.office_holders -> m.010fp7qb -> government.government_position_held.basic_title -> Prime minister\n# Answer:\nm.010fp7qb", "# Reasoning Path:\nPrime Minister of Jamaica -> common.topic.image -> Coat of Arms of Jamaica -> common.image.appears_in_topic_gallery -> Coat of arms of Jamaica\n# Answer:\nCoat of Arms of Jamaica", "# Reasoning Path:\nPrime Minister of Jamaica -> common.topic.image -> Coat of Arms of Jamaica -> common.image.size -> m.02bfsxv\n# Answer:\nCoat of Arms of Jamaica", "# Reasoning Path:\nPrime Minister of Jamaica -> government.government_office_or_title.office_holders -> m.010fp8pl -> government.government_position_held.basic_title -> Prime minister\n# Answer:\nm.010fp8pl", "# Reasoning Path:\nPrime Minister of Jamaica -> common.topic.image -> Coat of Arms of Jamaica -> common.image.appears_in_topic_gallery -> Governor-General of Jamaica\n# Answer:\nCoat of Arms of Jamaica"], "ground_truth": ["Portia Simpson-Miller"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-695", "prediction": ["# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> m.02hrk87 -> music.group_membership.role -> Bass\n# Answer:\nm.02hrk87", "# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> m.02hrk87 -> music.group_membership.role -> Backing vocalist\n# Answer:\nm.02hrk87", "# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> m.02hrk87 -> music.group_membership.role -> Lead Vocals\n# Answer:\nm.02hrk87", "# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> g.11b7_rbdpp\n# Answer:\ng.11b7_rbdpp", "# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> m.02hrk87 -> music.group_membership.group -> The Beatles\n# Answer:\nm.02hrk87", "# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> g.11b7_s0vtk\n# Answer:\ng.11b7_s0vtk", "# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> m.02hrk87 -> freebase.valuenotation.is_reviewed -> Group\n# Answer:\nm.02hrk87", "# Reasoning Path:\nPaul McCartney -> base.activism.activist.area_of_activism -> Animal rights\n# Answer:\nAnimal rights", "# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> m.02hrk87 -> freebase.valuenotation.is_reviewed -> Member\n# Answer:\nm.02hrk87", "# Reasoning Path:\nPaul McCartney -> award.award_nominee.award_nominations -> m.01062q3w -> freebase.valuenotation.is_reviewed -> Award\n# Answer:\nm.01062q3w"], "ground_truth": ["Backing vocalist", "Lead Vocals", "Bass"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-696", "prediction": ["# Reasoning Path:\nJuventus F.C. -> sports.sports_team.championships -> 2014 Supercoppa Italiana -> time.event.instance_of_recurring_event -> Supercoppa Italiana\n# Answer:\n2014 Supercoppa Italiana", "# Reasoning Path:\nJuventus F.C. -> sports.sports_team.championships -> 2014 Supercoppa Italiana -> time.event.next_in_series -> g.11btrbw12v\n# Answer:\n2014 Supercoppa Italiana", "# Reasoning Path:\nJuventus F.C. -> sports.sports_team.championships -> 2014 Supercoppa Italiana -> time.event.locations -> Italy\n# Answer:\n2014 Supercoppa Italiana", "# Reasoning Path:\nJuventus F.C. -> sports.sports_team.championships -> 1996 UEFA Super Cup -> sports.sports_championship_event.championship -> UEFA Super Cup\n# Answer:\n1996 UEFA Super Cup", "# Reasoning Path:\nJuventus F.C. -> soccer.football_team.matches -> 2014 A. S. Roma vs. Juventus F.C -> soccer.football_match.teams -> A.S. Roma\n# Answer:\n2014 A. S. Roma vs. Juventus F.C", "# Reasoning Path:\nJuventus F.C. -> sports.sports_team.championships -> 1996 UEFA Super Cup -> common.topic.article -> m.02rw0yx\n# Answer:\n1996 UEFA Super Cup", "# Reasoning Path:\nJuventus F.C. -> sports.sports_team.championships -> 1977 UEFA Cup Final -> time.event.locations -> Stadio Olimpico di Torino\n# Answer:\n1977 UEFA Cup Final", "# Reasoning Path:\nJuventus F.C. -> soccer.football_team.matches -> 2014 A. S. Roma vs. Juventus F.C -> common.topic.notable_for -> g.1s05cp90q\n# Answer:\n2014 A. S. Roma vs. Juventus F.C", "# Reasoning Path:\nJuventus F.C. -> soccer.football_team.matches -> 2014 Juventus vs. Benfica -> soccer.football_match.teams -> S.L. Benfica\n# Answer:\n2014 Juventus vs. Benfica", "# Reasoning Path:\nJuventus F.C. -> sports.sports_team.coaches -> m.0115jpvz -> sports.sports_team_coach_tenure.position -> First team coach\n# Answer:\nm.0115jpvz"], "ground_truth": ["1996 UEFA Champions League Final"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-698", "prediction": ["# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lawrence E. Roberts -> people.person.spouse_s -> m.0kcbtkr\n# Answer:\nLawrence E. Roberts", "# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lawrence E. Roberts -> common.topic.notable_types -> Military Person\n# Answer:\nLawrence E. Roberts", "# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lawrence E. Roberts -> people.person.children -> Lawrence E. Roberts, Jr.\n# Answer:\nLawrence E. Roberts", "# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lucimarian Tolliver -> common.topic.notable_for -> g.125fjdxy2\n# Answer:\nLucimarian Tolliver", "# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lawrence E. Roberts -> people.person.children -> Dorothy Roberts McEwen\n# Answer:\nLawrence E. Roberts", "# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lawrence E. Roberts -> people.person.children -> Sally-Ann Roberts\n# Answer:\nLawrence E. Roberts", "# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lucimarian Tolliver -> freebase.valuenotation.has_value -> Parents\n# Answer:\nLucimarian Tolliver", "# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lucimarian Tolliver -> people.person.children -> Dorothy Roberts McEwen\n# Answer:\nLucimarian Tolliver", "# Reasoning Path:\nRobin Roberts -> award.award_nominee.award_nominations -> m.010g1g1v -> freebase.valuenotation.is_reviewed -> Award\n# Answer:\nm.010g1g1v", "# Reasoning Path:\nRobin Roberts -> award.award_nominee.award_nominations -> m.010g1g1v -> award.award_nomination.award -> Daytime Emmy Award for Outstanding Morning Program\n# Answer:\nm.010g1g1v"], "ground_truth": ["Lawrence E. Roberts"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-699", "prediction": ["# Reasoning Path:\nVictoria -> location.administrative_division.capital -> m.0jvvh95 -> location.administrative_division_capital_relationship.capital -> Melbourne\n# Answer:\nm.0jvvh95", "# Reasoning Path:\nVictoria -> base.aareas.schema.administrative_area.capital -> Melbourne -> common.topic.image -> Melbourne Infobox Montage\n# Answer:\nMelbourne", "# Reasoning Path:\nVictoria -> base.aareas.schema.administrative_area.capital -> Melbourne -> common.topic.image -> Melbourneskyline\n# Answer:\nMelbourne", "# Reasoning Path:\nVictoria -> base.aareas.schema.administrative_area.capital -> Melbourne -> travel.travel_destination.tourist_attractions -> Melbourne City Centre\n# Answer:\nMelbourne", "# Reasoning Path:\nVictoria -> base.aareas.schema.administrative_area.capital -> Melbourne -> common.topic.image -> m.0292mf_\n# Answer:\nMelbourne", "# Reasoning Path:\nVictoria -> base.aareas.schema.administrative_area.capital -> Melbourne -> travel.travel_destination.tourist_attractions -> Abbotsford Convent\n# Answer:\nMelbourne", "# Reasoning Path:\nVictoria -> government.political_district.representatives -> m.04_774_ -> government.government_position_held.governmental_body -> Australian Senate\n# Answer:\nm.04_774_", "# Reasoning Path:\nVictoria -> base.aareas.schema.administrative_area.capital -> Melbourne -> travel.travel_destination.tourist_attractions -> Australian Centre for the Moving Image\n# Answer:\nMelbourne", "# Reasoning Path:\nVictoria -> government.political_district.representatives -> m.04_774_ -> government.government_position_held.office_position_or_title -> Australian Senator\n# Answer:\nm.04_774_", "# Reasoning Path:\nVictoria -> government.political_district.representatives -> m.04_8t1b -> government.government_position_held.governmental_body -> Australian Senate\n# Answer:\nm.04_8t1b"], "ground_truth": ["Melbourne"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Missouri\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Jasper County\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.geolocation -> m.0kdmvf\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Newton County\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_for -> g.1255959wd\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11b66mljn1\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> astronomy.extraterrestrial_location.on_celestial_object -> Moon\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11x1chmhk\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_types -> Extraterrestrial location\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver Academy -> location.location.geolocation -> m.0127s0kf\n# Answer:\nCarver Academy"], "ground_truth": ["Diamond"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-700", "prediction": ["# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Martinique -> base.aareas.schema.administrative_area.administrative_parent -> Overseas France\n# Answer:\nMartinique", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Martinique -> location.country.currency_used -> Euro\n# Answer:\nMartinique", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Algeria -> location.country.form_of_government -> Presidential system\n# Answer:\nAlgeria", "# Reasoning Path:\nFrench -> language.human_language.main_country -> France -> location.country.first_level_divisions -> Martinique\n# Answer:\nFrance", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Algeria -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nAlgeria", "# Reasoning Path:\nFrench -> language.human_language.countries_spoken_in -> Vatican City -> location.statistical_region.population -> g.11b7tfzdvm\n# Answer:\nVatican City", "# Reasoning Path:\nFrench -> language.human_language.countries_spoken_in -> Vatican City -> base.biblioness.bibs_location.city -> Rome\n# Answer:\nVatican City", "# Reasoning Path:\nFrench -> language.human_language.countries_spoken_in -> Martinique -> base.aareas.schema.administrative_area.administrative_parent -> Overseas France\n# Answer:\nMartinique", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Martinique -> location.country.form_of_government -> Semi-presidential system\n# Answer:\nMartinique", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Algeria -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAlgeria"], "ground_truth": ["Tunisia", "Ivory Coast", "Monaco", "Seychelles", "Belgium", "Qatar", "Algeria", "Rwanda", "Holy Roman Empire", "Central African Republic", "Vanuatu", "France", "Haiti", "Burkina Faso", "Cyprus", "Barbados", "Morocco", "Martinique", "Cameroon", "Chad", "Guinea", "Equatorial Guinea", "Djibouti", "Lebanon", "South Vietnam", "Madagascar", "Laos", "Habsburg Netherlands", "Mauritius", "Mali", "Cambodia", "Switzerland", "Senegal", "Niger", "Jersey", "Comoros", "Congo", "Gabon", "Dominica", "Burundi", "Vatican City", "Luxembourg", "Benin"], "ans_acc": 0.09302325581395349, "ans_hit": 1, "ans_f1": 0.1702127659574468, "ans_precission": 1.0, "ans_recall": 0.09302325581395349, "path_f1": 0.1283422459893048, "path_precision": 0.8, "path_recall": 0.06976744186046512, "path_ans_f1": 0.1702127659574468, "path_ans_precision": 1.0, "path_ans_recall": 0.09302325581395349}
{"id": "WebQTest-701", "prediction": ["# Reasoning Path:\nReggie Bush -> american_football.football_player.receiving -> m.09grq2h -> american_football.player_receiving_statistics.team -> New Orleans Saints\n# Answer:\nm.09grq2h", "# Reasoning Path:\nReggie Bush -> american_football.football_player.receiving -> m.09grq2h -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\nm.09grq2h", "# Reasoning Path:\nReggie Bush -> american_football.football_player.receiving -> m.0791gj4 -> american_football.player_receiving_statistics.team -> New Orleans Saints\n# Answer:\nm.0791gj4", "# Reasoning Path:\nReggie Bush -> american_football.football_player.receiving -> m.07mmpk7 -> american_football.player_receiving_statistics.team -> New Orleans Saints\n# Answer:\nm.07mmpk7", "# Reasoning Path:\nReggie Bush -> american_football.football_player.rushing -> m.09grpy_ -> american_football.player_rushing_statistics.team -> New Orleans Saints\n# Answer:\nm.09grpy_", "# Reasoning Path:\nReggie Bush -> american_football.football_player.rushing -> m.07sh67q -> american_football.player_rushing_statistics.team -> New Orleans Saints\n# Answer:\nm.07sh67q", "# Reasoning Path:\nReggie Bush -> american_football.football_player.rushing -> m.0791gbq -> american_football.player_rushing_statistics.team -> New Orleans Saints\n# Answer:\nm.0791gbq", "# Reasoning Path:\nReggie Bush -> american_football.football_player.receiving -> m.0791gj4 -> american_football.player_receiving_statistics.season -> 2008 NFL season\n# Answer:\nm.0791gj4", "# Reasoning Path:\nReggie Bush -> american_football.football_player.receiving -> m.07mmpk7 -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\nm.07mmpk7", "# Reasoning Path:\nReggie Bush -> american_football.football_player.rushing -> m.07sh67q -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\nm.07sh67q"], "ground_truth": ["Miami Dolphins", "New Orleans Saints"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.17910447761194026, "path_precision": 0.6, "path_recall": 0.10526315789473684, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.6, "path_ans_recall": 0.5}
{"id": "WebQTest-702", "prediction": ["# Reasoning Path:\nEmily Osment -> people.person.education -> m.0n1fn0k -> education.education.institution -> Flintridge Preparatory School\n# Answer:\nm.0n1fn0k", "# Reasoning Path:\nEmily Osment -> people.person.education -> g.11b62v8w8j\n# Answer:\ng.11b62v8w8j", "# Reasoning Path:\nEmily Osment -> people.person.place_of_birth -> Los Angeles -> fictional_universe.fictional_setting.characters_that_have_lived_here -> Linwood Murrow\n# Answer:\nLos Angeles", "# Reasoning Path:\nEmily Osment -> people.person.place_of_birth -> Los Angeles -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nLos Angeles", "# Reasoning Path:\nEmily Osment -> people.person.place_of_birth -> Los Angeles -> fictional_universe.fictional_setting.characters_that_have_lived_here -> Winifred Burkle\n# Answer:\nLos Angeles", "# Reasoning Path:\nEmily Osment -> award.award_winner.awards_won -> m.0t5t4lr -> award.award_honor.award -> Canadian Screen Award for Best Performance by an Actress in a Leading Role in a Dramatic Program or Mini-Series\n# Answer:\nm.0t5t4lr", "# Reasoning Path:\nEmily Osment -> award.award_winner.awards_won -> m.0z915m5 -> award.award_honor.ceremony -> 2009 Teen Choice Awards\n# Answer:\nm.0z915m5", "# Reasoning Path:\nEmily Osment -> award.award_winner.awards_won -> m.0t5t4lr -> award.award_honor.honored_for -> Cyberbully\n# Answer:\nm.0t5t4lr", "# Reasoning Path:\nEmily Osment -> award.award_winner.awards_won -> m.0z915m5 -> award.award_honor.award -> Teen Choice Award for Choice TV: Sidekick\n# Answer:\nm.0z915m5"], "ground_truth": ["Flintridge Preparatory School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-703", "prediction": ["# Reasoning Path:\nMexico -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> North America\n# Answer:\nEarth", "# Reasoning Path:\nMexico -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> United States of America\n# Answer:\nEarth", "# Reasoning Path:\nMexico -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> Sovereign state\n# Answer:\nEarth", "# Reasoning Path:\nMexico -> location.location.containedby -> Latin America -> location.location.containedby -> Americas\n# Answer:\nLatin America", "# Reasoning Path:\nMexico -> location.location.containedby -> Latin America -> location.location.contains -> Cuauht\u00e9moc, D.F.\n# Answer:\nLatin America", "# Reasoning Path:\nMexico -> location.location.containedby -> Americas -> location.location.contains -> Latin America\n# Answer:\nAmericas", "# Reasoning Path:\nMexico -> location.location.containedby -> North America -> base.locations.continents.countries_within -> Guatemala\n# Answer:\nNorth America", "# Reasoning Path:\nMexico -> location.location.containedby -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nMexico -> location.location.containedby -> Americas -> location.location.contains -> Guatemala\n# Answer:\nAmericas", "# Reasoning Path:\nMexico -> location.location.containedby -> North America -> location.location.partially_contains -> United States of America\n# Answer:\nNorth America"], "ground_truth": ["Mexican peso"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-704", "prediction": ["# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Scientist -> common.topic.subject_of -> Autobiography\n# Answer:\nScientist", "# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Cinematographer -> people.profession.specialization_of -> Camera Operator\n# Answer:\nCinematographer", "# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Scientist -> tv.tv_subject.tv_programs -> The Secret Life of Scientists & Engineers\n# Answer:\nScientist", "# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Scientist -> common.topic.subject_of -> Biography\n# Answer:\nScientist", "# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Scientist -> base.descriptive_names.names.descriptive_name -> m.01053kvm\n# Answer:\nScientist", "# Reasoning Path:\nThor Heyerdahl -> freebase.valuenotation.is_reviewed -> Profession -> rdf-schema#domain -> Person\n# Answer:\nProfession", "# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Scientist -> common.topic.subject_of -> Book\n# Answer:\nScientist", "# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Film director -> fictional_universe.character_occupation.characters_with_this_occupation -> Chad Dylan Cooper\n# Answer:\nFilm director", "# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Film director -> award.award_discipline.awards_in_this_discipline -> AACTA Award for Best Direction\n# Answer:\nFilm director", "# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Scientist -> base.descriptive_names.names.descriptive_name -> m.01053kyr\n# Answer:\nScientist"], "ground_truth": ["Ethnographer", "Film director", "Film Producer", "Adventurer", "Scientist", "Cinematographer"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6428571428571429, "ans_precission": 0.9, "ans_recall": 0.5, "path_f1": 0.6428571428571429, "path_precision": 0.9, "path_recall": 0.5, "path_ans_f1": 0.6428571428571429, "path_ans_precision": 0.9, "path_ans_recall": 0.5}
{"id": "WebQTest-706", "prediction": ["# Reasoning Path:\nTroy Aikman -> sports.pro_athlete.teams -> m.0hpxsvm -> sports.sports_team_roster.team -> Dallas Cowboys\n# Answer:\nm.0hpxsvm", "# Reasoning Path:\nTroy Aikman -> sports.pro_athlete.teams -> m.0hpxsvm -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.0hpxsvm", "# Reasoning Path:\nTroy Aikman -> sports.pro_athlete.sports_played_professionally -> m.0d5j7sd -> sports.pro_sports_played.sport -> American football\n# Answer:\nm.0d5j7sd", "# Reasoning Path:\nTroy Aikman -> award.award_nominee.award_nominations -> m.0_qzp0y -> freebase.valuenotation.has_no_value -> Ceremony\n# Answer:\nm.0_qzp0y", "# Reasoning Path:\nTroy Aikman -> award.award_nominee.award_nominations -> m.0zcps9m -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nm.0zcps9m", "# Reasoning Path:\nTroy Aikman -> award.award_nominee.award_nominations -> m.0_qzp0y -> award.award_nomination.award -> Walter Payton NFL Man of the Year Award\n# Answer:\nm.0_qzp0y", "# Reasoning Path:\nTroy Aikman -> award.award_nominee.award_nominations -> m.0zf3nbm -> award.award_nomination.ceremony -> 27th Sports Emmy Awards\n# Answer:\nm.0zf3nbm", "# Reasoning Path:\nTroy Aikman -> award.award_nominee.award_nominations -> m.0_qzp0y -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nm.0_qzp0y", "# Reasoning Path:\nTroy Aikman -> award.award_nominee.award_nominations -> m.0zcps9m -> award.award_nomination.award -> Sports Emmy Award for Outstanding Sports Personality - Sports Event Analyst\n# Answer:\nm.0zcps9m", "# Reasoning Path:\nTroy Aikman -> award.award_nominee.award_nominations -> m.0zf3nbm -> freebase.valuenotation.is_reviewed -> Award\n# Answer:\nm.0zf3nbm"], "ground_truth": ["Dallas Cowboys"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-707", "prediction": ["# Reasoning Path:\nCletus Hogg -> film.film_character.portrayed_in_films -> m.03hqvvw -> film.performance.actor -> Jack Polick\n# Answer:\nm.03hqvvw", "# Reasoning Path:\nCletus Hogg -> tv.tv_character.appeared_in_tv_program -> m.03hqvml -> tv.regular_tv_appearance.actor -> Rick Hurst\n# Answer:\nm.03hqvml", "# Reasoning Path:\nCletus Hogg -> common.topic.article -> m.0bxccc\n# Answer:\nm.0bxccc", "# Reasoning Path:\nCletus Hogg -> film.film_character.portrayed_in_films -> m.03hqvvw -> film.performance.film -> The Dukes of Hazzard\n# Answer:\nm.03hqvvw"], "ground_truth": ["Jack Polick"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-708", "prediction": ["# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.0bfpn_5 -> sports.sports_award.season -> 1984\u201385 FA Cup\n# Answer:\nm.0bfpn_5", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05l1m7b -> sports.sports_award.season -> 1988\u201389 FA Cup\n# Answer:\nm.05l1m7b", "# Reasoning Path:\nFA Cup -> common.topic.notable_for -> g.125f13scs\n# Answer:\ng.125f13scs", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.0bfpn_5 -> sports.sports_award.award_winner -> Manchester United F.C.\n# Answer:\nm.0bfpn_5", "# Reasoning Path:\nFA Cup -> freebase.valuenotation.has_no_value -> Date of final occurrence -> rdf-schema#range -> Date/Time\n# Answer:\nDate of final occurrence", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05l1m7b -> sports.sports_award.award_winner -> Liverpool F.C.\n# Answer:\nm.05l1m7b", "# Reasoning Path:\nFA Cup -> freebase.valuenotation.has_no_value -> Date of final occurrence -> type.property.schema -> Recurring event\n# Answer:\nDate of final occurrence", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.010lkdyj -> sports.sports_award.season -> 2013\u201314 FA Cup\n# Answer:\nm.010lkdyj", "# Reasoning Path:\nFA Cup -> freebase.valuenotation.has_no_value -> Date of final occurrence -> type.property.expected_type -> Date/Time\n# Answer:\nDate of final occurrence", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.010lkdyj -> sports.sports_award.award_winner -> Arsenal F.C.\n# Answer:\nm.010lkdyj"], "ground_truth": ["Southampton F.C."], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-709", "prediction": ["# Reasoning Path:\nRobert E. Lee -> people.deceased_person.cause_of_death -> Pneumonia -> people.cause_of_death.includes_causes_of_death -> Aspiration pneumonia\n# Answer:\nPneumonia", "# Reasoning Path:\nRobert E. Lee -> people.deceased_person.cause_of_death -> Pneumonia -> medicine.disease.includes_diseases -> Aspiration pneumonia\n# Answer:\nPneumonia", "# Reasoning Path:\nRobert E. Lee -> military.military_commander.military_commands -> m.048z_8v -> military.military_command.military_conflict -> Battle of Gettysburg\n# Answer:\nm.048z_8v", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> General Lee's Army -> book.written_work.subjects -> Army of Northern Virginia\n# Answer:\nGeneral Lee's Army", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> General Lee's Army -> book.book.editions -> General Lee's Army: From Victory to Collapse\n# Answer:\nGeneral Lee's Army", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> Robert E. Lee's Civil War -> book.written_work.subjects -> American Civil War\n# Answer:\nRobert E. Lee's Civil War", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> General Lee's Army -> book.written_work.author -> Joseph Glatthaar\n# Answer:\nGeneral Lee's Army", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> General Lee's Army -> book.written_work.subjects -> American Civil War\n# Answer:\nGeneral Lee's Army", "# Reasoning Path:\nRobert E. Lee -> military.military_commander.military_commands -> m.048z_8v -> military.military_command.military_combatant -> Confederate States of America\n# Answer:\nm.048z_8v", "# Reasoning Path:\nRobert E. Lee -> military.military_commander.military_commands -> m.049y372 -> military.military_command.military_conflict -> Battle of Fredericksburg\n# Answer:\nm.049y372"], "ground_truth": ["Pneumonia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> common.topic.notable_types -> Person\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.nationality -> United States of America\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Esm\u00e9 Annabelle Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nEsm\u00e9 Annabelle Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.spouse -> Tracy Pollan\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.location_of_ceremony -> Arlington\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.profession -> Actor\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.sibling_s -> m.0j217jw\n# Answer:\nSam Michael Fox"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-710", "prediction": ["# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Impressionism -> book.book_subject.works -> \u00c9douard Manet\n# Answer:\nImpressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Impressionism -> book.book_subject.works -> Monet\n# Answer:\nImpressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Impressionism -> book.written_work.subjects -> Art\n# Answer:\nImpressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Impressionism -> base.schemastaging.context_name.pronunciation -> g.125_m6dv2\n# Answer:\nImpressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Fauvism -> visual_art.art_period_movement.associated_artists -> Albert Marquet\n# Answer:\nFauvism", "# Reasoning Path:\nHenri Matisse -> book.book_subject.works -> Matisse -> common.topic.notable_for -> g.125dhx5l1\n# Answer:\nMatisse", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Impressionism -> book.book_subject.works -> Ce\u0301zanne\n# Answer:\nImpressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Fauvism -> common.topic.notable_for -> g.1257w3_9g\n# Answer:\nFauvism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Eyewitness: Reports from an Art World in Crisis\n# Answer:\nModern art", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nModern art"], "ground_truth": ["Neo-impressionism", "Modernism", "Impressionism", "Modern art", "Fauvism"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7200000000000001, "ans_precission": 0.9, "ans_recall": 0.6, "path_f1": 0.7200000000000001, "path_precision": 0.9, "path_recall": 0.6, "path_ans_f1": 0.7200000000000001, "path_ans_precision": 0.9, "path_ans_recall": 0.6}
{"id": "WebQTest-711", "prediction": ["# Reasoning Path:\nChuck Bass -> tv.tv_character.appeared_in_tv_program -> m.04hby1k -> tv.regular_tv_appearance.actor -> Ed Westwick\n# Answer:\nm.04hby1k", "# Reasoning Path:\nGossip Girl -> tv.tv_program.regular_cast -> m.010blsdj -> tv.regular_tv_appearance.actor -> Margaret Colin\n# Answer:\nm.010blsdj", "# Reasoning Path:\nGossip Girl -> tv.tv_program.regular_cast -> m.03jqr3k -> tv.regular_tv_appearance.actor -> Leighton Meester\n# Answer:\nm.03jqr3k", "# Reasoning Path:\nChuck Bass -> tv.tv_character.appeared_in_tv_program -> m.04hby1k -> tv.regular_tv_appearance.seasons -> Gossip Girl - Season 5\n# Answer:\nm.04hby1k", "# Reasoning Path:\nGossip Girl -> tv.tv_program.regular_cast -> m.03jtc3r -> tv.regular_tv_appearance.actor -> Penn Dayton Badgley\n# Answer:\nm.03jtc3r", "# Reasoning Path:\nGossip Girl -> tv.tv_program.regular_cast -> m.010blsdj -> tv.regular_tv_appearance.character -> Eleanor Waldorf\n# Answer:\nm.010blsdj", "# Reasoning Path:\nChuck Bass -> tv.tv_character.appeared_in_tv_program -> m.04hby1k -> tv.regular_tv_appearance.seasons -> Gossip Girl - Season 6\n# Answer:\nm.04hby1k", "# Reasoning Path:\nGossip Girl -> tv.tv_program.regular_cast -> m.03jqr3k -> tv.regular_tv_appearance.seasons -> Gossip Girl - Season 5\n# Answer:\nm.03jqr3k", "# Reasoning Path:\nChuck Bass -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Multiple myeloma\n# Answer:\nMale", "# Reasoning Path:\nGossip Girl -> award.award_winning_work.awards_won -> m.0y4k8mm -> award.award_honor.ceremony -> 2010 Teen Choice Awards\n# Answer:\nm.0y4k8mm"], "ground_truth": ["Ed Westwick"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-712", "prediction": ["# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.location.contains -> Line Islands\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Australia -> location.country.currency_used -> Australian dollar\n# Answer:\nAustralia", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.location.contains -> Phoenix Islands\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Australia -> location.country.languages_spoken -> English Language\n# Answer:\nAustralia", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.location.contains -> Tarawa\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.country.official_language -> English Language\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Australia -> sports.sport_country.multi_event_tournaments_participated_in -> 2010 Commonwealth Games\n# Answer:\nAustralia", "# Reasoning Path:\nOceania -> location.location.containedby -> DVD Region 4 -> location.location.contains -> South America\n# Answer:\nDVD Region 4", "# Reasoning Path:\nOceania -> location.location.contains -> Adams Seamount -> common.topic.article -> m.02q9sc5\n# Answer:\nAdams Seamount"], "ground_truth": ["Tuvalu", "Tokelau", "New Zealand", "New Caledonia", "Timor-Leste", "Papua New Guinea", "Kiribati", "Samoa", "Cook Islands", "Vanuatu", "Nauru", "Federated States of Micronesia", "Australia", "Marshall Islands", "Pitcairn Islands", "Niue", "American Samoa", "Indonesia", "Guam", "Wallis and Futuna", "Tonga", "French Polynesia", "Northern Mariana Islands", "Palau", "Fiji", "Solomon Islands", "Norfolk Island"], "ans_acc": 0.07407407407407407, "ans_hit": 1, "ans_f1": 0.13559322033898305, "ans_precission": 0.8, "ans_recall": 0.07407407407407407, "path_f1": 0.13559322033898305, "path_precision": 0.8, "path_recall": 0.07407407407407407, "path_ans_f1": 0.13559322033898305, "path_ans_precision": 0.8, "path_ans_recall": 0.07407407407407407}
{"id": "WebQTest-713", "prediction": ["# Reasoning Path:\nSt. Louis Park -> location.location.containedby -> Hennepin County -> location.location.containedby -> Minnesota\n# Answer:\nHennepin County", "# Reasoning Path:\nSt. Louis Park -> location.location.containedby -> Hennepin County -> location.location.containedby -> Minneapolis-St. Paul-Bloomington, MN-WI Metropolitan Statistical Area\n# Answer:\nHennepin County", "# Reasoning Path:\nSt. Louis Park -> location.location.containedby -> Minnesota -> location.location.containedby -> United States of America\n# Answer:\nMinnesota", "# Reasoning Path:\nSt. Louis Park -> location.location.containedby -> Minneapolis\u2013Saint Paul -> location.location.containedby -> Washington County\n# Answer:\nMinneapolis\u2013Saint Paul", "# Reasoning Path:\nSt. Louis Park -> location.hud_county_place.county -> Hennepin County -> location.location.containedby -> Minneapolis-St. Paul-Bloomington, MN-WI Metropolitan Statistical Area\n# Answer:\nHennepin County", "# Reasoning Path:\nSt. Louis Park -> location.location.containedby -> Hennepin County -> location.statistical_region.population -> g.11b674hc3d\n# Answer:\nHennepin County", "# Reasoning Path:\nSt. Louis Park -> location.statistical_region.population -> g.11b66k1kc0\n# Answer:\ng.11b66k1kc0", "# Reasoning Path:\nSt. Louis Park -> location.location.containedby -> Hennepin County -> location.us_county.hud_county_place -> Bloomington\n# Answer:\nHennepin County", "# Reasoning Path:\nSt. Louis Park -> location.location.containedby -> Minnesota -> location.location.containedby -> United States, with Territories\n# Answer:\nMinnesota", "# Reasoning Path:\nSt. Louis Park -> location.location.containedby -> Minnesota -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nMinnesota"], "ground_truth": ["Hennepin County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-714", "prediction": ["# Reasoning Path:\nSouth Africa -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nSouth Africa -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Botswana\n# Answer:\nParliamentary republic", "# Reasoning Path:\nSouth Africa -> location.statistical_region.electricity_consumption_per_capita -> g.1245_4m5f\n# Answer:\ng.1245_4m5f", "# Reasoning Path:\nSouth Africa -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Greece\n# Answer:\nConstitutional republic", "# Reasoning Path:\nSouth Africa -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f730\n# Answer:\ng.12tb6f730", "# Reasoning Path:\nSouth Africa -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> India\n# Answer:\nConstitutional republic", "# Reasoning Path:\nSouth Africa -> location.statistical_region.electricity_consumption_per_capita -> g.1245_f78y\n# Answer:\ng.1245_f78y", "# Reasoning Path:\nSouth Africa -> location.statistical_region.electricity_consumption_per_capita -> g.1245_f78z\n# Answer:\ng.1245_f78z", "# Reasoning Path:\nSouth Africa -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6fdy5\n# Answer:\ng.12tb6fdy5", "# Reasoning Path:\nSouth Africa -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6ft1s\n# Answer:\ng.12tb6ft1s"], "ground_truth": ["Constitutional republic", "Parliamentary republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-716", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.language_family -> Germanic languages -> language.language_family.geographic_distribution -> Northern Europe\n# Answer:\nGermanic languages", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> Indo-European languages -> language.language_family.sub_families -> Germanic languages\n# Answer:\nIndo-European languages", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> Germanic languages -> base.schemastaging.context_name.pronunciation -> g.125_qp31h\n# Answer:\nGermanic languages", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> Germanic languages -> language.language_family.sub_families -> East Germanic languages\n# Answer:\nGermanic languages", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> Indo-European languages -> language.language_family.sub_families -> Goidelic languages\n# Answer:\nIndo-European languages", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> Indo-European languages -> common.topic.article -> m.03v0m\n# Answer:\nIndo-European languages", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> Indo-European languages -> base.schemastaging.context_name.pronunciation -> g.125_r38g5\n# Answer:\nIndo-European languages", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> West Germanic languages -> language.language_family.member_of_language_families -> Germanic languages\n# Answer:\nWest Germanic languages", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> Germanic languages -> language.language_family.sub_families -> North Germanic languages\n# Answer:\nGermanic languages", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0102xvg7 -> education.education.student -> Sigurd Burckhardt\n# Answer:\nm.0102xvg7"], "ground_truth": ["Germanic languages", "West Germanic languages", "Indo-European languages"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-717", "prediction": ["# Reasoning Path:\nTucson -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nTucson -> location.location.time_zones -> Mountain Time Zone -> common.topic.notable_for -> g.125brs1ct\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nTucson -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nTucson -> location.location.nearby_airports -> La Cholla Airpark -> aviation.airport.serves -> Oro Valley\n# Answer:\nLa Cholla Airpark", "# Reasoning Path:\nTucson -> location.location.nearby_airports -> Davis\u2013Monthan Air Force Base -> common.topic.notable_for -> g.1257ryx5p\n# Answer:\nDavis\u2013Monthan Air Force Base", "# Reasoning Path:\nTucson -> location.citytown.postal_codes -> 85701 -> common.topic.notable_types -> Postal Code\n# Answer:\n85701", "# Reasoning Path:\nTucson -> location.location.nearby_airports -> La Cholla Airpark -> location.location.containedby -> 85737\n# Answer:\nLa Cholla Airpark", "# Reasoning Path:\nTucson -> location.location.nearby_airports -> La Cholla Airpark -> common.topic.webpage -> m.040tvy4\n# Answer:\nLa Cholla Airpark", "# Reasoning Path:\nTucson -> location.location.nearby_airports -> Marana Regional Airport -> aviation.airport.serves -> Marana\n# Answer:\nMarana Regional Airport", "# Reasoning Path:\nTucson -> location.citytown.postal_codes -> 85702 -> common.topic.notable_for -> g.1258hl5sk\n# Answer:\n85702"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-718", "prediction": ["# Reasoning Path:\nLucy Lawless -> people.person.place_of_birth -> Mount Albert, New Zealand -> location.location.containedby -> New Zealand\n# Answer:\nMount Albert, New Zealand", "# Reasoning Path:\nLucy Lawless -> people.person.place_of_birth -> Mount Albert, New Zealand -> common.topic.article -> m.04jnl_\n# Answer:\nMount Albert, New Zealand", "# Reasoning Path:\nLucy Lawless -> people.person.place_of_birth -> Mount Albert, New Zealand -> common.topic.notable_types -> Location\n# Answer:\nMount Albert, New Zealand", "# Reasoning Path:\nLucy Lawless -> people.person.place_of_birth -> Mount Albert, New Zealand -> location.location.containedby -> Auckland\n# Answer:\nMount Albert, New Zealand", "# Reasoning Path:\nLucy Lawless -> freebase.valuenotation.has_value -> Siblings -> rdf-schema#domain -> Person\n# Answer:\nSiblings", "# Reasoning Path:\nLucy Lawless -> tv.tv_actor.starring_roles -> m.02ht1fx -> tv.regular_tv_appearance.series -> Xena: Warrior Princess\n# Answer:\nm.02ht1fx", "# Reasoning Path:\nLucy Lawless -> freebase.valuenotation.has_value -> Siblings -> type.property.schema -> Person\n# Answer:\nSiblings", "# Reasoning Path:\nLucy Lawless -> freebase.valuenotation.has_value -> Siblings -> type.property.master_property -> Sibling\n# Answer:\nSiblings", "# Reasoning Path:\nLucy Lawless -> tv.tv_actor.starring_roles -> m.02t8f60 -> tv.regular_tv_appearance.series -> Tarzan\n# Answer:\nm.02t8f60", "# Reasoning Path:\nLucy Lawless -> tv.tv_actor.starring_roles -> m.0gy7hm0 -> tv.regular_tv_appearance.character -> Stacy\n# Answer:\nm.0gy7hm0"], "ground_truth": ["1968-03-29"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-719", "prediction": ["# Reasoning Path:\nBarack Obama -> government.politician.party -> m.03gjhww -> government.political_party_tenure.party -> Democratic Party\n# Answer:\nm.03gjhww", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.party -> Democratic Party\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> government.politician.party -> m.03gjhww -> freebase.valuenotation.is_reviewed -> Party\n# Answer:\nm.03gjhww", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.venues -> Pepsi Center\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> government.politician.party -> m.03gjhww -> freebase.valuenotation.is_reviewed -> Politician\n# Answer:\nm.03gjhww", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.speeches -> m.04j00hr\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.author -> Aberjhani\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Bound Man -> book.written_work.subjects -> African-American studies\n# Answer:\nA Bound Man", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.venues -> Sports Authority Field at Mile High\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Writer's Journey to Selma, Alabama -> book.written_work.subjects -> African American\n# Answer:\nA Writer's Journey to Selma, Alabama"], "ground_truth": ["Democratic Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.2, "path_recall": 0.3333333333333333, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04yvq68 -> military.military_command.military_conflict -> How Few Remain\n# Answer:\nm.04yvq68", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9nd -> military.military_command.military_conflict -> Battle of Port Republic\n# Answer:\nm.04fv9nd", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> g.11bcf3yybd\n# Answer:\ng.11bcf3yybd", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Foot cavalry\n# Answer:\nFoot cavalry", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> time.event.includes_event -> Battle of McDowell\n# Answer:\nJackson's Valley Campaign", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Abraham Lincoln\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Mexican\u2013American War -> time.event.includes_event -> Battle of La Mesa\n# Answer:\nMexican\u2013American War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Mexican\u2013American War -> time.event.includes_event -> Action of Atlixco\n# Answer:\nMexican\u2013American War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Mexican\u2013American War -> base.culturalevent.event.entity_involved -> Agust\u00edn Jer\u00f3nimo de Iturbide y Huarte\n# Answer:\nMexican\u2013American War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> book.book_subject.musical_compositions_about_this_topic -> the CIVIL warS: a tree is best measured when it is down\n# Answer:\nAmerican Civil War"], "ground_truth": ["First Battle of Rappahannock Station", "Battle of Hancock", "Manassas Station Operations", "American Civil War", "Battle of McDowell", "Battle of Hoke's Run", "Romney Expedition", "Jackson's Valley Campaign", "First Battle of Winchester", "Battle of Front Royal", "Battle of Port Republic", "Battle of Harpers Ferry", "How Few Remain", "Battle of White Oak Swamp", "Battle of Chantilly", "Second Battle of Bull Run", "Battle of Cedar Mountain", "First Battle of Kernstown", "Battle of Chancellorsville"], "ans_acc": 0.2631578947368421, "ans_hit": 1, "ans_f1": 0.1558441558441558, "ans_precission": 0.3, "ans_recall": 0.10526315789473684, "path_f1": 0.1739130434782609, "path_precision": 0.4, "path_recall": 0.1111111111111111, "path_ans_f1": 0.3448275862068966, "path_ans_precision": 0.5, "path_ans_recall": 0.2631578947368421}
{"id": "WebQTest-721", "prediction": ["# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> Leukemia -> people.cause_of_death.parent_cause_of_death -> Cancer\n# Answer:\nLeukemia", "# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> B-cell chronic lymphocytic leukemia -> medicine.disease.notable_people_with_this_condition -> Clive James\n# Answer:\nB-cell chronic lymphocytic leukemia", "# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> Leukemia -> people.cause_of_death.parent_cause_of_death -> Hibakusha\n# Answer:\nLeukemia", "# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> Leukemia -> medicine.disease.symptoms -> Arthritis\n# Answer:\nLeukemia", "# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> B-cell chronic lymphocytic leukemia -> common.topic.article -> m.02bg7v\n# Answer:\nB-cell chronic lymphocytic leukemia", "# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> Leukemia -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nLeukemia", "# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> B-cell chronic lymphocytic leukemia -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nB-cell chronic lymphocytic leukemia", "# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> Leukemia -> medicine.disease.symptoms -> Bleeding\n# Answer:\nLeukemia", "# Reasoning Path:\nJill Clayburgh -> film.actor.film -> m.01z0gn4 -> film.performance.film -> Starting Over\n# Answer:\nm.01z0gn4", "# Reasoning Path:\nJill Clayburgh -> film.actor.film -> m.02tb0z1 -> film.performance.character -> Barbara Jane Bookman\n# Answer:\nm.02tb0z1"], "ground_truth": ["B-cell chronic lymphocytic leukemia", "Leukemia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-723", "prediction": ["# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Luna Park Sydney -> location.location.time_zones -> Australian Eastern Time Zone\n# Answer:\nLuna Park Sydney", "# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Powerhouse Museum -> architecture.structure.architectural_style -> Postmodern architecture\n# Answer:\nPowerhouse Museum", "# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Sydney Opera House -> location.location.events -> 17th Australian Film Institute Awards\n# Answer:\nSydney Opera House", "# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Sydney Opera House -> projects.project_focus.projects -> Design and Construction of the Sydney Opera House\n# Answer:\nSydney Opera House", "# Reasoning Path:\nSydney -> freebase.valuenotation.is_reviewed -> Time zone(s) -> type.property.master_property -> Locations in this time zone\n# Answer:\nTime zone(s)", "# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Luna Park Sydney -> location.location.events -> 1979 Sydney Ghost Train fire\n# Answer:\nLuna Park Sydney", "# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Sydney Opera House -> location.location.containedby -> Australia\n# Answer:\nSydney Opera House", "# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Sydney Opera House -> location.location.events -> 1st AACTA Awards\n# Answer:\nSydney Opera House", "# Reasoning Path:\nSydney -> freebase.valuenotation.is_reviewed -> Time zone(s) -> rdf-schema#range -> Time Zone\n# Answer:\nTime zone(s)", "# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Powerhouse Museum -> architecture.museum.director -> Dawn Casey\n# Answer:\nPowerhouse Museum"], "ground_truth": ["Sydney Film Festival", "The Rocks", "Australian National Maritime Museum", "Bondi Beach", "Featherdale Wildlife Park", "St Mary's Cathedral, Sydney", "Sydney Markets", "Luna Park Sydney", "City2Surf", "North Head Quarantine Station", "Sydney Harbour Bridge", "Hillsong Church", "Kamay Botany Bay National Park", "Powerhouse Museum", "Sydney Opera House", "Port Jackson", "Oatley Park, New South Wales", "Darling Harbour", "Wild Life Sydney", "Rose Seidler House"], "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0.2526315789473684, "ans_precission": 0.8, "ans_recall": 0.15, "path_f1": 0.2526315789473684, "path_precision": 0.8, "path_recall": 0.15, "path_ans_f1": 0.2526315789473684, "path_ans_precision": 0.8, "path_ans_recall": 0.15}
{"id": "WebQTest-724", "prediction": ["# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.place_of_death -> Brentwood -> location.location.containedby -> California\n# Answer:\nBrentwood", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.place_of_death -> Brentwood -> location.neighborhood.neighborhood_of -> Greater Los Angeles Area\n# Answer:\nBrentwood", "# Reasoning Path:\nMarilyn Monroe -> people.person.place_of_birth -> Los Angeles -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nLos Angeles", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.place_of_death -> Brentwood -> location.location.containedby -> Greater Los Angeles Area\n# Answer:\nBrentwood", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.place_of_death -> Brentwood -> location.location.containedby -> Los Angeles\n# Answer:\nBrentwood", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.place_of_death -> Brentwood -> location.neighborhood.neighborhood_of -> Los Angeles\n# Answer:\nBrentwood", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.place_of_death -> Brentwood -> organization.organization_scope.organizations_with_this_scope -> Brentwood Public School\n# Answer:\nBrentwood", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.place_of_death -> Brentwood -> organization.organization_scope.organizations_with_this_scope -> Saketini\n# Answer:\nBrentwood", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.07zn09y -> award.award_nomination.ceremony -> 9th British Academy Film Awards\n# Answer:\nm.07zn09y", "# Reasoning Path:\nMarilyn Monroe -> people.person.place_of_birth -> Los Angeles -> location.location.containedby -> United States of America\n# Answer:\nLos Angeles"], "ground_truth": ["Brentwood"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-725", "prediction": ["# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> Nevada -> location.location.containedby -> Contiguous United States\n# Answer:\nNevada", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> Nevada -> location.location.people_born_here -> A. J. Haglund\n# Answer:\nNevada", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> California -> location.location.events -> 1987 Wine Country Film Festival\n# Answer:\nCalifornia", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> Nevada -> location.location.containedby -> Mountain States\n# Answer:\nNevada", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> Nevada -> location.location.partiallycontains -> m.0wg93c0\n# Answer:\nNevada", "# Reasoning Path:\nSierra Nevada -> location.location.containedby -> United States of America -> location.location.partially_containedby -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> Nevada -> location.location.people_born_here -> Adrian C. Louis\n# Answer:\nNevada", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> California -> location.location.events -> 1988 Wine Country Film Festival\n# Answer:\nCalifornia", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> California -> common.topic.webpage -> m.09w19w3\n# Answer:\nCalifornia", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> Nevada -> location.location.containedby -> Southwestern United States\n# Answer:\nNevada"], "ground_truth": ["Nevada", "California"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-726", "prediction": ["# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> media_common.netflix_genre.titles -> A Girl in Black\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> language.human_language.region -> Europe\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> media_common.netflix_genre.titles -> A Matter of Dignity\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.main_country -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> media_common.netflix_genre.titles -> A Touch of Spice\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.region -> Southeast Europe\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Italy\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Greek Language -> media_common.netflix_genre.titles -> A Girl in Black\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Greek Language -> language.human_language.region -> Europe\n# Answer:\nGreek Language"], "ground_truth": ["Greek Language", "Albanian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-728", "prediction": ["# Reasoning Path:\nSpeaker of the United States House of Representatives -> government.government_office_or_title.office_holders -> m.0g4_6k1 -> government.government_position_held.office_holder -> John Boehner\n# Answer:\nm.0g4_6k1", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> government.government_office_or_title.office_holders -> m.03hcgxq -> government.government_position_held.office_holder -> Nancy Pelosi\n# Answer:\nm.03hcgxq", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> government.government_office_or_title.office_holders -> m.0g4_6k1 -> government.government_position_held.governmental_body -> United States House of Representatives\n# Answer:\nm.0g4_6k1", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> government.government_office_or_title.office_holders -> m.0g4_6k1 -> government.government_position_held.basic_title -> Speaker\n# Answer:\nm.0g4_6k1", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> government.government_office_or_title.office_holders -> m.04mmc8c -> government.government_position_held.office_holder -> Theodore Sedgwick\n# Answer:\nm.04mmc8c", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> government.government_office_or_title.office_holders -> m.03hcgxq -> government.government_position_held.basic_title -> Speaker\n# Answer:\nm.03hcgxq", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> base.schemastaging.context_name.plural -> m.0w0qtlx -> base.schemastaging.plural_form.language -> Hebrew\n# Answer:\nm.0w0qtlx", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> common.topic.image -> Nancy Pelosig -> common.image.size -> m.03rxy3k\n# Answer:\nNancy Pelosig", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> government.government_office_or_title.office_holders -> m.04mmc8c -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nm.04mmc8c", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> base.schemastaging.context_name.plural -> m.0w0qtlx -> base.schemastaging.plural_form.gender -> All Female\n# Answer:\nm.0w0qtlx"], "ground_truth": ["Nancy Pelosi", "John Boehner"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.16666666666666669, "ans_precission": 0.1, "ans_recall": 0.5, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-729", "prediction": ["# Reasoning Path:\nStanley Hudson -> tv.tv_character.appeared_in_tv_program -> m.05nphmt -> tv.regular_tv_appearance.actor -> Leslie David Baker\n# Answer:\nm.05nphmt", "# Reasoning Path:\nStanley Hudson -> tv.tv_character.appeared_in_tv_program -> m.05nphmt -> tv.regular_tv_appearance.series -> The Office\n# Answer:\nm.05nphmt", "# Reasoning Path:\nStanley Hudson -> tv.tv_character.appeared_in_tv_program -> m.05nphmt -> tv.regular_tv_appearance.seasons -> The Office\n# Answer:\nm.05nphmt", "# Reasoning Path:\nStanley Hudson -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Kidney cancer\n# Answer:\nMale", "# Reasoning Path:\nStanley Hudson -> tv.tv_character.appeared_in_tv_program -> m.05nphmt -> tv.regular_tv_appearance.seasons -> The Office - Season 7\n# Answer:\nm.05nphmt", "# Reasoning Path:\nStanley Hudson -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nStanley Hudson -> tv.tv_character.appeared_in_tv_program -> m.05nphmt -> tv.regular_tv_appearance.seasons -> The Office - Season 8\n# Answer:\nm.05nphmt", "# Reasoning Path:\nStanley Hudson -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nStanley Hudson -> common.topic.notable_types -> TV Character -> type.type.properties -> Appeared In TV Episode Segments\n# Answer:\nTV Character", "# Reasoning Path:\nStanley Hudson -> common.topic.notable_types -> TV Character -> type.type.expected_by -> Character\n# Answer:\nTV Character"], "ground_truth": ["Leslie David Baker"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.parent -> Ongamo-Maa Group\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ng.1256fv3pv", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> common.topic.article -> m.064klf\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.document -> Maasai Detailed Description\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.document -> Maasai Genesis Translation\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai -> common.image.size -> m.02bgrp_\n# Answer:\nMaasai", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.document -> Maasai Phonology\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai women and children -> common.image.size -> m.03qy0z9\n# Answer:\nMaasai women and children", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai-jump -> common.image.size -> m.02br_p7\n# Answer:\nMaasai-jump"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-730", "prediction": ["# Reasoning Path:\nKelly Clarkson -> music.artist.label -> Sony BMG -> business.defunct_company.reason_for_ceasing_operations -> Buyout\n# Answer:\nSony BMG", "# Reasoning Path:\nKelly Clarkson -> music.artist.label -> Sony BMG -> common.topic.article -> m.03mp8v\n# Answer:\nSony BMG", "# Reasoning Path:\nKelly Clarkson -> music.artist.label -> Sony Music Entertainment -> business.business_operation.industry -> Music\n# Answer:\nSony Music Entertainment", "# Reasoning Path:\nKelly Clarkson -> music.artist.label -> RCA Records -> award.award_nominee.award_nominations -> m.0sgj7wg\n# Answer:\nRCA Records", "# Reasoning Path:\nKelly Clarkson -> music.artist.label -> Sony BMG -> business.defunct_company.reason_for_ceasing_operations -> Rebranding\n# Answer:\nSony BMG", "# Reasoning Path:\nKelly Clarkson -> base.saturdaynightlive.snl_musical_guest.musical_performance -> m.04p4_dv -> base.saturdaynightlive.snl_musical_performance.episode -> SNL - 30.12\n# Answer:\nm.04p4_dv", "# Reasoning Path:\nKelly Clarkson -> music.artist.label -> Sony BMG -> organization.organization.child -> m.04kjv9q\n# Answer:\nSony BMG", "# Reasoning Path:\nKelly Clarkson -> music.artist.label -> RCA Records -> common.topic.notable_types -> Record label\n# Answer:\nRCA Records", "# Reasoning Path:\nKelly Clarkson -> music.artist.label -> Sony Music Entertainment -> business.business_operation.industry -> Arts, Entertainment, and Recreation\n# Answer:\nSony Music Entertainment", "# Reasoning Path:\nKelly Clarkson -> music.artist.label -> Sony BMG -> organization.organization.child -> m.04m0wdw\n# Answer:\nSony BMG"], "ground_truth": ["RCA Records", "19 Recordings", "Sony Music Entertainment", "Sony BMG"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.8181818181818182, "path_precision": 0.9, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-731", "prediction": ["# Reasoning Path:\nTim Roth -> people.person.place_of_birth -> Dulwich -> location.location.containedby -> London\n# Answer:\nDulwich", "# Reasoning Path:\nTim Roth -> people.person.place_of_birth -> Dulwich -> location.location.people_born_here -> A. E. W. Mason\n# Answer:\nDulwich", "# Reasoning Path:\nTim Roth -> people.person.place_of_birth -> Dulwich -> travel.travel_destination.tourist_attractions -> Dulwich Picture Gallery\n# Answer:\nDulwich", "# Reasoning Path:\nTim Roth -> people.person.place_of_birth -> Dulwich -> location.location.people_born_here -> Alan Ratcliffe\n# Answer:\nDulwich", "# Reasoning Path:\nTim Roth -> people.person.place_of_birth -> Dulwich -> location.location.people_born_here -> Anne Shelton\n# Answer:\nDulwich", "# Reasoning Path:\nTim Roth -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nTim Roth -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nTim Roth -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nTim Roth -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#domain -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nTim Roth -> freebase.valuenotation.is_reviewed -> Gender -> type.property.schema -> Person\n# Answer:\nGender"], "ground_truth": ["Dulwich"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-733", "prediction": ["# Reasoning Path:\nVietnam -> location.country.currency_used -> Vietnamese dong -> common.topic.image -> 1 \u0111\u1ed3ng 1976\n# Answer:\nVietnamese dong", "# Reasoning Path:\nVietnam -> location.country.currency_used -> Vietnamese dong -> common.topic.notable_types -> Currency\n# Answer:\nVietnamese dong", "# Reasoning Path:\nVietnam -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d1ntkd\n# Answer:\ng.11b6d1ntkd", "# Reasoning Path:\nVietnam -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71w7lzm\n# Answer:\ng.11b71w7lzm", "# Reasoning Path:\nVietnam -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6p1vnp7\n# Answer:\ng.11b6p1vnp7", "# Reasoning Path:\nVietnam -> location.statistical_region.renewable_freshwater_per_capita -> g.1hhc3bvgz\n# Answer:\ng.1hhc3bvgz", "# Reasoning Path:\nVietnam -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6f73_\n# Answer:\ng.12tb6f73_", "# Reasoning Path:\nVietnam -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc3b60b\n# Answer:\ng.1hhc3b60b"], "ground_truth": ["Vietnamese dong"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-734", "prediction": ["# Reasoning Path:\nRoger Federer -> people.person.children -> Charlene Riva Federer -> people.person.parents -> Mirka Federer\n# Answer:\nCharlene Riva Federer", "# Reasoning Path:\nRoger Federer -> people.person.children -> Lenny Federer -> people.person.parents -> Mirka Federer\n# Answer:\nLenny Federer", "# Reasoning Path:\nRoger Federer -> people.person.children -> Charlene Riva Federer -> people.person.sibling_s -> m.010h346t\n# Answer:\nCharlene Riva Federer", "# Reasoning Path:\nRoger Federer -> people.person.children -> Leo Federer -> people.person.parents -> Mirka Federer\n# Answer:\nLeo Federer", "# Reasoning Path:\nRoger Federer -> people.person.children -> Charlene Riva Federer -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nCharlene Riva Federer", "# Reasoning Path:\nRoger Federer -> people.person.spouse_s -> m.0j12_p2 -> people.marriage.spouse -> Mirka Federer\n# Answer:\nm.0j12_p2", "# Reasoning Path:\nRoger Federer -> people.person.spouse_s -> m.0j12_p2 -> people.marriage.location_of_ceremony -> Riehen\n# Answer:\nm.0j12_p2", "# Reasoning Path:\nRoger Federer -> people.person.children -> Charlene Riva Federer -> freebase.valuenotation.has_value -> Place of birth\n# Answer:\nCharlene Riva Federer", "# Reasoning Path:\nRoger Federer -> people.person.children -> Lenny Federer -> people.person.nationality -> Switzerland\n# Answer:\nLenny Federer", "# Reasoning Path:\nRoger Federer -> people.person.spouse_s -> m.0j12_p2 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0j12_p2"], "ground_truth": ["Mirka Federer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5333333333333333, "path_precision": 0.4, "path_recall": 0.8, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-735", "prediction": ["# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.image -> 10colonesbcrfront\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.webpage -> m.04m2m2k\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60wmkvh\n# Answer:\ng.11b60wmkvh", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.11b71w3_6h\n# Answer:\ng.11b71w3_6h", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_5xgy\n# Answer:\ng.1245_5xgy", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_6n42\n# Answer:\ng.1245_6n42", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.1hhc37pjz\n# Answer:\ng.1hhc37pjz", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.1hhc37x5q\n# Answer:\ng.1hhc37x5q"], "ground_truth": ["Costa Rican col\u00f3n"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-737", "prediction": ["# Reasoning Path:\nBelgium -> location.country.capital -> Brussels -> location.location.contains -> Laeken\n# Answer:\nBrussels", "# Reasoning Path:\nBelgium -> location.country.capital -> Brussels -> location.location.time_zones -> Central European Time Zone\n# Answer:\nBrussels", "# Reasoning Path:\nBelgium -> location.statistical_region.gni_in_ppp_dollars -> g.11b61jk1pw\n# Answer:\ng.11b61jk1pw", "# Reasoning Path:\nBelgium -> location.country.capital -> City of Brussels -> location.location.time_zones -> Central European Time Zone\n# Answer:\nCity of Brussels", "# Reasoning Path:\nBelgium -> location.statistical_region.gni_in_ppp_dollars -> g.1245__rg8\n# Answer:\ng.1245__rg8", "# Reasoning Path:\nBelgium -> location.statistical_region.gni_in_ppp_dollars -> g.1245_dgwn\n# Answer:\ng.1245_dgwn", "# Reasoning Path:\nBelgium -> location.country.capital -> City of Brussels -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nCity of Brussels", "# Reasoning Path:\nBelgium -> media_common.netflix_genre.titles -> Altiplano -> film.film.prequel -> Khadak\n# Answer:\nAltiplano", "# Reasoning Path:\nBelgium -> media_common.netflix_genre.titles -> Altiplano -> film.film.language -> English Language\n# Answer:\nAltiplano", "# Reasoning Path:\nBelgium -> media_common.netflix_genre.titles -> 22nd of May -> film.film.country -> Germany\n# Answer:\n22nd of May"], "ground_truth": ["Brussels", "City of Brussels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-738", "prediction": ["# Reasoning Path:\nJapan -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Australia\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Canada\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> City\n# Answer:\nEarth", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\ng.1245_2h9t", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> New Zealand\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Philippines\n# Answer:\nUnitary state", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nJapan -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> City/Town/Village\n# Answer:\nEarth", "# Reasoning Path:\nJapan -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> France\n# Answer:\nEarth"], "ground_truth": ["Constitutional monarchy", "Parliamentary system", "Unitary state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-739", "prediction": ["# Reasoning Path:\nClaude Debussy -> common.topic.image -> Claude Debussy, ca. 1908 (photo by F\u00e9lix Nadar) -> common.image.appears_in_topic_gallery -> Claude\n# Answer:\nClaude Debussy, ca. 1908 (photo by F\u00e9lix Nadar)", "# Reasoning Path:\nClaude Debussy -> common.topic.image -> Claude Debussy, ca. 1908 (photo by F\u00e9lix Nadar) -> common.image.size -> m.02bcsxr\n# Answer:\nClaude Debussy, ca. 1908 (photo by F\u00e9lix Nadar)", "# Reasoning Path:\nClaude Debussy -> common.topic.image -> Debussy at the Villa M\u00e9dici in Rome, 1885, at centre in the white jacket -> common.image.size -> m.041td0y\n# Answer:\nDebussy at the Villa M\u00e9dici in Rome, 1885, at centre in the white jacket", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> Art song -> music.genre.artists -> Aaron Copland\n# Answer:\nArt song", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> Art song -> music.compositional_form.compositions -> L'adieu du cavalier\n# Answer:\nArt song", "# Reasoning Path:\nClaude Debussy -> common.topic.image -> Claude Debussy, ca. 1908 (photo by F\u00e9lix Nadar) -> common.image.appears_in_topic_gallery -> Estampes\n# Answer:\nClaude Debussy, ca. 1908 (photo by F\u00e9lix Nadar)", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> Art song -> music.genre.artists -> Albert Roussel\n# Answer:\nArt song", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> Art song -> music.compositional_form.compositions -> Danse macabre\n# Answer:\nArt song", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> 20th-century classical music -> music.genre.albums -> Octet / Music for a Large Ensemble / Violin Phase\n# Answer:\n20th-century classical music", "# Reasoning Path:\nClaude Debussy -> common.topic.image -> Claude Debussy, ca. 1908 (photo by F\u00e9lix Nadar) -> common.image.appears_in_topic_gallery -> Images pour orchestre\n# Answer:\nClaude Debussy, ca. 1908 (photo by F\u00e9lix Nadar)"], "ground_truth": ["Pi\u00e8ce pour le V\u00eatement du bless\u00e9 (Page d'album), L. 133", "Paysage sentimental", "Ballade des femmes de Paris", "Je tremble en voyant ton visage", "Cinq po\u00e8mes de Baudelaire, L. 64, CD 70: I. Le Balcon", "Les papillons", "Pour le piano", "Le Martyre de Saint-S\u00e9bastien, L. 124: Acte III \\\"Le Concile des faux dieux\\\". Fanfare no. 1", "La mort des amants", "Apparition, L. 53, CD 57, m\u00e9lodie pour voix et piano \\\"La lune s\u2019attristait. Des s\u00e9raphins en pleurs\\\"", "Pantomime", "Romance, L. 43, CD 53, m\u00e9lodie pour voix et piano \\\"Silence ineffable de l\u2019heure\\\"", "Syrinx huilulle, L 129", "Ariettes oubli\u00e9es, L. 60, CD 63a : II. Il pleut doucement sur la ville, m\u00e9lodie pour voix et piano \\\"Il pleure dans mon c\u0153ur comme il pleure sur la ville\\\"", "UTSUKUSHII YUUGURE", "Deux danses pour Harpe, L. 103: Danse sacr\u00e9e", "Jeux", "Mandoline", "Images oubli\u00e9es, L. 87: Dans le mouvement d'une 'Sarabande', c'est-\u00e0-dire avec une \u00e9l\u00e9gance gr\u00e2ve et lente, m\u00eame un peu vieux portrait, souvenir du Louvre, etc.", "Le Martyre de Saint-S\u00e9bastien: III. Le concile des faux dieux: N\u00b01", "Ballade (slave), L. 70, CD 78, pour piano", "Le Martyre de Saint-S\u00e9bastien, L. 124: Acte III \\\"Le Concile des faux dieux\\\"", "Chansons de Bilitis", "F\u00eates galantes II, L. 104: I. Les Ing\u00e9nus", "Clair de lune, L. 32, CD 45, m\u00e9lodie pour voix et piano \\\"Votre \u00e2me est un paysage choisi\\\"", "La Mer: II. Jeux de vagues", "\u00c9ventail", "Trois M\u00e9lodies, L. 81, CD 85, pour une voix avec accompagnement de piano : III. L\u2019\u00e9chelonnement des haies moutonne \u00e0 l\u2019infini", "Ariettes oubli\u00e9es, L. 60, CD 63a : I. Le vent dans la plaine suspend son haleine, m\u00e9lodie pour voix et piano \\\"C'est l'extase langoureuse\\\"", "Sonata for flute, viola and harp, L. 137: I. Pastorale", "Colloque sentimental", "Arabesque 1", "Le Martyre de Saint-S\u00e9bastien, fragments symphoniques: IV. Le Bon Pasteur", "Nocturnes, L 91: I. Nuages", "Pell\u00e9as et M\u00e9lisande: Act III, Scene IV. \\\"Viens, nous allons nous asseoir ici, Yniold\\\" (Golaud, Yniold)", "Iberia No. 2 from \\\"Images\\\" for Orchestra: II. The Perfumes of the Night", "Yver, vous n'estes qu'un vilain", "C'est l'extase", "Trois po\u00e8mes de St\u00e9phane Mallarm\u00e9, L. 127, CD 135: II. Placet futile \\\"Princesse! \u00c0 jalouser le destin d'une H\u00e9b\u00e9\\\"", "Les Chansons de Bilitis, L. 96: No. 1. Chant pastoral", "Clair de lune", "Nuits blanches", "Children's Corner, L. 113: IV. The Snow is Dancing", "Premi\u00e8re Rapsodie pour clarinette en si b\u00e9mol, avec accompagnement d'orchestre, L. 116, CD 124b", "De fleurs", "Cort\u00e8ge et Air de danse de \\\"L\u2019Enfant prodigue\\\"", "Images oubli\u00e9es, L. 87: Lent (m\u00e9lancolique et doux)", "Nocturnes, L. 91, CD 98: I. Nuages", "Le faune", "Khamma: Troisi\u00e8me Danse. Tr\u00e8s lent \u2013 Plus p\u00e9n\u00e9trant \u2013 Doucement contenu \u2013", "Romance", "Cinq po\u00e8mes de Baudelaire, L. 64, CD 70: II. Harmonie du soir", "\u00c9tudes, L. 136: VI. Pour les huits doigts", "Masques, L. 105", "Les Chansons de Bilitis, L. 96: No. 12. La Pluie au matin", "Sonate pour violoncelle et piano: III. Finale", "Children's Corner, L. 113: VI. Golliwogg's Cake-Walk", "Bilitis: II. Pour un tombeau sans nom", "Iberia No. 2 from \\\"Images\\\" for Orchestra: III. The Morning of a Holiday", "Proses lyriques, L. 84, CD 90 : IV. De Soirs, m\u00e9lodie \\\"Dimanche sur les villes\\\"", "Chansons de Bilitis, L. 90: Le Tombeau des na\u00efades: \u00abLe long du bois couvert de givre\u00bb", "Regret", "Les Chansons de Bilitis, L. 96: No. 4. Chanson", "Images II: I. Cloches trevers les feuilles", "La mer", "La Damoiselle \u00e9lue, L. 62, CD 69, po\u00e8me lyrique pour voix de femmes, solo, ch\u0153ur et orchestre", "Suite bergamasque, L. 75, CD 82b, pour orchestre : III. Clair de lune", "Trois Chansons de France, L. 102: III. Rondel: Pour ce que Plaisance est morte", "Pell\u00e9as et M\u00e9lisande: Act I, Scene III. \\\"Il fait sombre dans les jardins\\\" (M\u00e9lisande, Genevi\u00e8ve, Pell\u00e9as)", "From Dawn Till Noon on the Sea", "L\u2019Archet", "No\u00ebl des enfants qui n'ont plus de maison, L. 139, CD 147, m\u00e9lodie pour voix et piano \\\"Nous n\u2019avons plus de maison\\\"", "Chanson espagnole, L. 42, CD 49, duo pour 2 voix \u00e9gales \\\"Nous venions de voir le taureau\\\"", "Le lilas", "\u00c9l\u00e9gie, L. 138", "La Bo\u00eete \u00e0 joujoux : II. 1er tableau. Le Magasin de jouets", "Aimons-nous et dormons, L. 16, CD 7, m\u00e9lodie pour voix et piano \\\"Aimons-nous et dormons, sans songer au reste du monde\\\"", "Khamma: Sc\u00e8ne 3", "Brouillards", "Petite Suite", "La Bo\u00eete \u00e0 joujoux : III. 2e tableau. Le Champ de bataille", "Suite Bergamasque: Passepied: Allegretto ma non troppo", "Suite bergamasque, L. 75, CD 82, pour orchestre : III. Clair de lune", "Le promenoir des deux amants, L. 118, CD 129, m\u00e9lodies pour voix et piano", "g.1234nfvz", "Minstrels", "Children's Corner, L. 113 No. 6: Golliwog's Cake-walk", "Des pas sur la neige", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene I. \\\"Pell\u00e9as part ce soir\\\" (Golaud, Arkel, M\u00e9lisande)", "Beau soir", "Petite Suite: Menuet", "Petite Pi\u00e8ce", "Pr\u00e9lude \u00e0 l'apr\u00e8s-midi d'un faune, L. 86, CD 87", "Images, Livre 2, L. 111: No. 3. Poissons d'or", "L'isle joyeuse", "Pagodes", "Proses Lyriques", "Sonate pour violoncelle et piano: II. S\u00e9r\u00e9nade", "Le Martyre de Saint-S\u00e9bastien: III. Le concile des faux dieux: N\u00b05", "Cello Sonata", "Sonata for flute, viola and harp, L. 137: III. Finale", "Deux danses pour Harpe, L. 103", "La Bo\u00eete \u00e0 Joujoux : I. Pr\u00e9lude. Le Sommeil de la bo\u00eete", "Trois Chansons de Charles d'Orl\u00e9ans", "Le Martyre de Saint-S\u00e9bastien: IV. Le laurier bless\u00e9: N\u00b01", "Le promenoir des deux amants, L. 118, CD 129, m\u00e9lodies pour voix et piano : I. \\\"Aupr\u00e8s de cette grotte sombre\\\"", "Trio in G Major for Violin, Cello and Piano: IV. Finale. Appassionato", "\u00c9tudes, L. 136: X. Pour les sonorit\u00e9s oppos\u00e9es", "Voici que le printemps", "Images oubli\u00e9es, L. 87: Quelques aspects de 'Nous n'irons plus au bois' parce qu'il fait un temps insupportable. Tr\u00e8s vite - Mod\u00e9r\u00e9 - Premier Mouvement (vit et joyeax)", "Fantaisie for piano and orchestra", "My Reverie (Debussy's \\\"Reverie\\\")", "Petite Suite, L. 65, CD 71a, pour piano \u00e0 4 mains : II. Cort\u00e8ge", "Ariettes Oubli\u00e9es", "Pell\u00e9as et M\u00e9lisande: Act I, Scene II. \\\"Voici ce qu'il \u00e9crit \u00e0 son fr\u00e8re Pell\u00e9as: 'Un soir, je l'ai trouv\u00e9e'\\\" (Genevi\u00e8ve)", "Nuit d\u2019\u00e9toiles, L. 4, CD 2, m\u00e9lodie pour voix et piano \\\"Nuit d\u2019\u00e9toiles, sous tes voiles\\\"", "Premi\u00e8re Rapsodie pour clarinette en si b\u00e9mol, avec accompagnement de piano, L. 116, CD 124a", "Danse: Tarantelle styrienne", "En blanc et noir, L. 134, CD 142", "Khamma: Au Mouvement \u2013", "Three Preludes: Feuilles mortes", "Piano Trio", "Pantomime, L. 31, CD 47, m\u00e9lodie pour voix et piano \\\"Pierrot, qui n\u2019a rien d\u2019un Clitandre\\\"", "Morceau de concours, L. 108", "Trois po\u00e8mes de St\u00e9phane Mallarm\u00e9, L. 127, CD 135: I. Soupir \\\"Mon \u00e2me vers ton front o\u00f9 r\u00eave, \u00f4 calme s\u0153ur\\\"", "Pell\u00e9as et M\u00e9lisande, L. 88, CD 93: Acte IV", "Menuet", "Pell\u00e9as et M\u00e9lisande: Act V. \\\"Non, non, nous n'avons pas \u00e9t\u00e9 coupables\\\" (M\u00e9lisande, Golaud)", "Preludes, Book II: 2. Feuilles mortes", "La Romance d\u2019Ariel, L. 54, CD 58, m\u00e9lodie pour voix et piano \\\"Au long de ces montagnes douces\\\"", "R\u00eaverie", "Le Tombeau des Na\u00efades", "Pr\u00e9ludes, Livre I, L. 117: VIII. La fille aux cheveux de lin. Tr\u00e8s calme et doucement expressif", "Streichquartett in g-Moll, Op. 10: Andantino, doucement expressif (Endress-Quartett)", "La Fille aux cheveux de lin, L. 33, CD 15, m\u00e9lodie pour voix et piano \\\"Sur la luzerne en fleur\\\"", "Suite bergamasque", "Pour le piano: Sarabande", "Rapsodie pour saxophone et piano, L. 98", "L'\u00e9chelonnement des haies", "Le Martyre de Saint-S\u00e9bastien, fragments symphoniques: I. La Cour de lys", "Pell\u00e9as et M\u00e9lisande - Concert Suite: Acte I. Une for\u00eat", "Syrinx", "Les Ang\u00e9lus", "Bilitis: VI. Pour remercier la pluie au matin", "La pluie au matin", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene II. \\\"Oh! Cette pierre est lourde ...\\\" (Yniold, Le Berger)", "My Reverie", "Sonate f\u00fcr Violine und Klavier in g-Moll: Finale: Tr\u00e8s anim\u00e9", "Khamma", "Clair de lune Samba", "Preludes, Book II: 10. Canope", "Soupir", "F\u00eates galantes II, L. 104: III. Colloque sentimental", "Six \u00e9pigraphes antiques, L. 131: IV. Pour la danseuse aux crotales", "Trois M\u00e9lodies, L. 81, CD 85, pour une voix avec accompagnement de piano : II. Le son du cor s\u2019afflige vers les bois.", "Nocturnes, L. 91, CD 98: II. F\u00eates", "\u00c9tudes, L. 136: V. Pour les octaves", "Bilitis: V. Pour l'\u00e9gyptienne", "Dans le Jardin", "Estampes", "Green", "Cinq Po\u00e8mes de Baudelaire", "Estampes II: La soir e dans Grenade", "Trois Ballades de Fran\u00e7ois Villon", "Les Chansons de Bilitis, L. 96: No. 6. Bilitis", "No\u00ebl des enfants qui n'ont plus de maisons", "Quatuor \u00e0 cordes en sol mineur, op. 10, L. 85, CD 91 : II. Assez vif et bien rythm\u00e9", "L\u2019Enfant prodigue : 2b. Pourquoi m'as-tu quitt\u00e9e (Lia)", "Les Chansons de Bilitis, L. 96: No. 5. La Partie d'osselets", "Children's Corner", "L'enfant prodigue", "Nocturnes, L 91: II. F\u00eates", "Dans le jardin, L. 78, CD 107, m\u00e9lodie pour voix et piano \\\"Je regardais dans le jardin\\\"", "Le Martyre de Saint-S\u00e9bastien: III. Le concile des faux dieux: N\u00b06", "Le Martyre de Saint-S\u00e9bastien: I. La cour des lys: N\u00b02", "La mer est plus belle", "Khamma: Pr\u00e9lude. Mod\u00e9r\u00e9ment anim\u00e9 (comme un lointain tumulte) \u2013", "Suite Bergamasque - Menuet", "Nocturnes, L 91: III. Sir\u00e8nes", "Reflets dans l'eau", "Crois mon conseil, ch\u00e8re Clim\u00e8ne", "Proses lyriques, L. 84, CD 90 : III. De Fleurs, m\u00e9lodie \\\"Dans l\u2019ennui si d\u00e9sol\u00e9ment vert\\\"", "Images pour orchestre, L 122: I. Gigues", "Chevaux de bois", "g.11b821q1dm", "Les Chansons de Bilitis, L. 96: No. 9. L'Eau pure du bassin", "Pell\u00e9as et M\u00e9lisande: Act I, Scene III. \\\"Ho\u00e9! Hisse ho\u00e9! Ho\u00e9! Ho\u00e9!\\\" (Ch\u0153r, M\u00e9lisande, Pell\u00e9as, Genevi\u00e8ve)", "La Bo\u00eete \u00e0 joujoux : V. 4e tableau. Apr\u00e8s fortune faite", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene I. \\\"O\u00f9 vas-tu? Il faut que je te parle ce soir\\\" (Pell\u00e9as, M\u00e9lisande)", "Trio in G Major for Violin, Cello and Piano: III. Andante espressivo", "Petite Suite, L. 65, CD 71b, pour orchestre : I. En bateau", "Cort\u00e8ge et Air de danse de \\\"L\u2019Enfant prodigue\\\": Air de danse", "Quant j'ai ouy le tabourin", "Placet futile", "Two movements from \\\"L\u2019Enfant prodigue\\\": Pr\u00e9lude", "Les ing\u00e9nus", "Danse boh\u00e9mienne, L. 9, CD 4, pour piano", "\u00c9tudes, L. 136: XII. Pour les accords", "F\u00eates galantes II, L. 104: II. Le Faune", "g.1234bndn", "F\u00eates galantes (Premier recueil), L. 80, CD 86 : II. Fantoches, m\u00e9lodie pour voix et piano \\\"Scaramouche et Pulcinella\\\"", "R\u00eaverie, L. 8, CD 3, m\u00e9lodie pour voix et piano \\\"Le Z\u00e9phir \u00e0 la douce haleine\\\"", "Pell\u00e9as et M\u00e9lisande: Act I, Scene I. \\\"Je ne pourrai plus sortir de cette for\u00eat\\\" (Golaud, M\u00e9lisande)", "Khamma: Sc\u00e8ne 2 \u2013", "Le Matelot qui tombe \u00e0 l\u2019eau", "Musique, L. 44, CD 54, m\u00e9lodie pour voix et piano \\\"La lune se levait, pure, mais plus glac\u00e9e\\\"", "Les soirs illumin\u00e9s par l'ardeur du charbon", "Ministrels for Cello and Piano", "Rodrigue et Chim\u00e8ne", "Le petit N\u00e8gre, L. 114", "Suite: Pour le Piano, L.95: III. Toccata (vif)", "La Bo\u00eete \u00e0 joujoux : IV. 3e tableau. La Bergerie \u00e0 vendre", "Sonata for flute, viola and harp, L. 137: II. Interlude", "Printemps, L. 61, CD 68a, suite symphonique en mi majeur pour 2 pianos et ch\u0153ur", "Le Martyre de Saint-S\u00e9bastien: III. Le concile des faux dieux: N\u00b03", "Preludes, Book I: 7. Ce qu'a vu le vent d'ouest", "Triolet \u00e0 Philis \\\"Z\u00e9phir\\\", L. 12, CD 19, m\u00e9lodie pour voix et piano \\\"Si j\u2019\u00e9tais le z\u00e9phir ail\u00e9\\\"", "III. Le Vent dans la plaine", "Premi\u00e8re Suite d\u2019orchestre, L. 50, CD 46 : III. R\u00eave", "Les cloches", "Petite Suite, L. 65, CD 71a, pour piano \u00e0 4 mains", "Suite bergamasque, L. 75, CD 82a, pour orchestre : III. Clair de lune", "Petite Suite, L. 65, CD 71b, pour orchestre : III. Menuet", "Petite Suite, L. 65, CD 71a, pour piano \u00e0 4 mains : I. En bateau", "Les roses", "F\u00eates galantes (Premier recueil), L. 80, CD 86 : III. Clair de lune, m\u00e9lodie pour voix et piano \\\"Votre \u00e2me est un paysage choisi\\\"", "Marche \u00e9cossaise sur un th\u00e8me populaire, L. 77, CD 83a (Marche des anciens comtes de Ross, d\u00e9di\u00e9e \u00e0 leur descendant le G\u00e9n\u00e9ral Meredith Reid, grand-croix de l'ordre royal du R\u00e9dempteur)", "\u00c9tudes, L. 136: VII. Pour les degr\u00e9s chromatiques", "Ariettes oubli\u00e9es, L. 60, CD 63a : VI. Aquarelles, 2. Spleen, m\u00e9lodie pour voix et piano \\\"Les roses \u00e9taient toutes rouges\\\"", "Six \u00e9pigraphes antiques, L. 131", "En blanc et noir", "Trois chansons de Charles d'Orl\u00e9ans, pour ch\u0153ur \u00e0 quatre voix mixtes, L. 92, CD 99: III. \\\"Yver, vous n'estes qu'un vilain \\\"", "Les contes", "Premi\u00e8re rhapsodie", "Trois po\u00e8mes de St\u00e9phane Mallarm\u00e9, L. 127, CD 135: III. \u00c9ventail \\\"\u00d4 r\u00eaveuse pour que je plonge\\\"", "Chant pastoral", "Romance, L. 52, CD 56, m\u00e9lodie pour voix et piano \\\"Voici que le printemps, ce fil l\u00e9ger d\u2019avril\\\"", "Pell\u00e9as et M\u00e9lisande: Act I, Scene II. \\\"Grand-p\u00e8re, j'ai re\u00e7u en m\u00eame temps que la lettre de mon fr\u00e8re...\\\" (Pell\u00e9as, Arkel, Genevi\u00e8ve)", "Cort\u00e8ge et Air de danse de \\\"L\u2019Enfant prodigue\\\": Cort\u00e8ge", "Violin Sonata", "Prelude (From Suite Bergamasque)", "Images, Livre 2, L. 111: No. 2. Et la lune descend sur le temple qui fut", "Petite Suite, L. 65, CD 71b, pour orchestre : II. Cort\u00e8ge", "Valse romantique", "Pell\u00e9as et M\u00e9lisande: Act V. \\\"Qu'avez-vous fait? Vous allez la tuer\\\" (Arkel, Golaud, M\u00e9lisande)", "Le promenoir des deux amants", "Ballade de Villon a s'amye", "Musique pour \u201cLe Roi Lear\u201d, L. 107: Le Sommeil de Lear", "En blanc et noir: I. Avec emportement", "Les Chansons de Bilitis, L. 96: No. 8. Les Courtisanes \u00e9gyptiennes", "Pell\u00e9as et M\u00e9lisande: Act III, Scene IV. \\\"Ah! Ah! Petite m\u00e8re a allum\u00e9 sa lampe\\\" (Yniold, Golaud)", "\u00c9tudes, L. 136: I. Pour les \u00ab cinq doigts \u00bb d'apr\u00e8s monsieur Czerny", "Nuit sans fin", "Le Martyre de Saint-S\u00e9bastien, L. 124: Prologue", "Pell\u00e9as et M\u00e9lisande: Act III, Scene I. Une des tours du ch\u00e2teau", "Images, Livre 1, L. 110: No. 3. Mouvement", "Pr\u00e9ludes, Livre II, L. 123: IV. \u00abLes f\u00e9es sont d'exquises danseuses\u00bb. Rapide et l\u00e9ger", "Preludes, Book I: 6. Des pas sur la neige", "Pr\u00e9ludes, Livre II, L. 123: IX. Hommage \u00e0 S. Pickwick Esq. P.P.M.P.C.. Grave", "Children's Corner, L. 113: II. Jimbo's Lullaby", "Khamma: Sc\u00e8ne 1. Le Temple int\u00e9rieur du Grand-Dieu Amun-Ra \u2013", "Spleen", "Sim\u00e9on's Recitative and Aria", "Preludes, Book II: 3. La Puerta del Vino", "Les Comparaisons", "\u00c9tudes, L. 136: IV. Pour les sixtes", "Le Martyre de Saint-S\u00e9bastien, fragments symphoniques: III. La Passion", "Premi\u00e8re Suite d\u2019orchestre, L. 50, CD 46 : I. F\u00eate", "Harmonie du soir", "Bilitis: IV. Pour la danseuse aux crotales", "Lindaraja", "Children's Corner - I. Doctor Gradus ad Parnassum", "La Bo\u00eete \u00e0 joujoux : VI. \u00c9pilogue", "Mandoline, L. 29, CD 43, m\u00e9lodie pour voix et piano \\\"Les donneurs de s\u00e9r\u00e9nades\\\"", "Les Courtisanes \u00e9gyptiennes", "Cinq po\u00e8mes de Baudelaire, L. 64, CD 70: V. La Mort des amants", "Three Preludes: Feux d'artifice", "Sonata for flute, viola and harp", "Premi\u00e8re Suite d\u2019orchestre, L. 50, CD 46 : II. Ballet", "\u00c9tudes", "Cello sonata in D minor: I. Prologue", "Dieu! qu'il la fait bon regarder!", "Fantaisie pour piano et orchestre, L. 73, CD 72", "En blanc et noir: III. Scherzando", "Ariettes oubli\u00e9es, L. 60, CD 63a : V. Aquarelles, 1. Green, m\u00e9lodie pour voix et piano \\\"Voici des fruits, des fleurs, des feuilles\\\"", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene II. \\\"Quel est ce bruit? On ferme les portes\\\" (Pell\u00e9as, M\u00e9lisande)", "Pour ce que Plaisance est morte", "Preludes, Book I: 1. Danseuses de Delphes", "La Partie d'osselets", "Pagodes from Estampes", "Le Martyre de Saint-S\u00e9bastien, L. 124: Acte III \\\"Le Concile des faux dieux\\\". Fanfare no. 2", "Regret, L. 55, CD 59, m\u00e9lodie pour voix et piano \\\"Devant le ciel d\u2019\u00e9t\u00e9, ti\u00e8de et calme\\\"", "L\u2019Enfant prodigue, L. 57, CD 61, sc\u00e8ne lyrique \\\"L\u2019ann\u00e9e, en vain chasse l\u2019ann\u00e9e\\\"", "De r\u00eave", "Le son du cor s'afflige", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene II. \\\"C'est le dernier soir ...\\\" (Pell\u00e9as, M\u00e9lisande)", "Musique", "\u590f\u306e\u98a8\u306e\u795e\uff08\u300c\uff16\u3064\u306e\u53e4\u4ee3\u5893\u7891\u540d\u300d\u3088\u308a\uff09", "Trio in G Major for Violin, Cello and Piano: II. Scherzo - Intermezzo. Moderato con allegro", "Nuit d'\u00c9toiles", "Cinq po\u00e8mes de Baudelaire, L. 64, CD 70: IV. Recueillement", "F\u00eates Galantes I", "Printemps, L. 61, CD 68b : II. Mod\u00e9r\u00e9", "Nocturne, L. 82, CD 89, pour piano", "\u00c9tude retrouv\u00e9e", "\u00c9tudes, L. 136: II. Pour les tierces", "Pell\u00e9as et M\u00e9lisande: Act III, Scene II. \\\"Prenez garde; par ici, par ici\\\" (Golaud, Pell\u00e9as)", "La Belle au Bois dormant", "Petite Suite, L. 65, CD 71a, pour piano \u00e0 4 mains : IV. Ballet", "Suite bergamasque, L. 75, CD 82 : III. Clair de lune, pour violon et piano", "Les Chansons de Bilitis, L. 96: No. 3. Les Contes", "Star Gazers Theme Song", "Lia's Recitative and Aria", "Le Martyre de Saint-S\u00e9bastien: III. Le concile des faux dieux: N\u00b02", "Le balcon", "Ariettes oubli\u00e9es, L. 60, CD 63a : III. Le rossignol qui, du haut d\u2019une branche, m\u00e9lodie pour voix et piano \\\"L\u2019ombre des arbres dans la rivi\u00e8re embrum\u00e9e\\\"", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene I. \\\"Apporte-la\\\" (Golaud, Arkel, M\u00e9lisande)", "Pell\u00e9as et M\u00e9lisande - Concert Suite: Acte II, sc\u00e8ne 1. Une fontaine dans le parc", "N\u00b0 5: The Film: Clair de Lune", "\u00c9tudes, L. 136: XI. Pour les arp\u00e8ges compos\u00e9s", "Le Martyre de Saint-S\u00e9bastien: II. La chambre magique: N\u00b02", "Pell\u00e9as et M\u00e9lisande: Act III, Scene I. \\\"Je les noue, je les noue aux branches du saule\\\" (Pell\u00e9as, M\u00e9lisande, Golaud)", "Premi\u00e8re Suite d\u2019orchestre, L. 50, CD 46 : IV. Bacchanale (Cort\u00e8ge et Bacchanale)", "3 Preludes from Book II: V. Bruyer\u00e8s. Calme", "La grotte", "Rapsodie pour orchestre et saxophone", "Bilitis: III. Pour que la nuit soit propice", "Pell\u00e9as et M\u00e9lisande: Act II, Scene II. \\\"Tiens, o\u00f9 est l'anneau que je t'avais donn\u00e9?\\\" (Golaud, M\u00e9lisande)", "Fantaisie pour piano et orchestre, L. 73, CD 72 : I. Andante - Allegro", "Musique pour \u201cLe Roi Lear\u201d, L. 107: Fanfare d\u2019ouverture", "\u00c9tudes, L. 136: VIII. Pour les agr\u00e9ments", "L'eau pure du bassin", "Petite Suite, L. 65, CD 71, pour violon et piano : III. Menuet", "Trois Chansons de France, L. 102: II. La Grotte: Aupr\u00e8s de cette grotte sombre", "Sonata for Violin and Piano in G Minor, L 140: II. Interm\u00e8de: fantasque et l\u00e9ger", "La chevelure", "Ce qu'a vu le vent d'ouest", "Le Martyre de Saint-S\u00e9bastien: IV. Le laurier bless\u00e9: N\u00b02", "Quatuor \u00e0 cordes en sol mineur, op. 10, L. 85, CD 91 : I. Anim\u00e9 et tr\u00e8s d\u00e9cid\u00e9", "Le Martyre de Saint-S\u00e9bastien: III. Le concile des faux dieux: N\u00b04", "Trois Po\u00e8mes de St\u00e9phane Mallarm\u00e9", "Engulfed Cathedral (Debussy)", "Le jet d'eau", "Pell\u00e9as et M\u00e9lisande: Act II, Scene II. \\\"Je suis ... je suis malade ici\\\" (M\u00e9lisande, Golaud)", "L'ombre des arbres", "La Belle au bois dormant, L. 74, CD 81, m\u00e9lodie \\\"Des trous \u00e0 son pourpoint vermeil\\\"", "Le promenoir des deux amants, L. 118, CD 129, m\u00e9lodies pour voix et piano : II. \\\"Crois mon conseil, ch\u00e8re Clim\u00e8ne\\\"", "Il pleure dans mon c\u0153ur", "Le Tombeau sans nom", "Sonata for Violin and Piano in G Minor, L 140: I. Allegro vivo", "Prelude to the Afternoon of a Faun", "Petite Suite, L. 65, CD 71b, pour orchestre : IV. Ballet", "Ariettes oubli\u00e9es, L. 60, CD 63a : IV. Paysages belges. Chevaux de bois, m\u00e9lodie pour voix et piano \\\"Tournez, tournez, bons chevaux de bois\\\"", "Pell\u00e9as et M\u00e9lisande: Act I, Scene I. \\\"Qu'est-ce qui brille ainsi, au fond de l'eau?\\\" (Golaud, M\u00e9lisande)", "Rhapsodie pour saxophone et orchestre, L. 98", "Fantoches", "Deux arabesques", "Preludes, Book II: 8. Ondine", "Jardins sous la pluie", "S\u00e9r\u00e9nade", "Pell\u00e9as et M\u00e9lisande: Act III, Scene III. \\\"Ah! Je respire enfin!\\\" (Pell\u00e9as, Golaud)", "Paysage sentimental, L 45, CD 55, m\u00e9lodie pour voix et piano \\\"Le ciel d\u2019hiver si doux, si triste, si dormant\\\"", "Deux Romances", "La chute de la maison Usher", "Preludes, Book II: 12. Feux d'artifice", "Printemps, L. 61, CD 68b, suite symphonique en mi majeur pour 2 pianos et orchestre", "Nocturnes, L. 91, CD 98: III. Sir\u00e8nes", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene I. \\\"Maintenant que le p\u00e8re de Pell\u00e9as est sauv\u00e9 ...\\\" (Arkel, M\u00e9lisande)", "Bilitis", "Le Martyre de Saint-S\u00e9bastien: II. La chambre magique: N\u00b03", "Le Martyre de Saint-S\u00e9bastien: V. Le paradis: N\u00b02", "Images pour orchestre", "La Romance d'Ariel", "Chansons de Bilitis, L. 90: La Fl\u00fbte de Pan \u00abPour le jour des Hyacinthies\u00bb", "Six \u00e9pigraphes antiques, L. 131: V. Pour l'\u00c9gyptienne", "Prelude No. 8: The Girl with the Flaxen Hair", "Preludes, Book I: 2. Voiles", "Estampes, L. 100: III. Jardins sous la pluie. Net et vif", "Pell\u00e9as et M\u00e9lisande - Concert Suite: Acte V. Une chambre dans le ch\u00e2teau", "Three Preludes: Ce qu'a vu le vent de l'ouest", "Bilitis: I. Pour invoquer Pan, dieu du vent d'\u00e9t\u00e9", "Le Martyre de saint S\u00e9bastien", "Pell\u00e9as et M\u00e9lisande: Act II, Scene I. \\\"Vous ne savez pas o\u00f9 je vous ai men\u00e9e?\\\" (Pell\u00e9as, M\u00e9lisande)", "Preludes, Book II: 7. La Terrasse des audiences du clair de lune", "Fleur des Bl\u00e9s", "Coquetterie posthume", "Pour le piano: II. Sarabande", "Duo", "Pell\u00e9as et M\u00e9lisande: Act III, Scene I. \\\"Mes longs cheveux descendent\\\"", "Preludes, Book II: 6. General Lavine - Eccentric", "Tarantelle styrienne, L. 69, CD 77a, (Danse) pour piano", "Le promenoir des deux amants, L. 118, CD 129, m\u00e9lodies pour voix et piano : III. \\\"Je tremble en voyant ton visage\\\"", "Pour le piano, L. 95: I. Pr\u00e9lude. \u00c0 Mademoiselle M.W. de Romilly. Assez anim\u00e9 et tr\u00e8s rythm\u00e9 - Tempo di cadenza - Tempo I", "Il dort encore", "Pr\u00e9ludes", "Preludes, Book II: 1. Brouillards", "Trois chansons de Charles d'Orl\u00e9ans, pour ch\u0153ur \u00e0 quatre voix mixtes, L. 92, CD 99: II. \\\"Quand j'ay ouy le tabourin\\\"", "F\u00eate galante, L. 23, CD 31, m\u00e9lodie pour voix et piano \\\"Voil\u00e0 Sylandre et Lycas et Myrtil\\\"", "Rondel chinois", "Le Martyre de Saint-S\u00e9bastien: I. La cour des lys: N\u00b01", "Chanson", "F\u00eate galante", "Trag\u00e9die", "Trois Ballades de Fran\u00e7ois Villon, L. 119: II. Ballade que Villon feit \u00e0 la requeste de sa m\u00e8re pour prier Nostre-Dame", "Petite Suite, L. 65, CD 71a, pour piano \u00e0 4 mains : III. Menuet", "Flots, palmes, sables, L. 25, CD 38, m\u00e9lodie pour voix et piano (ou harpe) \\\"Loin des yeux du monde\\\"", "Images I: II. Hommages Rameau", "Sonate pour violon et piano en sol mineur, L. 140", "Proses lyriques, L. 84, CD 90 : II. De Gr\u00e8ve, m\u00e9lodie \\\"Sur la mer les cr\u00e9puscules tombent\\\"", "Rondel chinois, L. 17, CD 11, m\u00e9lodie pour voix et piano \\\"Sur le lac bord\u00e9 d\u2019azal\u00e9e\\\"", "Children's Corner, L. 113 No. 5: The Little Shepherd", "Le Martyre de Saint-S\u00e9bastien: IV. Le laurier bless\u00e9: N\u00b03", "Calme dans le demi-jour", "Ballade que Villon feit \u00e0 la requeste de sa m\u00e8re pour prier Nostre-Dame", "Trois Chansons de France, L. 102, CD 115: I. Rondel: Le temps a laiss\u00e9 son manteau", "Le Souvenir de Mnasidika", "\u00c9tudes, L. 136: IX. Pour les notes r\u00e9p\u00e9t\u00e9es", "Petite Suite, L. 65: I. En bateau", "Fantaisie pour piano et orchestre, L. 73, CD 72 : III. Allegro molto", "Chanson triste", "Les Chansons de Bilitis, L. 96: No. 11. Le Souvenir de Mnasidika", "Hommage \u00e0 Joseph Haydn, L. 115", "Les Chansons de Bilitis, L. 96: No. 7. Le Tombeau sans nom", "Pierrot", "Estampes - I. Pagodes", "Le Martyre de Saint-S\u00e9bastien, fragments symphoniques: II. Danse extatique et final du premier acte", "Pell\u00e9as et M\u00e9lisande: Act V. \\\"Attention... attention\\\" (Arkel, Le m\u00e9decin)", "Danse profane", "Aupr\u00e8s de cette grotte sombre", "The Tears of Billie Blue", "Khamma: Premi\u00e8re Danse. Grave et lent \u2013 Animez \u2013 Revenez au Mouvement \u2013 Plus lent \u2013 Animez peu \u00e0 peu \u2013", "De soir", "Pell\u00e9as et M\u00e9lisande - Concert Suite: Acte III, sc\u00e8ne 2. Les souterrains du ch\u00e2teau / Acte IV, sc\u00e8ne 2. Un appartement dans le ch\u00e2teau", "Six \u00e9pigraphes antiques, L. 131: III. Pour que la nuit soit propice", "Le Martyre de Saint-S\u00e9bastien: III. Le concile des faux dieux: N\u00b07", "D'un cahier d'esquisses, L. 99", "Dialogue of the Wind and the Sea", "Romance (Deux Romances, No. 2, 1891)", "Berceuse h\u00e9ro\u00efque pour orchestre, L. 132", "Chansons de Bilitis, L. 90: \u00abLa Chevelure \u00abIl m'a dit \u00abCette nuit d'ai r\u00eav\u00e9\u00bb\u00bb", "Mazurka, L. 67, CD 75, pour piano", "Images pour orchestre, L 122: III. Rondes de printemps", "Pell\u00e9as et M\u00e9lisande: Act II, Scene I. \\\"C'est au bord d'une fontaine aussi qu'il vous a trouv\u00e9e?\\\" (Pell\u00e9as, M\u00e9lisande)", "Deux arabesques, L. 66, CD 74, pour piano : No. 2 en sol majeur, Allegretto scherzando", "Six \u00e9pigraphes antiques, L. 131: II. Pour un tombeau sans nom", "Pr\u00e9ludes, Livre 1, L. 117 No. 9: La s\u00e9r\u00e9nade interrompue", "Pr\u00e9ludes, Livre II, L. 123: XI. Les tierces altern\u00e9es. Mod\u00e9r\u00e9ment anim\u00e9", "Apparition", "Chanson espagnole", "Mes longs cheveux", "Carry", "Pell\u00e9as et M\u00e9lisande: Act V. \\\"M\u00e9lisande...M\u00e9lisande ...\\\" - \\\"Est-ce vous, Golaud?\\\" (Golaud, M\u00e9lisande)", "Le temps a laiss\u00e9 son manteau", "Trois M\u00e9lodies, L. 81, CD 85, pour une voix avec accompagnement de piano : I. La mer est plus belle que les cath\u00e9drales.", "La Danseuse aux crotales", "F\u00eates Galantes II", "La damoiselle \u00e9lue", "Lorsqu'elle est entr\u00e9e", "Coquetterie posthume, L. 39, CD 50, m\u00e9lodie pour voix et piano \\\"Quand je mourrai, que l\u2019on me mette\\\"", "Les Chansons de Bilitis, L. 96: No. 2. Les Comparaisons", "Children's Corner, L. 113: III. Serenade for the Doll", "Preludes, Book I: 11. La Danse de Puck", "Marche \u00e9cossaise sur un th\u00e8me populaire, L. 77, CD 83b (Marche des anciens comtes de Ross, d\u00e9di\u00e9e \u00e0 leur descendant le G\u00e9n\u00e9ral Meredith Reid, grand-croix de l'ordre royal du R\u00e9dempteur)", "Le Martyre de Saint-S\u00e9bastien: II. La chambre magique: N\u00b01", "Hommage \u00e0 S. Pickwick Esq. P.P.M.P.C.", "Rondeau: 'Fut-il jamais'", "Voiles", "La plus que lente", "Pell\u00e9as et M\u00e9lisande: Act II, Scene III. \\\"Oui, c'est ici, nous y sommes\\\" (Pell\u00e9as, M\u00e9lisande)", "Proses lyriques, L. 84, CD 90 : I. De R\u00eave, m\u00e9lodie \\\"La nuit a des douceurs de femme\\\"", "Children's Corner, L. 113: V. The Little Shepherd", "Fleurs des bl\u00e9s, L. 7, CD 16, m\u00e9lodie pour voix et piano \\\"Le long des bl\u00e9s que la brise fait onduler\\\"", "Souhait", "Pell\u00e9as et M\u00e9lisande: Act V. \\\"Qu'y-a-t-il? Qu'est-ce que toutes ces femmes viennent faire ici?\\\" (Golaud, Le m\u00e9decin, Arkel)", "String Quartet", "Iberia No. 2 from \\\"Images\\\" for Orchestra: I. Through the Streets and Roads", "Quatuor \u00e0 cordes en sol mineur, op. 10, L. 85, CD 91 : IV. Tr\u00e8s mod\u00e9r\u00e9", "Pell\u00e9as et M\u00e9lisande: Act V. \\\"Ce n'est pas de cette petite blessure qu'elle peut mourir ...\\\" (Le m\u00e9decin, Arkel, Golaud)", "Trois Ballades de Fran\u00e7ois Villon, L. 119: III. Ballade des femmes de Paris", "Berceuse h\u00e9ro\u00efque pour piano, L. 132", "Khamma, L 125", "Printemps, L. 61, CD 68b : I. Tr\u00e8s mod\u00e9r\u00e9", "Khamma: Deuxi\u00e8me Danse. Assez anim\u00e9 \u2013 Plus anim\u00e9 peu \u00e0 peu \u2013 Tr\u00e8s anim\u00e9 \u2013", "De gr\u00e8ve", "Le Martyre de Saint-S\u00e9bastien: V. Le paradis: N\u00b01", "Pell\u00e9as et M\u00e9lisande: Act II, Scene II. \\\"Ah! Ah! Tout va bien, ce la ne sera rien\\\" (Golaud, M\u00e9lisande)", "En blanc et noir: II. Lent, sombre", "Trio in G Major for Violin, Cello and Piano: I. Andantino con moto allegro", "Fantaisie pour piano et orchestre, L. 73, CD 72 : II. Lento e molto espressivo", "Six \u00e9pigraphes antiques, L. 131: VI. Pour remercier la pluie au matin", "\u00c9tudes, L. 136: III. Pour les quartes", "Pell\u00e9as et M\u00e9lisande: Act III, Scene I. \\\"Oh! Oh! Mes cheveux descendent de la tour!\\\" (M\u00e9lisande, Pell\u00e9as)", "Recueillement", "En sourdine", "Images pour orchestre, L 122: II. Iberia", "Pell\u00e9as et M\u00e9lisande", "Les Chansons de Bilitis, L. 96: No. 10. La Danseuse aux crotales", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene II. \\\"On dirait que ta voix a pass\u00e9 sur la mer au printemps!\\\" (Choeur, M\u00e9lisande, Pell\u00e9as, Genevi\u00e8ve)", "Trois Chansons de France", "Trois Ballades de Fran\u00e7ois Villon, L. 119: I. Ballade de Villon \u00e0 s'Amye", "Nocturnes", "La Fl\u00fbte de Pan", "Pell\u00e9as et M\u00e9lisande: Act V. \\\"Ouvrez la fen\u00eatre, ouvrez la fen\u00eatre ...\\\" (M\u00e9lisande, Arkel, Le m\u00e9decin)", "Les Elfes, CD 25, m\u00e9lodie pour voix et piano \\\"Du sentier des bois aux daims familiers\\\"", "Pell\u00e9as et M\u00e9lisande: Act I, Scene II. \\\"Je n'en dis rien\\\" (Arkel, Genevi\u00e8ve)", "Le diable dans le beffroi"], "ans_acc": 0.003952569169960474, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.08888888888888888, "path_precision": 0.2, "path_recall": 0.05714285714285714, "path_ans_f1": 0.007751937984496123, "path_ans_precision": 0.2, "path_ans_recall": 0.003952569169960474}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Anne Harris\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Ebenezer Franklin\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.place_of_birth -> Ecton\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Elizabeth Douse\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.spouse_s -> m.0j4k0mt\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger -> people.person.parents -> Mary Morrill\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger -> people.person.parents -> Peter Folger\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger -> people.deceased_person.place_of_burial -> Granary Burying Ground\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nm.012zbkk5", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.spouse_s -> m.0j4k0ll\n# Answer:\nJosiah Franklin"], "ground_truth": ["Deborah Read"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-740", "prediction": ["# Reasoning Path:\nBarney & Friends -> tv.tv_program.regular_cast -> m.0bgyvq1 -> tv.regular_tv_appearance.character -> Gianna\n# Answer:\nm.0bgyvq1", "# Reasoning Path:\nSelena Gomez -> tv.tv_actor.starring_roles -> m.0bgyvq1 -> tv.regular_tv_appearance.character -> Gianna\n# Answer:\nm.0bgyvq1", "# Reasoning Path:\nSelena Gomez -> tv.tv_actor.starring_roles -> g.11b7vzwxql\n# Answer:\ng.11b7vzwxql", "# Reasoning Path:\nSelena Gomez -> tv.tv_actor.starring_roles -> m.0bgyvq1 -> tv.regular_tv_appearance.series -> Barney & Friends\n# Answer:\nm.0bgyvq1", "# Reasoning Path:\nBarney & Friends -> tv.tv_program.regular_cast -> m.0bng6c0 -> tv.regular_tv_appearance.character -> Barney\n# Answer:\nm.0bng6c0", "# Reasoning Path:\nBarney & Friends -> tv.tv_program.regular_cast -> m.0bgyvq1 -> tv.regular_tv_appearance.actor -> Selena Gomez\n# Answer:\nm.0bgyvq1", "# Reasoning Path:\nBarney & Friends -> tv.tv_program.regular_cast -> m.0bw2qbm -> tv.regular_tv_appearance.character -> Linda\n# Answer:\nm.0bw2qbm", "# Reasoning Path:\nSelena Gomez -> tv.tv_actor.starring_roles -> m.03lyrfq -> tv.regular_tv_appearance.character -> Alex Russo\n# Answer:\nm.03lyrfq", "# Reasoning Path:\nBarney & Friends -> award.award_nominated_work.award_nominations -> m.0m2fbpv -> award.award_nomination.award -> Daytime Emmy Award for Outstanding Achievement in Costume Design/Styling\n# Answer:\nm.0m2fbpv", "# Reasoning Path:\nBarney & Friends -> tv.tv_program.regular_cast -> m.0bng6c0 -> tv.regular_tv_appearance.actor -> Bob West\n# Answer:\nm.0bng6c0"], "ground_truth": ["Gianna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14814814814814817, "path_precision": 0.2, "path_recall": 0.11764705882352941, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-741", "prediction": ["# Reasoning Path:\nThe Hunger Games -> film.film.starring -> m.0gw7kw1 -> film.performance.actor -> Stanley Tucci\n# Answer:\nm.0gw7kw1", "# Reasoning Path:\nThe Hunger Games -> film.film.starring -> m.0gw7kv0 -> film.performance.actor -> Jennifer Lawrence\n# Answer:\nm.0gw7kv0", "# Reasoning Path:\nThe Hunger Games -> film.film.starring -> m.0gw7kw1 -> film.performance.character -> Caesar Flickerman\n# Answer:\nm.0gw7kw1", "# Reasoning Path:\nThe Hunger Games -> film.film.starring -> m.0gw7kv7 -> film.performance.actor -> Liam Hemsworth\n# Answer:\nm.0gw7kv7", "# Reasoning Path:\nThe Hunger Games -> film.film.starring -> m.0gw7kv0 -> film.performance.character -> Katniss Everdeen\n# Answer:\nm.0gw7kv0", "# Reasoning Path:\nThe Hunger Games -> film.film.starring -> m.0gw7kv7 -> film.performance.character -> Gale Hawthorne\n# Answer:\nm.0gw7kv7", "# Reasoning Path:\nThe Hunger Games -> book.book.characters -> Peeta Mellark -> film.film_character.portrayed_in_films -> m.0gw7kvp\n# Answer:\nPeeta Mellark", "# Reasoning Path:\nThe Hunger Games -> book.book.characters -> Peeta Mellark -> fictional_universe.fictional_character.character_created_by -> Suzanne Collins\n# Answer:\nPeeta Mellark", "# Reasoning Path:\nThe Hunger Games -> book.book.characters -> Haymitch Abernathy -> film.film_character.portrayed_in_films -> m.0gw7kvg\n# Answer:\nHaymitch Abernathy", "# Reasoning Path:\nThe Hunger Games -> book.book.characters -> Haymitch Abernathy -> book.book_character.appears_in_book -> Catching Fire\n# Answer:\nHaymitch Abernathy"], "ground_truth": ["Stanley Tucci"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-742", "prediction": ["# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> location.location.containedby -> Adams County\n# Answer:\nGettysburg", "# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> travel.travel_destination.tourist_attractions -> Gettysburg National Cemetery\n# Answer:\nGettysburg", "# Reasoning Path:\nGettysburg Address -> common.topic.notable_for -> g.1255f923w\n# Answer:\ng.1255f923w", "# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> location.location.containedby -> Pennsylvania\n# Answer:\nGettysburg", "# Reasoning Path:\nGettysburg Address -> common.topic.article -> m.037z4\n# Answer:\nm.037z4", "# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> location.location.contains -> Gettysburg National Cemetery\n# Answer:\nGettysburg", "# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> travel.travel_destination.tourist_attractions -> American Civil War Museum\n# Answer:\nGettysburg", "# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> travel.travel_destination.tourist_attractions -> Eisenhower National Historic Site\n# Answer:\nGettysburg", "# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> location.location.contains -> Dobbin House Tavern\n# Answer:\nGettysburg", "# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> location.location.contains -> Eisenhower National Historic Site\n# Answer:\nGettysburg"], "ground_truth": ["Gettysburg"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-743", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> people.person.sibling_s -> m.02_wjwf -> people.sibling_relationship.sibling -> Ted Kennedy\n# Answer:\nm.02_wjwf", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.sibling_s -> m.03hs9fc -> people.sibling_relationship.sibling -> Robert F. Kennedy\n# Answer:\nm.03hs9fc", "# Reasoning Path:\nJohn F. Kennedy -> people.person.sibling_s -> m.03lfpfx -> people.sibling_relationship.sibling -> Eunice Kennedy Shriver\n# Answer:\nm.03lfpfx", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> freebase.valuenotation.has_no_value -> Children\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Caroline Kennedy -> people.person.sibling_s -> m.07ydm_0\n# Answer:\nCaroline Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Caroline Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nCaroline Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.nationality -> United States of America\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> John F. Kennedy Jr. -> people.person.sibling_s -> m.07ydm_0\n# Answer:\nJohn F. Kennedy Jr.", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School -> education.educational_institution.students_graduates -> m.0n0tr1k\n# Answer:\nBloomington Kennedy High School"], "ground_truth": ["Ted Kennedy", "Robert F. Kennedy", "Joseph P. Kennedy, Jr."], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666666, "path_precision": 0.2, "path_recall": 0.14285714285714285, "path_ans_f1": 0.30769230769230765, "path_ans_precision": 0.2, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-744", "prediction": ["# Reasoning Path:\nTupac Shakur -> music.artist.label -> Interscope Records -> common.topic.notable_types -> Record label\n# Answer:\nInterscope Records", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Amaru Entertainment -> music.record_label.artist -> Outlawz\n# Answer:\nAmaru Entertainment", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Death Row Records -> music.record_label.artist -> Bad Azz\n# Answer:\nDeath Row Records", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Death Row Records -> common.topic.notable_types -> Record label\n# Answer:\nDeath Row Records", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Amaru Entertainment -> organization.organization.founders -> Afeni Shakur\n# Answer:\nAmaru Entertainment", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Amaru Entertainment -> music.record_label.artist -> Thug Life\n# Answer:\nAmaru Entertainment", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Death Row Records -> music.record_label.artist -> Big Syke\n# Answer:\nDeath Row Records", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Death Row Records -> music.record_label.artist -> Bow Wow\n# Answer:\nDeath Row Records", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Death Row Records -> organization.organization.founders -> Dr. Dre\n# Answer:\nDeath Row Records", "# Reasoning Path:\nTupac Shakur -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality"], "ground_truth": ["Amaru Entertainment", "Interscope Records", "Death Row Records"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-745", "prediction": ["# Reasoning Path:\nAugustus -> people.person.profession -> Roman emperor -> people.profession.people_with_this_profession -> Antoninus Pius\n# Answer:\nRoman emperor", "# Reasoning Path:\nAugustus -> people.person.profession -> Roman emperor -> common.topic.article -> m.01g152\n# Answer:\nRoman emperor", "# Reasoning Path:\nAugustus -> people.person.profession -> Roman emperor -> common.topic.notable_types -> Character Occupation\n# Answer:\nRoman emperor", "# Reasoning Path:\nAugustus -> people.person.profession -> Roman emperor -> people.profession.people_with_this_profession -> Caligula\n# Answer:\nRoman emperor", "# Reasoning Path:\nAugustus -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdn_\n# Answer:\nPolitician", "# Reasoning Path:\nAugustus -> people.person.profession -> Roman emperor -> people.profession.people_with_this_profession -> Caracalla\n# Answer:\nRoman emperor", "# Reasoning Path:\nAugustus -> people.person.profession -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nAugustus -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdq3\n# Answer:\nPolitician", "# Reasoning Path:\nAugustus -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101h_l2\n# Answer:\nPolitician", "# Reasoning Path:\nAugustus -> people.person.profession -> Politician -> people.profession.specializations -> Politician & Trade Unionist\n# Answer:\nPolitician"], "ground_truth": ["Politician", "Roman emperor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-747", "prediction": ["# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Belize\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Chile\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Achawa language -> language.human_language.language_family -> Macro-Arawakan languages\n# Answer:\nAchawa language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> Belize\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.statistical_region.gdp_nominal_per_capita -> g.11b60ywwvv\n# Answer:\ng.11b60ywwvv", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Cuba\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language"], "ground_truth": ["Ponares Language", "Yucuna Language", "Macaguaje Language", "Ticuna language", "Carijona Language", "Andaqui Language", "Desano Language", "Inga Language", "Coyaima Language", "Tunebo, Western Language", "Guahibo language", "Cocama language", "Macagu\u00e1n Language", "Guayabero Language", "Anserma Language", "Tomedes Language", "Romani, Vlax Language", "Hupd\u00eb Language", "Ocaina Language", "Piapoco Language", "Providencia Sign Language", "Catio language", "Quechua, Napo Lowland Language", "Natagaimas Language", "Tanimuca-Retuar\u00e3 Language", "Cagua Language", "P\u00e1ez language", "Kogi Language", "Tinigua language", "Malayo Language", "Kuna, Border Language", "Piratapuyo Language", "Puinave Language", "Baudo language", "Siona Language", "Tuyuca language", "Tunebo, Barro Negro Language", "Curripaco Language", "Waimaj\u00e3 Language", "Guambiano Language", "Cubeo Language", "Macuna Language", "Palenquero Language", "Chipiajes Language", "Guanano Language", "Totoro Language", "Spanish Language", "Tunebo, Angosturas Language", "Minica Huitoto", "Coxima Language", "Piaroa Language", "Bora Language", "Bar\u00ed Language", "Cumeral Language", "Runa Language", "Barasana Language", "Cof\u00e1n Language", "Muinane Language", "Inga, Jungle Language", "Yukpa Language", "Andoque Language", "Islander Creole English", "Murui Huitoto language", "Nukak language", "Carabayo Language", "Arhuaco Language", "Playero language", "Nheengatu language", "Tunebo, Central Language", "Koreguaje Language", "Cuiba language", "Colombian Sign Language", "Tama Language", "Tucano Language", "Nonuya language", "Wayuu Language", "Cams\u00e1 Language", "Omejes Language", "Siriano Language", "Achawa language", "Ember\u00e1, Northern Language", "Cabiyar\u00ed Language", "Pijao Language", "S\u00e1liba Language", "Awa-Cuaiquer Language", "Uwa language"], "ans_acc": 0.023255813953488372, "ans_hit": 1, "ans_f1": 0.04534005037783375, "ans_precission": 0.9, "ans_recall": 0.023255813953488372, "path_f1": 0.045283018867924525, "path_precision": 0.6, "path_recall": 0.023529411764705882, "path_ans_f1": 0.04534005037783375, "path_ans_precision": 0.9, "path_ans_recall": 0.023255813953488372}
{"id": "WebQTest-748", "prediction": ["# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12301 -> common.topic.notable_for -> g.125fmlxn3\n# Answer:\n12301", "# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12301 -> location.postal_code.country -> United States of America\n# Answer:\n12301", "# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12304 -> location.location.geometry -> m.058lw2n\n# Answer:\n12304", "# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12301 -> common.topic.notable_types -> Postal Code\n# Answer:\n12301", "# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12308 -> location.location.geolocation -> m.03dv207\n# Answer:\n12308", "# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12304 -> common.topic.notable_types -> Postal Code\n# Answer:\n12304", "# Reasoning Path:\nSchenectady -> location.statistical_region.population -> g.11b66fk06n\n# Answer:\ng.11b66fk06n", "# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12304 -> location.location.containedby -> Schenectady County\n# Answer:\n12304", "# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12308 -> location.location.contains -> Achilles Rink\n# Answer:\n12308", "# Reasoning Path:\nSchenectady -> location.location.contains -> Bayou Cafe -> common.topic.notable_for -> g.125bv1wbt\n# Answer:\nBayou Cafe"], "ground_truth": ["12302", "12325", "12309", "12306", "12308", "12301", "12307", "12303", "12305", "12345", "12304"], "ans_acc": 0.2727272727272727, "ans_hit": 1, "ans_f1": 0.4067796610169491, "ans_precission": 0.8, "ans_recall": 0.2727272727272727, "path_f1": 0.4067796610169491, "path_precision": 0.8, "path_recall": 0.2727272727272727, "path_ans_f1": 0.4067796610169491, "path_ans_precision": 0.8, "path_ans_recall": 0.2727272727272727}
{"id": "WebQTest-749", "prediction": ["# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx817 -> government.government_position_held.office_holder -> Harry S. Truman\n# Answer:\nm.03fx817", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03q2h_9 -> government.government_position_held.office_holder -> Franklin D. Roosevelt\n# Answer:\nm.03q2h_9", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx8cf -> government.government_position_held.office_holder -> Richard Nixon\n# Answer:\nm.03fx8cf", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx817 -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nm.03fx817", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03q2h_9 -> government.government_position_held.basic_title -> President\n# Answer:\nm.03q2h_9", "# Reasoning Path:\nPresident of the United States -> freebase.equivalent_topic.equivalent_type -> US President -> freebase.type_profile.kind -> Title\n# Answer:\nUS President", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx8cf -> government.government_position_held.basic_title -> President\n# Answer:\nm.03fx8cf", "# Reasoning Path:\nPresident of the United States -> freebase.equivalent_topic.equivalent_type -> US President -> type.type.expected_by -> President\n# Answer:\nUS President", "# Reasoning Path:\nPresident of the United States -> freebase.equivalent_topic.equivalent_type -> US President -> freebase.type_profile.kind -> Significant\n# Answer:\nUS President", "# Reasoning Path:\nPresident of the United States -> freebase.equivalent_topic.equivalent_type -> US President -> type.type.properties -> President number\n# Answer:\nUS President"], "ground_truth": ["Ronald Reagan"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Gingivitis\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> African American\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9g\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Chronic pancreatitis\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.risk_factors -> Gingivitis\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9q\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.treatments -> Pancreatectomy\n# Answer:\nPancreatic cancer"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-751", "prediction": ["# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> organization.organization.founders -> Larry Page\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> internet.website_owner.websites_owned_new -> m.010g4by1 -> internet.website_ownership.website -> Like.com\n# Answer:\nm.010g4by1", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> AdSense -> common.topic.subject_of -> David Iwanow\n# Answer:\nAdSense", "# Reasoning Path:\nGoogle -> internet.website_owner.websites_owned_new -> m.0dc4zx1 -> internet.website_ownership.website -> Blogger\n# Answer:\nm.0dc4zx1", "# Reasoning Path:\nGoogle -> business.consumer_company.products -> m.010w_v_l -> freebase.valuenotation.has_no_value -> To date\n# Answer:\nm.010w_v_l", "# Reasoning Path:\nGoogle -> internet.website_owner.websites_owned_new -> m.0dc4zwr -> internet.website_ownership.website -> FeedBurner\n# Answer:\nm.0dc4zwr", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> common.topic.subject_of -> Google Chrome\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> AdSense -> common.topic.notable_types -> Software\n# Answer:\nAdSense", "# Reasoning Path:\nGoogle -> business.consumer_company.products -> Google Buzz -> common.topic.article -> m.0b2f4ws\n# Answer:\nGoogle Buzz", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> common.topic.subject_of -> Google Docs, Sheets, and Slides\n# Answer:\nGoogle Apps for Work"], "ground_truth": ["Vic Gundotra", "Patrick Pichette", "David Drummond", "Larry Page", "Nikesh Arora"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.13333333333333333, "path_ans_precision": 0.1, "path_ans_recall": 0.2}
{"id": "WebQTest-752", "prediction": ["# Reasoning Path:\nJohn Pemberton -> law.inventor.inventions -> Coca-Cola -> food.food.nutrients -> m.07xb760\n# Answer:\nCoca-Cola", "# Reasoning Path:\nJohn Pemberton -> law.inventor.inventions -> Coca-Cola -> base.popstra.product.sold_to -> m.064hhfy\n# Answer:\nCoca-Cola", "# Reasoning Path:\nJohn Pemberton -> law.inventor.inventions -> Coca-Cola -> food.food.nutrients -> m.07xb76d\n# Answer:\nCoca-Cola", "# Reasoning Path:\nJohn Pemberton -> law.inventor.inventions -> Coca-Cola -> common.topic.webpage -> m.02sr4y1\n# Answer:\nCoca-Cola", "# Reasoning Path:\nJohn Pemberton -> law.inventor.inventions -> Coca-Cola -> food.food.nutrients -> m.07xb76s\n# Answer:\nCoca-Cola", "# Reasoning Path:\nJohn Pemberton -> law.inventor.inventions -> Coca-Cola -> common.topic.webpage -> m.09x_pq4\n# Answer:\nCoca-Cola", "# Reasoning Path:\nJohn Pemberton -> law.inventor.inventions -> Coca-Cola -> common.topic.webpage -> m.09xl5g4\n# Answer:\nCoca-Cola", "# Reasoning Path:\nJohn Pemberton -> symbols.name_source.namesakes -> Pemberton Place -> location.location.geolocation -> m.01227nwv\n# Answer:\nPemberton Place", "# Reasoning Path:\nJohn Pemberton -> people.person.nationality -> Confederate States of America -> military.military_combatant.military_commanders -> m.048z_8v\n# Answer:\nConfederate States of America", "# Reasoning Path:\nJohn Pemberton -> symbols.name_source.namesakes -> Pemberton Place -> common.topic.article -> m.027nh4q\n# Answer:\nPemberton Place"], "ground_truth": ["Coca-Cola"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-753", "prediction": ["# Reasoning Path:\nSir Stanley Matthews -> sports.pro_athlete.sports_played_professionally -> m.0d5fbrd -> sports.pro_sports_played.sport -> Football\n# Answer:\nm.0d5fbrd", "# Reasoning Path:\nSir Stanley Matthews -> soccer.football_player.position_s -> Forward -> common.topic.webpage -> m.09w2grw\n# Answer:\nForward", "# Reasoning Path:\nSir Stanley Matthews -> soccer.football_player.position_s -> Forward -> sports.sport.pro_athletes -> m.011p3wnv\n# Answer:\nForward", "# Reasoning Path:\nSir Stanley Matthews -> common.topic.article -> m.013s75\n# Answer:\nm.013s75", "# Reasoning Path:\nSir Stanley Matthews -> soccer.football_player.position_s -> Forward -> common.topic.notable_types -> Football Position\n# Answer:\nForward", "# Reasoning Path:\nSir Stanley Matthews -> soccer.football_player.position_s -> Midfielder -> sports.sports_position.sport -> Football\n# Answer:\nMidfielder", "# Reasoning Path:\nSir Stanley Matthews -> soccer.football_player.position_s -> Forward -> common.topic.webpage -> m.09w2mrd\n# Answer:\nForward", "# Reasoning Path:\nSir Stanley Matthews -> soccer.football_player.position_s -> Midfielder -> common.topic.image -> 396px-Boisko PositionsMidfield\n# Answer:\nMidfielder", "# Reasoning Path:\nSir Stanley Matthews -> soccer.football_player.position_s -> Forward -> common.topic.webpage -> m.09w2q_3\n# Answer:\nForward", "# Reasoning Path:\nSir Stanley Matthews -> soccer.football_player.position_s -> Midfielder -> common.topic.webpage -> m.09y4zj1\n# Answer:\nMidfielder"], "ground_truth": ["Forward", "Midfielder", "Outside forward"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7272727272727272, "ans_precission": 0.8, "ans_recall": 0.6666666666666666, "path_f1": 0.7272727272727272, "path_precision": 0.8, "path_recall": 0.6666666666666666, "path_ans_f1": 0.7272727272727272, "path_ans_precision": 0.8, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-755", "prediction": ["# Reasoning Path:\nCharles Edward Stuart -> people.person.parents -> James Francis Edward Stuart -> people.person.parents -> James II of England\n# Answer:\nJames Francis Edward Stuart", "# Reasoning Path:\nCharles Edward Stuart -> people.person.parents -> Maria Clementina Sobieska -> people.person.children -> Henry Benedict Stuart\n# Answer:\nMaria Clementina Sobieska", "# Reasoning Path:\nCharles Edward Stuart -> people.person.parents -> James Francis Edward Stuart -> people.person.sibling_s -> m.0w4g8zr\n# Answer:\nJames Francis Edward Stuart", "# Reasoning Path:\nCharles Edward Stuart -> people.person.parents -> James Francis Edward Stuart -> people.person.parents -> Mary of Modena\n# Answer:\nJames Francis Edward Stuart", "# Reasoning Path:\nCharles Edward Stuart -> people.person.parents -> James Francis Edward Stuart -> people.person.nationality -> England\n# Answer:\nJames Francis Edward Stuart", "# Reasoning Path:\nCharles Edward Stuart -> people.person.children -> Charlotte Stuart, Duchess of Albany -> people.person.parents -> Clementina Walkinshaw\n# Answer:\nCharlotte Stuart, Duchess of Albany", "# Reasoning Path:\nCharles Edward Stuart -> people.person.parents -> Maria Clementina Sobieska -> common.topic.image -> Maria Clementina Sobieska from NPG\n# Answer:\nMaria Clementina Sobieska", "# Reasoning Path:\nCharles Edward Stuart -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Dupuytren's contracture\n# Answer:\nMale", "# Reasoning Path:\nCharles Edward Stuart -> people.person.parents -> Maria Clementina Sobieska -> common.topic.article -> m.0155dl\n# Answer:\nMaria Clementina Sobieska", "# Reasoning Path:\nCharles Edward Stuart -> people.person.children -> Charlotte Stuart, Duchess of Albany -> common.topic.article -> m.04lq8d\n# Answer:\nCharlotte Stuart, Duchess of Albany"], "ground_truth": ["Charles Edward Stuart"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-756", "prediction": ["# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> people.person.profession -> Television producer\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nStewie Griffin -> tv.tv_character.appeared_in_tv_program -> m.02kk65p -> tv.regular_tv_appearance.actor -> Seth MacFarlane\n# Answer:\nm.02kk65p", "# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> people.person.profession -> Actor\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.010gysx9\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> people.person.profession -> Animator\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> tv.tv_producer.programs_produced -> m.010hsx_r\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.02kk65p\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nStewie Griffin -> tv.tv_character.appeared_in_tv_program -> m.02kk65p -> tv.regular_tv_appearance.seasons -> Family Guy - Season 1\n# Answer:\nm.02kk65p", "# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.02ntr0h\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> tv.tv_producer.programs_produced -> m.01140l3r\n# Answer:\nSeth MacFarlane"], "ground_truth": ["Seth MacFarlane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-757", "prediction": ["# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Marcheline Bertrand -> people.person.parents -> Lois June Gouwens\n# Answer:\nMarcheline Bertrand", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Marcheline Bertrand -> people.person.parents -> Rolland F. Bertrand\n# Answer:\nMarcheline Bertrand", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Marcheline Bertrand -> tv.tv_actor.guest_roles -> m.09p2yk0\n# Answer:\nMarcheline Bertrand", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Jon Voight -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nJon Voight", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Marcheline Bertrand -> people.person.children -> James Haven\n# Answer:\nMarcheline Bertrand", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Jon Voight -> people.person.children -> James Haven\n# Answer:\nJon Voight", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Jon Voight -> freebase.valuenotation.is_reviewed -> Children\n# Answer:\nJon Voight", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Jon Voight -> people.person.profession -> Actor\n# Answer:\nJon Voight", "# Reasoning Path:\nAngelina Jolie -> award.award_nominee.award_nominations -> m.010wr37v -> award.award_nomination.nominated_for -> Maleficent\n# Answer:\nm.010wr37v", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Jon Voight -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nJon Voight"], "ground_truth": ["Jon Voight", "Marcheline Bertrand"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-758", "prediction": ["# Reasoning Path:\nGovernor of Indiana -> common.topic.image -> Mitch Daniels -> government.politician.government_positions_held -> m.04l4slb\n# Answer:\nMitch Daniels", "# Reasoning Path:\nGovernor of Indiana -> common.topic.image -> Mitch Daniels -> common.image.appears_in_topic_gallery -> Indiana gubernatorial election, 2008\n# Answer:\nMitch Daniels", "# Reasoning Path:\nGovernor of Indiana -> common.topic.image -> Mitch Daniels -> government.political_appointer.appointees -> m.010hbc1b\n# Answer:\nMitch Daniels", "# Reasoning Path:\nGovernor of Indiana -> government.government_office_or_title.office_holders -> m.04l4slb -> government.government_position_held.office_holder -> Mitch Daniels\n# Answer:\nm.04l4slb", "# Reasoning Path:\nGovernor of Indiana -> government.government_office_or_title.office_holders -> m.04l4s5v -> government.government_position_held.office_holder -> James B. Ray\n# Answer:\nm.04l4s5v", "# Reasoning Path:\nGovernor of Indiana -> government.government_office_or_title.jurisdiction -> Indiana -> common.topic.image -> Indiana.png\n# Answer:\nIndiana", "# Reasoning Path:\nGovernor of Indiana -> government.government_office_or_title.office_holders -> m.04l4slb -> government.government_position_held.jurisdiction_of_office -> Indiana\n# Answer:\nm.04l4slb", "# Reasoning Path:\nGovernor of Indiana -> government.government_office_or_title.office_holders -> m.04l4s5v -> government.government_position_held.jurisdiction_of_office -> Indiana\n# Answer:\nm.04l4s5v", "# Reasoning Path:\nGovernor of Indiana -> government.government_office_or_title.office_holders -> m.04l4s5g -> government.government_position_held.office_holder -> Jonathan Jennings\n# Answer:\nm.04l4s5g", "# Reasoning Path:\nGovernor of Indiana -> government.government_office_or_title.jurisdiction -> Indiana -> common.topic.image -> Vista della riserva del fiume Salamonie nella Contea di Huntington, Indiana nord-orientale\n# Answer:\nIndiana"], "ground_truth": ["Mitch Daniels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-759", "prediction": ["# Reasoning Path:\nBrad Pitt -> base.popstra.celebrity.canoodled -> m.0652slx -> base.popstra.canoodled.participant -> Uma Thurman\n# Answer:\nm.0652slx", "# Reasoning Path:\nBrad Pitt -> film.actor.film -> g.11b6r79157\n# Answer:\ng.11b6r79157", "# Reasoning Path:\nBrad Pitt -> base.popstra.celebrity.canoodled -> m.063h8xj -> base.popstra.canoodled.participant -> Julia Ormond\n# Answer:\nm.063h8xj", "# Reasoning Path:\nBrad Pitt -> base.popstra.celebrity.canoodled -> m.064bhn3 -> base.popstra.canoodled.participant -> Angelina Jolie\n# Answer:\nm.064bhn3", "# Reasoning Path:\nBrad Pitt -> film.actor.film -> g.11bynvk8jk\n# Answer:\ng.11bynvk8jk", "# Reasoning Path:\nBrad Pitt -> base.popstra.celebrity.parties -> m.063hd7h -> base.popstra.party_attendance_person.party -> Le Baoli\n# Answer:\nm.063hd7h", "# Reasoning Path:\nBrad Pitt -> film.actor.film -> m.02tb3_5 -> film.performance.special_performance_type -> Uncredited\n# Answer:\nm.02tb3_5", "# Reasoning Path:\nBrad Pitt -> film.actor.film -> m.02tb3_5 -> film.performance.character -> Waiter\n# Answer:\nm.02tb3_5", "# Reasoning Path:\nBrad Pitt -> base.popstra.celebrity.parties -> m.064bjws -> base.popstra.party_attendance_person.party -> Exclusive Golden Globes Party\n# Answer:\nm.064bjws", "# Reasoning Path:\nBrad Pitt -> film.actor.film -> m.02tb3_5 -> film.performance.film -> No Man's Land\n# Answer:\nm.02tb3_5"], "ground_truth": ["Sinitta", "Juliette Lewis", "Thandie Newton", "Shalane McCall", "Robin Givens"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Raphael and his age\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Leonardo da Vinci and the Art of Sculpture: Inspiration and Invention\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> people.profession.people_with_this_profession -> Sandro Botticelli\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> visual_art.visual_artist.art_forms -> Fresco\n# Answer:\nAntonio da Correggio", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> common.topic.subject_of -> BRS Custom Painting\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> influence.influence_node.influenced_by -> Melozzo da Forl\u00ec\n# Answer:\nAntonio da Correggio", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Bernardino Luini -> visual_art.visual_artist.art_forms -> Painting\n# Answer:\nBernardino Luini", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> common.topic.subjects -> Fabrice de Villeneuve\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nPainting"], "ground_truth": ["Benois Madonna", "Drapery for a Seated Figure", "g.1219sb0g", "Horse and Rider", "Leda and the Swan", "Lady with an Ermine", "g.12215rxg", "g.120vt1gz", "Madonna of the Carnation", "g.12314dm1", "Head of a Woman", "Portrait of a Young Fianc\u00e9e", "The Baptism of Christ", "Vitruvian Man", "Sala delle Asse", "Bacchus", "g.1224tf0c", "Lucan portrait of Leonardo da Vinci", "Leonardo's horse", "The Virgin and Child with St. Anne", "The Holy Infants Embracing", "Portrait of a Musician", "The Battle of Anghiari", "Portrait of Isabella d'Este", "The Virgin and Child with St Anne and St John the Baptist", "Salvator Mundi", "Mona Lisa", "Madonna Litta", "Adoration of the Magi", "The Last Supper", "g.121yh91r", "Annunciation", "Madonna of Laroque", "St. Jerome in the Wilderness", "Madonna and Child with St Joseph", "St. John the Baptist", "La belle ferronni\u00e8re", "g.1239jd9p", "g.1213jb_b", "Virgin of the Rocks", "g.121wt37c", "Portrait of a man in red chalk", "Ginevra de' Benci", "Medusa", "Madonna of the Yarnwinder"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-760", "prediction": ["# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.party -> Democratic Party\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> government.politician.party -> m.03gjhww -> government.political_party_tenure.party -> Democratic Party\n# Answer:\nm.03gjhww", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.venues -> Pepsi Center\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.speeches -> m.04j00hr\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> government.politician.party -> m.03gjhww -> freebase.valuenotation.is_reviewed -> Party\n# Answer:\nm.03gjhww", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.venues -> Sports Authority Field at Mile High\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.speeches -> m.04j00j0\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.author -> Aberjhani\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Bound Man -> book.written_work.subjects -> African-American studies\n# Answer:\nA Bound Man", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.speeches -> m.04j00j9\n# Answer:\n2008 Democratic National Convention"], "ground_truth": ["Democratic Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.2, "path_recall": 0.3333333333333333, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-761", "prediction": ["# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94107 -> location.location.containedby -> California\n# Answer:\n94107", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94107 -> common.topic.notable_types -> Postal Code\n# Answer:\n94107", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94107 -> location.postal_code.country -> United States of America\n# Answer:\n94107", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94101 -> location.postal_code.country -> United States of America\n# Answer:\n94101", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94107 -> location.location.containedby -> South of Market\n# Answer:\n94107", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94118 -> location.location.containedby -> California\n# Answer:\n94118", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94118 -> location.postal_code.country -> United States of America\n# Answer:\n94118", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94107 -> location.location.containedby -> United States of America\n# Answer:\n94107", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94101 -> common.topic.notable_for -> g.125d1w2fz\n# Answer:\n94101", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94118 -> location.location.containedby -> United States of America\n# Answer:\n94118"], "ground_truth": ["94119", "94130", "94126", "94123", "94159", "94107", "94120", "94162", "94128", "94115", "94105", "94132", "94118-3208", "94101", "94131", "94114", "94104", "94134", "94112", "94144", "94177", "94107-7621", "94109", "94143", "94125", "94117-2212", "94188", "94133", "94139", "94122", "94164", "94127", "94145", "94108", "94199", "94171", "94154", "94163", "94151", "94172", "94111", "94140", "94103", "94110", "94146", "94158", "94160", "94124", "94156", "94161", "94118", "94141", "94142", "94102", "94153", "94129", "94117", "94116", "94137", "94147", "94102-4733", "94121"], "ans_acc": 0.04838709677419355, "ans_hit": 1, "ans_f1": 0.09230769230769231, "ans_precission": 1.0, "ans_recall": 0.04838709677419355, "path_f1": 0.09230769230769231, "path_precision": 1.0, "path_recall": 0.04838709677419355, "path_ans_f1": 0.09230769230769231, "path_ans_precision": 1.0, "path_ans_recall": 0.04838709677419355}
{"id": "WebQTest-762", "prediction": ["# Reasoning Path:\nStephen Ireland -> base.schemastaging.athlete_extra.salary -> m.0lh5l0q -> base.schemastaging.athlete_salary.team -> Aston Villa F.C.\n# Answer:\nm.0lh5l0q", "# Reasoning Path:\nStephen Ireland -> soccer.football_player.statistics -> m.0w8wygw -> soccer.football_player_stats.team -> Manchester City F.C.\n# Answer:\nm.0w8wygw", "# Reasoning Path:\nStephen Ireland -> soccer.football_player.statistics -> m.0w9mf8d -> soccer.football_player_stats.team -> Aston Villa F.C.\n# Answer:\nm.0w9mf8d", "# Reasoning Path:\nStephen Ireland -> base.schemastaging.athlete_extra.salary -> m.0lh5l0q -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nm.0lh5l0q", "# Reasoning Path:\nStephen Ireland -> soccer.football_player.statistics -> m.0w9mhjv -> soccer.football_player_stats.team -> Newcastle United F.C.\n# Answer:\nm.0w9mhjv", "# Reasoning Path:\nStephen Ireland -> sports.pro_athlete.teams -> m.04m44tt -> sports.sports_team_roster.team -> Manchester City F.C.\n# Answer:\nm.04m44tt", "# Reasoning Path:\nStephen Ireland -> sports.pro_athlete.teams -> m.0lf9x6n -> sports.sports_team_roster.team -> Aston Villa F.C.\n# Answer:\nm.0lf9x6n", "# Reasoning Path:\nStephen Ireland -> sports.pro_athlete.teams -> m.04m44tt -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.04m44tt", "# Reasoning Path:\nStephen Ireland -> sports.pro_athlete.teams -> m.0lgppdy -> sports.sports_team_roster.team -> Newcastle United F.C.\n# Answer:\nm.0lgppdy", "# Reasoning Path:\nStephen Ireland -> sports.pro_athlete.teams -> m.0lf9x6n -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nm.0lf9x6n"], "ground_truth": ["Newcastle United F.C.", "Stoke City F.C.", "Manchester City F.C.", "Aston Villa F.C."], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.7777777777777777, "path_precision": 0.7, "path_recall": 0.875, "path_ans_f1": 0.7241379310344827, "path_ans_precision": 0.7, "path_ans_recall": 0.75}
{"id": "WebQTest-764", "prediction": ["# Reasoning Path:\nJohn Mayer -> people.person.education -> m.02wnjyy -> education.education.institution -> Berklee College of Music\n# Answer:\nm.02wnjyy", "# Reasoning Path:\nJohn Mayer -> people.person.education -> m.0h30083 -> education.education.institution -> Brien McMahon High School\n# Answer:\nm.0h30083", "# Reasoning Path:\nJohn Mayer -> people.person.education -> m.0n0v79l -> education.education.institution -> Fairfield Warde High School\n# Answer:\nm.0n0v79l", "# Reasoning Path:\nJohn Mayer -> common.topic.webpage -> http://www.discogs.com/artist/John+Mayer -> common.webpage.category -> Topic Webpage\n# Answer:\nhttp://www.discogs.com/artist/John+Mayer", "# Reasoning Path:\nJohn Mayer -> tv.tv_actor.guest_roles -> m.0bv_4q4 -> tv.tv_guest_role.episodes_appeared_in -> Show #2662\n# Answer:\nm.0bv_4q4", "# Reasoning Path:\nJohn Mayer -> common.topic.webpage -> m.09w5t6j -> common.webpage.resource -> Celebrity date night at John Mayer's 'revue'\n# Answer:\nm.09w5t6j", "# Reasoning Path:\nJohn Mayer -> common.topic.webpage -> http://www.discogs.com/artist/John+Mayer -> common.webpage.resource -> m.0bm4m94\n# Answer:\nhttp://www.discogs.com/artist/John+Mayer", "# Reasoning Path:\nJohn Mayer -> common.topic.webpage -> m.09w5t6j -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w5t6j", "# Reasoning Path:\nJohn Mayer -> common.topic.webpage -> m.09w6d63 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w6d63", "# Reasoning Path:\nJohn Mayer -> tv.tv_actor.guest_roles -> m.0bv_shf -> tv.tv_guest_role.episodes_appeared_in -> Dave Chapelle, John Mayer\n# Answer:\nm.0bv_shf"], "ground_truth": ["Berklee College of Music"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-765", "prediction": ["# Reasoning Path:\nBrody Jenner -> base.popstra.celebrity.canoodled -> m.0652s8g -> base.popstra.canoodled.participant -> Paris Hilton\n# Answer:\nm.0652s8g", "# Reasoning Path:\nBrody Jenner -> base.popstra.celebrity.canoodled -> m.063k2n8 -> base.popstra.canoodled.participant -> Paris Hilton\n# Answer:\nm.063k2n8", "# Reasoning Path:\nBrody Jenner -> base.popstra.celebrity.canoodled -> m.063k2nm -> base.popstra.canoodled.participant -> Lauren Conrad\n# Answer:\nm.063k2nm", "# Reasoning Path:\nBrody Jenner -> base.popstra.celebrity.dated -> m.065q5tn -> base.popstra.dated.participant -> Lauren Conrad\n# Answer:\nm.065q5tn", "# Reasoning Path:\nBrody Jenner -> base.popstra.celebrity.dated -> m.063g5df -> base.popstra.dated.participant -> Paris Hilton\n# Answer:\nm.063g5df", "# Reasoning Path:\nBrody Jenner -> base.popstra.celebrity.dated -> m.065pvwb -> base.popstra.dated.participant -> Ryan J. Parker\n# Answer:\nRyan J. Parker", "# Reasoning Path:\nBrody Jenner -> tv.tv_actor.guest_roles -> m.09nm66k -> tv.tv_guest_role.episodes_appeared_in -> You Know What You Did\n# Answer:\nm.09nm66k", "# Reasoning Path:\nBrody Jenner -> tv.tv_actor.guest_roles -> m.0g0f2wz -> tv.tv_guest_role.episodes_appeared_in -> Brody Jenner, Steve Harvey, Chuck Nice, Patrick Meagher\n# Answer:\nm.0g0f2wz"], "ground_truth": ["Jayde Nicole", "Kristin Cavallari", "Nicole Richie", "Paris Hilton", "Cora Skinner", "Lauren Conrad", "Haylie Duff", "Ryan J. Parker"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.125, "ans_precission": 0.125, "ans_recall": 0.125, "path_f1": 0.48, "path_precision": 0.75, "path_recall": 0.35294117647058826, "path_ans_f1": 0.5, "path_ans_precision": 0.75, "path_ans_recall": 0.375}
{"id": "WebQTest-766", "prediction": ["# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.webpage -> m.03lfx0d\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.notable_types -> Currency\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.notable_for -> g.1258v8707\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.webpage -> m.04m10rm\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> City/Town/Village\n# Answer:\nEarth", "# Reasoning Path:\nMalaysia -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> United States of America\n# Answer:\nEarth", "# Reasoning Path:\nMalaysia -> location.statistical_region.size_of_armed_forces -> g.12cp_jm8t\n# Answer:\ng.12cp_jm8t", "# Reasoning Path:\nMalaysia -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> Asia\n# Answer:\nEarth", "# Reasoning Path:\nMalaysia -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> Sovereign state\n# Answer:\nEarth", "# Reasoning Path:\nMalaysia -> location.statistical_region.size_of_armed_forces -> g.1hhc37x7v\n# Answer:\ng.1hhc37x7v"], "ground_truth": ["Malaysian ringgit"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-768", "prediction": ["# Reasoning Path:\nVolkswagen Passenger Cars -> organization.organization.founders -> German Labour Front -> organization.organization_founder.organizations_founded -> Volkswagen Group\n# Answer:\nGerman Labour Front", "# Reasoning Path:\nVolkswagen Passenger Cars -> organization.organization.founders -> German Labour Front -> common.topic.notable_types -> Organization founder\n# Answer:\nGerman Labour Front", "# Reasoning Path:\nVolkswagen Passenger Cars -> organization.organization.leadership -> m.0vbmqq2 -> organization.leadership.person -> Martin Winterkorn\n# Answer:\nm.0vbmqq2", "# Reasoning Path:\nVolkswagen Passenger Cars -> organization.organization.founders -> German Labour Front -> base.schemastaging.context_name.pronunciation -> g.125_l36r8\n# Answer:\nGerman Labour Front", "# Reasoning Path:\nVolkswagen Passenger Cars -> organization.organization.leadership -> m.0vbmqq2 -> organization.leadership.role -> Chairman\n# Answer:\nm.0vbmqq2", "# Reasoning Path:\nVolkswagen Passenger Cars -> organization.organization.leadership -> m.0vbmqq2 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0vbmqq2", "# Reasoning Path:\nVolkswagen Passenger Cars -> common.topic.webpage -> m.09vk8xf -> common.webpage.resource -> Volkswagen Reviews and Pricing\n# Answer:\nm.09vk8xf", "# Reasoning Path:\nVolkswagen Passenger Cars -> common.topic.webpage -> m.01xwtq5 -> common.webpage.category -> Topic Webpage\n# Answer:\nm.01xwtq5", "# Reasoning Path:\nVolkswagen Passenger Cars -> common.topic.webpage -> m.01xwtq5 -> common.webpage.resource -> m.0bkfyrf\n# Answer:\nm.01xwtq5", "# Reasoning Path:\nVolkswagen Passenger Cars -> common.topic.webpage -> m.09vk8xf -> common.webpage.category -> Topic Webpage\n# Answer:\nm.09vk8xf"], "ground_truth": ["Volkswagen Group"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-769", "prediction": ["# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> aviation.airport.hub_for -> Aerolinee Itavia\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> common.topic.article -> m.033_5b\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Leonardo da Vinci\u2013Fiumicino Airport -> location.location.geolocation -> m.02_j8sl\n# Answer:\nLeonardo da Vinci\u2013Fiumicino Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> aviation.airport.serves -> Ciampino\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> aviation.airport.hub_for -> Mistral Air\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> travel.travel_destination.how_to_get_here -> m.052lyyd -> travel.transportation.mode_of_transportation -> Train\n# Answer:\nm.052lyyd", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> aviation.airport.hub_for -> Ryanair\n# Answer:\nRyanair", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Italian Met Office Airport -> location.location.geolocation -> m.04fngkg\n# Answer:\nItalian Met Office Airport", "# Reasoning Path:\nRome -> travel.travel_destination.how_to_get_here -> m.052lyyp -> travel.transportation.transport_terminus -> Civitavecchia Ferry Terminal\n# Answer:\nm.052lyyp", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Leonardo da Vinci\u2013Fiumicino Airport -> aviation.airport.hub_for -> Aermediterranea\n# Answer:\nLeonardo da Vinci\u2013Fiumicino Airport"], "ground_truth": ["Roma Termini railway station"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.capital -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Carinthia -> location.location.partially_contains -> Coglians\n# Answer:\nCarinthia", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Burgenland -> base.aareas.schema.administrative_area.administrative_area_type -> Austrian state\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Burgenland -> common.topic.notable_types -> Administrative Division\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Carinthia -> common.topic.notable_types -> Administrative Division\n# Answer:\nCarinthia", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Carinthia -> location.location.partially_contains -> Coglians\n# Answer:\nCarinthia", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Burgenland -> base.aareas.schema.administrative_area.administrative_area_type -> Austrian state\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Burgenland -> common.topic.notable_types -> Administrative Division\n# Answer:\nBurgenland"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-770", "prediction": ["# Reasoning Path:\nSteve McNair -> people.deceased_person.cause_of_death -> Ballistic trauma -> base.pethealth.cause.pet_diseases_or_conditions_caused -> Head Trauma in Animals\n# Answer:\nBallistic trauma", "# Reasoning Path:\nSteve McNair -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.article -> m.02qnd1f\n# Answer:\nBallistic trauma", "# Reasoning Path:\nSteve McNair -> people.deceased_person.cause_of_death -> Ballistic trauma -> base.schemastaging.context_name.pronunciation -> g.125_s3hjb\n# Answer:\nBallistic trauma", "# Reasoning Path:\nSteve McNair -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nSteve McNair -> people.person.gender -> Male -> medicine.risk_factor.diseases -> heart attack\n# Answer:\nMale", "# Reasoning Path:\nSteve McNair -> people.person.education -> m.02kybw3 -> education.education.institution -> Alcorn State University\n# Answer:\nm.02kybw3", "# Reasoning Path:\nSteve McNair -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nMale", "# Reasoning Path:\nSteve McNair -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nSteve McNair -> people.person.gender -> Male -> biology.hybrid_parent_gender.hybrids -> m.0blp580\n# Answer:\nMale", "# Reasoning Path:\nSteve McNair -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_98hd\n# Answer:\nMale"], "ground_truth": ["Ballistic trauma"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-771", "prediction": ["# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.agencies -> California Conservation Corps -> organization.organization.founders -> Jerry Brown\n# Answer:\nCalifornia Conservation Corps", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.agencies -> California Conservation Corps -> common.topic.notable_for -> g.1257jlrc9\n# Answer:\nCalifornia Conservation Corps", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.agencies -> 11-99 Foundation -> common.topic.notable_for -> g.1254xhy5c\n# Answer:\n11-99 Foundation", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.agencies -> Alameda County Superior Court -> location.location.street_address -> m.08d08r9\n# Answer:\nAlameda County Superior Court", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.agencies -> 11-99 Foundation -> common.topic.notable_types -> Government Agency\n# Answer:\n11-99 Foundation", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.agencies -> 11-99 Foundation -> common.topic.article -> m.027gbf3\n# Answer:\n11-99 Foundation", "# Reasoning Path:\nCalifornia -> location.location.events -> 1987 Wine Country Film Festival -> freebase.valuenotation.has_value -> Closing date\n# Answer:\n1987 Wine Country Film Festival", "# Reasoning Path:\nCalifornia -> location.location.events -> 1987 Wine Country Film Festival -> time.event.locations -> Sonoma Valley\n# Answer:\n1987 Wine Country Film Festival", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.agencies -> Alameda County Superior Court -> location.location.containedby -> Alameda County\n# Answer:\nAlameda County Superior Court", "# Reasoning Path:\nCalifornia -> location.location.events -> 1987 Wine Country Film Festival -> time.event.instance_of_recurring_event -> Wine Country Film Festival\n# Answer:\n1987 Wine Country Film Festival"], "ground_truth": ["Arnold Schwarzenegger"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-772", "prediction": ["# Reasoning Path:\nGhost Rider -> film.film.starring -> m.0jyrrc -> film.performance.actor -> Eva Mendes\n# Answer:\nm.0jyrrc", "# Reasoning Path:\nGhost Rider -> film.film.starring -> m.0hdy101 -> film.performance.actor -> Raquel Alessi\n# Answer:\nm.0hdy101", "# Reasoning Path:\nGhost Rider -> film.film.starring -> m.0jyrrc -> film.performance.character -> Roxanne Simpson\n# Answer:\nm.0jyrrc", "# Reasoning Path:\nGhost Rider -> film.film.starring -> m.03jrkkx -> film.performance.actor -> Brett Cullen\n# Answer:\nm.03jrkkx", "# Reasoning Path:\nGhost Rider -> film.film.starring -> m.0hdy101 -> film.performance.character -> Roxanne Simpson\n# Answer:\nm.0hdy101", "# Reasoning Path:\nGhost Rider -> common.topic.webpage -> m.03l8z6p -> common.webpage.category -> Official Website\n# Answer:\nm.03l8z6p", "# Reasoning Path:\nGhost Rider -> film.film.starring -> m.03jrkkx -> film.performance.character -> Barton Blaze\n# Answer:\nm.03jrkkx", "# Reasoning Path:\nGhost Rider -> common.topic.webpage -> m.03l8z6p -> common.webpage.resource -> m.0blx2k4\n# Answer:\nm.03l8z6p", "# Reasoning Path:\nGhost Rider -> film.film.genre -> Adventure Film -> media_common.literary_genre.books_in_this_genre -> A Dirty Job\n# Answer:\nAdventure Film", "# Reasoning Path:\nGhost Rider -> film.film.genre -> Adventure Film -> media_common.media_genre.parent_genre -> Entertainment\n# Answer:\nAdventure Film"], "ground_truth": ["Raquel Alessi", "Eva Mendes"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-773", "prediction": ["# Reasoning Path:\nSony Mobile Communications -> organization.organization.headquarters -> m.040r1dz -> location.mailing_address.citytown -> Minato\n# Answer:\nm.040r1dz", "# Reasoning Path:\nSony Mobile Communications -> organization.organization.headquarters -> m.040r1dz -> common.topic.notable_types -> Location\n# Answer:\nm.040r1dz", "# Reasoning Path:\nSony Mobile Communications -> organization.organization.headquarters -> m.040r1dz -> common.topic.notable_for -> g.1259dkczx\n# Answer:\nm.040r1dz", "# Reasoning Path:\nSony Mobile Communications -> base.schemastaging.organization_extra.phone_number -> m.0109_8hh -> base.schemastaging.phone_sandbox.service_location -> Costa Rica\n# Answer:\nm.0109_8hh", "# Reasoning Path:\nSony Mobile Communications -> computer.computer_manufacturer_brand.computer_models -> Sony Ericsson Live with Walkman -> common.topic.article -> m.0n49dzq\n# Answer:\nSony Ericsson Live with Walkman", "# Reasoning Path:\nSony Mobile Communications -> computer.computer_manufacturer_brand.computer_models -> Sony Ericsson Xperia X10 -> common.topic.notable_types -> Consumer product\n# Answer:\nSony Ericsson Xperia X10", "# Reasoning Path:\nSony Mobile Communications -> base.schemastaging.organization_extra.phone_number -> m.0109_8hh -> base.schemastaging.phone_sandbox.service_language -> Spanish Language\n# Answer:\nm.0109_8hh", "# Reasoning Path:\nSony Mobile Communications -> base.schemastaging.organization_extra.phone_number -> m.0109_8m_ -> base.schemastaging.phone_sandbox.service_language -> English Language\n# Answer:\nm.0109_8m_", "# Reasoning Path:\nSony Mobile Communications -> base.schemastaging.organization_extra.phone_number -> m.0109_8hh -> base.schemastaging.phone_sandbox.contact_category -> Customer Service\n# Answer:\nm.0109_8hh", "# Reasoning Path:\nSony Mobile Communications -> base.schemastaging.organization_extra.phone_number -> m.0109_8t3 -> base.schemastaging.phone_sandbox.service_location -> Dominican Republic\n# Answer:\nm.0109_8t3"], "ground_truth": ["Minato"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-774", "prediction": ["# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.article -> m.04lt88\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.webpage -> m.04m2gwk\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.statistical_region.co2_emissions_per_capita -> g.1245_1qfx\n# Answer:\ng.1245_1qfx", "# Reasoning Path:\nDominican Republic -> location.statistical_region.internet_users_percent_population -> g.11b60vv5zn\n# Answer:\ng.11b60vv5zn", "# Reasoning Path:\nDominican Republic -> location.statistical_region.co2_emissions_per_capita -> g.1245_22j7\n# Answer:\ng.1245_22j7", "# Reasoning Path:\nDominican Republic -> location.statistical_region.co2_emissions_per_capita -> g.1245_4lf3\n# Answer:\ng.1245_4lf3", "# Reasoning Path:\nDominican Republic -> location.statistical_region.internet_users_percent_population -> g.1245_22jn\n# Answer:\ng.1245_22jn", "# Reasoning Path:\nDominican Republic -> location.statistical_region.internet_users_percent_population -> g.1245_m66j\n# Answer:\ng.1245_m66j"], "ground_truth": ["Dominican peso"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-775", "prediction": ["# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Bedford Mall\n# Answer:\nBedford Mall", "# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Manchester Museum -> common.topic.image -> Manchester Museum by Nick Higham\n# Answer:\nManchester Museum", "# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Albert Hall, Manchester -> location.location.containedby -> England\n# Answer:\nAlbert Hall, Manchester", "# Reasoning Path:\nManchester -> location.statistical_region.population -> g.11b66bsm6f\n# Answer:\ng.11b66bsm6f", "# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Manchester Museum -> common.topic.image -> The Manchester Museum\n# Answer:\nManchester Museum", "# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Manchester Museum -> location.location.containedby -> England\n# Answer:\nManchester Museum", "# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Albert Hall, Manchester -> freebase.valuenotation.has_value -> Floors\n# Answer:\nAlbert Hall, Manchester", "# Reasoning Path:\nManchester -> location.statistical_region.population -> g.11btt603nx\n# Answer:\ng.11btt603nx", "# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Manchester Museum -> architecture.museum.type_of_museum -> Archaeological Museum\n# Answer:\nManchester Museum", "# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Albert Hall, Manchester -> architecture.structure.architect -> W.J. Morley\n# Answer:\nAlbert Hall, Manchester"], "ground_truth": ["Urbis", "Whitworth Art Gallery", "Great Manchester Run", "Manchester Jewish Museum", "Old Trafford", "Imperial War Museum North", "The Moon Under Water, Manchester", "Manchester Museum", "Chinatown, Manchester", "Albert Hall, Manchester", "Wheel of Manchester", "Manchester Cathedral", "Manchester Art Gallery"], "ans_acc": 0.15384615384615385, "ans_hit": 1, "ans_f1": 0.2522522522522523, "ans_precission": 0.7, "ans_recall": 0.15384615384615385, "path_f1": 0.2522522522522523, "path_precision": 0.7, "path_recall": 0.15384615384615385, "path_ans_f1": 0.2522522522522523, "path_ans_precision": 0.7, "path_ans_recall": 0.15384615384615385}
{"id": "WebQTest-776", "prediction": ["# Reasoning Path:\nItaly -> sports.sports_team_location.teams -> AS Vicenza -> sports.sports_team.sport -> Basketball\n# Answer:\nAS Vicenza", "# Reasoning Path:\nItaly -> sports.sports_team_location.teams -> Dinamo Basket Sassari -> sports.sports_team.sport -> Basketball\n# Answer:\nDinamo Basket Sassari", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\ng.11b71mdn0q", "# Reasoning Path:\nItaly -> sports.sports_team_location.teams -> AS Vicenza -> common.topic.notable_types -> Basketball Team\n# Answer:\nAS Vicenza", "# Reasoning Path:\nItaly -> sports.sports_team_location.teams -> Fortitudo Pallacanestro Bologna -> sports.sports_team.sport -> Basketball\n# Answer:\nFortitudo Pallacanestro Bologna", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\ng.12tb6gbb1", "# Reasoning Path:\nItaly -> sports.sports_team_location.teams -> Dinamo Basket Sassari -> common.topic.notable_types -> Basketball Team\n# Answer:\nDinamo Basket Sassari", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37cxr\n# Answer:\ng.1hhc37cxr", "# Reasoning Path:\nItaly -> sports.sports_team_location.teams -> Fortitudo Pallacanestro Bologna -> common.topic.notable_types -> Basketball Team\n# Answer:\nFortitudo Pallacanestro Bologna", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Nearco -> biology.organism.sex -> Male\n# Answer:\nNearco"], "ground_truth": ["Italy men's national volleyball team", "Pallacanestro Varese", "Italy women's national football team", "Italy women's national volleyball team", "Dinamo Basket Sassari", "Italy national baseball team", "AS Vicenza", "Team Liquigas-Cannondale", "Italy national rugby union team", "Italy Davis Cup team", "Italy men's national ice hockey team", "Italy women's national ice hockey team", "Victoria Libertas Pesaro", "Italy national basketball team", "Pallacanestro Treviso", "Virtus Pallacanestro Bologna", "Italy national handball team", "Mens Sana 1871 Basket", "Pallacanestro Cant\u00f9", "Italy national speedway team", "S.S. Felice Scandone", "Juvecaserta Basket", "Fortitudo Pallacanestro Bologna", "Italy women's national rugby union team", "Olimpia Milano", "Italy women's national water polo team", "Pallacanestro Virtus Roma", "Italy Fed Cup team", "Italy national football team", "Italy women's national beach handball team", "Partenope Napoli", "Italy national futsal team", "Italy national rugby league team", "Italy national cricket team", "g.11x7vhqcx", "Italy men's national pitch and putt team"], "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0.14634146341463414, "ans_precission": 0.6, "ans_recall": 0.08333333333333333, "path_f1": 0.14634146341463414, "path_precision": 0.6, "path_recall": 0.08333333333333333, "path_ans_f1": 0.14634146341463414, "path_ans_precision": 0.6, "path_ans_recall": 0.08333333333333333}
{"id": "WebQTest-777", "prediction": ["# Reasoning Path:\nGoogle -> organization.organization.headquarters -> m.030qthd -> location.mailing_address.citytown -> Mountain View\n# Answer:\nm.030qthd", "# Reasoning Path:\nGoogle -> organization.organization.headquarters -> m.030qthd -> common.topic.notable_types -> Location\n# Answer:\nm.030qthd", "# Reasoning Path:\nGoogle -> organization.organization.headquarters -> m.030qthd -> location.mailing_address.state_province_region -> California\n# Answer:\nm.030qthd", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> organization.organization.headquarters -> m.012cdccs\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> internet.website_owner.websites_owned_new -> m.010g4by1 -> internet.website_ownership.website -> Like.com\n# Answer:\nm.010g4by1", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> organization.organization_partnership.members -> Redcorus\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> internet.website_owner.websites_owned_new -> m.0dc4zx1 -> internet.website_ownership.website -> Blogger\n# Answer:\nm.0dc4zx1", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> common.topic.subject_of -> Google Chrome\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> AdSense -> common.topic.subject_of -> David Iwanow\n# Answer:\nAdSense", "# Reasoning Path:\nGoogle -> internet.website_owner.websites_owned_new -> m.0dc4zwr -> internet.website_ownership.website -> FeedBurner\n# Answer:\nm.0dc4zwr"], "ground_truth": ["Mountain View"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-778", "prediction": ["# Reasoning Path:\nJesse Owens -> people.person.education -> m.02wnc1b -> education.education.institution -> Ohio State University\n# Answer:\nm.02wnc1b", "# Reasoning Path:\nJesse Owens -> people.person.education -> m.0n1kct6 -> education.education.institution -> East Technical High School\n# Answer:\nm.0n1kct6", "# Reasoning Path:\nJesse Owens -> people.person.education -> m.02wnc1b -> freebase.valuenotation.has_value -> Degree\n# Answer:\nm.02wnc1b", "# Reasoning Path:\nJesse Owens -> people.person.education -> m.02wnc1b -> freebase.valuenotation.has_value -> End Date\n# Answer:\nm.02wnc1b", "# Reasoning Path:\nJesse Owens -> people.person.education -> m.0n1kct6 -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nm.0n1kct6", "# Reasoning Path:\nJesse Owens -> people.person.places_lived -> m.03pfl8z -> people.place_lived.location -> Cleveland\n# Answer:\nm.03pfl8z", "# Reasoning Path:\nJesse Owens -> people.person.education -> m.02wnc1b -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nm.02wnc1b", "# Reasoning Path:\nJesse Owens -> film.person_or_entity_appearing_in_film.films -> m.046qw5v -> film.personal_film_appearance.type_of_appearance -> Subject of film\n# Answer:\nm.046qw5v", "# Reasoning Path:\nJesse Owens -> people.person.education -> m.0n1kct6 -> freebase.valuenotation.has_no_value -> Major/Field Of Study\n# Answer:\nm.0n1kct6", "# Reasoning Path:\nJesse Owens -> people.person.places_lived -> m.0wk2xk3 -> people.place_lived.location -> Oakville, Alabama\n# Answer:\nm.0wk2xk3"], "ground_truth": ["Ohio State University", "East Technical High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-779", "prediction": ["# Reasoning Path:\nCarlos Santana -> people.person.places_lived -> m.0h3dwg3 -> people.place_lived.location -> San Francisco\n# Answer:\nm.0h3dwg3", "# Reasoning Path:\nCarlos Santana -> people.person.places_lived -> m.03psmj7 -> people.place_lived.location -> Jalisco\n# Answer:\nm.03psmj7", "# Reasoning Path:\nCarlos Santana -> people.person.places_lived -> m.0h3dwfk -> people.place_lived.location -> Tijuana\n# Answer:\nm.0h3dwfk", "# Reasoning Path:\nCarlos Santana -> people.person.children -> Salvador Santana -> music.artist.origin -> United States of America\n# Answer:\nSalvador Santana", "# Reasoning Path:\nCarlos Santana -> music.artist.origin -> Autl\u00e1n -> location.location.people_born_here -> Alfonso Corona Blake\n# Answer:\nAutl\u00e1n", "# Reasoning Path:\nCarlos Santana -> music.artist.origin -> Autl\u00e1n -> common.topic.article -> m.027941n\n# Answer:\nAutl\u00e1n", "# Reasoning Path:\nCarlos Santana -> people.person.children -> Salvador Santana -> people.person.place_of_birth -> San Francisco\n# Answer:\nSalvador Santana", "# Reasoning Path:\nCarlos Santana -> music.artist.origin -> Autl\u00e1n -> common.topic.image -> Municipality location in Jalisco\n# Answer:\nAutl\u00e1n", "# Reasoning Path:\nCarlos Santana -> people.person.children -> Stella Santana -> people.person.gender -> Female\n# Answer:\nStella Santana", "# Reasoning Path:\nCarlos Santana -> music.artist.origin -> Autl\u00e1n -> location.location.people_born_here -> Antonio Alatorre\n# Answer:\nAutl\u00e1n"], "ground_truth": ["Jalisco", "San Francisco", "Tijuana"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04d7nvm\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjf_c\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A Time to Mend\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjfp1\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Anthony Casso\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.person.gender -> Female -> medicine.risk_factor.diseases -> Ovarian cancer\n# Answer:\nFemale", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A season in hell\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> common.topic.webpage -> m.04tnn4f\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> Anticancer\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Bryan Murray\n# Answer:\nCancer"], "ground_truth": ["Cervical cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-780", "prediction": ["# Reasoning Path:\nJohn Carter -> film.film.release_date_s -> m.0gwq3ht -> film.film_regional_release_date.film_release_distribution_medium -> Theatrical\n# Answer:\nm.0gwq3ht", "# Reasoning Path:\nJohn Carter -> film.film.release_date_s -> m.0gwq3ht -> film.film_regional_release_date.film_release_region -> United States of America\n# Answer:\nm.0gwq3ht", "# Reasoning Path:\nJohn Carter -> film.film.release_date_s -> m.0k27srr -> film.film_regional_release_date.film_release_distribution_medium -> DVD\n# Answer:\nm.0k27srr", "# Reasoning Path:\nJohn Carter -> film.film.release_date_s -> m.0vxb_0b -> film.film_regional_release_date.film_release_distribution_medium -> Theatrical\n# Answer:\nm.0vxb_0b", "# Reasoning Path:\nJohn Carter -> film.film.starring -> m.064xypb -> film.performance.actor -> Taylor Kitsch\n# Answer:\nm.064xypb", "# Reasoning Path:\nJohn Carter -> film.film.release_date_s -> m.0k27srr -> film.film_regional_release_date.film_release_region -> United States of America\n# Answer:\nm.0k27srr", "# Reasoning Path:\nJohn Carter -> film.film.produced_by -> Colin Wilson -> common.topic.webpage -> m.03l6d5g\n# Answer:\nColin Wilson", "# Reasoning Path:\nJohn Carter -> film.film.produced_by -> Jim Morris -> common.topic.notable_types -> Film producer\n# Answer:\nJim Morris", "# Reasoning Path:\nJohn Carter -> film.film.release_date_s -> m.0vxb_0b -> film.film_regional_release_date.film_release_region -> Bulgaria\n# Answer:\nm.0vxb_0b", "# Reasoning Path:\nJohn Carter -> film.film.starring -> m.064xyph -> film.performance.character -> Dejah Thoris\n# Answer:\nm.064xyph"], "ground_truth": ["Adventure Film", "Fantasy", "Science Fiction", "Action Film"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-782", "prediction": ["# Reasoning Path:\nNorth American Free Trade Agreement -> organization.membership_organization.members -> m.03w60c3 -> organization.organization_membership.member -> Mexico\n# Answer:\nm.03w60c3", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.membership_organization.members -> m.03w60c8 -> organization.organization_membership.member -> Canada\n# Answer:\nm.03w60c8", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.membership_organization.members -> m.03w60cf -> organization.organization_membership.member -> United States of America\n# Answer:\nm.03w60cf", "# Reasoning Path:\nNorth American Free Trade Agreement -> freebase.valuenotation.has_value -> Place founded -> rdf-schema#domain -> Organization\n# Answer:\nPlace founded", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.organization.founders -> George H. W. Bush -> people.person.profession -> Businessperson\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nNorth American Free Trade Agreement -> freebase.valuenotation.has_value -> Place founded -> type.property.expected_type -> Location\n# Answer:\nPlace founded", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.organization.founders -> George H. W. Bush -> people.person.profession -> Entrepreneur\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.organization.founders -> George H. W. Bush -> government.political_appointer.appointees -> m.048zk8w\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nNorth American Free Trade Agreement -> freebase.valuenotation.has_value -> Place founded -> type.property.schema -> Organization\n# Answer:\nPlace founded", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.organization.founders -> George H. W. Bush -> people.person.profession -> Military aviator\n# Answer:\nGeorge H. W. Bush"], "ground_truth": ["Canada", "Mexico", "United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-783", "prediction": ["# Reasoning Path:\nHouston -> location.location.containedby -> Harris County -> location.location.containedby -> Texas\n# Answer:\nHarris County", "# Reasoning Path:\nHouston -> location.location.containedby -> Texas -> location.location.containedby -> United States of America\n# Answer:\nTexas", "# Reasoning Path:\nHouston -> location.location.containedby -> Harris County -> location.location.containedby -> United States of America\n# Answer:\nHarris County", "# Reasoning Path:\nHouston -> location.hud_county_place.county -> Houston County -> location.location.containedby -> Texas\n# Answer:\nHouston County", "# Reasoning Path:\nHouston -> location.location.containedby -> Harris County -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nHarris County", "# Reasoning Path:\nHouston -> location.location.containedby -> Area code 281 -> location.location.containedby -> Texas\n# Answer:\nArea code 281", "# Reasoning Path:\nHouston -> location.hud_county_place.county -> Montgomery County -> location.location.containedby -> Texas\n# Answer:\nMontgomery County", "# Reasoning Path:\nHouston -> location.location.containedby -> Texas -> organization.organization_scope.organizations_with_this_scope -> The Document Scanning Guys\n# Answer:\nTexas", "# Reasoning Path:\nHouston -> location.place_with_neighborhoods.neighborhoods -> Addicks -> location.location.containedby -> Harris County\n# Answer:\nAddicks", "# Reasoning Path:\nHouston -> location.hud_county_place.county -> Houston County -> location.location.time_zones -> Central Time Zone\n# Answer:\nHouston County"], "ground_truth": ["Montgomery County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-784", "prediction": ["# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.arena_stadium -> CenturyLink Field -> location.location.containedby -> Seattle\n# Answer:\nCenturyLink Field", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.arena_stadium -> CenturyLink Field -> sports.sports_facility.teams -> Seattle Sounders FC\n# Answer:\nCenturyLink Field", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.arena_stadium -> CenturyLink Field -> soccer.football_pitch.matches -> 2010 Lamar Hunt U.S. Open Cup Final\n# Answer:\nCenturyLink Field", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.location -> Seattle -> location.hud_county_place.county -> King County\n# Answer:\nSeattle", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.location -> Seattle -> travel.travel_destination.tourist_attractions -> Experience Music Project Museum\n# Answer:\nSeattle", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.arena_stadium -> CenturyLink Field -> soccer.football_pitch.matches -> 2011 Lamar Hunt U.S. Open Cup Final\n# Answer:\nCenturyLink Field", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.season_record -> m.075fsd1 -> sports.sports_team_season_record.season -> 2008 NFL season\n# Answer:\nm.075fsd1", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.arena_stadium -> CenturyLink Field -> soccer.football_pitch.matches -> 2014 LA Galaxy vs Seattle Sounders\n# Answer:\nCenturyLink Field", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.location -> Seattle -> travel.travel_destination.tourist_attractions -> Pike Place Market\n# Answer:\nSeattle", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.location -> Seattle -> location.location.containedby -> Area code 206\n# Answer:\nSeattle"], "ground_truth": ["CenturyLink Field"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-785", "prediction": ["# Reasoning Path:\nMichael Keaton -> film.actor.film -> m.0jw4g9 -> film.performance.character -> Chick Hicks\n# Answer:\nm.0jw4g9", "# Reasoning Path:\nCars -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> World of Cars -> fictional_universe.fictional_universe.characters -> Chick Hicks\n# Answer:\nWorld of Cars", "# Reasoning Path:\nMichael Keaton -> film.actor.film -> g.11b6dn6h31\n# Answer:\ng.11b6dn6h31", "# Reasoning Path:\nMichael Keaton -> film.actor.film -> m.0jw4g9 -> film.performance.film -> Cars\n# Answer:\nm.0jw4g9", "# Reasoning Path:\nCars -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> World of Cars -> fictional_universe.fictional_universe.characters -> Bob Cutlass\n# Answer:\nWorld of Cars", "# Reasoning Path:\nCars -> film.film.starring -> m.0jw4g9 -> film.performance.character -> Chick Hicks\n# Answer:\nm.0jw4g9", "# Reasoning Path:\nMichael Keaton -> film.actor.film -> g.11b6pd5lf0\n# Answer:\ng.11b6pd5lf0", "# Reasoning Path:\nMichael Keaton -> film.actor.film -> m.0jw4g9 -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0jw4g9", "# Reasoning Path:\nCars -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> World of Cars -> fictional_universe.fictional_universe.characters -> Buzz Lightyear Car\n# Answer:\nWorld of Cars", "# Reasoning Path:\nCars -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> World of Cars -> fictional_universe.fictional_universe.locations -> Radiator Springs\n# Answer:\nWorld of Cars"], "ground_truth": ["Chick Hicks"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-787", "prediction": ["# Reasoning Path:\nRiviera -> location.location.containedby -> Las Vegas -> location.location.containedby -> Nevada\n# Answer:\nLas Vegas", "# Reasoning Path:\nRiviera -> location.location.containedby -> Las Vegas -> location.location.containedby -> Area codes 702 and 725\n# Answer:\nLas Vegas", "# Reasoning Path:\nRiviera -> location.location.containedby -> Las Vegas -> travel.travel_destination.tourist_attractions -> Adventuredome\n# Answer:\nLas Vegas", "# Reasoning Path:\nRiviera -> location.location.containedby -> Las Vegas -> location.location.containedby -> Clark County\n# Answer:\nLas Vegas", "# Reasoning Path:\nRiviera -> common.image.size -> m.02930wg\n# Answer:\nm.02930wg", "# Reasoning Path:\nRiviera -> location.location.containedby -> Las Vegas -> travel.travel_destination.tourist_attractions -> Bellagio\n# Answer:\nLas Vegas", "# Reasoning Path:\nRiviera -> location.location.containedby -> Las Vegas -> travel.travel_destination.tourist_attractions -> Chapel of the Flowers\n# Answer:\nLas Vegas", "# Reasoning Path:\nRiviera -> location.location.containedby -> Las Vegas -> film.film_location.featured_in_films -> 21\n# Answer:\nLas Vegas", "# Reasoning Path:\nRiviera -> common.image.size -> m.03s8648\n# Answer:\nm.03s8648", "# Reasoning Path:\nRiviera -> common.image.size -> m.041x0l3\n# Answer:\nm.041x0l3"], "ground_truth": ["Las Vegas"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-788", "prediction": ["# Reasoning Path:\nBuddhism -> religion.religion.texts -> Amitabha Sutra -> religion.religious_text.religious_text_of -> Mahayana\n# Answer:\nAmitabha Sutra", "# Reasoning Path:\nBuddhism -> religion.religion.texts -> Amitabha Sutra -> religion.religious_text.religious_text_of -> Pure Land Buddhism\n# Answer:\nAmitabha Sutra", "# Reasoning Path:\nBuddhism -> religion.religion.texts -> Amitabha Sutra -> common.topic.image -> Amitabha Sutra book\n# Answer:\nAmitabha Sutra", "# Reasoning Path:\nBuddhism -> religion.religion.texts -> Longchen Nyingthig -> common.topic.notable_types -> Religious Text\n# Answer:\nLongchen Nyingthig", "# Reasoning Path:\nBuddhism -> religion.religion.texts -> Amitabha Sutra -> common.topic.notable_for -> g.1259v001w\n# Answer:\nAmitabha Sutra", "# Reasoning Path:\nBuddhism -> religion.religion.texts -> Dhammapada -> common.topic.notable_types -> Religious Text\n# Answer:\nDhammapada", "# Reasoning Path:\nBuddhism -> base.skosbase.skos_concept.narrower -> Buddhist civilization\n# Answer:\nBuddhist civilization", "# Reasoning Path:\nBuddhism -> religion.religion.texts -> Longchen Nyingthig -> common.topic.notable_for -> g.125flk0bl\n# Answer:\nLongchen Nyingthig", "# Reasoning Path:\nBuddhism -> base.skosbase.skos_concept.narrower -> Buddhist fundamentalism\n# Answer:\nBuddhist fundamentalism", "# Reasoning Path:\nBuddhism -> religion.religion.texts -> Amitabha Sutra -> common.topic.image -> Sukhavati\n# Answer:\nAmitabha Sutra"], "ground_truth": ["Longchen Nyingthig", "\u015ar\u012bm\u0101l\u0101dev\u012b Si\u1e43han\u0101da S\u016btra", "Amitabha Sutra", "N\u012blaka\u1e47\u1e6dha Dh\u0101ra\u1e47\u012b", "Mah\u0101vastu", "Mah\u0101y\u0101na s\u016btras", "\u0100gama", "Tibetan Buddhist canon", "Dhammapada", "Vimalakirti Sutra", "Gang\u014dji Garan Engi", "Chinese Buddhist canon", "P\u0101li Canon", "U\u1e63\u1e47\u012b\u1e63a Vijaya Dh\u0101ra\u1e47\u012b S\u016btra"], "ans_acc": 0.21428571428571427, "ans_hit": 1, "ans_f1": 0.3380281690140845, "ans_precission": 0.8, "ans_recall": 0.21428571428571427, "path_f1": 0.3380281690140845, "path_precision": 0.8, "path_recall": 0.21428571428571427, "path_ans_f1": 0.3380281690140845, "path_ans_precision": 0.8, "path_ans_recall": 0.21428571428571427}
{"id": "WebQTest-789", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.religion -> Deism\n# Answer:\nJohn Tyler", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.nationality -> United States of America\n# Answer:\nJohn Tyler", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\nm.02bk8zt", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.religion -> Episcopal Church\n# Answer:\nJohn Tyler", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.children -> Lyon Gardiner Tyler\n# Answer:\nJohn Tyler", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.03qtjkt\n# Answer:\nm.03qtjkt", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.children -> Alice Tyler\n# Answer:\nJohn Tyler", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.children -> Anne Contesse Tyler\n# Answer:\nJohn Tyler", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.03fx87q -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nm.03fx87q", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.03fx87q -> government.government_position_held.basic_title -> President\n# Answer:\nm.03fx87q"], "ground_truth": ["John Tyler"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Abhiyaza\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_for -> g.12582gf_5\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> book.book_subject.works -> Remaking Buddhism for Medieval Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_types -> City/Town/Village\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Nanda\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> location.location.time_zones -> Nepal Time Zone\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> R\u0101hula\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> location.location.geolocation -> m.0wmmlzp\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> common.topic.article -> m.0hznzjg\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Tooth Relic Temple and Museum -> common.topic.webpage -> m.0gb1hxk\n# Answer:\nBuddha Tooth Relic Temple and Museum"], "ground_truth": ["Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-790", "prediction": ["# Reasoning Path:\nRichard Arkwright -> people.person.place_of_birth -> Preston, Lancashire -> location.location.containedby -> Lancashire\n# Answer:\nPreston, Lancashire", "# Reasoning Path:\nRichard Arkwright -> people.person.place_of_birth -> Preston, Lancashire -> location.location.contains -> Avenham Park\n# Answer:\nPreston, Lancashire", "# Reasoning Path:\nRichard Arkwright -> people.person.place_of_birth -> Preston, Lancashire -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nPreston, Lancashire", "# Reasoning Path:\nRichard Arkwright -> people.person.place_of_birth -> Preston, Lancashire -> location.location.containedby -> City of Preston, Lancashire\n# Answer:\nPreston, Lancashire", "# Reasoning Path:\nRichard Arkwright -> common.topic.notable_types -> Inventor -> base.descriptive_names.names.descriptive_name -> m.0101hljz\n# Answer:\nInventor", "# Reasoning Path:\nRichard Arkwright -> people.person.place_of_birth -> Preston, Lancashire -> location.location.contains -> Preston Central Methodist Church\n# Answer:\nPreston, Lancashire", "# Reasoning Path:\nRichard Arkwright -> people.person.place_of_birth -> Preston, Lancashire -> location.location.containedby -> England\n# Answer:\nPreston, Lancashire", "# Reasoning Path:\nRichard Arkwright -> people.person.place_of_birth -> Preston, Lancashire -> location.location.contains -> St Joseph's Church, Preston\n# Answer:\nPreston, Lancashire", "# Reasoning Path:\nRichard Arkwright -> common.topic.notable_types -> Inventor -> base.descriptive_names.names.descriptive_name -> m.0105_nsh\n# Answer:\nInventor", "# Reasoning Path:\nRichard Arkwright -> common.topic.notable_types -> Inventor -> base.descriptive_names.names.descriptive_name -> m.0105_nx4\n# Answer:\nInventor"], "ground_truth": ["Preston, Lancashire"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-791", "prediction": ["# Reasoning Path:\nLouis Sachar -> book.author.works_written -> The Boy Who Lost His Face -> common.topic.notable_types -> Book\n# Answer:\nThe Boy Who Lost His Face", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> The Boy Who Lost His Face -> book.book.editions -> Boy Who Lost His Face\n# Answer:\nThe Boy Who Lost His Face", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> The Cardturner -> award.award_nominated_work.award_nominations -> m.0g2978l\n# Answer:\nThe Cardturner", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> The Cardturner -> common.topic.article -> m.0fq08x4\n# Answer:\nThe Cardturner", "# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> common.topic.article -> m.03tgtz\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> The Boy Who Lost His Face -> book.book.editions -> The boy who lost his face\n# Answer:\nThe Boy Who Lost His Face", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> A Flying Birthday Cake? -> common.topic.notable_for -> g.11bbqmptyt\n# Answer:\nA Flying Birthday Cake?", "# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> film.film.production_companies -> Phoenix Pictures\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> common.topic.article -> m.0dq62d\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> A Flying Birthday Cake? -> common.topic.notable_types -> Book\n# Answer:\nA Flying Birthday Cake?"], "ground_truth": ["Small Steps", "Il y a un gar\u00e7on dans les toilettes des filles", "Someday Angeline", "Wayside School is Falling Down", "Dogs Don't Tell Jokes", "There's a boy in the girls bathroom", "Johnny's in the Basement", "Wayside School Gets A Little Stranger", "Sixth Grade Secrets", "Boy Who Lost His Face", "Hay Un Chico En El Bano De Las Chicas", "Stanley Yelnats Survival Guide to Camp Green Lake", "Wayside School is falling down", "Wayside School Is Falling Down", "Kidnapped at Birth?", "Class President (A Stepping Stone Book(TM))", "Super Fast, Out of Control! (A Stepping Stone Book(TM))", "Wayside School Boxed Set", "Johnny's in the basement", "Holes (Listening Library)", "Why Pick on Me? (A Stepping Stone Book(TM))", "Small Steps (Readers Circle)", "Louis Sacher Collection", "Holes (World Book Day 2001)", "Pequenos Pasos/ Small Steps", "Sideways Arithmetic from Wayside School", "A Flying Birthday Cake?", "Sideways Stories from Wayside School", "The Boy Who Lost His Face", "Sixth Grade Secrets (Apple Paperbacks)", "More Sideways Arithmetic from Wayside School", "Holes", "Sideways stories from Wayside School", "g.1218f5g0", "Stanley Yelnats' Survival Guide to Camp Green Lake", "Sixth Grade Secrets (An Apple Paperback)", "Holes (with \\\"Connections\\\") HRW Library (HRW library)", "Class President", "Why Pick on Me?", "Small steps", "Holes. (Lernmaterialien)", "Wayside School is falling down (Celebrate reading, Scott Foresman)", "Wayside School gets a little stranger", "Holes (Cascades)", "Super Fast, Out of Control!", "Hoyos/Holes", "Kidnapped at Birth? (A Stepping Stone Book(TM))", "Wayside School Gets a Little Stranger (rack) (Wayside School)", "More Sideways Arithmetic From Wayside School", "The boy who lost his face", "Wayside School Gets a Little Stranger", "Someday Angeline (Avon/Camelot Book)", "The Cardturner", "Marvin Redpost.", "Holes (Yearling Books)", "Sixth grade secrets", "Holes Activity Pack", "There's a Boy in the Girls' Bathroom", "Sideways Arithmetic From Wayside School", "Holes (Readers Circle)", "Wayside School Collection", "A Flying Birthday Cake? (A Stepping Stone Book(TM))", "Alone in His Teacher's House", "Marvin Redpost", "L\u00f6cher", "A magic crystal?", "Monkey soup", "Der Fluch des David Ballinger. ( Ab 11 J.)."], "ans_acc": 0.08823529411764706, "ans_hit": 1, "ans_f1": 0.1621621621621622, "ans_precission": 1.0, "ans_recall": 0.08823529411764706, "path_f1": 0.09225700164744645, "path_precision": 0.7, "path_recall": 0.04938271604938271, "path_ans_f1": 0.1621621621621622, "path_ans_precision": 1.0, "path_ans_recall": 0.08823529411764706}
{"id": "WebQTest-792", "prediction": ["# Reasoning Path:\nRoth IRA -> common.topic.article -> m.023_lv\n# Answer:\nm.023_lv", "# Reasoning Path:\nRoth IRA -> symbols.namesake.named_after -> William V. Roth, Jr. -> people.person.place_of_birth -> Great Falls\n# Answer:\nWilliam V. Roth, Jr.", "# Reasoning Path:\nRoth IRA -> symbols.namesake.named_after -> William V. Roth, Jr. -> book.author.works_written -> Complexity of the Individual Income Tax\n# Answer:\nWilliam V. Roth, Jr.", "# Reasoning Path:\nRoth IRA -> symbols.namesake.named_after -> William V. Roth, Jr. -> book.author.works_written -> Consequences of the Asian Financial Crisis\n# Answer:\nWilliam V. Roth, Jr.", "# Reasoning Path:\nRoth IRA -> symbols.namesake.named_after -> William V. Roth, Jr. -> book.author.works_written -> Increasing Savings for Retirement\n# Answer:\nWilliam V. Roth, Jr."], "ground_truth": ["William V. Roth, Jr."], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-794", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.actor -> James Earl Jones\n# Answer:\nm.03jr0x9", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.actor -> James Earl Jones\n# Answer:\nm.02nwtmz", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.special_performance_type -> Voice\n# Answer:\nm.03jr0x9", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.film -> The Benchwarmers\n# Answer:\nm.03jr0x9", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.film -> Return of the Jedi\n# Answer:\nm.02nwtmz", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.part_of_series -> Jedi Quest\n# Answer:\nPath to Truth"], "ground_truth": ["Abraham Benrubi", "Matt Lanter", "Zac Efron", "Dr. Smoov", "James Earl Jones"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.28571428571428564, "path_precision": 0.3, "path_recall": 0.2727272727272727, "path_ans_f1": 0.24, "path_ans_precision": 0.3, "path_ans_recall": 0.2}
{"id": "WebQTest-796", "prediction": ["# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Mathematician -> common.topic.notable_types -> Profession\n# Answer:\nMathematician", "# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Economist -> common.topic.notable_types -> Profession\n# Answer:\nEconomist", "# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Mathematician -> base.descriptive_names.names.descriptive_name -> m.01066gkh\n# Answer:\nMathematician", "# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Mathematician -> fictional_universe.character_occupation.characters_with_this_occupation -> Beremiz Samir\n# Answer:\nMathematician", "# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Economist -> base.schemastaging.context_name.pronunciation -> g.125_pmh72\n# Answer:\nEconomist", "# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Scientist -> common.topic.subject_of -> Autobiography\n# Answer:\nScientist", "# Reasoning Path:\nThomas Robert Malthus -> common.topic.image -> Thomas Malthus -> common.image.appears_in_topic_gallery -> Albert Allen Bartlett\n# Answer:\nThomas Malthus", "# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Mathematician -> base.descriptive_names.names.descriptive_name -> m.010f3g_r\n# Answer:\nMathematician", "# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Economist -> base.descriptive_names.names.descriptive_name -> m.0101nv80\n# Answer:\nEconomist", "# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Scientist -> tv.tv_subject.tv_programs -> The Secret Life of Scientists & Engineers\n# Answer:\nScientist"], "ground_truth": ["Scientist", "Mathematician", "Economist", "Demographer"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.8181818181818182, "path_precision": 0.9, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-797", "prediction": ["# Reasoning Path:\nNewark -> location.citytown.postal_codes -> 94560 -> location.location.geolocation -> m.03dnk73\n# Answer:\n94560", "# Reasoning Path:\nNewark -> location.citytown.postal_codes -> 94560 -> location.location.containedby -> Alameda County\n# Answer:\n94560", "# Reasoning Path:\nNewark -> location.location.containedby -> Alameda County -> location.us_county.hud_county_place -> Alameda\n# Answer:\nAlameda County", "# Reasoning Path:\nNewark -> location.statistical_region.population -> g.11b66cggvh\n# Answer:\ng.11b66cggvh", "# Reasoning Path:\nNewark -> location.citytown.postal_codes -> 94560 -> location.location.contains -> Jarvis Landing, California\n# Answer:\n94560", "# Reasoning Path:\nNewark -> location.statistical_region.population -> g.11bymn221g\n# Answer:\ng.11bymn221g", "# Reasoning Path:\nNewark -> location.citytown.postal_codes -> 94560 -> location.location.containedby -> California\n# Answer:\n94560", "# Reasoning Path:\nNewark -> location.location.containedby -> Alameda County -> location.us_county.county_seat -> Oakland\n# Answer:\nAlameda County", "# Reasoning Path:\nNewark -> location.statistical_region.population -> g.11x1cbcyw\n# Answer:\ng.11x1cbcyw", "# Reasoning Path:\nNewark -> location.citytown.postal_codes -> 94560 -> location.location.containedby -> United States of America\n# Answer:\n94560"], "ground_truth": ["94560"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-798", "prediction": ["# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> Mississippi\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> Alabama\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> meteorology.tropical_cyclone.affected_areas -> Mississippi\n# Answer:\nHurricane Bob", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> Kentucky\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.tropical_cyclone_season -> 1982 Atlantic hurricane season\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> meteorology.tropical_cyclone.affected_areas -> Kentucky\n# Answer:\nHurricane Bob", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> common.topic.notable_for -> g.1255tjcrg\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> 1915 New Orleans hurricane -> meteorology.tropical_cyclone.affected_areas -> Mississippi\n# Answer:\n1915 New Orleans hurricane", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> meteorology.tropical_cyclone.affected_areas -> Alabama\n# Answer:\nHurricane Bob", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> common.topic.notable_for -> g.1259xs2jv\n# Answer:\nHurricane Bob"], "ground_truth": ["Missouri", "Mississippi", "Alabama", "Georgia", "Kentucky", "North Carolina", "Virginia", "Arkansas"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.0736842105263158, "path_precision": 0.7, "path_recall": 0.03888888888888889, "path_ans_f1": 0.48837209302325574, "path_ans_precision": 0.7, "path_ans_recall": 0.375}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove -> common.topic.article -> m.029mn3\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> Glass\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> medicine.medical_treatment.used_to_treat -> Presbyopia\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_trwk\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nm.012zbkk5", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.image -> A bifocal corrective eyeglasses lens\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> BenFranklinDuplessis -> common.image.size -> m.02fglp1\n# Answer:\nBenFranklinDuplessis", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_ty__\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_wj2d\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> An Armonica\n# Answer:\nGlass harmonica"], "ground_truth": ["Lightning rod", "Glass harmonica", "Bifocals", "Franklin stove"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.8, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.8, "path_ans_recall": 0.75}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpsq_\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.us_county.hud_county_place -> Aristocrat Ranchettes\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpst3\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County -> location.location.adjoin_s -> m.03jpsq_\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado -> location.location.containedby -> United States of America\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpt45\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.us_county.hud_county_place -> Ault\n# Answer:\nWeld County"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-800", "prediction": ["# Reasoning Path:\nUnited States of America -> military.military_combatant.casualties -> m.043wpr_ -> military.casualties.type_of_casualties -> Military - Dead\n# Answer:\nm.043wpr_", "# Reasoning Path:\nUnited States of America -> military.military_combatant.casualties -> m.010h8rpj -> military.casualties.military_conflict -> Battle of the Little Bighorn\n# Answer:\nm.010h8rpj", "# Reasoning Path:\nUnited States of America -> military.military_combatant.casualties -> m.010h8rq6 -> military.casualties.military_conflict -> Battle of the Little Bighorn\n# Answer:\nm.010h8rq6", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\ng.125_r6z32", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nm.010r7f0z", "# Reasoning Path:\nUnited States of America -> military.military_combatant.casualties -> m.010h8rpj -> military.casualties.type_of_casualties -> Military - Dead\n# Answer:\nm.010h8rpj", "# Reasoning Path:\nUnited States of America -> military.military_combatant.casualties -> m.010h8rq6 -> military.casualties.type_of_casualties -> Military - Wounded\n# Answer:\nm.010h8rq6", "# Reasoning Path:\nUnited States of America -> periodicals.newspaper_circulation_area.newspapers -> American Free Press -> book.periodical.language -> English Language\n# Answer:\nAmerican Free Press", "# Reasoning Path:\nUnited States of America -> periodicals.newspaper_circulation_area.newspapers -> American Free Press -> common.topic.notable_types -> Newspaper\n# Answer:\nAmerican Free Press", "# Reasoning Path:\nUnited States of America -> periodicals.newspaper_circulation_area.newspapers -> C\u00f3ndor -> book.newspaper.circulation_areas -> Germany\n# Answer:\nC\u00f3ndor"], "ground_truth": ["World War II"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-801", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Syria\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> France\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\ng.12cp_k6sh", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Portugal\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\ng.1hhc3_4cn", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> common.topic.notable_types -> Governmental Body\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3f_h6\n# Answer:\ng.1hhc3f_h6", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nHouse of Representatives"], "ground_truth": ["Provisional government", "Semi-presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-802", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.role -> Vocals\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> Somebody to Love\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0115qhzk -> award.award_honor.award -> MTV Europe Music Voices Award\n# Answer:\nm.0115qhzk", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber's Believe -> film.film.personal_appearances -> m.0101fs_z\n# Answer:\nJustin Bieber's Believe", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> freebase.valuenotation.is_reviewed -> Award category\n# Answer:\nm.010lkp2z", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> award.award_honor.ceremony -> 2014 Billboard Music Awards\n# Answer:\nm.010lkp2z", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> award.award_honor.award -> Billboard Music Award for Top Social Artist\n# Answer:\nm.010lkp2z", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber's Believe -> film.film.personal_appearances -> m.0101fsyr\n# Answer:\nJustin Bieber's Believe"], "ground_truth": ["Vocals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-805", "prediction": ["# Reasoning Path:\nJohn Lennon -> people.deceased_person.place_of_death -> New York City -> music.composition.composer -> Yoko Ono\n# Answer:\nNew York City", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis -> film.film.directed_by -> Yoko Ono\n# Answer:\nApotheosis", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 1Club.FM: 70s (Pop) -> broadcast.content.location -> Chicago\n# Answer:\n1Club.FM: 70s (Pop)", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis -> film.film.starring -> m.0w0m5l0\n# Answer:\nApotheosis", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Clock -> film.film.directed_by -> Yoko Ono\n# Answer:\nClock", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 1Club.FM: 70s (Pop) -> common.topic.image -> 1clubfm.jpg\n# Answer:\n1Club.FM: 70s (Pop)", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis -> film.film.genre -> Short Film\n# Answer:\nApotheosis", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 1Club.FM: 70s (Pop) -> common.topic.notable_types -> Broadcast Content\n# Answer:\n1Club.FM: 70s (Pop)", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 1Club.FM: Folk -> broadcast.content.genre -> Country\n# Answer:\n1Club.FM: Folk", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 1Club.FM: Folk -> broadcast.content.location -> Chicago\n# Answer:\n1Club.FM: Folk"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-809", "prediction": ["# Reasoning Path:\nBangalore -> location.location.time_zones -> India Time Zone -> common.topic.image -> IST-CIA-TZ\n# Answer:\nIndia Time Zone", "# Reasoning Path:\nBangalore -> location.location.time_zones -> India Time Zone -> common.topic.notable_for -> g.1257lwlvq\n# Answer:\nIndia Time Zone", "# Reasoning Path:\nBangalore -> location.location.time_zones -> India Time Zone -> common.topic.image -> IST-Mirzapur\n# Answer:\nIndia Time Zone", "# Reasoning Path:\nBangalore -> location.location.time_zones -> India Time Zone -> freebase.valuenotation.has_no_value -> DST offset from UTC\n# Answer:\nIndia Time Zone", "# Reasoning Path:\nBangalore -> location.location.time_zones -> India Time Zone -> freebase.valuenotation.has_no_value -> Day DST begins\n# Answer:\nIndia Time Zone", "# Reasoning Path:\nBangalore -> travel.travel_destination.tourist_attractions -> Bangalore Palace -> common.topic.notable_for -> g.12564hvs2\n# Answer:\nBangalore Palace", "# Reasoning Path:\nBangalore -> location.location.time_zones -> India Time Zone -> freebase.valuenotation.has_no_value -> Day DST ends\n# Answer:\nIndia Time Zone", "# Reasoning Path:\nBangalore -> travel.travel_destination.tourist_attractions -> Cubbon Park -> location.location.time_zones -> India Time Zone\n# Answer:\nCubbon Park", "# Reasoning Path:\nBangalore -> travel.travel_destination.tourist_attractions -> Bangalore Palace -> location.location.geolocation -> m.0wms3ds\n# Answer:\nBangalore Palace", "# Reasoning Path:\nBangalore -> travel.travel_destination.tourist_attractions -> Bannerghatta National Park -> location.location.containedby -> India\n# Answer:\nBannerghatta National Park"], "ground_truth": ["India Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-810", "prediction": ["# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.main_country -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.region -> Southeast Europe\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Italy\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Greek Language -> media_common.netflix_genre.titles -> A Girl in Black\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Greek Language -> language.human_language.region -> Europe\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> media_common.netflix_genre.titles -> A Girl in Black\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> language.human_language.region -> Europe\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Montenegro\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Greek Language -> media_common.netflix_genre.titles -> A Matter of Dignity\n# Answer:\nGreek Language"], "ground_truth": ["Greek Language", "Albanian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-811", "prediction": ["# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Citizens Bank Park -> location.location.events -> 2008 National League Championship Series\n# Answer:\nCitizens Bank Park", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Citizens Bank Park -> architecture.structure.architect -> Stanley Cole\n# Answer:\nCitizens Bank Park", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Citizens Bank Park -> location.location.events -> 2008 World Series\n# Answer:\nCitizens Bank Park", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Baker Bowl -> common.topic.article -> m.038js6\n# Answer:\nBaker Bowl", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Citizens Bank Park -> location.location.events -> 2009 National League Championship Series\n# Answer:\nCitizens Bank Park", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Citizens Bank Park -> architecture.structure.contractor -> Don Todd Associates, Inc\n# Answer:\nCitizens Bank Park", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Bright House Field -> base.schemastaging.sports_facility_extra.training_ground_for -> m.0x25ctr\n# Answer:\nBright House Field", "# Reasoning Path:\nPhiladelphia Phillies -> sports.professional_sports_team.draft_picks -> m.04vw__p -> sports.sports_league_draft_pick.school -> University of Oklahoma\n# Answer:\nm.04vw__p", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Citizens Bank Park -> architecture.structure.contractor -> Hunt Construction\n# Answer:\nCitizens Bank Park", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Baker Bowl -> sports.sports_facility.home_venue_for -> m.0wz2_kv\n# Answer:\nBaker Bowl"], "ground_truth": ["Bright House Field"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-812", "prediction": ["# Reasoning Path:\nMatt Dallas -> film.actor.film -> m.0cg5qxx -> film.performance.film -> As Good as Dead\n# Answer:\nm.0cg5qxx", "# Reasoning Path:\nMatt Dallas -> film.actor.film -> m.0h2ky9r -> film.performance.film -> Wyatt Earp's Revenge\n# Answer:\nm.0h2ky9r", "# Reasoning Path:\nMatt Dallas -> film.actor.film -> m.0h2kycv -> film.performance.film -> Wannabe\n# Answer:\nm.0h2kycv", "# Reasoning Path:\nMatt Dallas -> film.actor.film -> m.0cg5qxx -> film.performance.character -> Jake\n# Answer:\nm.0cg5qxx", "# Reasoning Path:\nMatt Dallas -> film.actor.film -> m.0h2ky9r -> film.performance.character -> Bat Masterson\n# Answer:\nm.0h2ky9r", "# Reasoning Path:\nMatt Dallas -> people.person.spouse_s -> m.0vsbrys -> people.marriage.spouse -> Blue Hamilton\n# Answer:\nm.0vsbrys", "# Reasoning Path:\nMatt Dallas -> tv.tv_actor.guest_roles -> m.09nyhkx -> tv.tv_guest_role.episodes_appeared_in -> Matt Dallas, Geena Davis, Cesar Millan, guest co-host Mo'nique\n# Answer:\nm.09nyhkx", "# Reasoning Path:\nMatt Dallas -> film.actor.film -> m.0h2kycv -> film.performance.character -> Monkee Dancer #1\n# Answer:\nm.0h2kycv", "# Reasoning Path:\nMatt Dallas -> people.person.spouse_s -> m.0vsbrys -> freebase.valuenotation.has_value -> From\n# Answer:\nm.0vsbrys", "# Reasoning Path:\nMatt Dallas -> people.person.spouse_s -> m.0vsbrys -> people.marriage.type_of_union -> Domestic partnership\n# Answer:\nm.0vsbrys"], "ground_truth": ["The Story of Bonnie and Clyde", "The Ghost of Goodnight Lane", "Wannabe", "In Between Days", "Camp Slaughter", "You, Me & The Circus", "Babysitter Wanted", "Hot Dudes with Kittens", "As Good as Dead", "Naughty or Nice", "Beauty & the Briefcase", "The Indian", "Wyatt Earp's Revenge", "Way of the Vampire", "Living the Dream"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.23076923076923075, "path_precision": 0.3, "path_recall": 0.1875, "path_ans_f1": 0.24, "path_ans_precision": 0.3, "path_ans_recall": 0.2}
{"id": "WebQTest-813", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Joseph P. Kennedy, Sr. -> people.person.children -> Ted Kennedy\n# Answer:\nJoseph P. Kennedy, Sr.", "# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Joseph P. Kennedy, Sr. -> people.person.religion -> Catholicism\n# Answer:\nJoseph P. Kennedy, Sr.", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Caroline Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nCaroline Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Joseph P. Kennedy, Sr. -> people.person.children -> Eunice Kennedy Shriver\n# Answer:\nJoseph P. Kennedy, Sr.", "# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Joseph P. Kennedy, Sr. -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nJoseph P. Kennedy, Sr.", "# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Rose Kennedy -> people.person.parents -> John F. Fitzgerald\n# Answer:\nRose Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Joseph P. Kennedy, Sr. -> people.person.children -> Jean Kennedy Smith\n# Answer:\nJoseph P. Kennedy, Sr.", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> John F. Kennedy Jr. -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nJohn F. Kennedy Jr.", "# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Joseph P. Kennedy, Sr. -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nJoseph P. Kennedy, Sr."], "ground_truth": ["Joseph P. Kennedy, Jr.", "Kathleen Cavendish", "Arabella Kennedy", "Ted Kennedy", "Patricia Kennedy Lawford", "Robert F. Kennedy", "John F. Kennedy Jr.", "Rosemary Kennedy", "Joseph P. Kennedy, Sr.", "Rose Kennedy", "Caroline Kennedy", "Jean Kennedy Smith", "Eunice Kennedy Shriver", "Patrick Bouvier Kennedy"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.5263157894736842, "ans_precission": 1.0, "ans_recall": 0.35714285714285715, "path_f1": 0.2870159453302962, "path_precision": 0.9, "path_recall": 0.17073170731707318, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 1.0, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-814", "prediction": ["# Reasoning Path:\nHarley-Davidson Motor Company -> organization.organization.headquarters -> m.03v5b_z -> location.mailing_address.citytown -> Milwaukee\n# Answer:\nm.03v5b_z", "# Reasoning Path:\nHarley-Davidson Motor Company -> organization.organization.headquarters -> m.03v5b_z -> location.mailing_address.postal_code -> 53208\n# Answer:\nm.03v5b_z", "# Reasoning Path:\nHarley-Davidson Motor Company -> organization.organization.headquarters -> m.03v5b_z -> location.location.geolocation -> m.04np6h4\n# Answer:\nm.03v5b_z", "# Reasoning Path:\nHarley-Davidson Motor Company -> organization.organization.place_founded -> Milwaukee -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMilwaukee", "# Reasoning Path:\nHarley-Davidson Motor Company -> organization.organization.place_founded -> Milwaukee -> travel.travel_destination.tourist_attractions -> Harley-Davidson Museum\n# Answer:\nMilwaukee", "# Reasoning Path:\nHarley-Davidson Motor Company -> base.motorcycle.motorcycle_part_manufacturer.part_numbers_of_manufactured_products -> 25343-04A -> base.motorcycle.motorcycle_part_number_manufacturer.motorcycle_part -> Derby cover\n# Answer:\n25343-04A", "# Reasoning Path:\nHarley-Davidson Motor Company -> base.motorcycle.motorcycle_part_manufacturer.part_numbers_of_manufactured_products -> 25343-04A -> common.topic.webpage -> m.03hqz98\n# Answer:\n25343-04A", "# Reasoning Path:\nHarley-Davidson Motor Company -> base.motorcycle.motorcycle_part_manufacturer.part_numbers_of_manufactured_products -> 32124-04 -> base.motorcycle.motorcycle_part_number_manufacturer.motorcycle_part -> Ignition system\n# Answer:\n32124-04", "# Reasoning Path:\nHarley-Davidson Motor Company -> base.motorcycle.motorcycle_part_manufacturer.part_numbers_of_manufactured_products -> 32124-04B -> common.topic.article -> m.03hr6z1\n# Answer:\n32124-04B", "# Reasoning Path:\nHarley-Davidson Motor Company -> base.motorcycle.motorcycle_part_manufacturer.part_numbers_of_manufactured_products -> 32124-04B -> base.motorcycle.motorcycle_part_number_manufacturer.motorcycle_part -> Ignition system\n# Answer:\n32124-04B"], "ground_truth": ["Milwaukee"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-818", "prediction": ["# Reasoning Path:\nOttoman Empire -> location.location.containedby -> Near East -> location.location.containedby -> Western Asia\n# Answer:\nNear East", "# Reasoning Path:\nOttoman Empire -> location.statistical_region.population -> m.04k6wf7\n# Answer:\nm.04k6wf7", "# Reasoning Path:\nOttoman Empire -> location.location.containedby -> Near East -> location.location.events -> Arab\u2013Israeli conflict\n# Answer:\nNear East", "# Reasoning Path:\nOttoman Empire -> location.location.containedby -> Near East -> book.book_subject.works -> Arabic political memoirs and other studies\n# Answer:\nNear East", "# Reasoning Path:\nOttoman Empire -> location.statistical_region.population -> m.04k6wff\n# Answer:\nm.04k6wff", "# Reasoning Path:\nOttoman Empire -> location.location.containedby -> Near East -> location.location.events -> Campaigns of Suleiman the Magnificent\n# Answer:\nNear East", "# Reasoning Path:\nOttoman Empire -> location.statistical_region.population -> m.04k6wfn\n# Answer:\nm.04k6wfn", "# Reasoning Path:\nOttoman Empire -> location.location.containedby -> Near East -> location.location.events -> First Crusade\n# Answer:\nNear East", "# Reasoning Path:\nOttoman Empire -> location.location.containedby -> Near East -> book.book_subject.works -> Bad moon rising\n# Answer:\nNear East", "# Reasoning Path:\nOttoman Empire -> location.location.containedby -> Near East -> book.book_subject.works -> Baghdad sketches\n# Answer:\nNear East"], "ground_truth": ["Near East"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-819", "prediction": ["# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Ali Azmat\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> Osama: The Making of a Terrorist -> book.written_work.subjects -> Al-Qaeda\n# Answer:\nOsama: The Making of a Terrorist", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.events -> Death of Osama bin Laden\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Amrit Singh Thapar\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.contains -> Abbottabad Hockey Stadium\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Faiza Khan\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> What Death of Osama Bin Laden Indicates About Barack Obama's Leadership -> book.written_work.subjects -> Death\n# Answer:\nWhat Death of Osama Bin Laden Indicates About Barack Obama's Leadership", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.contains -> Abbottabad International Medical College\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> Osama: The Making of a Terrorist -> common.topic.notable_types -> Book\n# Answer:\nOsama: The Making of a Terrorist", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.contains -> Abbottabad Public School\n# Answer:\nAbbottabad"], "ground_truth": ["Abbottabad"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> common.topic.notable_types -> Profession\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician -> people.profession.specializations -> Composer\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> common.topic.notable_types -> Composer -> common.topic.notable_types -> Profession\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> Music\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> base.descriptive_names.names.descriptive_name -> m.01260py_\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician -> common.topic.notable_types -> Profession\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> people.profession.specialization_of -> Writer\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> common.topic.notable_types -> Composer -> type.type.expected_by -> composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> Arranger\n# Answer:\nComposer"], "ground_truth": ["Musician", "Librettist", "Composer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-820", "prediction": ["# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Deathly Hallows -> fictional_universe.work_of_fiction.setting -> Wizarding world\n# Answer:\nHarry Potter and the Deathly Hallows", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Deathly Hallows -> book.written_work.previous_in_series -> Harry Potter and the Half-Blood Prince\n# Answer:\nHarry Potter and the Deathly Hallows", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Chamber of Secrets -> book.written_work.subjects -> England\n# Answer:\nHarry Potter and the Chamber of Secrets", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Deathly Hallows -> media_common.adapted_work.adaptations -> Harry Potter and the Deathly Hallows - Part I\n# Answer:\nHarry Potter and the Deathly Hallows", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Chamber of Secrets -> book.written_work.subjects -> Magic\n# Answer:\nHarry Potter and the Chamber of Secrets", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Chamber of Secrets -> film.film.release_date_s -> m.0vsq821\n# Answer:\nHarry Potter and the Chamber of Secrets", "# Reasoning Path:\nHarry Potter -> film.film_series.films_in_series -> Harry Potter and the Chamber of Secrets -> book.written_work.subjects -> England\n# Answer:\nHarry Potter and the Chamber of Secrets", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Deathly Hallows -> media_common.adapted_work.adaptations -> Harry Potter and the Deathly Hallows \u2013 Part 2\n# Answer:\nHarry Potter and the Deathly Hallows", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Chamber of Secrets -> book.written_work.subjects -> Schools\n# Answer:\nHarry Potter and the Chamber of Secrets", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Chamber of Secrets -> award.award_nominated_work.award_nominations -> m.03ml_z3\n# Answer:\nHarry Potter and the Chamber of Secrets"], "ground_truth": ["Harry Potter and the Chamber of Secrets", "Harry Potter and the Half-Blood Prince", "Harry Potter and the Prisoner of Azkaban", "Harry Potter and the Deathly Hallows \u2013 Part 2", "Harry Potter and the Philosopher's Stone", "Harry Potter and the Deathly Hallows - Part I", "Harry Potter and the Goblet of Fire", "Harry Potter and the Order of the Phoenix"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.20689655172413793, "ans_precission": 0.6, "ans_recall": 0.125, "path_f1": 0.2, "path_precision": 0.5, "path_recall": 0.125, "path_ans_f1": 0.6428571428571429, "path_ans_precision": 0.9, "path_ans_recall": 0.5}
{"id": "WebQTest-821", "prediction": ["# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Drug overdose -> people.cause_of_death.includes_causes_of_death -> Cocaine overdose\n# Answer:\nDrug overdose", "# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Heroin overdose -> people.cause_of_death.people -> Alice Ormsby-Gore\n# Answer:\nHeroin overdose", "# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Heroin overdose -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nHeroin overdose", "# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Drug overdose -> people.cause_of_death.includes_causes_of_death -> Heroin overdose\n# Answer:\nDrug overdose", "# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Heroin overdose -> people.cause_of_death.people -> Andrew Stewart\n# Answer:\nHeroin overdose", "# Reasoning Path:\nPamela Courson -> common.topic.notable_for -> g.12552mkbz\n# Answer:\ng.12552mkbz", "# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Heroin overdose -> people.cause_of_death.people -> Andrew Wood\n# Answer:\nHeroin overdose", "# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Heroin overdose -> people.cause_of_death.parent_cause_of_death -> Assassination by 'Accident'\n# Answer:\nHeroin overdose", "# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Drug overdose -> people.cause_of_death.parent_cause_of_death -> Assassination by 'Accident'\n# Answer:\nDrug overdose", "# Reasoning Path:\nPamela Courson -> common.topic.notable_for -> g.12599js9g\n# Answer:\ng.12599js9g"], "ground_truth": ["Drug overdose", "Heroin overdose"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-822", "prediction": ["# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Painter -> common.topic.notable_types -> Profession\n# Answer:\nPainter", "# Reasoning Path:\nJack Kevorkian -> common.topic.article -> m.01ctkp\n# Answer:\nm.01ctkp", "# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Musician -> common.topic.notable_types -> Profession\n# Answer:\nMusician", "# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Physician -> common.topic.notable_types -> Profession\n# Answer:\nPhysician", "# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Painter -> fictional_universe.character_occupation.characters_with_this_occupation -> Emerson Blackwood\n# Answer:\nPainter", "# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Musician -> freebase.equivalent_topic.equivalent_type -> Musical Artist\n# Answer:\nMusician", "# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Painter -> film.film_job.films_with_this_crew_job -> m.0102zqt0\n# Answer:\nPainter", "# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Musician -> common.topic.subjects -> MA$TADON\n# Answer:\nMusician", "# Reasoning Path:\nJack Kevorkian -> common.topic.notable_types -> Physician -> common.topic.notable_types -> Profession\n# Answer:\nPhysician", "# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Painter -> fictional_universe.character_occupation.characters_with_this_occupation -> Eug\u00e8ne Delacroix\n# Answer:\nPainter"], "ground_truth": ["Assisted suicide", "Right to die"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-823", "prediction": ["# Reasoning Path:\nBolivia -> location.location.containedby -> Americas -> location.location.contains -> Peru\n# Answer:\nAmericas", "# Reasoning Path:\nBolivia -> location.location.containedby -> Americas -> location.location.contains -> Venezuela\n# Answer:\nAmericas", "# Reasoning Path:\nBolivia -> location.location.containedby -> Americas -> location.location.time_zones -> Bolivia\u00a0Time Zone\n# Answer:\nAmericas", "# Reasoning Path:\nBolivia -> location.location.containedby -> Americas -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nAmericas", "# Reasoning Path:\nBolivia -> location.location.containedby -> South America -> travel.travel_destination.tour_operators -> Bunnik Tours\n# Answer:\nSouth America", "# Reasoning Path:\nBolivia -> location.location.containedby -> Latin America -> location.location.contains -> Peru\n# Answer:\nLatin America", "# Reasoning Path:\nBolivia -> location.location.containedby -> Americas -> location.location.contains -> Argentina\n# Answer:\nAmericas", "# Reasoning Path:\nBolivia -> location.location.containedby -> Latin America -> location.location.containedby -> Americas\n# Answer:\nLatin America", "# Reasoning Path:\nBolivia -> location.location.partially_contains -> Abuna River -> location.location.partially_containedby -> Brazil\n# Answer:\nAbuna River", "# Reasoning Path:\nBolivia -> location.location.containedby -> South America -> location.location.partially_contains -> Ecuador\n# Answer:\nSouth America"], "ground_truth": ["Paraguay", "Brazil", "Peru", "Argentina", "Chile"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.06106870229007634, "path_precision": 0.4, "path_recall": 0.03305785123966942, "path_ans_f1": 0.48, "path_ans_precision": 0.4, "path_ans_recall": 0.6}
{"id": "WebQTest-825", "prediction": ["# Reasoning Path:\nNolan Ryan -> baseball.baseball_player.batting_stats -> m.06sdlhm -> baseball.batting_statistics.team -> Houston Astros\n# Answer:\nm.06sdlhm", "# Reasoning Path:\nNolan Ryan -> baseball.baseball_player.batting_stats -> m.06sdlmg -> baseball.batting_statistics.team -> Houston Astros\n# Answer:\nm.06sdlmg", "# Reasoning Path:\nNolan Ryan -> baseball.baseball_player.batting_stats -> m.06sdl5r -> baseball.batting_statistics.team -> Houston Astros\n# Answer:\nm.06sdl5r", "# Reasoning Path:\nNolan Ryan -> baseball.baseball_player.batting_stats -> m.06sdlhm -> baseball.batting_statistics.season -> 1986 Major League Baseball Season\n# Answer:\nm.06sdlhm", "# Reasoning Path:\nNolan Ryan -> baseball.baseball_player.batting_stats -> m.06sdlmg -> baseball.batting_statistics.season -> 1988 Major League Baseball Season\n# Answer:\nm.06sdlmg", "# Reasoning Path:\nNolan Ryan -> baseball.baseball_player.batting_stats -> m.06sdl5r -> baseball.batting_statistics.season -> 1980 Major League Baseball Season\n# Answer:\nm.06sdl5r", "# Reasoning Path:\nNolan Ryan -> people.person.places_lived -> m.03pkl3k -> people.place_lived.location -> Texas\n# Answer:\nm.03pkl3k", "# Reasoning Path:\nNolan Ryan -> sports.pro_athlete.teams -> m.05yvwy6 -> sports.sports_team_roster.position -> Pitcher\n# Answer:\nm.05yvwy6", "# Reasoning Path:\nNolan Ryan -> sports.pro_athlete.teams -> m.05yvwyc -> sports.sports_team_roster.team -> Houston Astros\n# Answer:\nm.05yvwyc", "# Reasoning Path:\nNolan Ryan -> sports.pro_athlete.teams -> m.05yvwy6 -> sports.sports_team_roster.team -> New York Mets\n# Answer:\nm.05yvwy6"], "ground_truth": ["Houston Astros", "California Angels"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.27586206896551724, "path_precision": 0.4, "path_recall": 0.21052631578947367, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.4, "path_ans_recall": 0.5}
{"id": "WebQTest-827", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.administrative_divisions -> Ontario\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Canada\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> common.topic.notable_types -> Country\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> people.person.ethnicity -> Canadian -> common.topic.notable_types -> Ethnicity\n# Answer:\nCanadian", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.languages_spoken -> English Language\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> common.topic.notable_types -> City/Town/Village\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.ethnicity -> Canadian -> common.topic.notable_for -> g.1255pjt30\n# Answer:\nCanadian", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.languages_spoken -> Spanish Language\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Ontario\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.ethnicity -> Canadian -> people.ethnicity.includes_groups -> Scottish Canadian\n# Answer:\nCanadian"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-828", "prediction": ["# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> people.appointed_role.appointment -> m.0hn35mm\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> people.appointed_role.appointment -> m.063dqnv\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> law.judicial_title.judges -> m.046x3bs\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Making Our Democracy Work -> book.written_work.author -> Stephen Breyer\n# Answer:\nMaking Our Democracy Work", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> people.appointed_role.appointment -> m.0hmsrqy\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> common.topic.image -> Official roberts CJ\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> law.judicial_title.judges -> m.05lwk8w\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Courtiers of the Marble Palace: The Rise and Influence of the Supreme Court Law Clerk -> book.written_work.subjects -> Law clerk\n# Answer:\nCourtiers of the Marble Palace: The Rise and Influence of the Supreme Court Law Clerk", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> law.judicial_title.judges -> m.0cv2v4z\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> common.topic.image -> Seal of the Supreme Court\n# Answer:\nChief Justice of the United States"], "ground_truth": ["Chief Justice of the United States"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-829", "prediction": ["# Reasoning Path:\nFrida Kahlo -> influence.influence_node.influenced -> Arturo Estrada Hern\u00e1ndez -> influence.influence_node.influenced_by -> Diego Rivera\n# Answer:\nArturo Estrada Hern\u00e1ndez", "# Reasoning Path:\nFrida Kahlo -> media_common.netflix_title.netflix_genres -> Art & Design\n# Answer:\nArt & Design", "# Reasoning Path:\nFrida Kahlo -> influence.influence_node.influenced -> Arturo Estrada Hern\u00e1ndez -> people.person.nationality -> Mexico\n# Answer:\nArturo Estrada Hern\u00e1ndez", "# Reasoning Path:\nFrida Kahlo -> influence.influence_node.influenced -> Mark Ryden -> influence.influence_node.influenced_by -> Balthus\n# Answer:\nMark Ryden", "# Reasoning Path:\nFrida Kahlo -> film.film.genre -> Biographical film\n# Answer:\nBiographical film", "# Reasoning Path:\nFrida Kahlo -> influence.influence_node.influenced -> Arturo Estrada Hern\u00e1ndez -> visual_art.visual_artist.associated_periods_or_movements -> Mexican muralism\n# Answer:\nArturo Estrada Hern\u00e1ndez", "# Reasoning Path:\nFrida Kahlo -> media_common.netflix_title.netflix_genres -> Art and Design\n# Answer:\nArt and Design", "# Reasoning Path:\nFrida Kahlo -> influence.influence_node.influenced -> Mark Ryden -> influence.influence_node.influenced_by -> Hieronymus Bosch\n# Answer:\nMark Ryden", "# Reasoning Path:\nFrida Kahlo -> media_common.netflix_title.netflix_genres -> Documentary film\n# Answer:\nDocumentary film", "# Reasoning Path:\nFrida Kahlo -> film.film.genre -> Documentary film\n# Answer:\nDocumentary film"], "ground_truth": ["Diego Rivera"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Andreas Bielau\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Andreas Diebitz\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> olympics.olympic_participating_country.athletes -> m.04dq7vv\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Arkadii Dragomoshchenko\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics\n# Answer:\nAustria", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Liechtenstein\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> olympics.olympic_participating_country.athletes -> m.04dq8sf\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Eschen\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.country.first_level_divisions -> Balzers\n# Answer:\nLiechtenstein"], "ground_truth": ["Belgium", "Germany", "Luxembourg", "East Germany", "Austria", "Switzerland", "Liechtenstein"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.6990291262135921, "ans_precission": 0.9, "ans_recall": 0.5714285714285714, "path_f1": 0.5806451612903225, "path_precision": 0.9, "path_recall": 0.42857142857142855, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 1.0, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-830", "prediction": ["# Reasoning Path:\nTaylor Swift -> people.person.place_of_birth -> Reading -> location.location.containedby -> Berks County\n# Answer:\nReading", "# Reasoning Path:\nTaylor Swift -> people.person.place_of_birth -> Reading -> base.biblioness.bibs_location.state -> Pennsylvania\n# Answer:\nReading", "# Reasoning Path:\nTaylor Swift -> people.person.place_of_birth -> Reading -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nReading", "# Reasoning Path:\nTaylor Swift -> people.person.place_of_birth -> Reading -> location.location.containedby -> Pennsylvania\n# Answer:\nReading", "# Reasoning Path:\nTaylor Swift -> celebrities.celebrity.sexual_relationships -> m.0t54pz9 -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nm.0t54pz9", "# Reasoning Path:\nTaylor Swift -> celebrities.celebrity.sexual_relationships -> m.0t54pz9 -> celebrities.romantic_relationship.celebrity -> Joe Jonas\n# Answer:\nm.0t54pz9", "# Reasoning Path:\nTaylor Swift -> celebrities.celebrity.sexual_relationships -> m.0pcnxtx -> celebrities.romantic_relationship.celebrity -> Harry Styles\n# Answer:\nm.0pcnxtx", "# Reasoning Path:\nTaylor Swift -> base.popstra.celebrity.friendship -> m.06461gz -> base.popstra.friendship.participant -> Selena Gomez\n# Answer:\nm.06461gz", "# Reasoning Path:\nTaylor Swift -> celebrities.celebrity.sexual_relationships -> m.0pcnxtx -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nm.0pcnxtx", "# Reasoning Path:\nTaylor Swift -> base.popstra.celebrity.friendship -> m.06461h7 -> base.popstra.friendship.participant -> Miley Cyrus\n# Answer:\nm.06461h7"], "ground_truth": ["Reading"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-831", "prediction": ["# Reasoning Path:\nArgentina -> location.statistical_region.religions -> m.05bp679 -> location.religion_percentage.religion -> Catholicism\n# Answer:\nm.05bp679", "# Reasoning Path:\nArgentina -> location.statistical_region.religions -> m.05bp67g -> location.religion_percentage.religion -> Judaism\n# Answer:\nm.05bp67g", "# Reasoning Path:\nArgentina -> location.statistical_region.religions -> m.05bp674 -> location.religion_percentage.religion -> Protestantism\n# Answer:\nm.05bp674", "# Reasoning Path:\nArgentina -> location.location.partially_contains -> Alto San Juan -> location.location.partially_contained_by -> m.0wg8lvc\n# Answer:\nAlto San Juan", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Asiatic Boy -> base.thoroughbredracing.thoroughbred_racehorse.color -> Chestnut\n# Answer:\nAsiatic Boy", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Asiatic Boy -> biology.organism.sex -> Male\n# Answer:\nAsiatic Boy", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Candy Ride -> biology.organism.organism_type -> Horse\n# Answer:\nCandy Ride", "# Reasoning Path:\nArgentina -> location.location.partially_contains -> Alto San Juan -> location.location.partially_containedby -> Chile\n# Answer:\nAlto San Juan", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Bayakoa -> base.thoroughbredracing.thoroughbred_racehorse.color -> Bay\n# Answer:\nBayakoa", "# Reasoning Path:\nArgentina -> location.location.partially_contains -> Alto San Juan -> common.topic.notable_types -> Mountain\n# Answer:\nAlto San Juan"], "ground_truth": ["Catholicism", "Judaism", "Protestantism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-832", "prediction": ["# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.championships -> 2014 World Series -> sports.sports_championship_event.runner_up -> Kansas City Royals\n# Answer:\n2014 World Series", "# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.championships -> 2014 World Series -> sports.sports_championship_event.championship -> World Series\n# Answer:\n2014 World Series", "# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.championships -> 2014 World Series -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n2014 World Series", "# Reasoning Path:\nSan Francisco Giants -> time.participant.event -> 2010 Major League Baseball season -> common.topic.notable_types -> Sports League Season\n# Answer:\n2010 Major League Baseball season", "# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.championships -> 2012 World Series -> common.topic.notable_for -> g.125fqv3d3\n# Answer:\n2012 World Series", "# Reasoning Path:\nSan Francisco Giants -> time.participant.event -> 2010 Major League Baseball season -> sports.sports_league_season.championship -> 2010 World Series\n# Answer:\n2010 Major League Baseball season", "# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.championships -> 2010 World Series -> time.event.locations -> AT&T Park\n# Answer:\n2010 World Series", "# Reasoning Path:\nSan Francisco Giants -> time.participant.event -> 2012 Major League Baseball season -> sports.sports_league_season.league -> Major League Baseball\n# Answer:\n2012 Major League Baseball season", "# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.arena_stadium -> Candlestick Park -> common.topic.notable_for -> g.125fwckfk\n# Answer:\nCandlestick Park", "# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.arena_stadium -> AT&T Park -> location.location.events -> 2002 San Francisco Bowl\n# Answer:\nAT&T Park"], "ground_truth": ["2014 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-833", "prediction": ["# Reasoning Path:\nChina -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> France\n# Answer:\nEarth", "# Reasoning Path:\nChina -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> Asia\n# Answer:\nEarth", "# Reasoning Path:\nChina -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> Sovereign state\n# Answer:\nEarth", "# Reasoning Path:\nChina -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> United States of America\n# Answer:\nEarth", "# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> language.human_language.countries_spoken_in -> Macau\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> common.topic.notable_types -> Human Language\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> language.human_language.countries_spoken_in -> New Zealand\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Balikun horse -> common.topic.notable_types -> Organism Classification\n# Answer:\nBalikun horse", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Black Telescope -> biology.animal_breed.breed_of -> Goldfish\n# Answer:\nBlack Telescope"], "ground_truth": ["Standard Chinese"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-834", "prediction": ["# Reasoning Path:\nBattle of Waterloo -> time.event.locations -> Waterloo -> book.written_work.subjects -> Belgium\n# Answer:\nWaterloo", "# Reasoning Path:\nBattle of Waterloo -> time.event.locations -> Waterloo -> book.written_work.subjects -> Europe\n# Answer:\nWaterloo", "# Reasoning Path:\nBattle of Waterloo -> time.event.locations -> Waterloo -> location.location.people_born_here -> Arthur Tauchert\n# Answer:\nWaterloo", "# Reasoning Path:\nBattle of Waterloo -> base.ontologies.ontology_instance.equivalent_instances -> m.09bxfm0 -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nm.09bxfm0", "# Reasoning Path:\nBattle of Waterloo -> time.event.locations -> Waterloo -> book.written_work.subjects -> France\n# Answer:\nWaterloo", "# Reasoning Path:\nBattle of Waterloo -> time.event.locations -> Waterloo -> location.location.geolocation -> m.0239s57\n# Answer:\nWaterloo", "# Reasoning Path:\nBattle of Waterloo -> time.event.locations -> Waterloo -> location.location.people_born_here -> Clarrie Stevenson\n# Answer:\nWaterloo", "# Reasoning Path:\nBattle of Waterloo -> time.event.locations -> Waterloo -> location.location.people_born_here -> Dick Daley\n# Answer:\nWaterloo", "# Reasoning Path:\nBattle of Waterloo -> time.event.locations -> Waterloo -> location.location.geolocation -> m.0cpd0qs\n# Answer:\nWaterloo", "# Reasoning Path:\nBattle of Waterloo -> military.military_conflict.military_personnel_involved -> William II of the Netherlands -> people.person.place_of_birth -> Noordeinde Palace\n# Answer:\nWilliam II of the Netherlands"], "ground_truth": ["Waterloo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-836", "prediction": ["# Reasoning Path:\nVladimir Lenin -> base.activism.activist.area_of_activism -> Atheism -> base.activism.activism_issue.activists -> Joseph Stalin\n# Answer:\nAtheism", "# Reasoning Path:\nVladimir Lenin -> base.activism.activist.area_of_activism -> Atheism -> base.activism.activism_issue.activists -> Karl Marx\n# Answer:\nAtheism", "# Reasoning Path:\nVladimir Lenin -> base.activism.activist.area_of_activism -> Atheism -> base.activism.activism_issue.activists -> Bertrand Russell\n# Answer:\nAtheism", "# Reasoning Path:\nVladimir Lenin -> base.activism.activist.area_of_activism -> Atheism -> organization.organization_sector.organizations_in_this_sector -> American Atheists\n# Answer:\nAtheism", "# Reasoning Path:\nVladimir Lenin -> people.person.parents -> Ilya Ulyanov -> people.person.spouse_s -> m.0j81_b0\n# Answer:\nIlya Ulyanov", "# Reasoning Path:\nVladimir Lenin -> base.activism.activist.area_of_activism -> Atheism -> book.book_subject.works -> God Is Not Great: How Religion Poisons Everything\n# Answer:\nAtheism", "# Reasoning Path:\nVladimir Lenin -> base.activism.activist.area_of_activism -> Atheism -> organization.organization_sector.organizations_in_this_sector -> Atheist Alliance International\n# Answer:\nAtheism", "# Reasoning Path:\nVladimir Lenin -> people.person.parents -> Maria Alexandrovna Ulyanova -> people.person.children -> Alexandre Ilich Ulianov\n# Answer:\nMaria Alexandrovna Ulyanova", "# Reasoning Path:\nVladimir Lenin -> base.activism.activist.area_of_activism -> Atheism -> book.book_subject.works -> The Portable Atheist\n# Answer:\nAtheism", "# Reasoning Path:\nVladimir Lenin -> people.person.parents -> Ilya Ulyanov -> common.topic.notable_types -> Deceased Person\n# Answer:\nIlya Ulyanov"], "ground_truth": ["Nadezhda Krupskaya"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-837", "prediction": ["# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> sports.sports_championship_event.season -> 2000 NFL season\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> common.topic.notable_types -> Super bowl\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.article -> m.0642vqz\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> time.event.locations -> Raymond James Stadium\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> 2013 AFC Championship Game -> time.event.instance_of_recurring_event -> AFC Championship Game\n# Answer:\n2013 AFC Championship Game", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> film.film_subject.films -> The Band That Wouldn't Die -> film.film.music -> Marcello Zavros\n# Answer:\nThe Band That Wouldn't Die", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> sports.sports_championship_event.season -> 2012 NFL season\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> 2013 AFC Championship Game -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n2013 AFC Championship Game", "# Reasoning Path:\nBaltimore Ravens -> film.film_subject.films -> The Band That Wouldn't Die -> common.topic.image -> Baltimore Ravens Marching Band\n# Answer:\nThe Band That Wouldn't Die"], "ground_truth": ["Super Bowl XLVII", "Super Bowl XXXV"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-838", "prediction": ["# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nm.010dzr92", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.match -> 2014 Real Madrid CF vs. CA Osasuna football match\n# Answer:\nm.010dzr92", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzrlj -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nm.010dzrlj", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010p4jhc -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nm.010p4jhc", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzrlj -> soccer.football_goal.match -> 2014 Real Madrid CF vs. CA Osasuna football match\n# Answer:\nm.010dzrlj", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010p4jhc -> soccer.football_goal.match -> 2014 UEFA Champions League Final\n# Answer:\nm.010p4jhc", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w8w78v -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0w8w78v", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w8w78v -> soccer.football_player_stats.team -> Real Madrid C.F.\n# Answer:\nm.0w8w78v", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w9gdc1 -> soccer.football_player_stats.team -> Manchester United F.C.\n# Answer:\nm.0w9gdc1", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w8w7f0 -> soccer.football_player_stats.team -> Portugal national football team\n# Answer:\nm.0w8w7f0"], "ground_truth": ["Real Madrid C.F.", "Portugal national football team"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3333333333333333, "path_precision": 0.5, "path_recall": 0.25, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> common.topic.notable_types -> Musical genre\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> music.genre.subgenre -> Math rock\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Rock music -> award.hall_of_fame_discipline.halls_of_fame -> Rock and Roll Hall of Fame\n# Answer:\nRock music", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis -> film.film.genre -> Short Film\n# Answer:\nApotheosis", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Rock music -> common.topic.notable_types -> Musical genre\n# Answer:\nRock music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Rock music -> music.genre.parent_genre -> Folk music\n# Answer:\nRock music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Pop rock\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> common.topic.notable_types -> Musical genre\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> .997 Radiostorm Oldies -> broadcast.content.genre -> Rock music\n# Answer:\n.997 Radiostorm Oldies", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.recordings -> Chasing a Feeling\n# Answer:\nSoft rock"], "ground_truth": ["Experimental music", "Rock music", "Pop rock", "Art rock", "Blues rock", "Pop music", "Soft rock", "Experimental rock", "Psychedelic rock"], "ans_acc": 0.4444444444444444, "ans_hit": 1, "ans_f1": 0.47058823529411764, "ans_precission": 0.8, "ans_recall": 0.3333333333333333, "path_f1": 0.47058823529411764, "path_precision": 0.8, "path_recall": 0.3333333333333333, "path_ans_f1": 0.5950413223140495, "path_ans_precision": 0.9, "path_ans_recall": 0.4444444444444444}
{"id": "WebQTest-841", "prediction": ["# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Vatican City\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> common.topic.notable_types -> Currency\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\ng.11b71mdn0q", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Belgium\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\ng.12tb6gbb1", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37cxr\n# Answer:\ng.1hhc37cxr", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Nearco -> biology.organism.sex -> Male\n# Answer:\nNearco", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Ribot -> base.thoroughbredracing.thoroughbred_racehorse.sire -> Tenerani\n# Answer:\nRibot", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Ribot -> biology.organism.sex -> Male\n# Answer:\nRibot"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-843", "prediction": ["# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0n0tzlj -> education.education.institution -> Humboldt University of Berlin\n# Answer:\nm.0n0tzlj", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmy28 -> education.education.institution -> University of Bonn\n# Answer:\nm.0vzmy28", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmxyk -> education.education.institution -> University of W\u00fcrzburg\n# Answer:\nm.0vzmxyk", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0n0tzlj -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nm.0n0tzlj", "# Reasoning Path:\nTheodor Schwann -> people.deceased_person.place_of_death -> Cologne -> travel.travel_destination.tourist_attractions -> K\u00e4the Kollwitz Museum\n# Answer:\nCologne", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmxyk -> freebase.valuenotation.has_value -> End Date\n# Answer:\nm.0vzmxyk", "# Reasoning Path:\nTheodor Schwann -> common.topic.image -> Theodore Schwann -> common.image.size -> m.0klz7g\n# Answer:\nTheodore Schwann", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmy28 -> freebase.valuenotation.has_value -> End Date\n# Answer:\nm.0vzmy28", "# Reasoning Path:\nTheodor Schwann -> people.deceased_person.place_of_death -> Cologne -> travel.travel_destination.tourist_attractions -> Cologne Cable Car\n# Answer:\nCologne", "# Reasoning Path:\nTheodor Schwann -> people.deceased_person.place_of_death -> Cologne -> location.location.contains -> Academy of Media Arts Cologne\n# Answer:\nCologne"], "ground_truth": ["University of W\u00fcrzburg", "Humboldt University of Berlin", "University of Bonn"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-845", "prediction": ["# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> government.government_position_held.office_holder -> Tim Kaine\n# Answer:\nm.0r99pcd", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcw0 -> government.government_position_held.office_holder -> Jim Webb\n# Answer:\nm.05kfcw0", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0r99pcd", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nm.0r99pcd", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.04g7cb8 -> government.government_position_held.office_holder -> Gerald L. Baliles\n# Answer:\nm.04g7cb8", "# Reasoning Path:\nVirginia -> government.political_district.elections -> United States Senate election in Virginia, 2008 -> government.election.winner -> Mark Warner\n# Answer:\nUnited States Senate election in Virginia, 2008", "# Reasoning Path:\nVirginia -> government.political_district.elections -> United States Senate election in Virginia, 2008 -> government.election.office -> United States Senator\n# Answer:\nUnited States Senate election in Virginia, 2008", "# Reasoning Path:\nVirginia -> government.political_district.elections -> United States Senate election in Virginia, 2008 -> common.topic.notable_types -> Election contest\n# Answer:\nUnited States Senate election in Virginia, 2008", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcw0 -> government.government_position_held.legislative_sessions -> 110th United States Congress\n# Answer:\nm.05kfcw0", "# Reasoning Path:\nVirginia -> location.location.containedby -> Charles Irving Thornton Tombstone -> location.location.geolocation -> m.0zjvs72\n# Answer:\nCharles Irving Thornton Tombstone"], "ground_truth": ["Jim Webb", "Mark Warner", "Tim Kaine"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-846", "prediction": ["# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> theater.theater_production_staff_role.people_who_have_had_this_role -> m.0111n9h6\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> common.topic.subject_of -> Dum-Doodles\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Cartoonist -> freebase.equivalent_topic.equivalent_type -> Comic Strip Creator\n# Answer:\nCartoonist", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> theater.theater_production_staff_role.people_who_have_had_this_role -> m.0_grv_v\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Television producer -> common.topic.subjects -> Billy Sorrentino\n# Answer:\nTelevision producer", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Television producer -> base.lightweight.profession.specialization_of -> Producers and Directors\n# Answer:\nTelevision producer", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> theater.theater_production_staff_role.people_who_have_had_this_role -> m.0w3l6yj\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> base.descriptive_names.names.descriptive_name -> m.01066c2h\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Cartoonist -> common.topic.notable_types -> Profession\n# Answer:\nCartoonist", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Television producer -> common.topic.subjects -> Michael Palance\n# Answer:\nTelevision producer"], "ground_truth": ["Theodor Seuss Geisel"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-847", "prediction": ["# Reasoning Path:\nUnited Nations Security Council -> base.unitednations.united_nations_body.members -> m.08__kq7 -> base.unitednations.united_nations_body_membership.member -> Costa Rica\n# Answer:\nm.08__kq7", "# Reasoning Path:\nUnited Nations Security Council -> base.unitednations.united_nations_body.members -> m.08__knr -> base.unitednations.united_nations_body_membership.member -> Burkina Faso\n# Answer:\nm.08__knr", "# Reasoning Path:\nUnited Nations Security Council -> base.unitednations.united_nations_body.members -> m.08__kpd -> base.unitednations.united_nations_body_membership.member -> Croatia\n# Answer:\nm.08__kpd", "# Reasoning Path:\nUnited Nations Security Council -> common.image.appears_in_topic_gallery -> Great power\n# Answer:\nGreat power", "# Reasoning Path:\nUnited Nations Security Council -> common.topic.image -> UN security council 2005 -> common.image.appears_in_topic_gallery -> Arnstein Arneberg\n# Answer:\nUN security council 2005", "# Reasoning Path:\nUnited Nations Security Council -> common.topic.image -> UN security council 2005 -> common.image.size -> m.0291vv2\n# Answer:\nUN security council 2005", "# Reasoning Path:\nUnited Nations Security Council -> common.image.appears_in_topic_gallery -> Reform of the United Nations Security Council\n# Answer:\nReform of the United Nations Security Council", "# Reasoning Path:\nUnited Nations Security Council -> common.topic.image -> UN security council 2005 -> common.image.appears_in_topic_gallery -> Great power\n# Answer:\nUN security council 2005", "# Reasoning Path:\nUnited Nations Security Council -> common.image.appears_in_topic_gallery -> United Nations Security Council Resolution 1\n# Answer:\nUnited Nations Security Council Resolution 1", "# Reasoning Path:\nUnited Nations Security Council -> common.topic.image -> UN security council 2005 -> common.image.appears_in_topic_gallery -> Per Krohg\n# Answer:\nUN security council 2005"], "ground_truth": ["China", "United Kingdom", "Russia", "Vietnam", "United States of America", "Costa Rica", "Croatia", "Burkina Faso", "France"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3157894736842105, "path_precision": 0.3, "path_recall": 0.3333333333333333, "path_ans_f1": 0.3157894736842105, "path_ans_precision": 0.3, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-848", "prediction": ["# Reasoning Path:\nNordic countries -> location.location.contains -> \u00c5land Islands -> location.location.contains -> Fasta \u00c5land\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nNordic countries -> location.location.contains -> \u00c5land Islands -> location.administrative_division.country -> Finland\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nNordic countries -> base.schemastaging.context_name.pronunciation -> g.125_kxbsc\n# Answer:\ng.125_kxbsc", "# Reasoning Path:\nNordic countries -> location.location.contains -> \u00c5land Islands -> location.location.contains -> Archipelago\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nNordic countries -> location.location.contains -> \u00c5land Islands -> location.location.contains -> Countryside\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nNordic countries -> location.location.contains -> \u00c5land Islands -> location.location.containedby -> Eurasia\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nNordic countries -> location.location.contains -> \u00c5land Islands -> location.location.containedby -> Finland\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nNordic countries -> location.location.contains -> Greenland -> location.location.containedby -> Kingdom of Denmark\n# Answer:\nGreenland", "# Reasoning Path:\nNordic countries -> location.location.contains -> Greenland -> location.statistical_region.gdp_growth_rate -> g.1245__rfy\n# Answer:\nGreenland", "# Reasoning Path:\nNordic countries -> location.location.partially_contained_by -> m.0_gjrm_ -> location.partial_containment_relationship.partially_contained_by -> North America\n# Answer:\nm.0_gjrm_"], "ground_truth": ["\u00c5land Islands", "Sweden", "Iceland", "Denmark", "Finland", "Greenland", "Norway", "Faroe Islands"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.38095238095238093, "ans_precission": 0.8, "ans_recall": 0.25, "path_f1": 0.38095238095238093, "path_precision": 0.8, "path_recall": 0.25, "path_ans_f1": 0.6153846153846154, "path_ans_precision": 0.8, "path_ans_recall": 0.5}
{"id": "WebQTest-849", "prediction": ["# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.location.people_born_here -> Afonso Mendes de Melo\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> g.11btv6zfvr\n# Answer:\ng.11btv6zfvr", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.location.people_born_here -> Afonso Pires Gato\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> government.governmental_jurisdiction.governing_officials -> m.04_9736\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.location.people_born_here -> Ant\u00f3nio Lu\u00eds de Sousa, 2nd Marquis of Minas\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.country.capital -> Coimbra\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> government.governmental_jurisdiction.governing_officials -> m.04_975f\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> symbols.name_source.namesakes -> Magellan -> common.topic.article -> m.01m0j_\n# Answer:\nMagellan", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> government.governmental_jurisdiction.governing_officials -> m.0c0mt_c\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.country.capital -> Lisbon\n# Answer:\nKingdom of Portugal"], "ground_truth": ["Kingdom of Portugal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> government.government_position_held.office_holder -> Michael Bennet\n# Answer:\nm.05kh0nn", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.office_holder -> Mark Udall\n# Answer:\nm.05kfbwl", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr -> government.government_position_held.office_holder -> Wayne Allard\n# Answer:\nm.02_wstr", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nm.05kh0nn", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.05kfbwl", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.legislative_sessions -> 111th United States Congress\n# Answer:\nm.05kfbwl", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008 -> common.topic.image -> Mark Udall\n# Answer:\nUnited States Senate election in Colorado, 2008", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> freebase.valuenotation.is_reviewed -> District represented (if position is district-related)\n# Answer:\nm.05kh0nn", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nm.02_wstr", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Cavalry Regiment -> common.topic.notable_types -> Military unit\n# Answer:\n1st Colorado Cavalry Regiment"], "ground_truth": ["Michael Bennet", "Mark Udall"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-850", "prediction": ["# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Anthony Casso\n# Answer:\nCancer", "# Reasoning Path:\nVera-Ellen -> common.topic.article -> m.022smc\n# Answer:\nm.022smc", "# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A Time to Mend\n# Answer:\nCancer", "# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Bryan Murray\n# Answer:\nCancer", "# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04d7nvm\n# Answer:\nCancer", "# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Deborah King\n# Answer:\nCancer", "# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjf_c\n# Answer:\nCancer", "# Reasoning Path:\nVera-Ellen -> film.actor.film -> m.0220nxg -> film.performance.film -> Love Happy\n# Answer:\nm.0220nxg", "# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjfp1\n# Answer:\nCancer", "# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A season in hell\n# Answer:\nCancer"], "ground_truth": ["Cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-851", "prediction": ["# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Al Sharqia Governorate -> location.location.containedby -> North Africa\n# Answer:\nAl Sharqia Governorate", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo -> periodicals.newspaper_circulation_area.newspapers -> Al-Ahram Weekly\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.capital -> Cairo -> periodicals.newspaper_circulation_area.newspapers -> Al-Ahram Weekly\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Al Sharqia Governorate -> base.aareas.schema.administrative_area.administrative_area_type -> Egyptian governorate\n# Answer:\nAl Sharqia Governorate", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo -> common.topic.subject_of -> As Egypt Howls and History Tweets\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Alexandria Governorate -> location.location.containedby -> North Africa\n# Answer:\nAlexandria Governorate", "# Reasoning Path:\nEgypt -> location.country.capital -> Cairo -> common.topic.subject_of -> As Egypt Howls and History Tweets\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo -> periodicals.newspaper_circulation_area.newspapers -> Almesryoon\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.capital -> Cairo -> periodicals.newspaper_circulation_area.newspapers -> Almesryoon\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Alexandria Governorate -> common.topic.notable_types -> Administrative Division\n# Answer:\nAlexandria Governorate"], "ground_truth": ["Cairo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-852", "prediction": ["# Reasoning Path:\nToronto -> base.biblioness.bibs_location.state -> Ontario -> location.location.containedby -> Canada\n# Answer:\nOntario", "# Reasoning Path:\nToronto -> location.location.containedby -> Ontario -> location.location.containedby -> Canada\n# Answer:\nOntario", "# Reasoning Path:\nToronto -> base.biblioness.bibs_location.state -> Ontario -> location.administrative_division.country -> Canada\n# Answer:\nOntario", "# Reasoning Path:\nToronto -> location.citytown.postal_codes -> M4W 1A1 -> common.topic.notable_types -> Postal Code\n# Answer:\nM4W 1A1", "# Reasoning Path:\nToronto -> location.location.containedby -> Ontario -> location.administrative_division.country -> Canada\n# Answer:\nOntario", "# Reasoning Path:\nToronto -> location.citytown.postal_codes -> M5R 1J2 -> location.location.containedby -> Ontario\n# Answer:\nM5R 1J2", "# Reasoning Path:\nToronto -> location.citytown.postal_codes -> M4W 1A1 -> common.topic.notable_for -> g.1259jd9x_\n# Answer:\nM4W 1A1", "# Reasoning Path:\nToronto -> location.citytown.postal_codes -> M4W 1A1 -> location.postal_code.country -> Canada\n# Answer:\nM4W 1A1", "# Reasoning Path:\nToronto -> location.citytown.postal_codes -> M5H 2K1 -> location.location.containedby -> Ontario\n# Answer:\nM5H 2K1", "# Reasoning Path:\nToronto -> location.location.containedby -> Canada -> location.country.first_level_divisions -> Ontario\n# Answer:\nCanada"], "ground_truth": ["Ontario"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-855", "prediction": ["# Reasoning Path:\nBrazil -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> South America\n# Answer:\nEarth", "# Reasoning Path:\nBrazil -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> Sovereign state\n# Answer:\nEarth", "# Reasoning Path:\nBrazil -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.uncommon.topic.exceptions -> City/Town/Village should pertain to Ghana, since City/Town/Village subdivides Ghanaian Municipal District.\n# Answer:\nEarth", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\ng.12cp_jcjy", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\ng.11b71vdgdz", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\ng.1hhc390hy", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc39sjp\n# Answer:\ng.1hhc39sjp", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fm01\n# Answer:\ng.12tb6fm01", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc385hl\n# Answer:\ng.1hhc385hl"], "ground_truth": ["Brazilian real"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-856", "prediction": ["# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> visual_art.art_period_movement.associated_artworks -> Mickey\n# Answer:\nModern art", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.subjects -> Wil Emling\n# Answer:\nModern art", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.subject_of -> Wil Emling\n# Answer:\nModern art", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Pop art -> visual_art.art_period_movement.associated_artists -> Antonia Gerstacker\n# Answer:\nPop art", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.art_forms -> Printmaking\n# Answer:\nPrintmaking", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Pop art -> business.product_theme.product_lines -> TAG Registries\n# Answer:\nPop art", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Pop art -> visual_art.art_period_movement.associated_artists -> Claes Oldenburg\n# Answer:\nPop art", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Pop art -> business.product_theme.products -> Andy Warhol Registry\n# Answer:\nPop art", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Pop art -> visual_art.art_period_movement.associated_artists -> David Hockney\n# Answer:\nPop art", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Pop art -> business.product_theme.products -> Damien Hirst Registry\n# Answer:\nPop art"], "ground_truth": ["Pop art", "Modern art"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-857", "prediction": ["# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> organization.organization.geographic_scope -> Alabama\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> CRHOME -> organization.organization.geographic_scope -> Alabama\n# Answer:\nCRHOME", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> organization.organization.geographic_scope -> Arkansas\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> organization.organization.geographic_scope -> California\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> B'nai B'rith District Grand Lodge 7 -> organization.organization.geographic_scope -> Alabama\n# Answer:\nB'nai B'rith District Grand Lodge 7", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> common.topic.subjects -> Automation\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> CRHOME -> organization.organization.geographic_scope -> Arizona\n# Answer:\nCRHOME", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> CRHOME -> organization.organization.place_founded -> Los Angeles\n# Answer:\nCRHOME", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> organization.organization.sectors -> Automation\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> CRHOME -> organization.organization.geographic_scope -> Arkansas\n# Answer:\nCRHOME"], "ground_truth": ["Alabama", "Georgia"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615385, "path_precision": 0.3, "path_recall": 0.10344827586206896, "path_ans_f1": 0.37499999999999994, "path_ans_precision": 0.3, "path_ans_recall": 0.5}
{"id": "WebQTest-859", "prediction": ["# Reasoning Path:\nRussia -> location.statistical_region.religions -> m.06jmmyk -> location.religion_percentage.religion -> Islam\n# Answer:\nm.06jmmyk", "# Reasoning Path:\nRussia -> location.statistical_region.religions -> m.06jmmyb -> location.religion_percentage.religion -> Russian Orthodox Church\n# Answer:\nm.06jmmyb", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.11b60vv5th\n# Answer:\ng.11b60vv5th", "# Reasoning Path:\nRussia -> location.statistical_region.co2_emissions_per_capita -> g.1245_8036\n# Answer:\ng.1245_8036", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.12tb6gh0b\n# Answer:\ng.12tb6gh0b", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.1hhc3f_fx\n# Answer:\ng.1hhc3f_fx", "# Reasoning Path:\nRussia -> location.statistical_region.co2_emissions_per_capita -> g.1245_9w0x\n# Answer:\ng.1245_9w0x", "# Reasoning Path:\nRussia -> location.statistical_region.co2_emissions_per_capita -> g.1245_chng\n# Answer:\ng.1245_chng"], "ground_truth": ["Islam", "Russian Orthodox Church"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Nordic countries\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Capital Region of Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> g.1hhc37psk\n# Answer:\ng.1hhc37psk", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Europe\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Kingdom of Denmark -> location.location.contains -> Denmark\n# Answer:\nKingdom of Denmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Central Denmark Region\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Kingdom of Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> g.1hhc38qmq\n# Answer:\ng.1hhc38qmq", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> North Denmark Region\n# Answer:\nDenmark"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-861", "prediction": ["# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.0j81th4 -> american_football.football_historical_coach_position.coach -> John Fox\n# Answer:\nm.0j81th4", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.04xg_yc -> american_football.football_historical_coach_position.coach -> Dom Capers\n# Answer:\nm.04xg_yc", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.0j81th4 -> american_football.football_historical_coach_position.position -> Head coach\n# Answer:\nm.0j81th4", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.coaches -> m.0_3r22s -> sports.sports_team_coach_tenure.coach -> John Fox\n# Answer:\nm.0_3r22s", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.04xg_yl -> american_football.football_historical_coach_position.coach -> George Seifert\n# Answer:\nm.04xg_yl", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> award.award_winner.awards_won -> m.0_qw472\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.coaches -> m.0_3r22s -> sports.sports_team_coach_tenure.position -> Head coach\n# Answer:\nm.0_3r22s", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> people.person.profession -> Coach\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> people.person.education -> m.0g5vwbs\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> people.person.education -> m.0n0rkd4\n# Answer:\nRon Rivera"], "ground_truth": ["Ron Rivera"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-862", "prediction": ["# Reasoning Path:\nLouis Sachar -> people.person.place_of_birth -> East Meadow -> location.location.people_born_here -> Adam Busch\n# Answer:\nEast Meadow", "# Reasoning Path:\nLouis Sachar -> people.person.place_of_birth -> East Meadow -> location.location.people_born_here -> Betsy Aidem\n# Answer:\nEast Meadow", "# Reasoning Path:\nLouis Sachar -> people.person.place_of_birth -> East Meadow -> location.hud_foreclosure_area.estimated_number_of_mortgages -> m.07f2vhv\n# Answer:\nEast Meadow", "# Reasoning Path:\nLouis Sachar -> people.person.place_of_birth -> East Meadow -> location.location.people_born_here -> Billy Staples\n# Answer:\nEast Meadow", "# Reasoning Path:\nLouis Sachar -> people.person.place_of_birth -> East Meadow -> common.topic.image -> East-meadow-ny-map\n# Answer:\nEast Meadow", "# Reasoning Path:\nLouis Sachar -> influence.influence_node.influenced_by -> E. B. White -> influence.influence_node.influenced -> Andy Rooney\n# Answer:\nE. B. White", "# Reasoning Path:\nLouis Sachar -> influence.influence_node.influenced_by -> E. B. White -> people.person.place_of_birth -> Mount Vernon\n# Answer:\nE. B. White", "# Reasoning Path:\nLouis Sachar -> influence.influence_node.influenced_by -> E. B. White -> influence.influence_node.influenced_by -> Henry David Thoreau\n# Answer:\nE. B. White", "# Reasoning Path:\nLouis Sachar -> influence.influence_node.influenced_by -> J. D. Salinger -> people.person.place_of_birth -> New York City\n# Answer:\nJ. D. Salinger", "# Reasoning Path:\nLouis Sachar -> people.person.place_of_birth -> East Meadow -> common.topic.image -> Hempstead Town Hall on Front Street in Hempstead Village\n# Answer:\nEast Meadow"], "ground_truth": ["East Meadow"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-865", "prediction": ["# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Wet'n'Wild SplashTown -> amusement_parks.park.rides -> RipQurl\n# Answer:\nWet'n'Wild SplashTown", "# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Wet'n'Wild SplashTown -> common.topic.article -> m.063g20\n# Answer:\nWet'n'Wild SplashTown", "# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Wet'n'Wild SplashTown -> common.topic.notable_for -> g.125dd3mwm\n# Answer:\nWet'n'Wild SplashTown", "# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Houston Marathon -> common.topic.article -> m.09q54l\n# Answer:\nHouston Marathon", "# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Wet'n'Wild SplashTown -> amusement_parks.park.rides -> Stingray Racer\n# Answer:\nWet'n'Wild SplashTown", "# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Kemah Boardwalk -> location.location.containedby -> Kemah\n# Answer:\nKemah Boardwalk", "# Reasoning Path:\nHouston -> location.statistical_region.population -> g.11b66fk05m\n# Answer:\ng.11b66fk05m", "# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Wet'n'Wild SplashTown -> amusement_parks.park.rides -> Texas Freefall\n# Answer:\nWet'n'Wild SplashTown", "# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Kemah Boardwalk -> common.topic.notable_types -> Amusement Park\n# Answer:\nKemah Boardwalk", "# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Houston Marathon -> common.topic.notable_for -> g.1256qcb1k\n# Answer:\nHouston Marathon"], "ground_truth": ["Bayou Bend Collection and Gardens", "Kemah Boardwalk", "Houston Zoo", "Houston Marathon", "Toyota Center", "Houston Museum of Natural Science", "Hanna\u2013Barbera Land", "Rothko Chapel", "Houston Arboretum and Nature Center", "USS Texas (BB-35)", "Wet'n'Wild SplashTown", "Downtown Aquarium, Houston", "Gerald D. Hines Waterwall Park", "George R. Brown Convention Center", "Children's Museum of Houston", "The Galleria", "Museum of Fine Arts, Houston", "Lyndon B. Johnson Space Center", "Sam Houston Race Park", "Space Center Houston", "Lakewood Church"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0.24657534246575338, "ans_precission": 0.9, "ans_recall": 0.14285714285714285, "path_f1": 0.24657534246575338, "path_precision": 0.9, "path_recall": 0.14285714285714285, "path_ans_f1": 0.24657534246575338, "path_ans_precision": 0.9, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-866", "prediction": ["# Reasoning Path:\nOtto Frank -> people.person.education -> m.0gft51q -> education.education.institution -> Heidelberg University\n# Answer:\nm.0gft51q", "# Reasoning Path:\nOtto Frank -> common.topic.image -> 100_1380.jpg\n# Answer:\n100_1380.jpg", "# Reasoning Path:\nOtto Frank -> base.popstra.celebrity.friendship -> m.0gvp1lx -> base.popstra.friendship.participant -> Max Goldberg\n# Answer:\nm.0gvp1lx", "# Reasoning Path:\nOtto Frank -> common.topic.image -> 100_2022.jpg\n# Answer:\n100_2022.jpg", "# Reasoning Path:\nOtto Frank -> base.popstra.celebrity.friendship -> m.0gvp2cx -> base.popstra.friendship.participant -> Hilde Goldberg\n# Answer:\nm.0gvp2cx", "# Reasoning Path:\nOtto Frank -> base.popstra.celebrity.friendship -> m.0gvp2g0 -> base.popstra.friendship.participant -> Cor Suijk\n# Answer:\nm.0gvp2g0", "# Reasoning Path:\nOtto Frank -> common.topic.image -> 100_2237.jpg -> common.image.size -> m.01x3sy_\n# Answer:\n100_2237.jpg"], "ground_truth": ["Heidelberg University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-867", "prediction": ["# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0_vw5gg -> award.award_nomination.nominated_for -> The Hunger Games: Catching Fire\n# Answer:\nm.0_vw5gg", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0r9hngb -> award.award_nomination.nominated_for -> House at the End of the Street\n# Answer:\nm.0r9hngb", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0_vw7_t -> award.award_nomination.nominated_for -> American Hustle\n# Answer:\nm.0_vw7_t", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0_vw5gg -> freebase.valuenotation.is_reviewed -> Award\n# Answer:\nm.0_vw5gg", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0_vw5gg -> award.award_nomination.award_nominee -> Josh Hutchersonm\n# Answer:\nm.0_vw5gg", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0r9hngb -> award.award_nomination.award -> MTV Movie Award for Best Scared-As-S**t Performance\n# Answer:\nm.0r9hngb", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0r9hngb -> award.award_nomination.ceremony -> 2013 MTV Movie Awards\n# Answer:\nm.0r9hngb", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0_vw7_t -> award.award_nomination.award -> MTV Movie Award for Best Kiss\n# Answer:\nm.0_vw7_t", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0_vw5gg -> award.award_nomination.award_nominee -> Sam Claflin\n# Answer:\nm.0_vw5gg", "# Reasoning Path:\nJennifer Lawrence -> award.award_winner.awards_won -> m.0101mk_y -> award.award_honor.ceremony -> 2014 Kids' Choice Awards\n# Answer:\nm.0101mk_y"], "ground_truth": ["The Burning Plain", "East of Eden", "Not Another High School Show", "X-Men: First Class", "Like Crazy", "Garden Party", "Company Town", "The Hunger Games", "The Glass Castle", "The Hunger Games: Catching Fire", "The Poker House", "Serena", "Joy", "X-Men: Apocalypse", "Burial Rites", "Devil You Know", "American Hustle", "Silver Linings Playbook", "House at the End of the Street", "X-Men: Days of Future Past", "Winter\u2019s Bone", "The Hunger Games: Mockingjay, Part 2", "The Beaver", "The Hunger Games: Mockingjay, Part 1"], "ans_acc": 0.16666666666666666, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.0379746835443038, "path_precision": 0.3, "path_recall": 0.02027027027027027, "path_ans_f1": 0.21428571428571427, "path_ans_precision": 0.3, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTest-868", "prediction": ["# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.010f42tf -> government.government_position_held.office_holder -> Jim Cawley\n# Answer:\nm.010f42tf", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.011crdxd -> government.government_position_held.office_holder -> Rosemary Brown\n# Answer:\nm.011crdxd", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.048157l -> government.government_position_held.office_holder -> Catherine Baker Knoll\n# Answer:\nm.048157l", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.title -> Director, Office of Health Care Reform\n# Answer:\nm.04ds08c", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.010f42tf -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nm.010f42tf", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.011crdxd -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.011crdxd", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.person -> Rosemarie B. Greco\n# Answer:\nm.04ds08c", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.010f42tf -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nm.010f42tf", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.048157l -> government.government_position_held.office_position_or_title -> Lieutenant Governor of Pennsylvania\n# Answer:\nm.048157l", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.010f42tf -> freebase.valuenotation.is_reviewed -> Jurisdiction of office\n# Answer:\nm.010f42tf"], "ground_truth": ["Tom Corbett"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-869", "prediction": ["# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.location.containedby -> Arab world\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> location.location.geolocation -> m.0cmwfl8\n# Answer:\nm.0cmwfl8", "# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.location.containedby -> Asia\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.location.containedby -> Eurasia\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.statistical_region.internet_users_percent_population -> g.11b60vk8mh\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.statistical_region.gdp_real -> g.11b61jk247\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.statistical_region.internet_users_percent_population -> g.1245_2gxy\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.statistical_region.internet_users_percent_population -> g.1245_rxhk\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.statistical_region.gdp_real -> g.12tb6fdpr\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> base.folklore.mythical_creature_location.mythical_creature_s -> Phoenix -> education.school_mascot.school -> Aquinas University\n# Answer:\nPhoenix"], "ground_truth": ["Lebanon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_for -> g.125h4fcxl\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_types -> Postal Code\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98105 -> location.location.contains -> Burke Museum of Natural History and Culture\n# Answer:\n98105", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98105 -> location.postal_code.country -> United States of America\n# Answer:\n98105", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> g.11b66b70n7\n# Answer:\ng.11b66b70n7", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.location.contains -> Fifteen Twenty-One Second Avenue\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98105 -> common.topic.notable_types -> Postal Code\n# Answer:\n98105", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> common.topic.notable_for -> g.125f2tsfn\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.location.contains -> 1111 Third Avenue\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98105 -> location.location.contains -> Calvary Cemetery\n# Answer:\n98105"], "ground_truth": ["98199", "98127", "98188", "98114", "98154", "98106", "98138", "98175", "98194", "98108", "98117", "98158", "98155", "98198", "98161", "98104", "98174", "98112", "98170", "98144", "98178", "98124", "98132", "98109", "98184", "98103", "98181", "98111", "98195", "98136", "98145", "98165", "98119", "98118", "98129", "98113", "98115", "98139", "98191", "98146", "98131", "98177", "98116", "98102", "98122", "98164", "98134", "98107", "98101", "98105", "98190", "98119-4114", "98141", "98126", "98121", "98148", "98171", "98166", "98160", "98133", "98168", "98125", "98185"], "ans_acc": 0.047619047619047616, "ans_hit": 1, "ans_f1": 0.09045226130653267, "ans_precission": 0.9, "ans_recall": 0.047619047619047616, "path_f1": 0.09045226130653267, "path_precision": 0.9, "path_recall": 0.047619047619047616, "path_ans_f1": 0.09045226130653267, "path_ans_precision": 0.9, "path_ans_recall": 0.047619047619047616}
{"id": "WebQTest-870", "prediction": ["# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> TD Garden -> sports.sports_facility.teams -> Boston Bruins\n# Answer:\nTD Garden", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> TD Garden -> common.topic.notable_types -> Sports Facility\n# Answer:\nTD Garden", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> XL Center -> location.location.events -> Charlotte-UIC  1998 NCAA Men's Division I Basketball Tournament Game\n# Answer:\nXL Center", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> XL Center -> location.location.geolocation -> m.0v1p8cd\n# Answer:\nXL Center", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> TD Garden -> location.location.events -> 1999  NCAA Men's Division I Basketball Tournament- East Regional First Round\n# Answer:\nTD Garden", "# Reasoning Path:\nBoston Celtics -> common.topic.notable_types -> Professional Sports Team -> freebase.type_hints.included_types -> Sports Team\n# Answer:\nProfessional Sports Team", "# Reasoning Path:\nBoston Celtics -> common.topic.notable_types -> Professional Sports Team -> type.type.domain -> Sports\n# Answer:\nProfessional Sports Team", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> XL Center -> location.location.events -> Michigan St-Eastern Michigan  1998 NCAA Men's Division I Basketball Tournament Game\n# Answer:\nXL Center", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> TD Garden -> location.location.events -> 1999 NCAA Men's Division I Basketball Tournament\n# Answer:\nTD Garden", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> XL Center -> location.location.events -> Michigan St-Princeton 1998 NCAA Men's Division I Basketball Tournament Game\n# Answer:\nXL Center"], "ground_truth": ["TD Garden", "XL Center"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-872", "prediction": ["# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Ali Azmat\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> Osama: The Making of a Terrorist -> book.written_work.subjects -> Al-Qaeda\n# Answer:\nOsama: The Making of a Terrorist", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.events -> Death of Osama bin Laden\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Amrit Singh Thapar\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.contains -> Abbottabad Hockey Stadium\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Faiza Khan\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> What Death of Osama Bin Laden Indicates About Barack Obama's Leadership -> book.written_work.subjects -> Death\n# Answer:\nWhat Death of Osama Bin Laden Indicates About Barack Obama's Leadership", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.contains -> Abbottabad International Medical College\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> Osama: The Making of a Terrorist -> common.topic.notable_types -> Book\n# Answer:\nOsama: The Making of a Terrorist", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.contains -> Abbottabad Public School\n# Answer:\nAbbottabad"], "ground_truth": ["Abbottabad"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-873", "prediction": ["# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Centre for the Arts -> architecture.structure.architect -> Eberhard Zeidler\n# Answer:\nToronto Centre for the Arts", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Centre for the Arts -> travel.tourist_attraction.near_travel_destination -> Newmarket\n# Answer:\nToronto Centre for the Arts", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Centre for the Arts -> common.topic.article -> m.05398q\n# Answer:\nToronto Centre for the Arts", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Islands -> geography.island.body_of_water -> Lake Ontario\n# Answer:\nToronto Islands", "# Reasoning Path:\nToronto -> common.topic.webpage -> m.02nc8t6 -> common.webpage.resource -> City of Toronto Web site\n# Answer:\nm.02nc8t6", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Islands -> common.topic.notable_for -> g.1257xhscc\n# Answer:\nToronto Islands", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Islands -> location.location.containedby -> Ontario\n# Answer:\nToronto Islands", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Air Canada Centre -> location.location.events -> 1st Canadian Women's Hockey League All-Star Game\n# Answer:\nAir Canada Centre", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Air Canada Centre -> common.topic.notable_for -> g.125fsz2rg\n# Answer:\nAir Canada Centre", "# Reasoning Path:\nToronto -> common.topic.webpage -> m.059h8f0 -> common.webpage.resource -> Visitor Information\n# Answer:\nm.059h8f0"], "ground_truth": ["Ashbridges Bay Wastewater Treatment Plant"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-874", "prediction": ["# Reasoning Path:\nBosnia and Herzegovina -> location.country.languages_spoken -> Serbo-Croatian Language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nSerbo-Croatian Language", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.languages_spoken -> Serbo-Croatian Language -> common.topic.image -> Serbo croatian language2005\n# Answer:\nSerbo-Croatian Language", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.official_language -> Croatian language -> language.human_language.countries_spoken_in -> Croatia\n# Answer:\nCroatian language", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.languages_spoken -> Serbo-Croatian Language -> language.human_language.language_family -> Slavic languages\n# Answer:\nSerbo-Croatian Language", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.languages_spoken -> Serbian language -> base.rosetta.languoid.local_name -> Serbian\n# Answer:\nSerbian language", "# Reasoning Path:\nBosnia and Herzegovina -> location.statistical_region.health_expenditure_as_percent_of_gdp -> g.12cp_j3km\n# Answer:\ng.12cp_j3km", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.languages_spoken -> Serbo-Croatian Language -> language.human_language.language_family -> South Slavic languages\n# Answer:\nSerbo-Croatian Language", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.languages_spoken -> Serbo-Croatian Language -> media_common.netflix_genre.titles -> A Serbian Film\n# Answer:\nSerbo-Croatian Language", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.official_language -> Serbian language -> base.rosetta.languoid.local_name -> Serbian\n# Answer:\nSerbian language", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.languages_spoken -> Serbian language -> base.rosetta.languoid.parent -> Western South Group\n# Answer:\nSerbian language"], "ground_truth": ["Croatian language", "Bosnian language", "Serbian language", "Serbo-Croatian Language"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.7241379310344827, "path_precision": 0.7, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-875", "prediction": ["# Reasoning Path:\nHugo Ch\u00e1vez -> people.person.education -> m.0467s1m -> education.education.institution -> Sim\u00f3n Bol\u00edvar University\n# Answer:\nm.0467s1m", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.person.education -> m.0467s06 -> education.education.institution -> Daniel Florencio O'Leary School\n# Answer:\nm.0467s06", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.deceased_person.place_of_death -> Caracas -> location.location.contains -> Sim\u00f3n Bol\u00edvar University\n# Answer:\nCaracas", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.person.education -> m.0467r_k -> education.education.institution -> Julian Pino School\n# Answer:\nm.0467r_k", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.deceased_person.place_of_death -> Caracas -> location.location.contains -> Anauco Hilton\n# Answer:\nCaracas", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.deceased_person.place_of_death -> Caracas -> location.location.events -> 1969 Men's South American Volleyball Championship\n# Answer:\nCaracas", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.person.sibling_s -> m.0467rzz -> people.sibling_relationship.sibling -> Ad\u00e1n Ch\u00e1vez\n# Answer:\nm.0467rzz", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.deceased_person.place_of_death -> Caracas -> location.location.contains -> Andr\u00e9s Bello Catholic University\n# Answer:\nCaracas", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.deceased_person.place_of_death -> Caracas -> periodicals.newspaper_circulation_area.newspapers -> Diario VEA\n# Answer:\nCaracas", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.deceased_person.place_of_death -> Caracas -> location.location.events -> 1983 Pan American Games\n# Answer:\nCaracas"], "ground_truth": ["Julian Pino School", "Daniel Florencio O'Leary School", "Bolivarian Military University of Venezuela", "Sim\u00f3n Bol\u00edvar University"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5333333333333333, "path_precision": 0.4, "path_recall": 0.8, "path_ans_f1": 0.5217391304347827, "path_ans_precision": 0.4, "path_ans_recall": 0.75}
{"id": "WebQTest-876", "prediction": ["# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> Canada\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nJapan -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Kiribati\n# Answer:\nEnglish Language", "# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> Australia\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> China\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.participating_countries -> Canada\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n1952 Summer Olympics", "# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.sports -> Handball\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\ng.1245_2h9t", "# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.participating_countries -> Australia\n# Answer:\n1936 Summer Olympics"], "ground_truth": ["Tanzania", "Sudan", "New Zealand", "Kiribati", "Canada", "United States of America"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14925373134328357, "path_precision": 0.5, "path_recall": 0.08771929824561403, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-877", "prediction": ["# Reasoning Path:\nSonia Gandhi -> people.person.children -> Priyanka Gandhi -> people.person.children -> Miraya Vadra\n# Answer:\nPriyanka Gandhi", "# Reasoning Path:\nSonia Gandhi -> people.person.children -> Priyanka Gandhi -> people.person.children -> Raihan Vadra\n# Answer:\nPriyanka Gandhi", "# Reasoning Path:\nSonia Gandhi -> people.person.children -> Priyanka Gandhi -> freebase.valuenotation.has_value -> Height\n# Answer:\nPriyanka Gandhi", "# Reasoning Path:\nSonia Gandhi -> people.person.children -> Priyanka Gandhi -> common.topic.article -> m.02n20d\n# Answer:\nPriyanka Gandhi", "# Reasoning Path:\nSonia Gandhi -> people.person.children -> Rahul Gandhi -> people.person.religion -> Hinduism\n# Answer:\nRahul Gandhi", "# Reasoning Path:\nSonia Gandhi -> people.person.children -> Rahul Gandhi -> people.person.places_lived -> m.03pvw0h\n# Answer:\nRahul Gandhi", "# Reasoning Path:\nSonia Gandhi -> common.topic.notable_types -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdn_\n# Answer:\nPolitician", "# Reasoning Path:\nSonia Gandhi -> people.person.children -> Rahul Gandhi -> common.topic.notable_types -> Politician\n# Answer:\nRahul Gandhi", "# Reasoning Path:\nSonia Gandhi -> common.topic.notable_types -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nSonia Gandhi -> people.person.education -> m.0nby0w9 -> freebase.valuenotation.has_value -> Degree\n# Answer:\nm.0nby0w9"], "ground_truth": ["Priyanka Gandhi"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-878", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> religion.deity.deity_of -> Sunni Islam\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> common.topic.article -> m.0jfq\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.deities -> Yahweh\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.deities -> God -> common.topic.notable_for -> g.125d2mw4b\n# Answer:\nGod", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Hinduism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> common.topic.image -> Allah-eser2\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.article -> m.055v4j\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Jainism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.image -> Ramapir01\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> base.schemastaging.context_name.pronunciation -> g.125_r5my9\n# Answer:\nAbrahamic religions"], "ground_truth": ["Ramdev Pir", "God", "Allah"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-879", "prediction": ["# Reasoning Path:\nMary Todd Lincoln -> people.person.place_of_birth -> Lexington -> film.film_location.featured_in_films -> Secretariat\n# Answer:\nLexington", "# Reasoning Path:\nMary Todd Lincoln -> government.politician.government_positions_held -> m.0l0qwng -> government.government_position_held.office_position_or_title -> First Lady of the United States\n# Answer:\nm.0l0qwng", "# Reasoning Path:\nMary Todd Lincoln -> people.person.place_of_birth -> Lexington -> location.location.containedby -> Kentucky\n# Answer:\nLexington", "# Reasoning Path:\nMary Todd Lincoln -> people.person.place_of_birth -> Lexington -> location.location.contains -> 40536\n# Answer:\nLexington", "# Reasoning Path:\nMary Todd Lincoln -> people.person.place_of_birth -> Lexington -> location.location.containedby -> Area code 859\n# Answer:\nLexington", "# Reasoning Path:\nMary Todd Lincoln -> government.politician.government_positions_held -> m.0l0qwng -> government.government_position_held.basic_title -> First Lady\n# Answer:\nm.0l0qwng", "# Reasoning Path:\nMary Todd Lincoln -> people.person.place_of_birth -> Lexington -> location.location.containedby -> Fayette County\n# Answer:\nLexington", "# Reasoning Path:\nMary Todd Lincoln -> people.person.place_of_birth -> Lexington -> location.location.contains -> 40536-0596\n# Answer:\nLexington", "# Reasoning Path:\nMary Todd Lincoln -> people.person.place_of_birth -> Lexington -> location.location.contains -> African Cemetery No. 2\n# Answer:\nLexington"], "ground_truth": ["Lexington"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Adrienne Banfield-Jones\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.children -> Jaden Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Robsol Pinkett, Jr.\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.parents -> Caroline Bright\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> common.topic.image -> Pinkett-Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.parents -> Willard Christopher Smith, Sr.\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> tv.tv_actor.guest_roles -> m.09nbq82\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B -> music.genre.subgenre -> Quiet Storm\n# Answer:\nContemporary R&B", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> common.topic.image -> Profile of a man in a crowd of people wearing a blue-colored suit\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> tv.tv_actor.guest_roles -> m.09nbq8g\n# Answer:\nWill Smith"], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-880", "prediction": ["# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_death -> Down House -> location.location.containedby -> United Kingdom\n# Answer:\nDown House", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_death -> Down House -> common.topic.notable_for -> g.12578zv1w\n# Answer:\nDown House", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_death -> Down House -> common.topic.article -> m.02_67b\n# Answer:\nDown House", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_burial -> Westminster Abbey -> travel.tourist_attraction.near_travel_destination -> London\n# Answer:\nWestminster Abbey", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_burial -> Westminster Abbey -> location.location.containedby -> London\n# Answer:\nWestminster Abbey", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_burial -> Westminster Abbey -> religion.place_of_worship.religion -> Anglicanism\n# Answer:\nWestminster Abbey", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_burial -> Westminster Abbey -> travel.tourist_attraction.near_travel_destination -> City of Westminster\n# Answer:\nWestminster Abbey", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_burial -> Westminster Abbey -> location.location.containedby -> City of Westminster\n# Answer:\nWestminster Abbey", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology"], "ground_truth": ["Down House"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-881", "prediction": ["# Reasoning Path:\nNick Cannon -> film.actor.film -> m.062wcnd -> film.performance.film -> Ball Don't Lie\n# Answer:\nm.062wcnd", "# Reasoning Path:\nNick Cannon -> film.actor.film -> m.0ncf55j -> film.performance.film -> Men in Black II\n# Answer:\nm.0ncf55j", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0z9v3c6 -> award.award_nomination.nominated_for -> Love Don't Cost a Thing\n# Answer:\nm.0z9v3c6", "# Reasoning Path:\nNick Cannon -> film.actor.film -> g.11bttggxl1\n# Answer:\ng.11bttggxl1", "# Reasoning Path:\nNick Cannon -> film.actor.film -> m.062wcnd -> film.performance.character -> Mico\n# Answer:\nm.062wcnd", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0b4d586 -> award.award_nomination.nominated_for -> Bobby\n# Answer:\nm.0b4d586", "# Reasoning Path:\nNick Cannon -> film.actor.film -> m.0ncf55j -> film.performance.character -> MIB Autopsy Agent\n# Answer:\nm.0ncf55j", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0z9v3c6 -> award.award_nomination.ceremony -> 2004 Teen Choice Awards\n# Answer:\nm.0z9v3c6", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0z9v3c6 -> award.award_nomination.award -> Teen Choice Award for Choice Movie: Chemistry\n# Answer:\nm.0z9v3c6", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.010g2z6m -> award.award_nomination.nominated_for -> Disney Parks Christmas Day Parade\n# Answer:\nm.010g2z6m"], "ground_truth": ["Weapons", "Garfield: The Movie", "The Adventures of Brer Rabbit", "Drumline", "A Very School Gyrls Holla-Day", "Underclassman", "Bobby", "Even Money", "Ball Don't Lie", "Whatever It Takes", "Goal II: Living the Dream", "Drumline: A New Beat", "Men in Black II", "Shall We Dance?", "Chiraq", "Love Don't Cost a Thing", "Monster House", "Roll Bounce", "Day of the Dead", "The Killing Room", "American Son"], "ans_acc": 0.19047619047619047, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.1951219512195122, "path_precision": 0.4, "path_recall": 0.12903225806451613, "path_ans_f1": 0.25806451612903225, "path_ans_precision": 0.4, "path_ans_recall": 0.19047619047619047}
{"id": "WebQTest-882", "prediction": ["# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> book.book_subject.works -> 1906\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> base.schemastaging.context_name.pronunciation -> g.125_nh8xf\n# Answer:\ng.125_nh8xf", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> book.book_subject.works -> 1st to Die\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> base.petbreeds.city_with_dogs.top_breeds -> m.062_6p5\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> book.book_subject.works -> 2nd Chance\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> location.citytown.postal_codes -> 94101\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> common.topic.notable_types -> Disaster -> type.type.domain -> Event\n# Answer:\nDisaster", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> base.petbreeds.city_with_dogs.top_breeds -> m.062_6p9\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> base.petbreeds.city_with_dogs.top_breeds -> m.062_6pf\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> location.citytown.postal_codes -> 94102\n# Answer:\nSan Francisco"], "ground_truth": ["1906-04-18"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-883", "prediction": ["# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> people.person.parents -> Alexandra of Denmark\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> people.person.spouse_s -> m.02kp9gn\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> people.person.parents -> Edward VII\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> Mary of Teck -> people.person.children -> Edward VIII of the United Kingdom\n# Answer:\nMary of Teck", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> freebase.valuenotation.is_reviewed -> Children\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.spouse_s -> m.0j4k1c9 -> people.marriage.spouse -> Queen Elizabeth The Queen Mother\n# Answer:\nm.0j4k1c9", "# Reasoning Path:\nGeorge VI -> people.person.parents -> Mary of Teck -> people.person.children -> Prince John of the United Kingdom\n# Answer:\nMary of Teck", "# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.family_member.family -> British Royal Family\n# Answer:\nElizabeth II", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.spouse_s -> m.0j4k1c9 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.0j4k1c9"], "ground_truth": ["Queen Elizabeth The Queen Mother"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.13333333333333333, "path_precision": 0.1, "path_recall": 0.2, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-884", "prediction": ["# Reasoning Path:\nJesus Christ -> people.person.place_of_birth -> Judea -> location.location.containedby -> Middle East\n# Answer:\nJudea", "# Reasoning Path:\nJesus Christ -> people.person.place_of_birth -> Judea -> common.topic.article -> m.065shd\n# Answer:\nJudea", "# Reasoning Path:\nJesus Christ -> common.topic.notable_for -> g.12551f1xr\n# Answer:\ng.12551f1xr", "# Reasoning Path:\nJesus Christ -> influence.influence_node.influenced -> Anne Rice -> people.person.nationality -> United States of America\n# Answer:\nAnne Rice", "# Reasoning Path:\nJesus Christ -> influence.influence_node.influenced -> Augustine of Hippo -> influence.influence_node.influenced_by -> Paul the Apostle\n# Answer:\nAugustine of Hippo", "# Reasoning Path:\nJesus Christ -> influence.influence_node.influenced -> Constantine the Great -> people.person.place_of_birth -> Ni\u0161\n# Answer:\nConstantine the Great", "# Reasoning Path:\nJesus Christ -> common.topic.notable_for -> g.1255sm1pr\n# Answer:\ng.1255sm1pr", "# Reasoning Path:\nJesus Christ -> influence.influence_node.influenced -> Anne Rice -> book.book_subject.works -> Called Out of Darkness: A Spiritual Confession\n# Answer:\nAnne Rice", "# Reasoning Path:\nJesus Christ -> common.topic.notable_for -> g.12578_nd4\n# Answer:\ng.12578_nd4", "# Reasoning Path:\nJesus Christ -> influence.influence_node.influenced -> Augustine of Hippo -> influence.influence_node.influenced_by -> Aristotle\n# Answer:\nAugustine of Hippo"], "ground_truth": ["Judea"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-885", "prediction": ["# Reasoning Path:\nJames Dyson -> people.person.profession -> Designer -> base.descriptive_names.names.descriptive_name -> m.01053pmf\n# Answer:\nDesigner", "# Reasoning Path:\nJames Dyson -> common.topic.notable_types -> Organization leader -> freebase.type_profile.equivalent_topic -> board member\n# Answer:\nOrganization leader", "# Reasoning Path:\nJames Dyson -> people.person.profession -> Designer -> base.descriptive_names.names.descriptive_name -> m.01053pnt\n# Answer:\nDesigner", "# Reasoning Path:\nJames Dyson -> people.person.profession -> Designer -> people.profession.specializations -> Aircraft designer\n# Answer:\nDesigner", "# Reasoning Path:\nJames Dyson -> people.person.profession -> Designer -> base.descriptive_names.names.descriptive_name -> m.01053pps\n# Answer:\nDesigner", "# Reasoning Path:\nJames Dyson -> people.person.profession -> Designer -> film.film_job.films_with_this_crew_job -> m.0_8yj45\n# Answer:\nDesigner", "# Reasoning Path:\nJames Dyson -> people.person.profession -> Industrial designer -> people.profession.specialization_of -> Designer\n# Answer:\nIndustrial designer", "# Reasoning Path:\nJames Dyson -> common.topic.notable_types -> Organization leader -> type.type.expected_by -> Person\n# Answer:\nOrganization leader", "# Reasoning Path:\nJames Dyson -> people.person.nationality -> United Kingdom -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nJames Dyson -> people.person.profession -> Designer -> people.profession.specializations -> Anime character designer\n# Answer:\nDesigner"], "ground_truth": ["Industrial designer", "Designer", "Engineer", "Inventor"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5833333333333334, "ans_precission": 0.7, "ans_recall": 0.5, "path_f1": 0.5833333333333334, "path_precision": 0.7, "path_recall": 0.5, "path_ans_f1": 0.5833333333333334, "path_ans_precision": 0.7, "path_ans_recall": 0.5}
{"id": "WebQTest-886", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.actor -> James Earl Jones\n# Answer:\nm.02nv74t", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.actor -> James Earl Jones\n# Answer:\nm.02nwtmz", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.film -> Star Wars\n# Answer:\nm.02nv74t", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Uncredited\n# Answer:\nm.02nv74t", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Voice\n# Answer:\nm.02nv74t", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.film -> Return of the Jedi\n# Answer:\nm.02nwtmz"], "ground_truth": ["James Earl Jones"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-887", "prediction": ["# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0n1mdbx -> education.education.institution -> George Washington High School\n# Answer:\nm.0n1mdbx", "# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0vv8k0g -> education.education.institution -> California Labor School\n# Answer:\nm.0vv8k0g", "# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0n1mdbx -> freebase.valuenotation.has_no_value -> Major/Field Of Study\n# Answer:\nm.0n1mdbx", "# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0n1mdbx -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nm.0n1mdbx", "# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0n1mdbx -> freebase.valuenotation.has_no_value -> Specialization\n# Answer:\nm.0n1mdbx", "# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0vv8k0g -> freebase.valuenotation.has_value -> Degree\n# Answer:\nm.0vv8k0g", "# Reasoning Path:\nMaya Angelou -> music.artist.track -> Calypso Blues -> music.recording.releases -> Miss Calypso\n# Answer:\nCalypso Blues", "# Reasoning Path:\nMaya Angelou -> music.artist.track -> Calypso Blues -> common.topic.notable_types -> Musical Recording\n# Answer:\nCalypso Blues", "# Reasoning Path:\nMaya Angelou -> people.person.quotations -> A bird doesn't sing because it has an answer, it sings because it has a song. -> media_common.quotation.subjects -> Action\n# Answer:\nA bird doesn't sing because it has an answer, it sings because it has a song.", "# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0vv8k0g -> freebase.valuenotation.has_value -> End Date\n# Answer:\nm.0vv8k0g"], "ground_truth": ["George Washington High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-888", "prediction": ["# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Austria\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Liechtenstein\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.sports -> Ice Hockey\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Australia\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics -> olympics.olympic_games.participating_countries -> Austria\n# Answer:\n1896 Summer Olympics", "# Reasoning Path:\nSwitzerland -> location.location.partially_contains -> Alps -> location.location.partially_containedby -> Austria\n# Answer:\nAlps", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d0tc3g\n# Answer:\ng.11b6d0tc3g", "# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics -> olympics.olympic_games.sports -> Tennis\n# Answer:\n1896 Summer Olympics", "# Reasoning Path:\nSwitzerland -> location.location.partially_contains -> Alps -> location.location.partially_contained_by -> m.0wg8qkm\n# Answer:\nAlps"], "ground_truth": ["Italy", "Germany", "Austria", "France", "Liechtenstein"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.028169014084507043, "path_precision": 0.3, "path_recall": 0.014778325123152709, "path_ans_f1": 0.4000000000000001, "path_ans_precision": 0.4, "path_ans_recall": 0.4}
{"id": "WebQTest-889", "prediction": ["# Reasoning Path:\nBenito Mussolini -> government.politician.government_positions_held -> m.0c9_j0x -> government.government_position_held.jurisdiction_of_office -> Italy\n# Answer:\nm.0c9_j0x", "# Reasoning Path:\nBenito Mussolini -> government.politician.government_positions_held -> m.0c9_j0x -> government.government_position_held.office_position_or_title -> Duce\n# Answer:\nm.0c9_j0x", "# Reasoning Path:\nBenito Mussolini -> people.person.nationality -> Italy -> government.governmental_jurisdiction.governing_officials -> m.0c9_j0x\n# Answer:\nItaly", "# Reasoning Path:\nBenito Mussolini -> people.person.nationality -> Italy -> base.locations.countries.continent -> Europe\n# Answer:\nItaly", "# Reasoning Path:\nBenito Mussolini -> organization.organization_founder.organizations_founded -> National Fascist Party -> government.political_party.politicians_in_this_party -> m.012qyfs6\n# Answer:\nNational Fascist Party", "# Reasoning Path:\nBenito Mussolini -> organization.organization_founder.organizations_founded -> National Fascist Party -> organization.organization.geographic_scope -> Italy\n# Answer:\nNational Fascist Party", "# Reasoning Path:\nBenito Mussolini -> organization.organization_founder.organizations_founded -> Cinecitt\u00e0 -> base.schemastaging.organization_extra.phone_number -> m.010h48jv\n# Answer:\nCinecitt\u00e0", "# Reasoning Path:\nBenito Mussolini -> people.person.nationality -> Italy -> location.country.languages_spoken -> Italian Language\n# Answer:\nItaly", "# Reasoning Path:\nBenito Mussolini -> organization.organization_founder.organizations_founded -> National Fascist Party -> organization.organization.sectors -> Anti-communism\n# Answer:\nNational Fascist Party", "# Reasoning Path:\nBenito Mussolini -> organization.organization_founder.organizations_founded -> Grand Council of Fascism -> common.topic.notable_types -> Organization\n# Answer:\nGrand Council of Fascism"], "ground_truth": ["Italy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_types -> Language Writing System\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_for -> g.1258512fl\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> N\u00fcshu script -> common.topic.article -> m.014lbq\n# Answer:\nN\u00fcshu script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> language.language_writing_system.languages -> Mongolian language\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> language.language_writing_system.languages -> Chinese, Hakka Language\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> book.book_subject.works -> Flags of Our Fathers\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> common.topic.notable_for -> g.1259bftrt\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> base.schemastaging.context_name.pronunciation -> g.125_l82wv\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> education.field_of_study.students_majoring -> m.0104b7h1 -> education.education.degree -> PhD\n# Answer:\nm.0104b7h1", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> language.language_writing_system.languages -> Old Mandarin\n# Answer:\n'Phags-pa script"], "ground_truth": ["Simplified Chinese character", "Traditional Chinese characters", "'Phags-pa script", "Chinese characters", "N\u00fcshu script"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6461538461538462, "ans_precission": 0.7, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.6461538461538462, "path_ans_precision": 0.7, "path_ans_recall": 0.6}
{"id": "WebQTest-891", "prediction": ["# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Mao Zedong -> people.person.quotations -> A revolution is not a dinner party, or writing an essay, or painting a picture, or doing embroidery; it cannot be so refined, so leisurely and gentle, so temperate, kind, courteous, restrained and magnanimous. A revolution is an insurrection, an act of violence by which one class overthrows another.\n# Answer:\nMao Zedong", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Li Dazhao -> people.person.education -> m.04hd15_\n# Answer:\nLi Dazhao", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Zhou Enlai -> government.politician.party -> m.0lr13p4\n# Answer:\nZhou Enlai", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Mao Zedong -> people.person.quotations -> All reactionaries are paper tigers.\n# Answer:\nMao Zedong", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Li Dazhao -> people.person.nationality -> China\n# Answer:\nLi Dazhao", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Mao Zedong -> people.person.quotations -> All the reputedly powerful reactionaries are merely paper tigers. The reason is that they are divorced from the people. Look! Was not Hitler a paper tiger? Was Hitler not overthrown? U.S. imperialism has not yet been overthrown and it has the atomic bomb. I believe it also will be overthrown. It, too, is a paper tiger.\n# Answer:\nMao Zedong", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Mao Zedong -> influence.influence_node.influenced_by -> Alexander Herzen\n# Answer:\nMao Zedong", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Mao Zedong -> influence.influence_node.influenced -> Alain Badiou\n# Answer:\nMao Zedong", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Zhou Enlai -> people.person.religion -> Atheism\n# Answer:\nZhou Enlai", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Mao Zedong -> influence.influence_node.influenced_by -> Friedrich Engels\n# Answer:\nMao Zedong"], "ground_truth": ["Li Dazhao", "Zhou Enlai", "Mao Zedong", "Chen Duxiu"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.8571428571428571, "path_precision": 1.0, "path_recall": 0.75, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-892", "prediction": ["# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.includes_causes_of_death -> Choking\n# Answer:\nRespiratory failure", "# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nRespiratory failure", "# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Pneumonia -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nPneumonia", "# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Respiratory failure -> base.schemastaging.context_name.pronunciation -> g.125_ph2ct\n# Answer:\nRespiratory failure", "# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Stroke -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nStroke", "# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.includes_causes_of_death -> Erotic asphyxiation\n# Answer:\nRespiratory failure", "# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.includes_causes_of_death -> Strangling\n# Answer:\nRespiratory failure", "# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Stroke -> medicine.disease.risk_factors -> Male\n# Answer:\nStroke", "# Reasoning Path:\nMiles Davis -> music.composer.compositions -> A. Flute Song, B. Hotel Me -> music.composition.composer -> Gil Evans\n# Answer:\nA. Flute Song, B. Hotel Me", "# Reasoning Path:\nMiles Davis -> music.featured_artist.recordings -> Ah-Leu-Cha -> music.recording.artist -> Charlie Parker All-Stars\n# Answer:\nAh-Leu-Cha"], "ground_truth": ["Pneumonia", "Respiratory failure", "Stroke"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-893", "prediction": ["# Reasoning Path:\nAzle -> location.citytown.postal_codes -> 76020 -> common.topic.notable_for -> g.1258t5sn5\n# Answer:\n76020", "# Reasoning Path:\nAzle -> location.citytown.postal_codes -> 76020 -> location.postal_code.country -> United States of America\n# Answer:\n76020", "# Reasoning Path:\nAzle -> location.citytown.postal_codes -> 76098 -> common.topic.notable_for -> g.125ccr2vm\n# Answer:\n76098", "# Reasoning Path:\nAzle -> location.citytown.postal_codes -> 76020 -> common.topic.notable_types -> Postal Code\n# Answer:\n76020", "# Reasoning Path:\nAzle -> location.statistical_region.population -> g.11b66gyw0f\n# Answer:\ng.11b66gyw0f", "# Reasoning Path:\nAzle -> location.citytown.postal_codes -> 76098 -> common.topic.notable_types -> Postal Code\n# Answer:\n76098", "# Reasoning Path:\nAzle -> location.citytown.postal_codes -> 76098 -> location.postal_code.country -> United States of America\n# Answer:\n76098", "# Reasoning Path:\nAzle -> location.statistical_region.population -> g.11bymmps1m\n# Answer:\ng.11bymmps1m", "# Reasoning Path:\nAzle -> location.statistical_region.population -> g.11x1ct1cb\n# Answer:\ng.11x1ct1cb", "# Reasoning Path:\nAzle -> location.location.containedby -> Parker County -> location.location.containedby -> Fort Worth-Arlington, TX Metropolitan Division\n# Answer:\nParker County"], "ground_truth": ["76020", "76098"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-894", "prediction": ["# Reasoning Path:\nSally Pearson -> olympics.olympic_athlete.medals_won -> m.04hdw0b -> olympics.olympic_medal_honor.event -> Athletics at the 2008 Summer Olympics - Women's 100 metres hurdles\n# Answer:\nm.04hdw0b", "# Reasoning Path:\nSally Pearson -> olympics.olympic_athlete.medals_won -> m.0kl1f80 -> olympics.olympic_medal_honor.event -> Athletics at the 2012 Summer Olympics \u2013 Women's 100 metres hurdles\n# Answer:\nm.0kl1f80", "# Reasoning Path:\nSally Pearson -> olympics.olympic_athlete.medals_won -> m.04hdw0b -> olympics.olympic_medal_honor.country -> Australia\n# Answer:\nm.04hdw0b", "# Reasoning Path:\nSally Pearson -> olympics.olympic_athlete.medals_won -> m.0kl1f80 -> olympics.olympic_medal_honor.olympics -> The London 2012 Summer Olympics\n# Answer:\nm.0kl1f80", "# Reasoning Path:\nSally Pearson -> common.topic.webpage -> m.0kdqgcl -> common.webpage.category -> About\n# Answer:\nm.0kdqgcl", "# Reasoning Path:\nSally Pearson -> people.person.spouse_s -> m.0j6k6kp -> people.marriage.spouse -> Kieran Pearson\n# Answer:\nm.0j6k6kp", "# Reasoning Path:\nSally Pearson -> common.topic.webpage -> m.0kdqgcl -> common.webpage.resource -> 2012 Summer Olympics Athlete NBC Site\n# Answer:\nm.0kdqgcl", "# Reasoning Path:\nSally Pearson -> people.person.spouse_s -> m.0j6k6kp -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0j6k6kp"], "ground_truth": ["The London 2012 Summer Olympics"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.125, "path_recall": 0.3333333333333333, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-895", "prediction": ["# Reasoning Path:\nDetroit Pistons -> sports.sports_team.championships -> 2004 NBA Finals -> common.topic.notable_for -> g.1254zz_2p\n# Answer:\n2004 NBA Finals", "# Reasoning Path:\nDetroit Pistons -> sports.sports_team.championships -> 2004 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n2004 NBA Finals", "# Reasoning Path:\nDetroit Pistons -> sports.sports_team.championships -> 1989 NBA Finals -> common.topic.article -> m.08x9ts\n# Answer:\n1989 NBA Finals", "# Reasoning Path:\nDetroit Pistons -> sports.sports_team.championships -> 2004 NBA Finals -> sports.sports_championship_event.runner_up -> Los Angeles Lakers\n# Answer:\n2004 NBA Finals", "# Reasoning Path:\nDetroit Pistons -> sports.sports_team.championships -> 1989 NBA Finals -> common.topic.notable_for -> g.12550n6gn\n# Answer:\n1989 NBA Finals", "# Reasoning Path:\nDetroit Pistons -> sports.sports_team.championships -> 1989 NBA Finals -> sports.sports_championship_event.runner_up -> Los Angeles Lakers\n# Answer:\n1989 NBA Finals", "# Reasoning Path:\nDetroit Pistons -> sports.sports_team.championships -> 1990 NBA Finals -> sports.sports_championship_event.season -> 1989\u201390 NBA season\n# Answer:\n1990 NBA Finals", "# Reasoning Path:\nDetroit Pistons -> sports.sports_team.roster -> m.010fy68k -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nm.010fy68k", "# Reasoning Path:\nDetroit Pistons -> sports.sports_team.championships -> 1990 NBA Finals -> common.topic.article -> m.08x9rp\n# Answer:\n1990 NBA Finals", "# Reasoning Path:\nDetroit Pistons -> award.award_nominee.award_nominations -> m.0sgq5w4 -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nm.0sgq5w4"], "ground_truth": ["2004 NBA Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-896", "prediction": ["# Reasoning Path:\nPhiladelphia Flyers -> sports.sports_team.championships -> 1974 Stanley Cup Finals -> sports.sports_championship_event.runner_up -> Boston Bruins\n# Answer:\n1974 Stanley Cup Finals", "# Reasoning Path:\nPhiladelphia Flyers -> common.topic.article -> m.0hn2_\n# Answer:\nm.0hn2_", "# Reasoning Path:\nPhiladelphia Flyers -> sports.sports_team.championships -> 1974 Stanley Cup Finals -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1974 Stanley Cup Finals", "# Reasoning Path:\nPhiladelphia Flyers -> sports.sports_team.championships -> 1975 Stanley Cup Finals -> common.topic.article -> m.03by14z\n# Answer:\n1975 Stanley Cup Finals", "# Reasoning Path:\nPhiladelphia Flyers -> common.topic.notable_for -> g.1256f_q8t\n# Answer:\ng.1256f_q8t", "# Reasoning Path:\nPhiladelphia Flyers -> sports.sports_team.championships -> 1974 Stanley Cup Finals -> sports.sports_championship_event.season -> 1973\u201374 NHL season\n# Answer:\n1974 Stanley Cup Finals", "# Reasoning Path:\nPhiladelphia Flyers -> sports.sports_team.championships -> 1975 Stanley Cup Finals -> common.topic.notable_for -> g.125dc4pr3\n# Answer:\n1975 Stanley Cup Finals"], "ground_truth": ["1975 Stanley Cup Finals", "1974 Stanley Cup Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8333333333333333, "ans_precission": 0.7142857142857143, "ans_recall": 1.0, "path_f1": 0.8333333333333333, "path_precision": 0.7142857142857143, "path_recall": 1.0, "path_ans_f1": 0.8333333333333333, "path_ans_precision": 0.7142857142857143, "path_ans_recall": 1.0}
{"id": "WebQTest-898", "prediction": ["# Reasoning Path:\nSt. Pauls -> location.location.containedby -> Robeson County -> location.location.containedby -> North Carolina\n# Answer:\nRobeson County", "# Reasoning Path:\nSt. Pauls -> location.location.containedby -> North Carolina -> location.location.containedby -> United States of America\n# Answer:\nNorth Carolina", "# Reasoning Path:\nSt. Pauls -> location.location.containedby -> Robeson County -> location.location.containedby -> Eastern North Carolina\n# Answer:\nRobeson County", "# Reasoning Path:\nSt. Pauls -> location.hud_county_place.county -> Robeson County -> location.location.containedby -> Eastern North Carolina\n# Answer:\nRobeson County", "# Reasoning Path:\nSt. Pauls -> base.wikipedia_infobox.settlement.area_code -> Area code 910 -> location.location.containedby -> North Carolina\n# Answer:\nArea code 910", "# Reasoning Path:\nSt. Pauls -> location.location.containedby -> Robeson County -> location.statistical_region.population -> g.11b66dwnq2\n# Answer:\nRobeson County", "# Reasoning Path:\nSt. Pauls -> location.location.containedby -> North Carolina -> base.locations.states_and_provences.country -> United States of America\n# Answer:\nNorth Carolina", "# Reasoning Path:\nSt. Pauls -> location.location.containedby -> Robeson County -> location.us_county.hud_county_place -> Barker Ten Mile\n# Answer:\nRobeson County", "# Reasoning Path:\nSt. Pauls -> location.location.containedby -> North Carolina -> base.aareas.schema.administrative_area.capital -> Raleigh\n# Answer:\nNorth Carolina", "# Reasoning Path:\nSt. Pauls -> location.hud_county_place.county -> Robeson County -> location.location.containedby -> North Carolina\n# Answer:\nRobeson County"], "ground_truth": ["Robeson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-899", "prediction": ["# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Jefferson County -> location.location.containedby -> Washington\n# Answer:\nJefferson County", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Washington -> location.location.containedby -> Contiguous United States\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> base.usnationalparks.us_national_park.state -> Washington -> location.location.containedby -> Contiguous United States\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> common.topic.notable_for -> g.1257hvh8r\n# Answer:\ng.1257hvh8r", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Jefferson County -> location.statistical_region.population -> g.11b66j25ww\n# Answer:\nJefferson County", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Washington -> location.location.containedby -> Northwestern United States\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> base.usnationalparks.us_national_park.state -> Washington -> location.location.containedby -> Northwestern United States\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Jefferson County -> location.location.adjoin_s -> m.03jq636\n# Answer:\nJefferson County", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Washington -> location.location.containedby -> Pacific Northwest\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> base.usnationalparks.us_national_park.state -> Washington -> location.location.containedby -> Pacific Northwest\n# Answer:\nWashington"], "ground_truth": ["Jefferson County", "Washington"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Arthur Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.spouse_s -> m.0j4ks8g\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Donald Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.spouse -> Pat Nixon\n# Answer:\nm.02h98gq", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Edward Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> freebase.valuenotation.has_value -> Place of death\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.person.children -> Arthur Nixon\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.02h98gq", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.location_of_ceremony -> The Mission Inn Hotel & Spa\n# Answer:\nm.02h98gq", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.person.children -> Donald Nixon\n# Answer:\nFrancis A. Nixon"], "ground_truth": ["Pat Nixon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.11764705882352941, "path_precision": 0.1, "path_recall": 0.14285714285714285, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nm.07t6_k3", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.award -> NAACP Image Award for Outstanding Actress in a Comedy Series\n# Answer:\nm.0_yc_3q", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.ceremony -> 37th Primetime Emmy Awards\n# Answer:\nm.07t6_k3", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Supporting Actress in a Comedy Series\n# Answer:\nm.07t6_k3", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07nvvbj -> award.award_nomination.award_nominee -> Sherman Hemsley\n# Answer:\nm.07nvvbj", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.ceremony -> 16th NAACP Image Awards\n# Answer:\nm.0_yc_3q", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 0 -> tv.tv_series_season.episodes -> Entertainment Tonight: Whatever Happened To...\n# Answer:\nThe Jeffersons - Season 0", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0_yczwx -> award.award_honor.award_winner -> Marla Gibbs\n# Answer:\nm.0_yczwx", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 0 -> common.topic.notable_for -> g.1yl5pbtsv\n# Answer:\nThe Jeffersons - Season 0", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07nvvbj -> award.award_nomination.ceremony -> 36th Primetime Emmy Awards\n# Answer:\nm.07nvvbj"], "ground_truth": ["Sherman Hemsley", "Berlinda Tolbert", "Paul Benedict", "Isabel Sanford", "Jay Hammer", "Franklin Cover", "Roxie Roker", "Zara Cully", "Mike Evans", "Damon Evans", "Marla Gibbs"], "ans_acc": 0.18181818181818182, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.08695652173913043, "path_precision": 0.3, "path_recall": 0.05084745762711865, "path_ans_f1": 0.22641509433962265, "path_ans_precision": 0.3, "path_ans_recall": 0.18181818181818182}
{"id": "WebQTest-900", "prediction": ["# Reasoning Path:\nNat Turner's slave rebellion -> time.event.locations -> Southampton County -> location.location.containedby -> Virginia\n# Answer:\nSouthampton County", "# Reasoning Path:\nNat Turner's slave rebellion -> time.event.locations -> Southampton County -> base.aareas.schema.administrative_area.administrative_parent -> Virginia\n# Answer:\nSouthampton County", "# Reasoning Path:\nNat Turner's slave rebellion -> time.event.locations -> Southampton County -> location.location.contains -> 23827\n# Answer:\nSouthampton County", "# Reasoning Path:\nNat Turner's slave rebellion -> time.event.locations -> Southampton County -> location.location.containedby -> United States of America\n# Answer:\nSouthampton County", "# Reasoning Path:\nNat Turner's slave rebellion -> symbols.namesake.named_after -> Nat Turner -> people.person.place_of_birth -> Southampton County\n# Answer:\nNat Turner", "# Reasoning Path:\nNat Turner's slave rebellion -> common.topic.notable_types -> Event -> freebase.type_profile.published -> Published\n# Answer:\nEvent", "# Reasoning Path:\nNat Turner's slave rebellion -> time.event.locations -> Southampton County -> location.location.contains -> 23828\n# Answer:\nSouthampton County", "# Reasoning Path:\nNat Turner's slave rebellion -> symbols.namesake.named_after -> Nat Turner -> people.deceased_person.place_of_death -> Courtland\n# Answer:\nNat Turner", "# Reasoning Path:\nNat Turner's slave rebellion -> time.event.locations -> Southampton County -> location.location.contains -> 23829\n# Answer:\nSouthampton County", "# Reasoning Path:\nNat Turner's slave rebellion -> common.topic.notable_types -> Event -> type.type.expected_by -> event\n# Answer:\nEvent"], "ground_truth": ["Southampton County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-901", "prediction": ["# Reasoning Path:\nLuke Skywalker -> film.film_character.portrayed_in_films -> m.010fs6tg -> film.performance.actor -> Mark Hamill\n# Answer:\nm.010fs6tg", "# Reasoning Path:\nLuke Skywalker -> film.film_character.portrayed_in_films -> m.0k6jzc -> film.performance.actor -> Mark Hamill\n# Answer:\nm.0k6jzc", "# Reasoning Path:\nLuke Skywalker -> film.film_character.portrayed_in_films -> m.0k3r3j -> film.performance.actor -> Mark Hamill\n# Answer:\nm.0k3r3j", "# Reasoning Path:\nLuke Skywalker -> tv.tv_character.appeared_in_tv_program -> m.0gy7h8w -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nm.0gy7h8w", "# Reasoning Path:\nLuke Skywalker -> film.film_character.portrayed_in_films -> m.010fs6tg -> film.performance.film -> Star Wars: The Force Awakens\n# Answer:\nm.010fs6tg", "# Reasoning Path:\nLuke Skywalker -> tv.tv_character.appeared_in_tv_program -> m.0gy7h8w -> tv.regular_tv_appearance.actor -> Mark Hamill\n# Answer:\nm.0gy7h8w", "# Reasoning Path:\nLuke Skywalker -> film.film_character.portrayed_in_films -> m.0k6jzc -> film.performance.film -> Star Wars Holiday Special\n# Answer:\nm.0k6jzc", "# Reasoning Path:\nLuke Skywalker -> film.film_character.portrayed_in_films -> m.0k3r3j -> film.performance.film -> Return of the Jedi\n# Answer:\nm.0k3r3j", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.powers_or_abilities -> Force -> fictional_universe.character_powers.characters_with_this_ability -> Darth Maul\n# Answer:\nForce", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.powers_or_abilities -> Force -> common.topic.notable_for -> g.125f342hs\n# Answer:\nForce"], "ground_truth": ["Mark Hamill"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.47058823529411764, "path_precision": 0.4, "path_recall": 0.5714285714285714, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-903", "prediction": ["# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> Austria -> location.location.containedby -> Eurasia\n# Answer:\nAustria", "# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> Austria -> location.country.languages_spoken -> Bosnian language\n# Answer:\nAustria", "# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> Austria -> location.location.containedby -> Europe\n# Answer:\nAustria", "# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> Austria -> location.location.containedby -> Western Europe\n# Answer:\nAustria", "# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> Austria -> location.country.languages_spoken -> Turkish Language\n# Answer:\nAustria", "# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> Austria -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics\n# Answer:\nAustria", "# Reasoning Path:\nTurkish people -> people.ethnicity.people -> Al-Mu'tasim -> people.person.nationality -> Abbasid Caliphate\n# Answer:\nAl-Mu'tasim", "# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> Belgium -> location.country.languages_spoken -> Dutch Language\n# Answer:\nBelgium", "# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> Belgium -> location.location.containedby -> Eurasia\n# Answer:\nBelgium", "# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> Belgium -> periodicals.newspaper_circulation_area.newspapers -> C\u00f3ndor\n# Answer:\nBelgium"], "ground_truth": ["Belgium", "Syria", "Algeria", "Turkey", "Republic of Kosovo", "Kazakhstan", "Canada", "Bulgaria", "France", "Iraq", "United States of America", "Sweden", "Netherlands", "Germany", "Kyrgyzstan", "Azerbaijan", "Norway", "United Kingdom", "Bosnia and Herzegovina", "Northern Cyprus", "Denmark", "Austria"], "ans_acc": 0.09090909090909091, "ans_hit": 1, "ans_f1": 0.16513761467889906, "ans_precission": 0.9, "ans_recall": 0.09090909090909091, "path_f1": 0.16513761467889906, "path_precision": 0.9, "path_recall": 0.09090909090909091, "path_ans_f1": 0.16513761467889906, "path_ans_precision": 0.9, "path_ans_recall": 0.09090909090909091}
{"id": "WebQTest-904", "prediction": ["# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> base.biblioness.bibs_location.state -> Hawaii\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> location.location.containedby -> Hawaii\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Hawaii\n# Answer:\nUnited States of America", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Writer's Journey to Selma, Alabama -> book.written_work.subjects -> African American\n# Answer:\nA Writer's Journey to Selma, Alabama", "# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> location.location.containedby -> United States of America\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Illinois\n# Answer:\nUnited States of America", "# Reasoning Path:\nBarack Obama -> people.person.nationality -> United States of America -> location.location.primarily_containedby -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nBarack Obama -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> California\n# Answer:\nUnited States of America", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Writer's Journey to Selma, Alabama -> book.written_work.subjects -> Selma\n# Answer:\nA Writer's Journey to Selma, Alabama"], "ground_truth": ["Hawaii"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-905", "prediction": ["# Reasoning Path:\nJoe Montana -> sports.sports_award_winner.awards -> m.04kcv8k -> sports.sports_award.season -> 1981 NFL season\n# Answer:\nm.04kcv8k", "# Reasoning Path:\nJoe Montana -> sports.pro_athlete.teams -> m.04nk3bx -> sports.sports_team_roster.team -> San Francisco 49ers\n# Answer:\nm.04nk3bx", "# Reasoning Path:\nJoe Montana -> sports.pro_athlete.teams -> m.04nk3bx -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.04nk3bx", "# Reasoning Path:\nJoe Montana -> sports.sports_award_winner.awards -> m.04kcv8k -> sports.sports_award.award -> Super Bowl Most Valuable Player Award\n# Answer:\nm.04kcv8k", "# Reasoning Path:\nJoe Montana -> sports.sports_award_winner.awards -> m.04kcvcr -> sports.sports_award.season -> 1989 NFL season\n# Answer:\nm.04kcvcr", "# Reasoning Path:\nJoe Montana -> sports.pro_athlete.teams -> m.04nk3c7 -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.04nk3c7", "# Reasoning Path:\nJoe Montana -> sports.sports_award_winner.awards -> m.04kcv46 -> sports.sports_award.season -> 1984 NFL season\n# Answer:\nm.04kcv46", "# Reasoning Path:\nJoe Montana -> sports.pro_athlete.teams -> m.04nk3c7 -> sports.sports_team_roster.team -> Kansas City Chiefs\n# Answer:\nm.04nk3c7", "# Reasoning Path:\nJoe Montana -> sports.pro_athlete.teams -> m.04nk3cl -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.04nk3cl", "# Reasoning Path:\nJoe Montana -> sports.pro_athlete.teams -> m.04nk3cl -> sports.sports_team_roster.team -> Notre Dame Fighting Irish football\n# Answer:\nm.04nk3cl"], "ground_truth": ["1990-01-28", "1985-01-20", "1982-01-24", "1989-01-22"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-908", "prediction": ["# Reasoning Path:\nGary Paulsen -> award.award_nominee.award_nominations -> m.09kbw63 -> award.award_nomination.award -> Dorothy Canfield Fisher Children's Book Award\n# Answer:\nm.09kbw63", "# Reasoning Path:\nGary Paulsen -> award.award_winner.awards_won -> m.09kkr7w -> award.award_honor.award -> Anne V. Zarrow Award for Young Readers' Literature\n# Answer:\nm.09kkr7w", "# Reasoning Path:\nGary Paulsen -> award.award_nominee.award_nominations -> m.09kbw63 -> award.award_nomination.nominated_for -> How Angel Peterson Got His Name\n# Answer:\nm.09kbw63", "# Reasoning Path:\nGary Paulsen -> award.award_nominee.award_nominations -> m.09kby6s -> award.award_nomination.award -> Dorothy Canfield Fisher Children's Book Award\n# Answer:\nm.09kby6s", "# Reasoning Path:\nGary Paulsen -> award.award_nominee.award_nominations -> m.0zcvpyg -> award.award_nomination.award -> John Newbery Medal\n# Answer:\nm.0zcvpyg", "# Reasoning Path:\nGary Paulsen -> award.award_winner.awards_won -> m.046cq91 -> award.award_honor.award -> Regina Medal\n# Answer:\nm.046cq91", "# Reasoning Path:\nGary Paulsen -> award.award_nominee.award_nominations -> m.09kby6s -> award.award_nomination.nominated_for -> Guts: The True Stories Behind Hatchet and the Brian Books\n# Answer:\nm.09kby6s", "# Reasoning Path:\nGary Paulsen -> award.award_winner.awards_won -> m.087149q -> award.award_honor.award -> Dorothy Canfield Fisher Children's Book Award\n# Answer:\nm.087149q", "# Reasoning Path:\nGary Paulsen -> award.award_nominee.award_nominations -> m.0zcvpyg -> freebase.valuenotation.is_reviewed -> Award\n# Answer:\nm.0zcvpyg", "# Reasoning Path:\nGary Paulsen -> award.award_winner.awards_won -> m.087149q -> award.award_honor.honored_for -> Hatchet\n# Answer:\nm.087149q"], "ground_truth": ["Dorothy Canfield Fisher Children's Book Award", "Regina Medal", "Anne V. Zarrow Award for Young Readers' Literature", "Margaret Edwards Award", "John Newbery Medal", "Spur Award for Best Juvenile Fiction", "Spur Award for Best Juvenile Nonfiction"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.6, "path_recall": 0.42857142857142855, "path_ans_f1": 0.5853658536585366, "path_ans_precision": 0.6, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Street Sheet -> book.periodical.format -> m.02npbt7\n# Answer:\nStreet Sheet", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> The Golden Era -> common.topic.article -> m.04zg6d\n# Answer:\nThe Golden Era", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Street Sheet -> book.newspaper.owner -> Coalition on Homelessness\n# Answer:\nStreet Sheet", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Times -> book.periodical.frequency_or_issues_per_year -> m.09s58j6\n# Answer:\nSan Francisco Bay Times", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> The Golden Era -> common.topic.notable_for -> g.1258tnwlj\n# Answer:\nThe Golden Era", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Times -> common.topic.notable_for -> g.125d1300_\n# Answer:\nSan Francisco Bay Times", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Times -> common.topic.webpage -> m.03kz_rr\n# Answer:\nSan Francisco Bay Times", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94101 -> location.postal_code.country -> United States of America\n# Answer:\n94101", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> common.topic.notable_for -> g.1258179ql\n# Answer:\n1906", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1st to Die -> book.written_work.subjects -> California\n# Answer:\n1st to Die"], "ground_truth": ["Dock of the Bay", "San Francisco Chronicle", "Bay Area Reporter", "Synapse", "San Francisco Business Times", "Free Society", "AsianWeek", "California Star", "San Francisco Daily", "San Francisco Bay Times", "San Francisco Call", "San Francisco Bay Guardian", "The Golden Era", "Street Sheet", "San Francisco News-Call Bulletin Newspaper", "The Daily Alta California", "Sing Tao Daily", "The San Francisco Examiner", "San Francisco Bay View", "San Francisco Foghorn"], "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0.24705882352941178, "ans_precission": 0.7, "ans_recall": 0.15, "path_f1": 0.24705882352941178, "path_precision": 0.7, "path_recall": 0.15, "path_ans_f1": 0.24705882352941178, "path_ans_precision": 0.7, "path_ans_recall": 0.15}
{"id": "WebQTest-910", "prediction": ["# Reasoning Path:\nAlexander Graham Bell -> people.deceased_person.place_of_death -> Beinn Bhreagh, Nova Scotia -> common.topic.article -> m.08npxf\n# Answer:\nBeinn Bhreagh, Nova Scotia", "# Reasoning Path:\nAlexander Graham Bell -> people.deceased_person.place_of_death -> Beinn Bhreagh, Nova Scotia -> common.topic.notable_for -> g.1258ng70q\n# Answer:\nBeinn Bhreagh, Nova Scotia", "# Reasoning Path:\nAlexander Graham Bell -> people.deceased_person.place_of_death -> Beinn Bhreagh, Nova Scotia -> location.location.geolocation -> m.02_t0ql\n# Answer:\nBeinn Bhreagh, Nova Scotia", "# Reasoning Path:\nAlexander Graham Bell -> people.person.profession -> Businessperson -> base.descriptive_names.names.descriptive_name -> m.0100z128\n# Answer:\nBusinessperson", "# Reasoning Path:\nAlexander Graham Bell -> book.author.works_written -> Discovery and invention -> common.topic.notable_for -> g.1257cwk61\n# Answer:\nDiscovery and invention", "# Reasoning Path:\nAlexander Graham Bell -> book.author.works_written -> Graphical studies of marriages of the deaf -> common.topic.notable_types -> Book\n# Answer:\nGraphical studies of marriages of the deaf", "# Reasoning Path:\nAlexander Graham Bell -> people.person.profession -> Businessperson -> base.descriptive_names.names.descriptive_name -> m.0105_bmw\n# Answer:\nBusinessperson", "# Reasoning Path:\nAlexander Graham Bell -> people.person.profession -> Businessperson -> people.profession.specializations -> Actuary\n# Answer:\nBusinessperson", "# Reasoning Path:\nAlexander Graham Bell -> people.person.profession -> Engineer -> business.job_title.people_with_this_title -> m.012nc26f\n# Answer:\nEngineer", "# Reasoning Path:\nAlexander Graham Bell -> book.author.works_written -> Discovery and invention -> common.topic.notable_types -> Book\n# Answer:\nDiscovery and invention"], "ground_truth": ["Beinn Bhreagh, Nova Scotia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-911", "prediction": ["# Reasoning Path:\nGolden State Warriors -> sports.sports_team.championships -> 1975 NBA Finals -> sports.sports_championship_event.season -> 1974\u201375 NBA season\n# Answer:\n1975 NBA Finals", "# Reasoning Path:\nGolden State Warriors -> sports.sports_team.championships -> 1975 NBA Finals -> sports.sports_championship_event.runner_up -> Washington Wizards\n# Answer:\n1975 NBA Finals", "# Reasoning Path:\nGolden State Warriors -> tv.tv_actor.guest_roles -> m.0ghllvw -> tv.tv_guest_role.episodes_appeared_in -> 1975 NBA Finals Game 3\n# Answer:\nm.0ghllvw", "# Reasoning Path:\nGolden State Warriors -> tv.tv_actor.guest_roles -> m.0ghpsgt -> tv.tv_guest_role.episodes_appeared_in -> 1975 NBA Finals Game 4\n# Answer:\nm.0ghpsgt", "# Reasoning Path:\nGolden State Warriors -> basketball.basketball_team.previous_coaches -> m.010gy4q9 -> basketball.basketball_historical_coach_position.coach -> Mark Jackson\n# Answer:\nm.010gy4q9", "# Reasoning Path:\nGolden State Warriors -> tv.tv_actor.guest_roles -> m.0ghn9j6 -> tv.tv_guest_role.episodes_appeared_in -> 1975 NBA Finals Game 2\n# Answer:\nm.0ghn9j6", "# Reasoning Path:\nGolden State Warriors -> basketball.basketball_team.previous_coaches -> m.04nsd2d -> basketball.basketball_historical_coach_position.coach -> Rick Adelman\n# Answer:\nm.04nsd2d", "# Reasoning Path:\nGolden State Warriors -> basketball.basketball_team.previous_coaches -> m.04nscwm -> basketball.basketball_historical_coach_position.coach -> Don Nelson\n# Answer:\nm.04nscwm"], "ground_truth": ["1975 NBA Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-912", "prediction": ["# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Revaluation of the Turkish Lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nRevaluation of the Turkish Lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.065sxsl -> military.military_combatant_group.conflict -> Korean War\n# Answer:\nm.065sxsl", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.066402y -> military.military_combatant_group.conflict -> Iraqi no-fly zones\n# Answer:\nm.066402y"], "ground_truth": ["Turkish lira"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-914", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> base.aareas.schema.administrative_area.administrative_children -> Anglesey\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Scotland -> location.country.first_level_divisions -> Aberdeen\n# Answer:\nScotland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> base.aareas.schema.administrative_area.administrative_children -> Blaenau Gwent\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> location.country.first_level_divisions -> Anglesey\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Down\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> base.aareas.schema.administrative_area.administrative_children -> Bridgend County Borough\n# Answer:\nWales"], "ground_truth": ["England", "Wales", "Scotland", "Northern Ireland"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-915", "prediction": ["# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> sports.sports_championship_event.season -> 2000 NFL season\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> common.topic.notable_types -> Super bowl\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.article -> m.0642vqz\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> time.event.locations -> Raymond James Stadium\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> 2001 AFC Championship Game -> sports.sports_championship_event.runner_up -> Oakland Raiders\n# Answer:\n2001 AFC Championship Game", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> film.film_subject.films -> The Band That Wouldn't Die -> film.film.music -> Marcello Zavros\n# Answer:\nThe Band That Wouldn't Die", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> sports.sports_championship_event.season -> 2012 NFL season\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> 2001 AFC Championship Game -> sports.sports_championship_event.season -> 2000 NFL season\n# Answer:\n2001 AFC Championship Game", "# Reasoning Path:\nBaltimore Ravens -> film.film_subject.films -> The Band That Wouldn't Die -> common.topic.image -> Baltimore Ravens Marching Band\n# Answer:\nThe Band That Wouldn't Die"], "ground_truth": ["Super Bowl XLVII", "2013 AFC Championship Game", "2001 AFC Championship Game", "Super Bowl XXXV"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.8, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.8, "path_ans_recall": 0.75}
{"id": "WebQTest-916", "prediction": ["# Reasoning Path:\nRussia -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.countries_spoken_in -> Kazakhstan\n# Answer:\nUkrainian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.main_country -> Ukraine\n# Answer:\nUkrainian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.region -> Europe\n# Answer:\nUkrainian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.countries_spoken_in -> Soviet Union\n# Answer:\nUkrainian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Russian Language -> language.human_language.countries_spoken_in -> Belarus\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Russian Language -> common.topic.notable_types -> Human Language\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Azerbaijani language -> language.human_language.language_family -> Altaic languages\n# Answer:\nAzerbaijani language", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.11b60vv5th\n# Answer:\ng.11b60vv5th", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.countries_spoken_in -> Ukraine\n# Answer:\nUkrainian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Russian Language -> language.human_language.region -> Eurasia\n# Answer:\nRussian Language"], "ground_truth": ["Lezgi Language", "Azerbaijani language", "Lak Language", "Erzya Language", "Ingush Language", "Nogai Language", "Ukrainian Language", "Udmurt Language", "Osetin Language", "Aghul language", "Yiddish Language", "Buryat language", "Mari language", "Rutul language", "Tatar Language", "Tabassaran Language", "Adyghe Language", "Moksha Language", "Tsakhur Language", "Crimean Turkish Language", "Bashkir Language", "Kalmyk-Oirat Language", "Altai language", "Yakut Language", "Kumyk Language", "Abaza Language", "Komi language", "Karachay-Balkar Language", "Tuvin Language", "Chechen Language", "Russian Language", "Khakas Language", "Kabardian Language", "Avar Language", "Dargwa Language"], "ans_acc": 0.08571428571428572, "ans_hit": 1, "ans_f1": 0.1565217391304348, "ans_precission": 0.9, "ans_recall": 0.08571428571428572, "path_f1": 0.1043478260869565, "path_precision": 0.6, "path_recall": 0.05714285714285714, "path_ans_f1": 0.1565217391304348, "path_ans_precision": 0.9, "path_ans_recall": 0.08571428571428572}
{"id": "WebQTest-918", "prediction": ["# Reasoning Path:\nCambodia -> royalty.kingdom.rulers -> Ang Duong -> freebase.valuenotation.has_value -> Place of birth\n# Answer:\nAng Duong", "# Reasoning Path:\nCambodia -> government.governmental_jurisdiction.governing_officials -> m.010ggj4c -> government.government_position_held.office_holder -> Hun Sen\n# Answer:\nm.010ggj4c", "# Reasoning Path:\nCambodia -> royalty.kingdom.rulers -> Ang Duong -> people.person.gender -> Male\n# Answer:\nAng Duong", "# Reasoning Path:\nCambodia -> royalty.kingdom.rulers -> Norodom of Cambodia -> people.person.gender -> Male\n# Answer:\nNorodom of Cambodia", "# Reasoning Path:\nCambodia -> royalty.kingdom.rulers -> Norodom of Cambodia -> people.deceased_person.place_of_death -> Phnom Penh\n# Answer:\nNorodom of Cambodia", "# Reasoning Path:\nCambodia -> royalty.kingdom.rulers -> Ang Mey -> people.person.religion -> Theravada\n# Answer:\nAng Mey", "# Reasoning Path:\nCambodia -> government.governmental_jurisdiction.governing_officials -> m.010ggj4c -> government.government_position_held.basic_title -> Second Prime Minister\n# Answer:\nm.010ggj4c", "# Reasoning Path:\nCambodia -> government.governmental_jurisdiction.governing_officials -> m.010ggj20 -> government.government_position_held.office_holder -> Norodom Ranariddh\n# Answer:\nm.010ggj20", "# Reasoning Path:\nCambodia -> government.governmental_jurisdiction.governing_officials -> m.010ggj6j -> government.government_position_held.office_holder -> Ung Huot\n# Answer:\nm.010ggj6j", "# Reasoning Path:\nCambodia -> royalty.kingdom.rulers -> Ang Mey -> common.topic.notable_types -> Noble person\n# Answer:\nAng Mey"], "ground_truth": ["Norodom Sihamoni"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-919", "prediction": ["# Reasoning Path:\nSerena Williams -> people.person.parents -> Oracene Price -> people.person.education -> m.0n1kw8m\n# Answer:\nOracene Price", "# Reasoning Path:\nSerena Williams -> people.person.parents -> Richard Williams -> people.person.parents -> Julia Mae Williams\n# Answer:\nRichard Williams", "# Reasoning Path:\nSerena Williams -> people.person.parents -> Oracene Price -> people.person.children -> Venus Williams\n# Answer:\nOracene Price", "# Reasoning Path:\nSerena Williams -> people.person.parents -> Oracene Price -> common.topic.notable_for -> g.1256r8wrk\n# Answer:\nOracene Price", "# Reasoning Path:\nSerena Williams -> people.person.parents -> Richard Williams -> common.topic.notable_types -> Person or entity appearing in film\n# Answer:\nRichard Williams", "# Reasoning Path:\nSerena Williams -> base.siswimsuitmodels.si_swimsuit_model.issues -> m.05dy_g1 -> base.siswimsuitmodels.si_swimsuit_appearance.year -> 2003 Sports Illustrated Swimsuit Issue\n# Answer:\nm.05dy_g1", "# Reasoning Path:\nSerena Williams -> people.person.parents -> Richard Williams -> base.schemastaging.sports_coach.athletes_coached -> m.0h_k9kf\n# Answer:\nRichard Williams", "# Reasoning Path:\nSerena Williams -> award.award_nominee.award_nominations -> m.010_zz0q -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nm.010_zz0q", "# Reasoning Path:\nSerena Williams -> people.person.parents -> Richard Williams -> base.schemastaging.sports_coach.athletes_coached -> m.0h_k9mh\n# Answer:\nRichard Williams", "# Reasoning Path:\nSerena Williams -> award.award_nominee.award_nominations -> m.010_zz0q -> award.award_nomination.ceremony -> 2014 ESPY Awards\n# Answer:\nm.010_zz0q"], "ground_truth": ["Oracene Price", "Richard Williams"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-920", "prediction": ["# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncxj8d -> education.education.institution -> Trinity College, Cambridge\n# Answer:\nm.0ncxj8d", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.02kq1mm -> education.education.institution -> Trinity College, Cambridge\n# Answer:\nm.02kq1mm", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncl4g6 -> education.education.institution -> The King's School, Grantham\n# Answer:\nm.0ncl4g6", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncxj8d -> education.education.degree -> Master of Arts\n# Answer:\nm.0ncxj8d", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.02kq1mm -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nm.02kq1mm", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncl4g6 -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nm.0ncl4g6", "# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> Newton -> astronomy.extraterrestrial_location.geolocation -> m.0w2sw5y\n# Answer:\nNewton", "# Reasoning Path:\nIsaac Newton -> base.kwebbase.kwtopic.connections_to -> abraham de moivre studied work of isaac newton -> base.kwebbase.kwconnection.subject -> Abraham de Moivre\n# Answer:\nabraham de moivre studied work of isaac newton", "# Reasoning Path:\nIsaac Newton -> base.kwebbase.kwtopic.connections_to -> anton van leeuwenhoek royal society member with isaac newton -> base.kwebbase.kwconnection.sentence -> In 1680 Leeuwenhoek was elected a full member of the Royal Society, joining Newton, Hooke, Oldenburg, Boyle, Wren,  Edmond Halley, and other scientific luminaries of his day, who exchanged letters with him, suggesting subjects for his research.\n# Answer:\nanton van leeuwenhoek royal society member with isaac newton", "# Reasoning Path:\nIsaac Newton -> base.kwebbase.kwtopic.connections_to -> abraham de moivre studied work of isaac newton -> base.kwebbase.kwconnection.sentence -> On his release in 1686, he fled to England, where he made a modest living tutoring in mathematics, and continued to study recent mathematical texts, the most notable being Newton's 'Principia Mathematica.' In 1692 he met the astronomer Halley who was so impressed by him that he read his first paper (on Newton's calculus), to the Royal Society in 1695.\n# Answer:\nabraham de moivre studied work of isaac newton"], "ground_truth": ["Trinity College, Cambridge", "The King's School, Grantham"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-921", "prediction": ["# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> medicine.disease.notable_people_with_this_condition -> Alan Joyce\n# Answer:\nProstate cancer", "# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> medicine.disease.notable_people_with_this_condition -> Andrew Grove\n# Answer:\nProstate cancer", "# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> common.topic.webpage -> m.04r0btg\n# Answer:\nProstate cancer", "# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> medicine.disease.notable_people_with_this_condition -> Billy Connolly\n# Answer:\nProstate cancer", "# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjf8n\n# Answer:\nProstate cancer", "# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> common.topic.webpage -> m.09wv45d\n# Answer:\nProstate cancer", "# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> common.topic.webpage -> m.09x3mvh\n# Answer:\nProstate cancer", "# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.05b7d26\n# Answer:\nProstate cancer", "# Reasoning Path:\nHuell Howser -> people.person.profession -> Actor -> people.profession.specializations -> Audio Dramatist\n# Answer:\nActor", "# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.05c6wtp\n# Answer:\nProstate cancer"], "ground_truth": ["Prostate cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-922", "prediction": ["# Reasoning Path:\nSuzanne Collins -> book.author.book_editions_published -> Gregor and the Prophecy of Bane -> book.written_work.original_language -> English Language\n# Answer:\nGregor and the Prophecy of Bane", "# Reasoning Path:\nSuzanne Collins -> film.producer.films_executive_produced -> The Hunger Games -> award.award_nominated_work.award_nominations -> m.0_r6fh7\n# Answer:\nThe Hunger Games", "# Reasoning Path:\nSuzanne Collins -> film.producer.films_executive_produced -> The Hunger Games -> book.book_edition.place_of_publication -> New York City\n# Answer:\nThe Hunger Games", "# Reasoning Path:\nSuzanne Collins -> film.producer.films_executive_produced -> The Hunger Games -> film.film.songs -> m.0j6q1bx\n# Answer:\nThe Hunger Games", "# Reasoning Path:\nSuzanne Collins -> book.author.book_editions_published -> Gregor and the Prophecy of Bane -> common.topic.notable_types -> Book\n# Answer:\nGregor and the Prophecy of Bane", "# Reasoning Path:\nSuzanne Collins -> book.author.book_editions_published -> Gregor The Overlander (Underland Chronicles) -> book.book_edition.cover_price -> m.05qvt6w\n# Answer:\nGregor The Overlander (Underland Chronicles)", "# Reasoning Path:\nSuzanne Collins -> film.producer.films_executive_produced -> The Hunger Games -> award.award_nominated_work.award_nominations -> m.0_r6j06\n# Answer:\nThe Hunger Games", "# Reasoning Path:\nSuzanne Collins -> film.producer.films_executive_produced -> The Hunger Games: Catching Fire -> film.film.production_companies -> Color Force\n# Answer:\nThe Hunger Games: Catching Fire", "# Reasoning Path:\nSuzanne Collins -> book.author.book_editions_published -> Gregor and the Prophecy of Bane -> book.book.genre -> Children's literature\n# Answer:\nGregor and the Prophecy of Bane", "# Reasoning Path:\nSuzanne Collins -> book.author.book_editions_published -> Gregor the Overlander -> book.book.editions -> Gregor The Overlander (Underland Chronicles)\n# Answer:\nGregor the Overlander"], "ground_truth": ["Gregor And The Code Of Claw", "Gregor and the Prophecy of Bane (Underland Chronicles (Paperback))", "Gregor and the Code of Claw (Underland Chronicles, Book 5)", "Gregor and the Prophecy of Bane (Underland Chronicles (Audio))", "Gregor and the Curse of the Warmbloods", "Gregor and the Prophecy of Bane", "Gregor the Overlander (Underland Chronicles)", "Gregor the Overlander (Underland Chronicles (Sagebrush))", "Gregor and the curse of the warmbloods", "Gregor and the Code of Claw", "Mockingjay", "Gregor The Overlander (Underland Chronicles)", "The Hunger Games", "Catching Fire", "The Underland Chronicles Book Three", "Gregor the Overlander", "Gregor and the Code of Claw (Thorndike Press Large Print Literacy Bridge Series)", "Gregor the Overlander (Underland Chronicles (Turtleback))", "FIRE PROOF", "The Hunger Games Trilogy Boxed Set", "When Charlie McButton lost power", "Gregor and the marks of secret", "Gregor and the Prophecy of Bane (Underland Chronicles (Turtleback))", "Fire Proof", "Gregor and the Marks of Secret"], "ans_acc": 0.24, "ans_hit": 1, "ans_f1": 0.3870967741935484, "ans_precission": 1.0, "ans_recall": 0.24, "path_f1": 0.31265508684863524, "path_precision": 0.9, "path_recall": 0.1891891891891892, "path_ans_f1": 0.3870967741935484, "path_ans_precision": 1.0, "path_ans_recall": 0.24}
{"id": "WebQTest-923", "prediction": ["# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Border Collie -> biology.animal_breed.place_of_origin -> Scotland\n# Answer:\nBorder Collie", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Border Collie -> biology.animal_breed.place_of_origin -> Wales\n# Answer:\nBorder Collie", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Border Collie -> base.petbreeds.dog_breed.family_friendly -> Friendly\n# Answer:\nBorder Collie", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Airedale Terrier -> biology.animal_breed.place_of_origin -> United Kingdom\n# Answer:\nAiredale Terrier", "# Reasoning Path:\nEngland -> organization.organization_scope.organizations_with_this_scope -> Employment Appeal Tribunal -> organization.organization.geographic_scope -> Wales\n# Answer:\nEmployment Appeal Tribunal", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Border Collie -> biology.animal_breed.place_of_origin -> Ireland\n# Answer:\nBorder Collie", "# Reasoning Path:\nEngland -> organization.organization_scope.organizations_with_this_scope -> Employment Appeal Tribunal -> organization.organization.geographic_scope -> Scotland\n# Answer:\nEmployment Appeal Tribunal", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0hq0kmy\n# Answer:\nm.0hq0kmy", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Border Collie -> biology.animal_breed.coloring -> Black\n# Answer:\nBorder Collie", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> American Pit Bull Terrier -> biology.animal_breed.place_of_origin -> Ireland\n# Answer:\nAmerican Pit Bull Terrier"], "ground_truth": ["Wales", "Scotland"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.0851063829787234, "path_precision": 0.4, "path_recall": 0.047619047619047616, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-924", "prediction": ["# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010h62gm -> soccer.football_goal.match -> 2014 Barcelona vs Athletic Bilbao\n# Answer:\nm.010h62gm", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010h62gm -> soccer.football_goal.point_awarded_to -> FC Barcelona\n# Answer:\nm.010h62gm", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010gkw62 -> soccer.football_goal.point_awarded_to -> FC Barcelona\n# Answer:\nm.010gkw62", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010r25mp -> soccer.football_goal.point_awarded_to -> Argentina national football team\n# Answer:\nm.010r25mp", "# Reasoning Path:\nLionel Messi -> base.schemastaging.athlete_extra.salary -> m.0qtkxlg -> base.schemastaging.athlete_salary.team -> FC Barcelona\n# Answer:\nm.0qtkxlg", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010gkw62 -> soccer.football_goal.match -> 2014 Barcelona F.C. vs Real Madrid C.F. football match\n# Answer:\nm.010gkw62", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010r25mp -> soccer.football_goal.match -> 2014 Argentina vs Slovenia friendly match\n# Answer:\nm.010r25mp", "# Reasoning Path:\nLionel Messi -> base.schemastaging.athlete_extra.salary -> m.0110zlj9 -> base.schemastaging.athlete_salary.team -> FC Barcelona\n# Answer:\nm.0110zlj9", "# Reasoning Path:\nLionel Messi -> soccer.football_player.matches_played -> m.0c0ltpz -> soccer.football_player_match_participation.match -> 2010 FIFA World Cup Group B - ARG ./. NGA\n# Answer:\nm.0c0ltpz", "# Reasoning Path:\nLionel Messi -> base.schemastaging.athlete_extra.salary -> m.0110zlj9 -> base.schemastaging.athlete_salary.currency -> Euro\n# Answer:\nm.0110zlj9"], "ground_truth": ["Argentina national football team", "FC Barcelona"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3448275862068966, "path_precision": 0.5, "path_recall": 0.2631578947368421, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-925", "prediction": ["# Reasoning Path:\nGreensburg -> location.location.time_zones -> Eastern Time Zone -> common.topic.notable_for -> g.125bn2wcf\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nGreensburg -> location.location.time_zones -> Eastern Time Zone -> common.topic.webpage -> m.03hc_rb\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nGreensburg -> location.statistical_region.population -> g.11b66hhtj5\n# Answer:\ng.11b66hhtj5", "# Reasoning Path:\nGreensburg -> location.location.time_zones -> Eastern Time Zone -> common.topic.article -> m.02hcvg\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nGreensburg -> location.statistical_region.population -> g.11bymk01vg\n# Answer:\ng.11bymk01vg", "# Reasoning Path:\nGreensburg -> location.location.time_zones -> Eastern Time Zone -> common.topic.article -> m.0j44rkj\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nGreensburg -> location.statistical_region.population -> g.11x1cdqvq\n# Answer:\ng.11x1cdqvq", "# Reasoning Path:\nGreensburg -> location.location.time_zones -> Eastern Time Zone -> common.topic.article -> m.0j62c65\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nGreensburg -> common.topic.webpage -> m.04lvlb7 -> common.webpage.category -> Official Website\n# Answer:\nm.04lvlb7", "# Reasoning Path:\nGreensburg -> common.topic.webpage -> m.04lvlb7 -> common.webpage.resource -> m.0bjssfv\n# Answer:\nm.04lvlb7"], "ground_truth": ["Eastern Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-927", "prediction": ["# Reasoning Path:\nLord Voldemort -> film.film_character.portrayed_in_films -> m.0jz6fl -> film.performance.actor -> Ralph Fiennes\n# Answer:\nm.0jz6fl", "# Reasoning Path:\nLord Voldemort -> film.film_character.portrayed_in_films -> m.09lybcq -> film.performance.actor -> Ralph Fiennes\n# Answer:\nm.09lybcq", "# Reasoning Path:\nLord Voldemort -> film.film_character.portrayed_in_films -> m.0jz6fl -> film.performance.film -> Harry Potter and the Goblet of Fire\n# Answer:\nm.0jz6fl", "# Reasoning Path:\nLord Voldemort -> film.film_character.portrayed_in_films -> m.02t9_zv -> film.performance.actor -> Richard Bremmer\n# Answer:\nm.02t9_zv", "# Reasoning Path:\nLord Voldemort -> film.film_character.portrayed_in_films -> m.09lybcq -> film.performance.film -> Harry Potter and the Deathly Hallows \u2013 Part 2\n# Answer:\nm.09lybcq", "# Reasoning Path:\nLord Voldemort -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nLord Voldemort -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Hogwarts School of Witchcraft and Wizardry -> fictional_universe.fictional_employer.employees -> m.02vhkr0\n# Answer:\nHogwarts School of Witchcraft and Wizardry", "# Reasoning Path:\nLord Voldemort -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nLord Voldemort -> fictional_universe.fictional_character.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nMale", "# Reasoning Path:\nLord Voldemort -> film.film_character.portrayed_in_films -> m.02t9_zv -> film.performance.film -> Harry Potter and the Philosopher's Stone\n# Answer:\nm.02t9_zv"], "ground_truth": ["Frank Dillane", "Ralph Fiennes", "Christian Coulson", "Richard Bremmer", "Hero Fiennes-Tiffin"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2727272727272727, "path_precision": 0.3, "path_recall": 0.25, "path_ans_f1": 0.34285714285714286, "path_ans_precision": 0.3, "path_ans_recall": 0.4}
{"id": "WebQTest-928", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> University of Alabama School of Law\n# Answer:\nm.0n1l46h", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nm.0lwxmy1", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy9 -> education.education.institution -> University of Oxford\n# Answer:\nm.0lwxmy9", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.place_of_birth -> Los Angeles\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.gender -> Female\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_types -> Book Edition\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_for -> g.125920htw\n# Answer:\nHarper Lee's To Kill a Mockingbird"], "ground_truth": ["Huntingdon College", "University of Alabama", "University of Alabama School of Law", "University of Oxford"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-929", "prediction": ["# Reasoning Path:\nKu Klux Klan -> organization.organization.founders -> Frank O. McCord -> common.topic.notable_types -> Organization founder\n# Answer:\nFrank O. McCord", "# Reasoning Path:\nKu Klux Klan -> organization.organization.founders -> J. Calvin Jones -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nJ. Calvin Jones", "# Reasoning Path:\nKu Klux Klan -> organization.organization.founders -> Frank O. McCord -> people.person.gender -> Male\n# Answer:\nFrank O. McCord", "# Reasoning Path:\nKu Klux Klan -> organization.organization.founders -> Frank O. McCord -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nFrank O. McCord", "# Reasoning Path:\nKu Klux Klan -> organization.organization.founders -> J. Calvin Jones -> people.person.profession -> Soldier\n# Answer:\nJ. Calvin Jones", "# Reasoning Path:\nKu Klux Klan -> organization.organization.organization_type -> Secret society -> organization.organization_type.organizations_of_this_type -> ANAK Society\n# Answer:\nSecret society", "# Reasoning Path:\nKu Klux Klan -> organization.organization.organization_type -> Secret society -> common.topic.notable_for -> g.12556r00x\n# Answer:\nSecret society", "# Reasoning Path:\nKu Klux Klan -> organization.organization.founders -> J. Calvin Jones -> common.topic.notable_types -> Organization founder\n# Answer:\nJ. Calvin Jones", "# Reasoning Path:\nKu Klux Klan -> organization.organization.founders -> James R. Crowe -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nJames R. Crowe", "# Reasoning Path:\nKu Klux Klan -> common.topic.image -> Kkk-carpetbagger-cartoon -> common.image.size -> m.04rcdm5\n# Answer:\nKkk-carpetbagger-cartoon"], "ground_truth": ["John B. Kennedy", "J. Calvin Jones", "Richard R. Reed", "Frank O. McCord", "James R. Crowe", "John C. Lester"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5833333333333334, "ans_precission": 0.7, "ans_recall": 0.5, "path_f1": 0.5833333333333334, "path_precision": 0.7, "path_recall": 0.5, "path_ans_f1": 0.5833333333333334, "path_ans_precision": 0.7, "path_ans_recall": 0.5}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.partially_contains -> Didi Abuli\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Denmark\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> base.locations.continents.countries_within -> Denmark\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> location.location.partially_contains -> Didi Abuli\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.containedby -> Eurasia -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurasia", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Georgia\n# Answer:\nEurope"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-931", "prediction": ["# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.09ryhrl -> government.government_position_held.office_position_or_title -> Delegate to the Continental Congress\n# Answer:\nm.09ryhrl", "# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.04466xb -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nm.04466xb", "# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.04mm9px -> government.government_position_held.office_position_or_title -> Vice President of the United States\n# Answer:\nm.04mm9px", "# Reasoning Path:\nJohn Adams -> people.person.profession -> Lawyer -> common.topic.subjects -> Criminal defense lawyer\n# Answer:\nLawyer", "# Reasoning Path:\nJohn Adams -> government.political_appointer.appointees -> m.04kq1pf -> government.government_position_held.office_position_or_title -> United States Secretary of the Treasury\n# Answer:\nm.04kq1pf", "# Reasoning Path:\nJohn Adams -> people.person.profession -> Lawyer -> business.industry.companies -> Legal Action Workshop\n# Answer:\nLawyer", "# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.09ryhrl -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nm.09ryhrl", "# Reasoning Path:\nJohn Adams -> people.person.profession -> Lawyer -> common.topic.subjects -> Sally & Fitch LLP\n# Answer:\nLawyer", "# Reasoning Path:\nJohn Adams -> people.person.profession -> Lawyer -> common.topic.subject_of -> Legal Action Workshop\n# Answer:\nLawyer", "# Reasoning Path:\nJohn Adams -> people.person.profession -> Politician -> freebase.type_profile.kind -> Title\n# Answer:\nPolitician"], "ground_truth": ["Delegate to the Continental Congress", "United States Ambassador to the United Kingdom", "United States Ambassador to the Netherlands", "Vice President of the United States"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.23529411764705882, "path_precision": 0.2, "path_recall": 0.2857142857142857, "path_ans_f1": 0.28571428571428575, "path_ans_precision": 0.2, "path_ans_recall": 0.5}
{"id": "WebQTest-932", "prediction": ["# Reasoning Path:\nSuper Mario Bros. 2 -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Super Mario Universe -> fictional_universe.fictional_universe.created_by -> Shigeru Miyamoto\n# Answer:\nSuper Mario Universe", "# Reasoning Path:\nSuper Mario Bros. 2 -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Super Mario Universe -> fictional_universe.fictional_universe.works_set_here -> Mario Bros.\n# Answer:\nSuper Mario Universe", "# Reasoning Path:\nSuper Mario Bros. 2 -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Super Mario Universe -> fictional_universe.fictional_universe.created_by -> Nintendo\n# Answer:\nSuper Mario Universe", "# Reasoning Path:\nSuper Mario Bros. 2 -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Super Mario Universe -> fictional_universe.fictional_universe.works_set_here -> Dance Dance Revolution Mario Mix\n# Answer:\nSuper Mario Universe", "# Reasoning Path:\nSuper Mario Bros. 2 -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Super Mario Universe -> fictional_universe.fictional_universe.works_set_here -> Donkey Kong\n# Answer:\nSuper Mario Universe", "# Reasoning Path:\nSuper Mario Bros. 2 -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Super Mario Universe -> fictional_universe.fictional_universe.species -> Bob-omb\n# Answer:\nSuper Mario Universe", "# Reasoning Path:\nSuper Mario Bros. 2 -> base.wikipedia_infobox.video_game.platforms -> Family Computer Disk System -> common.topic.notable_types -> Video Game Platform\n# Answer:\nFamily Computer Disk System", "# Reasoning Path:\nSuper Mario Bros. 2 -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Super Mario Universe -> fictional_universe.fictional_universe.species -> Boos\n# Answer:\nSuper Mario Universe", "# Reasoning Path:\nSuper Mario Bros. 2 -> book.book_subject.works -> For Super Players -> book.published_work.published_in_published_as -> m.065tqzx\n# Answer:\nFor Super Players", "# Reasoning Path:\nSuper Mario Bros. 2 -> base.wikipedia_infobox.video_game.platforms -> Family Computer Disk System -> common.topic.notable_for -> g.125c8cm0_\n# Answer:\nFamily Computer Disk System"], "ground_truth": ["1988-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-933", "prediction": ["# Reasoning Path:\nAndrew Johnson -> government.us_vice_president.to_president -> Abraham Lincoln -> organization.organization_founder.organizations_founded -> Republican Party\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAndrew Johnson -> government.us_vice_president.to_president -> Abraham Lincoln -> organization.organization_founder.organizations_founded -> Freedmen's Bureau\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAndrew Johnson -> government.us_vice_president.to_president -> Abraham Lincoln -> government.politician.party -> m.03gjfyh\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAndrew Johnson -> government.politician.party -> m.03gjg0m -> government.political_party_tenure.party -> Democratic Party\n# Answer:\nm.03gjg0m", "# Reasoning Path:\nAndrew Johnson -> government.us_vice_president.to_president -> Abraham Lincoln -> organization.organization_founder.organizations_founded -> Illinois Republican Party\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAndrew Johnson -> government.us_vice_president.to_president -> Abraham Lincoln -> government.politician.party -> m.03gjfyn\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAndrew Johnson -> government.us_vice_president.to_president -> Abraham Lincoln -> government.politician.party -> m.03ld2ph\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAndrew Johnson -> government.us_vice_president.to_president -> Abraham Lincoln -> people.person.places_lived -> m.03pgr_5\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAndrew Johnson -> government.politician.party -> m.0lr1qnz -> government.political_party_tenure.party -> Republican Party\n# Answer:\nm.0lr1qnz", "# Reasoning Path:\nAndrew Johnson -> government.politician.party -> m.03gjg0g -> government.political_party_tenure.party -> National Union Party\n# Answer:\nm.03gjg0g"], "ground_truth": ["National Union Party", "Democratic Party", "Republican Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-935", "prediction": ["# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Ali Azmat\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> Osama: The Making of a Terrorist -> book.written_work.subjects -> Al-Qaeda\n# Answer:\nOsama: The Making of a Terrorist", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.events -> Death of Osama bin Laden\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Amrit Singh Thapar\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.contains -> Abbottabad Hockey Stadium\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Faiza Khan\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> What Death of Osama Bin Laden Indicates About Barack Obama's Leadership -> book.written_work.subjects -> Death\n# Answer:\nWhat Death of Osama Bin Laden Indicates About Barack Obama's Leadership", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.contains -> Abbottabad International Medical College\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> Osama: The Making of a Terrorist -> common.topic.notable_types -> Book\n# Answer:\nOsama: The Making of a Terrorist", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.contains -> Abbottabad Public School\n# Answer:\nAbbottabad"], "ground_truth": ["Abbottabad"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-936", "prediction": ["# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Temple of Aphaea -> location.location.containedby -> Aegina\n# Answer:\nTemple of Aphaea", "# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Temple of Aphaea -> architecture.structure.architectural_style -> Classical architecture\n# Answer:\nTemple of Aphaea", "# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Temple of Zeus, Olympia -> location.location.containedby -> Ancient Greece\n# Answer:\nTemple of Zeus, Olympia", "# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Temple of Aphaea -> location.location.geolocation -> m.0cqr4qf\n# Answer:\nTemple of Aphaea", "# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Erechtheion -> location.location.containedby -> Athens\n# Answer:\nErechtheion", "# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Temple of Zeus, Olympia -> common.topic.notable_for -> g.125529h7s\n# Answer:\nTemple of Zeus, Olympia", "# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Temple of Zeus, Olympia -> symbols.namesake.named_after -> Zeus\n# Answer:\nTemple of Zeus, Olympia", "# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Erechtheion -> location.location.containedby -> Acropolis of Athens\n# Answer:\nErechtheion", "# Reasoning Path:\nAncient Greek temple -> common.topic.image -> Athens Acropolis -> common.image.appears_in_topic_gallery -> Athens\n# Answer:\nAthens Acropolis", "# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Erechtheion -> travel.tourist_attraction.near_travel_destination -> Athens\n# Answer:\nErechtheion"], "ground_truth": ["Ku\u015fadas\u0131", "Shahhat", "Athens", "Ephesus", "Olympia", "Corfu"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.4, "ans_recall": 0.3333333333333333, "path_f1": 0.1904761904761905, "path_precision": 0.2, "path_recall": 0.18181818181818182, "path_ans_f1": 0.45161290322580644, "path_ans_precision": 0.7, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-937", "prediction": ["# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Algeria -> location.location.events -> Insurgency in the Maghreb\n# Answer:\nAlgeria", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Algeria -> book.book_subject.works -> A Tragedy of Arms\n# Answer:\nAlgeria", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Algeria -> location.location.events -> Middle East Theatre of World War II\n# Answer:\nAlgeria", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Algeria -> location.location.partially_contains -> Sahel\n# Answer:\nAlgeria", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.location.containedby -> Near East\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.location.geolocation -> m.0zwv97z\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Algeria -> location.location.events -> North African Campaign\n# Answer:\nAlgeria", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> South Africa -> location.country.languages_spoken -> English Language\n# Answer:\nSouth Africa", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> South Africa -> common.topic.notable_types -> Country\n# Answer:\nSouth Africa", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Algeria -> location.location.partially_contains -> Atlas Mountains\n# Answer:\nAlgeria"], "ground_truth": ["Tunisia", "Saudi Arabia", "Qatar", "Syria", "Algeria", "Turkey", "Canada", "Iran", "South Yemen", "Jordan", "Iraq", "Morocco", "Tanzania", "Yemen", "United Arab Emirates", "Oman", "Eritrea", "Libya", "Djibouti", "Kuwait", "Sudan", "Lebanon", "Mandatory Palestine", "Egypt", "Comoros", "Israel", "Mauritania", "Seljuk Empire", "South Africa", "Bahrain"], "ans_acc": 0.1, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 1.0, "ans_recall": 0.1, "path_f1": 0.18181818181818182, "path_precision": 1.0, "path_recall": 0.1, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 1.0, "path_ans_recall": 0.1}
{"id": "WebQTest-938", "prediction": ["# Reasoning Path:\nElizabeth II -> people.person.parents -> George VI -> people.person.parents -> George V\n# Answer:\nGeorge VI", "# Reasoning Path:\nElizabeth II -> people.person.parents -> George VI -> people.person.parents -> Mary of Teck\n# Answer:\nGeorge VI", "# Reasoning Path:\nElizabeth II -> people.person.parents -> Queen Elizabeth The Queen Mother -> people.person.parents -> Cecilia Bowes-Lyon, Countess of Strathmore and Kinghorne\n# Answer:\nQueen Elizabeth The Queen Mother", "# Reasoning Path:\nElizabeth II -> people.person.parents -> George VI -> royalty.noble_person.titles -> m.04_b_2d\n# Answer:\nGeorge VI", "# Reasoning Path:\nElizabeth II -> people.person.parents -> Queen Elizabeth The Queen Mother -> people.person.parents -> Claude Bowes-Lyon, 14th Earl of Strathmore and Kinghorne\n# Answer:\nQueen Elizabeth The Queen Mother", "# Reasoning Path:\nElizabeth II -> people.person.parents -> George VI -> military.military_person.service -> m.04hxg98\n# Answer:\nGeorge VI", "# Reasoning Path:\nElizabeth II -> people.person.parents -> Queen Elizabeth The Queen Mother -> people.family_member.family -> House of Windsor\n# Answer:\nQueen Elizabeth The Queen Mother", "# Reasoning Path:\nElizabeth II -> royalty.chivalric_order_officer.memberships_presented -> m.010hv442 -> royalty.chivalric_order_membership.title -> Officer of the Order of the British Empire\n# Answer:\nm.010hv442", "# Reasoning Path:\nElizabeth II -> people.person.parents -> George VI -> royalty.noble_person.titles -> m.05yx_c0\n# Answer:\nGeorge VI", "# Reasoning Path:\nElizabeth II -> people.person.parents -> Queen Elizabeth The Queen Mother -> people.person.sibling_s -> m.0b5lmgx\n# Answer:\nQueen Elizabeth The Queen Mother"], "ground_truth": ["Queen Elizabeth The Queen Mother"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-939", "prediction": ["# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Cubism -> visual_art.art_period_movement.associated_artworks -> Chicago Picasso\n# Answer:\nCubism", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Cubism -> common.topic.image -> Braque\n# Answer:\nCubism", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Analytic cubism -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nAnalytic cubism", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Cubism -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Picasso and the Avant-Garde in Paris\n# Answer:\nCubism", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Cubism -> visual_art.art_period_movement.associated_artworks -> Las Meninas (after Vel\u00e1zquez)\n# Answer:\nCubism", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Analytic cubism -> visual_art.art_period_movement.associated_artworks -> Woman with a Guitar\n# Answer:\nAnalytic cubism", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Cubism -> visual_art.art_period_movement.associated_artworks -> Nude Descending a Staircase, No. 2\n# Answer:\nCubism", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Picasso's Blue Period -> common.topic.image -> Picasso la vie\n# Answer:\nPicasso's Blue Period", "# Reasoning Path:\nPablo Picasso -> book.author.works_written -> Picasso -> award.award_nominated_work.award_nominations -> m.0zctbnr\n# Answer:\nPicasso", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Analytic cubism -> visual_art.art_period_movement.associated_artists -> Georges Braque\n# Answer:\nAnalytic cubism"], "ground_truth": ["Picasso's Blue Period", "Picasso's Rose Period", "Surrealism", "Cubism", "Picasso's African Period", "Modern art", "Synthetic cubism", "Analytic cubism"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.5294117647058825, "ans_precission": 0.9, "ans_recall": 0.375, "path_f1": 0.5294117647058825, "path_precision": 0.9, "path_recall": 0.375, "path_ans_f1": 0.5294117647058825, "path_ans_precision": 0.9, "path_ans_recall": 0.375}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w0hpl\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w4lwz\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> ACE inhibitor\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> heart attack\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09wjtbj\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Diabetes mellitus\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> Acebutolol\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> Anisindione\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Family History of Ischaemic Heart disease\n# Answer:\nheart attack"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-941", "prediction": ["# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> Temperance Fitzgerald -> freebase.valuenotation.has_value -> Parents\n# Answer:\nTemperance Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> Temperance Fitzgerald -> people.person.spouse_s -> m.0ncm17l\n# Answer:\nTemperance Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> William Fitzgerald -> freebase.valuenotation.has_value -> Parents\n# Answer:\nWilliam Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> Temperance Fitzgerald -> people.deceased_person.cause_of_death -> heart attack\n# Answer:\nTemperance Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> Temperance Fitzgerald -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nTemperance Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> William Fitzgerald -> common.topic.notable_types -> Deceased Person\n# Answer:\nWilliam Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> Temperance Fitzgerald -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nTemperance Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> William Fitzgerald -> people.person.gender -> Male\n# Answer:\nWilliam Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> William Fitzgerald -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nWilliam Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> William Fitzgerald -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nWilliam Fitzgerald"], "ground_truth": ["William Fitzgerald", "Temperance Fitzgerald"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-942", "prediction": ["# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Gerald Hathaway -> freebase.valuenotation.has_value -> Parents\n# Answer:\nGerald Hathaway", "# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Kate McCauley Hathaway -> people.person.parents -> Joe McCauley\n# Answer:\nKate McCauley Hathaway", "# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Gerald Hathaway -> people.person.children -> Thomas Hathaway\n# Answer:\nGerald Hathaway", "# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Gerald Hathaway -> people.person.spouse_s -> m.0j4k1mh\n# Answer:\nGerald Hathaway", "# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Gerald Hathaway -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nGerald Hathaway", "# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Gerald Hathaway -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nGerald Hathaway", "# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Kate McCauley Hathaway -> common.topic.notable_types -> Film actor\n# Answer:\nKate McCauley Hathaway", "# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Gerald Hathaway -> people.person.children -> Michael Hathaway\n# Answer:\nGerald Hathaway", "# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Kate McCauley Hathaway -> people.person.children -> Thomas Hathaway\n# Answer:\nKate McCauley Hathaway", "# Reasoning Path:\nAnne Hathaway -> base.popstra.celebrity.canoodled -> m.0645mrc -> base.popstra.canoodled.participant -> Adam Shulman\n# Answer:\nm.0645mrc"], "ground_truth": ["Kate McCauley Hathaway", "Gerald Hathaway"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-943", "prediction": ["# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.character_created_by -> Matthew Weiner -> fictional_universe.fictional_character_creator.fictional_characters_created -> Peggy Olson\n# Answer:\nMatthew Weiner", "# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.character_created_by -> Matthew Weiner -> fictional_universe.fictional_character_creator.fictional_characters_created -> Betty Draper\n# Answer:\nMatthew Weiner", "# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.character_created_by -> Matthew Weiner -> film.writer.film -> Are You Here\n# Answer:\nMatthew Weiner", "# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.character_created_by -> Matthew Weiner -> fictional_universe.fictional_character_creator.fictional_characters_created -> Freddy Rumsen\n# Answer:\nMatthew Weiner", "# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.character_created_by -> Matthew Weiner -> award.award_nominee.award_nominations -> m.010bdsp2\n# Answer:\nMatthew Weiner", "# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.children -> Bobby Draper -> common.topic.notable_types -> TV Character\n# Answer:\nBobby Draper", "# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.children -> Bobby Draper -> tv.tv_character.appeared_in_tv_episodes -> m.0jx71n2\n# Answer:\nBobby Draper", "# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.character_created_by -> Matthew Weiner -> award.award_nominee.award_nominations -> m.011409jp\n# Answer:\nMatthew Weiner", "# Reasoning Path:\nDon Draper -> common.topic.webpage -> m.09y333s -> common.webpage.resource -> Watch the 'Mad Men' pilot episode... for free!\n# Answer:\nm.09y333s", "# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.children -> Bobby Draper -> tv.tv_character.appeared_in_tv_episodes -> m.0ztq90m\n# Answer:\nBobby Draper"], "ground_truth": ["Betty Draper"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-944", "prediction": ["# Reasoning Path:\nVan Halen -> music.artist.album -> Ain't Talkin' 'Bout Love -> common.topic.notable_for -> g.12h2y4yvf\n# Answer:\nAin't Talkin' 'Bout Love", "# Reasoning Path:\nVan Halen -> music.artist.album -> Ain't Talkin' 'Bout Love -> common.topic.notable_types -> Musical Album\n# Answer:\nAin't Talkin' 'Bout Love", "# Reasoning Path:\nVan Halen -> music.artist.album -> 1984 -> music.album.releases -> g.11btqyy1wl\n# Answer:\n1984", "# Reasoning Path:\nVan Halen -> music.artist.album -> (Oh) Pretty Woman -> music.album.compositions -> Oh, Pretty Woman\n# Answer:\n(Oh) Pretty Woman", "# Reasoning Path:\nVan Halen -> common.topic.webpage -> m.03l1y8s -> common.webpage.resource -> official website\n# Answer:\nm.03l1y8s", "# Reasoning Path:\nVan Halen -> music.artist.album -> (Oh) Pretty Woman -> music.album.release_type -> Single\n# Answer:\n(Oh) Pretty Woman", "# Reasoning Path:\nVan Halen -> music.artist.album -> 1984 -> music.album.release_type -> Album\n# Answer:\n1984", "# Reasoning Path:\nVan Halen -> music.musical_group.member -> m.010kz9hr -> music.group_membership.member -> Wolfgang Van Halen\n# Answer:\nm.010kz9hr", "# Reasoning Path:\nVan Halen -> common.topic.webpage -> m.03l1y8s -> common.webpage.category -> Official Website\n# Answer:\nm.03l1y8s", "# Reasoning Path:\nVan Halen -> common.topic.webpage -> m.09wjqhm -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09wjqhm"], "ground_truth": ["Ain't Talkin' 'Bout Love"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-945", "prediction": ["# Reasoning Path:\nSpain -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Andorra\n# Answer:\nParliamentary system", "# Reasoning Path:\nSpain -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nSpain -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Andorra\n# Answer:\nUnitary state", "# Reasoning Path:\nSpain -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Antigua and Barbuda\n# Answer:\nParliamentary system", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\ng.1hhc37x4q", "# Reasoning Path:\nSpain -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Belgium\n# Answer:\nParliamentary system", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> France\n# Answer:\nEarth", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc38hhk\n# Answer:\ng.1hhc38hhk", "# Reasoning Path:\nSpain -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> United States of America\n# Answer:\nEarth"], "ground_truth": ["Monarchy", "Constitutional monarchy", "Parliamentary system", "Unitary state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.6666666666666665, "path_precision": 0.6, "path_recall": 0.75, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-948", "prediction": ["# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Savannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal -> book.written_work.subjects -> Law\n# Answer:\nSavannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Savannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal -> book.written_work.subjects -> Prison\n# Answer:\nSavannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Savannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal -> book.written_work.subjects -> Savannah\n# Answer:\nSavannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Savannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal -> book.written_work.previous_in_series -> Savannah Talks Troy Anthony Davis No. 11: Judge Moore says \\\"Not innocent\\\"\n# Answer:\nSavannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Making Our Democracy Work -> book.written_work.subjects -> United States Constitution\n# Answer:\nMaking Our Democracy Work", "# Reasoning Path:\nSupreme Court of the United States -> common.topic.notable_for -> g.125dtprfx\n# Answer:\ng.125dtprfx", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Savannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal -> book.written_work.school_or_movement -> African American\n# Answer:\nSavannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Courtiers of the Marble Palace: The Rise and Influence of the Supreme Court Law Clerk -> book.written_work.subjects -> Law clerk\n# Answer:\nCourtiers of the Marble Palace: The Rise and Influence of the Supreme Court Law Clerk", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Making Our Democracy Work -> book.written_work.original_language -> English Language\n# Answer:\nMaking Our Democracy Work", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Making Our Democracy Work -> common.topic.notable_for -> g.125728tkq\n# Answer:\nMaking Our Democracy Work"], "ground_truth": ["Oncale v. Sundowner Offshore Services, Inc.", "Reed v. Reed", "Bradwell v. Illinois", "Planned Parenthood of Central Missouri v. Danforth", "Stenberg v. Carhart", "Luther v. Borden", "Island Trees School District v. Pico", "Hodgson v. Minnesota", "Tinker v. Des Moines Independent Community School District", "United Building & Construction Trades Council v. Mayor and Council of Camden", "Martin v. Wilks", "Federal Baseball Club v. National League", "Black and White Taxicab and Transfer Co. v. Brown and Yellow Taxicab and Transfer Co.", "Strawbridge v. Curtiss", "Markman v. Westview Instruments, Inc.", "Parratt v. Taylor", "Hamdan v. Rumsfeld", "Zelman v. Simmons-Harris", "Red Lion Broadcasting Co. v. FCC", "Wheaton v. Peters", "Brandenburg v. Ohio", "The Paquete Habana", "Ingraham v. Wright", "Worcester v. Georgia", "United States v. E. C. Knight Co.", "Rose v. Locke", "Oyama v. California", "Walton v. Arizona", "Torcaso v. Watkins", "United States v. The Amistad", "Poe v. Ullman", "Saenz v. Roe", "Katzenbach v. Morgan", "Planned Parenthood v. Casey", "Roe v. Wade", "Passenger Cases", "United States v. Curtiss-Wright Export Corp.", "United States v. Williams", "Gilbert v. California", "Florida v. Bostick", "Wheeling Steel Corp. v. Glander", "Fletcher v. Peck", "Grove City College v. Bell", "Runyon v. McCrary", "Hartford Fire Insurance Co. v. California", "eBay Inc. v. MercExchange, L.L.C.", "United States v. Kirby", "Kelo v. City of New London", "Parisi v. Davidson", "Owen Equipment & Erection Co. v. Kroger", "Radovich v. National Football League", "United States v. Miller", "Utah v. Evans", "New York v. Connecticut", "Panama Refining Co. v. Ryan", "Cort v. Ash", "Near v. Minnesota", "Feist Publications, Inc., v. Rural Telephone Service Co.", "Northwestern National Life Insurance Co. v. Riggs", "Alden v. Maine", "Chisholm v. Georgia", "Commissioner v. Glenshaw Glass Co.", "City of Cleburne v. Cleburne Living Center, Inc.", "Massiah v. United States", "Pruneyard Shopping Center v. Robins", "Hernandez v. Texas", "Central Laborers' Pension Fund v. Heinz", "Hudson v. McMillian", "Eastern Associated Coal Corp. v. United Mine Workers of America", "Schriro v. Summerlin", "Hazelwood v. Kuhlmeier", "Daubert v. Merrell Dow Pharmaceuticals, Inc.", "Eli Lilly & Co. v. Medtronic, Inc.", "Easley v. Cromartie", "Immigration and Naturalization Service v. Stevic", "United States v. Butler", "Buchanan v. Warley", "Motor Vehicles Manufacturers Ass'n v. State Farm Mutual Automobile Insurance Co.", "Late Corp. of the Church of Jesus Christ of Latter-Day Saints v. United States", "United States v. Salerno", "Chambers v. Florida", "Cantwell v. State of Connecticut", "Meyer v. Nebraska", "Gade v. National Solid Wastes Management Ass'n", "South Dakota v. Opperman", "Champion v. Ames", "Downes v. Bidwell", "Johnson v. Eisentrager", "Kennedy v. Louisiana", "Tee-Hit-Ton Indians v. United States", "Cramer v. United States", "Korematsu v. United States", "Merrell Dow Pharmaceuticals Inc. v. Thompson", "Street v. New York", "Missouri ex rel. Gaines v. Canada", "New York Times Co. v. United States", "Garcia v. San Antonio Metropolitan Transit Authority", "Beacon Theatres, Inc. v. Westover", "Wesberry v. Sanders", "Savana Redding v. Safford Unified School District #1", "Poulos v. New Hampshire", "Scheidler v. National Organization for Women", "International News Service v. Associated Press", "United States v. Stanley", "Rumsfeld v. Padilla", "Gonzales v. Oregon", "Hickman v. Taylor", "United States v. Shabani", "Heart of Atlanta Motel, Inc. v. United States", "Ashcroft v. Free Speech Coalition", "South Carolina v. Katzenbach", "Gonzales v. O Centro Espirita Beneficente Uniao do Vegetal", "Pfaff v. Wells Electronics, Inc.", "United States v. Raines", "Trop v. Dulles", "Williamson v. Lee Optical Co.", "Tenet v. Doe", "United States v. International Boxing Club of New York", "Skinner v. Railway Labor Executives Ass'n", "National Socialist Party of America v. Village of Skokie", "Ayotte v. Planned Parenthood of Northern New England", "McCleskey v. Kemp", "Exxon Mobil Corp. v. Saudi Basic Industries Corp.", "Mapp v. Ohio", "Herring v. United States", "Universal Camera Corp. v. NLRB", "Dred Scott v. Sandford", "Regents of the University of California v. Bakke", "Baird v. State Bar of Arizona", "Beauharnais v. Illinois", "United States v. Fordice", "Helicopteros Nacionales de Colombia, S. A. v. Hall", "Will v. Michigan Department of State Police", "Jones v. United States", "Kidd v. Pearson", "Stromberg v. California", "Braunfeld v. Brown", "Philip Morris USA Inc. v. Williams", "Hirabayashi v. United States", "Maine v. Taylor", "Ex parte Young", "Burger King Corp. v. Rudzewicz", "Roberts v. United States Jaycees", "Texaco Inc. v. Dagher", "Bronston v. United States", "Feiner v. New York", "United States v. Cruikshank", "Pryor v. United States", "Holden v. Hardy", "Frontiero v. Richardson", "Reynolds v. Sims", "Hayburn's Case", "Goldberg v. Kelly", "Huddleston v. United States", "Civil Rights Cases", "Illinois v. Rodriguez", "Perry v. Sindermann", "Sibbach v. Wilson & Co.", "Witherspoon v. Illinois", "Cutter v. Wilkinson", "Bobbs-Merrill Co. v. Straus", "Curtis Publishing Co. v. Butts", "Hiibel v. Sixth Judicial District Court of Nevada", "Blanton v. City of North Las Vegas", "Old Chief v. United States", "Bailey v. Alabama", "Lehnert v. Ferris Faculty Ass'n", "Marbury v. Madison", "Reynolds v. United States", "Shelley v. Kraemer", "Ledbetter v. Goodyear Tire & Rubber Co.", "Riley v. California", "Feres v. United States", "Prize Cases", "U.S. Term Limits, Inc. v. Thornton", "International Salt Co. v. United States", "Buck v. Bell", "Crosby v. National Foreign Trade Council", "McDonnell Douglas Corp. v. Green", "Prigg v. Pennsylvania", "Jacobson v. United States", "Zorach v. Clauson", "Hylton v. United States", "Frank Lyon Co. v. United States", "San Antonio Independent School District v. Rodriguez", "Watson v. Jones", "Batson v. Kentucky", "Kumho Tire Co. v. Carmichael", "Santa Clara County v. Southern Pacific Railroad Co.", "Boynton v. Virginia", "Board of Trade of City of Chicago v. Olsen", "Continental Paper Bag Co. v. Eastern Paper Bag Co.", "Rostker v. Goldberg", "Cunningham v. California", "Arizona v. Evans", "Burdick v. United States", "Dowling v. United States", "Berea College v. Kentucky", "Kimbrough v. United States", "United States v. Ross", "California Democratic Party v. Jones", "United States v. Bajakajian", "Board of Trustees of the University of Alabama v. Garrett", "Craig v. Boren", "Immigration and Naturalization Service v. Aguirre-Aguirre", "Lechmere, Inc. v. NLRB", "Engel v. Vitale", "Colorado v. Connelly", "Debs v. United States", "Avegno v. Schmidt", "Pacific States Box & Basket Co. v. White", "Leser v. Garnett", "Bowers v. Hardwick", "Stanford v. Texas", "United States v. Eichman", "Taylor v. Taintor", "American Well Works Co. v. Layne & Bowler Co.", "Dastar Corp. v. Twentieth Century Fox Film Corp.", "Board of Education of Kiryas Joel Village School District v. Grumet", "FEC v. Akins", "National Cable & Telecommunications Ass'n v. Brand X Internet Services", "Alaska v. Native Village of Venetie Tribal Government", "United States v. Shoshone Tribe of Indians", "Northern Insurance Co. of New York v. Chatham County", "Missouri v. Jenkins", "Gregg v. Georgia", "Shaffer v. Heitner", "Doe v. Chao", "Leegin Creative Leather Products, Inc. v. PSKS, Inc.", "Arizona v. Hicks", "City of Mobile v. Bolden", "Bowen v. Roy", "California v. Byers", "Tellabs, Inc. v. Makor Issues & Rights, Ltd.", "Law v. Siegel", "Edwards v. California", "Schneider v. State (of New Jersey)", "Wolf v. Colorado", "Turner v. Safley", "Mitchell v. Forsyth", "Epperson v. Arkansas", "C&A Carbone, Inc. v. Town of Clarkstown", "Sparf v. United States", "United States v. Continental Can Co.", "Duncan v. Louisiana", "Hanna v. Plumer", "Ball v. United States", "City of Indianapolis v. Edmond", "Miller v. Johnson", "S. D. Warren Co. v. Maine Board of Environmental Protection", "Illinois Tool Works Inc. v. Independent Ink, Inc.", "Jaffee v. Redmond", "District of Columbia Court of Appeals v. Feldman", "Carey v. Musladin", "Crawford v. Marion County Election Board", "Dickerson v. United States", "Gonzales v. Carhart", "Dillon v. Gloss", "Goldwater v. Carter", "Hurley v. Irish-American Gay, Lesbian, & Bisexual Group of Boston", "Texas v. White", "Martin v. Hunter's Lessee", "Nebraska Press Ass'n v. Stuart", "McCollum v. Board of Education", "United States v. Booker", "Marshall v. Marshall", "Atwater v. City of Lago Vista", "Lisenba v. California", "Dent v. West Virginia", "Campbell v. Acuff-Rose Music, Inc.", "Eisner v. Macomber", "Collins v. Yosemite Park & Curry Co.", "McGowan v. Maryland", "Dennis v. United States", "Bolling v. Sharpe", "Nixon v. Condon", "Everson v. Board of Education", "Board of Airport Commissioners of Los Angeles v. Jews for Jesus, Inc.", "Schuette v. Coalition to Defend Affirmative Action", "Ferguson v. City of Charleston", "Ewing v. California", "Wisconsin v. Mitchell", "Crawford v. Washington", "Adamson v. California", "Bivens v. Six Unknown Named Agents", "Warner-Jenkinson Co. v. Hilton Davis Chemical Co.", "Stewart v. Abend", "Powell v. Alabama", "Bellotti v. Baird", "Leocal v. Ashcroft", "Chaplinsky v. New Hampshire", "Cohen v. Cowles Media Co.", "Illinois v. Lidster", "Sale v. Haitian Centers Council, Inc.", "Forsyth County v. Nationalist Movement", "Van Orden v. Perry", "Webster v. Reproductive Health Services", "Barnes v. Glen Theatre, Inc.", "Burford v. Sun Oil Co.", "Wickard v. Filburn", "Swift v. Tyson", "Illinois Central Railroad Co. v. Illinois", "Consolidated Edison Co. v. Public Service Commission", "Illinois v. Gates", "PGA Tour, Inc. v. Martin", "Oregon v. Mitchell", "Calder v. Bull", "United States v. Montoya De Hernandez", "Strader v. Graham", "Arthur Andersen LLP v. United States", "Diamond v. Diehr", "Elk Grove Unified School District v. Newdow", "Farrington v. Tokushige", "Connecticut General Life Insurance Co. v. Johnson", "McCreary County v. American Civil Liberties Union", "College Savings Bank v. Florida Prepaid Postsecondary Education Expense Board", "Schlesinger v. Councilman", "Payton v. New York", "Murdock v. Pennsylvania", "Immigration and Naturalization Service v. Abudu", "Dartmouth College v. Woodward", "Flood v. Kuhn", "United States v. Students Challenging Regulatory Agency Procedures", "Meredith v. Jefferson County Board of Education", "Hunt v. Cromartie", "Octane Fitness, LLC v. ICON Health & Fitness, Inc.", "American Insurance Co. v. 356 Bales of Cotton", "Bauer & Cie. v. O'Donnell", "Fox Film Corp. v. Muller", "Muller v. Oregon", "Eisenstadt v. Baird", "Baker v. Carr", "Hawker v. New York", "Thomas v. Review Board of the Indiana Employment Security Division", "Fuentes v. Shevin", "United Public Workers v. Mitchell", "Virginia State Pharmacy Board v. Virginia Citizens Consumer Council", "Beck v. Alabama", "American Broadcasting Cos. v. Aereo, Inc.", "Mistretta v. United States", "Burwell v. Hobby Lobby", "Pollock v. Farmers' Loan & Trust Co.", "United States v. Wong Kim Ark", "United States v. Playboy Entertainment Group", "United States v. Harris", "Randall v. Sorrell", "United States v. Leon", "New Jersey v. T. L. O.", "United Steelworkers v. Weber", "Sipuel v. Board of Regents of the University of Oklahoma", "Humphrey's Executor v. United States", "General Motors streetcar conspiracy", "Bowers v. Kerbaugh-Empire Co.", "Microsoft Corp. v. AT&T Corp.", "Lovell v. City of Griffin", "United States v. Kagama", "Brown v. Board of Education", "Charles River Bridge v. Warren Bridge", "Martin v. City of Struthers", "Old Colony Trust Co. v. Commissioner", "McLaughlin v. Florida", "Sherman v. United States", "In re Debs", "Simmons v. United States", "Oregon v. Bradshaw", "Willson v. Black-Bird Creek Marsh Co.", "United States v. Russell", "De Jonge v. Oregon", "NBC, Inc. v. United States", "Bailey v. Drexel Furniture Co.", "Solem v. Helm", "Oregon v. Guzek", "Plyler v. Doe", "Storer v. Brown", "Brown v. Hotel and Restaurant Employees", "Credit Suisse Securities (USA) LLC v. Billing", "Pace v. Alabama", "DeLima v. Bidwell", "Hustler Magazine v. Falwell", "Totten v. United States", "Gregory v. Helvering", "Betts v. Brady", "Egelhoff v. Egelhoff", "West Coast Hotel Co. v. Parrish", "Adams v. Texas", "Commissioner v. Wilcox", "New York Times Co. v. Sullivan", "Bates v. State Bar of Arizona", "McCullen v. Coakley", "Graver Tank & Manufacturing Co. v. Linde Air Products Co.", "Gray v. Sanders", "First National Bank of Boston v. Bellotti", "Reno v. American Civil Liberties Union", "Roth v. United States", "Qualitex Co. v. Jacobson Products Co.", "Pinkerton v. United States", "Brown v. Entertainment Merchants Ass'n", "Board of Regents of State Colleges v. Roth", "Baker v. Selden", "Egbert v. Lippmann", "MGM Studios, Inc. v. Grokster, Ltd.", "Hills v. Gautreaux", "Edwards v. South Carolina", "Lum v. Rice", "Flagg Bros., Inc. v. Brooks", "Church of the Holy Trinity v. United States", "League of United Latin American Citizens v. Perry", "Brigham City v. Stuart", "City of Richmond v. J.A. Croson Co.", "Rogers v. Tennessee", "Morissette v. United States", "Giles v. Harris", "Pierce v. Society of Sisters", "Smith v. Doe", "Loving v. Virginia", "Tison v. Arizona", "Town of Castle Rock v. Gonzales", "City of Elizabeth v. American Nicholson Pavement Co.", "Marsh v. Alabama", "Texas v. Johnson", "Silver v. New York Stock Exchange", "Jones v. Flowers", "Central Virginia Community College v. Katz", "Georgia v. Stanton", "Washington v. Davis", "Califano v. Yamasaki", "United States v. Reynolds", "Doyle v. Ohio", "Barron v. Baltimore", "Gitlow v. New York", "Gibson v. United States", "Hope v. Pelzer", "Westside School District v. Mergens", "Grosjean v. American Press Co.", "Tucker v. Texas", "Florida Lime & Avocado Growers, Inc. v. Paul", "United States v. Virginia", "New York City Transit Authority v. Beazer", "Penn Central Transportation Co. v. New York City", "Sereboff v. Mid Atlantic Medical Services, Inc.", "Lockett v. Ohio", "Memoirs v. Massachusetts", "Strauder v. West Virginia", "Brendlin v. California", "Fowler v. Rhode Island", "Nixon v. Fitzgerald", "United States v. Klein", "Northern Pipeline Construction Co. v. Marathon Pipe Line Co.", "O'Connor v. Donaldson", "TrafFix Devices, Inc. v. Marketing Displays, Inc.", "Maryland v. Craig", "District of Columbia v. Heller", "New York Times Co. v. Tasini", "Griffith v. Kentucky", "Eldred v. Ashcroft", "McLaurin v. Oklahoma State Regents", "Cooley v. Board of Wardens", "California v. Acevedo", "Baze v. Rees", "Chung Fook v. White", "Skinner v. Oklahoma", "Paul v. Virginia", "Rummel v. Estelle", "Gall v. United States", "Hans v. Louisiana", "Branzburg v. Hayes", "Byrd v. Blue Ridge Rural Electric Cooperative, Inc.", "Pickering v. Board of Education", "United States v. Morrison", "United States v. Grubbs", "Smith v. Allwright", "United States v. Ninety-Five Barrels Alleged Apple Cider Vinegar", "Caterpillar, Inc. v. Lewis", "Lujan v. Defenders of Wildlife", "Irwin v. Gavit", "Perez v. Brownell", "Adkins v. Children's Hospital", "City of Boerne v. Flores", "Pennoyer v. Neff", "Keystone Bituminous Coal Ass'n v. DeBenedictis", "Flint v. Stone Tracy Co.", "Cherokee Nation v. Georgia", "Immigration and Naturalization Service v. Chadha", "Milkovich v. Lorain Journal Co.", "Panetti v. Quarterman", "Missouri v. Holland", "Graham v. John Deere Co.", "Immigration and Naturalization Service v. Doherty", "Bunting v. Oregon", "Immigration and Naturalization Service v. Elias-Zacarias", "Anderson v. Mt. Clemens Pottery Co.", "South Dakota v. Dole", "Nixon v. United States", "Apprendi v. New Jersey", "Jamison v. Texas", "United States v. Forty Barrels & Twenty Kegs of Coca-Cola", "Cumming v. Richmond County Board of Education", "Adair v. United States", "Wisconsin v. Yoder", "Dames & Moore v. Regan", "Gibbons v. Ogden", "Wilkinson v. Austin", "Nebbia v. New York", "Cottage Savings Ass'n v. Commissioner", "Duro v. Reina", "Kansas v. Hendricks", "Peretz v. United States", "United States v. Shipp", "Washington v. Glucksberg", "Lochner v. New York", "United States v. Sioux Nation of Indians", "Parents Involved in Community Schools v. Seattle School District No. 1", "Greenholtz v. Inmates of the Nebraska Penal & Correctional Complex", "Northern Securities Co. v. United States", "Upjohn Co. v. United States", "Bernal v. Fainter", "Vermont Yankee Nuclear Power Corp. v. Natural Resources Defense Council, Inc.", "Parker v. Flook", "Cox v. Louisiana", "Johnson v. M'Intosh", "Stanford v. Kentucky", "Indiana v. Edwards", "Wisconsin v. Illinois", "Massachusetts v. Environmental Protection Agency", "Webster v. Doe", "Small v. United States", "Douglas v. City of Jeannette", "Griswold v. Connecticut", "Mertens v. Hewitt Associates", "Prince v. Massachusetts", "Cox v. United States", "Rochin v. California", "Bartnicki v. Vopper", "Gonzales v. Raich", "Joint Anti-Fascist Refugee Committee v. McGrath", "J.E.B. v. Alabama ex rel. T.B.", "Escobedo v. Illinois", "Puerto Rico v. Branstad", "Coffin v. United States", "Rumsfeld v. Forum for Academic & Institutional Rights, Inc.", "Thompson v. Oklahoma", "United States v. Hudson", "Hunt v. Washington State Apple Advertising Commission", "Berger v. New York", "Chauffeurs, Teamsters, & Helpers Local No. 391 v. Terry", "Arkansas Department of Human Services v. Ahlborn", "Pocket Veto Case", "Lee v. Weisman", "Republican Party of Minnesota v. White", "Katz v. United States", "North Carolina v. Alford", "Munn v. Illinois", "New Negro Alliance v. Sanitary Grocery Co.", "United States v. O'Brien", "Hollingsworth v. Virginia", "United Mine Workers of America v. Bagwell", "Whitney v. California", "United States v. Oakland Cannabis Buyers' Cooperative", "Cleveland Board of Education v. LaFleur", "United States v. Mead Corp.", "Osborne v. Ohio", "Hansberry v. Lee", "Penry v. Lynaugh", "Schenck v. United States", "Yates v. United States", "Miranda v. Arizona", "Buckley v. Valeo", "Florida v. Royer", "Allen v. Wright", "Talbot v. Janson", "Exxon Corp. v. Governor of Maryland", "Minersville School District v. Gobitis", "Youngstown Sheet & Tube Co. v. Sawyer", "Lopez v. Gonzales", "Loretto v. Teleprompter Manhattan CATV Corp.", "Follett v. Town of McCormick", "Cohen v. California", "Citizens to Preserve Overton Park v. Volpe", "Tennessee v. Garner", "Ohio v. Robinette", "Tennessee v. Lane", "Holmes v. South Carolina", "Schechter Poultry Corp. v. United States", "Schenck v. Pro-Choice Network of Western New York", "Bush v. Gore", "Taylor v. United States", "Ray v. Blair", "Leary v. United States", "Speiser v. Randall", "Rapanos v. United States", "Hepburn v. Griswold", "Muskrat v. United States", "DeShaney v. Winnebago County", "Wooley v. Maynard", "Kyllo v. United States", "Miami Herald Publishing Co. v. Tornillo", "Maynard v. Cartwright", "Henderson v. United States", "County of Allegheny v. American Civil Liberties Union", "Estelle v. Gamble", "Godfrey v. Georgia", "Houston East & West Texas Railway Co. v. United States", "Younger v. Harris", "Department of Transportation v. Public Citizen", "Watchtower Bible & Tract Society of New York, Inc. v. Village of Stratton", "New State Ice Co. v. Liebmann", "Mississippi University for Women v. Hogan", "Knowles v. Iowa", "Insular Cases", "National Federation of Independent Business v. Sebelius", "Clinton v. City of New York", "Joseph Burstyn, Inc. v. Wilson", "Dolan v. City of Tigard", "Ex parte Madrazzo", "Harper & Row v. Nation Enterprises", "Miller v. California", "City of Akron v. Akron Center for Reproductive Health", "Commodity Futures Trading Commission v. Schor", "United States v. Schwimmer", "United States v. Lopez", "Romer v. Evans", "Florida Prepaid Postsecondary Education Expense Board v. College Savings Bank", "Central Hudson Gas & Electric Corp. v. Public Service Commission", "Ring v. Arizona", "Burrow-Giles Lithographic Co. v. Sarony", "Troxel v. Granville", "Duke Power Co. v. Carolina Environmental Study Group", "Swidler & Berlin v. United States", "United States v. Bhagat Singh Thind", "Newberry v. United States", "Boumediene v. Bush", "Cox v. New Hampshire", "Baker v. Morton", "Piper Aircraft Co. v. Reyno", "United States v. Dominguez Benitez", "Yasui v. United States", "Bell v. Wolfish", "Ake v. Oklahoma", "Strickland v. Washington", "DeFunis v. Odegaard", "Hurtado v. California", "Goss v. Lopez", "Clay v. United States", "Burlington Northern & Santa Fe Railway Co. v. White", "Republic of Austria v. Altmann", "Londoner v. City and County of Denver", "Scott v. Harris", "Illinois v. Caballes", "Oregon Waste Systems, Inc. v. Department of Environmental Quality of Oregon", "Gertz v. Robert Welch, Inc.", "Lau v. Nichols", "Samson v. California", "Osborn v. Bank of the United States", "Harmelin v. Michigan", "Palazzolo v. Rhode Island", "Zablocki v. Redhail", "Railroad Commission v. Pullman Co.", "Oliphant v. Suquamish Indian Tribe", "United States v. Sprague", "United States v. Place", "TSC Industries, Inc. v. Northway, Inc.", "Jacobellis v. Ohio", "Stump v. Sparkman", "Atkins v. Virginia", "Hill v. Wallace", "Shaw v. Reno", "Festo Corp. v. Shoketsu Kinzoku Kogyo Kabushiki Co.", "Lucas v. South Carolina Coastal Council", "McCulloch v. Maryland", "Steward Machine Co. v. Davis", "BMW of North America, Inc. v. Gore", "Roper v. Simmons", "Sturges v. Crowninshield", "Asahi Metal Industry Co. v. Superior Court", "Diamond v. Chakrabarty", "Shapiro v. Thompson", "Slaughter-House Cases", "Alexander v. Sandoval", "Palko v. Connecticut", "Brown v. Louisiana", "Twining v. New Jersey", "Minnesota v. Mille Lacs Band of Chippewa Indians", "Kansas v. Marsh", "Milliken v. Bradley", "Bob Jones University v. United States", "United States v. Nixon", "Ex parte Endo", "Louisville & Nashville Railroad Co. v. Mottley", "Bragdon v. Abbott", "Arbaugh v. Y & H Corp.", "Saint Francis College v. al-Khazraji", "Sherbert v. Verner", "United States v. Moreland", "Ex parte Quirin", "Terry v. Ohio", "Vernonia School District 47J v. Acton", "Tahoe-Sierra Preservation Council, Inc. v. Tahoe Regional Planning Agency", "Hinderlider v. La Plata River & Cherry Creek Ditch Co.", "Georgia v. Randolph", "Jones v. Alfred H. Mayer Co.", "Colorado River Water Conservation District v. United States", "County of Sacramento v. Lewis", "Cruzan v. Director, Missouri Department of Health", "Domino's Pizza, Inc. v. McDonald", "Warth v. Seldin", "Plessy v. Ferguson", "James v. United States", "Rooker v. Fidelity Trust Co.", "Bose Corp. v. Consumers Union of United States, Inc.", "Hawaii Housing Authority v. Midkiff", "Whitman v. American Trucking Ass'ns, Inc.", "Estep v. United States", "Crandall v. Nevada", "Myers v. United States", "United States v. Seeger", "United States v. Ballard", "Harris v. Quinn", "Rita v. United States", "United States v. Thompson-Center Arms Co.", "Doe v. Bolton", "Edelman v. Jordan", "Rasul v. Bush", "Rankin v. McPherson", "National Treasury Employees Union v. Von Raab", "Barker v. Wingo", "MedImmune, Inc. v. Genentech, Inc.", "Chevron U.S.A., Inc. v. Natural Resources Defense Council, Inc.", "West Virginia State Board of Education v. Barnette", "Bouie v. City of Columbia", "Ex parte Garland", "Addington v. Texas", "Pennsylvania Coal Co. v. Mahon", "Goesaert v. Cleary", "Fullilove v. Klutznick", "Village of Belle Terre v. Boraas", "Verizon Communications Inc. v. Law Offices of Curtis V. Trinko, LLP", "Fitzpatrick v. Bitzer", "Gideon v. Wainwright", "Correctional Services Corp. v. Malesko", "United States v. Matlock", "Haynes v. United States", "Village of Arlington Heights v. Metropolitan Housing Development Corp.", "Lauro Lines v. Chasser", "Spector v. Norwegian Cruise Line Ltd.", "Heckler v. Chaney", "Stanley v. Georgia", "California v. Greenwood", "Flast v. Cohen", "Harper v. Virginia State Board of Elections", "Furman v. Georgia", "Santa Fe Independent School District v. Doe", "Ex parte Milligan", "Dickinson v. United States", "Lemon v. Kurtzman", "Dean Milk Co. v. City of Madison", "McNeil v. Wisconsin", "Taylor v. Mississippi", "Dolan v. United States Postal Service", "Good News Club v. Milford Central School", "United States v. Southwestern Cable Co.", "Brown v. Mississippi", "Granholm v. Heald", "Davis v. Washington", "Sicurella v. United States", "National Gay Task Force v. Board of Education", "Reid v. Covert", "Edwards v. Aguillard", "United States v. Constantine", "Bi-Metallic Investment Co. v. State Board of Equalization", "Fogerty v. Fantasy, Inc.", "Intel Corp. v. Advanced Micro Devices, Inc.", "Church of Lukumi Babalu Aye v. City of Hialeah", "Morrison v. Olson", "Harris v. McRae", "Marquez v. Screen Actors Guild Inc.", "Minor v. Happersett", "United States v. Robel", "Federal Election Commission v. Wisconsin Right to Life, Inc.", "Smith v. Maryland", "Blakely v. Washington", "Chimel v. California", "United States v. United States District Court", "Estelle v. Smith", "Gomillion v. Lightfoot", "Lawrence v. Texas", "United States v. Darby Lumber Co.", "Gratz v. Bollinger", "Toolson v. New York Yankees", "Erie Railroad Co. v. Tompkins", "Head Money Cases", "Banco Nacional de Cuba v. Sabbatino", "United States v. Paramount Pictures, Inc.", "Time, Inc. v. Firestone", "United States v. Flores-Montano", "Cooper v. Aaron", "Bowsher v. Synar", "Little v. Barreme", "Immigration and Naturalization Service v. Cardoza-Fonseca", "Saia v. New York", "Sweatt v. Painter", "Brushaber v. Union Pacific Railroad Co.", "DaimlerChrysler Corp. v. Cuno", "United States v. Hubbell", "Afroyim v. Rusk", "Wyoming v. Colorado", "Tennessee Valley Authority v. Hill", "Standard Oil Co. of New Jersey v. United States", "Sony Corp. of America v. Universal City Studios, Inc.", "New Mexico v. Texas", "KSR International Co. v. Teleflex Inc.", "Cohens v. Virginia", "Yick Wo v. Hopkins", "Friends of the Earth, Inc. v. Laidlaw Environmental Services, Inc.", "United States v. Kirby Lumber Co.", "City of Philadelphia v. New Jersey", "Bates v. City of Little Rock", "Board of Education v. Earls", "Wallace v. Cutten", "Hague v. Committee for Industrial Organization", "In re Gault", "World-Wide Volkswagen Corp. v. Woodson", "Boy Scouts of America v. Dale", "Swann v. Charlotte-Mecklenburg Board of Education", "Clearfield Trust Co. v. United States", "Williams v. Mississippi", "Federal Power Commission v. Tuscarora Indian Nation", "Taft v. Bowers", "Sheldon v. Sill", "Cannon v. University of Chicago", "Michigan v. Long", "Bailey v. United States", "Brady v. Maryland", "2003 term per curiam opinions of the Supreme Court of the United States", "Wilkerson v. Utah", "United States v. Trans-Missouri Freight Ass'n", "Interstate Commerce Commission v. Cincinnati, New Orleans & Texas Pacific Railway Co.", "Wabash, St. Louis & Pacific Railway Co. v. Illinois", "Powell v. McCormack", "United States v. Gouveia", "New York v. United States", "NLRB v. J. Weingarten, Inc.", "Nixon v. Herndon", "Hamdi v. Rumsfeld", "Kassel v. Consolidated Freightways Corp.", "Mississippi v. Johnson", "Linder v. United States", "Merck KGaA v. Integra Lifesciences I, Ltd.", "Kawakita v. United States", "Balzac v. Porto Rico", "Niemotko v. Maryland", "England v. Louisiana State Board of Medical Examiners", "Hammer v. Dagenhart", "Cheek v. United States", "Cantwell v. Connecticut", "Scott v. Illinois", "Ogden v. Saunders", "Chicago, Milwaukee & St. Paul Railway Co. v. Minnesota", "Lockyer v. Andrade", "Rust v. Sullivan", "Largent v. Texas", "United States v. Verdugo-Urquidez", "BP America Production Co. v. Burton", "Jones v. City of Opelika", "Quality King Distributors Inc., v. L'anza Research International Inc.", "Schweiker v. Chilicky", "Enmund v. Florida", "New York v. Ferber", "United States v. Carolene Products Co.", "Cooper Industries, Inc. v. Leatherman Tool Group, Inc.", "Nix v. Hedden", "Calder v. Jones", "Kohl v. United States", "Nixon v. Shrink Missouri Government PAC", "Monell v. Department of Social Services of the City of New York", "Witmer v. United States", "Landmark Communications, Inc. v. Virginia", "Berman v. Parker", "Coker v. Georgia", "Falbo v. United States", "Community for Creative Non-Violence v. Reid", "Olmstead v. United States", "Wallace v. Jaffree", "Moore v. Dempsey", "Carnival Cruise Lines, Inc. v. Shute", "Busey v. District of Columbia", "Ex parte McCardle", "International Shoe Co. v. Washington", "New England Mutual Life Insurance Co. v. Woodworth", "NLRB v. Jones & Laughlin Steel Corp.", "Guinn v. United States", "United States v. Harriss", "Locke v. Davey", "Sanchez-Llamas v. Oregon", "Albertson v. Subversive Activities Control Board", "Ohio v. Roberts", "SEC v. W. J. Howey Co.", "Edmonson v. Leesville Concrete Co.", "Harris v. Balk", "United States v. X-Citement Video, Inc.", "Coleman v. Miller", "Terminiello v. City of Chicago", "Ford v. Wainwright", "Guaranty Trust Co. v. York", "Mathews v. Eldridge", "Morse v. Frederick", "Sorrells v. United States", "Virginia v. Black", "Permanent Mission of India v. City of New York", "Moyer v. Peabody", "National League of Cities v. Usery", "Celotex Corp. v. Catrett", "Florida Bar v. Went For It, Inc.", "Hein v. Freedom From Religion Foundation", "Garcetti v. Ceballos", "Florida v. Riley", "Dun & Bradstreet, Inc. v. Greenmoss Builders, Inc.", "Clinton v. Jones", "Marsh v. Chambers", "United States v. Stewart", "Day v. McDonough", "Meritor Savings Bank v. Vinson", "Ozawa v. United States", "R.A.V. v. City of St. Paul", "Nix v. Whiteside", "Bethel School District v. Fraser", "Manual Enterprises, Inc. v. Day", "McConnell v. FEC", "Village of Euclid v. Ambler Realty Co.", "Virginia v. Moore", "Abington School District v. Schempp", "Agostini v. Felton", "Abrams v. United States", "Benton v. Maryland", "Breard v. Greene", "Griggs v. Duke Power Co.", "Mullane v. Central Hanover Bank & Trust Co.", "Lynch v. Donnelly", "Ex parte Bollman", "Almendarez-Torres v. United States", "Schneider v. New Jersey", "Seminole Tribe of Florida v. Florida", "Grutter v. Bollinger", "Stuart v. Laird", "Coppage v. Kansas", "Rosenberger v. University of Virginia", "Hudson v. Michigan", "Rice v. Cayetano", "NAACP v. Alabama", "Tory v. Cochran", "Employment Division v. Smith", "FCC v. Pacifica Foundation", "Hart v. United States"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-949", "prediction": ["# Reasoning Path:\nPennsylvania State University -> location.location.containedby -> Pennsylvania -> location.location.containedby -> United States of America\n# Answer:\nPennsylvania", "# Reasoning Path:\nPennsylvania State University -> location.location.containedby -> State College -> location.location.containedby -> Pennsylvania\n# Answer:\nState College", "# Reasoning Path:\nPennsylvania State University -> location.location.containedby -> Pennsylvania -> location.location.nearby_airports -> University Park Airport\n# Answer:\nPennsylvania", "# Reasoning Path:\nPennsylvania State University -> education.educational_institution.campuses -> Penn State University Creamery -> location.location.geolocation -> m.0z8b_gr\n# Answer:\nPenn State University Creamery", "# Reasoning Path:\nPennsylvania State University -> location.location.containedby -> Pennsylvania -> base.aareas.schema.administrative_area.administrative_children -> Harrisburg\n# Answer:\nPennsylvania", "# Reasoning Path:\nPennsylvania State University -> location.location.containedby -> Pennsylvania -> base.aareas.schema.administrative_area.administrative_children -> Hazleton\n# Answer:\nPennsylvania", "# Reasoning Path:\nPennsylvania State University -> education.educational_institution.campuses -> Penn State University Creamery -> education.educational_institution_campus.educational_institution -> Penn State College of Agricultural Sciences\n# Answer:\nPenn State University Creamery", "# Reasoning Path:\nPennsylvania State University -> location.location.containedby -> State College -> location.location.containedby -> Centre County\n# Answer:\nState College", "# Reasoning Path:\nPennsylvania State University -> location.location.containedby -> Pennsylvania -> base.aareas.schema.administrative_area.administrative_children -> New Kensington\n# Answer:\nPennsylvania", "# Reasoning Path:\nPennsylvania State University -> education.educational_institution.campuses -> Penn State University Creamery -> common.topic.notable_types -> Location\n# Answer:\nPenn State University Creamery"], "ground_truth": ["University Park"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 3: 1844-1846\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin -> book.author.works_written -> The Autobiography of Charles Darwin\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin -> book.author.works_written -> Observations on Stomata\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Anne Darwin -> people.person.parents -> Emma Darwin\n# Answer:\nAnne Darwin"], "ground_truth": ["Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Darwin Reader First Edition", "The living thoughts of Darwin", "The Descent of Man, and Selection in Relation to Sex", "From Darwin's unpublished notebooks", "Die geschlechtliche Zuchtwahl", "To the members of the Down Friendly Club", "Notebooks on transmutation of species", "Die fundamente zur entstehung der arten", "La vie et la correspondance de Charles Darwin", "The Variation of Animals and Plants under Domestication", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Kleinere geologische Abhandlungen", "Metaphysics, Materialism, & the evolution of mind", "Les moyens d'expression chez les animaux", "The Correspondence of Charles Darwin, Volume 8: 1860", "La facult\u00e9 motrice dans les plantes", "Monographs of the fossil Lepadidae and the fossil Balanidae", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "The Life and Letters of Charles Darwin Volume 1", "The Orgin of Species", "On Natural Selection", "H.M.S. Beagle in South America", "Questions about the breeding of animals", "On the origin of species by means of natural selection", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The Formation of Vegetable Mould through the Action of Worms", "The Essential Darwin", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "ontstaan der soorten door natuurlijke teeltkeus", "vari\u00eberen der huisdieren en cultuurplanten", "Diary of the voyage of H.M.S. Beagle", "Evolutionary Writings: Including the Autobiographies", "The Power of Movement in Plants", "Part I: Contributions to the Theory of Natural Selection / Part II", "The collected papers of Charles Darwin", "The Correspondence of Charles Darwin, Volume 17: 1869", "Darwin-Wallace", "The foundations of the Origin of species", "On evolution", "The Expression of the Emotions in Man and Animals", "The Correspondence of Charles Darwin, Volume 15: 1867", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "Volcanic Islands", "The Correspondence of Charles Darwin, Volume 18: 1870", "Charles Darwin's marginalia", "Evolution and natural selection", "red notebook of Charles Darwin", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Beagle letters", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Darwin en Patagonia", "Darwin", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "The principal works", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "The Correspondence of Charles Darwin, Volume 10: 1862", "Darwin and Henslow", "Voyage d'un naturaliste autour du monde", "Darwin for Today", "Les mouvements et les habitudes des plantes grimpantes", "Fertilisation of Orchids", "The Correspondence of Charles Darwin, Volume 12: 1864", "monograph on the sub-class Cirripedia", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "On the Movements and Habits of Climbing Plants", "Darwin's Ornithological notes", "Reise um die Welt 1831 - 36", "The Correspondence of Charles Darwin, Volume 11: 1863", "Darwin's insects", "Leben und Briefe von Charles Darwin", "The Autobiography of Charles Darwin", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Tesakneri tsagume\u030c", "Darwin's journal", "Rejse om jorden", "Darwin Darwin", "Reise eines Naturforschers um die Welt", "Cartas de Darwin 18251859", "Charles Darwin", "The Life of Erasmus Darwin", "genese\u014ds t\u014dn eid\u014dn", "A student's introduction to Charles Darwin", "The action of carbonate of ammonia on the roots of certain plants", "Evolution by natural selection", "The Life and Letters of Charles Darwin Volume 2", "Darwin on humus and the earthworm", "South American Geology", "The Correspondence of Charles Darwin, Volume 9: 1861", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "On the tendency of species to form varieties", "Del Plata a Tierra del Fuego", "The voyage of Charles Darwin", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "The\u0301orie de l'e\u0301volution", "Motsa ha-minim", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "El Origin De Las Especies", "The Darwin Reader Second Edition", "Les r\u00e9cifs de corail, leur structure et leur distribution", "A Darwin Selection", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Diario del Viaje de Un Naturalista Alrededor", "Geological Observations on South America", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "Notes on the fertilization of orchids", "The Structure and Distribution of Coral Reefs", "Het uitdrukken van emoties bij mens en dier", "Works", "The Correspondence of Charles Darwin, Volume 16: 1868", "Gesammelte kleinere Schriften", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Opsht\u0323amung fun menshen", "The education of Darwin", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Darwin's notebooks on transmutation of species", "Darwinism stated by Darwin himself", "The portable Darwin", "From so simple a beginning", "The Voyage of the Beagle", "Darwin from Insectivorous Plants to Worms", "Memorias y epistolario i\u0301ntimo", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "More Letters of Charles Darwin", "Darwin Compendium", "Wu zhong qi yuan", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "Origins", "Charles Darwin on the routes of male humble bees", "The Different Forms of Flowers on Plants of the Same Species", "Human nature, Darwin's view", "On a remarkable bar of sandstone off Pernambuco", "The geology of the voyage of H.M.S. Beagle", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Insectivorous Plants", "Charles Darwin's natural selection", "Geological Observations on the Volcanic Islands", "The Correspondence of Charles Darwin, Volume 13: 1865", "Charles Darwin's letters", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "Proiskhozhdenie vidov", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "Evolution", "The Correspondence of Charles Darwin, Volume 14: 1866", "Resa kring jorden"], "ans_acc": 0.05228758169934641, "ans_hit": 1, "ans_f1": 0.02566452795600367, "ans_precission": 0.7, "ans_recall": 0.013071895424836602, "path_f1": 0.3636363636363636, "path_precision": 1.0, "path_recall": 0.2222222222222222, "path_ans_f1": 0.09937888198757763, "path_ans_precision": 1.0, "path_ans_recall": 0.05228758169934641}
{"id": "WebQTest-952", "prediction": ["# Reasoning Path:\nAlaska -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> location.administrative_division.country -> United States of America -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.location.containedby -> Americas\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.location.partially_contains -> Alsek River\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> location.administrative_division.country -> United States of America -> location.location.containedby -> Americas\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> location.location.containedby -> United States of America -> location.location.containedby -> Americas\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.location.partially_contains -> Fortymile River\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> location.administrative_division.country -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America"], "ground_truth": ["United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-953", "prediction": ["# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> location.location.containedby -> United States of America\n# Answer:\nKansas", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nKansas", "# Reasoning Path:\nKansas City -> location.location.containedby -> Missouri -> location.location.containedby -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Platte County -> location.location.containedby -> Missouri\n# Answer:\nPlatte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> location.us_state.capital -> Topeka\n# Answer:\nKansas", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Platte County -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nPlatte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Kansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Platte County -> periodicals.newspaper_circulation_area.newspapers -> The Kansas City Star\n# Answer:\nPlatte County", "# Reasoning Path:\nKansas City -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nKansas City -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone"], "ground_truth": ["Platte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-954", "prediction": ["# Reasoning Path:\nPink Floyd -> music.musical_group.member -> m.02hz8bl -> music.group_membership.member -> Syd Barrett\n# Answer:\nm.02hz8bl", "# Reasoning Path:\nPink Floyd -> music.musical_group.member -> g.11b6bd_cl2\n# Answer:\ng.11b6bd_cl2", "# Reasoning Path:\nPink Floyd -> music.musical_group.member -> g.11b6bf5_mc\n# Answer:\ng.11b6bf5_mc", "# Reasoning Path:\nPink Floyd -> music.musical_group.member -> m.02hz8bl -> music.group_membership.role -> Lead Vocals\n# Answer:\nm.02hz8bl", "# Reasoning Path:\nPink Floyd -> music.musical_group.member -> m.02hz8bl -> freebase.valuenotation.is_reviewed -> Group\n# Answer:\nm.02hz8bl", "# Reasoning Path:\nPink Floyd -> music.musical_group.member -> m.02hz8bl -> music.group_membership.role -> Lead guitar\n# Answer:\nm.02hz8bl", "# Reasoning Path:\nPink Floyd -> music.musical_group.member -> m.02hz8bl -> freebase.valuenotation.is_reviewed -> Member\n# Answer:\nm.02hz8bl", "# Reasoning Path:\nPink Floyd -> music.artist.genre -> Acid rock -> music.genre.parent_genre -> Garage rock\n# Answer:\nAcid rock", "# Reasoning Path:\nPink Floyd -> music.artist.genre -> Acid rock -> common.topic.notable_for -> g.1257tm1jy\n# Answer:\nAcid rock", "# Reasoning Path:\nPink Floyd -> music.musical_group.member -> m.02hz8bl -> freebase.valuenotation.is_reviewed -> Period (end)\n# Answer:\nm.02hz8bl"], "ground_truth": ["Psychedelic Session-compilation album"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-955", "prediction": ["# Reasoning Path:\nGranville Woods -> people.person.spouse_s -> m.0j6f6k_ -> people.marriage.spouse -> Loretta Woods\n# Answer:\nm.0j6f6k_", "# Reasoning Path:\nGranville Woods -> people.person.spouse_s -> m.0j6f6k_ -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0j6f6k_", "# Reasoning Path:\nGranville Woods -> people.person.spouse_s -> m.0j6f6k_ -> freebase.valuenotation.has_value -> From\n# Answer:\nm.0j6f6k_", "# Reasoning Path:\nGranville Woods -> people.deceased_person.place_of_death -> New York City -> travel.travel_destination.tourist_attractions -> A.I.R. Gallery\n# Answer:\nNew York City", "# Reasoning Path:\nGranville Woods -> people.deceased_person.place_of_death -> New York City -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nNew York City", "# Reasoning Path:\nGranville Woods -> people.person.spouse_s -> m.0j6f6k_ -> freebase.valuenotation.has_value -> To\n# Answer:\nm.0j6f6k_", "# Reasoning Path:\nGranville Woods -> people.deceased_person.place_of_death -> New York City -> travel.travel_destination.tourist_attractions -> American Folk Art Museum\n# Answer:\nNew York City", "# Reasoning Path:\nGranville Woods -> people.deceased_person.place_of_death -> New York City -> travel.travel_destination.tourist_attractions -> American Museum of Natural History\n# Answer:\nNew York City", "# Reasoning Path:\nGranville Woods -> people.deceased_person.place_of_death -> New York City -> location.location.containedby -> Area code 917\n# Answer:\nNew York City", "# Reasoning Path:\nGranville Woods -> law.inventor.inventions -> Incubator -> common.topic.image -> Drebbel's Incubator\n# Answer:\nIncubator"], "ground_truth": ["Loretta Woods"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-957", "prediction": ["# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> French -> language.human_language.language_family -> Indo-European languages\n# Answer:\nFrench", "# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> English Language -> fictional_universe.fictional_language.where_spoken -> Heroes in Hell fictional universe\n# Answer:\nEnglish Language", "# Reasoning Path:\nSioux -> people.ethnicity.includes_groups -> Brul\u00e9 -> people.ethnicity.languages_spoken -> Lakota Language\n# Answer:\nBrul\u00e9", "# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> English Language -> organization.organization_sector.organizations_in_this_sector -> dragon-class.com\n# Answer:\nEnglish Language", "# Reasoning Path:\nSioux -> people.ethnicity.included_in_group -> Native Americans in the United States -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nNative Americans in the United States", "# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Barbados\n# Answer:\nEnglish Language", "# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> French -> fictional_universe.fictional_language.where_spoken -> Heroes in Hell fictional universe\n# Answer:\nFrench", "# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> Sioux language -> language.human_language.main_country -> United States of America\n# Answer:\nSioux language", "# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Cameroon\n# Answer:\nEnglish Language"], "ground_truth": ["Sioux language", "English Language", "French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-958", "prediction": ["# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> base.culturalevent.event.entity_involved -> Osm\u00edn Aguirre y Salinas\n# Answer:\n1932 Salvadoran peasant massacre", "# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> base.culturalevent.event.entity_involved -> Maximiliano Hern\u00e1ndez Mart\u00ednez\n# Answer:\n1932 Salvadoran peasant massacre", "# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> common.topic.notable_for -> g.125btm68w\n# Answer:\n1932 Salvadoran peasant massacre", "# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> base.culturalevent.event.entity_involved -> Armed Forces of El Salvador\n# Answer:\n1932 Salvadoran peasant massacre", "# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> common.topic.article -> m.0bjpf4\n# Answer:\n1932 Salvadoran peasant massacre", "# Reasoning Path:\nEl Salvador -> location.location.events -> 2009 El Salvador floods and mudslides -> event.disaster.type_of_disaster -> Flood\n# Answer:\n2009 El Salvador floods and mudslides", "# Reasoning Path:\nEl Salvador -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Venezuela\n# Answer:\nConstitutional republic", "# Reasoning Path:\nEl Salvador -> location.statistical_region.poverty_rate_2dollars_per_day -> g.11b6c_pzzl\n# Answer:\ng.11b6c_pzzl", "# Reasoning Path:\nEl Salvador -> location.location.events -> 2014 Grand Prix GSB -> award.competition.winner -> Olga Zabelinskaya\n# Answer:\n2014 Grand Prix GSB", "# Reasoning Path:\nEl Salvador -> location.location.events -> 2009 El Salvador floods and mudslides -> common.topic.article -> m.09gjd8v\n# Answer:\n2009 El Salvador floods and mudslides"], "ground_truth": ["Robert Renderos", "Wilfredo Iraheta", "Mauricio Alvarenga", "Francisca Gonz\u00e1lez", "Keoki", "Jose B. Gonzalez", "Marlon Menj\u00edvar", "Tom\u00e1s Medina", "Rene Moran", "Takeshi Fujiwara", "Jos\u00e9 Francisco Valiente", "Jose Orlando Martinez", "Sarah Ramos", "Patricia Chica", "Gualberto Fern\u00e1ndez", "Pedro Chavarria", "Armando Chac\u00f3n", "Carlos Linares", "Jos\u00e9 Castellanos Contreras", "Joel Aguilar", "Jorge B\u00facaro", "Elmer Acevedo", "Norman Quijano", "Jose Solis", "Damaris Qu\u00e9les", "Malin Arvidsson", "Victor Manuel Ochoa", "Eduardo \\\"Volkswagen\\\" Hern\u00e1ndez", "Arturo Rivera y Damas", "Victor Lopez", "Salvador Castaneda Castro", "Papa A.P.", "Rub\u00e9n Zamora", "Elena Diaz", "\u00c1ngel Orellana", "Jos\u00e9 Luis Rugamas", "Richard Oriani", "Am\u00e9rico Gonz\u00e1lez", "Genaro Serme\u00f1o", "Roberto Rivas", "Fausto Omar V\u00e1squez", "Consuelo de Saint Exup\u00e9ry", "Eva Dimas", "William L\u00f3pez", "Eduardo Hern\u00e1ndez", "Melvin Barrera", "Mario Montoya", "Miguel Cruz", "Ra\u00fal Cicero", "Emilio Guardado", "Andr\u00e9s Eduardo Men\u00e9ndez", "Francisco Gavidia", "Ricardo L\u00f3pez Tenorio", "Mauricio Alfaro", "Julio Adalberto Rivera Carballo", "Johnny Lopez", "Milton Palacios", "Camilo Minero", "William Armando", "Doroteo Vasconcelos", "Miguel Angel Deras", "Xenia Estrada", "Francisco Due\u00f1as", "Prudencia Ayala", "Paula Heredia", "Arturo Armando Molina", "Jos\u00e9 Manfredi Portillo", "Rafael Campo", "Mario Wilfredo Contreras", "Mauricio Alonso Rodr\u00edguez", "Manuel Enrique Araujo", "Jos\u00e9 Mar\u00eda Ca\u00f1as", "Roberto Carlos Martinez", "Juan Ram\u00f3n S\u00e1nchez", "V\u00edctor Ram\u00edrez", "\u00d3scar Antonio Ulloa", "Enrique \u00c1lvarez C\u00f3rdova", "Alfredo Ruano", "Gerardo Barrios", "Nicolas F. Shi", "Francisco Men\u00e9ndez", "Miguel Ca\u00f1izalez", "Juan Rafael Bustillo", "Jos\u00e9 Inocencio Alas", "Selvin Gonz\u00e1lez", "Isa\u00edas Choto", "Francisco Funes", "Ana Maria de Martinez", "Alexander Campos", "Carlos Barrios", "Jorge Rivera", "Pedro Geoffroy Rivas", "Santiago \\\"Jimmy\\\" Mellado", "Erwin McManus", "Claudia Lars", "Steve Montenegro", "Rafael Menj\u00edvar Ochoa", "Jaime Portillo", "William Renderos Iraheta", "Pedro Jos\u00e9 Escal\u00f3n", "F\u00e9lix Pineda", "Bobby Rivas", "Diego Vel\u00e1zquez", "Ernesto Aparicio", "Alexander M\u00e9ndoza", "Bernard Lewinsky", "Guillermo Garc\u00eda", "Ruben Cedillos", "Jorge Mel\u00e9ndez", "g.11b8058v7j", "Saturnino Osorio", "Ana Sol Gutierrez", "Laura Molina", "Edwin Ramos", "DJ Quest", "Rutilio Grande"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-959", "prediction": ["# Reasoning Path:\nBrett Gardner -> sports.pro_athlete.teams -> m.03z8p87 -> sports.sports_team_roster.team -> New York Yankees\n# Answer:\nm.03z8p87", "# Reasoning Path:\nBrett Gardner -> sports.pro_athlete.teams -> m.03z8p87 -> sports.sports_team_roster.position -> Outfielder\n# Answer:\nm.03z8p87", "# Reasoning Path:\nBrett Gardner -> baseball.baseball_player.batting_stats -> m.06qf764 -> baseball.batting_statistics.team -> New York Yankees\n# Answer:\nm.06qf764", "# Reasoning Path:\nBrett Gardner -> sports.pro_athlete.teams -> m.03z8p87 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.03z8p87", "# Reasoning Path:\nBrett Gardner -> baseball.baseball_player.batting_stats -> m.06qf764 -> baseball.batting_statistics.season -> 2008 Major League Baseball season\n# Answer:\nm.06qf764", "# Reasoning Path:\nBrett Gardner -> people.person.places_lived -> m.03pk8j5 -> people.place_lived.location -> South Carolina\n# Answer:\nm.03pk8j5", "# Reasoning Path:\nBrett Gardner -> people.person.places_lived -> m.0wk30xl -> people.place_lived.location -> Holly Hill\n# Answer:\nm.0wk30xl"], "ground_truth": ["New York Yankees"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4, "path_precision": 0.2857142857142857, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 1.0}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> location.administrative_division.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.film -> End of the Road: How Money Became Worthless\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15 -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.02shm15", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.0_0cs2_ -> film.personal_film_appearance.film -> Our Nixon\n# Answer:\nm.0_0cs2_", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsgl5 -> tv.tv_guest_role.episodes_appeared_in -> The Future of the GOP\n# Answer:\nm.09nsgl5", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglb -> tv.tv_guest_role.episodes_appeared_in -> The American Film Institute Salute to James Cagney\n# Answer:\nm.09nsglb", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15 -> film.personal_film_appearance.film -> Sicko\n# Answer:\nm.02shm15", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglh -> tv.tv_guest_role.episodes_appeared_in -> Johnny Grant\n# Answer:\nm.09nsglh"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-960", "prediction": ["# Reasoning Path:\nThe Office -> tv.tv_series_season.regular_cast -> m.02l00g_ -> tv.regular_tv_appearance.actor -> Rainn Wilson\n# Answer:\nm.02l00g_", "# Reasoning Path:\nThe Office -> tv.tv_series_season.regular_cast -> m.02l00g_ -> tv.regular_tv_appearance.character -> Dwight Schrute\n# Answer:\nm.02l00g_", "# Reasoning Path:\nThe Office -> tv.tv_series_season.regular_cast -> m.021y6jp -> tv.regular_tv_appearance.actor -> Steve Carell\n# Answer:\nm.021y6jp", "# Reasoning Path:\nThe Office -> tv.tv_series_season.regular_cast -> m.02l00g_ -> tv.regular_tv_appearance.seasons -> The Office - Season 7\n# Answer:\nm.02l00g_", "# Reasoning Path:\nThe Office -> tv.tv_series_season.regular_cast -> m.02l00hg -> tv.regular_tv_appearance.actor -> Jenna Fischer\n# Answer:\nm.02l00hg", "# Reasoning Path:\nThe Office -> award.award_winning_work.awards_won -> m.0nbwfjm -> award.award_honor.award_winner -> Rainn Wilson\n# Answer:\nm.0nbwfjm", "# Reasoning Path:\nThe Office -> award.award_winning_work.awards_won -> m.0z86k10 -> award.award_honor.award_winner -> Rainn Wilson\n# Answer:\nm.0z86k10", "# Reasoning Path:\nThe Office -> tv.tv_series_season.episodes -> A Benihana Christmas\n# Answer:\nA Benihana Christmas", "# Reasoning Path:\nThe Office -> tv.tv_series_season.regular_cast -> m.021y6jp -> tv.regular_tv_appearance.seasons -> The Office - Season 7\n# Answer:\nm.021y6jp", "# Reasoning Path:\nThe Office -> award.award_winning_work.awards_won -> m.0z86k10 -> award.award_honor.award -> TV Land Future Classic Award\n# Answer:\nm.0z86k10"], "ground_truth": ["Rainn Wilson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-962", "prediction": ["# Reasoning Path:\nSan Diego -> location.location.containedby -> San Diego County -> location.location.containedby -> California\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego -> location.location.nearby_airports -> San Diego International Airport -> location.location.containedby -> San Diego County\n# Answer:\nSan Diego International Airport", "# Reasoning Path:\nSan Diego -> location.location.containedby -> San Diego County -> location.hud_foreclosure_area.total_90_day_vacant_residential_addresses -> m.07hb05d\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego -> location.location.containedby -> 92101 -> location.location.containedby -> San Diego County\n# Answer:\n92101", "# Reasoning Path:\nSan Diego -> location.location.containedby -> San Diego County -> location.location.containedby -> United States of America\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego -> location.location.containedby -> Area code 858 -> location.location.containedby -> San Diego County\n# Answer:\nArea code 858", "# Reasoning Path:\nSan Diego -> location.location.nearby_airports -> San Diego International Airport -> location.location.containedby -> United States of America\n# Answer:\nSan Diego International Airport", "# Reasoning Path:\nSan Diego -> location.location.nearby_airports -> Brown Field Municipal Airport -> location.location.containedby -> San Diego County\n# Answer:\nBrown Field Municipal Airport", "# Reasoning Path:\nSan Diego -> location.statistical_region.population -> g.11b66mljnh\n# Answer:\ng.11b66mljnh", "# Reasoning Path:\nSan Diego -> location.location.containedby -> San Diego County -> location.location.people_born_here ->  Jamie Bock\n# Answer:\nSan Diego County"], "ground_truth": ["San Diego County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-963", "prediction": ["# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.010hdmz0 -> sports.sports_league_draft_pick.player -> Jarvis Landry\n# Answer:\nm.010hdmz0", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.05bj8vr -> sports.sports_league_draft_pick.player -> Jake Long\n# Answer:\nm.05bj8vr", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.010hdmz0 -> sports.sports_league_draft_pick.school -> Louisiana State University\n# Answer:\nm.010hdmz0", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.010q80zg -> sports.sports_league_draft_pick.player -> Dan Marino\n# Answer:\nm.010q80zg", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.010hdmz0 -> sports.sports_league_draft_pick.draft -> 2014 NFL draft\n# Answer:\nm.010hdmz0", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.010q80zg -> sports.sports_league_draft_pick.school -> University of Pittsburgh\n# Answer:\nm.010q80zg", "# Reasoning Path:\nMiami Dolphins -> base.schemastaging.organization_extra.contact_webpages -> m.010g8ggp -> freebase.valuenotation.is_reviewed -> Language\n# Answer:\nm.010g8ggp", "# Reasoning Path:\nMiami Dolphins -> base.schemastaging.organization_extra.contact_webpages -> m.010g8ggp -> internet.localized_uri.language -> English\n# Answer:\nm.010g8ggp", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.010q80zg -> sports.sports_league_draft_pick.draft -> 1983 NFL draft\n# Answer:\nm.010q80zg", "# Reasoning Path:\nMiami Dolphins -> base.schemastaging.organization_extra.contact_webpages -> m.010g8ggp -> internet.localized_uri.location -> United States of America\n# Answer:\nm.010g8ggp"], "ground_truth": ["Ryan Tannehill", "Matt Moore"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-965", "prediction": ["# Reasoning Path:\nAnne Frank -> base.popstra.celebrity.arrest -> m.0dj_t7p -> base.popstra.arrest.location -> Amsterdam\n# Answer:\nm.0dj_t7p", "# Reasoning Path:\nAnne Frank -> people.person.education -> m.0bdw7f8 -> education.education.institution -> Montessori Lyceum Amsterdam\n# Answer:\nm.0bdw7f8", "# Reasoning Path:\nAnne Frank -> base.popstra.celebrity.arrest -> m.0dj_t7p -> base.popstra.arrest.offense -> Karl Silberbauer\n# Answer:\nm.0dj_t7p", "# Reasoning Path:\nAnne Frank -> people.person.education -> m.0t5tz6k -> education.education.institution -> Jewish Lyceum\n# Answer:\nm.0t5tz6k", "# Reasoning Path:\nAnne Frank -> people.person.places_lived -> m.0djzwlm -> people.place_lived.location -> Amsterdam\n# Answer:\nm.0djzwlm", "# Reasoning Path:\nAnne Frank -> people.person.education -> m.0bdw7f8 -> freebase.valuenotation.has_value -> Degree\n# Answer:\nm.0bdw7f8", "# Reasoning Path:\nAnne Frank -> people.person.education -> m.0bdw7f8 -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nm.0bdw7f8", "# Reasoning Path:\nAnne Frank -> people.person.education -> m.0t5tz6k -> freebase.valuenotation.has_value -> Degree\n# Answer:\nm.0t5tz6k", "# Reasoning Path:\nAnne Frank -> people.person.places_lived -> m.0djzwl6 -> people.place_lived.location -> Auschwitz concentration camp\n# Answer:\nm.0djzwl6", "# Reasoning Path:\nAnne Frank -> people.person.education -> m.0bdw7f8 -> freebase.valuenotation.has_value -> Specialization\n# Answer:\nm.0bdw7f8"], "ground_truth": ["Jewish Lyceum", "Montessori Lyceum Amsterdam"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.28571428571428575, "path_precision": 0.2, "path_recall": 0.5, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-966", "prediction": ["# Reasoning Path:\nMonica -> people.person.parents -> M.C. Arnold Jr. -> people.person.children -> Jermond Grant Arnold\n# Answer:\nM.C. Arnold Jr.", "# Reasoning Path:\nMonica -> celebrities.celebrity.sexual_relationships -> m.05nlg42 -> celebrities.romantic_relationship.celebrity -> Rocko\n# Answer:\nm.05nlg42", "# Reasoning Path:\nMonica -> people.person.parents -> M.C. Arnold Jr. -> people.person.children -> Montez Arnold\n# Answer:\nM.C. Arnold Jr.", "# Reasoning Path:\nMonica -> people.person.parents -> M.C. Arnold Jr. -> freebase.valuenotation.has_value -> Parents\n# Answer:\nM.C. Arnold Jr.", "# Reasoning Path:\nMonica -> people.person.parents -> M.C. Arnold Jr. -> people.person.gender -> Male\n# Answer:\nM.C. Arnold Jr.", "# Reasoning Path:\nMonica -> people.person.parents -> Marilyn Best -> people.person.children -> Tron Best\n# Answer:\nMarilyn Best", "# Reasoning Path:\nMonica -> celebrities.celebrity.sexual_relationships -> m.05nlg42 -> celebrities.romantic_relationship.relationship_type -> Engaged to\n# Answer:\nm.05nlg42", "# Reasoning Path:\nMonica -> people.person.spouse_s -> m.0h3gsqb -> people.marriage.spouse -> Shannon Brown\n# Answer:\nm.0h3gsqb", "# Reasoning Path:\nMonica -> people.person.parents -> Marilyn Best -> freebase.valuenotation.has_value -> Parents\n# Answer:\nMarilyn Best", "# Reasoning Path:\nMonica -> people.person.parents -> Marilyn Best -> common.topic.notable_for -> g.12557crfr\n# Answer:\nMarilyn Best"], "ground_truth": ["Shannon Brown"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-967", "prediction": ["# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Suicide -> common.topic.notable_types -> Cause Of Death\n# Answer:\nSuicide", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ng.1254zbtvv", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.article -> m.02qnd1f\n# Answer:\nBallistic trauma", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nBallistic trauma", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.notable_for -> g.12558k_cd\n# Answer:\nBallistic trauma", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09wf140", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.resource -> Another Megan Fox letter -- and this one sings her praises!\n# Answer:\nm.09wf140", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wgn1j -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09wgn1j", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wgn1j -> common.webpage.resource -> Charles Manson: 40 years later, the movie about him you have to see\n# Answer:\nm.09wgn1j", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wqfb3 -> common.webpage.resource -> Erykah Badu teases 'New Amerykah Part Two' with trippy, funky clip\n# Answer:\nm.09wqfb3"], "ground_truth": ["Suicide", "Ballistic trauma"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-968", "prediction": ["# Reasoning Path:\nGermany -> location.country.languages_spoken -> Bavarian Language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nBavarian Language", "# Reasoning Path:\nGermany -> location.country.official_language -> German Language -> language.human_language.dialects -> Bavarian Language\n# Answer:\nGerman Language", "# Reasoning Path:\nGermany -> location.country.languages_spoken -> Bavarian Language -> language.language_dialect.language -> German Language\n# Answer:\nBavarian Language", "# Reasoning Path:\nGermany -> location.country.languages_spoken -> Bavarian Language -> language.human_language.main_country -> Austria\n# Answer:\nBavarian Language", "# Reasoning Path:\nGermany -> location.country.official_language -> German Language -> language.human_language.dialects -> Saxon, Upper Language\n# Answer:\nGerman Language", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\ng.11b60v9zy7", "# Reasoning Path:\nGermany -> location.country.languages_spoken -> German Language -> language.human_language.dialects -> Bavarian Language\n# Answer:\nGerman Language", "# Reasoning Path:\nGermany -> location.country.official_language -> German Language -> language.human_language.region -> Europe\n# Answer:\nGerman Language", "# Reasoning Path:\nGermany -> location.country.official_language -> German Language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nGerman Language", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fdl8\n# Answer:\ng.12tb6fdl8"], "ground_truth": ["Saxon, Upper Language", "German Language", "Frisian, Northern Language", "Low German", "Turoyo Language", "Bavarian Language"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.47058823529411764, "ans_precission": 0.8, "ans_recall": 0.3333333333333333, "path_f1": 0.45161290322580644, "path_precision": 0.7, "path_recall": 0.3333333333333333, "path_ans_f1": 0.6153846153846154, "path_ans_precision": 0.8, "path_ans_recall": 0.5}
{"id": "WebQTest-969", "prediction": ["# Reasoning Path:\nDenver Broncos -> sports.sports_team.championships -> Super Bowl XXXII -> base.schemastaging.context_name.pronunciation -> g.125_r38hn\n# Answer:\nSuper Bowl XXXII", "# Reasoning Path:\nDenver Broncos -> sports.sports_team.championships -> Super Bowl XXXII -> sports.sports_championship_event.runner_up -> Green Bay Packers\n# Answer:\nSuper Bowl XXXII", "# Reasoning Path:\nDenver Broncos -> sports.sports_team.championships -> Super Bowl XXXII -> base.newsevents.news_reported_event.news_report_s -> m.0bj7xr9\n# Answer:\nSuper Bowl XXXII", "# Reasoning Path:\nDenver Broncos -> sports.sports_team.championships -> 1998 AFC Championship Game -> time.event.instance_of_recurring_event -> AFC Championship Game\n# Answer:\n1998 AFC Championship Game", "# Reasoning Path:\nDenver Broncos -> sports.sports_team.championships -> 1990 AFC Championship Game -> common.topic.notable_for -> g.1q3sfq3cn\n# Answer:\n1990 AFC Championship Game", "# Reasoning Path:\nDenver Broncos -> sports.sports_team.championships -> 1998 AFC Championship Game -> sports.sports_championship_event.season -> 1997 NFL season\n# Answer:\n1998 AFC Championship Game", "# Reasoning Path:\nDenver Broncos -> sports.sports_team.venue -> m.0tm17p3 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0tm17p3", "# Reasoning Path:\nDenver Broncos -> american_football.football_team.home_games -> Buffalo Bills at Denver Broncos, 2008-12-21 -> american_football.football_game.season -> 2008 NFL season\n# Answer:\nBuffalo Bills at Denver Broncos, 2008-12-21", "# Reasoning Path:\nDenver Broncos -> sports.sports_team.championships -> 1990 AFC Championship Game -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n1990 AFC Championship Game", "# Reasoning Path:\nDenver Broncos -> sports.sports_team.venue -> m.0tm17p3 -> sports.team_venue_relationship.venue -> Sports Authority Field at Mile High\n# Answer:\nm.0tm17p3"], "ground_truth": ["Super Bowl XXXIII"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.size -> m.0kjrkq\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> After All -> music.composition.recordings -> After All (stereo mix)\n# Answer:\nAfter All", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.appears_in_topic_gallery -> Grammy Legend Award\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.appears_in_topic_gallery -> Zubin Mehta\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> Come Spy with Me -> common.topic.notable_for -> g.1yl5jhd14\n# Answer:\nCome Spy with Me", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.size -> m.02cljr8\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.subjects -> Bleona\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Music executive -> common.topic.article -> m.047rgq1\n# Answer:\nMusic executive", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Musician -> freebase.equivalent_topic.equivalent_type -> Musical Artist\n# Answer:\nMusician"], "ground_truth": ["The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "Just a Touch Away", "You Are So Beautiful (feat. Dave Koz)", "Cruisin", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "I've Got You Under My Skin", "We\u2019ve Come Too Far to End It Now", "Share It", "The Tears of a Clown", "Just To See Her Again", "Photograph in My Mind", "I'll Keep My Light In My Window", "As You Do", "Hanging on by a Thread", "I Am I Am", "Walk on By", "Standing On Jesus", "Tell Me Tomorrow", "Let Me Be The Clock", "Be Kind to the Growing Mind", "Love Bath", "You Cannot Laugh Alone", "Ever Had A Dream", "Happy (Love Theme From Lady Sings the Blues)", "Night and Day", "You Go to My Head", "Just to See Her", "The Hurt's On You", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "Love Is The Light", "Will You Still Love Me Tomorrow", "Love So Fine", "Time Flies", "You're Just My Life (feat. India.Arie)", "Medley: Never My Love / Never Can Say Goodbye", "Crusin'", "Vitamin U", "We've Saved The Best For Last (Kenny G with Smokey Robinson)", "Ebony Eyes (Duet with Rick James)", "It's Christmas Time", "Coincidentally", "You Really Got a Hold on Me", "One Time", "Bad Girl", "Will You Love Me Tomorrow", "Satisfy You", "Fly Me to the Moon (In Other Words)", "If You Can Want", "Everything You Touch", "Blame It On Love (Duet with Barbara Mitchell)", "God Rest Ye Merry Gentlemen", "I Can't Find", "Keep Me", "I\u2019ve Got You Under My Skin", "Tears of a Clown", "Don't Know Why", "Get Ready", "Away in the Manger / Coventry Carol", "Melody Man", "There Will Come a Day (I'm Gonna Happen to You)", "Rewind", "I Love Your Face", "Noel", "That Place", "Nearness of You", "A Silent Partner in a Three-Way Love Affair", "Gone Forever", "Girlfriend", "Jingle Bells", "The Love Between Me and My Kids", "Tea for Two", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "Close Encounters of the First Kind", "Don't Wanna Be Just Physical", "Going to a Go Go", "The Agony and the Ecstasy", "I Have Prayed On It", "Going to a Go-Go", "If You Want My Love", "Tears of a Sweet Free Clown", "Easy", "Love Letters", "Let Me Be the Clock", "Baby That's Backatcha", "The Road to Damascus", "Save Me", "Quiet Storm (Groove Boutique Chill Jazz mix)", "Quiet Storm (single version)", "Baby Come Close", "You Made Me Feel Love", "And I Don't Love You (Larry Levan instrumental dub)", "It's a Good Feeling", "I Am, I Am", "Whatcha Gonna Do", "There Will Come A Day ( I'm Gonna Happen To You )", "I Know You by Heart", "And I Love Her", "A Tattoo", "Love Brought Us Here", "Holly", "Did You Know (Berry's Theme)", "Fallin'", "My Guy", "I Can't Give You Anything but Love", "Christmas Everyday", "Fulfill Your Need", "Why Do Happy Memories Hurt So Bad", "Food For Thought", "The Tears Of A Clown", "Ooo Baby Baby", "I Want You Back", "In My Corner", "Really Gonna Miss You", "No Time to Stop Believing", "Heavy On Pride (Light On Love)", "Tears Of A Clown", "Cruisin'", "She's Only a Baby Herself", "I Second That Emotion", "Because of You It's the Best It's Ever Been", "Virgin Man", "We Are The Warriors", "Ain't That Peculiar", "Love Don' Give No Reason (12 Inch Club Mix)", "I've Made Love to You a Thousand Times", "The Christmas Song", "I Care About Detroit", "More Than You Know", "Asleep on My Love", "Daylight & Darkness", "Quiet Storm", "Please Come Home for Christmas", "Mickey's Monkey", "Little Girl Little Girl", "Come to Me Soon", "Same Old Love", "Let Your Light Shine On Me", "Speak Low", "Sweet Harmony", "Come by Here (Kum Ba Ya)", "Rack Me Back", "More Love", "Jesus Told Me To Love You", "I Love The Nearness Of You", "Be Who You Are", "Crusin", "Tracks of my Tears", "Deck the Halls", "The Tracks Of My Tears", "When Smokey Sings Tears Of A Clown", "(It's The) Same Old Love", "You Take Me Away", "Blame It on Love", "Being With You", "Can't Fight Love", "Just My Soul Responding", "Love' n Life", "No\u00ebl", "I'm Glad There Is You", "Tracks of My Tears", "Just Another Kiss", "Ebony Eyes", "A Child Is Waiting", "Tracks Of My Tears (Live)", "Wishful Thinking", "The Tracks of My Heart", "Ooo Baby Baby (live)", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "Tell Me Tomorrow (12\\\" extended mix)", "I Second That Emotions", "The Track of My Tears", "Sleepless Nights", "Jasmin", "My Girl", "You Don't Know What It's Like", "With Your Love Came", "Be Kind To The Growing Mind (with The Temptations)", "The Tracks of My Tears (live)", "So Bad", "Love Don't Give No Reason", "Christmas Every Day", "Theme From the Big Time", "He Can Fix Anything", "Open", "You Are Forever", "Girl I'm Standing There", "Will You Love Me Tomorrow?", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "What's Too Much", "Be Careful What You Wish For (instrumental)", "It's Time to Stop Shoppin' Around", "If You Wanna Make Love (Come 'round Here)", "Time After Time", "Tell Me Tomorrow, Part 1", "Quiet Storm (Groove Boutique remix)", "You're the One for Me (feat. Joss Stone)", "I Praise & Worship You Father", "Please Don't Take Your Love (feat. Carlos Santana)", "Never My Love / Never Can Say Goodbye", "Driving Thru Life in the Fast Lane", "If You Wanna Make Love", "Wanna Know My Mind", "Shoe Soul", "I Hear The Children Singing", "Little Girl, Little Girl", "Christmas Greeting", "It's Fantastic", "The Tracks of My Tears", "Train of Thought", "I Like Your Face", "Yes It's You Lady", "Aqui Con Tigo (Being With You)", "The Way You Do (The Things You Do)", "It's A Good Night", "My World", "The Family Song", "It's Her Turn to Live", "Season's Greetings from Smokey Robinson", "I'm in the Mood for Love", "Why", "Gang Bangin'", "And I Don't Love You", "I've Made Love To You A Thousand Times", "Just Like You", "Mother's Son", "Some People Will Do Anything for Love", "Everything for Christmas", "You've Really Go a Hold on Me", "I Can\u2019t Stand to See You Cry (Commercial version)", "The Agony And The Ecstasy", "Ooh Baby Baby", "Be Careful What You Wish For", "Going to a Gogo", "Pops, We Love You", "Double Good Everything", "Hold on to Your Love", "Yester Love", "Why Are You Running From My Love", "Wedding Song", "Just Passing Through", "One Heartbeat", "Santa Claus is Coming to Town", "Skid Row", "Shop Around", "Pops, We Love You (disco)", "You've Really Got a Hold on Me", "Te Quiero Como Si No Hubiera Un Manana", "Unless You Do It Again", "Who's Sad", "Take Me Through The Night", "Don't Play Another Love Song", "We've Saved the Best for Last", "Winter Wonderland", "Our Love Is Here to Stay", "I Can't Get Enough", "When A Woman Cries"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-970", "prediction": ["# Reasoning Path:\nJulius Caesar -> people.deceased_person.place_of_death -> Theatre of Pompey -> common.topic.notable_for -> g.1256qj4g5\n# Answer:\nTheatre of Pompey", "# Reasoning Path:\nJulius Caesar -> people.deceased_person.place_of_death -> Theatre of Pompey -> common.topic.image -> Artist rendition of the front exterior of the Theatre of Pompey\n# Answer:\nTheatre of Pompey", "# Reasoning Path:\nJulius Caesar -> people.deceased_person.place_of_death -> Theatre of Pompey -> location.location.containedby -> Campus Martius\n# Answer:\nTheatre of Pompey", "# Reasoning Path:\nJulius Caesar -> common.topic.article -> m.02667zw\n# Answer:\nm.02667zw", "# Reasoning Path:\nJulius Caesar -> film.film.starring -> m.010gs_d2 -> film.performance.character -> Cleopatra\n# Answer:\nm.010gs_d2", "# Reasoning Path:\nJulius Caesar -> common.topic.article -> m.03y1_p\n# Answer:\nm.03y1_p", "# Reasoning Path:\nJulius Caesar -> common.topic.article -> m.03zb8qf\n# Answer:\nm.03zb8qf"], "ground_truth": ["Theatre of Pompey"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.6, "path_precision": 0.42857142857142855, "path_recall": 1.0, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-971", "prediction": ["# Reasoning Path:\nBessie Smith -> music.artist.genre -> Jazz -> book.book_subject.works -> Lush Life: A Biography of Billy Strayhorn\n# Answer:\nJazz", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Jazz -> book.book_subject.works -> 52nd Street\n# Answer:\nJazz", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Jazz -> broadcast.genre.content -> Classic Soul Network\n# Answer:\nJazz", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Blues -> music.genre.subgenre -> Americana\n# Answer:\nBlues", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Blues -> book.book_subject.works -> Jazz\n# Answer:\nBlues", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Jazz -> book.book_subject.works -> Celebrating Bird\n# Answer:\nJazz", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Jazz -> music.genre.subgenre -> Americana\n# Answer:\nJazz", "# Reasoning Path:\nBessie Smith -> book.written_work.subjects -> Jazz -> book.book_subject.works -> 52nd Street\n# Answer:\nJazz", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Jazz -> broadcast.genre.content -> SoulfulClassicJazz.com Radio\n# Answer:\nJazz", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Blues -> music.genre.subgenre -> Bluegrass\n# Answer:\nBlues"], "ground_truth": ["Jazz", "Blues"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.6, "path_recall": 0.5, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-972", "prediction": ["# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nm.06sbpz2", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nm.06sbq0l", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.season -> 1953 Major League Baseball season\n# Answer:\nm.06sbpz2", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nm.06sbpzt", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.season -> 1956 Major League Baseball Season\n# Answer:\nm.06sbq0l", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.season -> 1954 Major League Baseball season\n# Answer:\nm.06sbpzt", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h -> sports.sports_team_roster.team -> UCLA Bruins football\n# Answer:\nm.0hpgh_h", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0gggrzr -> sports.sports_team_roster.team -> Brooklyn Dodgers\n# Answer:\nm.0gggrzr", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgj2z -> sports.sports_team_roster.team -> Los Angeles Bulldogs\n# Answer:\nm.0hpgj2z", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0gggrzr -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nm.0gggrzr"], "ground_truth": ["UCLA Bruins football", "Los Angeles Bulldogs", "Montreal Royals", "Kansas City Monarchs", "Brooklyn Dodgers"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.48, "path_precision": 0.6, "path_recall": 0.4, "path_ans_f1": 0.6, "path_ans_precision": 0.6, "path_ans_recall": 0.6}
{"id": "WebQTest-973", "prediction": ["# Reasoning Path:\nChicago -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nChicago -> location.statistical_region.population -> g.11b66kxd4l\n# Answer:\ng.11b66kxd4l", "# Reasoning Path:\nChicago -> location.statistical_region.population -> g.11b7vbyfz4\n# Answer:\ng.11b7vbyfz4", "# Reasoning Path:\nChicago -> location.statistical_region.population -> g.11btt77639\n# Answer:\ng.11btt77639", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> Dziennik Zwi\u0105zkowy -> book.newspaper.circulation_areas -> United States of America\n# Answer:\nDziennik Zwi\u0105zkowy", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> Dziennik Zwi\u0105zkowy -> book.periodical.first_issue_date -> m.0h_vfsx\n# Answer:\nDziennik Zwi\u0105zkowy", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> Dziennik Zwi\u0105zkowy -> common.topic.notable_for -> g.125gmm_ll\n# Answer:\nDziennik Zwi\u0105zkowy", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> American Medical News -> common.topic.article -> m.03mf90t\n# Answer:\nAmerican Medical News", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> American Medical News -> common.topic.notable_for -> g.1254zz3kn\n# Answer:\nAmerican Medical News", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> Arbeiter-Zeitung -> common.topic.notable_types -> Newspaper\n# Answer:\nArbeiter-Zeitung"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-975", "prediction": ["# Reasoning Path:\nRonald Reagan -> government.us_president.vice_president -> George H. W. Bush -> base.activism.activist.area_of_activism -> Anti-communism\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nRonald Reagan -> government.us_president.vice_president -> George H. W. Bush -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nRonald Reagan -> government.us_president.vice_president -> George H. W. Bush -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nRonald Reagan -> architecture.building_occupant.buildings_occupied -> m.0dfzl8d -> architecture.occupancy.building -> White House\n# Answer:\nm.0dfzl8d", "# Reasoning Path:\nRonald Reagan -> government.us_president.vice_president -> George H. W. Bush -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nRonald Reagan -> people.person.quotations -> Abortion is advocated only by persons who have themselves been born. -> common.topic.notable_for -> g.1259jvdzj\n# Answer:\nAbortion is advocated only by persons who have themselves been born.", "# Reasoning Path:\nRonald Reagan -> people.person.quotations -> Abortion is advocated only by persons who have themselves been born. -> common.topic.notable_types -> Quotation\n# Answer:\nAbortion is advocated only by persons who have themselves been born.", "# Reasoning Path:\nRonald Reagan -> people.person.quotations -> Above all, we must realize that no arsenal, or no weapon in the arsenals of the world, is so formidable as the will and moral courage of free men and women. It is a weapon our adversaries in today's world do not have.  It is a weapon that we as Americans do have. Let that be understood by those who practice terrorism and prey upon their neighbours. -> media_common.quotation.source -> First Inaugural address of Ronald Reagan\n# Answer:\nAbove all, we must realize that no arsenal, or no weapon in the arsenals of the world, is so formidable as the will and moral courage of free men and women. It is a weapon our adversaries in today's world do not have.  It is a weapon that we as Americans do have. Let that be understood by those who practice terrorism and prey upon their neighbours.", "# Reasoning Path:\nRonald Reagan -> people.person.quotations -> Above all, we must realize that no arsenal, or no weapon in the arsenals of the world, is so formidable as the will and moral courage of free men and women. It is a weapon our adversaries in today's world do not have.  It is a weapon that we as Americans do have. Let that be understood by those who practice terrorism and prey upon their neighbours. -> common.topic.notable_types -> Quotation\n# Answer:\nAbove all, we must realize that no arsenal, or no weapon in the arsenals of the world, is so formidable as the will and moral courage of free men and women. It is a weapon our adversaries in today's world do not have.  It is a weapon that we as Americans do have. Let that be understood by those who practice terrorism and prey upon their neighbours.", "# Reasoning Path:\nRonald Reagan -> people.person.quotations -> Approximately 80 % of our air pollution stems from hydrocarbons released by vegetation, so let's not go overboard in setting and enforcing tough emission standards from man-made sources. -> media_common.quotation.subjects -> Pollution\n# Answer:\nApproximately 80 % of our air pollution stems from hydrocarbons released by vegetation, so let's not go overboard in setting and enforcing tough emission standards from man-made sources."], "ground_truth": ["George H. W. Bush"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-976", "prediction": ["# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> U.S. Cellular Field -> business.sponsored_recipient.sponsorship -> m.05_g4w7\n# Answer:\nU.S. Cellular Field", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.location -> Chicago -> location.hud_county_place.county -> Cook County\n# Answer:\nChicago", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> Comiskey Park -> location.location.events -> 1917 World Series\n# Answer:\nComiskey Park", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> Comiskey Park -> location.location.geolocation -> m.02_tzz7\n# Answer:\nComiskey Park", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> U.S. Cellular Field -> location.location.events -> 2003 Major League Baseball All-Star Game\n# Answer:\nU.S. Cellular Field", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.location -> Chicago -> location.location.partially_containedby -> Area code 312\n# Answer:\nChicago", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> Comiskey Park -> location.location.events -> 1918 World Series\n# Answer:\nComiskey Park", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.location -> Chicago -> base.biblioness.bibs_location.state -> Illinois\n# Answer:\nChicago", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> U.S. Cellular Field -> location.location.events -> 2005 World Series\n# Answer:\nU.S. Cellular Field", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> Comiskey Park -> location.location.events -> 1919 World Series\n# Answer:\nComiskey Park"], "ground_truth": ["U.S. Cellular Field", "Comiskey Park"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-977", "prediction": ["# Reasoning Path:\nJeff Conaway -> people.person.profession -> Actor -> people.profession.specializations -> Narrator\n# Answer:\nActor", "# Reasoning Path:\nGrease -> film.film.featured_song -> You're the One That I Want -> music.composition.recordings -> Grease: The Grease Mega-Mix\n# Answer:\nYou're the One That I Want", "# Reasoning Path:\nGrease -> film.film.starring -> m.0jy_bs -> film.performance.character -> Kenickie Murdoch\n# Answer:\nm.0jy_bs", "# Reasoning Path:\nGrease -> film.film.starring -> m.0y5cjv1 -> film.performance.character -> Girl Fixing Hair in Mirror at Drive-in\n# Answer:\nm.0y5cjv1", "# Reasoning Path:\nJeff Conaway -> people.person.profession -> Actor -> owl#inverseOf -> Film performances\n# Answer:\nActor", "# Reasoning Path:\nGrease -> film.film.featured_song -> You're the One That I Want -> music.composition.language -> English Language\n# Answer:\nYou're the One That I Want", "# Reasoning Path:\nJeff Conaway -> people.person.profession -> Actor -> freebase.equivalent_topic.equivalent_type -> Film actor\n# Answer:\nActor", "# Reasoning Path:\nGrease -> film.film.starring -> m.0jy_bs -> film.performance.actor -> Jeff Conaway\n# Answer:\nm.0jy_bs", "# Reasoning Path:\nGrease -> film.film.featured_song -> You're the One That I Want -> music.composition.recorded_as_album -> You're The One That I Want: Grease\n# Answer:\nYou're the One That I Want", "# Reasoning Path:\nJeff Conaway -> film.actor.film -> m.03lhv6t -> film.performance.film -> Elvira: Mistress of the Dark\n# Answer:\nm.03lhv6t"], "ground_truth": ["Kenickie Murdoch"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-978", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> Letters of Silence Dogood -> book.literary_series.works_in_this_series -> Silence Dogood, No. 1\n# Answer:\nLetters of Silence Dogood", "# Reasoning Path:\nBenjamin Franklin -> book.author.book_editions_published -> A Dissertation On Liberty, Necessity, Pleasure, And Pain (Notable American Authors) -> book.book_edition.book -> A Dissertation on Liberty and Necessity, Pleasure and Pain\n# Answer:\nA Dissertation On Liberty, Necessity, Pleasure, And Pain (Notable American Authors)", "# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> Letters of Silence Dogood -> book.literary_series.works_in_this_series -> Silence Dogood, No. 10\n# Answer:\nLetters of Silence Dogood", "# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> Letters of Silence Dogood -> book.literary_series.works_in_this_series -> Silence Dogood, No. 11\n# Answer:\nLetters of Silence Dogood", "# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> Letters of Silence Dogood -> common.topic.notable_types -> Literary Series\n# Answer:\nLetters of Silence Dogood", "# Reasoning Path:\nBenjamin Franklin -> book.author.book_editions_published -> A Dissertation On Liberty, Necessity, Pleasure, And Pain (Notable American Authors) -> common.topic.notable_for -> g.125fb3zvq\n# Answer:\nA Dissertation On Liberty, Necessity, Pleasure, And Pain (Notable American Authors)", "# Reasoning Path:\nBenjamin Franklin -> book.author.book_editions_published -> Autobiography of Benjamin Franklin 2e & Narrative of the Life of Frederick Douglass 2e -> book.book_edition.book -> The Autobiography of Benjamin Franklin\n# Answer:\nAutobiography of Benjamin Franklin 2e & Narrative of the Life of Frederick Douglass 2e", "# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> The Papers of Benjamin Franklin -> common.topic.image -> papers of ben franklin vol 1 cover\n# Answer:\nThe Papers of Benjamin Franklin", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nm.012zbkk5", "# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> The Papers of Benjamin Franklin -> book.literary_series.works_in_this_series -> The Papers of Benjamin Franklin, Vol. 28: Volume 28: November 1, 1778 through February 28, 1779\n# Answer:\nThe Papers of Benjamin Franklin"], "ground_truth": ["Some Fruits of Solitude in Reflections and Maxims", "Bite-size Ben Franklin", "The life of the late Doctor Benjamin Franklin", "The life of Doctor Benjamin Franklin", "Silence Dogood, No. 2", "\\\"The sayings of Poor Richard\\\"", "On war and peace", "Silence Dogood, No. 7", "Benjamin Franklin on balloons", "Memoirs of Benjamin Franklin", "Benjamin Franklin's own story", "The works of Dr. Benjamin Franklin, in philosophy, politics, and morals", "Address", "Autobiography", "A letter from B. Franklin to a young man", "The life of Dr. Benjamin Franklin", "The works of Dr. Benjn. Franklin;", "The life of the late Dr. Benjamin Franklin", "Poor Richard, 1733", "Observations Concerning the Increase of Mankind, Peopling of Countries, etc.", "The life of Benjamin Franklin, written chiefly by himself", "Compleated Autobiography by Benjamin Franklin", "Articles Of Belief And Acts Of Religion Vol.2", "Silence Dogood, No. 4", "Collected Works Of Benjamin Franklin", "Some account of the Pennsylvania Hospital", "Benjamin Franklin and Jonathan Edwards", "The complete works in philosophy, politics, and morals, of the late Dr. Benjamin Franklin, now first collected and arranged: with memoirs of his early life, written by himself", "Benjamin Franklin's Proposals for the education of youth in Pennsylvania, 1749", "The autobiography, with an introd", "The Works Of Benjamin Franklin V1", "Representative selections", "Early to bed, and early to rise, makes a man healthy, wealthy, and wise, or, Early rising, a natural, social, and religious duty", "Private correspondence of Benjamin Franklin", "Essays And Letters V1", "The writings of Benjamin Franklin", "Satires & Bagatelles", "The glory of eternity", "New experiments and observations on electricity", "B. Franklin, innovator", "The Life of Benjamin Franklin", "Reflection On Courtship And Marriage", "Sheep will never make insurrections", "A letter of advice to a young man concerning marriage", "The Papers of Benjamin Franklin, Vol. 28: Volume 28: November 1, 1778 through February 28, 1779", "The life and essays, of Dr. Franklin", "Founding Fathers Benjamin Franklin Volume 2", "Select works, including his autobiography", "Avis n\u00e9cessaire \u00e0 ceux qui veulent devenir riche", "Benjamin Franklin's Experiments", "The letters of Benjamin Franklin & Jane Mecom", "The autobiography of Benjamin Franklin, and a sketch of Franklin's life from the point where the autobiography ends, drawn chiefly from his letters", "Political, Miscellaneous And Philosophical Pieces", "The sayings of Benjamin Franklin", "The way to wealth, or, Poor Richard improved", "The life and letters", "Works of the late Doctor Benjamin Franklin", "Silence Dogood, No. 13", "The Drinker's Dictionary", "A Dissertation on Liberty and Necessity, Pleasure and Pain", "Poor Richard's Horse Keeper", "The Writings Of Benjamin Franklin, Vol. 1", "Benjamin Franklin's autobiographical writings", "Letters to the press, 1758-1775", "Silence Dogood, No. 9", "The select works of Benjamin Franklin", "Silence Dogood, No. 8", "Silence Dogood, No. 6", "Poor Richard's Almanack", "The essays, humorous, moral and literary of the late Dr. Benjamin Franklin", "Articles of belief", "Father Abraham's speech to a great number of people, at a vendue of merchant-goods", "The works of the late Dr. Benjamin Franklin", "Faceti\u00e6 Frankliana.   [sic]", "The autobiography of Benjamin Franklin; a restoration of a \\\"fair copy\\\"", "Not Your Usual Founding Father", "The art of making money plenty", "Silence Dogood, No. 3", "The Morals of Chess", "Silence Dogood, No. 12", "Silence Dogood, No. 10", "Poor Richard, or, The way to wealth", "Experiments and observations on electricity, made at Philadelphia in America", "Selected Works of Benjamin Franklin", "Apology for printers", "Memoirs of the life and writings of Benjamin Franklin", "Poor Richard day by day", "My Dear Girl Ii", "Franklin was there", "Free silver, and some other things", "The works of Dr. Benjamin Franklin", "Observations on the causes and cure of smoky chimneys", "Letters and papers of Benjamin Franklin and Richard Jackson, 1753-1785", "Silence Dogood, No. 14", "Franklin's wit & folly", "Conseils pour s'enrichir", "Authobiography Of Benjamin Franklin", "The life and essays of the late Doctor Benjamin Franklin", "The Papers of Benjamin Franklin, Volume 1: January 1, 1706 through December 31, 1734", "The Means and Manner of Obtaining Virtue", "Benjamin Franklin's the art of virtue", "Silence Dogood, No. 5", "The wisdom of Benjamin Franklin", "The autobiography of Benjamin Franklin and selections from his other writings", "What good is a newborn baby?", "The ingenious Dr. Franklin", "The works of Benjamin Franklin", "The Immortal Mentor", "The Way to Wealth", "Silence Dogood, No. 11", "Silence Dogood, No. 1", "A parable", "The bagatelles from Passy", "The autobiography and other writings", "A letter from Mr. Franklin to Mr. Peter Collinson, F.R.S. concerning the effect of lightning ; A letter of Benjamin Franklin, Esq. to Mr. Peter Collinson, F.R.S. concerning an electrical kite", "The autobiography, Poor Richard's almanac and other papers", "The political thought of Benjamin Franklin", "America's Big Ben", "Franklin's boyhood in Boston", "Autobiography, Poor Richard, and later writings", "How to Attain Moral Perfection", "The Autobiography of Benjamin Franklin", "Franklin's Way to wealth, or, \\\"Poor Richard improved\\\""], "ans_acc": 0.056910569105691054, "ans_hit": 1, "ans_f1": 0.027972027972027972, "ans_precission": 0.1, "ans_recall": 0.016260162601626018, "path_f1": 0.19672131147540983, "path_precision": 0.6, "path_recall": 0.11764705882352941, "path_ans_f1": 0.10396039603960396, "path_ans_precision": 0.6, "path_ans_recall": 0.056910569105691054}
{"id": "WebQTest-979", "prediction": ["# Reasoning Path:\nBarbados -> location.country.currency_used -> Barbadian dollar -> common.topic.notable_for -> g.125b468qw\n# Answer:\nBarbadian dollar", "# Reasoning Path:\nBarbados -> location.country.currency_used -> Barbadian dollar -> common.topic.article -> m.05hy7y\n# Answer:\nBarbadian dollar", "# Reasoning Path:\nBarbados -> location.statistical_region.official_development_assistance -> g.1hhc37pj3\n# Answer:\ng.1hhc37pj3", "# Reasoning Path:\nBarbados -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nBarbados -> location.country.official_language -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nBarbados -> location.statistical_region.official_development_assistance -> g.1hhc382cm\n# Answer:\ng.1hhc382cm", "# Reasoning Path:\nBarbados -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nBarbados -> location.country.official_language -> English Language -> language.human_language.main_country -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nBarbados -> location.statistical_region.official_development_assistance -> g.1hhc38m3z\n# Answer:\ng.1hhc38m3z", "# Reasoning Path:\nBarbados -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Bahamas\n# Answer:\nEnglish Language"], "ground_truth": ["Barbadian dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> base.americancomedy.impersonated_celebrity.impersonated_by -> Rich Little\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> Mortal Error: The Shot That Killed JFK\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of death\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nIrish American", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> common.topic.notable_types -> Event\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School -> common.topic.notable_for -> g.125583hnb\n# Answer:\nBloomington Kennedy High School", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> base.culturalevent.event.entity_involved -> Lee Harvey Oswald\n# Answer:\nAssassination of John F. Kennedy"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-980", "prediction": ["# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 2014 Stanley Cup Finals -> time.event.locations -> Staples Center\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1933 Stanley Cup Finals -> sports.sports_championship_event.champion -> New York Rangers\n# Answer:\n1933 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 2014 Stanley Cup Finals -> sports.sports_championship_event.champion -> Los Angeles Kings\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1954 Stanley Cup Finals -> sports.sports_championship_event.champion -> Detroit Red Wings\n# Answer:\n1954 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 2014 Stanley Cup Finals -> time.event.instance_of_recurring_event -> Stanley Cup Finals\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 2014 Stanley Cup Finals -> time.event.locations -> Madison Square Garden\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> time.recurring_event.current_frequency -> Yearly -> base.events.festival_event.part_of_series -> National Poetry Month\n# Answer:\nYearly", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1933 Stanley Cup Finals -> sports.sports_championship_event.season -> 1932\u201333 NHL season\n# Answer:\n1933 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> time.recurring_event.current_frequency -> Yearly -> common.topic.notable_types -> Unit of frequency\n# Answer:\nYearly", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1954 Stanley Cup Finals -> common.topic.article -> m.03cj1q6\n# Answer:\n1954 Stanley Cup Finals"], "ground_truth": ["Frederick Stanley, 16th Earl of Derby"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-982", "prediction": ["# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13201 -> location.postal_code.country -> United States of America\n# Answer:\n13201", "# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13201 -> common.topic.notable_types -> Postal Code\n# Answer:\n13201", "# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13202 -> common.topic.notable_types -> Postal Code\n# Answer:\n13202", "# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13201 -> common.topic.notable_for -> g.125h5bq4l\n# Answer:\n13201", "# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13203 -> location.postal_code.country -> United States of America\n# Answer:\n13203", "# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13202 -> location.location.geolocation -> m.03dv6m2\n# Answer:\n13202", "# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13202 -> location.location.containedby -> Onondaga County\n# Answer:\n13202", "# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13203 -> location.location.geolocation -> m.03dv6m8\n# Answer:\n13203", "# Reasoning Path:\nSyracuse -> location.place_with_neighborhoods.neighborhoods -> Armory Square -> location.location.containedby -> Syracuse, NY Metropolitan Statistical Area\n# Answer:\nArmory Square", "# Reasoning Path:\nSyracuse -> location.location.events -> 1983 NCAA Men's Division I Basketball Tournament -> base.marchmadness.ncaa_basketball_tournament.stages -> 1983 NCAA Men's Division I Basketball Tournament - East Regional Finals\n# Answer:\n1983 NCAA Men's Division I Basketball Tournament"], "ground_truth": ["13206", "13244-1020", "13215", "13252", "13224", "13205", "13201", "13220", "13208", "13261", "13209", "13211", "13221", "13251", "13207", "13219", "13250", "13218", "13212", "13235", "13217", "13210", "13202", "13204", "13203", "13244", "13214", "13225", "13290"], "ans_acc": 0.10344827586206896, "ans_hit": 1, "ans_f1": 0.183206106870229, "ans_precission": 0.8, "ans_recall": 0.10344827586206896, "path_f1": 0.183206106870229, "path_precision": 0.8, "path_recall": 0.10344827586206896, "path_ans_f1": 0.183206106870229, "path_ans_precision": 0.8, "path_ans_recall": 0.10344827586206896}
{"id": "WebQTest-983", "prediction": ["# Reasoning Path:\nTennessee Williams -> people.person.education -> m.02wpn3_ -> education.education.institution -> University of Missouri\n# Answer:\nm.02wpn3_", "# Reasoning Path:\nTennessee Williams -> people.person.education -> m.0n0mldl -> education.education.institution -> The New School\n# Answer:\nm.0n0mldl", "# Reasoning Path:\nTennessee Williams -> people.person.education -> m.0n1m266 -> education.education.institution -> Washington University in St. Louis\n# Answer:\nm.0n1m266", "# Reasoning Path:\nTennessee Williams -> people.person.quotations -> A high station in life is earned by the gallantry with which appalling experiences are survived with grace. -> common.topic.notable_for -> g.1258jbcmd\n# Answer:\nA high station in life is earned by the gallantry with which appalling experiences are survived with grace.", "# Reasoning Path:\nTennessee Williams -> people.person.quotations -> A high station in life is earned by the gallantry with which appalling experiences are survived with grace. -> media_common.quotation.subjects -> Elegance\n# Answer:\nA high station in life is earned by the gallantry with which appalling experiences are survived with grace.", "# Reasoning Path:\nTennessee Williams -> book.author.works_written -> 27 Wagons Full of Cotton -> common.topic.notable_types -> Play\n# Answer:\n27 Wagons Full of Cotton", "# Reasoning Path:\nTennessee Williams -> people.person.quotations -> All cruel people describe themselves as paragons of frankness. -> common.topic.notable_for -> g.1258jtb7f\n# Answer:\nAll cruel people describe themselves as paragons of frankness.", "# Reasoning Path:\nTennessee Williams -> book.author.works_written -> 27 Wagons Full of Cotton -> media_common.adapted_work.adaptations -> Baby Doll\n# Answer:\n27 Wagons Full of Cotton", "# Reasoning Path:\nTennessee Williams -> people.person.quotations -> All cruel people describe themselves as paragons of frankness. -> media_common.quotation.subjects -> Frankness\n# Answer:\nAll cruel people describe themselves as paragons of frankness.", "# Reasoning Path:\nTennessee Williams -> people.person.quotations -> All your Western theologies, the whole mythology of them, are based on the concept of God as a senile delinquent. -> common.topic.notable_for -> g.125fsz5lf\n# Answer:\nAll your Western theologies, the whole mythology of them, are based on the concept of God as a senile delinquent."], "ground_truth": ["University of Iowa", "University of Missouri", "Washington University in St. Louis", "The New School"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4285714285714285, "path_ans_precision": 0.3, "path_ans_recall": 0.75}
{"id": "WebQTest-984", "prediction": ["# Reasoning Path:\nNew York City -> location.location.nearby_airports -> LaGuardia Airport -> common.topic.notable_types -> Airport\n# Answer:\nLaGuardia Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> LaGuardia Airport -> aviation.airport.serves -> MBSF - Private Jets\n# Answer:\nLaGuardia Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> LaGuardia Airport -> travel.transport_terminus.travel_destinations_served -> m.0519ccq\n# Answer:\nLaGuardia Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> Flushing Airport -> common.topic.notable_types -> Airport\n# Answer:\nFlushing Airport", "# Reasoning Path:\nNew York City -> book.book_subject.works -> A Christmas Caroline -> book.written_work.subjects -> Christmas\n# Answer:\nA Christmas Caroline", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> aviation.airport.serves -> MBSF - Private Jets\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> location.location.containedby -> New York\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> travel.transport_terminus.travel_destinations_served -> m.0519ccq\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> travel.travel_destination.how_to_get_here -> m.0519cd5 -> travel.transportation.mode_of_transportation -> Bus\n# Answer:\nm.0519cd5", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> LaGuardia Airport -> aviation.airport.serves -> New York metropolitan area\n# Answer:\nLaGuardia Airport"], "ground_truth": ["John F. Kennedy International Airport"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-985", "prediction": ["# Reasoning Path:\nQueen Victoria -> people.person.children -> Edward VII -> people.person.spouse_s -> m.02kp9gy\n# Answer:\nEdward VII", "# Reasoning Path:\nQueen Victoria -> people.person.children -> Alfred, Duke of Saxe-Coburg and Gotha -> people.person.children -> Alfred, Hereditary Prince of Saxe-Coburg and Gotha\n# Answer:\nAlfred, Duke of Saxe-Coburg and Gotha", "# Reasoning Path:\nQueen Victoria -> people.person.spouse_s -> m.02kp9lp -> people.marriage.spouse -> Albert, Prince Consort\n# Answer:\nm.02kp9lp", "# Reasoning Path:\nQueen Victoria -> people.person.children -> Alfred, Duke of Saxe-Coburg and Gotha -> people.person.spouse_s -> m.0j4k6hp\n# Answer:\nAlfred, Duke of Saxe-Coburg and Gotha", "# Reasoning Path:\nQueen Victoria -> people.person.children -> Alfred, Duke of Saxe-Coburg and Gotha -> people.family_member.family -> House of Saxe-Coburg and Gotha\n# Answer:\nAlfred, Duke of Saxe-Coburg and Gotha", "# Reasoning Path:\nQueen Victoria -> people.person.children -> Prince Arthur, Duke of Connaught and Strathearn -> people.person.children -> Prince Arthur of Connaught\n# Answer:\nPrince Arthur, Duke of Connaught and Strathearn", "# Reasoning Path:\nQueen Victoria -> people.person.children -> Prince Arthur, Duke of Connaught and Strathearn -> people.person.spouse_s -> m.0j4k694\n# Answer:\nPrince Arthur, Duke of Connaught and Strathearn", "# Reasoning Path:\nQueen Victoria -> people.person.spouse_s -> m.02kp9lp -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.02kp9lp", "# Reasoning Path:\nQueen Victoria -> people.person.children -> Alfred, Duke of Saxe-Coburg and Gotha -> people.person.children -> Marie of Romania\n# Answer:\nAlfred, Duke of Saxe-Coburg and Gotha", "# Reasoning Path:\nQueen Victoria -> people.person.children -> Edward VII -> people.person.children -> George V\n# Answer:\nEdward VII"], "ground_truth": ["Albert, Prince Consort"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.09523809523809525, "path_precision": 0.1, "path_recall": 0.09090909090909091, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-986", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.parents -> Jane White Franklin\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Josiah Franklin Jr.\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.place_of_birth -> Ecton\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Anne Harris\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger -> people.person.parents -> Mary Morrill\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger -> people.deceased_person.place_of_burial -> Granary Burying Ground\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Ebenezer Franklin\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.profession -> Diplomat -> base.descriptive_names.names.descriptive_name -> m.0105_09r\n# Answer:\nDiplomat", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger -> people.person.parents -> Peter Folger\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> people.person.profession -> Diplomat -> common.topic.notable_for -> g.125c5ykvs\n# Answer:\nDiplomat"], "ground_truth": ["United States Postal Service", "Josiah Franklin", "James Franklin Printing Shop"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.5, "ans_recall": 0.3333333333333333, "path_f1": 0.4, "path_precision": 0.5, "path_recall": 0.3333333333333333, "path_ans_f1": 0.4, "path_ans_precision": 0.5, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-987", "prediction": ["# Reasoning Path:\nBarbara Bush -> people.person.profession -> Homemaking -> fictional_universe.character_occupation.characters_with_this_occupation -> Betty Draper\n# Answer:\nHomemaking", "# Reasoning Path:\nBarbara Bush -> people.person.profession -> Homemaking -> book.magazine_genre.magazines_in_this_genre -> Better Homes and Gardens\n# Answer:\nHomemaking", "# Reasoning Path:\nBarbara Bush -> people.person.profession -> Homemaking -> base.ontologies.ontology_instance.equivalent_instances -> m.07ngjqb\n# Answer:\nHomemaking", "# Reasoning Path:\nBarbara Bush -> people.person.profession -> Homemaking -> fictional_universe.character_occupation.characters_with_this_occupation -> Carmela Soprano\n# Answer:\nHomemaking", "# Reasoning Path:\nBarbara Bush -> people.person.profession -> Homemaking -> fictional_universe.character_occupation.characters_with_this_occupation -> Celia Hodes\n# Answer:\nHomemaking", "# Reasoning Path:\nBarbara Bush -> government.politician.party -> m.0lr18t_ -> government.political_party_tenure.party -> Republican Party\n# Answer:\nm.0lr18t_", "# Reasoning Path:\nBarbara Bush -> base.famouspets.pet_owner.pets_owned -> m.05hnbrv -> base.famouspets.pet_ownership.pet -> Millie\n# Answer:\nm.05hnbrv", "# Reasoning Path:\nBarbara Bush -> base.famouspets.pet_owner.pets_owned -> m.05hnbrv -> base.famouspets.pet_ownership.owner -> George H. W. Bush\n# Answer:\nm.05hnbrv"], "ground_truth": ["First Lady"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-988", "prediction": ["# Reasoning Path:\nJacob Latimore -> people.person.profession -> Dancer -> people.profession.specializations -> Ballet Dancer\n# Answer:\nDancer", "# Reasoning Path:\nJacob Latimore -> people.person.profession -> Dancer -> common.topic.notable_for -> g.125h67_9m\n# Answer:\nDancer", "# Reasoning Path:\nJacob Latimore -> people.person.profession -> Dancer -> people.profession.specializations -> Ballroom Dancer\n# Answer:\nDancer", "# Reasoning Path:\nJacob Latimore -> people.person.profession -> Dancer -> base.descriptive_names.names.descriptive_name -> m.0102lxg3\n# Answer:\nDancer", "# Reasoning Path:\nJacob Latimore -> people.person.profession -> Singer -> base.lightweight.profession.specialization_of -> Musicians and Singers\n# Answer:\nSinger", "# Reasoning Path:\nJacob Latimore -> people.person.profession -> Dancer -> people.profession.specializations -> Belly Dancer\n# Answer:\nDancer", "# Reasoning Path:\nJacob Latimore -> people.person.profession -> Singer -> common.topic.subject_of -> Alan Motley\n# Answer:\nSinger", "# Reasoning Path:\nJacob Latimore -> people.person.profession -> Actor -> people.profession.specializations -> Voice Actor\n# Answer:\nActor", "# Reasoning Path:\nJacob Latimore -> people.person.parents -> Latitia Taylor -> common.topic.notable_for -> g.126tksywd\n# Answer:\nLatitia Taylor", "# Reasoning Path:\nJacob Latimore -> people.person.profession -> Dancer -> base.descriptive_names.names.descriptive_name -> m.0102lxkn\n# Answer:\nDancer"], "ground_truth": ["Singer", "Dancer", "Actor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-989", "prediction": ["# Reasoning Path:\nDunkirk -> common.topic.article -> m.0md6r\n# Answer:\nm.0md6r", "# Reasoning Path:\nDunkirk -> location.location.events -> Battle of Dunkirk -> base.culturalevent.event.entity_involved -> Maxime Weygand\n# Answer:\nBattle of Dunkirk", "# Reasoning Path:\nDunkirk -> location.location.events -> Battle of Dunkirk -> common.topic.notable_types -> Military Conflict\n# Answer:\nBattle of Dunkirk", "# Reasoning Path:\nDunkirk -> location.location.people_born_here -> Alex Dupont -> common.topic.notable_types -> Football player\n# Answer:\nAlex Dupont", "# Reasoning Path:\nDunkirk -> location.location.events -> Battle of Dunkirk -> base.culturalevent.event.entity_involved -> French Third Republic\n# Answer:\nBattle of Dunkirk", "# Reasoning Path:\nDunkirk -> location.location.events -> Battle of Dunkirk -> military.military_conflict.casualties -> m.04lkt8p\n# Answer:\nBattle of Dunkirk", "# Reasoning Path:\nDunkirk -> location.location.events -> Action of 18 February 1639 -> base.culturalevent.event.entity_involved -> Maarten Tromp\n# Answer:\nAction of 18 February 1639", "# Reasoning Path:\nDunkirk -> location.location.events -> Battle of Dunkirk -> base.culturalevent.event.entity_involved -> Gerd von Rundstedt\n# Answer:\nBattle of Dunkirk", "# Reasoning Path:\nDunkirk -> location.location.events -> Battle of the Dunes -> military.military_conflict.military_personnel_involved -> Bernard de Gomme\n# Answer:\nBattle of the Dunes", "# Reasoning Path:\nDunkirk -> location.location.people_born_here -> Armand Charles Guilleminot -> common.topic.notable_for -> g.1255h8kn8\n# Answer:\nArmand Charles Guilleminot"], "ground_truth": ["Siege of Dunkirk", "Battle of Dunkirk"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> South Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> common.topic.notable_types -> Human Language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Bosnia and Herzegovina\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Croatia\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> sports.multi_event_tournament.participating_countries -> United States of America\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> common.topic.article -> m.0l8p56r\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Torlakian dialect -> common.topic.image -> Torlak\n# Answer:\nTorlakian dialect"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-990", "prediction": ["# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.position_s -> Small forward -> sports.sports_position.sport -> Basketball\n# Answer:\nSmall forward", "# Reasoning Path:\nScottie Pippen -> sports.pro_athlete.sports_played_professionally -> m.0d5jh3n -> sports.pro_sports_played.sport -> Basketball\n# Answer:\nm.0d5jh3n", "# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.position_s -> Small forward -> common.topic.notable_types -> Basketball Position\n# Answer:\nSmall forward", "# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.position_s -> Small forward -> common.topic.image -> Basketball half-court\n# Answer:\nSmall forward", "# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.position_s -> Small forward -> common.topic.image -> John Salmons\n# Answer:\nSmall forward", "# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.player_statistics -> m.04q9rcg -> basketball.basketball_player_stats.season -> 2003\u201304 NBA season\n# Answer:\nm.04q9rcg", "# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.player_statistics -> m.04qb_fd -> basketball.basketball_player_stats.team -> Chicago Bulls\n# Answer:\nm.04qb_fd", "# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.player_statistics -> m.04q9rcg -> basketball.basketball_player_stats.team -> Chicago Bulls\n# Answer:\nm.04q9rcg", "# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.player_statistics -> m.04qg18x -> basketball.basketball_player_stats.team -> Houston Rockets\n# Answer:\nm.04qg18x", "# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.player_statistics -> m.04qb_fd -> basketball.basketball_player_stats.season -> 1988\u201389 NBA season\n# Answer:\nm.04qb_fd"], "ground_truth": ["Basketball"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-991", "prediction": ["# Reasoning Path:\nDonnatella Moss -> tv.tv_character.appeared_in_tv_program -> m.02kvknt -> tv.regular_tv_appearance.actor -> Janel Moloney\n# Answer:\nm.02kvknt", "# Reasoning Path:\nDonnatella Moss -> tv.tv_character.appeared_in_tv_program -> m.02kvknt -> tv.regular_tv_appearance.seasons -> The West Wing Season 6\n# Answer:\nm.02kvknt", "# Reasoning Path:\nDonnatella Moss -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The West Wing Universe -> fictional_universe.fictional_universe.characters -> Abbey Bartlet\n# Answer:\nThe West Wing Universe", "# Reasoning Path:\nDonnatella Moss -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The West Wing Universe -> fictional_universe.fictional_universe.works_set_here -> The West Wing\n# Answer:\nThe West Wing Universe", "# Reasoning Path:\nDonnatella Moss -> tv.tv_character.appeared_in_tv_program -> m.02kvknt -> tv.regular_tv_appearance.seasons -> The West Wing Season 7\n# Answer:\nm.02kvknt", "# Reasoning Path:\nDonnatella Moss -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The West Wing Universe -> fictional_universe.fictional_universe.languages -> American English\n# Answer:\nThe West Wing Universe", "# Reasoning Path:\nDonnatella Moss -> fictional_universe.fictional_character.ethnicity -> Irish American -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Archbishop Gilday\n# Answer:\nIrish American", "# Reasoning Path:\nDonnatella Moss -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The West Wing Universe -> fictional_universe.fictional_universe.characters -> Abdul ibn Shareef\n# Answer:\nThe West Wing Universe", "# Reasoning Path:\nDonnatella Moss -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The West Wing Universe -> fictional_universe.fictional_universe.characters -> Ainsley Hayes\n# Answer:\nThe West Wing Universe", "# Reasoning Path:\nDonnatella Moss -> fictional_universe.fictional_character.ethnicity -> Irish American -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Bridget\n# Answer:\nIrish American"], "ground_truth": ["Janel Moloney"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-993", "prediction": ["# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.0k6pxpv -> tv.regular_tv_appearance.actor -> Lacey Chabert\n# Answer:\nm.0k6pxpv", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.04d4q86 -> tv.regular_tv_appearance.actor -> Lacey Chabert\n# Answer:\nm.04d4q86", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.0k6pxpv -> tv.regular_tv_appearance.series -> Family Guy\n# Answer:\nm.0k6pxpv", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntq_4 -> tv.regular_tv_appearance.actor -> Mila Kunis\n# Answer:\nm.02ntq_4", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.04d4q86 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nm.04d4q86", "# Reasoning Path:\nMeg Griffin -> cvg.game_character.games -> m.0ypsv3n -> cvg.game_performance.voice_actor -> Mila Kunis\n# Answer:\nm.0ypsv3n", "# Reasoning Path:\nMeg Griffin -> cvg.game_character.games -> m.0ypsv3n -> cvg.game_performance.performance_type -> Voice\n# Answer:\nm.0ypsv3n", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntq_4 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.02ntq_4", "# Reasoning Path:\nMeg Griffin -> film.film_character.portrayed_in_films -> m.0h10hz6 -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0h10hz6", "# Reasoning Path:\nMeg Griffin -> film.film_character.portrayed_in_films -> m.010x342y -> film.performance.actor -> Mila Kunis\n# Answer:\nm.010x342y"], "ground_truth": ["Lacey Chabert"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-994", "prediction": ["# Reasoning Path:\nDallas Cowboys -> sports.sports_team.championships -> Super Bowl XXX -> time.event.instance_of_recurring_event -> Super Bowl\n# Answer:\nSuper Bowl XXX", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.championships -> Super Bowl XXX -> common.topic.notable_types -> Super bowl\n# Answer:\nSuper Bowl XXX", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.championships -> Super Bowl XII -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XII", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.championships -> Super Bowl VI -> time.event.instance_of_recurring_event -> Super Bowl\n# Answer:\nSuper Bowl VI", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.season_record -> m.075fk0q -> sports.sports_team_season_record.season -> 1978 NFL season\n# Answer:\nm.075fk0q", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.season_record -> m.075fj_6 -> sports.sports_team_season_record.season -> 1984 NFL season\n# Answer:\nm.075fj_6", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.championships -> Super Bowl VI -> common.topic.notable_for -> g.125b2cvcd\n# Answer:\nSuper Bowl VI", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.season_record -> m.075fj_g -> sports.sports_team_season_record.season -> 1983 NFL season\n# Answer:\nm.075fj_g", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.championships -> Super Bowl XII -> time.event.locations -> Mercedes-Benz Superdome\n# Answer:\nSuper Bowl XII", "# Reasoning Path:\nDallas Cowboys -> american_football.football_team.away_games -> Dallas Cowboys at Philadelphia Eagles, 2008-12-28 -> american_football.football_game.rushing -> m.0793cnj\n# Answer:\nDallas Cowboys at Philadelphia Eagles, 2008-12-28"], "ground_truth": ["Super Bowl XXX"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-996", "prediction": ["# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> location.location.containedby -> Lansdowne Road\n# Answer:\nAviva Stadium", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> location.location.containedby -> Ballsbridge\n# Answer:\nAviva Stadium", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> sports.sports_facility.teams -> League of Ireland XI\n# Answer:\nAviva Stadium", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> location.location.containedby -> Dublin\n# Answer:\nAviva Stadium", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> sports.sports_facility.teams -> Leinster Rugby\n# Answer:\nAviva Stadium", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> soccer.football_pitch.matches -> 2010 FAI Cup Final\n# Answer:\nAviva Stadium", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.fight_song -> Ireland's Call -> common.topic.image -> The lyrics of Ireland's Call displayed on big screen at Croke Park, Dublin, February 11th, 2007\n# Answer:\nIreland's Call", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> sports.sports_facility.teams -> Republic of Ireland national football team\n# Answer:\nAviva Stadium", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.venue -> m.0wz2kl3 -> sports.team_venue_relationship.venue -> Aviva Stadium\n# Answer:\nm.0wz2kl3", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> soccer.football_pitch.matches -> 2011 FAI Cup Final\n# Answer:\nAviva Stadium"], "ground_truth": ["Aviva Stadium"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-997", "prediction": ["# Reasoning Path:\nLee Harvey Oswald -> base.kwebbase.kwtopic.connections_to -> john fitzgerald kennedy allegedly assassinated by lee harvey oswald -> base.kwebbase.kwconnection.subject -> John F. Kennedy\n# Answer:\njohn fitzgerald kennedy allegedly assassinated by lee harvey oswald", "# Reasoning Path:\nLee Harvey Oswald -> base.kwebbase.kwtopic.connections_from -> lee harvey oswald allegedly assassinated john fitzgerald kennedy -> base.kwebbase.kwconnection.other -> John F. Kennedy\n# Answer:\nlee harvey oswald allegedly assassinated john fitzgerald kennedy", "# Reasoning Path:\nLee Harvey Oswald -> base.kwebbase.kwtopic.connections_to -> fidel castro self-proclaimed follower was lee harvey oswald -> base.kwebbase.kwconnection.subject -> Fidel Castro\n# Answer:\nfidel castro self-proclaimed follower was lee harvey oswald", "# Reasoning Path:\nLee Harvey Oswald -> people.deceased_person.cause_of_death -> Ballistic trauma -> base.pethealth.cause.pet_diseases_or_conditions_caused -> Head Trauma in Animals\n# Answer:\nBallistic trauma", "# Reasoning Path:\nLee Harvey Oswald -> base.kwebbase.kwtopic.connections_to -> john fitzgerald kennedy allegedly assassinated by lee harvey oswald -> base.kwebbase.kwconnection.relation -> allegedly assassinated by\n# Answer:\njohn fitzgerald kennedy allegedly assassinated by lee harvey oswald", "# Reasoning Path:\nLee Harvey Oswald -> base.kwebbase.kwtopic.connections_to -> karl marx influenced lee harvey oswald -> base.kwebbase.kwconnection.subject -> Karl Marx\n# Answer:\nkarl marx influenced lee harvey oswald", "# Reasoning Path:\nLee Harvey Oswald -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.article -> m.02qnd1f\n# Answer:\nBallistic trauma", "# Reasoning Path:\nLee Harvey Oswald -> people.deceased_person.cause_of_death -> Bleeding -> people.cause_of_death.people -> Anthony Minghella\n# Answer:\nBleeding", "# Reasoning Path:\nLee Harvey Oswald -> base.kwebbase.kwtopic.connections_from -> lee harvey oswald allegedly assassinated john fitzgerald kennedy -> base.kwebbase.kwconnection.sentence -> He was also seen on the first floor of the depository about 10 minutes before President Kennedy was shot as he drove in an open car at 12.30 pm.\n# Answer:\nlee harvey oswald allegedly assassinated john fitzgerald kennedy", "# Reasoning Path:\nLee Harvey Oswald -> base.kwebbase.kwtopic.connections_to -> fidel castro self-proclaimed follower was lee harvey oswald -> base.kwebbase.kwconnection.sentence -> Knew ^knew^ ^Hemingway^ Oswald claimed to be a follower.\n# Answer:\nfidel castro self-proclaimed follower was lee harvey oswald"], "ground_truth": ["Jack Ruby"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-998", "prediction": ["# Reasoning Path:\nNew York Knicks -> sports.sports_team.championships -> 1973 NBA Finals -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1973 NBA Finals", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.championships -> 1970 NBA Finals -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1970 NBA Finals", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.championships -> 1973 NBA Finals -> sports.sports_championship_event.runner_up -> Los Angeles Lakers\n# Answer:\n1973 NBA Finals", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.championships -> 1970 NBA Finals -> common.topic.article -> m.0cs1x9\n# Answer:\n1970 NBA Finals", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.championships -> 1970 NBA Finals -> common.topic.notable_for -> g.1255ddd2t\n# Answer:\n1970 NBA Finals", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.coaches -> m.010r7q18 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.010r7q18", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.venue -> m.0wz1zjx -> sports.team_venue_relationship.venue -> Madison Square Garden\n# Answer:\nm.0wz1zjx", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.coaches -> m.0112zxyh -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0112zxyh", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.venue -> m.0wz1zjx -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.0wz1zjx", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.coaches -> m.010r7q18 -> sports.sports_team_coach_tenure.coach -> Derek Fisher\n# Answer:\nm.010r7q18"], "ground_truth": ["1973 NBA Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-999", "prediction": ["# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1961 NBA Finals -> sports.sports_championship_event.season -> 1960\u201361 NBA season\n# Answer:\n1961 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1961 NBA Finals -> common.topic.notable_for -> g.125fg41zl\n# Answer:\n1961 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1957 NBA Finals -> common.topic.notable_for -> g.125906rkn\n# Answer:\n1957 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1961 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n1961 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1959 NBA Finals -> common.topic.article -> m.0cs2j1\n# Answer:\n1959 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1957 NBA Finals -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1957 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1957 NBA Finals -> common.topic.article -> m.0cs2n5\n# Answer:\n1957 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.professional_sports_team.draft_picks -> m.0110p_t3 -> sports.sports_league_draft_pick.draft -> 2014 NBA draft\n# Answer:\nm.0110p_t3", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1959 NBA Finals -> common.topic.notable_for -> g.125fgwwq2\n# Answer:\n1959 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.roster -> m.0110pz2r -> sports.sports_team_roster.player -> Marcus Smart\n# Answer:\nm.0110pz2r"], "ground_truth": ["1964 NBA Finals", "1957 NBA Finals", "1965 NBA Finals", "1960 NBA Finals", "1969 NBA Finals", "1984 NBA Finals", "2008 NBA Finals", "1963 NBA Finals", "1968 NBA Finals", "1959 NBA Finals", "1966 NBA Finals", "1986 NBA Finals", "1962 NBA Finals", "1961 NBA Finals", "1976 NBA Finals", "1981 NBA Finals", "1974 NBA Finals"], "ans_acc": 0.17647058823529413, "ans_hit": 1, "ans_f1": 0.28915662650602414, "ans_precission": 0.8, "ans_recall": 0.17647058823529413, "path_f1": 0.28915662650602414, "path_precision": 0.8, "path_recall": 0.17647058823529413, "path_ans_f1": 0.28915662650602414, "path_ans_precision": 0.8, "path_ans_recall": 0.17647058823529413}
