{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Belize\n# Answer:\nBelize", "# Reasoning Path:\nJamaica -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Bahamas\n# Answer:\nBahamas", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language\n# Answer:\nJamaian Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English\n# Answer:\nEnglish Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language\n# Answer:\nJamaican CreoleEnglish Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language\n# Answer:\nJamaicanCreole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language\n# Answer:\n Jamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English\n# Answer:\nEnglish Language\n# Jamaican English\nJamaican English"], "ground_truth": ["Jamaican English", "Jamaican Creole English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nUnited States Representative", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> United States Representative\n# Answer:\nUnited States Representative", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Representative\n# Answer:\nUnited States Representative", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kc -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nUnited States Representative", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.0944j8_ -> government.government_position_held.office_position_or_title -> Speaker of the United States House of Representatives\n# Answer:\nSpeaker of the United States House of Representatives", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_bodies -> United States House of Representatives\n# Answer:\nUnited States House of Representatives", "# Reasoning Path:\nJames K. Polk -> common.topic.notable_types -> US President -> freebase.type_profile.equivalent_topic -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nJames K. Polk -> common.topic.notable_types -> US President -> type.type.expected_by -> President\n# Answer:\nPresident", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Lawyer\n# Answer:\nLawyer", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kc -> government.government_position_held.governmental_body -> United States House of Representatives\n# Answer:\nUnited States House of Representatives"], "ground_truth": ["Governor of Tennessee", "Speaker of the United States House of Representatives", "United States Representative"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.5, "ans_recall": 0.6666666666666666, "path_f1": 0.625, "path_precision": 0.5, "path_recall": 0.8333333333333334, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.5, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> French\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.currency_used -> Haitian gourde\n# Answer:\nHaitian gourde", "# Reasoning Path:\nHaiti -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Haiti\n# Answer:\nHaiti", "# Reasoning Path:\nHaiti -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nFrance", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole -> language.human_language.countries_spoken_in -> Haiti\n# Answer:\nHaiti", "# Reasoning Path:\nHaiti -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Haiti\n# Answer:\nHaiti", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole\n# Answer:\nH Haitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole\n# Answer:\nHaiti Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole\n# Answer:\nHaitien Creole"], "ground_truth": ["Haitian Creole", "French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> DC Universe -> comic_books.comic_book_fictional_universe.characters_primarily_appearing_in_this_universe -> The Warlord\n# Answer:\nThe Warlord", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> DC Universe -> comic_books.comic_book_fictional_universe.characters_primarily_appearing_in_this_universe -> Captain Cold\n# Answer:\nCaptain Cold", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> DC Universe -> comic_books.comic_book_fictional_universe.characters_primarily_appearing_in_this_universe -> Superman\n# Answer:\nSuperman", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> DC Universe -> fictional_universe.fictional_universe.works_set_here -> The Sandman: Dream Country\n# Answer:\nThe Sandman: Dream Country", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> DC Universe -> fictional_universe.fictional_universe.works_set_here -> Batman: Year One\n# Answer:\nBatman: Year One", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> DC Universe -> comic_books.comic_book_fictional_universe.characters_primarily_appearing_in_this_universe -> Mr. Freeze\n# Answer:\nMr. Freeze", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> DC Universe -> comic_books.comic_book_fictional_universe.characters_primarily_appearing_in_this_universe -> Green Arrow\n# Answer:\nGreen Arrow", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> DC Universe -> comic_books.comic_book_fictional_universe.characters_primarily_appearing_in_this_universe -> Captain Cold\n# Answer:\nCaptainCold", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> DC Universe -> comic_books.comic_book_fictional_universe.characters_primarily_appearing_in_this_universe -> Captain Cold\n# Answer:\n Captain Cold", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> DC Universe -> comic_books.comic_book_fictional_universe.characters_primarily_appearing_in_this_universe -> Mister Toad\n# Answer:\nMister Toad"], "ground_truth": ["Ilyssa Fradin", "Melinda McGraw", "Hannah Gunn"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qd4y1 -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qstvs -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> sports.drafted_athlete.drafted -> m.04fw77r -> sports.sports_league_draft_pick.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbchh -> sports.sports_award.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qcyjt -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qck_w -> basketball.basketball_player_stats.team -> Miami Heat\n# Answer:\nMiami Heat", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qpksh -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc58 -> sports.sports_award.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjnfl -> basketball.basketball_player_stats.team -> Miami Heat\n# Answer:\nMiami Heat"], "ground_truth": ["Miami Heat", "Phoenix Suns", "Orlando Magic", "Boston Celtics", "LSU Tigers men's basketball", "Cleveland Cavaliers", "Los Angeles Lakers"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 1.0, "ans_recall": 0.42857142857142855, "path_f1": 0.5, "path_precision": 1.0, "path_recall": 0.3333333333333333, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.nationality -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> New York City\n# Answer:\nNew York City", "# Reasoning Path:\nJay Leno -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Chicago\n# Answer:\nChicago", "# Reasoning Path:\nJay Leno -> influence.influence_node.influenced_by -> Jack Paar\n# Answer:\nJack Paar", "# Reasoning Path:\nJay Leno -> influence.influence_node.influenced_by -> George Carlin\n# Answer:\nGeorge Carlin", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> influence.influence_node.influenced_by -> George Carlin -> people.person.place_of_birth -> Manhattan\n# Answer:\nManhattan", "# Reasoning Path:\nJay Leno -> influence.influence_node.influenced_by -> George Carlin -> people.person.nationality -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJay Leno -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Manhattan\n# Answer:\nManhattan"], "ground_truth": ["New Rochelle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> location.country.languages_spoken -> Malay, Pattani Language\n# Answer:\nMalay, Pattani Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Phu Thai language\n# Answer:\nPhu Thai language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Mlabri Language\n# Answer:\nMlabri Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Lao Language\n# Answer:\nLao Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Cham language\n# Answer:\nCham language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language\n# Answer:\nVietnamese Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Khmer language\n# Answer:\nKhmer language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Hmong language\n# Answer:\nHmong language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Nyaw Language\n# Answer:\nNyaw Language"], "ground_truth": ["Phu Thai language", "Mlabri Language", "Vietnamese Language", "Thai Language", "Akha Language", "Nyaw Language", "Cham language", "Mon Language", "Saek language", "Khmer language", "Hmong language", "Malay, Pattani Language", "Lao Language"], "ans_acc": 0.8461538461538461, "ans_hit": 1, "ans_f1": 0.9166666666666666, "ans_precission": 1.0, "ans_recall": 0.8461538461538461, "path_f1": 0.8695652173913044, "path_precision": 1.0, "path_recall": 0.7692307692307693, "path_ans_f1": 0.9166666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.8461538461538461}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Programmer\n# Answer:\nProgrammer", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Entrepreneur\n# Answer:\nEntrepreneur", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> internet.website_owner.websites_owned -> Facebook\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Businessperson\n# Answer:\nBusinessperson", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.owner -> Mark Zuckerberg\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.directed_by -> David Fincher -> film.director.film -> The Curious Case of Benjamin Button\n# Answer:\nThe Curious Case of Benjamin Button", "# Reasoning Path:\nThe Social Network -> film.film.country -> United States of America -> location.country.administrative_divisions -> New York City\n# Answer:\nNew York City", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> film.film_subject.films -> The Social Network\n# Answer:\nThe Social Network", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Programmer\n# Answer:\nprogrammer"], "ground_truth": ["Tyler Winklevoss", "Cameron Winklevoss"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Architect\n# Answer:\nArchitect", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Lawyer\n# Answer:\nLawyer", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Statesman\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Farmer\n# Answer:\nFarmer", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Inventor\n# Answer:\nInventor", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Author\n# Answer:\nAuthor", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Teacher\n# Answer:\nTeacher"], "ground_truth": ["Author", "Architect", "Inventor", "Statesman", "Teacher", "Philosopher", "Farmer", "Writer", "Archaeologist", "Lawyer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin, Volume 1: 1821-1836", "# Reasoning Path:\nCharles Darwin -> people.person.quotations -> There is grandeur in this view of life, with its several powers, having been originally breathed into a few forms or into one; and that, whilst this planet has gone cycling on according to the fixed law of gravity, from so simple a beginning endless forms most beautiful and most wonderful have been, and are being, evolved. -> media_common.quotation.source -> On the origin of species by means of natural selection\n# Answer:\nOn the origin of species by means of natural selection", "# Reasoning Path:\nCharles Darwin -> base.concepts.concept_developer.concepts_developed -> Natural selection -> book.book_subject.works -> On the origin of species by means of natural selection\n# Answer:\nOn the origin of species by means of natural selection", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 6: 1856-1857\n# Answer:\nThe Correspondence of Charles Darwin, Volume 6: 1856-1857", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 3: 1844-1846\n# Answer:\nThe Correspondence of Charles Darwin, Volume 3: 1844-1846", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 12: 1864\n# Answer:\nThe Correspondence of Charles Darwin, Volume 12: 1864", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 4: 1847-1850\n# Answer:\nThe Correspondence of Charles Darwin, Volume 4: 1847-1850", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 2: 1837-1843\n# Answer:\nThe Correspondence of Charles Darwin, Volume 2: 1837-1843", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin -> book.author.works_written -> The Autobiography of Charles Darwin\n# Answer:\nThe Autobiography of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nOn the origin of species by means of natural selection"], "ground_truth": ["The Voyage of the Beagle (Mentor)", "Questions about the breeding of animals", "Opsht\u0323amung fun menshen", "Origin of Species (Everyman's University Paperbacks)", "El Origin De Las Especies", "The Correspondence of Charles Darwin, Volume 5", "Darwin en Patagonia", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "The Correspondence of Charles Darwin, Volume 6", "The Power of Movement in Plants", "Darwin's journal", "The Descent of Man and Selection in Relation to Sex", "Darwin for Today", "The Origin of Species (Oxford World's Classics)", "The Correspondence of Charles Darwin, Volume 9", "The Origin of Species (Collector's Library)", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "Wu zhong qi yuan", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "The Expression of the Emotions in Man and Animals", "La vie et la correspondance de Charles Darwin", "The Correspondence of Charles Darwin, Volume 13: 1865", "Die fundamente zur entstehung der arten", "Les moyens d'expression chez les animaux", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Voyage of the Beagle (Adventure Classics)", "The Origin of Species (Mentor)", "The Origin of Species", "The Correspondence of Charles Darwin, Volume 16: 1868", "A student's introduction to Charles Darwin", "The Correspondence of Charles Darwin, Volume 14: 1866", "Evolution", "The origin of species : complete and fully illustrated", "On the origin of species by means of natural selection", "The structure and distribution of coral reefs.", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "The expression of the emotions in man and animals.", "The Formation of Vegetable Mould through the Action of Worms", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The Autobiography of Charles Darwin [EasyRead Edition]", "The structure and distribution of coral reefs", "Del Plata a Tierra del Fuego", "The Correspondence of Charles Darwin, Volume 2", "The action of carbonate of ammonia on the roots of certain plants", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Diary of the voyage of H.M.S. Beagle", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "Cartas de Darwin 18251859", "The Correspondence of Charles Darwin, Volume 10", "Diario del Viaje de Un Naturalista Alrededor", "Darwin Compendium", "The Correspondence of Charles Darwin, Volume 9: 1861", "Human nature, Darwin's view", "The Variation of Animals and Plants under Domestication", "The descent of man, and selection in relation to sex", "The Voyage of the Beagle (Everyman Paperbacks)", "Voyage of the Beagle (NG Adventure Classics)", "Fertilisation of Orchids", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "Darwin's notebooks on transmutation of species", "Geological Observations on South America", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The principal works", "Part I: Contributions to the Theory of Natural Selection / Part II", "More Letters of Charles Darwin", "The voyage of the Beagle.", "The Correspondence of Charles Darwin, Volume 15: 1867", "Kleinere geologische Abhandlungen", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Beagle letters", "Voyage of the Beagle", "The Correspondence of Charles Darwin, Volume 7", "A Darwin Selection", "On the Movements and Habits of Climbing Plants", "The Correspondence of Charles Darwin, Volume 11", "Rejse om jorden", "From So Simple a Beginning", "The living thoughts of Darwin", "vari\u00eberen der huisdieren en cultuurplanten", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Charles Darwin on the routes of male humble bees", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The Correspondence of Charles Darwin, Volume 8: 1860", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "The Autobiography of Charles Darwin (Great Minds Series)", "The origin of species", "Darwin-Wallace", "The Correspondence of Charles Darwin, Volume 4", "The Correspondence of Charles Darwin, Volume 14", "Darwin's Ornithological notes", "Tesakneri tsagume\u030c", "Works", "Motsa ha-minim", "The Darwin Reader First Edition", "The descent of man and selection in relation to sex.", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "The Orgin of Species", "Charles Darwin's letters", "Darwinism stated by Darwin himself", "Insectivorous Plants", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "The Autobiography of Charles Darwin (Large Print)", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The Autobiography of Charles Darwin", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Reise um die Welt 1831 - 36", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Proiskhozhdenie vidov", "On Natural Selection", "Darwin's insects", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "The autobiography of Charles Darwin, 1809-1882", "The geology of the voyage of H.M.S. Beagle", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "Voyage d'un naturaliste autour du monde", "On evolution", "ontstaan der soorten door natuurlijke teeltkeus", "The portable Darwin", "The Autobiography Of Charles Darwin", "The Correspondence of Charles Darwin, Volume 12: 1864", "The Origin Of Species", "Charles Darwin's marginalia", "Origin of Species", "Memorias y epistolario i\u0301ntimo", "The Descent of Man, and Selection in Relation to Sex", "The\u0301orie de l'e\u0301volution", "The Voyage of the Beagle (Great Minds Series)", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "From so simple a beginning", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Het uitdrukken van emoties bij mens en dier", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "Les mouvements et les habitudes des plantes grimpantes", "The Autobiography of Charles Darwin, and selected letters", "Resa kring jorden", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "The Expression Of The Emotions In Man And Animals", "The Correspondence of Charles Darwin, Volume 11: 1863", "The Origin of Species (Great Books : Learning Channel)", "The Essential Darwin", "Darwin on humus and the earthworm", "Darwin Darwin", "The education of Darwin", "H.M.S. Beagle in South America", "The Origin of Species (World's Classics)", "The Correspondence of Charles Darwin, Volume 12", "Darwin and Henslow", "Evolution and natural selection", "The Correspondence of Charles Darwin, Volume 8", "Gesammelte kleinere Schriften", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "The Correspondence of Charles Darwin, Volume 10: 1862", "The voyage of Charles Darwin", "The Darwin Reader Second Edition", "La facult\u00e9 motrice dans les plantes", "Origins", "From Darwin's unpublished notebooks", "The Correspondence of Charles Darwin, Volume 13", "The descent of man, and selection in relation to sex.", "The Correspondence of Charles Darwin, Volume 3", "The Correspondence of Charles Darwin, Volume 18: 1870", "Charles Darwin", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "On the tendency of species to form varieties", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Autobiography of Charles Darwin", "The Voyage of the Beagle (Unabridged Classics)", "The Different Forms of Flowers on Plants of the Same Species", "Notebooks on transmutation of species", "The Expression of the Emotions in Man And Animals", "Voyage of the Beagle (Harvard Classics, Part 29)", "On a remarkable bar of sandstone off Pernambuco", "The collected papers of Charles Darwin", "The Structure and Distribution of Coral Reefs", "Les r\u00e9cifs de corail, leur structure et leur distribution", "red notebook of Charles Darwin", "Voyage Of The Beagle", "To the members of the Down Friendly Club", "The Life of Erasmus Darwin", "Reise eines Naturforschers um die Welt", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Metaphysics, Materialism, & the evolution of mind", "Origin of Species (Harvard Classics, Part 11)", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The Correspondence of Charles Darwin, Volume 15", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "The Origin of Species (Variorum Reprint)", "The autobiography of Charles Darwin", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Darwin", "monograph on the sub-class Cirripedia", "The Correspondence of Charles Darwin, Volume 1", "The expression of the emotions in man and animals", "genese\u014ds t\u014dn eid\u014dn", "Die geschlechtliche Zuchtwahl", "Charles Darwin's natural selection", "The foundations of the Origin of species", "Notes on the fertilization of orchids", "The Structure And Distribution of Coral Reefs", "The Correspondence of Charles Darwin, Volume 17: 1869", "Evolution by natural selection", "Leben und Briefe von Charles Darwin", "Voyage of the Beagle (Dover Value Editions)", "The Origin of Species (Enriched Classics)", "The Autobiography of Charles Darwin (Dodo Press)", "Volcanic Islands", "The Voyage of the Beagle", "The Origin of Species (Great Minds Series)"], "ans_acc": 0.12149532710280374, "ans_hit": 1, "ans_f1": 0.19409282700421943, "ans_precission": 1.0, "ans_recall": 0.10747663551401869, "path_f1": 0.6190476190476191, "path_precision": 1.0, "path_recall": 0.4482758620689655, "path_ans_f1": 0.21666666666666667, "path_ans_precision": 1.0, "path_ans_recall": 0.12149532710280374}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gator football", "# Reasoning Path:\nTim Tebow -> people.person.nationality -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nTim Tebow -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Florida\n# Answer:\nFlorida", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gaters football", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\n Florida Gators football", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gator Football", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gator's football", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gatorss football"], "ground_truth": ["Florida Gators football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\n Denver Broncos", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nColorado Broncos", "# Reasoning Path:\nPeyton Manning -> sports.drafted_athlete.drafted -> m.04vb3pp -> sports.sports_league_draft_pick.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> sports.drafted_athlete.drafted -> m.04vb3pp -> sports.sports_league_draft_pick.school -> University of Tennessee\n# Answer:\nUniversity of Tennessee", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\n Denver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nColorado Broncos"], "ground_truth": ["Denver Broncos"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Hungary\n# Answer:\nHungary", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Romania\n# Answer:\nRomania", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Slovakia\n# Answer:\nSlovakia", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Serbia\n# Answer:\nSerbia", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Poland\n# Answer:\nPoland", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Ukraine\n# Answer:\nUkraine", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Czech Republic\n# Answer:\nCzech Republic", "# Reasoning Path:\nCarpathian Mountains -> common.topic.notable_types -> Mountain range\n# Answer:\nMountain range", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Romania -> location.location.containedby -> Europe\n# Answer:\nEurope"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Poet\n# Answer:\nPoet", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Author\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Emily Bront\u00eb -> people.person.profession -> Poet\n# Answer:\nPoet", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> James Whitcomb Riley -> people.person.profession -> Poet\n# Answer:\nPoet", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Emily Bront\u00eb -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Emily Bront\u00eb\n# Answer:\nEmily Bront\u00eb", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> James Whitcomb Riley -> common.topic.notable_types -> Author\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> James Whitcomb Riley -> people.person.profession -> Writer\n# Answer:\nWriter"], "ground_truth": ["Writer", "Author", "Bard", "Poet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.actor -> Hayden Christensen\n# Answer:\nHayden Christensen", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.actor -> Hayden Christensen\n# Answer:\nHayden Christensen", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0235q84 -> film.performance.actor -> David Prowse\n# Answer:\nDavid Prowse", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0y5kksv -> film.performance.actor -> Zac Efron\n# Answer:\nZac Efron", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0235q8f -> film.performance.actor -> David Prowse\n# Answer:\nDavid Prowse", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.actor -> Hayden Christensen\n# Answer:\nHaden Christensen", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.actor -> Hayden Christensen\n# Answer:\nHaden Christensen", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.04m6bsf -> film.performance.actor -> Matt Lanter\n# Answer:\nMatt Lanter"], "ground_truth": ["Hayden Christensen"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.season -> 2006 NFL season\n# Answer:\n2006 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\n Denver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nColorado Broncos", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncosodia\n# Answer:\nDenver Broncosodia", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos Caf\u00e9\n# Answer:\nDenver Broncos Caf\u00e9", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.season -> 2006 NFL season\n# Answer:\n2006 NFL Season"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.ethnicity -> Canadian\n# Answer:\nCanadian", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford\n# Answer:\nStreton", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford\n# Answer:\nStrextford", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford\n# Answer:\nSt Stratford", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford\n# Answer:\n Stratford", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford\n# Answer:\nStrengthen\n# Answer:\nStrengthen", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford\n# Answer:\nStrengthen"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.06vz4t9 -> military.military_combatant_group.combatants -> Argentina\n# Answer:\nArgentina", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> base.culturalevent.event.entity_involved -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wphl -> military.casualties.combatant -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wpj2 -> military.casualties.combatant -> France\n# Answer:\nFrance", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> base.culturalevent.event.entity_involved -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> film.film_subject.films -> Three Kings -> film.film.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> film.film_subject.films -> Three Kings -> film.film.country -> Australia\n# Answer:\nAustralia"], "ground_truth": ["Australia", "Saudi Arabia", "United Kingdom", "Iraq", "Argentina", "United States of America", "France"], "ans_acc": 0.8571428571428571, "ans_hit": 1, "ans_f1": 0.8333333333333333, "ans_precission": 1.0, "ans_recall": 0.7142857142857143, "path_f1": 0.29850746268656714, "path_precision": 1.0, "path_recall": 0.17543859649122806, "path_ans_f1": 0.923076923076923, "path_ans_precision": 1.0, "path_ans_recall": 0.8571428571428571}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nBrenda Song", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkz86 -> award.award_nomination.award_nominee -> Brenda Song\n# Answer:\nBrenda Song", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.character -> London Tipton\n# Answer:\nLondon Tipton", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nBrendaSong", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nBrenna Song", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nBridgit Mendler\n# Answer:\nBridgit Mendler", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nBridgit Mendler", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nBrenda Songodia\n# Answer:\nBrenda Songodia", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\n Brenda Song"], "ground_truth": ["Brenda Song"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 0.4444444444444444, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_holder -> Sherrod Brown\n# Answer:\nSherrod Brown", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.office_holder -> Rob Portman\n# Answer:\nRob Portman", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_holder -> Sherrod Brown\n# Answer:\nSherod Brown", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_holder -> Sherrod Brown\n# Answer:\nSherrrod Brown", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_holder -> Sherrod Brown\n# Answer:\nsherrod brown", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_holder -> Sherrod Brown\n# Answer:\n Sherrod Brown", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.office_holder -> Rob Portman\n# Answer:\nRobPortman", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_holder -> Sherrod Brown\n# Answer:\nsherrod Brown", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_holder -> Sherrod Brown\n# Answer:\nHerrod Brown"], "ground_truth": ["John Kasich", "Ted Strickland", "Return J. Meigs, Jr."], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team -> m.07m4bqp -> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.02nr829 -> sports.sports_team_roster.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m -> soccer.football_player_stats.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.loans -> m.0bvdd3n -> soccer.football_player_loan.lending_team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.0qd3y3c -> sports.sports_team_roster.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team -> m.07m4bqp -> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club -> LA Galaxy\n# Answer:\nLos Angeles Galaxy", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nParisSaint-Germain F.C."], "ground_truth": ["LA Galaxy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7706422018348624, "path_precision": 0.7, "path_recall": 0.8571428571428571, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.nationality -> Spain\n# Answer:\nSpain", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.deceased_person.place_of_death -> Mexico City\n# Answer:\nMexico City", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.nationality -> Spain -> location.location.primarily_containedby -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.nationality -> Spain -> location.country.languages_spoken -> Spanish Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.gender -> Male\n# Answer:\nMale", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.location.containedby -> California\n# Answer:\nCalifornia", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.nationality -> Spain -> location.country.administrative_divisions -> Galicia\n# Answer:\nGalicia", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.deceased_person.place_of_death -> Mexico City -> location.location.containedby -> Greater Mexico City\n# Answer:\nGreater Mexico City"], "ground_truth": ["Salamanca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> base.kwebbase.kwtopic.connections_from -> john fitzgerald kennedy allegedly assassinated by lee harvey oswald -> base.kwebbase.kwconnection.other -> Lee Harvey Oswald\n# Answer:\nLee Harvey Oswald", "# Reasoning Path:\nJohn F. Kennedy -> base.kwebbase.kwtopic.connections_to -> lee harvey oswald allegedly assassinated john fitzgerald kennedy -> base.kwebbase.kwconnection.subject -> Lee Harvey Oswald\n# Answer:\nLee Harvey Oswald", "# Reasoning Path:\nJohn F. Kennedy -> film.film_subject.films -> Thirteen Days -> film.film.subjects -> Robert F. Kennedy\n# Answer:\nRobert F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.sibling_s -> m.03hs9fc -> people.sibling_relationship.sibling -> Robert F. Kennedy\n# Answer:\nRobert F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.family_member.family -> Kennedy family -> people.family.members -> Robert F. Kennedy\n# Answer:\nRobert F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Joseph P. Kennedy, Sr. -> people.person.children -> Robert F. Kennedy\n# Answer:\nRobert F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr.\n# Answer:\nJohn F. Kennedy Jr.", "# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Rose Kennedy -> people.person.children -> Robert F. Kennedy\n# Answer:\nRobert F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Joseph P. Kennedy, Sr.\n# Answer:\nJoseph P. Kennedy, Sr."], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.street_address -> m.0ggj3z2 -> location.mailing_address.citytown -> Fukushima\n# Answer:\nFukushima", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Fukushima Prefecture\n# Answer:\nFukushima Prefecture", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.country.administrative_divisions -> Fukui Prefecture\n# Answer:\nFukui Prefecture", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Aomori Prefecture\n# Answer:\nAomori Prefecture", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Yamagata Prefecture\n# Answer:\nYamagata Prefecture", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Hiroshima Prefecture\n# Answer:\nHiroshima Prefecture", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Okinawa Prefecture\n# Answer:\nOkinawa Prefecture", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.country.administrative_divisions -> Niigata Prefecture\n# Answer:\nNiigata Prefecture"], "ground_truth": ["Japan", "Okuma"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Scotland\n# Answer:\nScotland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> government.governmental_jurisdiction.government_bodies -> Parliament of the United Kingdom -> government.governmental_body.jurisdiction -> England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> government.governmental_jurisdiction.government_bodies -> Parliament of the United Kingdom -> government.governmental_body.jurisdiction -> Wales\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Scotland\n# Answer:\nScotland", "# Reasoning Path:\nUnited Kingdom -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Republic of Ireland\n# Answer:\nRepublic of Ireland", "# Reasoning Path:\nUnited Kingdom -> base.locations.countries.continent -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> location.country.currency_used -> Pound sterling\n# Answer:\nPound sterling"], "ground_truth": ["England", "Scotland", "Wales", "Northern Ireland"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> meteorology.cyclone_affected_area.cyclones -> Hurricane Katrina -> meteorology.tropical_cyclone.tropical_cyclone_season -> 2005 Atlantic hurricane season\n# Answer:\n2005 Atlantic hurricane season", "# Reasoning Path:\nLouisiana -> meteorology.cyclone_affected_area.cyclones -> Hurricane Danny -> meteorology.tropical_cyclone.affected_areas -> Gulf Coast of the United States\n# Answer:\nGulf Coast of the United States", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zoneodia\n# Answer:\nCentral Time Zoneodia", "# Reasoning Path:\nLouisiana -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Fay -> meteorology.tropical_cyclone.affected_areas -> Mississippi\n# Answer:\nMississippi", "# Reasoning Path:\nLouisiana -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> Mississippi\n# Answer:\nMississippi", "# Reasoning Path:\nLouisiana -> meteorology.cyclone_affected_area.cyclones -> Hurricane Ida -> meteorology.tropical_cyclone.affected_areas -> Mississippi\n# Answer:\nMississippi", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone Caf\u00e9\n# Answer:\nCentral Time Zone Caf\u00e9", "# Reasoning Path:\nLouisiana -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Arlene -> meteorology.tropical_cyclone.affected_areas -> Mississippi\n# Answer:\nMississippi", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone\n# Answer:\nCentral Time Zone"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> base.saints.saint.venerated_in -> Oriental Orthodoxy -> religion.religion.is_part_of -> Christianity\n# Answer:\nChristianity", "# Reasoning Path:\nAugustine of Hippo -> base.saints.saint.venerated_in -> Eastern Christianity -> religion.religion.is_part_of -> Christianity\n# Answer:\nChristianity", "# Reasoning Path:\nAugustine of Hippo -> base.saints.saint.venerated_in -> Eastern Christianity -> religion.religion.deities -> Jesus Christ\n# Answer:\nJesus Christ", "# Reasoning Path:\nAugustine of Hippo -> base.saints.saint.venerated_in -> Eastern Christianity -> religion.religion.deities -> Holy Spirit\n# Answer:\nHoly Spirit", "# Reasoning Path:\nAugustine of Hippo -> base.saints.saint.venerated_in -> Eastern Christianity -> religion.religion.deities -> God\n# Answer:\nGod", "# Reasoning Path:\nAugustine of Hippo -> book.author.works_written -> The Confessions of St. Augustine\n# Answer:\nThe Confessions of St. Augustine", "# Reasoning Path:\nAugustine of Hippo -> base.saints.saint.venerated_in -> Oriental Orthodoxy -> religion.religion.deities -> Jesus Christ\n# Answer:\nJesus Christ"], "ground_truth": ["Writer", "Philosopher", "Physician"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Sal Gibson\n# Answer:\nSal Gibson", "# Reasoning Path:\nKeyshia Cole -> people.person.children -> Daniel Hiram Gibson Jr.\n# Answer:\nDaniel Hiram Gibson Jr.", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons\n# Answer:\nFrancoise Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons\n# Answer:\n Francine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons\n# Answer:\nFrancoine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons\n# Answer:\nFrancois Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons\n# Answer:\nFrancine Lones", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons\n# Answer:\nFranco Lons"], "ground_truth": ["Francine Lons", "Leon Cole", "Sal Gibson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system\n# Answer:\nSemi-presential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system\n# Answer:\n Semi-presidential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system\n# Answer:\nS Semi-presidential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government\n# Answer:\nProvisonal government", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government\n# Answer:\nprovisional government", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system\n# Answer:\nSemi-presidential System", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system\n# Answer:\nSemi-presidental system"], "ground_truth": ["Semi-presidential system", "Provisional government"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.activism.activism_issue.activists -> Robert F. Kennedy\n# Answer:\nRobert F. Kennedy", "# Reasoning Path:\nMartin Luther King, Jr. -> book.book_subject.works -> Countdown of 10 Amazing Moments from the Year 2011: No. 4 the Martin Luther King Jr. Memorial\n# Answer:\nCountdown of 10 Amazing Moments from the Year 2011: No. 4 the Martin Luther King Jr. Memorial", "# Reasoning Path:\nMartin Luther King, Jr. -> book.book_subject.works -> Countdown of 10 Amazing Moments from the Year 2011: No. 9 Belafonte's New Song -> book.written_work.subjects -> Civil rights movements\n# Answer:\nCivil rights movements", "# Reasoning Path:\nMartin Luther King, Jr. -> book.book_subject.works -> Countdown of 10 Amazing Moments from the Year 2011: No. 4 the Martin Luther King Jr. Memorial -> book.written_work.subjects -> Washington, D.C.\n# Answer:\nWashington, D.C.", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.activism.activism_issue.supporting_political_parties -> African National Congress\n# Answer:\nAfrican National Congress", "# Reasoning Path:\nMartin Luther King, Jr. -> book.book_subject.works -> Countdown of 10 Amazing Moments from the Year 2011: No. 4 the Martin Luther King Jr. Memorial -> book.written_work.subjects -> Martin Luther King, Jr. Memorial\n# Answer:\nMartin Luther King, Jr. Memorial", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.place_of_birth -> Atlanta\n# Answer:\nAtlanta", "# Reasoning Path:\nMartin Luther King, Jr. -> book.book_subject.works -> Countdown of 10 Amazing Moments from the Year 2011: No. 4 the Martin Luther King Jr. Memorial -> book.written_work.subjects -> African American\n# Answer:\nAfrican American", "# Reasoning Path:\nMartin Luther King, Jr. -> book.book_subject.works -> Countdown of 10 Amazing Moments from the Year 2011: No. 9 Belafonte's New Song -> book.written_work.subjects -> Human rights\n# Answer:\nHuman rights"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_burial -> Westminster Hall and Burying Ground\n# Answer:\nWestminster Hall and Burying Ground", "# Reasoning Path:\nEdgar Allan Poe -> symbols.name_source.namesakes -> Edgar Allan Poe House and Museum -> location.location.containedby -> Baltimore\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Charles Baudelaire -> people.deceased_person.place_of_death -> Paris\n# Answer:\nParis", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore\n# Answer:\nBaltimor", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore\n# Answer:\nMaryland\n# Answer:\nMaryland", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_burial -> Westminster Hall and Burying Ground\n# Answer:\nWestminister Hall and Burying Ground", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_burial -> Westminster Hall and Burying Ground\n# Answer:\nWestminsterHall and Burying Ground", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_burial -> Westminster Hall and Burying Ground\n# Answer:\nWebster Hall and Burying Ground"], "ground_truth": ["Baltimore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.6153846153846153, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia State Capitol\n# Answer:\nGeorgia State Capitol", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Martin Luther King, Jr. National Historic Site\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Turner Field\n# Answer:\nTurner Field", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Fernbank Museum of Natural History\n# Answer:\nFernbank Museum of Natural History", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola\n# Answer:\nWorld of Coca-Cola", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Zoo Atlanta\n# Answer:\nZoo Atlanta", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Fernbank Science Center\n# Answer:\nFernbank Science Center", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Fernbank Museum of Natural History\n# Answer:\nFernback Museum of Natural History"], "ground_truth": ["Jimmy Carter Library and Museum", "Georgia State Capitol", "Six Flags White Water", "Hyatt Regency Atlanta", "The Tabernacle", "World of Coca-Cola", "Fernbank Museum of Natural History", "Georgia Aquarium", "Atlanta Jewish Film Festival", "CNN Center", "Fernbank Science Center", "Center for Puppetry Arts", "Georgia Dome", "Atlanta Cyclorama & Civil War Museum", "Woodruff Arts Center", "Atlanta Symphony Orchestra", "Four Seasons Hotel Atlanta", "Philips Arena", "Variety Playhouse", "Atlanta Marriott Marquis", "Cobb Energy Performing Arts Centre", "Underground Atlanta", "Zoo Atlanta", "Arbor Place Mall", "Fox Theatre", "Atlanta History Center", "Georgia World Congress Center", "Margaret Mitchell House & Museum", "Masquerade", "Peachtree Road Race", "Centennial Olympic Park", "Atlanta Ballet", "Omni Coliseum", "Turner Field", "Martin Luther King, Jr. National Historic Site", "Six Flags Over Georgia"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.391304347826087, "ans_precission": 0.9, "ans_recall": 0.25, "path_f1": 0.4, "path_precision": 1.0, "path_recall": 0.25, "path_ans_f1": 0.4, "path_ans_precision": 1.0, "path_ans_recall": 0.25}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> people.person.nationality -> Australia -> location.country.administrative_divisions -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> people.person.nationality -> Australia -> base.aareas.schema.administrative_area.administrative_children -> Australian Capital Territory\n# Answer:\nAustralian Capital Territory", "# Reasoning Path:\nAnna Bligh -> people.person.nationality -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nAnna Bligh -> people.person.nationality -> Australia -> base.aareas.schema.administrative_area.administrative_children -> Victoria\n# Answer:\nVictoria", "# Reasoning Path:\nAnna Bligh -> common.image.appears_in_topic_gallery -> Premier of Queensland -> government.government_office_or_title.jurisdiction -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> people.person.nationality -> Australia -> location.country.administrative_divisions -> New South Wales\n# Answer:\nNew South Wales", "# Reasoning Path:\nAnna Bligh -> people.person.gender -> Female -> base.thoroughbredracing.thoroughbred_racehorse_sex.horses_of_this_sex -> Apology\n# Answer:\nApollos", "# Reasoning Path:\nAnna Bligh -> people.person.nationality -> Australia -> base.aareas.schema.administrative_area.subdividing_type -> Australian state\n# Answer:\nAustralian state", "# Reasoning Path:\nAnna Bligh -> people.person.gender -> Female -> base.thoroughbredracing.thoroughbred_racehorse_sex.horses_of_this_sex -> Apology\n# Answer:\nApology", "# Reasoning Path:\nAnna Bligh -> people.person.nationality -> Australia -> base.aareas.schema.administrative_area.administrative_children -> Northern Territory\n# Answer:\nNorthern Territory"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ken Barlow\n# Answer:\nKen Barlow", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> David Barlow\n# Answer:\nDavid Barlow", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ken Barlow\n# Answer:\nKen Barrow", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> tv.tv_program_creator.programs_created -> Inspector Morse\n# Answer:\nInspector Morse", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Dennis Tanner\n# Answer:\nDennis Tanner", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> people.person.nationality -> United Kingdom\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Annie Walker\n# Answer:\nAnnie Walker", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ken Barlow\n# Answer:\nKen barlow", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ken Barlow\n# Answer:\nKenneth Barlow", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ken Barlow\n# Answer:\n Ken Barlow"], "ground_truth": ["William Roache"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.nationality -> United Kingdom\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nAndy Murray -> people.person.nationality -> United Kingdom -> location.country.capital -> London\n# Answer:\nLondon", "# Reasoning Path:\nAndy Murray -> people.person.parents -> Judy Murray\n# Answer:\nJudy Murray", "# Reasoning Path:\nAndy Murray -> people.person.nationality -> United Kingdom -> olympics.olympic_participating_country.olympics_participated_in -> The London 2012 Summer Olympics\n# Answer:\nThe London 2012 Summer Olympics", "# Reasoning Path:\nAndy Murray -> people.person.nationality -> United Kingdom -> location.country.official_language -> English Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nAndy Murray -> people.person.profession -> Tennis player\n# Answer:\nTennis player", "# Reasoning Path:\nAndy Murray -> people.person.nationality -> United Kingdom -> common.topic.notable_types -> Country\n# Answer:\nCountry", "# Reasoning Path:\nAndy Murray -> people.person.parents -> Judy Murray\n# Answer:\nJudith Murray", "# Reasoning Path:\nAndy Murray -> people.person.nationality -> United Kingdom -> olympics.olympic_participating_country.olympics_participated_in -> The London 2012 Summer Olympics\n# Answer:\nTheLondon 2012 Summer Olympics"], "ground_truth": ["2005"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.country.currency_used -> Australian dollar\n# Answer:\nAustralian dollar", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Kiribati -> location.country.currency_used -> Kiribati dollar\n# Answer:\nKiribati dollar", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Kiribati -> location.country.currency_used -> Australian dollar\n# Answer:\nAustralian dollar", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Kiribati -> location.location.containedby -> Oceania\n# Answer:\nOceania", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Tuvalu -> location.country.currency_used -> Australian dollar\n# Answer:\nAustralian dollar", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Tuvalu -> location.location.containedby -> Oceania\n# Answer:\nOceania", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.country.languages_spoken -> English Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> royalty.kingdom.rulers -> Elizabeth II\n# Answer:\nElizabeth II", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.country.currency_used -> Australian dollar\n# Answer:\nAustralian Dollar\n# Answer:\nAustralian Dollar", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.country.currency_used -> Australian dollar\n# Answer:\nAustralian Dollar"], "ground_truth": ["AUD"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone\n# Answer:\nCentral Europe Time Zone", "# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone\n# Answer:\nCentralEuropean Time Zone", "# Reasoning Path:\nSweden -> location.country.official_language -> Swedish Language -> language.human_language.region -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone\n# Answer:\n Central European Time Zone", "# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone\n# Answer:\nCentral European Time_zone", "# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone\n# Answer:\nCentral European Times Zone", "# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone\n# Answer:\nCentral European Time zone", "# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone\n# Answer:\nCentral EuropeanTime Zone", "# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone\n# Answer:\nCentral-European Time Zone"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> people.person.place_of_birth -> Atlanta -> sports.sports_team_location.teams -> Atlanta Falcons\n# Answer:\nAtlanta Falcons", "# Reasoning Path:\nCam Newton -> american_football.football_player.position_s -> Quarterback -> sports.sports_position.sport -> American football\n# Answer:\nAmerican football", "# Reasoning Path:\nCam Newton -> people.person.nationality -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nCam Newton -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Nebraska\n# Answer:\nNebraska", "# Reasoning Path:\nCam Newton -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> North Dakota\n# Answer:\nNorth Dakota", "# Reasoning Path:\nCam Newton -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Iowa\n# Answer:\nIowa", "# Reasoning Path:\nCam Newton -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Oregon\n# Answer:\nOregon", "# Reasoning Path:\nCam Newton -> people.person.place_of_birth -> Atlanta -> sports.sports_team_location.teams -> Atlanta Falcons\n# Answer:\n Atlanta Falcons", "# Reasoning Path:\nCam Newton -> american_football.football_player.position_s -> Quarterback -> sports.sports_position.sport -> American football\n# Answer:\nAmerican Football", "# Reasoning Path:\nCam Newton -> american_football.football_player.position_s -> Quarterback -> sports.sports_position.sport -> American football\n# Answer:\nAmericanfootball"], "ground_truth": ["Carolina Panthers"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Maryland\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.citytown.postal_codes -> 21701\n# Answer:\n21701", "# Reasoning Path:\nFrederick -> location.citytown.postal_codes -> 21702\n# Answer:\n21702", "# Reasoning Path:\nFrederick -> location.citytown.postal_codes -> 21703\n# Answer:\n21703", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County\n# Answer:\nFredrick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County\n# Answer:\nFrederic County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County\n# Answer:\nFrederick Countyodia\n# Answer:\nFrederick Countyodia"], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nHuntingdon College", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> University of Alabama School of Law\n# Answer:\nUniversity of Alabama School of Law", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy9 -> education.education.institution -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138 -> education.education.institution -> University of Alabama\n# Answer:\nUniversity of Alabama", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nHuntington College", "# Reasoning Path:\nHarper Lee -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Alabama\n# Answer:\nAlabama", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> University of Alabama School of Law\n# Answer:\nUniversity of AlabamaSchool of Law", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmyl -> education.education.institution -> Monroe County High School\n# Answer:\nMonroe County High School", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> University of Alabama School of Law\n# Answer:\nUniversity ofAlabama School of Law", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nHuntingdonCollege"], "ground_truth": ["Monroe County High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nPacific Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nMTZ\n# Answer:\nMTZ", "# Reasoning Path:\nUtah -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nMTZ\n# Time Zone:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nMountain Time Zoneodia\n# Answer:\nMountain Time Zoneodia", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nMTZ\n# Question:\nMTZ", "# Reasoning Path:\nUtah -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nMTZ\n# Answer:\nMountain Time Zone"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2004 -> government.election_campaign.party -> Republican Party\n# Answer:\nRepublican Party", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2004 -> government.election_campaign.election -> United States presidential election, 2004\n# Answer:\nUnited States presidential election, 2004", "# Reasoning Path:\nGeorge W. Bush -> government.political_appointer.appointees -> m.08v_t2j -> government.government_position_held.office_holder -> Colin Powell\n# Answer:\nColin Powell", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2000 -> government.election_campaign.party -> Republican Party\n# Answer:\nRepublican Party", "# Reasoning Path:\nGeorge W. Bush -> government.political_appointer.appointees -> m.04d1yn9 -> government.government_position_held.office_holder -> Condoleezza Rice\n# Answer:\nCondoleezza Rice", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2004 -> government.election_campaign.election -> United States presidential election, 2004\n# Answer:\nUnited States Presidential Election, 2004", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2004 -> government.election_campaign.party -> Republican Party\n# Answer:\n Republican Party", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2004 -> government.election_campaign.party -> Republican Party\n# Answer:\nRepublic Party", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2004 -> government.election_campaign.party -> Republican Party\n# Answer:\nDemocratic Party", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2004 -> government.election_campaign.election -> United States presidential election, 2004\n# Answer:\nUnited States Presidential election, 2004"], "ground_truth": ["Michael Peroutka", "Ralph Nader", "John Kerry", "Gene Amondson"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.nationality -> United Kingdom -> location.country.capital -> London\n# Answer:\nLondon", "# Reasoning Path:\nNiall Ferguson -> people.person.nationality -> United Kingdom -> base.aareas.schema.administrative_area.subdivides_place -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nNiall Ferguson -> people.person.profession -> Author -> people.profession.specializations -> Memoirist\n# Answer:\nMemoirist", "# Reasoning Path:\nNiall Ferguson -> people.person.nationality -> United Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Scotland\n# Answer:\nScotland", "# Reasoning Path:\nNiall Ferguson -> people.person.place_of_birth -> Glasgow -> location.location.events -> 2014 Glasgow Film Festival\n# Answer:\n2014 Glasgow Film Festival", "# Reasoning Path:\nNiall Ferguson -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> Scotland\n# Answer:\nScotland", "# Reasoning Path:\nNiall Ferguson -> people.person.profession -> Author\n# Answer:\nAuthor", "# Reasoning Path:\nNiall Ferguson -> people.person.place_of_birth -> Glasgow -> location.location.events -> 2010 Glasgow Film Festival\n# Answer:\n2010 Glasgow Film Festival", "# Reasoning Path:\nNiall Ferguson -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nNiall Ferguson -> people.person.place_of_birth -> Glasgow -> location.location.events -> 2011 Glasgow Short Film Festival\n# Answer:\n2011 Glasgow Short Film Festival"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Pacific Ocean\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> San Crist\u00f3bal Island\n# Answer:\nSan Crist\u00f3bal Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> San Crist\u00f3bal Island -> location.location.containedby -> Ecuador\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Santa Cruz Island\n# Answer:\nSanta Cruz Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Darwin Island\n# Answer:\nDarwin Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.nearby_airports -> San Crist\u00f3bal Airport -> location.location.containedby -> Ecuador\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> San Crist\u00f3bal Island\n# Answer:\nSan Crist\u00f3BAL Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> San Crist\u00f3bal Island\n# Answer:\nSan Crist\u00f3bul Island"], "ground_truth": ["Gal\u00e1pagos Province", "Ecuador", "Pacific Ocean"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.composer.compositions -> As Long as You Love Me\n# Answer:\nAs Long as You Love Me", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> My World\n# Answer:\nMy World", "# Reasoning Path:\nJustin Bieber -> music.composer.compositions -> Baby\n# Answer:\nBaby", "# Reasoning Path:\nJustin Bieber -> music.composer.compositions -> Beauty And A Beat -> music.composition.recordings -> Beautiful and the Beat\n# Answer:\nBeautiful and the Beat", "# Reasoning Path:\nJustin Bieber -> music.composer.compositions -> Beauty And A Beat\n# Answer:\nBeauty And A Beat", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nJustin Bieber -> music.composer.compositions -> As Long as You Love Me -> music.composition.recordings -> As Long as You Love Me (acoustic version)\n# Answer:\nAs Long as You Love Me (acoustic version)", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0\n# Answer:\nrqp4h0", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> Somebody to Love\n# Answer:\nSomebody to Love"], "ground_truth": ["Bad Day", "As Long as You Love Me", "Never Say Never", "Wait for a Minute", "Baby", "Somebody to Love", "Home to Mama", "First Dance", "Turn to You (Mother's Day Dedication)", "Bigger", "Change Me", "Thought Of You", "Confident", "Never Let You Go", "Recovery", "Heartbreaker", "Lolly", "Roller Coaster", "PYD", "Right Here", "All That Matters", "All Around The World", "All Bad", "Boyfriend", "Eenie Meenie", "Live My Life", "Pray", "Die in Your Arms", "Hold Tight", "#thatPower", "Beauty And A Beat"], "ans_acc": 0.12903225806451613, "ans_hit": 1, "ans_f1": 0.20512820512820512, "ans_precission": 0.5, "ans_recall": 0.12903225806451613, "path_f1": 0.16216216216216214, "path_precision": 0.5, "path_recall": 0.0967741935483871, "path_ans_f1": 0.21238938053097342, "path_ans_precision": 0.6, "path_ans_recall": 0.12903225806451613}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> people.profession.specializations -> Statesman\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Publisher\n# Answer:\nPublisher", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.nationality -> France\n# Answer:\nFrance", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician\n# Answer:\n Politician"], "ground_truth": ["Publisher", "Statesman", "Writer", "Journalist", "Physician"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nSaguaro", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nSaguaroodia\n# Answer:\nSaguaroodia", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nSaguaroodia Saguaro\n# Answer:\nSaguaroodia Saguaro", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nSaguaro Cunningham\n# Answer:\nSaguaro Cunningham", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nSaguato", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nSaguano", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nSaguaro Caf\u00e9\n# Answer:\nSaguaro Caf\u00e9", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nSaguaro Assassination\n# Answer:\nSaguaro Assassination", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nSaguaro Cassidy\n# Answer:\nSaguaro Cassidy"], "ground_truth": ["Saguaro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> people.person.ethnicity -> Afro-Guyanese\n# Answer:\nAfro-Guyanese", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish\n# Answer:\nSt. Michael Parish", "# Reasoning Path:\nRihanna -> broadcast.artist.content -> Big R Radio - The Hawk -> broadcast.content.artist -> Lifehouse\n# Answer:\nLifehouse", "# Reasoning Path:\nRihanna -> people.person.ethnicity -> Afro-Guyanese\n# Answer:\nAfra-Guyanese", "# Reasoning Path:\nRihanna -> people.person.ethnicity -> Afro-Guyanese\n# Answer:\nAfrican-Guyanese", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish\n# Answer:\nSaint Michaels Parish", "# Reasoning Path:\nRihanna -> people.person.ethnicity -> Afro-Guyanese\n# Answer:\nAfrro-Guyanese", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish\n# Answer:\nSaintMichael Parish"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> base.inaugurations.inauguration_speaker.inauguration -> John Tyler 1841 presidential inauguration\n# Answer:\nJohn Tyler 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> United States Representative\n# Answer:\nUnited States Representative", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Virginia\n# Answer:\nVirginia", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Mississippi\n# Answer:\nMississippi", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> base.inaugurations.inauguration_speaker.inauguration -> John Tyler 1841 presidential inauguration\n# Answer:\nJohnTyler 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> base.inaugurations.inauguration_speaker.inauguration -> John Tyler 1841 presidential inauguration\n# Answer:\nJohn Tyler 1841 Presidential Inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> President of the United States\n# Answer:\n President of the United States", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_bodies -> United States Senate\n# Answer:\nUnited States Senate"], "ground_truth": ["1841-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.nationality -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills\n# Answer:\nMission Hillsodia\n# Answer:\nMission Hillsodia", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills\n# Answer:\nMission Hills Garc\u00eda\n# Answer:\nMission Hills Garc\u00eda", "# Reasoning Path:\nGeorge Lopez -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> California\n# Answer:\nCalifornia", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills\n# Answer:\nMission Hills Garc\u00eda, California\n# Answer:\nMission Hills Garc\u00eda, California", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills\n# Answer:\nMission Hillsodia, California\n# Answer:\nMission Hillsodia, California", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills\n# Answer:\nMission Hills Garc\u00eda, CA\n# Answer:\nMission Hills Garc\u00eda, CA", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills\n# Answer:\nMissio Hills"], "ground_truth": ["Mission Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.administrative_division.country -> South Korea\n# Answer:\nSouth Korea", "# Reasoning Path:\nSamsung Group -> organization.organization.founders -> Lee Byung-chul -> people.deceased_person.place_of_death -> Seoul\n# Answer:\nSeoul", "# Reasoning Path:\nSamsung Group -> organization.organization.founders -> Lee Byung-chul\n# Answer:\nLee Byung-chul", "# Reasoning Path:\nSamsung Group -> organization.organization.founders -> Lee Byung-chul -> people.person.nationality -> South Korea\n# Answer:\nSouth Korea", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.contains -> Daegu Stadium\n# Answer:\nDaegu Stadium", "# Reasoning Path:\nSamsung Group -> organization.organization.founders -> Lee Byung-chul\n# Answer:\nLee Byung-Chul", "# Reasoning Path:\nSamsung Group -> organization.organization.founders -> Lee Byung-chul\n# Answer:\nLee Byung_chul", "# Reasoning Path:\nSamsung Group -> organization.organization.founders -> Lee Byung-chul -> people.deceased_person.place_of_death -> Seoul\n# Answer:\nSoul"], "ground_truth": ["Suwon"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.beliefs -> \u1e6c\u016bb\u0101\n# Answer:\n\u1e6c\u016bb\u0101", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Masih ad-Dajjal\n# Answer:\nMasih ad-Dajjal", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Islamic view of angels\n# Answer:\nIslamic view of angels", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Prophets in Islam\n# Answer:\nProphets in Islam", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive\n# Answer:\nEntering Heaven alive", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Islamic holy books\n# Answer:\nIslamic holy books", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Sharia\n# Answer:\nSharia", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Tawhid\n# Answer:\nTawhid", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Mahdi\n# Answer:\nMahdi"], "ground_truth": ["Monotheism", "\u1e6c\u016bb\u0101", "Qiyamah", "Tawhid", "Entering Heaven alive", "Sharia", "God in Islam", "Prophets in Islam", "Islamic view of angels", "Islamic holy books", "Mahdi", "Predestination in Islam", "Masih ad-Dajjal"], "ans_acc": 0.7692307692307693, "ans_hit": 1, "ans_f1": 0.8695652173913044, "ans_precission": 1.0, "ans_recall": 0.7692307692307693, "path_f1": 0.8695652173913044, "path_precision": 1.0, "path_recall": 0.7692307692307693, "path_ans_f1": 0.8695652173913044, "path_ans_precision": 1.0, "path_ans_recall": 0.7692307692307693}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Topic\n# Answer:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0djz10s -> film.performance.actor -> Taylor Lautner\n# Reasoning Path:\nTaylor Lautner -> award.award_nominee.award_nominations -> m.0j2vjzw -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nChristian Grey -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Topic\n# Answer:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0djz10s -> film.performance.actor -> Taylor Lautner\n# Reasoning Path:\nTaylor Lautner -> award.award_nominee.award_nominations -> m.0j2vj_g -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nChristian Grey -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Topic\n# Answer:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0djz10s -> film.performance.actor -> Taylor Lautner\n# Reasoning Path:\nTaylor Lautner -> award.award_nominee.award_nominations -> m.0z83scz -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nChristian Grey -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Topic\n# Answer:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0djz0r8 -> film.performance.actor -> Taylor Lautner\n# Reasoning Path:\nTaylor Lautner -> award.award_nominee.award_nominations -> m.0j2vjzw -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nChristian Grey -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Topic\n# Answer:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0djz10s -> film.performance.actor -> Taylor Lautner\n# Reasoning Path:\nTaylor Lautner -> award.award_winner.awards_won -> m.0r436z2 -> award.award_honor.honored_for -> The Twilight Saga: Breaking Dawn - Part 2\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 2", "# Reasoning Path:\nChristian Grey -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Topic\n# Answer:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0djz10s -> film.performance.actor -> Taylor Lautner\n# Reasoning Path:\nTaylor Lautner -> award.award_winner.awards_won -> m.0r43x82 -> award.award_honor.honored_for -> The Twilight Saga: Breaking Dawn - Part 2\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 2", "# Reasoning Path:\nChristian Grey -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Topic\n# Answer:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0djz10s -> film.performance.actor -> Taylor Lautner\n# Reasoning Path:\nTaylor Lautner -> film.actor.film -> m.0b68zn5 -> film.performance.film -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nChristian Grey -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Topic\n# Answer:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0djz0r8 -> film.performance.actor -> Taylor Lautner\n# Reasoning Path:\nTaylor Lautner -> film.actor.film -> m.0b68zn5 -> film.performance.film -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nChristian Grey -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Topic\n# Answer:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0djz10s -> film.performance.actor -> Taylor Lautner\n# Reasoning Path:\nTaylor Lautner -> award.award_nominee.award_nominations -> m.0z837pz -> award.award_nomination.nominated_for -> Abduction\n# Answer:\nAbduction", "# Reasoning Path:\nChristian Grey -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Topic\n# Answer:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0djz10s -> film.performance.actor -> Taylor Lautner\n# Reasoning Path:\nTaylor Lautner -> film.actor.film -> m.0b68zn5 -> film.performance.character -> Christian Grey\n# Answer:\nChristian Grey"], "ground_truth": ["Jamie Dornan"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis\n# Answer:\nTuberculous cervical lymphadenitis\n# Reasoning Path:\nTuberculous cervical lymphadenitis -> medicine.disease_cause.diseases -> Tuberculosis -> people.cause_of_death.includes_causes_of_death -> Tuberculous cervical lymphadenitis\n# Answer:\nTuberculous cervical lymphadenitis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis\n# Answer:\nTuberculous cervical lymphadenitis\n# Reasoning Path:\nTuberculous cervical lymphadenitis -> medicine.disease.notable_people_with_this_condition -> Samuel Johnson -> people.deceased_person.cause_of_death -> Tuberculosis\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis\n# Answer:\nTuberculous cervical lymphadenitis\n# Reasoning Path:\nTuberculous cervical lymphadenitis -> medicine.disease_cause.diseases -> Tuberculosis -> medicine.disease.includes_diseases -> Tuberculous cervical lymphadenitis\n# Answer:\nTuberculous cervical lymphadenitis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis\n# Answer:\nTuberculous cervical lymphadenitis\n# Reasoning Path:\nTuberculous cervical lymphadenitis -> medicine.disease_cause.diseases -> Tuberculosis -> people.cause_of_death.people -> Anton Chekhov -> people.deceased_person.cause_of_death -> Tuberculosis\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis\n# Answer:\nTuberculous cervical lymphadenitis\n# Reasoning Path:\nTuberculous cervical lymphadenitis -> medicine.disease_cause.diseases -> Tuberculosis -> people.cause_of_death.includes_causes_of_death -> Tubercular cervical lymphadenitis\n# Answer:\nTubercular cervical lymphadenitis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis\n# Answer:\nTuberculous cervical lymphadenitis\n# Reasoning Path:\nTuberculous cervical lymphadenitis -> medicine.disease_cause.diseases -> Tuberculosis -> people.cause_of_death.people -> Anton Chekhov -> people.deceased_person.cause_of_death -> Lung cancer\n# Answer:\nLung cancer", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis\n# Answer:\nTuberculous cervical lymphadenitis\n# Reasoning Path:\nTuberculous cervical lymphadenitis -> medicine.disease_cause.diseases -> Tuberculosis -> people.cause_of_death.people -> Anton Chekhov -> people.deceased_person.cause_of_death -> Hypertension\n# Answer:\nHypertension", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis\n# Answer:\nTuberculous cervical lymphadenitis\n# Reasoning Path:\nTuberculous cervical lymphadenitis -> medicine.disease.notable_people_with_this_condition -> George Orwell -> people.deceased_person.cause_of_death -> Tuberculosis\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis\n# Answer:\nTuberculous cervical lymphadenitis\n# Reasoning Path:\nTuberculous cervical lymphadenitis -> medicine.disease.notable_people_with_this_condition -> Samuel Johnson -> people.deceased_person.cause_of_death -> heart attack\n# Answer:\nheart attack"], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> military.military_commander.military_commands -> m.04ls_tb -> military.military_command.military_combatant -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> government.political_appointer.appointees -> m.07jrjrl -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.place_founded -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> military.military_commander.military_commands -> m.02h7nns -> military.military_command.military_combatant -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> military.military_commander.military_commands -> m.0dl81ld -> military.military_command.military_combatant -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Schutzstaffel -> organization.organization.place_founded -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> government.political_appointer.appointees -> m.07jrh6q -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> organization.organization.geographic_scope -> Germany\n# Answer:\nGermany"], "ground_truth": ["Nazi Germany"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.8571428571428572, "path_precision": 0.9, "path_recall": 0.8181818181818182, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Actor\n# Answer:\nActor", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.genre -> Blue-eyed soul\n# Answer:\nBlue-eyed soul", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.genre -> Adult contemporary music\n# Answer:\nAdult contemporary music", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.notable_types -> Musical Artist\n# Answer:\nMusical Artist", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.genre -> Pop rock\n# Answer:\nPop rock", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.genre -> Blues\n# Answer:\nBlues", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter\n# Answer:\nsongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter\n# Answer:\n_songwriter"], "ground_truth": ["Songwriter", "Singer", "Actor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> base.biblioness.bibs_location.country -> United States of America -> location.country.administrative_divisions -> Kansas\n# Answer:\nKansas", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas\n# Answer:\nKansas", "# Reasoning Path:\nKansas City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nKansas City -> location.citytown.postal_codes -> 66104\n# Answer:\n66104", "# Reasoning Path:\nKansas City -> location.citytown.postal_codes -> 66105\n# Answer:\n66105", "# Reasoning Path:\nKansas City -> location.citytown.postal_codes -> 66102\n# Answer:\n66102", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County\n# Answer:\n Wyandotte County"], "ground_truth": ["Wyandotte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpwy -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpts -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpw6 -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbp_k -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpyc -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpxn -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h -> sports.sports_team_roster.team -> UCLA Bruins football\n# Answer:\nUCLA Bruins football"], "ground_truth": ["UCLA Bruins football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> theater.play.orchestrator -> Philip J. Lang -> people.person.nationality -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> base.biblioness.bibs_location.state -> New York\n# Answer:\nNew York", "# Reasoning Path:\nAnnie -> theater.play.orchestrator -> Philip J. Lang\n# Answer:\nPhilip J. Lang", "# Reasoning Path:\nAnnie -> theater.play.orchestrator -> Philip J. Lang -> people.person.place_of_birth -> New York\n# Answer:\nNew York", "# Reasoning Path:\nAnnie -> film.film.production_companies -> Columbia Pictures -> organization.organization.place_founded -> Los Angeles\n# Answer:\nLos Angeles", "# Reasoning Path:\nAnnie -> theater.play.orchestrator -> Philip J. Lang -> theater.theatrical_orchestrator.plays_orchestrated -> Annie\n# Answer:\nAnnie", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> theater.play.orchestrator -> Philip J. Lang -> people.person.profession -> Composer\n# Answer:\nComposer", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nAnnie -> theater.play.orchestrator -> Philip J. Lang -> theater.theatrical_orchestrator.plays_orchestrated -> 42nd Street\n# Answer:\n42nd Street"], "ground_truth": ["1976-08-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.nationality -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Alabama\n# Answer:\nAlabama", "# Reasoning Path:\nJaMarcus Russell -> people.person.ethnicity -> African American\n# Answer:\nAfrican American", "# Reasoning Path:\nJaMarcus Russell -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Alabama\n# Answer:\nAlabama", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile\n# Answer:\n Mobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Mobile County\n# Answer:\nMobile County", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile\n# Answer:\n-Mobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile\n# Answer:\nMobile Lauderdale County\n# Answer:\nMobile Lauderdale County"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan\n# Answer:\nNew York City", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York City\n# Answer:\nNew York City", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan\n# Answer:\nNew York City\n# Answer:\nNew York City", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York\n# Answer:\nNew York", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan\n# Answer:\nNew York City\n# Question:\nNew York City", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan\n# Answer:\nNew York City (Manhattan)\n# Answer:\nNew York City (Manhattan)", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan\n# Answer:\nNew York City\n# Answered by:\nNew York City", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan\n# Answer:\nNew York City, Manhattan\n# Answer:\nNew York City, Manhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan\n# Answer:\nNew York City\n# Related Topic:\nNew York City\n# Answer:\nNew York City"], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> India\n# Answer:\nIndia", "# Reasoning Path:\nIndonesia -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nIndonesia -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> Asia\n# Answer:\nAsia", "# Reasoning Path:\nIndonesia -> olympics.olympic_participating_country.olympics_participated_in -> 2000 Summer Olympics -> olympics.olympic_games.participating_countries -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nIndonesia -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> Philippines\n# Answer:\nPhilippines", "# Reasoning Path:\nIndonesia -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> Indonesia\n# Answer:\nIndonesia", "# Reasoning Path:\nIndonesia -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> Thailand\n# Answer:\nThailand", "# Reasoning Path:\nIndonesia -> base.aareas.schema.administrative_area.administrative_children -> Jakarta -> location.location.containedby -> Java\n# Answer:\nJava", "# Reasoning Path:\nIndonesia -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> Netherlands\n# Answer:\nNetherlands", "# Reasoning Path:\nIndonesia -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> United States of America\n# Answer:\nUnited States of America"], "ground_truth": ["Protestantism", "Hinduism", "Catholicism", "Islam"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination\n# Answer:\nAssassination", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm\n# Answer:\nFirearms", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm\n# Answer:\nfirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> people.cause_of_death.includes_causes_of_death -> Assassination by firearm\n# Answer:\nAssassination by firearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm\n# Answer:\n_firearm", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph\n# Answer:\nSt. Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm\n# Answer:\nFirerarm"], "ground_truth": ["Firearm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> military.military_commander.military_commands -> m.04h_gx9 -> military.military_command.military_commander -> Abraham Lincoln\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAbraham Lincoln -> base.inaugurations.inauguration_speaker.inauguration -> Abraham Lincoln 1865 presidential inauguration\n# Answer:\nAbraham Lincoln 1865 presidential inauguration", "# Reasoning Path:\nAbraham Lincoln -> military.military_commander.military_commands -> m.04h_gx9 -> military.military_command.military_conflict -> American Civil War\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nAbraham Lincoln -> base.inaugurations.inauguration_speaker.inauguration -> Abraham Lincoln 1861 presidential inauguration -> base.inaugurations.inauguration.president -> Abraham Lincoln\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin\n# Answer:\nH Hannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> media_common.dedicatee.dedications -> m.04tl_wn -> media_common.dedication.dedicated_to -> Abraham Lincoln\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin\n# Answer:\nHannibal Hamline", "# Reasoning Path:\nAbraham Lincoln -> people.person.children -> Robert Todd Lincoln\n# Answer:\nRobert Todd Lincoln"], "ground_truth": ["Hannibal Hamlin", "Andrew Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Fezziwig -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> TV Ghost of Christmas Past -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Bob Cratchit -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Edward Murdstone -> book.book_character.appears_in_book -> David Copperfield\n# Answer:\nDavid Copperfield", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Wilkins Micawber -> book.book_character.appears_in_book -> David Copperfield\n# Answer:\nDavid Copperfield", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> award.award_nominee.award_nominations -> m.011lncpm -> award.award_nomination.nominated_for -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Tiny Tim -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Oliver Twist\n# Answer:\nOliver Twist"], "ground_truth": ["A Christmas Carol (Great Stories)", "A Christmas Carol (Penguin Readers, Level 2)", "A Tale of Two Cities (Classics Illustrated Notes)", "The old curiosity shop", "A Christmas Carol (Oxford Bookworms Library)", "A Tale of Two Cities (Masterworks)", "A Tale of Two Cities (Dramatized)", "A Tale of Two Cities (Progressive English)", "A Tale of Two Cities (Barnes & Noble Classics Series)", "A Christmas Carol (Reissue)", "The life and adventures of Nicholas Nickleby", "A Tale of Two Cities (Bookcassette(r) Edition)", "Our mutual friend.", "A Tale of Two Cities (Dover Thrift Editions)", "A Tale of Two Cities (Konemann Classics)", "A CHRISTMAS CAROL", "A Tale of Two Cities (Saddleback Classics)", "A Christmas Carol (Soundings)", "A Christmas Carol (Value Books)", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "A Tale Of Two Cities (Adult Classics in Audio)", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "Great expectations.", "A Christmas Carol (Classics Illustrated)", "A Tale of Two Cities (Classic Retelling)", "A Tale of Two Cities (Classic Literature with Classical Music)", "A Christmas Carol (Pacemaker Classics)", "A Christmas Carol (Radio Theatre)", "A Tale of Two Cities (Cyber Classics)", "David Copperfield.", "A Tale of Two Cities (Student's Novels)", "A Christmas Carol (Illustrated Classics)", "The Pickwick Papers", "A Tale of Two Cities (Silver Classics)", "A Christmas Carol (Saddleback Classics)", "A Christmas Carol (Limited Editions)", "Great expectations", "Bleak House", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "A Tale of Two Cities (Puffin Classics)", "A Tale of Two Cities (The Classic Collection)", "A Tale of Two Cities (Illustrated Classics)", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "Oliver Twist", "A Tale of Two Cities (Everyman's Library (Paper))", "A Tale of Two Cities (Paperback Classics)", "A Tale of Two Cities (Longman Classics, Stage 2)", "A Tale of Two Cities (Classic Fiction)", "A Tale of Two Cities (Dodo Press)", "A Tale of Two Cities (Acting Edition)", "A Christmas Carol (Penguin Student Editions)", "A Tale of Two Cities (Ultimate Classics)", "A Christmas Carol (Watermill Classic)", "A Christmas Carol (Usborne Young Reading)", "A Tale of Two Cities (Clear Print)", "A Tale of Two Cities (Signet Classics)", "A Christmas Carol (Classics for Young Adults and Adults)", "A Christmas Carol (Large Print)", "A Tale of Two Cities (Everyman's Library Classics)", "A Tale of Two Cities (Large Print Edition)", "A Tale of Two Cities (Dramascripts S.)", "A Tale of Two Cities (Bantam Classic)", "Dombey and son", "A Tale of Two Cities (Penguin Popular Classics)", "A Tale of Two Cities (Oxford Bookworms Library)", "A Tale of Two Cities (Macmillan Students' Novels)", "A Christmas Carol (Scholastic Classics)", "A Tale of Two Cities (BBC Audio Series)", "A Christmas Carol (Through the Magic Window Series)", "Great Expectations", "A Tale of Two Cities (Everyman Paperbacks)", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "A Christmas Carol (Children's Theatre Playscript)", "A Christmas Carol (Puffin Classics)", "A Christmas Carol (Dramascripts Classic Texts)", "The Mystery of Edwin Drood", "A Christmas Carol (Green Integer, 50)", "A Tale of Two Cities (Isis Clear Type Classic)", "A Tale Of Two Cities (Adult Classics)", "A Tale of Two Cities (Cover to Cover Classics)", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "A Tale of Two Cities (Prentice Hall Science)", "A Christmas Carol (Nelson Graded Readers)", "A Christmas Carol (Classic Fiction)", "Martin Chuzzlewit", "The mystery of Edwin Drood", "A Tale of Two Cities (Simple English)", "A Christmas Carol (Family Classics)", "A Christmas Carol (Tor Classics)", "A Christmas Carol (Pacemaker Classic)", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "Little Dorrit", "A Christmas Carol (Puffin Choice)", "A Christmas Carol (Dramascripts)", "A Christmas Carol (Clear Print)", "A Tale of Two Cities (Ladybird Children's Classics)", "Our mutual friend", "The cricket on the hearth", "A Christmas Carol (Aladdin Classics)", "A Tale of Two Cities (Penguin Readers, Level 5)", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Wordsworth Classics)", "David Copperfield", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "A Christmas Carol (Read & Listen Books)", "A Tale of Two Cities (Oxford Playscripts)", "A Tale of Two Cities (Adopted Classic)", "The Old Curiosity Shop", "A Tale of Two Cities (Longman Fiction)", "A Christmas Carol. (Lernmaterialien)", "Sketches by Boz", "A Christmas Carol (Chrysalis Children's Classics Series)", "A Christmas Carol (Young Reading Series 2)", "A Christmas Carol (New Longman Literature)", "A Christmas Carol (Classic Collection)", "A Tale of Two Cities (Pacemaker Classics)", "A Tale of Two Cities (Piccolo Books)", "The Pickwick papers", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "A Tale of Two Cities (Collector's Library)", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "Bleak House.", "A Christmas Carol (Watermill Classics)", "Dombey and Son.", "Great Expectations.", "A Tale of Two Cities", "Bleak house", "A Christmas Carol (Everyman's Library Children's Classics)", "A Christmas Carol (Apple Classics)", "A Tale of Two Cities (Soundings)", "A Christmas Carol (Classic, Picture, Ladybird)", "A Christmas Carol", "A Tale of Two Cities (Classics Illustrated)", "A Tale of Two Cities (Enriched Classic)", "A Christmas Carol (Audio Editions)", "A Christmas Carol (Classic Books on Cassettes Collection)", "The old curiosity shop.", "A Christmas Carol (Cover to Cover)", "A Christmas Carol (The Kennett Library)", "Hard times", "A Tale of Two Cities (Naxos AudioBooks)", "A Tale of Two Cities (10 Cassettes)", "A Christmas Carol (Acting Edition)", "A Christmas Carol (Ladybird Children's Classics)", "A Tale of Two Cities (Courage Literary Classics)", "A Christmas Carol (Take Part)", "A Christmas Carol (Cp 1135)", "A Tale of Two Cities (The Greatest Historical Novels)", "A Tale of Two Cities (Illustrated Junior Library)", "A Christmas Carol (Children's Classics)", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "A Tale of Two Cities (40th Anniversary Edition)", "A Christmas Carol (Bantam Classic)", "Dombey and Son", "A Tale of Two Cities (Compact English Classics)", "A Christmas Carol (R)", "A Christmas Carol (Thornes Classic Novels)", "A Tale of Two Cities (Cassette (1 Hr).)", "A Christmas Carol (Whole Story)", "A Tale of Two Cities (Penguin Classics)", "A Tale of Two Cities (Collected Works of Charles Dickens)", "A Christmas Carol (Gollancz Children's Classics)", "A Tale of Two Cities (Tor Classics)", "A Christmas Carol (Ladybird Classics)", "A Christmas Carol (Enriched Classics)", "A Tale of Two Cities (Unabridged Classics)", "A TALE OF TWO CITIES"], "ans_acc": 0.03550295857988166, "ans_hit": 1, "ans_f1": 0.05747126436781609, "ans_precission": 1.0, "ans_recall": 0.029585798816568046, "path_f1": 0.6852589641434264, "path_precision": 1.0, "path_recall": 0.5212121212121212, "path_ans_f1": 0.05747126436781609, "path_ans_precision": 1.0, "path_ans_recall": 0.029585798816568046}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_commander -> Nikita Khrushchev\n# Answer:\nNikita Khrushchev", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_holder -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.office_holder -> Nikita Khrushchev\n# Answer:\nNikita Khrushchev", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf -> military.military_command.military_commander -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.04fvhtr -> military.military_command.military_commander -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0dl79fr -> military.military_command.military_commander -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> location.location.events -> Eastern Front -> base.culturalevent.event.entity_involved -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> military.military_combatant.armed_forces -> Red Army -> organization.organization.founders -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.0c9_j12 -> government.government_position_held.office_holder -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> location.location.events -> Operation Barbarossa -> base.culturalevent.event.entity_involved -> Joseph Stalin\n# Answer:\nJoseph Stalin"], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> base.aareas.schema.administrative_area.administrative_children -> Coamo -> location.location.people_born_here -> Alejandro Garc\u00eda Padilla\n# Answer:\nAlejandro Garc\u00eda Padilla", "# Reasoning Path:\nPuerto Rico -> location.country.official_language -> Spanish Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nPuerto Rico -> location.location.containedby -> North America\n# Answer:\nNorth America", "# Reasoning Path:\nPuerto Rico -> location.location.containedby -> North America -> base.locations.continents.countries_within -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nPuerto Rico -> meteorology.cyclone_affected_area.cyclones -> 1919 Florida Keys hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\nFlorida", "# Reasoning Path:\nPuerto Rico -> base.aareas.schema.administrative_area.administrative_children -> Coamo -> location.location.people_born_here -> Alejandro Garc\u00eda Padilla\n# Answer:\nAlejandro Garc\u00edaPadilla", "# Reasoning Path:\nPuerto Rico -> base.aareas.schema.administrative_area.administrative_children -> Coamo -> location.location.people_born_here -> Alejandro Garc\u00eda Padilla\n# Answer:\nAlejandro Garcia Padilla", "# Reasoning Path:\nPuerto Rico -> base.aareas.schema.administrative_area.administrative_children -> Coamo -> location.location.people_born_here -> Alejandro Garc\u00eda Padilla\n# Answer:\nAlexandro Garc\u00eda Padilla", "# Reasoning Path:\nPuerto Rico -> base.aareas.schema.administrative_area.administrative_children -> Coamo -> location.location.people_born_here -> Alejandro Garc\u00eda Padilla\n# Answer:\nAlejando Garc\u00eda Padilla"], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Cholecalciferol\n# Answer:\nCholecalciferol", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Radiation therapy\n# Answer:\nRadiation therapy", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nChemotherapy", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease.treatments -> Radiation therapy\n# Answer:\nRadiation therapy", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease.treatments -> Surgery\n# Answer:\nSurgery", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nChemotherapy", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Surgery\n# Answer:\nSurgery", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor\n# Answer:\n Brain tumor"], "ground_truth": ["Lung cancer", "Brain tumor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nWilliam Daniels", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nWilliams Daniels", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nWilliam Danielsodia\n# Answer:\nWilliam Danielsodia", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nWiliam Daniels", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nWIlliam Daniels", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nWill Daniels", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nRobert Daniels", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nRichard Daniels", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nEdward Daniels"], "ground_truth": ["William Daniels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.us_state.capital -> Nashville\n# Answer:\nNashville", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County\n# Answer:\n Williamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County\n# Answer:\nWilliams County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County\n# Answer:\nWilkinson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County\n# Answer:\nWilson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County\n# Answer:\nWilliamson Countyodia\n# Answer:\nWilliamson Countyodia", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County\n# Answer:\nWilliamson Countyodia\n# County:\nWilliamson Countyodia"], "ground_truth": ["Williamson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 0.4444444444444444, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.nationality -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGeorge Washington Carver -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_bodies -> U.S. National Park Service\n# Answer:\nU.S. National Park Service", "# Reasoning Path:\nGeorge Washington Carver -> people.deceased_person.place_of_death -> Tuskegee\n# Answer:\nTuskegee", "# Reasoning Path:\nGeorge Washington Carver -> common.topic.notable_types -> Inventor -> people.profession.specialization_of -> Scientist\n# Answer:\nScientist", "# Reasoning Path:\nGeorge Washington Carver -> common.topic.notable_types -> Inventor\n# Answer:\nInventor", "# Reasoning Path:\nGeorge Washington Carver -> people.person.ethnicity -> African American\n# Answer:\nAfrican American", "# Reasoning Path:\nGeorge Washington Carver -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Kansas\n# Answer:\nKansas", "# Reasoning Path:\nGeorge Washington Carver -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Missouri\n# Answer:\nMissouri", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Alabama\n# Answer:\nAlabama"], "ground_truth": ["Diamond"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.parents -> Phyllis Piper\n# Answer:\nPhyllis Piper", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nTracy Pollan", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.0n54r5k -> award.award_nomination.award -> Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Comedy Series\n# Answer:\nScreen Actors Guild Award for Outstanding Performance by a Male Actor in a Comedy Series", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.0n4vp9w -> award.award_nomination.award -> Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Comedy Series\n# Answer:\nScreen Actors Guild Award for Outstanding Performance by a Male Actor in a Comedy Series", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.0n54r5k -> award.award_nomination.award -> Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Comedy Series\n# Answer:\nScreen Actors Guild Award for Outstanding Performance by a MaleActor in a Comedy Series", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.0n54r5k -> award.award_nomination.award -> Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Comedy Series\n# Answer:\nScreen ActorsGuild Award for Outstanding Performance by a Male Actor in a Comedy Series", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.0n54r5k -> award.award_nomination.award_nominee -> Michael J. Fox\n# Answer:\nMichael J. Fox", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.0n54r5k -> award.award_nomination.award -> Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Comedy Series\n# Answer:\n Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Comedy Series", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.0n54r5k -> award.award_nomination.award -> Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Comedy Series\n# Answer:\nScreen Actors Guild Award for Outstanding Performance by aMale Actor in a Comedy Series", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.08__5ql -> award.award_nomination.award -> Golden Globe Award for Best Actor \u2013 Motion Picture Musical or Comedy\n# Answer:\nGolden Globe Award for Best Actor \u2013 Motion Picture Musical or Comedy"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.11111111111111112, "path_precision": 0.1, "path_recall": 0.125, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04yvq68 -> military.military_command.military_conflict -> How Few Remain\n# Answer:\nHow Few Remain", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9q6 -> military.military_command.military_conflict -> Battle of McDowell\n# Answer:\nBattle of McDowell", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9nd -> military.military_command.military_conflict -> Battle of Port Republic\n# Answer:\nBattle of Port Republic", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Chancellorsville\n# Answer:\nBattle of Chancellorsville", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Mexican\u2013American War\n# Answer:\nMexican\u2013American War", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04q5wcf -> military.military_command.military_conflict -> First Battle of Kernstown\n# Answer:\nFirst Battle of Kernstown", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Cedar Mountain\n# Answer:\nBattle of Cedar Mountain", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9sc -> military.military_command.military_conflict -> Battle of Hancock\n# Answer:\nBattle of Hancock", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fvb1_ -> military.military_command.military_conflict -> First Battle of Winchester\n# Answer:\nFirst Battle of Winchester"], "ground_truth": ["First Battle of Kernstown", "Jackson's Valley Campaign", "Manassas Station Operations", "First Battle of Winchester", "Romney Expedition", "Battle of Hancock", "Battle of Port Republic", "Battle of Chantilly", "Battle of White Oak Swamp", "Battle of McDowell", "Battle of Harpers Ferry", "Battle of Chancellorsville", "Battle of Cedar Mountain", "Battle of Hoke's Run", "Second Battle of Bull Run", "How Few Remain", "First Battle of Rappahannock Station", "American Civil War", "Battle of Front Royal"], "ans_acc": 0.47368421052631576, "ans_hit": 1, "ans_f1": 0.6206896551724138, "ans_precission": 0.9, "ans_recall": 0.47368421052631576, "path_f1": 0.48648648648648646, "path_precision": 0.9, "path_recall": 0.3333333333333333, "path_ans_f1": 0.6206896551724138, "path_ans_precision": 0.9, "path_ans_recall": 0.47368421052631576}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language\n# Answer:\nMaaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language\n# Answer:\nMaa Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language\n# Answer:\nMaasai language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language\n# Answer:\n_maasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language\n# Answer:\n.Maasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language\n# Answer:\n \u041c\u0430asai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language\n# Answer:\nMasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language\n# Answer:\nmaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language\n# Answer:\nMaassai Language"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> book.author.book_editions_published -> The Autobiography of Benjamin Franklin -> book.written_work.subjects -> Biography\n# Answer:\nBiography", "# Reasoning Path:\nBenjamin Franklin -> book.author.book_editions_published -> Poor Richard's Almanac of 1753 -> book.book_edition.author_editor -> Benjamin Franklin\n# Answer:\nBenjamin Franklin", "# Reasoning Path:\nBenjamin Franklin -> book.author.book_editions_published -> Poor Richard's Almanac of 1749 -> book.book_edition.author_editor -> Benjamin Franklin\n# Answer:\nBenjamin Franklin", "# Reasoning Path:\nBenjamin Franklin -> book.author.book_editions_published -> Poor Richard's Almanac of 1743 -> book.book_edition.author_editor -> Benjamin Franklin\n# Answer:\nBenjamin Franklin", "# Reasoning Path:\nBenjamin Franklin -> book.author.book_editions_published -> Poor Richard's Almanac of 1748 -> book.book_edition.author_editor -> Benjamin Franklin\n# Answer:\nBenjamin Franklin", "# Reasoning Path:\nBenjamin Franklin -> book.author.book_editions_published -> Poor Richard's Almanac of 1745 -> book.book_edition.author_editor -> Benjamin Franklin\n# Answer:\nBenjamin Franklin", "# Reasoning Path:\nBenjamin Franklin -> book.author.book_editions_published -> Poor Richard's Almanac of 1751 -> book.book_edition.author_editor -> Benjamin Franklin\n# Answer:\nBenjamin Franklin", "# Reasoning Path:\nBenjamin Franklin -> book.author.book_editions_published -> Poor Richard's Almanac of 1741 -> book.book_edition.author_editor -> Benjamin Franklin\n# Answer:\nBenjamin Franklin", "# Reasoning Path:\nBenjamin Franklin -> book.author.book_editions_published -> Poor Richard's Almanac of 1742 -> book.book_edition.author_editor -> Benjamin Franklin\n# Answer:\nBenjamin Franklin"], "ground_truth": ["Deborah Read"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer\n# Answer:\nPancatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer\n# Answer:\nPancreatic Cancer", "# Reasoning Path:\nPatrick Swayze -> music.composer.compositions -> She's Like the Wind -> music.recording.releases -> Dirty Dancing\n# Answer:\nDirty Dancing", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer\n# Answer:\npancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer\n# Answer:\nPanatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer\n# Answer:\nPancreatic cancerodia\n# Answer:\nPancreatic cancerodia", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer\n# Answer:\nPanicatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer\n# Answer:\nPancretic cancer"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 0.4444444444444444, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> High Renaissance -> visual_art.art_period_movement.associated_artworks -> The Last Supper\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> Pastel Art\n# Answer:\nPastel Art", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> St. John the Baptist\n# Answer:\nSt. John the Baptist", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> High Renaissance -> visual_art.art_period_movement.associated_artists -> Michelangelo\n# Answer:\nMichelangelo", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> High Renaissance\n# Answer:\nHigh Renaissance", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> Pastel Art -> visual_art.art_period_movement.associated_artists -> Edgar Degas\n# Answer:\nEdgar Degas"], "ground_truth": ["Leonardo's horse", "Bacchus", "Sala delle Asse", "Lady with an Ermine", "g.1213jb_b", "The Last Supper", "Mona Lisa", "Medusa", "St. Jerome in the Wilderness", "The Battle of Anghiari", "Ginevra de' Benci", "g.12314dm1", "Leda and the Swan", "g.1224tf0c", "Portrait of a Young Fianc\u00e9e", "The Baptism of Christ", "Annunciation", "g.1219sb0g", "Madonna of Laroque", "Vitruvian Man", "Drapery for a Seated Figure", "The Holy Infants Embracing", "Portrait of Isabella d'Este", "Horse and Rider", "Madonna of the Carnation", "St. John the Baptist", "Madonna and Child with St Joseph", "g.121wt37c", "The Virgin and Child with St. Anne", "g.1239jd9p", "Portrait of a Musician", "Head of a Woman", "Benois Madonna", "g.121yh91r", "Virgin of the Rocks", "g.12215rxg", "Lucan portrait of Leonardo da Vinci", "Adoration of the Magi", "The Virgin and Child with St Anne and St John the Baptist", "Madonna Litta", "Portrait of a man in red chalk", "Madonna of the Yarnwinder", "Salvator Mundi", "La belle ferronni\u00e8re", "g.120vt1gz"], "ans_acc": 0.044444444444444446, "ans_hit": 1, "ans_f1": 0.07741935483870968, "ans_precission": 0.3, "ans_recall": 0.044444444444444446, "path_f1": 0.07407407407407407, "path_precision": 0.2, "path_recall": 0.045454545454545456, "path_ans_f1": 0.07741935483870968, "path_ans_precision": 0.3, "path_ans_recall": 0.044444444444444446}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> periodicals.newspaper_circulation_area.newspapers -> Der Standard -> book.newspaper.circulation_areas -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Answer:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Vienna:\nVienna\n# Reasoning Path:\nVienna -> periodicals.newspaper_circulation_area.newspapers -> Kronen Zeitung -> book.newspaper.circulation_areas -> Austria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Answer:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Vienna:\nVienna\n# Reasoning Path:\nVienna -> periodicals.newspaper_circulation_area.newspapers -> Kronen Zeitung -> book.newspaper.circulation_areas -> Austria -> location.country.administrative_divisions -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Answer:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Vienna:\nVienna\n# Reasoning Path:\nVienna -> periodicals.newspaper_circulation_area.newspapers -> Wiener Zeitung -> book.newspaper.circulation_areas -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Answer:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Vienna:\nVienna\n# Reasoning Path:\nVienna -> location.location.containedby -> Austria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Answer:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Vienna:\nVienna\n# Reasoning Path:\nVienna -> location.location.containedby -> Austria\n# Austria:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Answer:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Vienna:\nVienna\n# Reasoning Path:\nVienna -> periodicals.newspaper_circulation_area.newspapers -> Der Standard -> book.newspaper.circulation_areas -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Answer:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Vienna:\nVienna\n# Reasoning Path:\nVienna -> periodicals.newspaper_circulation_area.newspapers -> Kronen Zeitung -> book.newspaper.circulation_areas -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Answer:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Vienna:\nVienna\n# Reasoning Path:\nVienna -> film.film_location.featured_in_films -> All is Forgiven -> film.film.featured_film_locations -> Vienna\n# Answer:\nVienna"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nCancer -> medicine.disease.treatments -> Chemotherapy -> medicine.medical_treatment.used_to_treat -> Ovarian cancer\n# Answer:\nOvarian cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nCancer -> medicine.disease.treatments -> Radiation therapy -> medicine.medical_treatment.used_to_treat -> Ovarian cancer\n# Answer:\nOvarian cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nCancer -> medicine.disease.treatments -> Surgery -> medicine.medical_treatment.used_to_treat -> Ovarian cancer\n# Answer:\nOvarian cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nCancer -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural -> people.cause_of_death.includes_causes_of_death -> Ovarian cancer\n# Answer:\nOvarian cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nAssassination in ways which appear natural", "# Reasoning Path:\nEva Per\u00f3n -> fictional_universe.fictional_character.based_on -> Eva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nCancer -> medicine.disease.treatments -> Chemotherapy -> medicine.medical_treatment.used_to_treat -> Ovarian cancer\n# Question:\nOvarian cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nCancer -> medicine.disease.treatments -> Radiation therapy -> medicine.medical_treatment.used_to_treat -> Ovarian cancer\n# Question:\nOvarian cancer"], "ground_truth": ["Cervical cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism\n# Answer:\nBuddhism", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism -> religion.religion.texts -> Dhammapada\n# Answer:\nDhammapada", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism -> religion.religion.deities -> N\u0101ga\n# Answer:\nN\u0101ga", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu\n# Answer:\nKaplavastu", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism -> religion.religion.deities -> Yaksha\n# Answer:\nYaksha", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism -> religion.religion.deities -> Hariti\n# Answer:\nHariti", "# Reasoning Path:\nGautama Buddha -> people.deceased_person.place_of_death -> Kushinagar\n# Answer:\nKushinagar", "# Reasoning Path:\nGautama Buddha -> people.person.ethnicity -> Shakya\n# Answer:\nShakya"], "ground_truth": ["Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Lightning rod\n# Answer:\nLightning rod", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> Letters of Silence Dogood -> book.literary_series.works_in_this_series -> Silence Dogood, No. 1\n# Answer:\nSilence Dogood, No. 1", "# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> Letters of Silence Dogood -> book.literary_series.works_in_this_series -> Silence Dogood, No. 2\n# Answer:\nSilence Dogood, No. 2", "# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> Letters of Silence Dogood -> book.literary_series.works_in_this_series -> Silence Dogood, No. 12\n# Answer:\nSilence Dogood, No. 12", "# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> Letters of Silence Dogood -> book.literary_series.works_in_this_series -> Silence Dogood, No. 6\n# Answer:\nSilence Dogood, No. 6", "# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> Letters of Silence Dogood -> book.literary_series.works_in_this_series -> Silence Dogood, No. 5\n# Answer:\nSilence Dogood, No. 5", "# Reasoning Path:\nBenjamin Franklin -> book.author.book_editions_published -> Poor Richard's Almanac of 1753 -> book.book_edition.book -> Poor Richard's Almanack\n# Answer:\nPoor Richard's Almanack"], "ground_truth": ["Lightning rod", "Bifocals", "Glass harmonica", "Franklin stove"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Colorado\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.citytown.postal_codes -> 80634\n# Answer:\n80634", "# Reasoning Path:\nGreeley -> location.citytown.postal_codes -> 80632\n# Answer:\n80632", "# Reasoning Path:\nGreeley -> location.citytown.postal_codes -> 80631\n# Answer:\n80631", "# Reasoning Path:\nGreeley -> location.citytown.postal_codes -> 80638\n# Answer:\n80638", "# Reasoning Path:\nGreeley -> location.location.contains -> Aims Community College -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGreeley -> location.citytown.postal_codes -> 80633\n# Answer:\n80633"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> common.topic.notable_types -> Composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> music.artist.genre -> Classical music\n# Answer:\nClassical music", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> music.artist.genre -> Opera\n# Answer:\nOpera", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> music.artist.genre -> Art song\n# Answer:\nArt song", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician\n# Answer:\n_musician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician\n# Answer:\nMusicalian", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician\n# Answer:\n_musician_"], "ground_truth": ["Librettist", "Composer", "Musician"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Belgium\n# Answer:\nBelgium", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Switzerland\n# Answer:\nSwitzerland", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nAustria", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Luxembourg\n# Answer:\nLuxembourg", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> West Germany\n# Answer:\nWest Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Czech Republic\n# Answer:\nCzech Republic", "# Reasoning Path:\nGerman Language -> language.human_language.main_country -> Germany\n# Answer:\nGermany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Vatican City\n# Answer:\nVatican City"], "ground_truth": ["Belgium", "Luxembourg", "Switzerland", "East Germany", "Germany", "Austria", "Liechtenstein"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Psychedelic rock\n# Answer:\nPsychedelic rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental rock\n# Answer:\nExperimental rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop rock\n# Answer:\nPop rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Rock music\n# Answer:\nRock music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Art rock\n# Answer:\nArt rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Blues rock\n# Answer:\nBlues rock"], "ground_truth": ["Blues rock", "Pop music", "Pop rock", "Psychedelic rock", "Rock music", "Soft rock", "Experimental rock", "Experimental music", "Art rock"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> government.government_position_held.office_holder -> Michael Bennet\n# Answer:\nMichael Bennet", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.office_holder -> Mark Udall\n# Answer:\nMark Udall", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008 -> government.election.winner -> Mark Udall\n# Answer:\nMark Udall", "# Reasoning Path:\nColorado -> government.governmental_jurisdiction.governing_officials -> m.04krcwb -> government.government_position_held.office_holder -> Bill Ritter\n# Answer:\nBill Ritter", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> government.government_position_held.office_holder -> Michael Bennet\n# Answer:\nMichael Bennett", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.office_holder -> Mark Udall\n# Answer:\nMark Udally", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.office_holder -> Mark Udall\n# Answer:\nMark UDall", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> government.government_position_held.office_holder -> Michael Bennet\n# Answer:\nMichael Bennert", "# Reasoning Path:\nColorado -> government.governmental_jurisdiction.governing_officials -> m.04krcwb -> government.government_position_held.office_holder -> Bill Ritter\n# Answer:\nBill Ritters", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.office_holder -> Mark Udall\n# Answer:\nMark Udalls"], "ground_truth": ["Michael Bennet", "Mark Udall"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> government.governmental_jurisdiction.governing_officials -> m.010r2fjz -> government.government_position_held.office_holder -> Margrethe II of Denmark\n# Answer:\nMargrethe II of Denmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries\n# Answer:\nNordic countries", "# Reasoning Path:\nGreenland -> base.aareas.schema.administrative_area.administrative_parent -> Kingdom of Denmark\n# Answer:\nKingdom of Denmark", "# Reasoning Path:\nGreenland -> location.country.languages_spoken -> Danish Language -> language.human_language.countries_spoken_in -> Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.country.currency_used -> Danish krone -> finance.currency.countries_used -> Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> North America\n# Answer:\nNorth America", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Greenland\n# Answer:\nGreenland", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark\n# Answer:\nDanmark", "# Reasoning Path:\nGreenland -> government.governmental_jurisdiction.government_bodies -> Parliament of Greenland -> government.governmental_body.jurisdiction -> Greenland\n# Answer:\nGreenland"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98104\n# Answer:\n98104", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98108\n# Answer:\n98108", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98103\n# Answer:\n98103", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98105\n# Answer:\n98105", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98107\n# Answer:\n98107", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98118\n# Answer:\n98118", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98109\n# Answer:\n98109", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98146\n# Answer:\n98146", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98106\n# Answer:\n98106", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101\n# Answer:\n98101"], "ground_truth": ["98199", "98154", "98113", "98114", "98191", "98131", "98188", "98125", "98119-4114", "98177", "98112", "98168", "98166", "98133", "98121", "98158", "98194", "98134", "98190", "98174", "98124", "98141", "98185", "98126", "98122", "98117", "98164", "98111", "98139", "98103", "98129", "98175", "98195", "98155", "98104", "98136", "98101", "98119", "98198", "98178", "98105", "98108", "98144", "98115", "98118", "98184", "98165", "98170", "98161", "98145", "98181", "98148", "98132", "98138", "98146", "98171", "98102", "98107", "98106", "98109", "98116", "98160", "98127"], "ans_acc": 0.15873015873015872, "ans_hit": 1, "ans_f1": 0.27397260273972607, "ans_precission": 1.0, "ans_recall": 0.15873015873015872, "path_f1": 0.27397260273972607, "path_precision": 1.0, "path_recall": 0.15873015873015872, "path_ans_f1": 0.27397260273972607, "path_ans_precision": 1.0, "path_ans_recall": 0.15873015873015872}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> symbols.namesake.named_after -> Will Smith\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B -> broadcast.genre.content -> SoulfulHipHop.com Radio\n# Answer:\nSoulfulHipHop.com Radio", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Pop music -> base.musicpf.genre.artist -> Yves Bole\n# Answer:\nYves Bole", "# Reasoning Path:\nWillow Smith -> people.person.nationality -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nWillow Smith -> symbols.namesake.named_after -> Will Smith -> people.person.children -> Willow Smith\n# Answer:\nWillow Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Pop music -> music.genre.parent_genre -> Dance music\n# Answer:\nDance music", "# Reasoning Path:\nWillow Smith -> people.person.ethnicity -> African American\n# Answer:\nAfrican American", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B -> broadcast.genre.content -> SoulfulHipHop.com Radio\n# Answer:\nSoulfulHiphop.com Radio", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B -> broadcast.genre.content -> SoulfulHipHop.com Radio\n# Answer:\nSoulfulHip Hop.com Radio"], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Vietnam -> location.country.official_language -> Vietnamese Language\n# Answer:\nVietnamese Language", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> China -> location.country.official_language -> Standard Chinese\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Singapore -> location.country.official_language -> Standard Chinese\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Singapore -> location.country.official_language -> Tamil Language\n# Answer:\nTamil Language", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Traditional Chinese characters\n# Answer:\nTraditional Chinese characters", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Vietnam -> location.country.form_of_government -> Socialist state\n# Answer:\nSocialist state", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.country.languages_spoken -> Malay Language\n# Answer:\nMalay Language", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Malaysia -> location.country.languages_spoken -> Tamil Language\n# Answer:\nTamil Language", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Malaysia -> location.country.languages_spoken -> Chinese, Hakka Language\n# Answer:\nChinese, Hakka Language"], "ground_truth": ["Simplified Chinese character", "N\u00fcshu script", "Traditional Chinese characters", "Chinese characters", "'Phags-pa script"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.26666666666666666, "ans_precission": 0.2, "ans_recall": 0.4, "path_f1": 0.26666666666666666, "path_precision": 0.2, "path_recall": 0.4, "path_ans_f1": 0.26666666666666666, "path_ans_precision": 0.2, "path_ans_recall": 0.4}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> government.political_appointer.appointees -> m.04lmqy7 -> government.government_position_held.office_holder -> Henry Kissinger\n# Answer:\nHenry Kissinger", "# Reasoning Path:\nRichard Nixon -> government.political_appointer.appointees -> m.09tbpwp -> government.government_position_held.office_holder -> William P. Rogers\n# Answer:\nWilliam P. Rogers", "# Reasoning Path:\nRichard Nixon -> government.political_appointer.appointees -> m.04lmqx_ -> government.government_position_held.office_holder -> Henry Kissinger\n# Answer:\nHenry Kissinger", "# Reasoning Path:\nRichard Nixon -> government.politician.government_positions_held -> m.04sgdbj -> government.government_position_held.office_position_or_title -> Vice President of the United States\n# Answer:\nVice President of the United States", "# Reasoning Path:\nRichard Nixon -> book.book_subject.works -> Nixon and Kissinger: Partners in Power -> book.written_work.subjects -> Henry Kissinger\n# Answer:\nHenry Kissinger", "# Reasoning Path:\nRichard Nixon -> government.politician.government_positions_held -> m.04sgdbj -> government.government_position_held.office_holder -> Richard Nixon\n# Answer:\nRichard Nixon", "# Reasoning Path:\nRichard Nixon -> book.book_subject.works -> Years of Renewal: The Concluding Volume of his Memoirs -> book.written_work.author -> Henry Kissinger\n# Answer:\nHenry Kissinger", "# Reasoning Path:\nRichard Nixon -> government.politician.government_positions_held -> m.04sgdbj -> government.government_position_held.office_position_or_title -> Vice President of the United States\n# Answer:\nvice President of the United States", "# Reasoning Path:\nRichard Nixon -> government.politician.government_positions_held -> m.04sgdbj -> government.government_position_held.office_position_or_title -> Vice President of the United States\n# Answer:\n Vice President of the United States", "# Reasoning Path:\nRichard Nixon -> government.politician.government_positions_held -> m.04sgdbj -> government.government_position_held.office_position_or_title -> Vice President of the United States\n# Answer:\nVicePresident of the United States"], "ground_truth": ["Pat Nixon"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_y5jrl -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkdh -> tv.regular_tv_appearance.actor -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0zbmbwz -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_tlrg2 -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0_tlrff -> award.award_honor.award_winner -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.091nx8n -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_nq -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs"], "ground_truth": ["Marla Gibbs", "Sherman Hemsley", "Berlinda Tolbert", "Franklin Cover", "Roxie Roker", "Jay Hammer", "Zara Cully", "Paul Benedict", "Mike Evans", "Isabel Sanford", "Damon Evans"], "ans_acc": 0.09090909090909091, "ans_hit": 1, "ans_f1": 0.16666666666666669, "ans_precission": 1.0, "ans_recall": 0.09090909090909091, "path_f1": 0.2898550724637681, "path_precision": 1.0, "path_recall": 0.1694915254237288, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 1.0, "path_ans_recall": 0.09090909090909091}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Business Times\n# Answer:\nSan Francisco Business Times", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Foghorn\n# Answer:\nSan Francisco Foghorn", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Times\n# Answer:\nSan Francisco Bay Times", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay View\n# Answer:\nSan Francisco Bay View", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Daily\n# Answer:\nSan Francisco Daily", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> The Golden Era\n# Answer:\nThe Golden Era", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Sing Tao Daily\n# Answer:\nSing Tao Daily", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Synapse\n# Answer:\nSynapse", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Street Sheet\n# Answer:\nStreet Sheet"], "ground_truth": ["San Francisco News-Call Bulletin Newspaper", "San Francisco Daily", "The Daily Alta California", "San Francisco Bay Guardian", "California Star", "San Francisco Bay Times", "Bay Area Reporter", "Sing Tao Daily", "The San Francisco Examiner", "The Golden Era", "San Francisco Chronicle", "San Francisco Business Times", "San Francisco Call", "San Francisco Bay View", "Street Sheet", "AsianWeek", "Dock of the Bay", "Synapse", "Free Society", "San Francisco Foghorn"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Asia\n# Answer:\nAsia", "# Reasoning Path:\nArmenia -> location.location.containedby -> Eurasia\n# Answer:\nEurasia", "# Reasoning Path:\nArmenia -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Georgia\n# Answer:\nGeorgia", "# Reasoning Path:\nArmenia -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Georgia\n# Answer:\nGeorgia", "# Reasoning Path:\nArmenia -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> Azerbaijan\n# Answer:\nAzerbaijan", "# Reasoning Path:\nArmenia -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Azerbaijan\n# Answer:\nAzerbaijan", "# Reasoning Path:\nArmenia -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nArmenia -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nArmenia -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics\n# Answer:\n2014 Winter Olympics"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.symptom.symptom_of -> Systemic lupus erythematosus\n# Answer:\nSystemic lupus erythematosus", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.causes -> Cocaine\n# Answer:\nCocaine", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack\n# Answer:\nHeart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.causes -> Aortic dissection\n# Answer:\nAortic dissection", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.causes -> Aneurysm\n# Answer:\nAneurysm", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.causes -> Atherosclerosis\n# Answer:\nAtherosclerosis", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack\n# Answer:\n\u5fc3 attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack\n# Answer:\nheart Attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.symptom.symptom_of -> Systemic lupus erythematosus\n# Answer:\nSystemic lupus erythematosis"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin, Volume 1: 1821-1836", "# Reasoning Path:\nCharles Darwin -> people.person.quotations -> There is grandeur in this view of life, with its several powers, having been originally breathed into a few forms or into one; and that, whilst this planet has gone cycling on according to the fixed law of gravity, from so simple a beginning endless forms most beautiful and most wonderful have been, and are being, evolved. -> media_common.quotation.source -> On the origin of species by means of natural selection\n# Answer:\nOn the origin of species by means of natural selection", "# Reasoning Path:\nCharles Darwin -> base.concepts.concept_developer.concepts_developed -> Natural selection -> book.book_subject.works -> On the origin of species by means of natural selection\n# Answer:\nOn the origin of species by means of natural selection", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 6: 1856-1857\n# Answer:\nThe Correspondence of Charles Darwin, Volume 6: 1856-1857", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 12: 1864\n# Answer:\nThe Correspondence of Charles Darwin, Volume 12: 1864", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 3: 1844-1846\n# Answer:\nThe Correspondence of Charles Darwin, Volume 3: 1844-1846", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 7: 1858-1859\n# Answer:\nThe Correspondence of Charles Darwin, Volume 7: 1858-1859", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 4: 1847-1850\n# Answer:\nThe Correspondence of Charles Darwin, Volume 4: 1847-1850", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 2: 1837-1843\n# Answer:\nThe Correspondence of Charles Darwin, Volume 2: 1837-1843", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin -> book.author.works_written -> The Autobiography of Charles Darwin\n# Answer:\nThe Autobiography of Charles Darwin"], "ground_truth": ["Questions about the breeding of animals", "Opsht\u0323amung fun menshen", "El Origin De Las Especies", "Darwin en Patagonia", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "The Power of Movement in Plants", "Darwin's journal", "Darwin for Today", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "Wu zhong qi yuan", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The Expression of the Emotions in Man and Animals", "La vie et la correspondance de Charles Darwin", "The Correspondence of Charles Darwin, Volume 13: 1865", "Die fundamente zur entstehung der arten", "Les moyens d'expression chez les animaux", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Correspondence of Charles Darwin, Volume 16: 1868", "A student's introduction to Charles Darwin", "Evolutionary Writings: Including the Autobiographies", "The Correspondence of Charles Darwin, Volume 14: 1866", "Evolution", "On the origin of species by means of natural selection", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "The Formation of Vegetable Mould through the Action of Worms", "South American Geology", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "Del Plata a Tierra del Fuego", "The action of carbonate of ammonia on the roots of certain plants", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Diary of the voyage of H.M.S. Beagle", "Cartas de Darwin 18251859", "Diario del Viaje de Un Naturalista Alrededor", "Darwin Compendium", "The Correspondence of Charles Darwin, Volume 9: 1861", "Human nature, Darwin's view", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "The Variation of Animals and Plants under Domestication", "Fertilisation of Orchids", "Darwin's notebooks on transmutation of species", "Geological Observations on South America", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The principal works", "Part I: Contributions to the Theory of Natural Selection / Part II", "More Letters of Charles Darwin", "The Correspondence of Charles Darwin, Volume 15: 1867", "Kleinere geologische Abhandlungen", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Beagle letters", "A Darwin Selection", "On the Movements and Habits of Climbing Plants", "Rejse om jorden", "The living thoughts of Darwin", "vari\u00eberen der huisdieren en cultuurplanten", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Charles Darwin on the routes of male humble bees", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The Correspondence of Charles Darwin, Volume 8: 1860", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "Darwin-Wallace", "Darwin's Ornithological notes", "Motsa ha-minim", "Tesakneri tsagume\u030c", "Works", "The Darwin Reader First Edition", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "The Orgin of Species", "Charles Darwin's letters", "Darwinism stated by Darwin himself", "Insectivorous Plants", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The Autobiography of Charles Darwin", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Reise um die Welt 1831 - 36", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Proiskhozhdenie vidov", "On Natural Selection", "Darwin's insects", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "The geology of the voyage of H.M.S. Beagle", "Voyage d'un naturaliste autour du monde", "On evolution", "ontstaan der soorten door natuurlijke teeltkeus", "The portable Darwin", "The Correspondence of Charles Darwin, Volume 12: 1864", "Charles Darwin's marginalia", "Memorias y epistolario i\u0301ntimo", "The Descent of Man, and Selection in Relation to Sex", "The\u0301orie de l'e\u0301volution", "Geological Observations on the Volcanic Islands", "From so simple a beginning", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Het uitdrukken van emoties bij mens en dier", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "Les mouvements et les habitudes des plantes grimpantes", "The Life and Letters of Charles Darwin Volume 2", "Resa kring jorden", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "The Correspondence of Charles Darwin, Volume 11: 1863", "The Essential Darwin", "Darwin on humus and the earthworm", "Darwin Darwin", "The education of Darwin", "H.M.S. Beagle in South America", "Darwin and Henslow", "Evolution and natural selection", "Gesammelte kleinere Schriften", "The Correspondence of Charles Darwin, Volume 10: 1862", "The voyage of Charles Darwin", "The Darwin Reader Second Edition", "La facult\u00e9 motrice dans les plantes", "Origins", "From Darwin's unpublished notebooks", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "Darwin from Insectivorous Plants to Worms", "The Correspondence of Charles Darwin, Volume 18: 1870", "Charles Darwin", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "On the tendency of species to form varieties", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "The Different Forms of Flowers on Plants of the Same Species", "Notebooks on transmutation of species", "On a remarkable bar of sandstone off Pernambuco", "The collected papers of Charles Darwin", "The Structure and Distribution of Coral Reefs", "Les r\u00e9cifs de corail, leur structure et leur distribution", "red notebook of Charles Darwin", "The Life of Erasmus Darwin", "To the members of the Down Friendly Club", "Reise eines Naturforschers um die Welt", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "Metaphysics, Materialism, & the evolution of mind", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "Darwin", "monograph on the sub-class Cirripedia", "Die geschlechtliche Zuchtwahl", "genese\u014ds t\u014dn eid\u014dn", "Charles Darwin's natural selection", "The foundations of the Origin of species", "Notes on the fertilization of orchids", "The Correspondence of Charles Darwin, Volume 17: 1869", "Evolution by natural selection", "Leben und Briefe von Charles Darwin", "Volcanic Islands", "The Voyage of the Beagle", "The Life and Letters of Charles Darwin Volume 1"], "ans_acc": 0.08496732026143791, "ans_hit": 1, "ans_f1": 0.13414634146341461, "ans_precission": 1.0, "ans_recall": 0.0718954248366013, "path_f1": 0.5789473684210525, "path_precision": 1.0, "path_recall": 0.4074074074074074, "path_ans_f1": 0.1566265060240964, "path_ans_precision": 1.0, "path_ans_recall": 0.08496732026143791}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> film.film_subject.films -> All the President's Men -> film.film.featured_film_locations -> Washington, D.C.\n# Answer:\nWashington, D.C.", "# Reasoning Path:\nRichard Nixon -> film.film_subject.films -> Dick -> film.film.featured_film_locations -> Washington, D.C.\n# Answer:\nWashington, D.C.", "# Reasoning Path:\nRichard Nixon -> film.film_subject.films -> Nixon -> film.film.featured_film_locations -> Washington, D.C.\n# Answer:\nWashington, D.C.", "# Reasoning Path:\nRichard Nixon -> government.political_appointer.appointees -> m.04lmqx_ -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nRichard Nixon -> government.political_appointer.appointees -> m.09tbpwp -> government.government_position_held.office_holder -> William P. Rogers\n# Answer:\nWilliam P. Rogers", "# Reasoning Path:\nRichard Nixon -> government.politician.government_positions_held -> m.04sgdbj -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nRichard Nixon -> government.political_appointer.appointees -> m.04lmqy7 -> government.government_position_held.office_holder -> Henry Kissinger\n# Answer:\nHenry Kissinger", "# Reasoning Path:\nRichard Nixon -> government.political_appointer.appointees -> m.09tbpwp -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nRichard Nixon -> book.book_subject.works -> Years of Renewal: The Concluding Volume of his Memoirs -> book.written_work.author -> Henry Kissinger\n# Answer:\nHenry Kissinger"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> music.artist.album -> Smokey\n# Answer:\nSmokey", "# Reasoning Path:\nSmokey Robinson -> music.artist.genre -> Rhythm and blues\n# Answer:\nRhythm and blues", "# Reasoning Path:\nSmokey Robinson -> music.group_member.instruments_played -> Piano\n# Answer:\nPiano", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> Don't Look Back\n# Answer:\nDon't Look Back", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> My Girl\n# Answer:\nMy Girl", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> My Guy\n# Answer:\nMy Guy", "# Reasoning Path:\nSmokey Robinson -> music.artist.album -> Smokey\n# Answer:\nSmookey", "# Reasoning Path:\nSmokey Robinson -> music.artist.album -> Smokey\n# Answer:\nSmoky"], "ground_truth": ["We've Saved The Best For Last (Kenny G with Smokey Robinson)", "You've Really Go a Hold on Me", "I Am I Am", "The Family Song", "If You Wanna Make Love (Come 'round Here)", "Come by Here (Kum Ba Ya)", "One Heartbeat", "The Agony and the Ecstasy", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "Christmas Greeting", "Jasmin", "Coincidentally", "And I Don't Love You (Larry Levan instrumental dub)", "Tracks Of My Tears (Live)", "Te Quiero Como Si No Hubiera Un Manana", "I Want You Back", "Can't Fight Love", "Blame It On Love (Duet with Barbara Mitchell)", "Season's Greetings from Smokey Robinson", "Going to a Go-Go", "Shop Around", "No\u00ebl", "Blame It on Love", "Who's Sad", "Please Don't Take Your Love (feat. Carlos Santana)", "Let Your Light Shine On Me", "(It's The) Same Old Love", "Virgin Man", "Take Me Through The Night", "Why", "I Am, I Am", "Jingle Bells", "And I Don't Love You", "Yes It's You Lady", "My World", "Tea for Two", "Keep Me", "Tears of a Clown", "Ever Had A Dream", "Theme From the Big Time", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "Crusin", "Vitamin U", "Time After Time", "Will You Love Me Tomorrow?", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "It's Time to Stop Shoppin' Around", "The Tracks of My Tears", "A Silent Partner in a Three-Way Love Affair", "Love Is The Light", "A Child Is Waiting", "I Can't Give You Anything but Love", "Fly Me to the Moon (In Other Words)", "Rack Me Back", "I Know You by Heart", "The Tears of a Clown", "Be Who You Are", "You're the One for Me (feat. Joss Stone)", "Mickey's Monkey", "Love Letters", "Just to See Her", "I've Made Love to You a Thousand Times", "Just To See Her Again", "What's Too Much", "Holly", "You Are So Beautiful (feat. Dave Koz)", "Santa Claus is Coming to Town", "It's a Good Feeling", "Heavy On Pride (Light On Love)", "It's Her Turn to Live", "A Tattoo", "Deck the Halls", "Wishful Thinking", "Come to Me Soon", "Sleepless Nights", "You Are Forever", "Cruisin'", "Mother's Son", "Photograph in My Mind", "With Your Love Came", "Away in the Manger / Coventry Carol", "I Hear The Children Singing", "The Road to Damascus", "Love Brought Us Here", "Did You Know (Berry's Theme)", "The Tracks Of My Tears", "It's Christmas Time", "She's Only a Baby Herself", "When A Woman Cries", "I Like Your Face", "Our Love Is Here to Stay", "Happy (Love Theme From Lady Sings the Blues)", "Fulfill Your Need", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "Shoe Soul", "You Made Me Feel Love", "Be Careful What You Wish For", "We Are The Warriors", "Tears of a Sweet Free Clown", "Wedding Song", "Open", "Jesus Told Me To Love You", "More Love", "There Will Come A Day ( I'm Gonna Happen To You )", "No Time to Stop Believing", "Winter Wonderland", "Medley: Never My Love / Never Can Say Goodbye", "I Can\u2019t Stand to See You Cry (Commercial version)", "You Don't Know What It's Like", "Food For Thought", "Close Encounters of the First Kind", "Asleep on My Love", "Tell Me Tomorrow", "Walk on By", "Driving Thru Life in the Fast Lane", "Gone Forever", "I Can't Get Enough", "Don't Play Another Love Song", "I Love The Nearness Of You", "Christmas Everyday", "The Tracks of My Heart", "When Smokey Sings Tears Of A Clown", "Be Kind To The Growing Mind (with The Temptations)", "There Will Come a Day (I'm Gonna Happen to You)", "Daylight & Darkness", "My Girl", "I'm in the Mood for Love", "Standing On Jesus", "Love Don't Give No Reason", "Everything You Touch", "Going to a Gogo", "Wanna Know My Mind", "Save Me", "He Can Fix Anything", "Just a Touch Away", "More Than You Know", "Tell Me Tomorrow, Part 1", "We've Saved the Best for Last", "I Have Prayed On It", "Hold on to Your Love", "Girl I'm Standing There", "I Second That Emotions", "Don't Wanna Be Just Physical", "Unless You Do It Again", "Really Gonna Miss You", "Get Ready", "Why Do Happy Memories Hurt So Bad", "Love Don' Give No Reason (12 Inch Club Mix)", "In My Corner", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "Nearness of You", "I Can't Find", "Why Are You Running From My Love", "Whatcha Gonna Do", "Just Another Kiss", "Same Old Love", "Pops, We Love You", "You're Just My Life (feat. India.Arie)", "Tell Me Tomorrow (12\\\" extended mix)", "Crusin'", "The Way You Do (The Things You Do)", "You Go to My Head", "Being With You", "So Bad", "Quiet Storm", "Will You Still Love Me Tomorrow", "I\u2019ve Got You Under My Skin", "Gang Bangin'", "Little Girl Little Girl", "Be Kind to the Growing Mind", "Bad Girl", "Tracks of My Tears", "The Christmas Song", "Yester Love", "It's Fantastic", "Noel", "Never My Love / Never Can Say Goodbye", "I Love Your Face", "And I Love Her", "I'm Glad There Is You", "Double Good Everything", "I Second That Emotion", "Time Flies", "Love Bath", "Ebony Eyes", "Just Passing Through", "Fallin'", "Ebony Eyes (Duet with Rick James)", "I've Got You Under My Skin", "The Hurt's On You", "If You Can Want", "Ain't That Peculiar", "Hanging on by a Thread", "I Care About Detroit", "Sweet Harmony", "You Take Me Away", "Night and Day", "I'll Keep My Light In My Window", "Love So Fine", "The Track of My Tears", "You've Really Got a Hold on Me", "Rewind", "If You Want My Love", "One Time", "Be Careful What You Wish For (instrumental)", "Little Girl, Little Girl", "As You Do", "Baby That's Backatcha", "Will You Love Me Tomorrow", "We\u2019ve Come Too Far to End It Now", "Tracks of my Tears", "Baby Come Close", "Tears Of A Clown", "The Tears Of A Clown", "Please Come Home for Christmas", "That Place", "The Tracks of My Tears (live)", "Christmas Every Day", "I Praise & Worship You Father", "I've Made Love To You A Thousand Times", "Because of You It's the Best It's Ever Been", "The Love Between Me and My Kids", "Ooh Baby Baby", "It's A Good Night", "Speak Low", "Melody Man", "Everything for Christmas", "Quiet Storm (single version)", "Share It", "If You Wanna Make Love", "Satisfy You", "Train of Thought", "Quiet Storm (Groove Boutique remix)", "You Really Got a Hold on Me", "Just Like You", "Just My Soul Responding", "Ooo Baby Baby (live)", "Love' n Life", "The Agony And The Ecstasy", "You Cannot Laugh Alone", "Quiet Storm (Groove Boutique Chill Jazz mix)", "Girlfriend", "Don't Know Why", "Pops, We Love You (disco)", "Let Me Be The Clock", "Skid Row", "God Rest Ye Merry Gentlemen", "Easy", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "Aqui Con Tigo (Being With You)", "Some People Will Do Anything for Love", "Ooo Baby Baby", "Cruisin", "My Guy", "Going to a Go Go", "Let Me Be the Clock"], "ans_acc": 0.0078125, "ans_hit": 1, "ans_f1": 0.015151515151515152, "ans_precission": 0.25, "ans_recall": 0.0078125, "path_f1": 0.07567567567567568, "path_precision": 0.25, "path_recall": 0.044585987261146494, "path_ans_f1": 0.015151515151515152, "path_ans_precision": 0.25, "path_ans_recall": 0.0078125}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> base.kwebbase.kwtopic.connections_from -> john fitzgerald kennedy allegedly assassinated by lee harvey oswald -> base.kwebbase.kwconnection.other -> Lee Harvey Oswald\n# Answer:\nLee Harvey Oswald", "# Reasoning Path:\nJohn F. Kennedy -> base.kwebbase.kwtopic.connections_to -> lee harvey oswald allegedly assassinated john fitzgerald kennedy -> base.kwebbase.kwconnection.subject -> Lee Harvey Oswald\n# Answer:\nLee Harvey Oswald", "# Reasoning Path:\nJohn F. Kennedy -> film.film_subject.films -> Thirteen Days -> film.film.subjects -> Robert F. Kennedy\n# Answer:\nRobert F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Joseph P. Kennedy, Sr. -> people.person.children -> Robert F. Kennedy\n# Answer:\nRobert F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.family_member.family -> Kennedy family -> people.family.members -> Robert F. Kennedy\n# Answer:\nRobert F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr.\n# Answer:\nJohn F. Kennedy Jr.", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson\n# Answer:\nLyon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> book.book_subject.works -> The Making of the President, 1960 -> book.written_work.subjects -> Richard Nixon\n# Answer:\nRichard Nixon", "# Reasoning Path:\nJohn F. Kennedy -> base.kwebbase.kwtopic.connections_from -> john fitzgerald kennedy allegedly assassinated by lee harvey oswald -> base.kwebbase.kwconnection.other -> Lee Harvey Oswald\n# Answer:\n Lee Harvey Oswald"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Hungarian language\n# Answer:\nHungarian language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language\n# Answer:\nAlbanian language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Torlakian dialect\n# Answer:\nTorlakian dialect", "# Reasoning Path:\nSerbia -> location.country.currency_used -> Serbian dinar\n# Answer:\nSerbian dinar", "# Reasoning Path:\nSerbia -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Bosnia and Herzegovina\n# Answer:\nBosnia and Herzegovina", "# Reasoning Path:\nSerbia -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Slovenia\n# Answer:\nSlovenia", "# Reasoning Path:\nSerbia -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Croatia\n# Answer:\nCroatia", "# Reasoning Path:\nSerbia -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Republic of Kosovo\n# Answer:\nRepublic of Kosovo", "# Reasoning Path:\nSerbia -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Austria\n# Answer:\nAustria"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
