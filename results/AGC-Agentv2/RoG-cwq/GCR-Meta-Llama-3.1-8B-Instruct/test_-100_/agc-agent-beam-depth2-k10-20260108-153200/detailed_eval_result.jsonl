{"id": "WebQTest-100_524908899a8aa334a18a0ac00f8f2fe6", "prediction": ["# Reasoning Path:\nNord-Est Department -> location.administrative_division.first_level_division_of -> Haiti -> location.country.languages_spoken -> Haitian Creole\n# Answer:\nHaiti", "# Reasoning Path:\nNord-Est Department -> location.administrative_division.first_level_division_of -> Haiti -> location.country.languages_spoken -> French\n# Answer:\nHaiti", "# Reasoning Path:\nNord-Est Department -> location.administrative_division.first_level_division_of -> Haiti -> location.country.official_language -> Haitian Creole\n# Answer:\nHaiti", "# Reasoning Path:\nNord-Est Department -> location.administrative_division.first_level_division_of -> Haiti -> location.country.official_language -> French\n# Answer:\nHaiti", "# Reasoning Path:\nNord-Est Department -> location.administrative_division.first_level_division_of -> Haiti -> location.location.containedby -> Americas\n# Answer:\nHaiti", "# Reasoning Path:\nNord-Est Department -> location.administrative_division.first_level_division_of -> Haiti -> location.location.containedby -> Greater Antilles\n# Answer:\nHaiti", "# Reasoning Path:\nNord-Est Department -> base.aareas.schema.administrative_area.administrative_area_type -> Haitian department -> type.type.domain -> Administrative Areas\n# Answer:\nHaitian department", "# Reasoning Path:\nNord-Est Department -> location.administrative_division.first_level_division_of -> Haiti -> location.location.containedby -> Latin America\n# Answer:\nHaiti", "# Reasoning Path:\nNord-Est Department -> location.location.contains -> Capotille -> location.location.containedby -> Haiti\n# Answer:\nCapotille", "# Reasoning Path:\nNord-Est Department -> base.aareas.schema.administrative_area.administrative_area_type -> Haitian department -> base.aareas.schema.administrative_area_type.pertains_to -> Haiti\n# Answer:\nHaitian department"], "ground_truth": ["French", "Haitian Creole"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-100_bf66fd89b6e9fc5fcb96c8b3f7a0e616", "prediction": ["# Reasoning Path:\nNord-Ouest Department -> location.administrative_division.first_level_division_of -> Haiti -> location.country.languages_spoken -> French\n# Answer:\nHaiti", "# Reasoning Path:\nNord-Ouest Department -> location.administrative_division.first_level_division_of -> Haiti -> location.country.official_language -> French\n# Answer:\nHaiti", "# Reasoning Path:\nNord-Ouest Department -> location.administrative_division.first_level_division_of -> Haiti -> location.country.languages_spoken -> Haitian Creole\n# Answer:\nHaiti", "# Reasoning Path:\nNord-Ouest Department -> location.administrative_division.first_level_division_of -> Haiti -> location.country.official_language -> Haitian Creole\n# Answer:\nHaiti", "# Reasoning Path:\nNord-Ouest Department -> location.administrative_division.first_level_division_of -> Haiti -> location.location.containedby -> North America\n# Answer:\nHaiti", "# Reasoning Path:\nNord-Ouest Department -> location.administrative_division.first_level_division_of -> Haiti -> location.location.containedby -> Americas\n# Answer:\nHaiti", "# Reasoning Path:\nNord-Ouest Department -> common.topic.notable_types -> Administrative Division -> type.type.expected_by -> Administrative Divisions\n# Answer:\nAdministrative Division", "# Reasoning Path:\nNord-Ouest Department -> common.topic.notable_types -> Administrative Division -> type.type.domain -> Location\n# Answer:\nAdministrative Division", "# Reasoning Path:\nNord-Ouest Department -> location.administrative_division.first_level_division_of -> Haiti -> location.location.containedby -> Greater Antilles\n# Answer:\nHaiti", "# Reasoning Path:\nNord-Ouest Department -> common.topic.notable_types -> Administrative Division -> freebase.type_profile.strict_included_types -> Location\n# Answer:\nAdministrative Division"], "ground_truth": ["French", "Haitian Creole"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-100_de15ac1f762e3ec1e1261f6d9c81ebf9", "prediction": ["# Reasoning Path:\nLa Dessalinienne -> common.topic.notable_types -> National Anthem -> freebase.type_profile.strict_included_types -> Composition\n# Answer:\nNational Anthem", "# Reasoning Path:\nLa Dessalinienne -> common.topic.notable_types -> National Anthem -> freebase.type_hints.included_types -> Composition\n# Answer:\nNational Anthem", "# Reasoning Path:\nLa Dessalinienne -> government.national_anthem.national_anthem_of -> m.0d9ggc4 -> government.national_anthem_of_a_country.country -> Haiti\n# Answer:\nHaiti", "# Reasoning Path:\nLa Dessalinienne -> music.composition.recordings -> Haiti -> music.recording.artist -> Nicolas Geffrard\n# Answer:\nHaiti", "# Reasoning Path:\nLa Dessalinienne -> common.topic.notable_types -> National Anthem -> type.type.properties -> National anthem of\n# Answer:\nNational Anthem", "# Reasoning Path:\nLa Dessalinienne -> common.topic.notable_types -> National Anthem -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nNational Anthem", "# Reasoning Path:\nLa Dessalinienne -> music.composition.recordings -> Haiti -> location.country.national_anthem -> m.0d9ggc4\n# Answer:\nHaiti", "# Reasoning Path:\nLa Dessalinienne -> music.composition.recordings -> Haiti -> music.recording.artist -> Peter Breiner\n# Answer:\nHaiti", "# Reasoning Path:\nLa Dessalinienne -> music.composition.recordings -> Haiti -> common.topic.notable_for -> g.126tf08bm\n# Answer:\nHaiti", "# Reasoning Path:\nLa Dessalinienne -> common.topic.notable_types -> National Anthem -> freebase.type_hints.included_types -> Topic\n# Answer:\nNational Anthem"], "ground_truth": ["French", "Haitian Creole"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1091_d3301dedbdcb2d171a75be1631e8cec5", "prediction": ["# Reasoning Path:\nLumbini -> religion.place_of_worship.religion -> Buddhism -> religion.religion.sacred_sites -> Kushinagar\n# Answer:\nBuddhism", "# Reasoning Path:\nLumbini -> religion.place_of_worship.religion -> Buddhism -> religion.religion.founding_figures -> Gautama Buddha\n# Answer:\nBuddhism", "# Reasoning Path:\nLumbini -> common.topic.notable_for -> g.125bt8vfz\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nLumbini -> religion.place_of_worship.religion -> Buddhism -> symbols.namesake.named_after -> Gautama Buddha\n# Answer:\nBuddhism", "# Reasoning Path:\nUnited States of America -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60xs9dn\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Alaska -> location.location.containedby -> United States, with Territories\n# Answer:\nAlaska", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Arizona -> location.location.containedby -> United States, with Territories\n# Answer:\nArizona", "# Reasoning Path:\nUnited States of America -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_4dmh\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nLumbini -> common.topic.notable_types -> Place of worship -> type.type.expected_by -> Places of worship (current)\n# Answer:\nPlace of worship", "# Reasoning Path:\nUnited States of America -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_5xh3\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars"], "ground_truth": ["Buddhism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1119_8c41904eaef9fce2d95afd27eb9150ec", "prediction": ["# Reasoning Path:\nBentley B. Rayburn -> common.topic.notable_for -> g.125g59vh7\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nBentley B. Rayburn -> common.topic.notable_types -> Organization leader -> type.type.domain -> Organization\n# Answer:\nOrganization leader", "# Reasoning Path:\nBentley B. Rayburn -> business.board_member.leader_of -> m.0hprvkd -> organization.leadership.organization -> Colorado Technical University\n# Answer:\nColorado Technical University", "# Reasoning Path:\nBentley B. Rayburn -> business.board_member.leader_of -> m.0hprvkd -> organization.leadership.role -> President\n# Answer:\nPresident", "# Reasoning Path:\nBentley B. Rayburn -> common.topic.notable_types -> Organization leader -> freebase.type_profile.kind -> Role\n# Answer:\nOrganization leader", "# Reasoning Path:\nBentley B. Rayburn -> common.topic.notable_types -> Organization leader -> freebase.type_profile.equivalent_topic -> board member\n# Answer:\nOrganization leader"], "ground_truth": ["College/University"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121_333b95bd45af3e6e31e328bc8c24d84f", "prediction": ["# Reasoning Path:\nChase Lane Primary School -> education.educational_institution.students_graduates -> m.04dyl8v -> education.education.student -> David Beckham\n# Answer:\nDavid Beckham", "# Reasoning Path:\nChase Lane Primary School -> common.topic.notable_for -> g.1258yxzd_\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nChase Lane Primary School -> education.educational_institution.students_graduates -> m.04dyl8v -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nChase Lane Primary School -> common.topic.notable_types -> Educational Institution -> type.type.domain -> Education\n# Answer:\nEducational Institution", "# Reasoning Path:\nChase Lane Primary School -> education.educational_institution.students_graduates -> m.04dyl8v -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nChase Lane Primary School -> common.topic.notable_types -> Educational Institution -> type.type.expected_by -> Educational institution\n# Answer:\nEducational Institution", "# Reasoning Path:\nChase Lane Primary School -> common.topic.notable_types -> Educational Institution -> type.type.properties -> Athletics brand\n# Answer:\nEducational Institution", "# Reasoning Path:\nChase Lane Primary School -> common.topic.notable_types -> Educational Institution -> type.type.expected_by -> Accademia Anja\n# Answer:\nEducational Institution", "# Reasoning Path:\nChase Lane Primary School -> common.topic.notable_types -> Educational Institution -> type.type.expected_by -> Associated Institutions\n# Answer:\nEducational Institution", "# Reasoning Path:\nChase Lane Primary School -> common.topic.notable_types -> Educational Institution -> type.type.properties -> Campuses\n# Answer:\nEducational Institution"], "ground_truth": ["LA Galaxy"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1251_cbf2f20f6caf754bc49d672ca7b150b7", "prediction": ["# Reasoning Path:\nEast German mark -> finance.currency.countries_formerly_used -> East Germany -> location.country.currency_used -> Deutsche Mark\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_used -> East German mark\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_used -> Deutsche Mark\n# Answer:\nEast Germany", "# Reasoning Path:\nEast German mark -> common.topic.notable_types -> Currency -> freebase.type_profile.published -> Published\n# Answer:\nCurrency", "# Reasoning Path:\nEast German mark -> finance.currency.countries_used -> East Germany -> location.country.currency_used -> Deutsche Mark\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_formerly_used -> East German mark\n# Answer:\nEast Germany", "# Reasoning Path:\nEast German mark -> finance.currency.countries_formerly_used -> East Germany -> location.location.people_born_here -> Andreas Bielau\n# Answer:\nEast Germany", "# Reasoning Path:\nEast German mark -> common.topic.notable_types -> Currency -> type.type.expected_by -> currency\n# Answer:\nCurrency", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Germany\n# Answer:\nGerman, Standard", "# Reasoning Path:\nEast German mark -> finance.currency.countries_formerly_used -> East Germany -> location.location.people_born_here -> Andreas Diebitz\n# Answer:\nEast Germany"], "ground_truth": ["East Germany"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1260_5dd0eeca79ae03b7711252c032849eb2", "prediction": ["# Reasoning Path:\nSuper bowl -> freebase.type_profile.equivalent_topic -> Super Bowl -> time.recurring_event.instances -> Super Bowl X\n# Answer:\nSuper Bowl", "# Reasoning Path:\nRise -> sports.mascot.team -> Baltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII\n# Answer:\nBaltimore Ravens", "# Reasoning Path:\nSuper bowl -> freebase.type_profile.equivalent_topic -> Super Bowl -> sports.sports_championship.events -> Super Bowl X\n# Answer:\nSuper Bowl", "# Reasoning Path:\nRise -> common.topic.notable_for -> g.1yl5kmh55\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nSuper bowl -> freebase.type_profile.equivalent_topic -> Super Bowl -> time.recurring_event.instances -> Super Bowl 50\n# Answer:\nSuper Bowl", "# Reasoning Path:\nRise -> sports.mascot.team -> Baltimore Ravens -> sports.sports_team.championships -> 2001 AFC Championship Game\n# Answer:\nBaltimore Ravens", "# Reasoning Path:\nSuper bowl -> freebase.type_profile.equivalent_topic -> Super Bowl -> time.recurring_event.instances -> Super Bowl I\n# Answer:\nSuper Bowl", "# Reasoning Path:\nSuper bowl -> freebase.type_profile.equivalent_topic -> Super Bowl -> book.book_subject.works -> Boys Will Be Boys: The Glory Days and Party Nights of the Dallas Cowboys Dynasty\n# Answer:\nSuper Bowl", "# Reasoning Path:\nRise -> sports.mascot.team -> Baltimore Ravens -> sports.sports_team.championships -> 2013 AFC Championship Game\n# Answer:\nBaltimore Ravens", "# Reasoning Path:\nSuper bowl -> freebase.type_profile.published -> Published -> base.industrystandards.industry_standard_status.standards -> ISO 13250-2\n# Answer:\nPublished"], "ground_truth": ["Super Bowl XXXV"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-12_68d745a0657c86906382873e57294d6a", "prediction": ["# Reasoning Path:\nGovernor -> base.firsts.achievement.firsts -> m.04zr93l -> base.firsts.first_achievement.category -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGovernor -> base.firsts.achievement.firsts -> m.04zr93l -> base.firsts.first_achievement.first -> Nellie Tayloe Ross\n# Answer:\nNellie Tayloe Ross", "# Reasoning Path:\nGovernor -> base.firsts.achievement.firsts -> m.04zr93l -> base.firsts.first_achievement.category -> Female\n# Answer:\nFemale", "# Reasoning Path:\nGovernor -> organization.organization_committee_title.members_with_this_title -> m.0gj5628 -> organization.organization_committee_membership.committee -> World Bank Group for Portugal\n# Answer:\nWorld Bank Group for Portugal", "# Reasoning Path:\nGovernor -> organization.organization_committee_title.members_with_this_title -> m.0gj5628 -> organization.organization_committee_membership.member -> Fernando Teixeira dos Santos\n# Answer:\nFernando Teixeira dos Santos", "# Reasoning Path:\nOhio -> book.book_subject.works -> A Fistful of Charms -> book.book.genre -> Alternate history\n# Answer:\nA Fistful of Charms", "# Reasoning Path:\nOhio -> book.book_subject.works -> A Fistful of Charms -> book.book.editions -> A Fistful of Charms (Rachel Morgan, Book 4)\n# Answer:\nA Fistful of Charms", "# Reasoning Path:\nOhio -> location.location.partially_contained_by -> m.012w8ryz -> location.partial_containment_relationship.partially_contained_by -> Cincinnati Gymnastics Academy\n# Answer:\nCincinnati Gymnastics Academy", "# Reasoning Path:\nOhio -> book.book_subject.works -> All night long -> book.written_work.subjects -> Wisconsin\n# Answer:\nAll night long", "# Reasoning Path:\nOhio -> common.topic.notable_types -> Book -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nBook"], "ground_truth": ["Return J. Meigs, Jr.", "John Kasich"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1320_c5498ca807d2e1ec30d4c8fdd41f0bf7", "prediction": ["# Reasoning Path:\nHank -> sports.mascot.team -> Milwaukee Brewers -> sports.sports_team.arena_stadium -> Miller Park\n# Answer:\nMilwaukee Brewers", "# Reasoning Path:\nHank -> common.topic.notable_for -> g.11b5ltzrvv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nHank -> sports.mascot.team -> Milwaukee Brewers -> baseball.baseball_team.team_stats -> m.05n61_w\n# Answer:\nMilwaukee Brewers", "# Reasoning Path:\nHank -> sports.mascot.team -> Milwaukee Brewers -> common.topic.webpage -> m.0422hjw\n# Answer:\nMilwaukee Brewers", "# Reasoning Path:\nHank -> sports.mascot.team -> Milwaukee Brewers -> baseball.baseball_team.team_stats -> m.05n61hh\n# Answer:\nMilwaukee Brewers", "# Reasoning Path:\nHank -> sports.mascot.team -> Milwaukee Brewers -> baseball.baseball_team.team_stats -> m.05n61lk\n# Answer:\nMilwaukee Brewers", "# Reasoning Path:\nHank -> sports.mascot.team -> Milwaukee Brewers -> common.topic.webpage -> m.0jbcp8t\n# Answer:\nMilwaukee Brewers", "# Reasoning Path:\nHank -> common.topic.notable_types -> Mascot -> type.type.expected_by -> Team Mascot\n# Answer:\nMascot", "# Reasoning Path:\nHank -> common.topic.notable_types -> Mascot -> freebase.documented_object.documentation -> m.02ht4bx\n# Answer:\nMascot", "# Reasoning Path:\nHank -> common.topic.notable_types -> Mascot -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nMascot"], "ground_truth": ["Miller Park"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1348_3bddf705b2a346fbdc2f00e9a02b84f3", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.drafted_athlete.drafted -> m.04vb3pp -> sports.sports_league_draft_pick.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nViktor the Viking -> common.topic.notable_for -> g.12596ypgt\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nPeyton Manning -> sports.drafted_athlete.drafted -> m.04vb3pp -> sports.sports_league_draft_pick.school -> University of Tennessee\n# Answer:\nUniversity of Tennessee", "# Reasoning Path:\nViktor the Viking -> sports.mascot.team -> Minnesota Vikings -> sports.sports_team.season_record -> m.075fm_p\n# Answer:\nMinnesota Vikings", "# Reasoning Path:\nViktor the Viking -> sports.mascot.team -> Minnesota Vikings -> sports.sports_team.season_record -> m.075fm_y\n# Answer:\nMinnesota Vikings", "# Reasoning Path:\nViktor the Viking -> sports.mascot.team -> Minnesota Vikings -> tv.tv_actor.guest_roles -> m.0gf_0gm\n# Answer:\nMinnesota Vikings", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nViktor the Viking -> sports.mascot.team -> Minnesota Vikings -> sports.sports_team.season_record -> m.075fn05\n# Answer:\nMinnesota Vikings"], "ground_truth": ["Minnesota Vikings"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1384_744a496b907e407b16bc5d7c197dc3f0", "prediction": ["# Reasoning Path:\nOvadia Yosef -> people.person.religion -> Haredi Judaism -> religion.religion.is_part_of -> Judaism\n# Answer:\nHaredi Judaism", "# Reasoning Path:\nOvadia Yosef -> people.person.religion -> Haredi Judaism -> common.topic.image -> Klaagmuur Jeruzalem\n# Answer:\nHaredi Judaism", "# Reasoning Path:\nOvadia Yosef -> common.topic.article -> m.02qmp2\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nOvadia Yosef -> people.person.religion -> Haredi Judaism -> common.topic.notable_types -> Religion\n# Answer:\nHaredi Judaism", "# Reasoning Path:\nOvadia Yosef -> people.person.religion -> Judaism -> religion.religion.deities -> Elohim\n# Answer:\nJudaism", "# Reasoning Path:\nOvadia Yosef -> people.person.ethnicity -> Israelis -> people.ethnicity.people -> Yitzhak Rabin\n# Answer:\nIsraelis", "# Reasoning Path:\nOvadia Yosef -> people.person.ethnicity -> Israelis -> people.ethnicity.included_in_group -> Semitic people\n# Answer:\nIsraelis", "# Reasoning Path:\nOvadia Yosef -> people.person.ethnicity -> Israelis -> people.ethnicity.people -> Aharon Appelfeld\n# Answer:\nIsraelis", "# Reasoning Path:\nOvadia Yosef -> people.person.ethnicity -> Israelis -> common.topic.image -> Dana International\n# Answer:\nIsraelis", "# Reasoning Path:\nOvadia Yosef -> people.person.religion -> Judaism -> religion.religion.beliefs -> Entering Heaven alive\n# Answer:\nJudaism"], "ground_truth": ["Judaism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1528_25853c768670cd164d7793f094ba7cbb", "prediction": ["# Reasoning Path:\ng.122sg15v\n# Answer:\nMusician", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.03jq21w -> film.performance.film -> The Butterfly Effect\n# Answer:\nThe Butterfly Effect", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.03jq2p4 -> film.performance.film -> 3:10 to Yuma\n# Answer:\n3:10 to Yuma", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.04ls5pn -> film.performance.film -> My One and Only\n# Answer:\nMy One and Only", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.03jq21w -> film.performance.character -> Evan Treborn\n# Answer:\nEvan Treborn", "# Reasoning Path:\nLogan Lerman -> people.person.profession -> Musician -> common.topic.notable_types -> Profession\n# Answer:\nMusician", "# Reasoning Path:\nLogan Lerman -> people.person.ethnicity -> Jewish people -> common.topic.subject_of -> Nefesh B'Nefesh\n# Answer:\nJewish people", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.03jq2p4 -> film.performance.character -> William Evans\n# Answer:\nWilliam Evans", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.04ls5pn -> film.performance.character -> George Devereaux\n# Answer:\nGeorge Devereaux", "# Reasoning Path:\nLogan Lerman -> people.person.ethnicity -> Jewish people -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nJewish people"], "ground_truth": ["Percy Jackson & the Olympians: The Lightning Thief", "Percy Jackson: Sea of Monsters"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1686_29e74083744b3631541b29b4094fb273", "prediction": ["# Reasoning Path:\nGovernor -> base.firsts.achievement.firsts -> m.04zr93l -> base.firsts.first_achievement.category -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGovernor -> base.firsts.achievement.firsts -> m.04zr93l -> base.firsts.first_achievement.category -> Female\n# Answer:\nFemale", "# Reasoning Path:\nGovernor -> base.firsts.achievement.firsts -> m.04zr93l -> base.firsts.first_achievement.first -> Nellie Tayloe Ross\n# Answer:\nNellie Tayloe Ross", "# Reasoning Path:\nGovernor -> base.ikariam.ikariam_research.category -> Military branch -> book.periodical_subject.periodicals -> CAT\n# Answer:\nMilitary branch", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y42 -> government.government_position_held.office_holder -> Ernest McFarland\n# Answer:\nErnest McFarland", "# Reasoning Path:\nGovernor -> base.ikariam.ikariam_research.category -> Military branch -> film.film_subject.films -> A Bridge Too Far\n# Answer:\nMilitary branch", "# Reasoning Path:\nGovernor -> organization.organization_committee_title.members_with_this_title -> m.0gj5628 -> organization.organization_committee_membership.committee -> World Bank Group for Portugal\n# Answer:\nWorld Bank Group for Portugal", "# Reasoning Path:\nGovernor -> base.ikariam.ikariam_research.category -> Military branch -> film.film_subject.films -> Coming Out Under Fire\n# Answer:\nMilitary branch", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y42 -> government.government_position_held.basic_title -> Governor\n# Answer:\ngovernment.government_position_held.basic_title", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y46 -> government.government_position_held.office_holder -> Paul Fannin\n# Answer:\nPaul Fannin"], "ground_truth": ["Janet Napolitano"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1785_1c178196ae53ffd4b09e5787a35c3950", "prediction": ["# Reasoning Path:\nKim Richards -> tv.tv_actor.starring_roles -> m.0bngb3y -> tv.regular_tv_appearance.series -> Hello, Larry\n# Answer:\nHello, Larry", "# Reasoning Path:\nKim Richards -> tv.tv_actor.starring_roles -> m.0bngyp9 -> tv.regular_tv_appearance.series -> Nanny and the Professor\n# Answer:\nNanny and the Professor", "# Reasoning Path:\nKim Richards -> tv.tv_actor.starring_roles -> m.0bngb3y -> tv.regular_tv_appearance.character -> Ruthie Alder\n# Answer:\nRuthie Alder", "# Reasoning Path:\nKim Richards -> tv.tv_actor.starring_roles -> m.0bngq3l -> tv.regular_tv_appearance.series -> James at 15\n# Answer:\nJames at 15", "# Reasoning Path:\nKim Richards -> tv.tv_actor.starring_roles -> m.0bngq3l -> tv.regular_tv_appearance.character -> Sandy Hunter\n# Answer:\nSandy Hunter", "# Reasoning Path:\nKim Richards -> tv.tv_actor.guest_roles -> m.09nxsqd -> tv.tv_guest_role.episodes_appeared_in -> River of Fear\n# Answer:\nRiver of Fear", "# Reasoning Path:\nKim Richards -> film.actor.film -> m.0131hj6c -> film.performance.film -> Nanny and the Professor\n# Answer:\nNanny and the Professor", "# Reasoning Path:\nKim Richards -> tv.tv_actor.guest_roles -> m.09nxsqs -> tv.tv_guest_role.character -> Gail\n# Answer:\nGail", "# Reasoning Path:\nKim Richards -> tv.tv_actor.starring_roles -> m.0bngyp9 -> tv.regular_tv_appearance.character -> Prudence\n# Answer:\nPrudence", "# Reasoning Path:\nKim Richards -> tv.tv_actor.guest_roles -> m.09nxsqd -> tv.tv_guest_role.character -> Julie Todd\n# Answer:\nJulie Todd"], "ground_truth": ["Nanny and the Professor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1797_68a33792b0a1e18937dcd4b3f50d941e", "prediction": ["# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> location.location.containedby -> North America\n# Answer:\nConfederate States of America", "# Reasoning Path:\nMontgomery -> location.location.containedby -> Alabama -> location.location.containedby -> Contiguous United States\n# Answer:\nAlabama", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> book.book_subject.works -> An Honorable Defeat: The Last Days of the Confederate Government\n# Answer:\nConfederate States of America", "# Reasoning Path:\nMontgomery -> location.location.containedby -> Alabama -> location.location.containedby -> Gulf Coast of the United States\n# Answer:\nAlabama", "# Reasoning Path:\nMontgomery -> location.statistical_region.population -> g.11b66cwzby\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> location.country.national_anthem -> m.0glzsbn\n# Answer:\nConfederate States of America", "# Reasoning Path:\nMontgomery -> location.location.containedby -> Alabama -> location.location.containedby -> United States of America\n# Answer:\nAlabama", "# Reasoning Path:\nMontgomery -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Ulysses S. Grant -> government.politician.election_campaigns -> Ulysses S. Grant Presidential Campaign, 1868\n# Answer:\nUlysses S. Grant", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> book.book_subject.works -> Apostles of Disunion: Southern Secession Commissioners and the Causes of the Civil War\n# Answer:\nConfederate States of America"], "ground_truth": ["Confederate States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1812_c29807a955ea46fa79cdc9f7aeacba18", "prediction": ["# Reasoning Path:\nRihanna: Live in Concert Tour -> music.concert_tour.artist -> Rihanna -> music.artist.origin -> Saint Michael Parish\n# Answer:\nRihanna", "# Reasoning Path:\nRihanna: Live in Concert Tour -> music.concert_tour.artist -> Rihanna -> music.artist.origin -> Barbados\n# Answer:\nRihanna", "# Reasoning Path:\nRihanna: Live in Concert Tour -> common.topic.article -> m.09g79gd\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nRihanna: Live in Concert Tour -> music.concert_tour.artist -> Rihanna -> tv.tv_actor.guest_roles -> m.09ngsbm\n# Answer:\nRihanna", "# Reasoning Path:\nRihanna: Live in Concert Tour -> music.concert_tour.artist -> Rihanna -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nRihanna", "# Reasoning Path:\nRihanna: Live in Concert Tour -> music.concert_tour.artist -> Rihanna -> tv.tv_actor.guest_roles -> m.09ngsbs\n# Answer:\nRihanna", "# Reasoning Path:\nRihanna: Live in Concert Tour -> music.concert_tour.album_or_release_supporting -> A Girl like Me -> common.topic.webpage -> m.09xm5_1\n# Answer:\nA Girl like Me", "# Reasoning Path:\nRihanna: Live in Concert Tour -> music.concert_tour.artist -> Rihanna -> tv.tv_actor.guest_roles -> m.09ngsby\n# Answer:\nRihanna", "# Reasoning Path:\nRihanna: Live in Concert Tour -> music.concert_tour.artist -> Rihanna -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nRihanna", "# Reasoning Path:\nRihanna: Live in Concert Tour -> music.concert_tour.artist -> Rihanna -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nRihanna"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-361_e24533e28da40db99eb4b25773f9d38f", "prediction": ["# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Kunsthistorisches Museum -> visual_art.art_owner.artworks_owned -> m.043zwkv\n# Answer:\nKunsthistorisches Museum", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Kunsthistorisches Museum -> common.topic.image -> IMG 0089 - Wien - Kunsthistorisches Museum\n# Answer:\nKunsthistorisches Museum", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Haus der Musik -> common.topic.notable_types -> Tourist attraction\n# Answer:\nHaus der Musik", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Kunsthistorisches Museum -> visual_art.art_owner.artworks_owned -> m.043zwq8\n# Answer:\nKunsthistorisches Museum", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Kunsthistorisches Museum -> base.arthist.int_zm_nyek.mutargy -> m.06m79k8\n# Answer:\nKunsthistorisches Museum", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Kunsthistorisches Museum -> visual_art.art_owner.artworks_owned -> m.04vzd2v\n# Answer:\nKunsthistorisches Museum", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Haus der Musik -> common.topic.notable_for -> g.12566hnf1\n# Answer:\nHaus der Musik", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Albertina -> architecture.building.building_function -> Museum\n# Answer:\nAlbertina", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Kunsthistorisches Museum -> base.arthist.int_zm_nyek.mutargy -> m.07mgkf_\n# Answer:\nKunsthistorisches Museum", "# Reasoning Path:\nVienna -> travel.travel_destination.climate -> m.052l85k -> travel.travel_destination_monthly_climate.month -> January\n# Answer:\nJanuary"], "ground_truth": ["Kunsthistorisches Museum"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-397_c7ee26d5c3f80107d7ae5fede489209a", "prediction": ["# Reasoning Path:\nHarvard Art Museums -> travel.tourist_attraction.near_travel_destination -> Cambridge -> location.location.containedby -> Massachusetts\n# Answer:\nCambridge", "# Reasoning Path:\nHarvard Art Museums -> travel.tourist_attraction.near_travel_destination -> Boston -> travel.travel_destination.tourist_attractions -> African Meeting House\n# Answer:\nBoston", "# Reasoning Path:\nHarvard Art Museums -> travel.tourist_attraction.near_travel_destination -> Cambridge -> periodicals.newspaper_circulation_area.newspapers -> The Harvard Independent\n# Answer:\nCambridge", "# Reasoning Path:\nHarvard Art Museums -> visual_art.art_owner.artworks_owned -> m.075vbgc -> visual_art.artwork_owner_relationship.artwork -> Gare Saint-Lazare: Arrival of a Train\n# Answer:\nGare Saint-Lazare: Arrival of a Train", "# Reasoning Path:\nHarvard Art Museums -> travel.tourist_attraction.near_travel_destination -> Cambridge -> location.location.containedby -> Area code 857\n# Answer:\nCambridge", "# Reasoning Path:\nHarvard Art Museums -> travel.tourist_attraction.near_travel_destination -> Cambridge -> location.statistical_region.population -> g.11b66hc78c\n# Answer:\nCambridge", "# Reasoning Path:\nHarvard Art Museums -> travel.tourist_attraction.near_travel_destination -> Boston -> travel.travel_destination.tourist_attractions -> Arnold Arboretum\n# Answer:\nBoston", "# Reasoning Path:\nHarvard Art Museums -> travel.tourist_attraction.near_travel_destination -> Cambridge -> location.location.containedby -> Area codes 617 and 857\n# Answer:\nCambridge", "# Reasoning Path:\nHarvard Art Museums -> travel.tourist_attraction.near_travel_destination -> Cambridge -> periodicals.newspaper_circulation_area.newspapers -> Ergo\n# Answer:\nCambridge", "# Reasoning Path:\nHarvard Art Museums -> travel.tourist_attraction.near_travel_destination -> Boston -> travel.travel_destination.tourist_attractions -> Boston Athen\u00e6um\n# Answer:\nBoston"], "ground_truth": ["Crimson"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-484_ac67410188d0f2258139a3c84773885e", "prediction": ["# Reasoning Path:\nCentral Western Time Zone -> time.time_zone.locations_in_this_time_zone -> Australia -> location.country.languages_spoken -> Esperanto Language\n# Answer:\nAustralia", "# Reasoning Path:\nCentral Western Time Zone -> time.time_zone.locations_in_this_time_zone -> Australia -> location.country.languages_spoken -> Lojban\n# Answer:\nAustralia", "# Reasoning Path:\nCentral Western Time Zone -> time.time_zone.locations_in_this_time_zone -> Australia -> location.country.languages_spoken -> English Language\n# Answer:\nAustralia", "# Reasoning Path:\nCentral Western Time Zone -> time.time_zone.locations_in_this_time_zone -> Australia -> base.uncommon.topic.exceptions -> Adelaide should be contained in some Argentinian department, Municipality, or Brazilian municipality within Australia.\n# Answer:\nAustralia", "# Reasoning Path:\nCentral Western Time Zone -> time.time_zone.locations_in_this_time_zone -> Australia -> sports.sports_team_location.teams -> Australia men's national volleyball team\n# Answer:\nAustralia", "# Reasoning Path:\nCentral Western Time Zone -> time.time_zone.locations_in_this_time_zone -> Australia -> base.uncommon.topic.exceptions -> Alpurrurulam Community Government Council should be contained in some Australian territory, Australian state, or Australian external territory.\n# Answer:\nAustralia", "# Reasoning Path:\nCentral Western Time Zone -> freebase.valuenotation.has_no_value -> DST offset from UTC -> type.property.unit -> Hour\n# Answer:\nDST offset from UTC", "# Reasoning Path:\nCentral Western Time Zone -> time.time_zone.locations_in_this_time_zone -> Australia -> base.uncommon.topic.exceptions -> Australian Suburb is not suited to be the administrative division type for Australian Suburb.\n# Answer:\nAustralia", "# Reasoning Path:\nCentral Western Time Zone -> time.time_zone.locations_in_this_time_zone -> Australia -> sports.sports_team_location.teams -> Australia national cricket team\n# Answer:\nAustralia", "# Reasoning Path:\nCentral Western Time Zone -> common.topic.notable_types -> Time Zone -> type.type.properties -> DST offset from UTC\n# Answer:\nTime Zone"], "ground_truth": ["Esperanto Language", "English Language", "Lojban"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-538_49b4e9304f18a0a1cbe37bb162f61131", "prediction": ["# Reasoning Path:\nWest Reading El Ctr -> common.topic.notable_for -> g.125cx_402\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nWest Reading El Ctr -> education.school.school_district -> Wyomissing Area School District -> education.school_district.schools -> Wyomissing Area Junior/Senior High School\n# Answer:\nWyomissing Area School District", "# Reasoning Path:\nWest Reading El Ctr -> common.topic.notable_types -> School -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nSchool", "# Reasoning Path:\nWest Reading El Ctr -> education.school.school_district -> Wyomissing Area School District -> education.school_district.schools -> Wyomissing Hills El Ctr\n# Answer:\nWyomissing Area School District", "# Reasoning Path:\nWest Reading El Ctr -> education.school.school_district -> Wyomissing Area School District -> organization.organization.headquarters -> m.0ytvw8d\n# Answer:\nWyomissing Area School District", "# Reasoning Path:\nWest Reading El Ctr -> education.school.school_district -> Wyomissing Area School District -> common.topic.notable_types -> School district\n# Answer:\nWyomissing Area School District", "# Reasoning Path:\nWest Reading El Ctr -> education.school.school_district -> Wyomissing Area School District -> organization.organization.headquarters -> m.0ytw2mg\n# Answer:\nWyomissing Area School District", "# Reasoning Path:\nWest Reading El Ctr -> common.topic.notable_types -> School -> freebase.type_profile.strict_included_types -> Abstract\n# Answer:\nSchool", "# Reasoning Path:\nWest Reading El Ctr -> education.school.school_district -> Wyomissing Area School District -> organization.organization.headquarters -> m.0ytw7yf\n# Answer:\nWyomissing Area School District", "# Reasoning Path:\nWest Reading El Ctr -> common.topic.notable_types -> School -> freebase.type_profile.strict_included_types -> Agent\n# Answer:\nSchool"], "ground_truth": ["The Giver", "Jonas Brothers: The Concert Experience", "Hannah Montana: The Movie", "Valentine's Day", "The Lorax"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-538_92e606ef9c0429ad6820797ad2950730", "prediction": ["# Reasoning Path:\nOnce-ler -> common.topic.notable_for -> g.125bxt68_\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nOnce-ler -> film.film_character.portrayed_in_films -> m.0gwnx81 -> film.performance.film -> The Lorax\n# Answer:\nThe Lorax", "# Reasoning Path:\nOnce-ler -> common.topic.article -> m.02t2k8\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nOnce-ler -> film.film_character.portrayed_in_films -> m.0gwnx81 -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nTaylor Swift -> music.composer.compositions -> 22 -> music.composition.recordings -> 22 (feat. Sam Tsui, Kurt Schneider, Against the Current & King the Kid)\n# Answer:\n22", "# Reasoning Path:\nTaylor Swift -> music.artist.album -> Speak Now -> common.topic.notable_types -> Composition\n# Answer:\nSpeak Now", "# Reasoning Path:\nTaylor Swift -> music.artist.album -> Wonderland -> common.topic.notable_for -> g.11b7g39dd1\n# Answer:\nWonderland", "# Reasoning Path:\nOnce-ler -> film.film_character.portrayed_in_films -> m.0gwnx81 -> film.performance.actor -> Ed Helms\n# Answer:\nEd Helms", "# Reasoning Path:\nTaylor Swift -> music.artist.album -> Speak Now -> music.album.releases -> Taylor Swift Karaoke: Speak Now\n# Answer:\nSpeak Now", "# Reasoning Path:\nTaylor Swift -> music.artist.album -> Speak Now -> common.topic.notable_types -> Musical Album\n# Answer:\nSpeak Now"], "ground_truth": ["The Lorax"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-576_01e2da60a2779c4ae4b5d1547499a4f8", "prediction": ["# Reasoning Path:\nCountry -> fictional_universe.type_of_fictional_setting.settings -> Grand Fenwick -> fictional_universe.fictional_setting.contained_by -> Europe\n# Answer:\nGrand Fenwick", "# Reasoning Path:\nCentral America -> location.location.contains -> Guatemala -> location.country.form_of_government -> Unitary state\n# Answer:\nGuatemala", "# Reasoning Path:\nAlta Verapaz Department -> base.aareas.schema.administrative_area.administrative_parent -> Guatemala -> location.country.form_of_government -> Constitutional republic\n# Answer:\nGuatemala", "# Reasoning Path:\nCountry -> fictional_universe.type_of_fictional_setting.settings -> Ruritania -> fictional_universe.fictional_setting.contained_by -> Europe\n# Answer:\nRuritania", "# Reasoning Path:\nAlta Verapaz Department -> base.aareas.schema.administrative_area.administrative_parent -> Guatemala -> location.country.form_of_government -> Unitary state\n# Answer:\nGuatemala", "# Reasoning Path:\nCentral America -> location.location.geolocation -> m.0bg8fr1\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nCountry -> fictional_universe.type_of_fictional_setting.settings -> Arnor\n# Answer:\nArnor", "# Reasoning Path:\nCentral America -> location.location.contains -> Guatemala -> location.country.form_of_government -> Constitutional republic\n# Answer:\nGuatemala", "# Reasoning Path:\nAlta Verapaz Department -> location.statistical_region.population -> g.11bv5vs59b\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nCentral America -> location.location.contains -> Guatemala -> location.location.containedby -> Americas\n# Answer:\nGuatemala"], "ground_truth": ["Guatemala"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.04081632653061224, "path_precision": 0.5, "path_recall": 0.02127659574468085, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-576_8dfecb6548586cf236340abadabeb86c", "prediction": ["# Reasoning Path:\nCountry -> fictional_universe.type_of_fictional_setting.settings -> Grand Fenwick -> fictional_universe.fictional_setting.contained_by -> Europe\n# Answer:\nGrand Fenwick", "# Reasoning Path:\nUnited States Dollar -> finance.currency.countries_used -> Netherlands -> location.country.currency_used -> Euro\n# Answer:\nNetherlands", "# Reasoning Path:\nCountry -> fictional_universe.type_of_fictional_setting.settings -> Ruritania -> fictional_universe.fictional_setting.contained_by -> Europe\n# Answer:\nRuritania", "# Reasoning Path:\nCentral America -> location.location.contains -> Panama -> location.country.form_of_government -> Constitutional republic\n# Answer:\nPanama", "# Reasoning Path:\nUnited States Dollar -> finance.currency.countries_used -> El Salvador -> location.country.form_of_government -> Constitutional republic\n# Answer:\nEl Salvador", "# Reasoning Path:\nCentral America -> location.location.geolocation -> m.0bg8fr1\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nCountry -> fictional_universe.type_of_fictional_setting.settings -> Arnor\n# Answer:\nArnor", "# Reasoning Path:\nUnited States Dollar -> finance.currency.countries_used -> Netherlands -> location.country.administrative_divisions -> Amsterdam\n# Answer:\nNetherlands", "# Reasoning Path:\nCentral America -> location.location.contains -> El Salvador -> location.country.form_of_government -> Constitutional republic\n# Answer:\nEl Salvador", "# Reasoning Path:\nUnited States Dollar -> finance.currency.countries_used -> American Samoa -> location.country.form_of_government -> Constitutional republic\n# Answer:\nAmerican Samoa"], "ground_truth": ["El Salvador", "Panama"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.08108108108108107, "path_precision": 0.3, "path_recall": 0.046875, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-576_906ad6be7bec9d208f4dde4f7721c261", "prediction": ["# Reasoning Path:\nCountry -> fictional_universe.type_of_fictional_setting.settings -> Ruritania -> fictional_universe.fictional_setting.contained_by -> Europe\n# Answer:\nRuritania", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.location.containedby -> Americas\n# Answer:\nBelize", "# Reasoning Path:\nCountry -> fictional_universe.type_of_fictional_setting.settings -> Grand Fenwick -> fictional_universe.fictional_setting.contained_by -> Europe\n# Answer:\nGrand Fenwick", "# Reasoning Path:\nCentral America -> location.location.contains -> Panama -> base.aareas.schema.administrative_area.administrative_area_type -> Sovereign state\n# Answer:\nPanama", "# Reasoning Path:\nCentral America -> location.location.geolocation -> m.0bg8fr1\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nCountry -> fictional_universe.type_of_fictional_setting.settings -> Arnor\n# Answer:\nArnor", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.location.containedby -> Latin America\n# Answer:\nBelize", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> meteorology.cyclone_affected_area.cyclones -> 1934 Central America hurricane\n# Answer:\nBelize", "# Reasoning Path:\nCentral America -> location.location.contains -> Panama -> location.country.official_language -> Spanish Language\n# Answer:\nPanama", "# Reasoning Path:\nCountry -> freebase.type_profile.kind -> Definition\n# Answer:\nDefinition"], "ground_truth": ["Guatemala", "El Salvador", "Belize", "Panama", "Honduras", "Costa Rica"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.5, "ans_recall": 0.3333333333333333, "path_f1": 0.012903225806451615, "path_precision": 0.5, "path_recall": 0.006535947712418301, "path_ans_f1": 0.4, "path_ans_precision": 0.5, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-590_6aad73acb74f304bc9acae44314164be", "prediction": ["# Reasoning Path:\nLibya, Libya, Libya -> music.composition.language -> Arabic Language -> language.human_language.countries_spoken_in -> Libya\n# Answer:\nArabic Language", "# Reasoning Path:\nPrime Minister of Libya -> common.topic.notable_types -> Government Office or Title -> freebase.type_profile.published -> Published\n# Answer:\nGovernment Office or Title", "# Reasoning Path:\nLibya, Libya, Libya -> music.composition.language -> Arabic Language -> language.human_language.countries_spoken_in -> Algeria\n# Answer:\nArabic Language", "# Reasoning Path:\nPrime Minister of Libya -> common.topic.notable_for -> g.125fk0fd2\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nLibya, Libya, Libya -> music.composition.language -> Arabic Language -> language.human_language.countries_spoken_in -> Bahrain\n# Answer:\nArabic Language", "# Reasoning Path:\nPrime Minister of Libya -> common.topic.notable_types -> Government Office or Title -> freebase.type_hints.included_types -> Topic\n# Answer:\nGovernment Office or Title", "# Reasoning Path:\nLibya, Libya, Libya -> music.composition.language -> Arabic Language -> language.human_language.dialects -> Algerian Arabic\n# Answer:\nArabic Language", "# Reasoning Path:\nLibya, Libya, Libya -> music.composition.composer -> Abdul Wahab -> people.person.children -> Ahmed Mohammed Abdel Wahab\n# Answer:\nAbdul Wahab", "# Reasoning Path:\nPrime Minister of Libya -> common.topic.notable_types -> Government Office or Title -> type.type.expected_by -> Office\n# Answer:\nGovernment Office or Title", "# Reasoning Path:\nPrime Minister of Libya -> government.government_office_or_title.office_holders -> m.0105y7bt -> government.government_position_held.office_holder -> Abdullah al-Thani\n# Answer:\nAbdullah al-Thani"], "ground_truth": ["Abdullah al-Thani"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.11111111111111112, "path_precision": 0.1, "path_recall": 0.125, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-61_09020cbb000c86fda5910ec084690246", "prediction": ["# Reasoning Path:\nMecca -> religion.place_of_worship.religion -> Islam -> religion.religion.types_of_places_of_worship -> Mosque\n# Answer:\nIslam", "# Reasoning Path:\nMecca -> religion.place_of_worship.religion -> Islam -> religion.religion.sacred_sites -> Jerusalem\n# Answer:\nIslam", "# Reasoning Path:\nIndonesia -> travel.travel_destination.tour_operators -> FOX Vakanties -> travel.tour_operator.travel_destinations -> Bali\n# Answer:\nFOX Vakanties", "# Reasoning Path:\nMecca -> religion.place_of_worship.religion -> Islam -> religion.religion.notable_figures -> Ali\n# Answer:\nIslam", "# Reasoning Path:\nIndonesia -> location.location.containedby -> Asia -> location.location.partially_contains -> Istanbul\n# Answer:\nAsia", "# Reasoning Path:\nMecca -> religion.place_of_worship.religion -> Islam -> religion.religion.sacred_sites -> Medina\n# Answer:\nIslam", "# Reasoning Path:\nIndonesia -> travel.travel_destination.tour_operators -> FOX Vakanties -> travel.tour_operator.travel_destinations -> New York\n# Answer:\nFOX Vakanties", "# Reasoning Path:\nMecca -> religion.place_of_worship.religion -> Islam -> religion.religion.notable_figures -> Muhammad in Islam\n# Answer:\nIslam", "# Reasoning Path:\nIndonesia -> location.location.containedby -> Asia -> location.location.events -> World War I\n# Answer:\nAsia", "# Reasoning Path:\nIndonesia -> location.location.containedby -> Eurasia -> common.topic.notable_types -> Location\n# Answer:\nEurasia"], "ground_truth": ["Islam"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.5, "path_recall": 0.3333333333333333, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-61_8f0d44a78a2e50f47729db23c5850da0", "prediction": ["# Reasoning Path:\nMary -> base.saints.saint.venerated_in -> Catholicism -> religion.religion.texts -> The Bible\n# Answer:\nCatholicism", "# Reasoning Path:\nMary -> people.person.religion -> Catholicism -> religion.religion.deities -> Jesus Christ\n# Answer:\nCatholicism", "# Reasoning Path:\nMary -> people.person.religion -> Catholicism -> religion.religion.texts -> The Bible\n# Answer:\nCatholicism", "# Reasoning Path:\nMary -> people.person.religion -> Catholicism -> religion.religion.deities -> God\n# Answer:\nCatholicism", "# Reasoning Path:\nMary -> people.person.religion -> Catholicism -> religion.religion.branched_from -> Judaism\n# Answer:\nCatholicism", "# Reasoning Path:\nMary -> base.saints.saint.venerated_in -> Catholicism -> religion.religion.deities -> God\n# Answer:\nCatholicism", "# Reasoning Path:\nMary -> base.saints.saint.venerated_in -> Catholicism -> religion.religion.texts -> New Testament\n# Answer:\nCatholicism", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf2_w -> location.religion_percentage.religion -> Islam\n# Answer:\nIslam", "# Reasoning Path:\nMary -> people.person.religion -> Judaism -> religion.religion.deities -> God\n# Answer:\nJudaism", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf301 -> location.religion_percentage.religion -> Protestantism\n# Answer:\nProtestantism"], "ground_truth": ["Catholicism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-626_47b990334f91b6d3bd042f82b81740f6", "prediction": ["# Reasoning Path:\nDennis Daugaard -> people.person.nationality -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> South Dakota\n# Answer:\nUnited States of America", "# Reasoning Path:\nDennis Daugaard -> people.person.nationality -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Arkansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nDennis Daugaard -> government.politician.government_positions_held -> m.0jskk6b -> government.government_position_held.jurisdiction_of_office -> South Dakota\n# Answer:\nSouth Dakota", "# Reasoning Path:\nDennis Daugaard -> people.person.nationality -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Illinois\n# Answer:\nUnited States of America", "# Reasoning Path:\nDennis Daugaard -> people.person.nationality -> United States of America -> location.location.partially_containedby -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nDennis Daugaard -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> United States Representative\n# Answer:\nUnited States of America", "# Reasoning Path:\nDennis Daugaard -> government.politician.government_positions_held -> m.0jskk6b -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nDennis Daugaard -> government.politician.government_positions_held -> m.010hbrrk -> government.government_position_held.jurisdiction_of_office -> South Dakota\n# Answer:\nSouth Dakota", "# Reasoning Path:\nDennis Daugaard -> common.image.appears_in_topic_gallery -> Lieutenant Governor of South Dakota -> government.government_office_or_title.jurisdiction -> South Dakota\n# Answer:\nLieutenant Governor of South Dakota", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> South Dakota -> location.location.people_born_here -> Adam Emerson\n# Answer:\nSouth Dakota"], "ground_truth": ["South Dakota"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5555555555555556, "path_precision": 0.5, "path_recall": 0.625, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-626_8c6b952c6bd963f0ece4e401c9eb731a", "prediction": ["# Reasoning Path:\nCentral Time Zone -> common.topic.image -> Timezoneswest -> common.image.appears_in_topic_gallery -> Samoa Time Zone\n# Answer:\nTimezoneswest", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Nebraska -> common.topic.image -> Nebraska.png\n# Answer:\nNebraska", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Missouri -> location.location.containedby -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nCentral Time Zone -> common.topic.image -> Timezoneswest -> common.image.appears_in_topic_gallery -> Alaska Time Zone\n# Answer:\nTimezoneswest", "# Reasoning Path:\nCentral Time Zone -> common.topic.image -> Timezoneswest -> common.image.size -> m.02czb60\n# Answer:\nTimezoneswest", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Missouri -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nMissouri River -> base.schemastaging.context_name.pronunciation -> m.0131j5dt\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nCentral Time Zone -> common.topic.image -> Timezoneswest -> common.image.appears_in_topic_gallery -> Atlantic Time Zone\n# Answer:\nTimezoneswest", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Iowa -> base.aareas.schema.administrative_area.administrative_children -> Delaware County\n# Answer:\nIowa", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Iowa -> common.topic.image -> Lac DeSoto\n# Answer:\nIowa"], "ground_truth": ["Kansas", "North Dakota", "Missouri", "South Dakota", "Iowa", "Nebraska"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.3333333333333333, "path_precision": 0.5, "path_recall": 0.25, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.6, "path_ans_recall": 0.5}
{"id": "WebQTest-626_bd2c9bc1c0a692218e7c4182a1d52f85", "prediction": ["# Reasoning Path:\nMike Johanns -> government.politician.government_positions_held -> m.05kfds4 -> government.government_position_held.district_represented -> Nebraska\n# Answer:\nNebraska", "# Reasoning Path:\nMike Johanns -> government.politician.government_positions_held -> m.04l4prr -> government.government_position_held.jurisdiction_of_office -> Nebraska\n# Answer:\nNebraska", "# Reasoning Path:\nMike Johanns -> common.image.appears_in_topic_gallery -> United States Senate election in Nebraska, 2008 -> government.election.district -> Nebraska\n# Answer:\nUnited States Senate election in Nebraska, 2008", "# Reasoning Path:\nMike Johanns -> government.politician.government_positions_held -> m.05kfds4 -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nBasic title", "# Reasoning Path:\nMike Johanns -> government.politician.government_positions_held -> m.04l4prr -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nMissouri River -> geography.body_of_water.bridges -> ASB Bridge -> common.topic.notable_for -> g.125hbd7yd\n# Answer:\nASB Bridge", "# Reasoning Path:\nMike Johanns -> common.image.appears_in_topic_gallery -> United States Senate election in Nebraska, 2008 -> common.topic.image -> Mike Johanns official Senate photo\n# Answer:\nUnited States Senate election in Nebraska, 2008", "# Reasoning Path:\nMike Johanns -> government.politician.government_positions_held -> m.094tpcz -> government.government_position_held.office_position_or_title -> United States Secretary of Agriculture\n# Answer:\nUnited States Secretary of Agriculture", "# Reasoning Path:\nMike Johanns -> government.politician.government_positions_held -> m.05kfds4 -> freebase.valuenotation.is_reviewed -> District represented (if position is district-related)\n# Answer:\nDistrict represented (if position is district-related)", "# Reasoning Path:\nMissouri River -> geography.body_of_water.bridges -> ASB Bridge -> common.topic.notable_types -> Bridge\n# Answer:\nASB Bridge"], "ground_truth": ["Nebraska"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-712_6099ea03f4fd2476605c4a513318d29c", "prediction": ["# Reasoning Path:\nLiberal Government of New Zealand -> government.political_appointer.appointees -> m.0wzvl9f -> government.government_position_held.jurisdiction_of_office -> New Zealand\n# Answer:\nNew Zealand", "# Reasoning Path:\nLiberal Government of New Zealand -> government.political_appointer.appointees -> m.0wpq63h -> government.government_position_held.jurisdiction_of_office -> New Zealand\n# Answer:\nNew Zealand", "# Reasoning Path:\nLiberal Government of New Zealand -> government.political_appointer.appointees -> m.0wpwwvp -> government.government_position_held.jurisdiction_of_office -> New Zealand\n# Answer:\nNew Zealand", "# Reasoning Path:\nLiberal Government of New Zealand -> government.political_appointer.appointees -> m.0wzvl9f -> government.government_position_held.office_position_or_title -> Member of the New Zealand Legislative Council\n# Answer:\nMember of the New Zealand Legislative Council", "# Reasoning Path:\nLiberal Government of New Zealand -> common.topic.notable_for -> g.1yl5spjz4\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nLiberal Government of New Zealand -> government.political_appointer.appointees -> m.0wzvl9f -> government.government_position_held.governmental_body -> New Zealand Legislative Council\n# Answer:\nNew Zealand Legislative Council", "# Reasoning Path:\nCountry -> freebase.type_profile.kind -> Definition\n# Answer:\nDefinition", "# Reasoning Path:\nLiberal Government of New Zealand -> government.political_appointer.appointees -> m.0wpq63h -> government.government_position_held.office_holder -> James Kerr\n# Answer:\nJames Kerr", "# Reasoning Path:\nOceania -> location.location.contains -> New Zealand -> base.aareas.schema.administrative_area.administrative_children -> Auckland Region\n# Answer:\nNew Zealand", "# Reasoning Path:\nOceania -> location.location.contains -> New Zealand -> location.country.capital -> Wellington\n# Answer:\nNew Zealand"], "ground_truth": ["New Zealand"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.3333333333333333, "path_precision": 0.5, "path_recall": 0.25, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-743_0a8cdba29cf260283b7c890b3609c0b9", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Caroline Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nCaroline Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> John F. Kennedy Jr. -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nJohn F. Kennedy Jr.", "# Reasoning Path:\nMale -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm -> medicine.risk_factor.diseases -> Intracranial aneurysm\n# Answer:\nAbdominal aortic aneurysm", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Caroline Kennedy -> people.person.sibling_s -> m.07ydm_5\n# Answer:\nCaroline Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.sibling_s -> m.02_wjwf -> people.sibling_relationship.sibling -> Ted Kennedy\n# Answer:\nTed Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.sibling_s -> m.0pbycd1\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> freebase.valuenotation.has_no_value -> Children\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.sibling_s -> m.03hs9fc -> people.sibling_relationship.sibling -> Robert F. Kennedy\n# Answer:\nRobert F. Kennedy", "# Reasoning Path:\nMale -> freebase.domain_category.domains -> Massimo Zanini -> type.domain.owners -> Owners of ciccia's default types\n# Answer:\nMassimo Zanini"], "ground_truth": ["Robert F. Kennedy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.01342281879194631, "path_precision": 0.1, "path_recall": 0.007194244604316547, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-832_c334509bb5e02cacae1ba2e80c176499", "prediction": ["# Reasoning Path:\nLou Seal -> sports.mascot.team -> San Francisco Giants -> sports.sports_team.championships -> 2014 World Series\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nLou Seal -> sports.mascot.team -> San Francisco Giants -> sports.sports_team.championships -> 2010 World Series\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nLou Seal -> sports.mascot.team -> San Francisco Giants -> time.participant.event -> 2010 Major League Baseball season\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nLou Seal -> common.topic.article -> m.03_dwt\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nLou Seal -> sports.mascot.team -> San Francisco Giants -> sports.sports_team.championships -> 2012 World Series\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nLou Seal -> sports.mascot.team -> San Francisco Giants -> time.participant.event -> 2010 World Series\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nLou Seal -> sports.mascot.team -> San Francisco Giants -> time.participant.event -> 2012 Major League Baseball season\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nLou Seal -> sports.mascot.team -> San Francisco Giants -> baseball.baseball_team.team_stats -> m.05n69_j\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nLou Seal -> common.topic.image -> Lou-Seal.jpg -> common.image.size -> m.0kksz7\n# Answer:\nLou-Seal.jpg", "# Reasoning Path:\nLou Seal -> sports.mascot.team -> San Francisco Giants -> baseball.baseball_team.team_stats -> m.05n69l5\n# Answer:\nSan Francisco Giants"], "ground_truth": ["2014 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-96_11da03aa9cec8b011619c8ea0dbfdcf9", "prediction": ["# Reasoning Path:\nEast Whittier Elementary School -> common.topic.notable_for -> g.1q3sf7nqf\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nEast Whittier Elementary School -> education.educational_institution.students_graduates -> m.0_sp3dv -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nEast Whittier Elementary School -> education.educational_institution.students_graduates -> m.0_sp3dv -> education.education.student -> Richard Nixon\n# Answer:\nRichard Nixon", "# Reasoning Path:\nEast Whittier Elementary School -> education.educational_institution.students_graduates -> m.0_sp3dv -> freebase.valuenotation.has_no_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nEast Whittier Elementary School -> education.educational_institution.students_graduates -> m.0_sp3dv -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nEast Whittier Elementary School -> common.topic.notable_types -> Educational Institution -> type.type.expected_by -> School\n# Answer:\nEducational Institution", "# Reasoning Path:\nEast Whittier Elementary School -> education.educational_institution.students_graduates -> m.0_sp3dv -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nMinor", "# Reasoning Path:\nEast Whittier Elementary School -> education.educational_institution.students_graduates -> m.0_sp3dv -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nEast Whittier Elementary School -> common.topic.notable_types -> Educational Institution -> freebase.type_profile.strict_included_types -> Agent\n# Answer:\nEducational Institution", "# Reasoning Path:\nEast Whittier Elementary School -> common.topic.notable_types -> Educational Institution -> type.type.properties -> Athletics brand\n# Answer:\nEducational Institution"], "ground_truth": ["New York City"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-983_ccda690fb745939d0a62c3fbcf3e3769", "prediction": ["# Reasoning Path:\nThe Maneater -> education.school_newspaper.school -> University of Missouri -> education.educational_institution.faculty -> m.0k7ct71\n# Answer:\nUniversity of Missouri", "# Reasoning Path:\nThe Maneater -> education.school_newspaper.school -> University of Missouri -> education.educational_institution.faculty -> m.0kdpm6q\n# Answer:\nUniversity of Missouri", "# Reasoning Path:\nThe Maneater -> education.school_newspaper.school -> University of Missouri -> business.employer.employees -> m.0101mh0f\n# Answer:\nUniversity of Missouri", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> Universities -> rdf-schema#domain -> Cityscape\n# Answer:\nUniversities", "# Reasoning Path:\nTennessee Williams -> book.author.works_written -> 27 Wagons Full of Cotton -> media_common.adapted_work.adaptations -> Baby Doll\n# Answer:\n27 Wagons Full of Cotton", "# Reasoning Path:\nThe Maneater -> education.school_newspaper.school -> University of Missouri -> education.university.local_tuition -> m.0h6_sxs\n# Answer:\nUniversity of Missouri", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> Universities -> type.property.schema -> Cityscape\n# Answer:\nUniversities", "# Reasoning Path:\nThe Maneater -> book.newspaper.circulation_areas -> Columbia -> location.citytown.postal_codes -> 65201\n# Answer:\nColumbia", "# Reasoning Path:\nThe Maneater -> book.newspaper.circulation_areas -> Columbia -> base.biblioness.bibs_location.state -> Missouri\n# Answer:\nColumbia", "# Reasoning Path:\nThe Maneater -> book.newspaper.headquarters -> m.0h_n8wp -> common.topic.notable_types -> Location\n# Answer:\nLocation"], "ground_truth": ["University of Missouri"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.16000000000000003, "path_precision": 0.4, "path_recall": 0.1, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-998_f693ae2cb9aa6b8ffdf0f103a6777b62", "prediction": ["# Reasoning Path:\nJames L. Dolan -> sports.sports_team_owner.teams_owned -> New York Knicks -> sports.sports_team.championships -> 1973 NBA Finals\n# Answer:\nNew York Knicks", "# Reasoning Path:\nJames L. Dolan -> sports.sports_team_owner.teams_owned -> New York Knicks -> sports.sports_team.championships -> 1970 NBA Finals\n# Answer:\nNew York Knicks", "# Reasoning Path:\nJames L. Dolan -> sports.sports_team_owner.teams_owned -> New York Knicks -> sports.sports_team.arena_stadium -> Madison Square Garden\n# Answer:\nNew York Knicks", "# Reasoning Path:\nJames L. Dolan -> sports.sports_team_owner.teams_owned -> New York Knicks -> sports.professional_sports_team.draft_picks -> m.0110p_hq\n# Answer:\nNew York Knicks", "# Reasoning Path:\nJames L. Dolan -> people.person.parents -> Charles Dolan -> people.person.place_of_birth -> Cleveland\n# Answer:\nCharles Dolan", "# Reasoning Path:\nJames L. Dolan -> sports.sports_team_owner.teams_owned -> New York Knicks -> sports.professional_sports_team.draft_picks -> m.0110xhnk\n# Answer:\nNew York Knicks", "# Reasoning Path:\nJames L. Dolan -> sports.sports_team_owner.teams_owned -> New York Knicks -> sports.professional_sports_team.draft_picks -> m.0115dpt8\n# Answer:\nNew York Knicks", "# Reasoning Path:\nJames L. Dolan -> music.group_member.membership -> m.011nhcms -> freebase.valuenotation.is_reviewed -> Member\n# Answer:\nMember", "# Reasoning Path:\nJames L. Dolan -> people.person.parents -> Charles Dolan -> people.person.profession -> Businessperson\n# Answer:\nCharles Dolan", "# Reasoning Path:\nJames L. Dolan -> music.group_member.membership -> m.011nhcms -> freebase.valuenotation.has_no_value -> Period (end)\n# Answer:\nPeriod (end)"], "ground_truth": ["1973 NBA Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTrn-1023_1e7110e48c30a2cef3caf291e3b8d394", "prediction": ["# Reasoning Path:\nBuenos Aires -> location.administrative_division.country -> Argentina -> location.country.first_level_divisions -> Buenos Aires Province\n# Answer:\nArgentina", "# Reasoning Path:\nBuenos Aires -> location.administrative_division.country -> Argentina -> location.country.administrative_divisions -> Ezeiza Partido\n# Answer:\nArgentina", "# Reasoning Path:\nBuenos Aires -> location.administrative_division.country -> Argentina -> periodicals.newspaper_circulation_area.newspapers -> \u00c1mbito Financiero\n# Answer:\nArgentina", "# Reasoning Path:\nBuenos Aires -> location.administrative_division.country -> Argentina -> periodicals.newspaper_circulation_area.newspapers -> Clar\u00edn\n# Answer:\nArgentina", "# Reasoning Path:\nBuenos Aires -> film.film_location.featured_in_films -> Awesome Fuckers -> base.objectionablecontent.flagged_content.flags -> Adult\n# Answer:\nAwesome Fuckers", "# Reasoning Path:\nBuenos Aires -> periodicals.newspaper_circulation_area.newspapers -> Clar\u00edn -> book.newspaper.circulation_areas -> Argentina\n# Answer:\nClar\u00edn", "# Reasoning Path:\nBuenos Aires -> location.administrative_division.country -> Argentina -> location.country.administrative_divisions -> Buenos Aires Province\n# Answer:\nArgentina", "# Reasoning Path:\nBuenos Aires -> film.film_location.featured_in_films -> Awesome Fuckers -> award.award_nominated_work.award_nominations -> m.0bc5bgv\n# Answer:\nAwesome Fuckers", "# Reasoning Path:\nBuenos Aires -> location.administrative_division.country -> Argentina -> periodicals.newspaper_circulation_area.newspapers -> La Naci\u00f3n\n# Answer:\nArgentina", "# Reasoning Path:\nBuenos Aires -> location.administrative_division.country -> Argentina -> location.country.administrative_divisions -> Comuna 1\n# Answer:\nArgentina"], "ground_truth": ["Argentine peso"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-1077_0c34ca057060e35aa5c74fbbca682dee", "prediction": ["# Reasoning Path:\nBadakhshan Province -> base.aareas.schema.administrative_area.administrative_parent -> Afghanistan -> location.location.containedby -> Asia\n# Answer:\nAfghanistan", "# Reasoning Path:\nBadakhshan Province -> base.aareas.schema.administrative_area.administrative_parent -> Afghanistan -> location.statistical_region.religions -> m.0493b56\n# Answer:\nAfghanistan", "# Reasoning Path:\nBadakhshan Province -> location.location.containedby -> Afghanistan -> location.location.containedby -> Asia\n# Answer:\nAfghanistan", "# Reasoning Path:\nBadakhshan Province -> base.aareas.schema.administrative_area.administrative_parent -> Afghanistan -> location.location.containedby -> Eurasia\n# Answer:\nAfghanistan", "# Reasoning Path:\nBadakhshan Province -> base.aareas.schema.administrative_area.administrative_parent -> Afghanistan -> location.statistical_region.religions -> m.0493b5d\n# Answer:\nAfghanistan", "# Reasoning Path:\nBadakhshan Province -> location.location.containedby -> Afghanistan -> location.statistical_region.religions -> m.0493b56\n# Answer:\nAfghanistan", "# Reasoning Path:\nBadakhshan Province -> base.aareas.schema.administrative_area.administrative_parent -> Afghanistan -> location.location.containedby -> South Asia\n# Answer:\nAfghanistan", "# Reasoning Path:\nBadakhshan Province -> location.location.containedby -> Afghanistan -> location.location.containedby -> Eurasia\n# Answer:\nAfghanistan", "# Reasoning Path:\nBadakhshan Province -> location.location.containedby -> Afghanistan -> location.statistical_region.religions -> m.0493b5d\n# Answer:\nAfghanistan", "# Reasoning Path:\nBadakhshan Province -> base.aareas.schema.administrative_area.administrative_parent -> Afghanistan -> location.location.partially_contains -> Himalayas\n# Answer:\nAfghanistan"], "ground_truth": ["Shia Islam", "Sunni Islam"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-1077_f4a9e5f1e0dcfb82cbadf4771eda7bb5", "prediction": ["# Reasoning Path:\nAfghan National Anthem -> music.composition.language -> Pashto language -> language.human_language.main_country -> Pakistan\n# Answer:\nPashto language", "# Reasoning Path:\nAfghan National Anthem -> music.composition.language -> Pashto language -> language.human_language.countries_spoken_in -> Pakistan\n# Answer:\nPashto language", "# Reasoning Path:\nAfghan National Anthem -> music.composition.language -> Pashto language -> language.human_language.countries_spoken_in -> Iran\n# Answer:\nPashto language", "# Reasoning Path:\nAfghan National Anthem -> music.composition.language -> Pashto language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nPashto language", "# Reasoning Path:\nAfghan National Anthem -> common.topic.notable_types -> National Anthem -> freebase.type_profile.published -> Published\n# Answer:\nNational Anthem", "# Reasoning Path:\nAfghan National Anthem -> music.composition.language -> Pashto language -> language.human_language.countries_spoken_in -> Afghanistan\n# Answer:\nPashto language", "# Reasoning Path:\nAfghan National Anthem -> music.composition.lyricist -> Abdul Bari Jahani -> common.topic.notable_for -> g.1yp3d718c\n# Answer:\nAbdul Bari Jahani", "# Reasoning Path:\nAfghan National Anthem -> common.topic.notable_types -> National Anthem -> freebase.type_profile.equivalent_topic -> National anthem\n# Answer:\nNational Anthem", "# Reasoning Path:\nAfghan National Anthem -> common.topic.notable_types -> National Anthem -> type.type.expected_by -> Anthem\n# Answer:\nNational Anthem", "# Reasoning Path:\nAfghan National Anthem -> music.composition.lyricist -> Abdul Bari Jahani -> common.topic.notable_types -> Person\n# Answer:\nAbdul Bari Jahani"], "ground_truth": ["Shia Islam", "Sunni Islam"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-124_028bb5f442b37a4af9f9fd9fa0bc5e9a", "prediction": ["# Reasoning Path:\nWilliam O. Schaefer Elementary School -> education.educational_institution.students_graduates -> m.0w17_7x -> education.education.student -> Angelina Jolie\n# Answer:\nAngelina Jolie", "# Reasoning Path:\nWilliam O. Schaefer Elementary School -> education.educational_institution.students_graduates -> m.0w17_7x -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nWilliam O. Schaefer Elementary School -> common.topic.notable_types -> School -> type.type.expected_by -> Schools\n# Answer:\nSchool", "# Reasoning Path:\nWilliam O. Schaefer Elementary School -> education.school.lowest_grade_taught -> Pre-kindergarten -> common.topic.notable_types -> Grade level\n# Answer:\nPre-kindergarten", "# Reasoning Path:\nWilliam O. Schaefer Elementary School -> common.topic.notable_types -> School -> type.type.properties -> School district\n# Answer:\nSchool", "# Reasoning Path:\nWilliam O. Schaefer Elementary School -> common.topic.notable_types -> School -> freebase.type_hints.included_types -> Educational Institution\n# Answer:\nSchool", "# Reasoning Path:\nWilliam O. Schaefer Elementary School -> education.educational_institution.students_graduates -> m.0w17_7x -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nWilliam O. Schaefer Elementary School -> common.topic.notable_types -> School -> type.type.expected_by -> Contains School\n# Answer:\nSchool", "# Reasoning Path:\nWilliam O. Schaefer Elementary School -> common.topic.notable_types -> School -> type.type.expected_by -> schools\n# Answer:\nSchool", "# Reasoning Path:\nWilliam O. Schaefer Elementary School -> common.topic.notable_types -> School -> freebase.type_hints.included_types -> Topic\n# Answer:\nSchool"], "ground_truth": ["By the Sea", "In the Land of Blood and Honey", "Unbroken", "A Place in Time"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-124_f3990dc9aa470fa81ec4cf2912a9924f", "prediction": ["# Reasoning Path:\nAjla -> common.topic.notable_for -> g.125cl9wjn\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAngelina Jolie -> film.director.film -> Unbroken -> common.topic.notable_for -> g.1yl5y8gg2\n# Answer:\nUnbroken", "# Reasoning Path:\nAjla -> film.film_character.portrayed_in_films -> m.0gw7h9w -> film.performance.actor -> Zana Marjanovic\n# Answer:\nZana Marjanovic", "# Reasoning Path:\nAngelina Jolie -> film.editor.film -> Unbroken -> common.topic.notable_for -> g.1yl5y8gg2\n# Answer:\nUnbroken", "# Reasoning Path:\nAjla -> film.film_character.portrayed_in_films -> m.0gw7h9w -> film.performance.film -> In the Land of Blood and Honey\n# Answer:\nIn the Land of Blood and Honey", "# Reasoning Path:\nAngelina Jolie -> film.director.film -> Unbroken -> film.film.other_crew -> m.0zt6vwx\n# Answer:\nUnbroken", "# Reasoning Path:\nAngelina Jolie -> film.director.film -> In the Land of Blood and Honey -> film.film.starring -> m.0gw7h9w\n# Answer:\nIn the Land of Blood and Honey", "# Reasoning Path:\nAngelina Jolie -> film.director.film -> Unbroken -> film.film.starring -> m.0zt6s7n\n# Answer:\nUnbroken", "# Reasoning Path:\nAngelina Jolie -> film.director.film -> A Place in Time -> film.film.language -> English Language\n# Answer:\nA Place in Time", "# Reasoning Path:\nAjla -> common.topic.article -> m.0gwfc1x -> common.document.content -> m.0gwfc1m\n# Answer:\ncommon.document.content"], "ground_truth": ["In the Land of Blood and Honey"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTrn-1259_1997cb4922db71983be26e6a509950f4", "prediction": ["# Reasoning Path:\nCountry Nation World Tour -> music.concert_tour.artist -> Brad Paisley -> people.person.education -> m.0h3d7qb\n# Answer:\nBrad Paisley", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> Universities -> type.property.schema -> Cityscape\n# Answer:\nUniversities", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> University -> organization.organization_type.organizations_of_this_type -> University of Oxford\n# Answer:\nUniversity", "# Reasoning Path:\nCountry Nation World Tour -> music.concert_tour.artist -> Brad Paisley -> people.person.education -> m.0h3d7qj\n# Answer:\nBrad Paisley", "# Reasoning Path:\nBachelor's degree -> common.topic.article -> m.019v9v\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nCountry Nation World Tour -> music.concert_tour.artist -> Brad Paisley -> people.person.education -> m.0n1dd_6\n# Answer:\nBrad Paisley", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> Universities -> rdf-schema#domain -> Cityscape\n# Answer:\nUniversities", "# Reasoning Path:\nCountry Nation World Tour -> music.concert_tour.artist -> Brad Paisley -> tv.tv_actor.guest_roles -> m.09n_m_4\n# Answer:\nBrad Paisley", "# Reasoning Path:\nBachelor's degree -> common.topic.subject_of -> Europ\u00e4ische Fernhochschule Hamburg -> organization.organization.place_founded -> Hamburg\n# Answer:\nEurop\u00e4ische Fernhochschule Hamburg", "# Reasoning Path:\nCountry Nation World Tour -> music.concert_tour.artist -> Brad Paisley -> music.composer.compositions -> 364 Days to Go\n# Answer:\nBrad Paisley"], "ground_truth": ["Belmont University"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-1394_dc71f08a76d71762a0cfd30f11f66535", "prediction": ["# Reasoning Path:\nAnnaba Province -> location.administrative_division.country -> Algeria -> location.location.containedby -> Africa\n# Answer:\nAlgeria", "# Reasoning Path:\nAnnaba Province -> base.aareas.schema.administrative_area.administrative_parent -> Algeria -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAlgeria", "# Reasoning Path:\nAnnaba Province -> location.administrative_division.country -> Algeria -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAlgeria", "# Reasoning Path:\nAnnaba Province -> location.location.containedby -> Algeria -> location.location.containedby -> Africa\n# Answer:\nAlgeria", "# Reasoning Path:\nAnnaba Province -> base.aareas.schema.administrative_area.administrative_parent -> Algeria -> location.location.containedby -> Africa\n# Answer:\nAlgeria", "# Reasoning Path:\nAnnaba Province -> location.administrative_division.country -> Algeria -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nAlgeria", "# Reasoning Path:\nTurkey -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> Sovereign state\n# Answer:\nEarth", "# Reasoning Path:\nTurkey -> location.statistical_region.co2_emissions_per_capita -> g.1245_2gxt\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nAnnaba Province -> location.location.containedby -> Algeria -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAlgeria", "# Reasoning Path:\nTurkey -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60z059s\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp"], "ground_truth": ["Algeria"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.2, "path_recall": 0.3333333333333333, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTrn-1557_edfd3507d32929ce0e9d844f7a2674de", "prediction": ["# Reasoning Path:\nCharles R. Drew -> people.person.education -> m.0n0w451 -> education.education.institution -> McGill University Faculty of Medicine\n# Answer:\nMcGill University Faculty of Medicine", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> Universities -> type.property.schema -> Cityscape\n# Answer:\nUniversities", "# Reasoning Path:\nCharles R. Drew -> common.topic.article -> m.018t6f\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nLe D\u00e9lit fran\u00e7ais -> education.school_newspaper.school -> McGill University -> education.educational_institution.subsidiary_or_constituent_schools -> McGill University Faculty of Medicine\n# Answer:\nMcGill University", "# Reasoning Path:\nCharles R. Drew -> people.person.education -> m.03l72ff -> education.education.institution -> Columbia University\n# Answer:\nColumbia University", "# Reasoning Path:\nLe D\u00e9lit fran\u00e7ais -> book.newspaper.circulation_areas -> Montreal -> travel.travel_destination.tourist_attractions -> Bell Centre\n# Answer:\nMontreal", "# Reasoning Path:\nLe D\u00e9lit fran\u00e7ais -> education.school_newspaper.school -> McGill University -> education.educational_institution.subsidiary_or_constituent_schools -> Desautels Faculty of Management\n# Answer:\nMcGill University", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> Universities -> rdf-schema#domain -> Cityscape\n# Answer:\nUniversities", "# Reasoning Path:\nLe D\u00e9lit fran\u00e7ais -> education.school_newspaper.school -> McGill University -> education.university.departments -> McGill University Faculty of Education\n# Answer:\nMcGill University", "# Reasoning Path:\nLe D\u00e9lit fran\u00e7ais -> book.newspaper.circulation_areas -> Montreal -> travel.travel_destination.tourist_attractions -> Canadian Centre for Architecture\n# Answer:\nMontreal"], "ground_truth": ["McGill University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.2727272727272727, "path_precision": 0.3, "path_recall": 0.25, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTrn-1770_8db36acba886620a06031d39165d78de", "prediction": ["# Reasoning Path:\nI Am... World Tour -> music.concert_tour.artist -> Beyonc\u00e9 Knowles -> music.composer.compositions -> Ave Maria\n# Answer:\nBeyonc\u00e9 Knowles", "# Reasoning Path:\nI Am... World Tour -> music.concert_tour.artist -> Beyonc\u00e9 Knowles -> music.composer.compositions -> Beyonc\u00e9 Interlude\n# Answer:\nBeyonc\u00e9 Knowles", "# Reasoning Path:\nI Am... World Tour -> music.concert_tour.artist -> Beyonc\u00e9 Knowles -> music.artist.genre -> Contemporary R&B\n# Answer:\nBeyonc\u00e9 Knowles", "# Reasoning Path:\nI Am... World Tour -> time.event.locations -> United States of America -> periodicals.newspaper_circulation_area.newspapers -> Draugas\n# Answer:\nUnited States of America", "# Reasoning Path:\nI Am... World Tour -> music.concert_tour.artist -> Beyonc\u00e9 Knowles -> music.composer.compositions -> Blow\n# Answer:\nBeyonc\u00e9 Knowles", "# Reasoning Path:\nI Am... World Tour -> music.concert_tour.artist -> Beyonc\u00e9 Knowles -> music.lyricist.lyrics_written -> Ave Maria\n# Answer:\nBeyonc\u00e9 Knowles", "# Reasoning Path:\nI Am... World Tour -> music.concert_tour.artist -> Beyonc\u00e9 Knowles -> music.artist.genre -> Pop music\n# Answer:\nBeyonc\u00e9 Knowles", "# Reasoning Path:\nI Am... World Tour -> time.event.locations -> United States of America -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nUnited States of America", "# Reasoning Path:\nI Am... World Tour -> music.concert_tour.artist -> Beyonc\u00e9 Knowles -> music.lyricist.lyrics_written -> Beyonc\u00e9 Interlude\n# Answer:\nBeyonc\u00e9 Knowles", "# Reasoning Path:\nI Am... World Tour -> time.event.locations -> United States of America -> cvg.computer_game_region.versions_released_in_this_region -> Sonic Adventure\n# Answer:\nUnited States of America"], "ground_truth": ["Blue Ivy"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-1812_5fb6c8c4d53d55c03445e4b2454ef810", "prediction": ["# Reasoning Path:\nEast Caribbean dollar -> common.topic.webpage -> m.04lst4z -> common.webpage.resource -> m.0blmbf9\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nCaribbean -> location.location.contains -> Puerto Rico -> location.location.containedby -> Americas\n# Answer:\nPuerto Rico", "# Reasoning Path:\nCaribbean -> location.location.contains -> Puerto Rico -> location.country.languages_spoken -> English Language\n# Answer:\nPuerto Rico", "# Reasoning Path:\nCountry -> base.descriptive_names.names.descriptive_name -> m.012l2b96 -> base.schemastaging.plural_form.language -> Turkish\n# Answer:\nTurkish", "# Reasoning Path:\nEast Caribbean dollar -> common.topic.notable_for -> g.1257_pshm\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nEast Caribbean dollar -> common.topic.webpage -> m.04lst4z -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nCaribbean -> location.location.contains -> Puerto Rico -> location.location.containedby -> Greater Antilles\n# Answer:\nPuerto Rico", "# Reasoning Path:\nCountry -> base.descriptive_names.names.descriptive_name -> m.012r6gwr -> base.schemastaging.plural_form.language -> Chinese\n# Answer:\nChinese", "# Reasoning Path:\nCaribbean -> location.location.contains -> Puerto Rico -> location.location.containedby -> Latin America\n# Answer:\nPuerto Rico", "# Reasoning Path:\nCaribbean -> location.location.contains -> Puerto Rico -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nPuerto Rico"], "ground_truth": ["Saint Lucia", "Saint Kitts and Nevis", "Anguilla"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-1817_89670933168c3f4e5195a241f9d46e76", "prediction": ["# Reasoning Path:\nTi\u1ebfn Qu\u00e2n Ca -> music.composition.language -> Vietnamese Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nVietnamese Language", "# Reasoning Path:\nTi\u1ebfn Qu\u00e2n Ca -> music.composition.language -> Vietnamese Language -> language.human_language.main_country -> Vietnam\n# Answer:\nVietnamese Language", "# Reasoning Path:\nTi\u1ebfn Qu\u00e2n Ca -> music.composition.language -> Vietnamese Language -> language.human_language.countries_spoken_in -> Thailand\n# Answer:\nVietnamese Language", "# Reasoning Path:\nTi\u1ebfn Qu\u00e2n Ca -> music.composition.language -> Vietnamese Language -> base.rosetta.languoid.document -> Vietnamese\n# Answer:\nVietnamese Language", "# Reasoning Path:\nTi\u1ebfn Qu\u00e2n Ca -> music.composition.composer -> V\u0103n Cao -> people.person.profession -> Artist\n# Answer:\nV\u0103n Cao", "# Reasoning Path:\nTi\u1ebfn Qu\u00e2n Ca -> music.composition.recordings -> Vietnam -> music.recording.artist -> Slovak Radio Symphony Orchestra\n# Answer:\nVietnam", "# Reasoning Path:\nTi\u1ebfn Qu\u00e2n Ca -> music.composition.composer -> V\u0103n Cao -> people.person.profession -> Composer\n# Answer:\nV\u0103n Cao", "# Reasoning Path:\nTi\u1ebfn Qu\u00e2n Ca -> music.composition.composer -> V\u0103n Cao -> people.person.gender -> Male\n# Answer:\nV\u0103n Cao", "# Reasoning Path:\nTi\u1ebfn Qu\u00e2n Ca -> music.composition.recordings -> Vietnam -> music.recording.artist -> Peter Breiner\n# Answer:\nVietnam", "# Reasoning Path:\nTi\u1ebfn Qu\u00e2n Ca -> music.composition.recordings -> Vietnam -> common.topic.notable_for -> g.12mq391mw\n# Answer:\nVietnam"], "ground_truth": ["Single-party state", "Socialist state", "Communist state"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-1841_6bf772ee1b388c1b3c226b02c8244aa7", "prediction": ["# Reasoning Path:\n2014 Eurocup Finals -> sports.sports_championship_event.champion -> Valencia BC -> sports.sports_team.roster -> m.02397sx\n# Answer:\nValencia BC", "# Reasoning Path:\n2014 Eurocup Finals -> sports.sports_championship_event.champion -> Valencia BC -> sports.sports_team.location -> Valencia\n# Answer:\nValencia BC", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Real Madrid Baloncesto -> sports.sports_team.colors -> Black\n# Answer:\nReal Madrid Baloncesto", "# Reasoning Path:\n2014 Eurocup Finals -> sports.sports_championship_event.runner_up -> BC UNICS -> sports.sports_team.sport -> Basketball\n# Answer:\nBC UNICS", "# Reasoning Path:\n2014 Eurocup Finals -> sports.sports_championship_event.champion -> Valencia BC -> common.topic.notable_types -> Basketball Team\n# Answer:\nValencia BC", "# Reasoning Path:\n2014 Eurocup Finals -> sports.sports_championship_event.champion -> Valencia BC -> sports.sports_team.roster -> m.0sxflvf\n# Answer:\nValencia BC", "# Reasoning Path:\n2014 Eurocup Finals -> sports.sports_championship_event.champion -> Valencia BC -> sports.sports_team.location -> Spain\n# Answer:\nValencia BC", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Real Madrid Baloncesto -> common.topic.notable_types -> Basketball Team\n# Answer:\nReal Madrid Baloncesto", "# Reasoning Path:\n2014 Eurocup Finals -> sports.sports_championship_event.runner_up -> BC UNICS -> sports.sports_team.arena_stadium -> Basket-Hall Arena\n# Answer:\nBC UNICS", "# Reasoning Path:\n2014 Eurocup Finals -> sports.sports_championship_event.champion -> Valencia BC -> sports.sports_team.roster -> m.0t5bt1c\n# Answer:\nValencia BC"], "ground_truth": ["Valencia BC"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.6, "path_recall": 0.5, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTrn-1841_b8df00139e3fa59b8633ef551ed8ca9f", "prediction": ["# Reasoning Path:\n2010 FIFA World Cup -> base.x2010fifaworldcupsouthafrica.fifa_world_cup.champion -> Spain national football team -> base.x2010fifaworldcupsouthafrica.world_cup_participation.world_cup -> m.079pkv4\n# Answer:\nSpain national football team", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Valencia BC -> sports.sports_team.location -> Valencia\n# Answer:\nValencia BC", "# Reasoning Path:\n2010 FIFA World Cup -> base.x2010fifaworldcupsouthafrica.fifa_world_cup.champion -> Spain national football team -> sports.sports_team.championships -> 2010 FIFA World Cup Final\n# Answer:\nSpain national football team", "# Reasoning Path:\nSpain -> location.statistical_region.co2_emissions_per_capita -> g.1245_38p4\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\n2010 FIFA World Cup -> base.x2010fifaworldcupsouthafrica.fifa_world_cup.champion -> Spain national football team -> soccer.football_team.matches -> 2010 FIFA World Cup Final\n# Answer:\nSpain national football team", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Valencia BC -> common.topic.notable_types -> Basketball Team\n# Answer:\nValencia BC", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Spain national football team -> base.x2010fifaworldcupsouthafrica.world_cup_participation.world_cup -> m.079pkv4\n# Answer:\nSpain national football team", "# Reasoning Path:\nSpain -> location.statistical_region.gni_in_ppp_dollars -> g.11b61jk246\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Valencia BC -> sports.sports_team.sport -> Basketball\n# Answer:\nValencia BC", "# Reasoning Path:\n2010 FIFA World Cup -> base.x2010fifaworldcupsouthafrica.fifa_world_cup.world_cup_qualifications -> 2010 FIFA World Cup qualification UEFA -> common.topic.notable_types -> Event\n# Answer:\n2010 FIFA World Cup qualification UEFA"], "ground_truth": ["Spain national football team"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTrn-1864_67ecd1c247c3b2c9545fbcf1ad8d9d00", "prediction": ["# Reasoning Path:\nAnadyr Time Zone -> time.time_zone.locations_in_this_time_zone -> Asia -> base.locations.continents.countries_within -> Iraq\n# Answer:\nAsia", "# Reasoning Path:\nAnadyr Time Zone -> time.time_zone.locations_in_this_time_zone -> Asia -> base.locations.continents.countries_within -> India\n# Answer:\nAsia", "# Reasoning Path:\nAnadyr Time Zone -> time.time_zone.locations_in_this_time_zone -> Asia -> base.locations.continents.countries_within -> Indonesia\n# Answer:\nAsia", "# Reasoning Path:\nAnadyr Time Zone -> common.topic.notable_types -> Time Zone -> type.type.expected_by -> Time zone\n# Answer:\nTime Zone", "# Reasoning Path:\nAnadyr Time Zone -> common.topic.notable_for -> g.1yl5xk09m\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAnadyr Time Zone -> time.time_zone.locations_in_this_time_zone -> Asia -> location.location.partially_contains -> Soviet Union\n# Answer:\nAsia", "# Reasoning Path:\nAnadyr Time Zone -> time.time_zone.locations_in_this_time_zone -> Asia -> location.location.events -> 2009 flu pandemic\n# Answer:\nAsia", "# Reasoning Path:\nCountry -> base.descriptive_names.names.descriptive_name -> m.012l2b96 -> base.schemastaging.plural_form.language -> Turkish\n# Answer:\nTurkish", "# Reasoning Path:\nAnadyr Time Zone -> time.time_zone.locations_in_this_time_zone -> Asia -> location.location.partially_contains -> Russia\n# Answer:\nAsia", "# Reasoning Path:\nAnadyr Time Zone -> common.topic.notable_types -> Time Zone -> freebase.type_profile.published -> Published\n# Answer:\nTime Zone"], "ground_truth": ["India"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTrn-1864_9dc4e22121d3a46d45b8f9bd9e8c7013", "prediction": ["# Reasoning Path:\nASEAN Common Time Zone -> common.topic.notable_for -> g.125drk0q9\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCountry -> fictional_universe.type_of_fictional_setting.settings -> Arnor\n# Answer:\nArnor", "# Reasoning Path:\nASEAN Common Time Zone -> freebase.valuenotation.has_no_value -> DST offset from UTC -> rdf-schema#range -> Floating Point Number\n# Answer:\nDST offset from UTC", "# Reasoning Path:\nASEAN Common Time Zone -> freebase.valuenotation.has_no_value -> DST offset from UTC -> type.property.expected_type -> Floating Point Number\n# Answer:\nDST offset from UTC", "# Reasoning Path:\nASEAN Common Time Zone -> freebase.valuenotation.has_no_value -> Day DST begins -> rdf-schema#domain -> Time Zone\n# Answer:\nDay DST begins", "# Reasoning Path:\nCountry -> base.descriptive_names.names.descriptive_name -> m.012l2b96 -> base.schemastaging.plural_form.language -> Turkish\n# Answer:\nTurkish", "# Reasoning Path:\nASEAN Common Time Zone -> freebase.valuenotation.has_no_value -> DST offset from UTC -> rdf-schema#domain -> Time Zone\n# Answer:\nDST offset from UTC", "# Reasoning Path:\nASEAN Common Time Zone -> common.topic.notable_types -> Time Zone -> type.type.expected_by -> Time zone\n# Answer:\nTime Zone", "# Reasoning Path:\nASEAN Common Time Zone -> freebase.valuenotation.has_no_value -> Day DST ends -> type.property.schema -> Time Zone\n# Answer:\nDay DST ends", "# Reasoning Path:\nASEAN Common Time Zone -> freebase.valuenotation.has_no_value -> Day DST begins -> rdf-schema#range -> Day Of Year\n# Answer:\nDay DST begins"], "ground_truth": ["India"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-1938_7322a2a4d46bf36b95bfab4418c9a32b", "prediction": ["# Reasoning Path:\nNorthern District -> location.administrative_division.first_level_division_of -> Israel -> location.country.form_of_government -> Parliamentary system\n# Answer:\nIsrael", "# Reasoning Path:\nNorthern District -> base.schemastaging.context_name.pronunciation -> g.125_pgm1x\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nNorthern District -> location.administrative_division.first_level_division_of -> Israel -> location.statistical_region.co2_emissions_per_capita -> g.1245_4dkf\n# Answer:\nIsrael", "# Reasoning Path:\nNorthern District -> location.location.contains -> Rosh Pinna -> location.location.containedby -> Israel\n# Answer:\nRosh Pinna", "# Reasoning Path:\nNorthern District -> location.administrative_division.first_level_division_of -> Israel -> location.statistical_region.energy_use_per_capita -> g.1245_0s2m\n# Answer:\nIsrael", "# Reasoning Path:\nNorthern District -> location.administrative_division.first_level_division_of -> Israel -> location.statistical_region.co2_emissions_per_capita -> g.1245_5xfb\n# Answer:\nIsrael", "# Reasoning Path:\nNorthern District -> location.administrative_division.first_level_division_of -> Israel -> location.statistical_region.co2_emissions_per_capita -> g.1245_6fh_\n# Answer:\nIsrael", "# Reasoning Path:\nNorthern District -> location.administrative_division.first_level_division_of -> Israel -> location.statistical_region.energy_use_per_capita -> g.1245_3vpw\n# Answer:\nIsrael", "# Reasoning Path:\nNorthern District -> location.location.contains -> Rosh Pinna -> location.location.nearby_airports -> Rosh Pina Airport\n# Answer:\nRosh Pinna", "# Reasoning Path:\nNorthern District -> location.location.contains -> Rosh Pinna -> location.location.people_born_here -> Ami Assaf\n# Answer:\nRosh Pinna"], "ground_truth": ["Parliamentary system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTrn-2026_d059b24adec4064377b957ca598769be", "prediction": ["# Reasoning Path:\nSanto Domingo -> base.biblioness.bibs_location.country -> Dominican Republic -> location.location.containedby -> Greater Antilles\n# Answer:\nDominican Republic", "# Reasoning Path:\nSanto Domingo -> base.biblioness.bibs_location.country -> Dominican Republic -> location.country.official_language -> Spanish Language\n# Answer:\nDominican Republic", "# Reasoning Path:\nSanto Domingo -> location.location.containedby -> Dominican Republic -> location.location.containedby -> Greater Antilles\n# Answer:\nDominican Republic", "# Reasoning Path:\nSanto Domingo -> base.biblioness.bibs_location.country -> Dominican Republic -> location.location.containedby -> North America\n# Answer:\nDominican Republic", "# Reasoning Path:\nSanto Domingo -> base.biblioness.bibs_location.country -> Dominican Republic -> location.country.first_level_divisions -> Azua Province\n# Answer:\nDominican Republic", "# Reasoning Path:\nSanto Domingo -> location.location.contains -> Central Santo Domingo -> location.location.containedby -> Dominican Republic\n# Answer:\nCentral Santo Domingo", "# Reasoning Path:\nSanto Domingo -> location.location.containedby -> Dominican Republic -> location.location.containedby -> North America\n# Answer:\nDominican Republic", "# Reasoning Path:\nSanto Domingo -> base.biblioness.bibs_location.country -> Dominican Republic -> location.country.first_level_divisions -> Baoruco Province\n# Answer:\nDominican Republic", "# Reasoning Path:\nSanto Domingo -> location.location.contains -> Central Santo Domingo -> location.location.containedby -> Distrito Nacional\n# Answer:\nCentral Santo Domingo", "# Reasoning Path:\nSanto Domingo -> location.location.contains -> Central Santo Domingo -> location.location.geolocation -> m.011rl320\n# Answer:\nCentral Santo Domingo"], "ground_truth": ["Dominican peso"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-2057_b64f996ceca972c0d6fea3de2705ce63", "prediction": ["# Reasoning Path:\nAlta Verapaz Department -> base.aareas.schema.administrative_area.administrative_parent -> Guatemala -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nGuatemala", "# Reasoning Path:\nAlta Verapaz Department -> base.aareas.schema.administrative_area.administrative_parent -> Guatemala -> location.country.currency_used -> Guatemalan quetzal\n# Answer:\nGuatemala", "# Reasoning Path:\nAlta Verapaz Department -> location.administrative_division.country -> Guatemala -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nGuatemala", "# Reasoning Path:\nAlta Verapaz Department -> base.aareas.schema.administrative_area.administrative_parent -> Guatemala -> common.topic.notable_types -> Country\n# Answer:\nGuatemala", "# Reasoning Path:\nAlta Verapaz Department -> location.administrative_division.country -> Guatemala -> location.country.currency_used -> Guatemalan quetzal\n# Answer:\nGuatemala", "# Reasoning Path:\nAlta Verapaz Department -> location.administrative_division.country -> Guatemala -> common.topic.notable_types -> Country\n# Answer:\nGuatemala", "# Reasoning Path:\nAlta Verapaz Department -> location.location.contains -> Chisec -> location.location.containedby -> Guatemala\n# Answer:\nChisec", "# Reasoning Path:\nAlta Verapaz Department -> location.location.contains -> Chisec -> common.topic.notable_for -> g.12578bzfv\n# Answer:\nChisec", "# Reasoning Path:\nAlta Verapaz Department -> location.location.contains -> Chisec -> common.topic.webpage -> m.04d777j\n# Answer:\nChisec", "# Reasoning Path:\nAlta Verapaz Department -> location.location.contains -> Cob\u00e1n -> location.location.containedby -> Guatemala\n# Answer:\nCob\u00e1n"], "ground_truth": ["Guatemalan quetzal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTrn-2069_0fa727f3b282196eb1097410b4be6818", "prediction": ["# Reasoning Path:\nMichelle Bachelet -> people.person.nationality -> Chile -> location.country.official_language -> Spanish Language\n# Answer:\nChile", "# Reasoning Path:\nMichelle Bachelet -> people.person.nationality -> Chile -> location.country.languages_spoken -> Spanish Language\n# Answer:\nChile", "# Reasoning Path:\nMichelle Bachelet -> people.person.ethnicity -> Chileans -> people.ethnicity.languages_spoken -> Chilean Spanish\n# Answer:\nChileans", "# Reasoning Path:\nMichelle Bachelet -> people.person.nationality -> Chile -> base.aareas.schema.administrative_area.administrative_children -> Antofagasta Region\n# Answer:\nChile", "# Reasoning Path:\nMichelle Bachelet -> people.person.ethnicity -> Chileans -> people.ethnicity.languages_spoken -> Spanish Language\n# Answer:\nChileans", "# Reasoning Path:\nMichelle Bachelet -> freebase.type_hints.included_types -> Person -> type.type.properties -> age\n# Answer:\nPerson", "# Reasoning Path:\nMichelle Bachelet -> people.person.ethnicity -> Chileans -> people.ethnicity.people -> Alberto Larraguibel\n# Answer:\nChileans", "# Reasoning Path:\nMichelle Bachelet -> people.person.nationality -> Chile -> base.aareas.schema.administrative_area.administrative_children -> Atacama Region\n# Answer:\nChile", "# Reasoning Path:\nMichelle Bachelet -> people.person.ethnicity -> Chileans -> common.topic.image -> Cacique Lloncon aprox. 1890\n# Answer:\nChileans", "# Reasoning Path:\nMichelle Bachelet -> freebase.type_hints.included_types -> Person -> type.type.properties -> Languages\n# Answer:\nPerson"], "ground_truth": ["Mapudungun Language", "Puquina Language", "Aymara language", "Rapa Nui Language", "Spanish Language"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.24, "path_ans_precision": 0.3, "path_ans_recall": 0.2}
{"id": "WebQTrn-2152_3cdf60c15a8355981dd92e3c57ac2eed", "prediction": ["# Reasoning Path:\nAmerican League West -> sports.sports_league.teams -> m.0crtd3v -> sports.sports_league_participation.team -> California Angels\n# Answer:\nCalifornia Angels", "# Reasoning Path:\nAmerican League West -> sports.sports_league.teams -> m.0crtd3h -> sports.sports_league_participation.team -> Seattle Pilots\n# Answer:\nSeattle Pilots", "# Reasoning Path:\nAmerican League West -> common.topic.article -> m.07fbnl\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nMariner Moose -> common.topic.notable_for -> g.1255wtl7k\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAmerican League West -> baseball.baseball_division.teams -> Seattle Mariners -> sports.professional_sports_team.draft_picks -> m.04d0n1b\n# Answer:\nSeattle Mariners", "# Reasoning Path:\nMariner Moose -> sports.mascot.team -> Seattle Mariners -> sports.professional_sports_team.draft_picks -> m.04d0n1b\n# Answer:\nSeattle Mariners", "# Reasoning Path:\nAmerican League West -> sports.sports_league.teams -> m.0crtd18 -> sports.sports_league_participation.team -> Oakland Athletics\n# Answer:\nOakland Athletics", "# Reasoning Path:\nAmerican League West -> baseball.baseball_division.teams -> Seattle Mariners -> sports.professional_sports_team.draft_picks -> m.04vw_k5\n# Answer:\nSeattle Mariners", "# Reasoning Path:\nAmerican League West -> baseball.baseball_division.teams -> Seattle Mariners -> baseball.baseball_team.current_coaches -> m.0527dvr\n# Answer:\nSeattle Mariners", "# Reasoning Path:\nMariner Moose -> common.topic.image -> The mascot of the Mariners, the Mariner Moose -> common.image.size -> m.041qhnr\n# Answer:\nThe mascot of the Mariners, the Mariner Moose"], "ground_truth": ["Seattle Mariners"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTrn-2152_92fba37c9723caee68665ad9a5e4a468", "prediction": ["# Reasoning Path:\nAmerican League West -> baseball.baseball_division.teams -> Texas Rangers -> baseball.baseball_team.current_manager -> Tim Bogar\n# Answer:\nTexas Rangers", "# Reasoning Path:\nTom Hicks -> sports.sports_team_owner.teams_owned -> Texas Rangers -> baseball.baseball_team.current_manager -> Tim Bogar\n# Answer:\nTexas Rangers", "# Reasoning Path:\nAmerican League West -> baseball.baseball_division.teams -> Los Angeles Angels of Anaheim -> sports.sports_team.previously_known_as -> California Angels\n# Answer:\nLos Angeles Angels of Anaheim", "# Reasoning Path:\nAmerican League West -> common.topic.article -> m.07fbnl\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nAmerican League West -> baseball.baseball_division.teams -> Texas Rangers -> sports.professional_sports_team.owner_s -> Tom Hicks\n# Answer:\nTexas Rangers", "# Reasoning Path:\nAmerican League West -> sports.sports_league.teams -> m.0crtd3v -> sports.sports_league_participation.team -> California Angels\n# Answer:\nCalifornia Angels", "# Reasoning Path:\nAmerican League West -> baseball.baseball_division.teams -> Texas Rangers -> base.schemastaging.sports_team_extra.managers -> m.011qxts5\n# Answer:\nTexas Rangers", "# Reasoning Path:\nAmerican League West -> baseball.baseball_division.teams -> Los Angeles Angels of Anaheim -> sports.sports_team.previously_known_as -> Anaheim Angels\n# Answer:\nLos Angeles Angels of Anaheim", "# Reasoning Path:\nAmerican League West -> sports.sports_league.teams -> m.0crtd18 -> sports.sports_league_participation.team -> Oakland Athletics\n# Answer:\nOakland Athletics", "# Reasoning Path:\nAmerican League West -> baseball.baseball_division.teams -> Texas Rangers -> sports.professional_sports_team.owner_s -> Ray Davis\n# Answer:\nTexas Rangers"], "ground_truth": ["Texas Rangers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTrn-2189_f4440609f5cecb091bf8e86adb47be25", "prediction": ["# Reasoning Path:\nBilady, Bilady, Bilady -> music.composition.language -> Arabic Language -> language.human_language.dialects -> Arabic, Sudanese Spoken Language\n# Answer:\nArabic Language", "# Reasoning Path:\nBilady, Bilady, Bilady -> music.composition.language -> Arabic Language -> language.human_language.dialects -> Egyptian Arabic\n# Answer:\nArabic Language", "# Reasoning Path:\nBilady, Bilady, Bilady -> music.composition.language -> Arabic Language -> language.human_language.countries_spoken_in -> Israel\n# Answer:\nArabic Language", "# Reasoning Path:\nBilady, Bilady, Bilady -> music.composition.language -> Arabic Language -> language.human_language.dialects -> Bedawi Arabic\n# Answer:\nArabic Language", "# Reasoning Path:\nBilady, Bilady, Bilady -> common.topic.notable_for -> g.125d8rgxk\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nBilady, Bilady, Bilady -> music.composition.language -> Arabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire\n# Answer:\nArabic Language", "# Reasoning Path:\nBilady, Bilady, Bilady -> common.topic.article -> m.0d9nk7\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nBilady, Bilady, Bilady -> music.composition.language -> Arabic Language -> language.human_language.countries_spoken_in -> Sudan\n# Answer:\nArabic Language", "# Reasoning Path:\nBilady, Bilady, Bilady -> music.composition.language -> Arabic Language -> education.field_of_study.students_majoring -> m.05ptp06\n# Answer:\nArabic Language", "# Reasoning Path:\nBilady, Bilady, Bilady -> music.composition.language -> Arabic Language -> education.field_of_study.students_majoring -> m.063lwv7\n# Answer:\nArabic Language"], "ground_truth": ["Modern Standard Arabic"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-21_660138373d19bbffdd3d3f7a30234e4a", "prediction": ["# Reasoning Path:\nDire Dawa -> government.governmental_jurisdiction.governing_officials -> m.0115lk6q -> government.government_position_held.office_holder -> Mersha Nahusenay\n# Answer:\nMersha Nahusenay", "# Reasoning Path:\nPrime minister -> book.book_subject.works -> The Prime Minister -> book.written_work.original_language -> English Language\n# Answer:\nThe Prime Minister", "# Reasoning Path:\nDire Dawa -> common.topic.notable_for -> g.1258m_l7x\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nDire Dawa -> government.governmental_jurisdiction.governing_officials -> m.0115lk6q -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nPrime minister -> book.book_subject.works -> The Prime Minister -> common.topic.article -> m.07m419\n# Answer:\nThe Prime Minister", "# Reasoning Path:\nPrime minister -> book.book_subject.works -> The Prime Minister -> book.book.editions -> Prime Minister\n# Answer:\nThe Prime Minister", "# Reasoning Path:\nDire Dawa -> location.administrative_division.country -> Ethiopia -> location.location.containedby -> Africa\n# Answer:\nEthiopia", "# Reasoning Path:\nDire Dawa -> location.administrative_division.country -> Ethiopia -> location.country.languages_spoken -> English Language\n# Answer:\nEthiopia", "# Reasoning Path:\nPrime minister -> book.book_subject.works -> The Prime Minister -> book.book.editions -> Prime Minister (Oxford Paperbacks)\n# Answer:\nThe Prime Minister", "# Reasoning Path:\nDire Dawa -> location.administrative_division.country -> Ethiopia -> location.country.capital -> Addis Ababa\n# Answer:\nEthiopia"], "ground_truth": ["Hailemariam Desalegn"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-2209_c1374f388d9cc7a78365860c91218362", "prediction": ["# Reasoning Path:\nBrad Stevens -> basketball.basketball_coach.team -> Boston Celtics -> sports.sports_team.championships -> 1961 NBA Finals\n# Answer:\nBoston Celtics", "# Reasoning Path:\nBrad Stevens -> basketball.basketball_coach.team -> Boston Celtics -> sports.sports_team.championships -> 1986 NBA Finals\n# Answer:\nBoston Celtics", "# Reasoning Path:\nBrad Stevens -> basketball.basketball_coach.team -> Boston Celtics -> sports.sports_team.championships -> 1963 NBA Finals\n# Answer:\nBoston Celtics", "# Reasoning Path:\nBrad Stevens -> basketball.basketball_coach.team -> Boston Celtics -> sports.sports_team.roster -> m.0110pz2r\n# Answer:\nBoston Celtics", "# Reasoning Path:\nBrad Stevens -> basketball.basketball_coach.team -> Boston Celtics -> sports.sports_team.colors -> Black\n# Answer:\nBoston Celtics", "# Reasoning Path:\nBrad Stevens -> people.person.children -> Kinsley Stevens -> common.topic.notable_for -> g.12lm0_6ch\n# Answer:\nKinsley Stevens", "# Reasoning Path:\nBrad Stevens -> basketball.basketball_coach.team -> Boston Celtics -> sports.sports_team.roster -> m.0113ywh7\n# Answer:\nBoston Celtics", "# Reasoning Path:\nBrad Stevens -> sports.sports_team_coach.teams_coached -> m.0w3_qv3 -> sports.sports_team_coach_tenure.team -> Boston Celtics\n# Answer:\nBoston Celtics", "# Reasoning Path:\nBrad Stevens -> basketball.basketball_coach.team -> Boston Celtics -> sports.sports_team.roster -> m.0113ztjw\n# Answer:\nBoston Celtics", "# Reasoning Path:\nBrad Stevens -> basketball.basketball_coach.team -> Boston Celtics -> sports.sports_team.colors -> Buff\n# Answer:\nBoston Celtics"], "ground_truth": ["1984 NBA Finals", "1981 NBA Finals", "1968 NBA Finals", "1966 NBA Finals", "1963 NBA Finals", "2008 NBA Finals", "1962 NBA Finals", "1961 NBA Finals", "1969 NBA Finals", "1965 NBA Finals", "1959 NBA Finals", "1976 NBA Finals", "1957 NBA Finals", "1960 NBA Finals", "1974 NBA Finals", "1986 NBA Finals", "1964 NBA Finals"], "ans_acc": 0.17647058823529413, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.22222222222222224, "path_precision": 0.3, "path_recall": 0.17647058823529413, "path_ans_f1": 0.22222222222222224, "path_ans_precision": 0.3, "path_ans_recall": 0.17647058823529413}
{"id": "WebQTrn-2215_11c4cd5a25fd84f3980d7013c0329bad", "prediction": ["# Reasoning Path:\nBeat This Summer Tour -> music.concert_tour.artist -> Brad Paisley -> people.person.place_of_birth -> Glen Dale\n# Answer:\nBrad Paisley", "# Reasoning Path:\nBeat This Summer Tour -> common.topic.notable_for -> g.12pkcn6qn\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nBeat This Summer Tour -> music.concert_tour.artist -> Brad Paisley -> music.composer.compositions -> Beat This Summer\n# Answer:\nBrad Paisley", "# Reasoning Path:\nBeat This Summer Tour -> music.concert_tour.artist -> Brad Paisley -> tv.tv_actor.guest_roles -> m.09n_m_4\n# Answer:\nBrad Paisley", "# Reasoning Path:\nBeat This Summer Tour -> music.concert_tour.artist -> Brad Paisley -> tv.tv_actor.guest_roles -> m.09n_m_9\n# Answer:\nBrad Paisley", "# Reasoning Path:\nBeat This Summer Tour -> common.topic.notable_types -> Event -> type.type.expected_by -> Academy Awards show\n# Answer:\nEvent", "# Reasoning Path:\nBeat This Summer Tour -> common.topic.notable_types -> Event -> common.topic.article -> m.02zd1jj\n# Answer:\nEvent", "# Reasoning Path:\nBeat This Summer Tour -> music.concert_tour.artist -> Brad Paisley -> tv.tv_actor.guest_roles -> m.09n_m_g\n# Answer:\nBrad Paisley", "# Reasoning Path:\nBeat This Summer Tour -> music.concert_tour.artist -> Brad Paisley -> music.composer.compositions -> 364 Days to Go\n# Answer:\nBrad Paisley", "# Reasoning Path:\nBeat This Summer Tour -> common.topic.notable_types -> Event -> type.type.expected_by -> Associated event\n# Answer:\nEvent"], "ground_truth": ["Glen Dale"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTrn-2286_e0906c845ea8b2e22e08e1b0e6eb9b43", "prediction": ["# Reasoning Path:\nFrankfort -> base.biblioness.bibs_location.state -> Kentucky -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nKentucky", "# Reasoning Path:\nFrankfort -> base.biblioness.bibs_location.state -> Kentucky -> location.location.containedby -> Contiguous United States\n# Answer:\nKentucky", "# Reasoning Path:\nFrankfort -> location.location.containedby -> Franklin County -> location.location.containedby -> Frankfort, Kentucky micropolitan area\n# Answer:\nFranklin County", "# Reasoning Path:\nFrankfort -> base.biblioness.bibs_location.state -> Kentucky -> location.location.containedby -> United States of America\n# Answer:\nKentucky", "# Reasoning Path:\nFrankfort -> location.location.containedby -> Franklin County -> location.location.containedby -> Kentucky\n# Answer:\nFranklin County", "# Reasoning Path:\nFrankfort -> location.location.containedby -> Kentucky -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nKentucky", "# Reasoning Path:\nFrankfort -> base.biblioness.bibs_location.state -> Kentucky -> location.location.containedby -> United States, with Territories\n# Answer:\nKentucky", "# Reasoning Path:\nFrankfort -> base.biblioness.bibs_location.state -> Kentucky -> military.military_unit_place_of_origin.military_units -> 10th Kentucky Cavalry Regiment (Diamond's)\n# Answer:\nKentucky", "# Reasoning Path:\nFrankfort -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrankfort -> location.location.containedby -> Franklin County -> location.location.contains -> 40601\n# Answer:\nFranklin County"], "ground_truth": ["Contiguous United States", "United States of America", "United States, with Territories"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.15384615384615383, "ans_precission": 0.1, "ans_recall": 0.3333333333333333, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTrn-241_dfb6c97ac9bf2f0ac07f27dd80f9edc2", "prediction": ["# Reasoning Path:\nNijmegen -> location.location.containedby -> Gelderland -> location.location.containedby -> Kingdom of the Netherlands\n# Answer:\nGelderland", "# Reasoning Path:\nNijmegen -> location.location.containedby -> Kingdom of the Netherlands -> location.location.containedby -> Europe\n# Answer:\nKingdom of the Netherlands", "# Reasoning Path:\nNijmegen -> location.location.containedby -> Netherlands -> location.location.containedby -> Kingdom of the Netherlands\n# Answer:\nNetherlands", "# Reasoning Path:\nNijmegen -> location.location.containedby -> Gelderland -> location.location.containedby -> Netherlands\n# Answer:\nGelderland", "# Reasoning Path:\nNijmegen -> location.location.containedby -> Gelderland -> base.aareas.schema.administrative_area.administrative_parent -> Netherlands\n# Answer:\nGelderland", "# Reasoning Path:\nNijmegen -> base.aareas.schema.administrative_area.administrative_parent -> Gelderland -> location.location.containedby -> Kingdom of the Netherlands\n# Answer:\nGelderland", "# Reasoning Path:\nFrance -> film.film_location.featured_in_films -> Jump Britain -> film.film.country -> United Kingdom\n# Answer:\nJump Britain", "# Reasoning Path:\nFrance -> location.location.containedby -> Europe -> location.location.partially_contains -> Kingdom of the Netherlands\n# Answer:\nEurope", "# Reasoning Path:\nFrance -> location.location.partially_contains -> Aiguille d'Argenti\u00e8re -> geography.mountain.mountain_range -> Alps\n# Answer:\nAiguille d'Argenti\u00e8re", "# Reasoning Path:\nNijmegen -> location.location.containedby -> Kingdom of the Netherlands -> location.location.containedby -> Western Europe\n# Answer:\nKingdom of the Netherlands"], "ground_truth": ["Germany"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-2444_8f2cd432b509e5b8fe681bb55bca2767", "prediction": ["# Reasoning Path:\n1946 World Series -> sports.sports_championship_event.champion -> St. Louis Cardinals -> sports.sports_team.arena_stadium -> Busch Stadium\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\n1946 World Series -> sports.sports_championship_event.champion -> St. Louis Cardinals -> sports.sports_team.arena_stadium -> Roger Dean Stadium\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\n1946 World Series -> time.event.locations -> Fenway Park -> architecture.structure.architecture_firm -> Osborne Engineering Corp.\n# Answer:\nFenway Park", "# Reasoning Path:\n1946 World Series -> sports.sports_championship_event.champion -> St. Louis Cardinals -> sports.sports_team.venue -> m.0nf2b_y\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\n1946 World Series -> time.event.locations -> Fenway Park -> location.location.contains -> Green Monster\n# Answer:\nFenway Park", "# Reasoning Path:\n1946 World Series -> sports.sports_championship_event.champion -> St. Louis Cardinals -> baseball.baseball_team.historical_managers -> m.05pt11w\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\n1946 World Series -> common.topic.notable_types -> Sports League Championship Event -> type.type.expected_by -> Championship\n# Answer:\nSports League Championship Event", "# Reasoning Path:\n1946 World Series -> sports.sports_championship_event.champion -> St. Louis Cardinals -> sports.sports_team.venue -> m.0nf2byb\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\n1946 World Series -> sports.sports_championship_event.champion -> St. Louis Cardinals -> sports.sports_team.venue -> m.0nf2bzj\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\n1946 World Series -> sports.sports_championship_event.champion -> St. Louis Cardinals -> baseball.baseball_team.historical_managers -> m.05pt122\n# Answer:\nSt. Louis Cardinals"], "ground_truth": ["Busch Stadium"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTrn-2444_8fb3f377318c9e61f47779b2c188167b", "prediction": ["# Reasoning Path:\n1931 World Series -> sports.sports_championship_event.champion -> St. Louis Cardinals -> sports.sports_team.arena_stadium -> Busch Stadium\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\n1931 World Series -> sports.sports_championship_event.champion -> St. Louis Cardinals -> sports.sports_team.arena_stadium -> Roger Dean Stadium\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\n1931 World Series -> common.topic.article -> m.04tfzl\n# Answer:\ncommon.topic.article", "# Reasoning Path:\n1931 World Series -> sports.sports_championship_event.champion -> St. Louis Cardinals -> sports.sports_team.venue -> m.0nf2b_y\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\n1931 World Series -> time.event.locations -> Shibe Park -> location.location.events -> 1910 World Series\n# Answer:\nShibe Park", "# Reasoning Path:\n1931 World Series -> time.event.locations -> Shibe Park -> location.location.containedby -> Philadelphia\n# Answer:\nShibe Park", "# Reasoning Path:\n1931 World Series -> sports.sports_championship_event.champion -> St. Louis Cardinals -> baseball.baseball_team.historical_managers -> m.05pt11w\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\n1931 World Series -> time.event.locations -> Shibe Park -> common.topic.article -> m.02z1b8\n# Answer:\nShibe Park", "# Reasoning Path:\n1931 World Series -> sports.sports_championship_event.champion -> St. Louis Cardinals -> sports.sports_team.venue -> m.0nf2byb\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\n1931 World Series -> time.event.locations -> Shibe Park -> location.location.events -> 1911 World Series\n# Answer:\nShibe Park"], "ground_truth": ["Busch Stadium"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTrn-2444_e5059ff268415917df330817b9c8ef8c", "prediction": ["# Reasoning Path:\nWilliam DeWitt, Jr. -> sports.sports_team_owner.teams_owned -> St. Louis Cardinals -> sports.sports_team.arena_stadium -> Busch Stadium\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\nWilliam DeWitt, Jr. -> sports.sports_team_owner.teams_owned -> St. Louis Cardinals -> sports.sports_team_owner.teams_owned -> Springfield Cardinals\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\nWilliam DeWitt, Jr. -> sports.sports_team_owner.teams_owned -> St. Louis Cardinals -> sports.sports_team.arena_stadium -> Roger Dean Stadium\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\nWilliam DeWitt, Jr. -> sports.sports_team_owner.teams_owned -> St. Louis Cardinals -> sports.professional_sports_team.draft_picks -> m.0461q4y\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\nWilliam DeWitt, Jr. -> people.person.places_lived -> m.03pj17k -> people.place_lived.location -> Ohio\n# Answer:\nOhio", "# Reasoning Path:\nWilliam DeWitt, Jr. -> people.person.place_of_birth -> United States of America -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam DeWitt, Jr. -> sports.sports_team_owner.teams_owned -> St. Louis Cardinals -> sports.professional_sports_team.draft_picks -> m.04vw_l1\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\nWilliam DeWitt, Jr. -> people.person.place_of_birth -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Indiana\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam DeWitt, Jr. -> people.person.places_lived -> m.04hsksf -> people.place_lived.location -> St. Louis\n# Answer:\nSt. Louis", "# Reasoning Path:\nWilliam DeWitt, Jr. -> sports.sports_team_owner.teams_owned -> St. Louis Cardinals -> sports.professional_sports_team.draft_picks -> m.04vws6t\n# Answer:\nSt. Louis Cardinals"], "ground_truth": ["Busch Stadium"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTrn-2478_3e09b108c3448248556d4c34acae3cf7", "prediction": ["# Reasoning Path:\nEric F. Spina -> common.topic.notable_types -> Organization leader -> type.type.domain -> Organization\n# Answer:\nOrganization leader", "# Reasoning Path:\nEric F. Spina -> common.topic.notable_for -> g.1ypqs9dxr\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nEric F. Spina -> common.topic.notable_types -> Organization leader -> freebase.type_profile.kind -> Role\n# Answer:\nOrganization leader", "# Reasoning Path:\nEric F. Spina -> business.board_member.leader_of -> m.0zsrysz -> organization.leadership.role -> Interim Chancellor\n# Answer:\nInterim Chancellor", "# Reasoning Path:\nEric F. Spina -> common.topic.notable_types -> Organization leader -> freebase.type_profile.equivalent_topic -> board member\n# Answer:\nOrganization leader", "# Reasoning Path:\nEric F. Spina -> business.board_member.leader_of -> m.0zsrysz -> organization.leadership.organization -> Syracuse University\n# Answer:\nSyracuse University"], "ground_truth": ["Syracuse University Otto the Orange"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-2570_374d1789f1735b6f08e1a829c0d075a2", "prediction": ["# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.05rln07 -> event.speech_or_presentation.speaker_s -> Franklin D. Roosevelt\n# Answer:\nFranklin D. Roosevelt", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03q2h_9 -> government.government_position_held.office_holder -> Franklin D. Roosevelt\n# Answer:\nFranklin D. Roosevelt", "# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.05rpn5h -> event.speech_or_presentation.speaker_s -> Harry S. Truman\n# Answer:\nHarry S. Truman", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03q2h_9 -> government.government_position_held.basic_title -> President\n# Answer:\nPresident", "# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.05rln07 -> event.speech_or_presentation.presented_work -> Arsenal of Democracy\n# Answer:\nArsenal of Democracy", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx817 -> government.government_position_held.office_holder -> Harry S. Truman\n# Answer:\nHarry S. Truman", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.01xrx8q -> government.government_position_held.office_holder -> George H. W. Bush\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.010nmtv6 -> event.speech_or_presentation.speaker_s -> Y\u014dhei K\u014dno\n# Answer:\nY\u014dhei K\u014dno", "# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.05rpn5h -> event.speech_or_presentation.event -> Harry S. Truman 1949 presidential inauguration\n# Answer:\nHarry S. Truman 1949 presidential inauguration", "# Reasoning Path:\nPresident of the United States -> common.topic.webpage -> m.04lsvx0 -> common.webpage.resource -> m.0bm153k\n# Answer:\ncommon.webpage.resource"], "ground_truth": ["Harry S. Truman"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTrn-2664_471b83eade9707a4dba68e201bf29d73", "prediction": ["# Reasoning Path:\nChina -> location.statistical_region.places_exported_to -> m.04bccq0 -> location.imports_and_exports.exported_to -> Sierra Leone\n# Answer:\nSierra Leone", "# Reasoning Path:\nGreenwich Mean Time Zone -> time.time_zone.day_dst_begins -> Last Sunday in March -> time.day_of_year.calendar_system -> Gregorian calendar\n# Answer:\nLast Sunday in March", "# Reasoning Path:\nGreenwich Mean Time Zone -> common.topic.article -> m.03bf4\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nChina -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Sierra Leone\n# Answer:\nEnglish Language", "# Reasoning Path:\nChina -> location.statistical_region.places_exported_to -> m.049ygql -> location.imports_and_exports.exported_to -> Tanzania\n# Answer:\nTanzania", "# Reasoning Path:\nGreenwich Mean Time Zone -> time.time_zone.day_dst_begins -> Last Sunday in March -> common.topic.notable_for -> g.125dtc2vl\n# Answer:\nLast Sunday in March", "# Reasoning Path:\nChina -> location.country.languages_spoken -> Chinese language -> language.human_language.countries_spoken_in -> Vietnam\n# Answer:\nChinese language", "# Reasoning Path:\nChina -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> New Zealand\n# Answer:\nEnglish Language", "# Reasoning Path:\nChina -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71ybv7y\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nGreenwich Mean Time Zone -> time.time_zone.day_dst_begins -> Last Sunday in March -> common.topic.notable_types -> Day Of Year\n# Answer:\nLast Sunday in March"], "ground_truth": ["Sierra Leone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTrn-2722_8babdaa9ecd05a72e3227b43b1f98771", "prediction": ["# Reasoning Path:\nDewitt High School -> organization.organization.headquarters -> m.0dhqb2h -> location.mailing_address.postal_code -> 48820\n# Answer:\n48820", "# Reasoning Path:\nDewitt High School -> common.topic.notable_for -> g.1258j0t9s\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nDewitt High School -> organization.organization.headquarters -> m.0dhqb2h -> location.mailing_address.state_province_region -> Michigan\n# Answer:\nMichigan", "# Reasoning Path:\nDewitt High School -> organization.organization.headquarters -> m.0dhqb2h -> common.topic.notable_for -> g.11bbnkgrvk\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nDewitt High School -> education.educational_institution.students_graduates -> m.0w5lr60 -> education.education.student -> Jordyn Wieber\n# Answer:\nJordyn Wieber", "# Reasoning Path:\nGold medal -> olympics.olympic_medal_demonstration.medal_winners -> m.04njn4p -> olympics.olympic_demonstration_medal_honor.event -> Curling at the 1992 Winter Olympics - Women\n# Answer:\nCurling at the 1992 Winter Olympics - Women", "# Reasoning Path:\nGold medal -> common.topic.webpage -> m.09w7y6w -> common.webpage.resource -> Michael Phelps to appear on 'Entourage'\n# Answer:\nMichael Phelps to appear on 'Entourage'", "# Reasoning Path:\nGold medal -> common.topic.webpage -> m.09w7zmp -> common.webpage.resource -> Michael Phelps to appear on 'Entourage'\n# Answer:\nMichael Phelps to appear on 'Entourage'", "# Reasoning Path:\nGold medal -> olympics.olympic_medal_demonstration.medal_winners -> m.04njn4p -> olympics.olympic_demonstration_medal_honor.medalist -> Andrea Sch\u00f6pp\n# Answer:\nAndrea Sch\u00f6pp", "# Reasoning Path:\nGold medal -> common.topic.webpage -> m.09w7y6w -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Gymnastics at the 2012 Summer Olympics \u2013 Women's artistic team all-around"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-2834_6a27420dcf0528ae017dd74e40cfd38a", "prediction": ["# Reasoning Path:\nTamil Nadu Thowheed Jamath -> religion.religious_organization.associated_with -> Islam -> religion.religion.deities -> Ramdev Pir\n# Answer:\nIslam", "# Reasoning Path:\nTamil Nadu Thowheed Jamath -> religion.religious_organization.associated_with -> Islam -> religion.religion.deities -> God\n# Answer:\nIslam", "# Reasoning Path:\nTamil Nadu Thowheed Jamath -> religion.religious_organization.associated_with -> Islam -> religion.religion.deities -> Allah\n# Answer:\nIslam", "# Reasoning Path:\nTamil Nadu Thowheed Jamath -> religion.religious_organization.associated_with -> Islam -> religion.religion.practices -> Qurbani\n# Answer:\nIslam", "# Reasoning Path:\nTamil Nadu Thowheed Jamath -> organization.organization.headquarters -> m.0113sqbr -> location.mailing_address.state_province_region -> Tamil Nadu\n# Answer:\nTamil Nadu", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Abenaki language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nAbenaki language", "# Reasoning Path:\nUnited States of America -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60xs9dn\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nTamil Nadu Thowheed Jamath -> religion.religious_organization.associated_with -> Islam -> base.argumentmaps.thing_of_disputed_value.disparagement -> 23 Years\n# Answer:\nIslam", "# Reasoning Path:\nTamil Nadu Thowheed Jamath -> freebase.valuenotation.is_reviewed -> Associated With -> rdf-schema#domain -> Religious Organization\n# Answer:\nAssociated With", "# Reasoning Path:\nTamil Nadu Thowheed Jamath -> religion.religious_organization.associated_with -> Islam -> religion.religion.practices -> Adab\n# Answer:\nIslam"], "ground_truth": ["Islam"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.6, "path_recall": 0.5, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTrn-2871_d7d303efc1f901f14e6aae2bb469743c", "prediction": ["# Reasoning Path:\nMountain Time Zone -> common.topic.image -> Timezoneswest -> common.image.appears_in_topic_gallery -> Samoa Time Zone\n# Answer:\nTimezoneswest", "# Reasoning Path:\nMountain Time Zone -> common.topic.image -> Timezoneswest -> common.image.appears_in_topic_gallery -> Alaska Time Zone\n# Answer:\nTimezoneswest", "# Reasoning Path:\nMountain Time Zone -> common.topic.image -> Timezoneswest -> common.image.size -> m.02czb60\n# Answer:\nTimezoneswest", "# Reasoning Path:\nGrand Canyon -> film.film.featured_film_locations -> Grand Canyon National Park -> location.location.contains -> Desert View Watchtower\n# Answer:\nGrand Canyon National Park", "# Reasoning Path:\nMountain Time Zone -> common.topic.image -> Timezoneswest -> common.image.appears_in_topic_gallery -> Atlantic Time Zone\n# Answer:\nTimezoneswest", "# Reasoning Path:\nGrand Canyon -> film.film.featured_film_locations -> Grand Canyon National Park -> common.topic.notable_for -> g.1257vpkxk\n# Answer:\nGrand Canyon National Park", "# Reasoning Path:\nGrand Canyon -> travel.tourist_attraction.near_travel_destination -> Phoenix -> travel.travel_destination.tourist_attractions -> Castle Hot Springs\n# Answer:\nPhoenix", "# Reasoning Path:\nGrand Canyon -> location.location.nearby_airports -> Grand Canyon National Park Airport -> aviation.airport.serves -> Tusayan\n# Answer:\nGrand Canyon National Park Airport", "# Reasoning Path:\nMountain Time Zone -> common.topic.notable_types -> Time Zone -> type.type.expected_by -> Time zone\n# Answer:\nTime Zone", "# Reasoning Path:\nGrand Canyon -> film.film.featured_film_locations -> Grand Canyon National Park -> location.location.contains -> Grand Canyon Lodge\n# Answer:\nGrand Canyon National Park"], "ground_truth": ["Phoenix", "Lake Powell"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.16666666666666669, "ans_precission": 0.1, "ans_recall": 0.5, "path_f1": 0.13333333333333333, "path_precision": 0.1, "path_recall": 0.2, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 0.1, "path_ans_recall": 0.5}
{"id": "WebQTrn-3100_143c89d70679c3e5257c93d8e2bc4c67", "prediction": ["# Reasoning Path:\nUTC\u221205:00 -> common.topic.notable_for -> g.125fnb_r1\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nDominican Republic -> location.location.containedby -> Greater Antilles -> location.location.containedby -> Antilles\n# Answer:\nGreater Antilles", "# Reasoning Path:\nDominican Republic -> location.location.containedby -> Latin America -> location.location.containedby -> Americas\n# Answer:\nLatin America", "# Reasoning Path:\nDominican Republic -> location.location.containedby -> Americas -> location.location.contains -> Bahamas\n# Answer:\nAmericas", "# Reasoning Path:\nDominican Republic -> location.location.containedby -> Americas -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nAmericas", "# Reasoning Path:\nUTC\u221205:00 -> common.topic.article -> m.02_384\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nDominican Republic -> location.location.containedby -> Americas -> location.location.time_zones -> Central Time Zone\n# Answer:\nAmericas", "# Reasoning Path:\nDominican Republic -> location.statistical_region.gni_in_ppp_dollars -> g.11b60xj9z5\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nDominican Republic -> location.location.containedby -> Americas -> location.location.contains -> Belize\n# Answer:\nAmericas", "# Reasoning Path:\nUTC\u221205:00 -> common.topic.notable_types -> Time Zone -> type.type.properties -> DST offset from UTC\n# Answer:\nTime Zone"], "ground_truth": ["Greater Antilles"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTrn-3136_a2debf685e0c50491e35a9cf7a1e9ade", "prediction": ["# Reasoning Path:\nDown District Council -> location.location.containedby -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nDown District Council -> location.location.geolocation -> m.0clscg5\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nDown District Council -> location.location.containedby -> Northern Ireland -> location.location.containedby -> United Kingdom\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nDown District Council -> location.location.containedby -> Northern Ireland -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.location.containedby -> Eurasia -> base.locations.continents.countries_within -> France\n# Answer:\nEurasia", "# Reasoning Path:\nDown District Council -> location.uk_district.administrative_headquarters -> Downpatrick -> location.location.containedby -> United Kingdom\n# Answer:\nDownpatrick", "# Reasoning Path:\nDown District Council -> location.location.containedby -> Northern Ireland -> organization.organization_scope.organizations_with_this_scope -> Alliance Party of Northern Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.location.containedby -> United Kingdom, with Dependencies and Territories -> base.aareas.schema.administrative_area.administrative_children -> Akrotiri and Dhekelia\n# Answer:\nUnited Kingdom, with Dependencies and Territories"], "ground_truth": ["Northern Ireland"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTrn-3251_d8cddfe5e947e414b7735780ef1efff8", "prediction": ["# Reasoning Path:\nCollege/University -> type.type.expected_by -> University -> organization.organization_type.organizations_of_this_type -> University of California, Berkeley\n# Answer:\nUniversity", "# Reasoning Path:\nNorthern Colorado Bears football -> sports.school_sports_team.school -> University of Northern Colorado -> location.location.containedby -> Colorado\n# Answer:\nUniversity of Northern Colorado", "# Reasoning Path:\nNorthern Colorado Bears football -> sports.school_sports_team.school -> University of Northern Colorado -> location.location.containedby -> Greeley\n# Answer:\nUniversity of Northern Colorado", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado -> location.us_state.capital -> Denver\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.contains -> University of Northern Colorado -> location.location.containedby -> Colorado\n# Answer:\nUniversity of Northern Colorado", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> University -> organization.organization_type.organizations_of_this_type -> Dallas Baptist University\n# Answer:\nUniversity", "# Reasoning Path:\nNorthern Colorado Bears football -> sports.school_sports_team.school -> University of Northern Colorado -> education.university.acceptance_rate -> m.0h74628\n# Answer:\nUniversity of Northern Colorado", "# Reasoning Path:\nGreeley -> location.location.contains -> University of Northern Colorado -> education.university.acceptance_rate -> m.0h74628\n# Answer:\nUniversity of Northern Colorado", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> University -> organization.organization_type.organizations_of_this_type -> Edge Hill University\n# Answer:\nUniversity", "# Reasoning Path:\nNorthern Colorado Bears football -> sports.school_sports_team.school -> University of Northern Colorado -> location.location.containedby -> United States of America\n# Answer:\nUniversity of Northern Colorado"], "ground_truth": ["University of Northern Colorado"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.631578947368421, "path_precision": 0.6, "path_recall": 0.6666666666666666, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTrn-3376_0619d288bbed0ca782e60c6f841a6051", "prediction": ["# Reasoning Path:\nCastlemont High School -> organization.organization.headquarters -> m.0115t8hb -> location.mailing_address.citytown -> Oakland\n# Answer:\nOakland", "# Reasoning Path:\nCastlemont High School -> organization.organization.headquarters -> m.0115t8hb -> location.mailing_address.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nCastlemont High School -> common.topic.notable_for -> g.125g2wptw\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCastlemont High School -> organization.organization.headquarters -> m.0115t8hb -> common.topic.notable_for -> g.11b5pgvlq0\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCastlemont High School -> education.school.school_district -> Oakland Unified School District -> education.school_district.locations_served -> Oakland\n# Answer:\nOakland Unified School District", "# Reasoning Path:\nCastlemont High School -> education.school.school_district -> Oakland Unified School District -> common.topic.notable_for -> g.125dd3b9h\n# Answer:\nOakland Unified School District", "# Reasoning Path:\nCastlemont High School -> education.school.school_district -> Oakland Unified School District -> common.topic.notable_types -> School district\n# Answer:\nOakland Unified School District"], "ground_truth": ["Temple University", "Castlemont High School"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTrn-372_a806813629e86776ae3bfe26a3e000f8", "prediction": ["# Reasoning Path:\nBarcelona Cathedral -> religion.place_of_worship.religion -> Catholicism -> religion.religion.deities -> Jesus Christ\n# Answer:\nCatholicism", "# Reasoning Path:\nBarcelona Cathedral -> religion.place_of_worship.religion -> Catholicism -> religion.religion.deities -> God\n# Answer:\nCatholicism", "# Reasoning Path:\nBarcelona Cathedral -> religion.place_of_worship.religion -> Catholicism -> religion.religion.deities -> Holy Spirit\n# Answer:\nCatholicism", "# Reasoning Path:\nBarcelona Cathedral -> religion.place_of_worship.religion -> Catholicism -> common.topic.image -> Fulda kir\n# Answer:\nCatholicism", "# Reasoning Path:\nBarcelona Cathedral -> common.topic.notable_for -> g.1255d3h4k\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nBarcelona Cathedral -> religion.place_of_worship.religion -> Catholicism -> media_common.quotation_subject.quotations_about_this_subject -> A little skill in antiquity inclines a man to Popery.\n# Answer:\nCatholicism", "# Reasoning Path:\nBarcelona Cathedral -> religion.place_of_worship.religion -> Catholicism -> common.topic.image -> Rome basilica st peter 011c\n# Answer:\nCatholicism", "# Reasoning Path:\nBarcelona Cathedral -> people.place_of_interment.interred_here -> Alfonso III of Aragon -> common.topic.notable_types -> Noble person\n# Answer:\nAlfonso III of Aragon", "# Reasoning Path:\nBarcelona Cathedral -> religion.place_of_worship.religion -> Catholicism -> media_common.quotation_subject.quotations_about_this_subject -> All human life is here, but the Holy Ghost seems to be somewhere else.\n# Answer:\nCatholicism", "# Reasoning Path:\nBarcelona Cathedral -> people.place_of_interment.interred_here -> Alfonso III of Aragon -> people.family_member.family -> House of Barcelona\n# Answer:\nAlfonso III of Aragon"], "ground_truth": ["Jesus Christ", "God", "The Father", "Holy Spirit"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4285714285714285, "path_ans_precision": 0.3, "path_ans_recall": 0.75}
{"id": "WebQTrn-3744_26401b4c33bcb760afd734acaa0a1869", "prediction": ["# Reasoning Path:\nRussell Wilson -> people.person.education -> m.0hpny0z -> education.education.institution -> North Carolina State University\n# Answer:\nNorth Carolina State University", "# Reasoning Path:\nThe Daily Cardinal -> common.topic.notable_for -> g.125h51082\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nThe Daily Cardinal -> education.school_newspaper.school -> University of Wisconsin-Madison -> education.university.number_of_postgraduates -> m.04zkps7\n# Answer:\nUniversity of Wisconsin-Madison", "# Reasoning Path:\nThe Daily Cardinal -> education.school_newspaper.school -> University of Wisconsin-Madison -> location.location.geolocation -> m.0wmf76s\n# Answer:\nUniversity of Wisconsin-Madison", "# Reasoning Path:\nRussell Wilson -> people.person.education -> m.0hpny13 -> education.education.institution -> University of Wisconsin-Madison\n# Answer:\nUniversity of Wisconsin-Madison", "# Reasoning Path:\nRussell Wilson -> people.person.education -> m.0jvsppk -> education.education.institution -> Collegiate School\n# Answer:\nCollegiate School", "# Reasoning Path:\nThe Daily Cardinal -> education.school_newspaper.school -> University of Wisconsin-Madison -> education.university.number_of_postgraduates -> m.0h6k_hl\n# Answer:\nUniversity of Wisconsin-Madison", "# Reasoning Path:\nRussell Wilson -> people.person.education -> m.0hpny0z -> education.education.major_field_of_study -> Communication\n# Answer:\nCommunication", "# Reasoning Path:\nThe Daily Cardinal -> education.school_newspaper.school -> University of Wisconsin-Madison -> education.university.number_of_postgraduates -> m.0k7hv_f\n# Answer:\nUniversity of Wisconsin-Madison", "# Reasoning Path:\nThe Daily Cardinal -> education.school_newspaper.school -> University of Wisconsin-Madison -> education.university.acceptance_rate -> m.0h7449v\n# Answer:\nUniversity of Wisconsin-Madison"], "ground_truth": ["University of Wisconsin-Madison"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.48, "path_precision": 0.6, "path_recall": 0.4, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTrn-3744_b9bd90569bb7912ec3ea180bf164663c", "prediction": ["# Reasoning Path:\nCollege/University -> type.type.expected_by -> School -> type.type.domain -> Education\n# Answer:\nSchool", "# Reasoning Path:\nWisconsin Badgers men's basketball -> sports.school_sports_team.school -> University of Wisconsin-Madison -> education.university.fraternities_and_sororities -> Alpha Delta Phi\n# Answer:\nUniversity of Wisconsin-Madison", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> School -> type.property.expected_type -> Educational Institution\n# Answer:\nSchool", "# Reasoning Path:\nWisconsin Badgers men's basketball -> sports.school_sports_team.school -> University of Wisconsin-Madison -> location.location.geolocation -> m.0wmf76s\n# Answer:\nUniversity of Wisconsin-Madison", "# Reasoning Path:\nWisconsin Badgers men's basketball -> sports.school_sports_team.school -> University of Wisconsin-Madison -> education.university.fraternities_and_sororities -> Alpha Sigma Phi\n# Answer:\nUniversity of Wisconsin-Madison", "# Reasoning Path:\nWisconsin Badgers men's basketball -> sports.school_sports_team.school -> University of Wisconsin-Madison -> education.university.number_of_postgraduates -> m.04zkps7\n# Answer:\nUniversity of Wisconsin-Madison", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> Universities -> rdf-schema#domain -> Cityscape\n# Answer:\nUniversities", "# Reasoning Path:\nWisconsin Badgers men's basketball -> sports.school_sports_team.school -> University of Wisconsin-Madison -> education.university.fraternities_and_sororities -> Sigma Phi\n# Answer:\nUniversity of Wisconsin-Madison", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> School -> type.property.schema -> College gymnast\n# Answer:\nSchool", "# Reasoning Path:\nCollege/University -> type.type.expected_by -> Universities -> type.property.schema -> Cityscape\n# Answer:\nUniversities"], "ground_truth": ["University of Wisconsin-Madison"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.28571428571428575, "path_precision": 0.5, "path_recall": 0.2, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTrn-557_960c16ffdb29e173df0577fc76c7455d", "prediction": ["# Reasoning Path:\nWinged Monkey #7 -> common.topic.notable_types -> Film character -> type.type.expected_by -> Character\n# Answer:\nFilm character", "# Reasoning Path:\nDorothy Gale -> film.film_character.portrayed_in_films -> m.02vd7y2 -> film.performance.actor -> Romola Remus\n# Answer:\nRomola Remus", "# Reasoning Path:\nWinged Monkey #7 -> common.topic.notable_types -> Film character -> type.type.expected_by -> Character using Wham-O product\n# Answer:\nFilm character", "# Reasoning Path:\nWinged Monkey #7 -> film.film_character.portrayed_in_films -> m.0yj6tr7 -> film.performance.actor -> Jack Paul\n# Answer:\nJack Paul", "# Reasoning Path:\nDorothy Gale -> film.film_character.portrayed_in_films -> m.03jsncr -> film.performance.actor -> Aileen Quinn\n# Answer:\nAileen Quinn", "# Reasoning Path:\nWinged Monkey #7 -> common.topic.notable_for -> g.1yfp2fklf\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nWinged Monkey #7 -> common.topic.notable_types -> Film character -> type.type.expected_by -> Killed by\n# Answer:\nFilm character", "# Reasoning Path:\nDorothy Gale -> film.film_character.portrayed_in_films -> m.09z66lh -> film.performance.film -> The Wonderful Wizard of Oz\n# Answer:\nThe Wonderful Wizard of Oz", "# Reasoning Path:\nWinged Monkey #7 -> common.topic.notable_types -> Film character -> freebase.type_profile.strict_included_types -> Fictional Character\n# Answer:\nFilm character", "# Reasoning Path:\nWinged Monkey #7 -> common.topic.notable_types -> Film character -> type.type.properties -> Portrayed in films\n# Answer:\nFilm character"], "ground_truth": ["Judy Garland"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-567_693feb48c0515cd069014e7ca2846b37", "prediction": ["# Reasoning Path:\nTeklel Hafouli -> common.topic.notable_for -> g.1q56ftp58\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nRon Howard -> film.producer.film -> A Beautiful Mind -> media_common.netflix_title.netflix_genres -> 20th Century Period Pieces\n# Answer:\nA Beautiful Mind", "# Reasoning Path:\nRon Howard -> film.producer.film -> Dr. Seuss' How the Grinch Stole Christmas -> film.film.produced_by -> Brian Grazer\n# Answer:\nDr. Seuss' How the Grinch Stole Christmas", "# Reasoning Path:\nRon Howard -> film.producer.film -> A Beautiful Mind -> media_common.netflix_title.netflix_genres -> Biographical Dramas\n# Answer:\nA Beautiful Mind", "# Reasoning Path:\nRon Howard -> film.producer.film -> A Beautiful Mind -> award.award_nominated_work.award_nominations -> m.05bkyhb\n# Answer:\nA Beautiful Mind", "# Reasoning Path:\nRon Howard -> film.director.film -> A Beautiful Mind -> media_common.netflix_title.netflix_genres -> 20th Century Period Pieces\n# Answer:\nA Beautiful Mind", "# Reasoning Path:\nTeklel Hafouli -> common.topic.notable_types -> Film character -> type.type.properties -> Portrayed in films\n# Answer:\nFilm character", "# Reasoning Path:\nTeklel Hafouli -> common.topic.notable_types -> Film character -> type.type.domain -> Film\n# Answer:\nFilm character", "# Reasoning Path:\nRon Howard -> film.producer.film -> A Beautiful Mind -> media_common.netflix_title.netflix_genres -> Drama\n# Answer:\nA Beautiful Mind", "# Reasoning Path:\nRon Howard -> film.producer.film -> A Beautiful Mind -> freebase.valuenotation.is_reviewed -> Directed by\n# Answer:\nA Beautiful Mind"], "ground_truth": ["The Journey"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-568_d54918e8e89ad97237bce821087a9818", "prediction": ["# Reasoning Path:\nNBC Nightside -> tv.tv_program.filming_locations -> Charlotte -> location.location.containedby -> Mecklenburg County\n# Answer:\nCharlotte", "# Reasoning Path:\nNBC Nightside -> tv.tv_program.filming_locations -> Charlotte -> location.location.nearby_airports -> Charlotte Douglas International Airport\n# Answer:\nCharlotte", "# Reasoning Path:\nNBC Nightside -> tv.tv_program.country_of_origin -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nNBC Nightside -> tv.tv_program.filming_locations -> Charlotte -> location.location.containedby -> Area code 704\n# Answer:\nCharlotte", "# Reasoning Path:\nNBC Nightside -> tv.tv_program.filming_locations -> Charlotte -> location.location.containedby -> Area code 980\n# Answer:\nCharlotte", "# Reasoning Path:\nNBC Nightside -> tv.tv_program.filming_locations -> Charlotte -> location.statistical_region.population -> g.11b674q2v0\n# Answer:\nCharlotte", "# Reasoning Path:\nNBC Nightside -> tv.tv_program.filming_locations -> Charlotte -> location.statistical_region.population -> g.11b7tmvwpg\n# Answer:\nCharlotte", "# Reasoning Path:\nNBC Nightside -> tv.tv_program.languages -> English Language -> book.book_subject.works -> A Christmas Carol\n# Answer:\nEnglish Language", "# Reasoning Path:\nNBC Nightside -> tv.tv_program.filming_locations -> Charlotte -> location.statistical_region.population -> g.11btsv6nnh\n# Answer:\nCharlotte", "# Reasoning Path:\nNBC Nightside -> tv.tv_program.country_of_origin -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Missouri\n# Answer:\nUnited States of America"], "ground_truth": ["Mecklenburg County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTrn-60_0b79f757e511e73c7aca0bb6f6057f2d", "prediction": ["# Reasoning Path:\nPortuguese Language -> common.topic.article -> m.05zjq\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.administrative_divisions -> Bengo Province\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.administrative_divisions -> Benguela Province\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.form_of_government -> Unitary state\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Mozambique -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nMozambique", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Timor-Leste -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nTimor-Leste", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.administrative_divisions -> Bi\u00e9 Province\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> base.aareas.schema.administrative_area.administrative_children -> Bengo Province\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> base.rosetta.languoid.local_name -> Portuguese -> base.rosetta.local_name.locale -> Andorra\n# Answer:\nAndorra", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.form_of_government -> Presidential system\n# Answer:\nAngola"], "ground_truth": ["Macau"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-60_22027d5bb310c0d956b5f22248a6811c", "prediction": ["# Reasoning Path:\nPortuguese Language -> common.topic.article -> m.05zjq\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Mozambique -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nMozambique", "# Reasoning Path:\nPortuguese Language -> base.rosetta.languoid.local_name -> Portuguese -> base.rosetta.local_name.locale -> Andorra\n# Answer:\nAndorra", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Andorra -> location.country.administrative_divisions -> Andorra la Vella\n# Answer:\nAndorra", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Mozambique -> location.country.administrative_divisions -> Inhambane\n# Answer:\nMozambique", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Andorra -> location.country.form_of_government -> Unitary state\n# Answer:\nAndorra", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Andorra -> base.locations.countries.continent -> Europe\n# Answer:\nAndorra", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.administrative_divisions -> Bengo Province\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> base.rosetta.languoid.local_name -> Portuguese -> base.rosetta.local_name.locale -> Angola\n# Answer:\nPortuguese", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Andorra -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nAndorra"], "ground_truth": ["Macau"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-60_39ba4faa87698cb0767d1a5ee7ce1827", "prediction": ["# Reasoning Path:\nPortuguese Language -> common.topic.article -> m.05zjq\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.administrative_divisions -> Bengo Province\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.administrative_divisions -> Benguela Province\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.form_of_government -> Unitary state\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.administrative_divisions -> Bi\u00e9 Province\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> base.aareas.schema.administrative_area.administrative_children -> Bengo Province\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Mozambique -> location.country.form_of_government -> Unitary state\n# Answer:\nMozambique", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Mozambique -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nMozambique", "# Reasoning Path:\nPortuguese Language -> base.rosetta.languoid.local_name -> Portuguese -> base.rosetta.local_name.locale -> Mozambique\n# Answer:\nPortuguese", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.form_of_government -> Presidential system\n# Answer:\nAngola"], "ground_truth": ["South Africa"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-60_68f0d0ad309d64a4af858a5ef4fb5713", "prediction": ["# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.location.containedby -> Africa\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> common.topic.article -> m.05zjq\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nPortuguese Language -> base.rosetta.languoid.local_name -> Portuguese -> base.rosetta.local_name.locale -> Mozambique\n# Answer:\nPortuguese", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Mozambique -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nMozambique", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.administrative_divisions -> Bengo Province\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> base.rosetta.languoid.local_name -> Portuguese -> base.rosetta.local_name.locale -> Angola\n# Answer:\nPortuguese", "# Reasoning Path:\nPortuguese Language -> base.rosetta.languoid.local_name -> Portuguese -> base.rosetta.local_name.locale -> Guyana\n# Answer:\nPortuguese", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.administrative_divisions -> Benguela Province\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Mozambique -> location.country.administrative_divisions -> Cabo Delgado Province\n# Answer:\nMozambique", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Andorra -> location.country.form_of_government -> Unitary state\n# Answer:\nAndorra"], "ground_truth": ["Mozambique"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTrn-60_990f6babd500d25e3746174e6da58c84", "prediction": ["# Reasoning Path:\nBrazil -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> Portugal\n# Answer:\n1952 Summer Olympics", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> location.country.form_of_government -> Presidential system\n# Answer:\nAngola", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Brazil -> location.country.form_of_government -> Constitutional republic\n# Answer:\nConstitutional republic", "# Reasoning Path:\nBrazil -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> Belgium\n# Answer:\n1952 Summer Olympics", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Angola -> common.topic.notable_types -> Country\n# Answer:\nAngola", "# Reasoning Path:\nBrazil -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> Canada\n# Answer:\n1952 Summer Olympics", "# Reasoning Path:\nBrazil -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Mozambique\n# Answer:\nConstitutional republic", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Brazil -> location.country.form_of_government -> Presidential system\n# Answer:\nPresidential system", "# Reasoning Path:\nPortuguese Language -> language.human_language.countries_spoken_in -> Brazil -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics\n# Answer:\n1952 Summer Olympics", "# Reasoning Path:\nBrazil -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Andorra\n# Answer:\n2014 Winter Olympics"], "ground_truth": ["Angola"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.22222222222222224, "path_precision": 0.2, "path_recall": 0.25, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTrn-634_000f72b60ccaedff5f056f522ee06e98", "prediction": ["# Reasoning Path:\nMansfeld -> location.location.people_born_here -> Arthur Gaebelein -> common.topic.notable_types -> Football player\n# Answer:\nArthur Gaebelein", "# Reasoning Path:\nMansfeld -> common.topic.notable_for -> g.125cxczp7\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMansfeld -> location.location.people_born_here -> Arthur Gaebelein -> people.person.nationality -> Germany\n# Answer:\nArthur Gaebelein", "# Reasoning Path:\nMansfeld -> location.location.people_born_here -> Arthur Gaebelein -> sports.pro_athlete.teams -> m.0ztzb59\n# Answer:\nArthur Gaebelein", "# Reasoning Path:\nMansfeld -> location.location.people_born_here -> Franz Wilhelm Junghuhn -> people.person.profession -> Physician\n# Answer:\nFranz Wilhelm Junghuhn", "# Reasoning Path:\nMansfeld -> location.location.containedby -> Mansfelder Land -> common.topic.notable_types -> Administrative Division\n# Answer:\nMansfelder Land", "# Reasoning Path:\nMansfeld -> location.location.people_born_here -> Elisabeth of Mansfeld-Hinterort -> people.person.parents -> Margaret of Brunswick-L\u00fcneburg\n# Answer:\nElisabeth of Mansfeld-Hinterort", "# Reasoning Path:\nMansfeld -> location.location.people_born_here -> Franz Wilhelm Junghuhn -> people.person.profession -> Botanist\n# Answer:\nFranz Wilhelm Junghuhn", "# Reasoning Path:\nMansfeld -> location.location.people_born_here -> Elisabeth of Mansfeld-Hinterort -> people.deceased_person.place_of_death -> Marksuhl\n# Answer:\nElisabeth of Mansfeld-Hinterort", "# Reasoning Path:\nMansfeld -> location.location.people_born_here -> Franz Wilhelm Junghuhn -> common.topic.notable_types -> Deceased Person\n# Answer:\nFranz Wilhelm Junghuhn"], "ground_truth": ["Monk", "Writer", "Professor", "Physician", "Priest", "Theologian"], "ans_acc": 0.16666666666666666, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.125, "path_ans_precision": 0.1, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTrn-662_7a992044f94b39edfc37ac5dcfcb3c26", "prediction": ["# Reasoning Path:\n2008 FIFA Club World Cup Final -> sports.sports_championship_event.champion -> Manchester United F.C. -> sports.sports_team.colors -> Red\n# Answer:\nManchester United F.C.", "# Reasoning Path:\n2008 FIFA Club World Cup Final -> common.topic.notable_for -> g.1257vp1kh\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\n2008 FIFA Club World Cup Final -> sports.sports_championship_event.champion -> Manchester United F.C. -> sports.sports_team.colors -> White\n# Answer:\nManchester United F.C.", "# Reasoning Path:\n2008 FIFA Club World Cup Final -> sports.sports_championship_event.champion -> Manchester United F.C. -> base.associationfootball.soccer_team.squads -> Manchester United F.C. First team\n# Answer:\nManchester United F.C.", "# Reasoning Path:\n2008 FIFA Club World Cup Final -> sports.sports_championship_event.champion -> Manchester United F.C. -> soccer.football_team.manager -> m.010bt14d\n# Answer:\nManchester United F.C.", "# Reasoning Path:\n2008 FIFA Club World Cup Final -> sports.sports_championship_event.champion -> Manchester United F.C. -> base.associationfootball.soccer_team.squads -> Manchester United F.C. Reserves and Academy\n# Answer:\nManchester United F.C.", "# Reasoning Path:\n2008 FIFA Club World Cup Final -> sports.sports_championship_event.champion -> Manchester United F.C. -> soccer.football_team.manager -> m.010lm29_\n# Answer:\nManchester United F.C.", "# Reasoning Path:\n2008 FIFA Club World Cup Final -> soccer.football_match.held_at -> International Stadium Yokohama -> common.topic.image -> Yokohamakokusai20041208\n# Answer:\nInternational Stadium Yokohama", "# Reasoning Path:\n2008 FIFA Club World Cup Final -> sports.sports_championship_event.champion -> Manchester United F.C. -> soccer.football_team.manager -> m.07xycd7\n# Answer:\nManchester United F.C.", "# Reasoning Path:\n2008 FIFA Club World Cup Final -> soccer.football_match.held_at -> International Stadium Yokohama -> common.topic.notable_types -> Sports Facility\n# Answer:\nInternational Stadium Yokohama"], "ground_truth": ["Newton Heath L&YR F.C."], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-710_c264a6d11d7956741926d417b94327e2", "prediction": ["# Reasoning Path:\nLarry Baer -> common.topic.article -> m.0hhv_6m\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nLarry Baer -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nLarry Baer -> people.person.gender -> Male -> sports.sports_gender.sports_teams -> Vive Targi Kielce\n# Answer:\nMale", "# Reasoning Path:\nLarry Baer -> business.board_member.leader_of -> m.011zsc4_ -> organization.leadership.organization -> San Francisco Giants\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nLarry Baer -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nLarry Baer -> business.board_member.leader_of -> m.011zsc4_ -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nLarry Baer -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Acute myeloid leukemia\n# Answer:\nMale", "# Reasoning Path:\nLarry Baer -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nMale", "# Reasoning Path:\nLarry Baer -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_98hd\n# Answer:\nMale", "# Reasoning Path:\nLarry Baer -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02zd4tr\n# Answer:\nMale"], "ground_truth": ["2014 World Series"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-750_bb35e9b8023fbf9c05df55b4245a4775", "prediction": ["# Reasoning Path:\n1988 World Series -> time.event.locations -> Dodger Stadium -> location.location.events -> 1981 World Series\n# Answer:\nDodger Stadium", "# Reasoning Path:\n1988 World Series -> sports.sports_championship_event.champion -> Los Angeles Dodgers -> sports.sports_team.championships -> 1981 World Series\n# Answer:\nLos Angeles Dodgers", "# Reasoning Path:\n1988 World Series -> time.event.locations -> Dodger Stadium -> location.location.events -> 1963 World Series\n# Answer:\nDodger Stadium", "# Reasoning Path:\n1988 World Series -> common.topic.article -> m.04j7mr\n# Answer:\ncommon.topic.article", "# Reasoning Path:\n1988 World Series -> sports.sports_championship_event.champion -> Los Angeles Dodgers -> sports.sports_team.championships -> 1963 World Series\n# Answer:\nLos Angeles Dodgers", "# Reasoning Path:\n1988 World Series -> time.event.locations -> Dodger Stadium -> location.location.events -> 1965 World Series\n# Answer:\nDodger Stadium", "# Reasoning Path:\n1988 World Series -> time.event.locations -> Dodger Stadium -> sports.sports_facility.home_venue_for -> m.0wz1z2y\n# Answer:\nDodger Stadium", "# Reasoning Path:\n1988 World Series -> time.event.locations -> Dodger Stadium -> sports.sports_facility.teams -> Los Angeles Angels of Anaheim\n# Answer:\nDodger Stadium", "# Reasoning Path:\n1988 World Series -> time.event.locations -> Oakland-Alameda County Coliseum -> location.location.events -> 1972 World Series\n# Answer:\nOakland-Alameda County Coliseum", "# Reasoning Path:\n1988 World Series -> time.event.locations -> Oakland-Alameda County Coliseum -> common.topic.notable_types -> Sports Facility\n# Answer:\nOakland-Alameda County Coliseum"], "ground_truth": ["1981 World Series", "1965 World Series", "1988 World Series", "1963 World Series", "1959 World Series"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.8571428571428571, "path_precision": 1.0, "path_recall": 0.75, "path_ans_f1": 0.888888888888889, "path_ans_precision": 1.0, "path_ans_recall": 0.8}
{"id": "WebQTrn-810_a188aff4a054e1ec66fafba1b8021f67", "prediction": ["# Reasoning Path:\nBill Neukom -> sports.sports_team_owner.teams_owned -> San Francisco Giants -> sports.sports_team.championships -> 2014 World Series\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nBill Neukom -> sports.sports_team_owner.teams_owned -> San Francisco Giants -> sports.sports_team.championships -> 2010 World Series\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nBill Neukom -> sports.sports_team_owner.teams_owned -> San Francisco Giants -> sports.sports_team.championships -> 2012 World Series\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nBill Neukom -> sports.sports_team_owner.teams_owned -> San Francisco Giants -> sports.sports_team.arena_stadium -> AT&T Park\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nBill Neukom -> sports.sports_team_owner.teams_owned -> San Francisco Giants -> base.schemastaging.sports_team_extra.training_ground -> m.0k079pf\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nBill Neukom -> sports.sports_team_owner.teams_owned -> San Francisco Giants -> sports.sports_team.arena_stadium -> Candlestick Park\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nBill Neukom -> sports.sports_team_owner.teams_owned -> San Francisco Giants -> sports.sports_team.arena_stadium -> Hilltop Park\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nBill Neukom -> sports.sports_team_owner.teams_owned -> San Francisco Giants -> base.schemastaging.sports_team_extra.training_ground -> m.0k079pz\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nBill Neukom -> people.person.profession -> Lawyer -> book.book_subject.works -> Anonymous Lawyer\n# Answer:\nLawyer", "# Reasoning Path:\nBill Neukom -> people.person.education -> m.06wgl0n -> education.education.institution -> Dartmouth College\n# Answer:\nDartmouth College"], "ground_truth": ["2014 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTrn-810_c334509bb5e02cacae1ba2e80c176499", "prediction": ["# Reasoning Path:\nLou Seal -> sports.mascot.team -> San Francisco Giants -> sports.sports_team.championships -> 2014 World Series\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nLou Seal -> sports.mascot.team -> San Francisco Giants -> sports.sports_team.championships -> 2010 World Series\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nLou Seal -> sports.mascot.team -> San Francisco Giants -> sports.sports_team.championships -> 2012 World Series\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nLou Seal -> common.topic.article -> m.03_dwt\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nLou Seal -> sports.mascot.team -> San Francisco Giants -> freebase.valuenotation.is_reviewed -> Contact webpages\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nLou Seal -> sports.mascot.team -> San Francisco Giants -> baseball.baseball_team.team_stats -> m.05n69_j\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nLou Seal -> sports.mascot.team -> San Francisco Giants -> baseball.baseball_team.team_stats -> m.05n69l5\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nLou Seal -> common.topic.image -> Lou-Seal.jpg -> common.image.size -> m.0kksz7\n# Answer:\nLou-Seal.jpg", "# Reasoning Path:\nLou Seal -> sports.mascot.team -> San Francisco Giants -> baseball.baseball_team.team_stats -> m.05n69my\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nLou Seal -> common.topic.image -> Lou seal giants mascot -> common.image.appears_in_topic_gallery -> 1996 San Francisco Giants season\n# Answer:\nLou seal giants mascot"], "ground_truth": ["2014 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTrn-810_e3d40457273785e46c5b71732713a5f4", "prediction": ["# Reasoning Path:\nCrazy Crab -> sports.mascot.team -> San Francisco Giants -> sports.sports_team.championships -> 2014 World Series\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nCrazy Crab -> sports.mascot.team -> San Francisco Giants -> sports.sports_team.championships -> 2010 World Series\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nCrazy Crab -> sports.mascot.team -> San Francisco Giants -> sports.sports_team.championships -> 2012 World Series\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nCrazy Crab -> common.topic.article -> m.02q_hzl\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nCrazy Crab -> sports.mascot.team -> San Francisco Giants -> freebase.valuenotation.is_reviewed -> Contact webpages\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nCrazy Crab -> sports.mascot.team -> San Francisco Giants -> baseball.baseball_team.team_stats -> m.05n69_j\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nCrazy Crab -> sports.mascot.team -> San Francisco Giants -> baseball.baseball_team.team_stats -> m.05n69l5\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nCrazy Crab -> sports.mascot.team -> San Francisco Giants -> baseball.baseball_team.team_stats -> m.05n69my\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nCrazy Crab -> common.topic.notable_types -> Mascot -> type.type.expected_by -> Team Mascot\n# Answer:\nMascot", "# Reasoning Path:\nCrazy Crab -> common.topic.notable_types -> Mascot -> freebase.documented_object.documentation -> m.02ht4bx\n# Answer:\nMascot"], "ground_truth": ["2014 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTrn-846_a29552911617e890ca2e1d6564e0990e", "prediction": ["# Reasoning Path:\nThe Avril Lavigne Tour -> time.event.people_involved -> Avril Lavigne -> people.person.place_of_birth -> Belleville\n# Answer:\nAvril Lavigne", "# Reasoning Path:\nThe Avril Lavigne Tour -> music.concert_tour.artist -> Avril Lavigne -> people.person.place_of_birth -> Belleville\n# Answer:\nAvril Lavigne", "# Reasoning Path:\nThe Avril Lavigne Tour -> time.event.people_involved -> Avril Lavigne -> award.award_nominee.award_nominations -> m.011v278r\n# Answer:\nAvril Lavigne", "# Reasoning Path:\nThe Avril Lavigne Tour -> time.event.people_involved -> Avril Lavigne -> common.topic.webpage -> Discogs page\n# Answer:\nAvril Lavigne", "# Reasoning Path:\nThe Avril Lavigne Tour -> time.event.people_involved -> Avril Lavigne -> award.award_nominee.award_nominations -> m.05bnvd0\n# Answer:\nAvril Lavigne", "# Reasoning Path:\nThe Avril Lavigne Tour -> music.concert_tour.artist -> Avril Lavigne -> award.award_nominee.award_nominations -> m.011v278r\n# Answer:\nAvril Lavigne", "# Reasoning Path:\nThe Avril Lavigne Tour -> time.event.people_involved -> Avril Lavigne -> award.award_nominee.award_nominations -> m.05bpkgn\n# Answer:\nAvril Lavigne", "# Reasoning Path:\nThe Avril Lavigne Tour -> time.event.people_involved -> Avril Lavigne -> common.topic.webpage -> MusicMoz page for Avril Lavigne\n# Answer:\nAvril Lavigne", "# Reasoning Path:\nThe Avril Lavigne Tour -> music.concert_tour.artist -> Avril Lavigne -> common.topic.webpage -> Discogs page\n# Answer:\nAvril Lavigne", "# Reasoning Path:\nThe Avril Lavigne Tour -> music.concert_tour.artist -> Avril Lavigne -> award.award_nominee.award_nominations -> m.05bnvd0\n# Answer:\nAvril Lavigne"], "ground_truth": ["Belleville"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTrn-849_1eb0f1ddd5074471fbe3a7f6b575f202", "prediction": ["# Reasoning Path:\nGermany -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics -> olympics.olympic_games.participating_countries -> Denmark\n# Answer:\n1896 Summer Olympics", "# Reasoning Path:\nGermany -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Denmark\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nGermany -> location.location.events -> Winter Time in Europe, 2010-2011 -> time.event.locations -> Denmark\n# Answer:\nWinter Time in Europe, 2010-2011", "# Reasoning Path:\nGermany -> location.location.events -> Winter Time in Europe, 2011-2012 -> time.event.locations -> Denmark\n# Answer:\nWinter Time in Europe, 2011-2012", "# Reasoning Path:\nGermany -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics -> olympics.olympic_games.participating_countries -> Austria\n# Answer:\n1896 Summer Olympics", "# Reasoning Path:\nCopenhagen -> location.location.contains -> Amager East -> common.topic.notable_types -> Location\n# Answer:\nAmager East", "# Reasoning Path:\nGermany -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics -> olympics.olympic_games.participating_countries -> France\n# Answer:\n1896 Summer Olympics", "# Reasoning Path:\nGermany -> olympics.olympic_participating_country.olympics_participated_in -> 1900 Summer Olympics -> olympics.olympic_games.participating_countries -> Denmark\n# Answer:\n1900 Summer Olympics", "# Reasoning Path:\nGermany -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Austria\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nGermany -> location.location.events -> Winter Time in Europe, 2010-2011 -> time.event.locations -> Austria\n# Answer:\nWinter Time in Europe, 2010-2011"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2777777777777778, "path_precision": 0.5, "path_recall": 0.19230769230769232, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTrn-849_586aae7703d62aa44eb79759e1563309", "prediction": ["# Reasoning Path:\nGermany -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Denmark\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nLejre Municipality -> common.topic.notable_types -> Administrative Division -> type.type.domain -> Location\n# Answer:\nAdministrative Division", "# Reasoning Path:\nLejre Municipality -> location.location.containedby -> Denmark -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics\n# Answer:\nDenmark", "# Reasoning Path:\nGermany -> location.location.events -> Winter Time in Europe, 2010-2011 -> time.event.locations -> Denmark\n# Answer:\nWinter Time in Europe, 2010-2011", "# Reasoning Path:\nGermany -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Austria\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nGermany -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics -> olympics.olympic_games.participating_countries -> Denmark\n# Answer:\n1896 Summer Olympics", "# Reasoning Path:\nCountry -> base.descriptive_names.names.descriptive_name -> m.012l2b96 -> base.schemastaging.plural_form.language -> Turkish\n# Answer:\nTurkish", "# Reasoning Path:\nGermany -> location.location.events -> Winter Time in Europe, 2011-2012 -> time.event.locations -> Denmark\n# Answer:\nWinter Time in Europe, 2011-2012", "# Reasoning Path:\nGermany -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Czech Republic\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nGermany -> olympics.olympic_participating_country.olympics_participated_in -> 1900 Summer Olympics -> olympics.olympic_games.participating_countries -> Denmark\n# Answer:\n1900 Summer Olympics"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.2777777777777778, "path_precision": 0.5, "path_recall": 0.19230769230769232, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTrn-857_9392f3f06e288ee4e3437a74f6bf5a37", "prediction": ["# Reasoning Path:\nRamble Elementary -> education.educational_institution.students_graduates -> m.0b6xs_g -> education.education.student -> Bill Clinton\n# Answer:\nBill Clinton", "# Reasoning Path:\nRamble Elementary -> common.topic.notable_types -> Educational Institution -> type.type.expected_by -> Institution\n# Answer:\nEducational Institution", "# Reasoning Path:\nRamble Elementary -> common.topic.notable_for -> g.125dp7g5t\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nRamble Elementary -> education.educational_institution.students_graduates -> m.0b6xs_g -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nRamble Elementary -> common.topic.notable_types -> Educational Institution -> type.type.expected_by -> Educational institution\n# Answer:\nEducational Institution", "# Reasoning Path:\nRamble Elementary -> education.educational_institution.students_graduates -> m.0b6xs_g -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nRamble Elementary -> common.topic.notable_types -> Educational Institution -> type.type.expected_by -> Accademia Anja\n# Answer:\nEducational Institution", "# Reasoning Path:\nRamble Elementary -> common.topic.notable_types -> Educational Institution -> type.type.properties -> Athletics brand\n# Answer:\nEducational Institution", "# Reasoning Path:\nRamble Elementary -> common.topic.notable_types -> Educational Institution -> freebase.type_profile.strict_included_types -> Organization\n# Answer:\nEducational Institution", "# Reasoning Path:\nRamble Elementary -> common.topic.notable_types -> Educational Institution -> type.type.properties -> Campuses\n# Answer:\nEducational Institution"], "ground_truth": ["Al Gore"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTrn-962_032f61bfcfed69da8b215bb8f058c24e", "prediction": ["# Reasoning Path:\nJenny's Father -> film.film_character.portrayed_in_films -> m.0y54dnx -> film.performance.actor -> Kevin Mangan\n# Answer:\nKevin Mangan", "# Reasoning Path:\nForrest Gump -> film.film_character.portrayed_in_films -> m.02xgww5 -> film.performance.actor -> Michael Connor Humphreys\n# Answer:\nMichael Connor Humphreys", "# Reasoning Path:\nForrest Gump -> film.film_character.portrayed_in_films -> m.0jycvw -> film.performance.actor -> Tom Hanks\n# Answer:\nTom Hanks", "# Reasoning Path:\nJenny's Father -> common.topic.notable_for -> g.12z7tmq3p\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nJenny's Father -> film.film_character.portrayed_in_films -> m.0y54dnx -> film.performance.film -> Forrest Gump\n# Answer:\nfilm.performance.film", "# Reasoning Path:\nJenny's Father -> common.topic.notable_types -> Film character -> common.topic.article -> m.02_96_z\n# Answer:\nFilm character", "# Reasoning Path:\nForrest Gump -> book.book.editions -> Forrest Gump: My Favorite Chocolate Recipes\n# Answer:\nForrest Gump: My Favorite Chocolate Recipes", "# Reasoning Path:\nJenny's Father -> common.topic.notable_types -> Film character -> type.type.expected_by -> Character\n# Answer:\nFilm character", "# Reasoning Path:\nForrest Gump -> book.book.editions -> FORREST GUMP (Movie Tie in)\n# Answer:\nFORREST GUMP (Movie Tie in)", "# Reasoning Path:\nForrest Gump -> book.book.editions -> Forest Gump\n# Answer:\nForest Gump"], "ground_truth": ["Michael Connor Humphreys"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
