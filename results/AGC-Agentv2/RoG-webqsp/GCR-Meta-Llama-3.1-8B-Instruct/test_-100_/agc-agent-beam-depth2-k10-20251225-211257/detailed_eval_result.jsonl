{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Costa Rica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.region -> Americas\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Louisiana\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Mississippi\n# Answer:\n1812 Louisiana hurricane"], "ground_truth": ["Jamaican English", "Jamaican Creole English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nUnited States Representative", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kc -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nUnited States Representative", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.district_represented -> Tennessee's 9th congressional district\n# Answer:\nTennessee's 9th congressional district", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8 -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Representative\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> United States Representative\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kc -> government.government_position_held.governmental_body -> United States House of Representatives\n# Answer:\nUnited States House of Representatives", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> freebase.type_profile.kind -> Title\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> President of the United States\n# Answer:\nUnited States of America"], "ground_truth": ["United States Representative", "Governor of Tennessee", "Speaker of the United States House of Representatives"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.2, "ans_recall": 0.3333333333333333, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.4, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.main_country -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.countries_spoken_in -> Cuba\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_1j97\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French -> language.human_language.main_country -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> Martinique\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.countries_spoken_in -> Dominican Republic\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.main_country -> Puerto Rico\n# Answer:\nHaitian Creole"], "ground_truth": ["Haitian Creole", "French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t8hv2 -> tv.regular_tv_appearance.actor -> Yvonne Craig\n# Answer:\nYvonne Craig", "# Reasoning Path:\nBarbara Gordon -> common.topic.notable_for -> g.12596ymdk\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t91b7 -> tv.regular_tv_appearance.actor -> Melissa Gilbert\n# Answer:\nMelissa Gilbert", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> film.film_character.portrayed_in_films -> m.012hbb3h\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t91b7 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.03ljbs2 -> tv.regular_tv_appearance.actor -> Angie Harmon\n# Answer:\nAngie Harmon", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> book.book_subject.works -> A Cold Christmas\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer -> base.descriptive_names.names.descriptive_name -> m.0101pjk9\n# Answer:\nLawyer", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer -> people.profession.specializations -> Entertainment Lawyer\n# Answer:\nLawyer", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> fictional_universe.fictional_job_title.fictional_characters_with_this_job -> m.0110s7g9\n# Answer:\nDetective"], "ground_truth": ["Ilyssa Fradin", "Melinda McGraw", "Hannah Gunn"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qpksh -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.season -> 1992\u201393 NBA season\n# Answer:\n1992\u201393 NBA season", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player -> common.topic.notable_for -> g.125ctmqvt\n# Answer:\nPlayer", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qpksh -> basketball.basketball_player_stats.season -> 1994\u201395 NBA season\n# Answer:\n1994\u201395 NBA season", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.season -> 2003\u201304 NBA season\n# Answer:\n2003\u201304 NBA season", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc58 -> sports.sports_award.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player -> music.recording.releases -> You cant STOP the REIGN\n# Answer:\nPlayer"], "ground_truth": ["Orlando Magic", "LSU Tigers men's basketball", "Boston Celtics", "Cleveland Cavaliers", "Miami Heat", "Phoenix Suns", "Los Angeles Lakers"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.36363636363636365, "ans_precission": 0.5, "ans_recall": 0.2857142857142857, "path_f1": 0.25, "path_precision": 0.5, "path_recall": 0.16666666666666666, "path_ans_f1": 0.36363636363636365, "path_ans_precision": 0.5, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> travel.travel_destination.tourist_attractions -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> location.location.contains -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9 -> award.award_honor.award -> Emmy Award for Outstanding Variety, Music or Comedy Series\n# Answer:\nEmmy Award for Outstanding Variety, Music or Comedy Series", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.04lybjz\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.09wfyz1\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.image -> Mulberry Street NYC c1900 LOC 3g04637u edit\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Chandler Muriel Bing\n# Answer:\nScottish American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9 -> award.award_honor.honored_for -> The Tonight Show with Jay Leno\n# Answer:\nThe Tonight Show with Jay Leno", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.0n2qyfk -> award.award_honor.award -> People's Choice Award for Favorite Late Night Talk Show Host\n# Answer:\nPeople's Choice Award for Favorite Late Night Talk Show Host", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.09wz40b\n# Answer:\nItalian American"], "ground_truth": ["New Rochelle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.main_country -> Myanmar\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> media_common.netflix_genre.titles -> Shutter\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.region -> Asia\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language -> common.topic.notable_types -> Human Language\n# Answer:\nVietnamese Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> language.human_language.region -> Asia\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yg\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language -> language.human_language.main_country -> Vietnam\n# Answer:\nVietnamese Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> common.topic.notable_types -> Human Language\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Nyaw Language -> language.human_language.region -> Asia\n# Answer:\nNyaw Language", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6gh2z\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp"], "ground_truth": ["Lao Language", "Khmer language", "Cham language", "Akha Language", "Saek language", "Malay, Pattani Language", "Phu Thai language", "Nyaw Language", "Thai Language", "Mlabri Language", "Mon Language", "Vietnamese Language", "Hmong language"], "ans_acc": 0.3076923076923077, "ans_hit": 1, "ans_f1": 0.4444444444444444, "ans_precission": 0.8, "ans_recall": 0.3076923076923077, "path_f1": 0.3157894736842105, "path_precision": 0.5, "path_recall": 0.23076923076923078, "path_ans_f1": 0.4444444444444444, "path_ans_precision": 0.8, "path_ans_recall": 0.3076923076923077}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Programmer\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Entrepreneur\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.parents -> Edward Zuckerberg\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.owner -> Mark Zuckerberg\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Businessperson\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.parents -> Karen Kempner\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> influence.influence_node.influenced -> Austin Cameron\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.owner -> Facebook, Inc.\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.category -> Social Network (Consumer)\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> influence.influence_node.influenced -> andyforsberg\n# Answer:\nMark Zuckerberg"], "ground_truth": ["Cameron Winklevoss", "Tyler Winklevoss"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> common.topic.notable_types -> Profession\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> fictional_universe.character_occupation.characters_with_this_occupation -> Indiana Jones\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Farmer -> common.topic.notable_types -> Profession\n# Answer:\nFarmer", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Statesman -> common.topic.notable_types -> Profession\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> people.profession.specialization_of -> Scientist\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Architect -> common.topic.notable_types -> Profession\n# Answer:\nArchitect", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Statesman -> people.profession.specialization_of -> Politician\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Polly Jefferson\n# Answer:\nPolly Jefferson", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician"], "ground_truth": ["Statesman", "Author", "Writer", "Architect", "Teacher", "Philosopher", "Archaeologist", "Lawyer", "Farmer", "Inventor"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.509090909090909, "ans_precission": 0.7, "ans_recall": 0.4, "path_f1": 0.37499999999999994, "path_precision": 0.5, "path_recall": 0.3, "path_ans_f1": 0.509090909090909, "path_ans_precision": 0.7, "path_ans_recall": 0.4}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 11: 1863\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 12: 1864\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin -> book.author.works_written -> The Autobiography of Charles Darwin\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Anne Darwin -> people.person.parents -> Emma Darwin\n# Answer:\nAnne Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin -> book.author.works_written -> Observations on Stomata\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin -> people.person.spouse_s -> m.0j4l95f\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin -> book.author.works_written -> The Life and Letters of Charles Darwin\n# Answer:\nFrancis Darwin"], "ground_truth": ["From Darwin's unpublished notebooks", "The Correspondence of Charles Darwin, Volume 7", "On the Movements and Habits of Climbing Plants", "vari\u00eberen der huisdieren en cultuurplanten", "Voyage of the Beagle", "Charles Darwin's natural selection", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "The portable Darwin", "The Correspondence of Charles Darwin, Volume 17: 1869", "The Correspondence of Charles Darwin, Volume 10", "Part I: Contributions to the Theory of Natural Selection / Part II", "Del Plata a Tierra del Fuego", "Darwin-Wallace", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Reise eines Naturforschers um die Welt", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "The Origin of Species (Collector's Library)", "The Orgin of Species", "The Origin of Species (Enriched Classics)", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "The Correspondence of Charles Darwin, Volume 4", "The Correspondence of Charles Darwin, Volume 14", "On Natural Selection", "The Origin Of Species", "The foundations of the Origin of species", "Notes on the fertilization of orchids", "To the members of the Down Friendly Club", "The Correspondence of Charles Darwin, Volume 11", "The Autobiography Of Charles Darwin", "The Correspondence of Charles Darwin, Volume 14: 1866", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Evolution by natural selection", "genese\u014ds t\u014dn eid\u014dn", "The Structure and Distribution of Coral Reefs", "From so simple a beginning", "The Origin of Species (Great Minds Series)", "Volcanic Islands", "The geology of the voyage of H.M.S. Beagle", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The Autobiography of Charles Darwin (Dodo Press)", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Evolution", "The Descent of Man and Selection in Relation to Sex", "Motsa ha-minim", "A student's introduction to Charles Darwin", "The Correspondence of Charles Darwin, Volume 1", "Diary of the voyage of H.M.S. Beagle", "The structure and distribution of coral reefs", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "Die geschlechtliche Zuchtwahl", "Kleinere geologische Abhandlungen", "La vie et la correspondance de Charles Darwin", "More Letters of Charles Darwin", "Charles Darwin on the routes of male humble bees", "The Voyage of the Beagle (Great Minds Series)", "Origin of Species (Everyman's University Paperbacks)", "The education of Darwin", "Darwin and Henslow", "Reise um die Welt 1831 - 36", "Origin of Species (Harvard Classics, Part 11)", "Autobiography of Charles Darwin", "The Origin of Species (Oxford World's Classics)", "The Correspondence of Charles Darwin, Volume 13", "The Correspondence of Charles Darwin, Volume 12", "Darwin's insects", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Gesammelte kleinere Schriften", "Darwin Darwin", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "The living thoughts of Darwin", "From So Simple a Beginning", "The Correspondence of Charles Darwin, Volume 8: 1860", "Monographs of the fossil Lepadidae and the fossil Balanidae", "Resa kring jorden", "The Autobiography of Charles Darwin (Great Minds Series)", "The voyage of the Beagle.", "The Formation of Vegetable Mould through the Action of Worms", "Wu zhong qi yuan", "The Power of Movement in Plants", "Charles Darwin's marginalia", "On the tendency of species to form varieties", "The Correspondence of Charles Darwin, Volume 13: 1865", "Darwin en Patagonia", "The descent of man, and selection in relation to sex.", "Diario del Viaje de Un Naturalista Alrededor", "Darwin", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The Voyage of the Beagle", "The Expression of the Emotions in Man And Animals", "Voyage Of The Beagle", "Tesakneri tsagume\u030c", "The Origin of Species (World's Classics)", "Origins", "Het uitdrukken van emoties bij mens en dier", "Metaphysics, Materialism, & the evolution of mind", "Voyage d'un naturaliste autour du monde", "Die fundamente zur entstehung der arten", "The Autobiography of Charles Darwin [EasyRead Edition]", "The Autobiography of Charles Darwin", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "The expression of the emotions in man and animals.", "Proiskhozhdenie vidov", "The Correspondence of Charles Darwin, Volume 3", "The Correspondence of Charles Darwin, Volume 2", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Darwin's Ornithological notes", "The Origin of Species (Variorum Reprint)", "The Correspondence of Charles Darwin, Volume 6", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "Works", "The autobiography of Charles Darwin, 1809-1882", "The Origin of Species (Great Books : Learning Channel)", "Geological Observations on South America", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "Leben und Briefe von Charles Darwin", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "The descent of man, and selection in relation to sex", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The Essential Darwin", "Darwin on humus and the earthworm", "Beagle letters", "Voyage of the Beagle (Harvard Classics, Part 29)", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Cartas de Darwin 18251859", "The Autobiography of Charles Darwin, and selected letters", "Voyage of the Beagle (NG Adventure Classics)", "The Voyage of the Beagle (Mentor)", "The origin of species", "The Correspondence of Charles Darwin, Volume 10: 1862", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "Darwin for Today", "The Correspondence of Charles Darwin, Volume 15: 1867", "The Structure And Distribution of Coral Reefs", "The Different Forms of Flowers on Plants of the Same Species", "Insectivorous Plants", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "The Darwin Reader First Edition", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Charles Darwin's letters", "red notebook of Charles Darwin", "H.M.S. Beagle in South America", "The origin of species : complete and fully illustrated", "The Life of Erasmus Darwin", "The Origin of Species (Mentor)", "Rejse om jorden", "Charles Darwin", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "The Correspondence of Charles Darwin, Volume 5", "The Voyage of the Beagle (Adventure Classics)", "Opsht\u0323amung fun menshen", "The\u0301orie de l'e\u0301volution", "The Voyage of the Beagle (Unabridged Classics)", "Darwin's journal", "Voyage of the Beagle (Dover Value Editions)", "A Darwin Selection", "Les moyens d'expression chez les animaux", "Memorias y epistolario i\u0301ntimo", "The Expression Of The Emotions In Man And Animals", "The descent of man and selection in relation to sex.", "The Variation of Animals and Plants under Domestication", "Darwinism stated by Darwin himself", "The collected papers of Charles Darwin", "The Autobiography of Charles Darwin (Large Print)", "The Correspondence of Charles Darwin, Volume 9: 1861", "The Correspondence of Charles Darwin, Volume 16: 1868", "Questions about the breeding of animals", "Darwin's notebooks on transmutation of species", "monograph on the sub-class Cirripedia", "Human nature, Darwin's view", "The Correspondence of Charles Darwin, Volume 18: 1870", "La facult\u00e9 motrice dans les plantes", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "The structure and distribution of coral reefs.", "ontstaan der soorten door natuurlijke teeltkeus", "The principal works", "On the origin of species by means of natural selection", "The Expression of the Emotions in Man and Animals", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "The voyage of Charles Darwin", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The Darwin Reader Second Edition", "Les mouvements et les habitudes des plantes grimpantes", "The Origin of Species", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "The Correspondence of Charles Darwin, Volume 9", "On evolution", "The action of carbonate of ammonia on the roots of certain plants", "Darwin Compendium", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Fertilisation of Orchids", "On a remarkable bar of sandstone off Pernambuco", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Origin of Species", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "The autobiography of Charles Darwin", "The Correspondence of Charles Darwin, Volume 12: 1864", "The Descent of Man, and Selection in Relation to Sex", "The Correspondence of Charles Darwin, Volume 11: 1863", "Evolution and natural selection", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "The expression of the emotions in man and animals", "The Voyage of the Beagle (Everyman Paperbacks)", "Notebooks on transmutation of species", "The Correspondence of Charles Darwin, Volume 15", "The Correspondence of Charles Darwin, Volume 8", "El Origin De Las Especies", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)"], "ans_acc": 0.08878504672897196, "ans_hit": 1, "ans_f1": 0.018499486125385406, "ans_precission": 0.9, "ans_recall": 0.009345794392523364, "path_f1": 0.4324324324324324, "path_precision": 1.0, "path_recall": 0.27586206896551724, "path_ans_f1": 0.1630901287553648, "path_ans_precision": 1.0, "path_ans_recall": 0.08878504672897196}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.team -> New York Jets\n# Answer:\nNew York Jets", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0j5dkn6 -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nTim Tebow -> common.topic.notable_types -> Organization leader -> freebase.type_hints.included_types -> Person\n# Answer:\nOrganization leader", "# Reasoning Path:\nTim Tebow -> common.topic.notable_types -> Organization leader -> freebase.type_profile.kind -> Role\n# Answer:\nOrganization leader", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> book.written_work.author -> Nathan Whitaker\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> common.topic.notable_types -> Organization leader -> type.type.expected_by -> Person\n# Answer:\nOrganization leader", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nTim Tebow -> common.topic.notable_types -> Organization leader -> freebase.type_hints.included_types -> Topic\n# Answer:\nOrganization leader"], "ground_truth": ["Florida Gators football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.07mmh5w -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.07mmh5w -> american_football.player_rushing_statistics.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.0791773 -> american_football.player_rushing_statistics.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.07sgy3b -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.0791773 -> american_football.player_rushing_statistics.season -> 2008 NFL season\n# Answer:\n2008 NFL season"], "ground_truth": ["Denver Broncos"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Czech Republic -> location.location.containedby -> Eurasia\n# Answer:\nCzech Republic", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Eastern European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.partially_contains -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Hungary -> base.locations.countries.continent -> Europe\n# Answer:\nHungary", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Babia G\u00f3ra -> location.location.containedby -> Europe\n# Answer:\nBabia G\u00f3ra", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Czech Republic\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Czech Republic -> base.locations.countries.continent -> Europe\n# Answer:\nCzech Republic", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Poland -> location.location.containedby -> Eurasia\n# Answer:\nPoland"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Author -> tv.tv_subject.tv_episodes -> J. J. Woods \u2013 Author - The Forgotten Covenant\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.notable_types -> Profession\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Author -> type.type.expected_by -> author\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Author -> tv.tv_subject.tv_episodes -> Author David Kerr - The Ol' Turkey Hunter\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.article -> m.0knj9\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> J. D. Salinger -> people.person.profession -> Writer\n# Answer:\nJ. D. Salinger", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Author -> tv.tv_subject.tv_episodes -> Leisha Kelly \u2013 author- \\\"House on Malcolm Street\\\"\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Cardiovascular disease\n# Answer:\nMale", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> J. D. Salinger -> people.person.profession -> Novelist\n# Answer:\nJ. D. Salinger"], "ground_truth": ["Author", "Writer", "Poet", "Bard"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.5833333333333334, "ans_precission": 0.7, "ans_recall": 0.5, "path_f1": 0.5833333333333334, "path_precision": 0.7, "path_recall": 0.5, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.8, "path_ans_recall": 0.75}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.actor -> Hayden Christensen\n# Answer:\nHayden Christensen", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.actor -> Hayden Christensen\n# Answer:\nHayden Christensen", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.film -> Star Wars Episode III: Revenge of the Sith\n# Answer:\nStar Wars Episode III: Revenge of the Sith", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.film -> Star Wars Episode II: Attack of the Clones\n# Answer:\nStar Wars Episode II: Attack of the Clones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nThe Making of Star Wars", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.part_of_series -> Jedi Quest\n# Answer:\nPath to Truth", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.author -> Judy Blundell\n# Answer:\nPath to Truth"], "ground_truth": ["Hayden Christensen"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.season -> 2006 NFL season\n# Answer:\n2006 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_8 -> sports.sports_award.season -> 2003 NFL season\n# Answer:\n2003 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.award -> Super Bowl Most Valuable Player Award\n# Answer:\nSuper Bowl Most Valuable Player Award", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_j -> sports.sports_award.season -> 2004 NFL season\n# Answer:\n2004 NFL season", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_8 -> sports.sports_award.award -> National Football League Most Valuable Player Award\n# Answer:\nNational Football League Most Valuable Player Award", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_j -> sports.sports_award.award -> National Football League Most Valuable Player Award\n# Answer:\nNational Football League Most Valuable Player Award", "# Reasoning Path:\nPeyton Manning -> people.person.spouse_s -> m.0hpd4nj -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nFrom", "# Reasoning Path:\nPeyton Manning -> people.person.spouse_s -> m.0hpd4nj -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Canada\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> base.wikipedia_infobox.settlement.area_code -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.administrative_divisions -> Ontario\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Canada\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Ontario\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> common.topic.notable_types -> Country\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> common.topic.notable_types -> City/Town/Village\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.languages_spoken -> English Language\n# Answer:\nCanada"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.06vz4t9 -> military.military_combatant_group.combatants -> Argentina\n# Answer:\nArgentina", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z973l -> military.military_combatant_group.combatants -> Iraq\n# Answer:\nIraq", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> royalty.kingdom.rulers -> Abdullah of Saudi Arabia\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> France\n# Answer:\nFrance", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wphv -> military.casualties.combatant -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wphl -> military.casualties.combatant -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> United Kingdom\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> location.country.official_language -> Arabic Language\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.locations -> Arabian Peninsula -> location.location.contains -> Saudi Arabia\n# Answer:\nArabian Peninsula"], "ground_truth": ["France", "Saudi Arabia", "Iraq", "United States of America", "Argentina", "Australia", "United Kingdom"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.2129277566539924, "path_precision": 0.8, "path_recall": 0.12280701754385964, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nBrenda Song", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.actor -> Phill Lewis\n# Answer:\nPhill Lewis", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 1\n# Answer:\nThe Suite Life on Deck - Season 1", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Show\n# Answer:\nKids' Choice Award for Favorite TV Show", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 2\n# Answer:\nThe Suite Life on Deck - Season 2", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> award.award_nomination.ceremony -> 2011 Kids' Choice Awards\n# Answer:\n2011 Kids' Choice Awards", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkyvs -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Actor\n# Answer:\nKids' Choice Award for Favorite TV Actor", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.05v3ngr -> tv.regular_tv_appearance.actor -> Erin Cardillo\n# Answer:\nErin Cardillo", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 2\n# Answer:\nThe Suite Life on Deck - Season 2", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.05v3ngr -> tv.regular_tv_appearance.character -> Emma Tutweiller\n# Answer:\nEmma Tutweiller"], "ground_truth": ["Brenda Song"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.office_holder -> Rob Portman\n# Answer:\nRob Portman", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.office_holder -> George Voinovich\n# Answer:\nGeorge Voinovich", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nUnited States Senate", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_holder -> Sherrod Brown\n# Answer:\nSherrod Brown", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.image -> The Rhodes State Office Tower adjacent to the Statehouse\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.notable_for -> g.125h3hwcp\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> meteorology.tropical_cyclone.affected_areas -> Alabama\n# Answer:\nHurricane Bob", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> common.topic.notable_for -> g.1259xs2jv\n# Answer:\nHurricane Bob"], "ground_truth": ["John Kasich", "Ted Strickland", "Return J. Meigs, Jr."], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m -> soccer.football_player_stats.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9021c -> soccer.football_player_stats.team -> A.C. Milan\n# Answer:\nA.C. Milan", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> tv.tv_actor.guest_roles -> m.0_z851f -> tv.tv_guest_role.special_performance_type -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nPound sterling"], "ground_truth": ["LA Galaxy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.3529411764705882, "path_precision": 0.3, "path_recall": 0.42857142857142855, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.containedby -> Province of Salamanca\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.containedby -> Spain\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Hidalgo\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.location.containedby -> California\n# Answer:\nCoronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7t8559g\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Zacuto\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.location.containedby -> San Diego County\n# Answer:\nCoronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Alfonso XI of Castile\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7tbpl3m\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.location.containedby -> United States of America\n# Answer:\nCoronado"], "ground_truth": ["Salamanca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.namesake.named_after -> John F. Fitzgerald -> people.person.children -> Rose Kennedy\n# Answer:\nJohn F. Fitzgerald", "# Reasoning Path:\nJohn F. Kennedy -> symbols.namesake.named_after -> John F. Fitzgerald -> people.person.religion -> Catholicism\n# Answer:\nJohn F. Fitzgerald", "# Reasoning Path:\nJohn F. Kennedy -> symbols.namesake.named_after -> John F. Fitzgerald -> people.person.children -> Thomas Acton Fitzgerald\n# Answer:\nJohn F. Fitzgerald", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.namesake.named_after -> John F. Fitzgerald -> people.person.children -> Eunice Fitzgerald\n# Answer:\nJohn F. Fitzgerald", "# Reasoning Path:\nJohn F. Kennedy -> symbols.namesake.named_after -> John F. Fitzgerald -> people.person.education -> m.04hd724\n# Answer:\nJohn F. Fitzgerald", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> base.culturalevent.event.entity_involved -> Lee Harvey Oswald\n# Answer:\nAssassination of John F. Kennedy"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.location.containedby -> Asia\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> Fukushima Prefecture\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_for -> g.125btmfy0\n# Answer:\nFukushima I \u2013 1", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Aichi Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> Japan\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> common.topic.image -> Okuma town office\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.country.administrative_divisions -> Aichi Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> T\u014dhoku region\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_types -> Structure\n# Answer:\nFukushima I \u2013 1"], "ground_truth": ["Japan", "Okuma"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Scotland -> location.country.first_level_divisions -> Aberdeen\n# Answer:\nScotland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Scotland -> base.aareas.schema.administrative_area.administrative_children -> Stirling\n# Answer:\nScotland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Down\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Scotland -> location.country.first_level_divisions -> Aberdeenshire\n# Answer:\nScotland"], "ground_truth": ["Scotland", "Northern Ireland", "England", "Wales"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wg8__0 -> location.partial_containment_relationship.partially_contains -> Sabine River\n# Answer:\nSabine River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> location.location.containedby -> North America\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> geography.river.cities -> Denham Springs\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Bartholomew -> common.topic.notable_types -> River\n# Answer:\nBayou Bartholomew", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> location.location.containedby -> United States of America\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Macon -> common.topic.notable_types -> River\n# Answer:\nBayou Macon"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.profession -> Philosopher\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Albert Camus -> people.person.profession -> Philosopher\n# Answer:\nAlbert Camus", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.specializations -> Author\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.corresponding_type -> Author\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alvin Plantinga -> people.person.profession -> Physician\n# Answer:\nAlvin Plantinga", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.specializations -> Novelist\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> people.person.quotations -> A thing is not necessarily true because badly uttered, nor false because spoken magnificently. -> common.topic.notable_types -> Quotation\n# Answer:\nA thing is not necessarily true because badly uttered, nor false because spoken magnificently.", "# Reasoning Path:\nAugustine of Hippo -> people.person.quotations -> By faithfulness we are collected and wound up into unity within ourselves, whereas we had been scattered abroad in multiplicity. -> media_common.quotation.subjects -> Faithfulness\n# Answer:\nBy faithfulness we are collected and wound up into unity within ourselves, whereas we had been scattered abroad in multiplicity."], "ground_truth": ["Philosopher", "Writer", "Physician"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> people.person.children -> Sean Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Parents\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> freebase.valuenotation.has_value -> Parents\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> common.topic.notable_types -> Person\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> people.person.children -> Neffeteria Pugh\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan -> music.recording.featured_artists -> J. Holiday\n# Answer:\n#1 Fan", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> common.topic.notable_types -> Person\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Sal Gibson -> common.topic.notable_types -> Deceased Person\n# Answer:\nSal Gibson"], "ground_truth": ["Leon Cole", "Sal Gibson", "Francine Lons"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> France\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Portugal\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3f_h6\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Syria\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> common.topic.notable_types -> Governmental Body\n# Answer:\nParliament of Egypt"], "ground_truth": ["Provisional government", "Semi-presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.events -> Assassination of Martin Luther King, Jr.\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.containedby -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nAlpha Man: The Brotherhood of MLK", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> common.topic.article -> m.05nnh0\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> common.topic.image -> Dexter Avenue Baptist\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0_714v2 -> film.personal_film_appearance.film -> Alice Walker: Beauty in Truth\n# Answer:\nAlice Walker: Beauty in Truth", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0gbz10_ -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0_714v2 -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> Maryland\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Agatha Christie -> influence.influence_node.influenced_by -> Arthur Conan Doyle\n# Answer:\nAgatha Christie", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Jules Verne -> influence.influence_node.influenced -> Alexandru Macedonski\n# Answer:\nJules Verne", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Jules Verne -> base.kwebbase.kwtopic.connections_to -> edgar allan poe influenced jules verne\n# Answer:\nJules Verne", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Jules Verne -> freebase.valuenotation.is_reviewed -> Children\n# Answer:\nJules Verne", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime. -> media_common.quotation.subjects -> Christians and Christianity\n# Answer:\nA strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Jules Verne -> influence.influence_node.influenced -> Arthur C. Clarke\n# Answer:\nJules Verne", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Alan Moore -> influence.influence_node.influenced_by -> Arthur Machen\n# Answer:\nAlan Moore"], "ground_truth": ["Baltimore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> common.topic.notable_types -> Zoo\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Men's rings\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.geolocation -> m.0clwfck\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> travel.transport_terminus.travel_destinations_served -> m.052zwlt\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Women's artistic team all-around\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> United States of America\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall -> business.shopping_center.address -> g.11b7v_3l1h\n# Answer:\nArbor Place Mall", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> location.location.containedby -> Georgia\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport"], "ground_truth": ["Atlanta Cyclorama & Civil War Museum", "Center for Puppetry Arts", "Cobb Energy Performing Arts Centre", "Underground Atlanta", "Hyatt Regency Atlanta", "Fox Theatre", "Georgia World Congress Center", "Georgia State Capitol", "Atlanta Ballet", "Six Flags White Water", "Four Seasons Hotel Atlanta", "Atlanta Marriott Marquis", "Peachtree Road Race", "Arbor Place Mall", "Jimmy Carter Library and Museum", "Georgia Aquarium", "The Tabernacle", "Fernbank Museum of Natural History", "Masquerade", "Variety Playhouse", "Atlanta History Center", "Woodruff Arts Center", "Martin Luther King, Jr. National Historic Site", "Zoo Atlanta", "Atlanta Symphony Orchestra", "Fernbank Science Center", "World of Coca-Cola", "Philips Arena", "Centennial Olympic Park", "Atlanta Jewish Film Festival", "Omni Coliseum", "Georgia Dome", "Turner Field", "CNN Center", "Six Flags Over Georgia", "Margaret Mitchell House & Museum"], "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0.1509433962264151, "ans_precission": 0.8, "ans_recall": 0.08333333333333333, "path_f1": 0.1509433962264151, "path_precision": 0.8, "path_recall": 0.08333333333333333, "path_ans_f1": 0.19512195121951217, "path_ans_precision": 0.8, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.containedby -> Queensland\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.containedby -> Australia\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> common.topic.notable_types -> City/Town/Village\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.district_represented -> Electoral district of South Brisbane\n# Answer:\nElectoral district of South Brisbane", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Adrienne Pickering\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr301h -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.appears_in_topic_gallery -> Queensland state election, 2009\n# Answer:\nAnna Bligh crop", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Alan Marshal\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Alison Mosely\n# Answer:\nWarwick"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ken Barlow\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> people.person.gender -> Male\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> David Barlow\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> film.actor.film -> m.0h0_mvx\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Albert Tatlock\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> common.topic.notable_for -> g.1255pdp4f\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Soap opera -> common.topic.webpage -> m.09w1_hc\n# Answer:\nSoap opera", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Soap opera -> common.topic.article -> m.06q7y\n# Answer:\nSoap opera", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> common.topic.notable_types -> Multipart TV episode\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Soap opera -> common.topic.webpage -> m.09w1bxj\n# Answer:\nSoap opera"], "ground_truth": ["William Roache"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.administrative_division.second_level_division_of -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> base.biblioness.bibs_location.country -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> Scotland\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#domain -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Gender -> type.property.schema -> Person\n# Answer:\nGender"], "ground_truth": ["2005"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc4pgdn\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.country.languages_spoken -> Malay Language\n# Answer:\nChristmas Island", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Bunyip\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nCocos (Keeling) Islands", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc4y0zl\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> base.ontologies.ontology_instance.equivalent_instances -> m.09knr56 -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nOpenCyc", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Yowie\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands -> location.location.time_zones -> Cocos Islands\u00a0Time Zone\n# Answer:\nCocos (Keeling) Islands", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.statistical_region.population -> g.11bymn224r\n# Answer:\nChristmas Island", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands -> location.country.form_of_government -> Federal monarchy\n# Answer:\nCocos (Keeling) Islands"], "ground_truth": ["AUD"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ll\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22zj\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc378pv\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc37hbq\n# Answer:\nlocation.statistical_region.part_time_employment_percent"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> common.topic.notable_for -> g.125dysc88\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.team -> Carolina Panthers\n# Answer:\nCarolina Panthers", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> common.topic.notable_types -> American football player\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0z23kt0 -> sports.sports_team_roster.team -> Auburn Tigers football\n# Answer:\nAuburn Tigers football", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> people.person.children -> Cecil Newton\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> people.person.parents -> Jackie Newton -> common.topic.notable_for -> g.125dzwcd6\n# Answer:\nJackie Newton", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber"], "ground_truth": ["Carolina Panthers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.location.containedby -> Bethesda-Frederick-Gaithersburg, MD Metropolitan Division\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.location.containedby -> Maryland\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.location.containedby -> Bethesda-Frederick-Gaithersburg, MD Metropolitan Division\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.rent50_1 -> m.05gcgl3\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.population -> g.11b66h2c0w\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.containedby -> United States of America\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.location.containedby -> Maryland\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.statistical_region.rent50_1 -> m.05gcgl3\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.population -> g.11x1cfntl\n# Answer:\nFrederick County"], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nHuntingdon College", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> University of Alabama School of Law\n# Answer:\nUniversity of Alabama School of Law", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138 -> education.education.institution -> University of Alabama\n# Answer:\nUniversity of Alabama", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.place_of_birth -> Los Angeles\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.gender -> Female\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_types -> Book Edition\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_for -> g.125920htw\n# Answer:\nHarper Lee's To Kill a Mockingbird"], "ground_truth": ["Monroe County High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> book.book_subject.works -> Sagebrush country -> common.topic.notable_types -> Book\n# Answer:\nSagebrush country", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nUtah -> book.book_subject.works -> Sagebrush country -> common.topic.notable_for -> g.125c_wfqj\n# Answer:\nSagebrush country", "# Reasoning Path:\nUtah -> book.book_subject.works -> Sagebrush country -> book.book.editions -> Sagebrush country: land and the American West\n# Answer:\nSagebrush country", "# Reasoning Path:\nUtah -> book.book_subject.works -> Blossoms of faith -> common.topic.notable_types -> Book\n# Answer:\nBlossoms of faith", "# Reasoning Path:\nUtah -> book.book_subject.works -> Bound for Canaan -> book.written_work.author -> Margaret Blair Young\n# Answer:\nBound for Canaan", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> government.government_position_held.office_position_or_title -> Lieutenant Governor of Utah\n# Answer:\nLieutenant Governor of Utah"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> people.person.ethnicity -> White people\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> freebase.valuenotation.has_value -> Siblings\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> people.person.nationality -> United States of America\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointed_role -> Director of the Bureau of Counterterrorism\n# Answer:\nDirector of the Bureau of Counterterrorism", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pk -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointee -> Henry A. Crumpton\n# Answer:\nHenry A. Crumpton", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1 -> people.appointment.appointed_role -> United States Ambassador to Angola\n# Answer:\nUnited States Ambassador to Angola", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079q3lx -> people.appointment.appointed_role -> United States Ambassador to Brazil\n# Answer:\nUnited States Ambassador to Brazil", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pv -> film.personal_film_appearance.film -> All About Ann: Governor Richards of the Lone Star State\n# Answer:\nAll About Ann: Governor Richards of the Lone Star State", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1 -> people.appointment.appointee -> Dan Mozena\n# Answer:\nDan Mozena"], "ground_truth": ["Gene Amondson", "John Kerry", "Michael Peroutka", "Ralph Nader"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> people.person.parents -> Ayaan Hirsi Ali\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> people.person.gender -> Male\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.spouse -> Ayaan Hirsi Ali\n# Answer:\nAyaan Hirsi Ali", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> people.marriage.spouse -> Sue Douglas\n# Answer:\nSue Douglas", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> freebase.valuenotation.has_value -> Place of birth\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Pacific Ocean\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> base.aareas.schema.administrative_area.administrative_parent -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.containedby -> Americas\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> common.topic.article -> m.03660h\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Pacific Ocean -> location.location.containedby -> World Ocean\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.primarily_containedby -> South America\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.partially_containedby -> South America\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Santiago Island -> location.location.containedby -> Ecuador\n# Answer:\nSantiago Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Baltra Island -> location.location.containedby -> Santa Cruz Canton, Ecuador\n# Answer:\nBaltra Island"], "ground_truth": ["Gal\u00e1pagos Province", "Ecuador", "Pacific Ocean"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> Somebody to Love\n# Answer:\nSomebody to Love", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> As Long as You Love Me -> music.composition.composer -> Andre Lindal\n# Answer:\nAs Long as You Love Me", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> As Long as You Love Me -> music.album.releases -> As Long As You Love Me (remixes)\n# Answer:\nAs Long as You Love Me", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> As Long as You Love Me -> common.topic.notable_for -> g.1257jn46q\n# Answer:\nAs Long as You Love Me", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.featured_artists -> Ludacris\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> As Long as You Love Me -> music.composition.composer -> Big Sean\n# Answer:\nAs Long as You Love Me", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> music.composition.composer -> Dre & Vidal\n# Answer:\nAll Bad", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> As Long as You Love Me -> common.topic.notable_for -> g.125ddwtp0\n# Answer:\nAs Long as You Love Me", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> As Long as You Love Me -> music.composition.composer -> Nasri\n# Answer:\nAs Long as You Love Me"], "ground_truth": ["Change Me", "Lolly", "Die in Your Arms", "Turn to You (Mother's Day Dedication)", "Beauty And A Beat", "All Bad", "Never Say Never", "Home to Mama", "Pray", "As Long as You Love Me", "Thought Of You", "Live My Life", "Confident", "Bigger", "Roller Coaster", "Boyfriend", "Never Let You Go", "Bad Day", "Right Here", "PYD", "All Around The World", "Wait for a Minute", "Somebody to Love", "First Dance", "Eenie Meenie", "Hold Tight", "All That Matters", "#thatPower", "Baby", "Heartbreaker", "Recovery"], "ans_acc": 0.12903225806451613, "ans_hit": 1, "ans_f1": 0.22570532915360503, "ans_precission": 0.9, "ans_recall": 0.12903225806451613, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.22570532915360503, "path_ans_precision": 0.9, "path_ans_recall": 0.12903225806451613}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> common.topic.article -> m.016fc9\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> common.topic.notable_types -> Profession\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Acetylene Lamp\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> base.descriptive_names.names.descriptive_name -> m.0101fnw4\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> people.profession.corresponding_type -> Author\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Physician -> common.topic.notable_types -> Profession\n# Answer:\nPhysician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> type.type.expected_by -> politician\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> symbols.name_source.namesakes -> Champs-\u00c9lys\u00e9es \u2013 Clemenceau -> common.topic.notable_types -> Transit Stop\n# Answer:\nChamps-\u00c9lys\u00e9es \u2013 Clemenceau", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> fictional_universe.character_occupation.characters_with_this_occupation -> Arthur Wellesley, 1st Duke of Wellington\n# Answer:\nStatesman"], "ground_truth": ["Statesman", "Writer", "Publisher", "Physician", "Journalist"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.6, "ans_recall": 0.6, "path_f1": 0.6, "path_precision": 0.6, "path_recall": 0.6, "path_ans_f1": 0.6, "path_ans_precision": 0.6, "path_ans_recall": 0.6}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nSaguaro", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.symbol -> Petrified wood\n# Answer:\nPetrified wood", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.Kind_of_symbol -> State flower\n# Answer:\nState flower", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz -> location.location_symbol_relationship.symbol -> Mountain tree frog\n# Answer:\nMountain tree frog", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.Kind_of_symbol -> State fossil\n# Answer:\nState fossil", "# Reasoning Path:\nArizona -> location.statistical_region.religions -> m.04403h9 -> location.religion_percentage.religion -> Pentecostalism\n# Answer:\nPentecostalism", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.resource -> ATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!\n# Answer:\nATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1l1s -> common.webpage.resource -> ATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!\n# Answer:\nATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz -> location.location_symbol_relationship.Kind_of_symbol -> State Amphibian\n# Answer:\nState Amphibian"], "ground_truth": ["Saguaro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.location.contains -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.country.administrative_divisions -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> common.topic.notable_types -> Country\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> base.aareas.schema.administrative_area.administrative_parent -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.063y0bl -> base.popstra.infidelity.victim -> Beyonc\u00e9 Knowles\n# Answer:\nBeyonc\u00e9 Knowles", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> m.010_ydxv -> music.track_contribution.track -> If I Never See Your Face Again\n# Answer:\nIf I Never See Your Face Again"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> Harrison inauguration\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\ncommon.image.size", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.03qtjkt\n# Answer:\ncommon.image.size", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> 222px-Harrison_inauguration1841.jpg\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Illinois\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Illinois\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Indiana\n# Answer:\nUnited States of America"], "ground_truth": ["1841-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> common.topic.image -> Ryan Braun\n# Answer:\nRyan Braun", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> California\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> Los Angeles\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Ryan Braun\n# Answer:\nRyan Braun", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.03lppm1 -> common.webpage.resource -> m.0bjrztw\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> Los Angeles County\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Alan Muraoka\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> film.person_or_entity_appearing_in_film.films -> m.0cs3c3w -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.0948qtd -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Chris Snail\n# Answer:\nMission Hills"], "ground_truth": ["Mission Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> business.employer.number_of_employees -> m.0f7q8f_\n# Answer:\nbusiness.employer.number_of_employees", "# Reasoning Path:\nSamsung Group -> common.topic.article -> m.07gv7d\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> base.descriptive_names.names.descriptive_name -> m.0105_khq\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> common.topic.article -> m.0h6dr\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> base.descriptive_names.names.descriptive_name -> m.0105_kp0\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Mahaka Media\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> base.descriptive_names.names.descriptive_name -> m.0105_lnf\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Textron\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Time Warner\n# Answer:\nConglomerate"], "ground_truth": ["Suwon"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.beliefs -> \u1e6c\u016bb\u0101 -> common.topic.notable_for -> g.1q6hmhsk5\n# Answer:\n\u1e6c\u016bb\u0101", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> common.topic.notable_for -> g.1q69mrtxz\n# Answer:\nEntering Heaven alive", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Hinduism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> \u1e6c\u016bb\u0101 -> common.topic.article -> m.0hr6vbt\n# Answer:\n\u1e6c\u016bb\u0101", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.beliefs -> End time\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Jainism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> religion.deity.deity_of -> Sunni Islam\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> common.topic.article -> m.02wvcg8\n# Answer:\nEntering Heaven alive", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Islamic view of angels -> common.topic.article -> m.0698fy\n# Answer:\nIslamic view of angels", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Sikhism\n# Answer:\nRamdev Pir"], "ground_truth": ["Monotheism", "\u1e6c\u016bb\u0101", "Predestination in Islam", "God in Islam", "Prophets in Islam", "Mahdi", "Masih ad-Dajjal", "Islamic view of angels", "Entering Heaven alive", "Sharia", "Tawhid", "Islamic holy books", "Qiyamah"], "ans_acc": 0.23076923076923078, "ans_hit": 1, "ans_f1": 0.3157894736842105, "ans_precission": 0.5, "ans_recall": 0.23076923076923078, "path_f1": 0.3157894736842105, "path_precision": 0.5, "path_recall": 0.23076923076923078, "path_ans_f1": 0.3157894736842105, "path_ans_precision": 0.5, "path_ans_recall": 0.23076923076923078}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.film -> Fifty Shades of Grey\n# Answer:\nFifty Shades of Grey", "# Reasoning Path:\nChristian Grey -> common.topic.notable_for -> g.125czvn3w\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.actor -> Jamie Dornan\n# Answer:\nJamie Dornan", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Males\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Acute myeloid leukemia\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> common.topic.notable_for -> g.1255p_jgc\n# Answer:\nMale"], "ground_truth": ["Jamie Dornan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Anton Chekhov\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Albert Camus\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Isoniazid\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Franz Kafka\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> people.cause_of_death.parent_cause_of_death -> Infectious disease\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Antibiotics\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Pyrazinamide\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> I sometimes think that the price of liberty is not so much eternal vigilance as eternal dirt. -> media_common.quotation.subjects -> Freedom\n# Answer:\nI sometimes think that the price of liberty is not so much eternal vigilance as eternal dirt.", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> I sometimes think that the price of liberty is not so much eternal vigilance as eternal dirt. -> common.topic.notable_types -> Quotation\n# Answer:\nI sometimes think that the price of liberty is not so much eternal vigilance as eternal dirt.", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion. -> media_common.quotation.subjects -> Jokes and Jokers\n# Answer:\nA dirty joke is a sort of mental rebellion."], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.office_position_or_title -> Chancellor of Germany\n# Answer:\nChancellor of Germany", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.office_position_or_title -> F\u00fchrer\n# Answer:\nF\u00fchrer", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.basic_title -> F\u00fchrer\n# Answer:\nF\u00fchrer", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> base.schemastaging.context_name.pronunciation -> g.125_pt37m\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.place_founded -> Nazi Germany\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.founders -> Heinrich Himmler\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> book.book_subject.works -> Anne Frank and the Children of the Holocaust\n# Answer:\nNazi Party"], "ground_truth": ["Nazi Germany"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.28571428571428564, "path_precision": 0.3, "path_recall": 0.2727272727272727, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> common.topic.notable_types -> Profession\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.track_contributions -> g.11b7_lvdf2\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.specialization_of -> Musician\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.specialization_of -> Musician\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> http://www.discogs.com/artist/Michael+Bubl%E9 -> common.webpage.category -> Topic Webpage\n# Answer:\nhttp://www.discogs.com/artist/Michael+Bubl%E9", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.03lpqjt -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> common.topic.notable_types -> Profession\n# Answer:\nSongwriter"], "ground_truth": ["Actor", "Songwriter", "Singer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> location.location.containedby -> United States of America\n# Answer:\nKansas", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.statistical_region.co2_emissions_mobile -> m.045l26t\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111 -> location.location.containedby -> Wyandotte County\n# Answer:\n66111", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.us_county.hud_county_place -> Kansas City\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.location.containedby -> Kansas City, MO-KS Metropolitan Statistical Area\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nKansas", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas City, MO-KS Metropolitan Statistical Area\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.statistical_region.population -> g.11b674pwdp\n# Answer:\nWyandotte County"], "ground_truth": ["Wyandotte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h -> sports.sports_team_roster.team -> UCLA Bruins football\n# Answer:\nUCLA Bruins football", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.season -> 1954 Major League Baseball season\n# Answer:\n1954 Major League Baseball season", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.season -> 1953 Major League Baseball season\n# Answer:\n1953 Major League Baseball season", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0gggrzr -> sports.sports_team_roster.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.season -> 1956 Major League Baseball Season\n# Answer:\n1956 Major League Baseball Season", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgj2z -> sports.sports_team_roster.team -> Los Angeles Bulldogs\n# Answer:\nLos Angeles Bulldogs", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber"], "ground_truth": ["UCLA Bruins football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> people.person.place_of_birth -> Ossining\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> location.location.containedby -> New York\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> location.location.containedby -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> award.award_nominee.award_nominations -> m.010bvypw\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.story_by -> Harold Gray -> comic_strips.comic_strip_creator.comic_strips_written -> m.0gwbjrm\n# Answer:\nHarold Gray", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Play -> type.type.expected_by -> Appears in plays\n# Answer:\nPlay", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Choreographer\n# Answer:\nChoreographer", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> award.award_winner.awards_won -> m.0_5xrrh\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> award.award_nominee.award_nominations -> m.010by0k1\n# Answer:\nThomas Meehan"], "ground_truth": ["1976-08-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Alabama\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07ghzrz\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Area code 251\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b66g7msm\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.education -> m.03gkqtp -> education.education.institution -> Louisiana State University\n# Answer:\nLouisiana State University", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Mobile County\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b7tn_fd2\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11btt6sf_l\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07919ln -> american_football.player_passing_statistics.team -> Oakland Raiders\n# Answer:\nOakland Raiders", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07919ln -> american_football.player_passing_statistics.season -> 2008 NFL season\n# Answer:\n2008 NFL season"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> base.aareas.schema.administrative_area.administrative_parent -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.administrative_division.country -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Anna Roosevelt Halsted -> people.deceased_person.place_of_death -> New York City\n# Answer:\nAnna Roosevelt Halsted", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Elliott Roosevelt -> people.deceased_person.place_of_death -> Scottsdale\n# Answer:\nElliott Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> This I remember -> book.written_work.subjects -> United States of America\n# Answer:\nThis I remember", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Courage in a dangerous world -> book.book_edition.isbn -> 9780231111812\n# Answer:\nCourage in a dangerous world", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Anna Roosevelt Halsted -> people.deceased_person.cause_of_death -> Laryngeal cancer\n# Answer:\nAnna Roosevelt Halsted"], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60ptk2z\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf2_w -> location.religion_percentage.religion -> Islam\n# Answer:\nIslam", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.12cp_j7n1\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_4m6h\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf301 -> location.religion_percentage.religion -> Protestantism\n# Answer:\nProtestantism", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_67l9\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.1hhc378kt\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.064szjw -> location.religion_percentage.religion -> Hinduism\n# Answer:\nHinduism", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.1hhc38d0l\n# Answer:\nlocation.statistical_region.official_development_assistance"], "ground_truth": ["Islam", "Catholicism", "Hinduism", "Protestantism"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.46153846153846156, "ans_precission": 0.3333333333333333, "ans_recall": 0.75, "path_f1": 0.42857142857142855, "path_precision": 0.3333333333333333, "path_recall": 0.6, "path_ans_f1": 0.46153846153846156, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.75}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Countryway Gunshop\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Projectile weapon\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.image -> HK USP 45\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Springfield Armory, Inc.\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> base.weapons.weapon.subtypes -> Handgun\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.location.contains -> American College of Technology\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.image -> M&Prevolver\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> base.fight.crime_type.crimes_of_this_type -> Anna Lindh assassination\n# Answer:\nAssassination", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.location.containedby -> Missouri\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> base.weapons.weapon.subtypes -> Rifle\n# Answer:\nFirearm"], "ground_truth": ["Firearm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.nationality -> United States of America\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.profession -> Politician\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> common.topic.article -> m.03mpv\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pgr_5 -> people.place_lived.location -> Kentucky\n# Answer:\nKentucky", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> people.person.gender -> Male\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pn4x_ -> people.place_lived.location -> Springfield\n# Answer:\nSpringfield", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.04hdfss -> people.place_lived.location -> Illinois\n# Answer:\nIllinois"], "ground_truth": ["Andrew Johnson", "Hannibal Hamlin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7272727272727273, "ans_precission": 0.5714285714285714, "ans_recall": 1.0, "path_f1": 0.7272727272727273, "path_precision": 0.5714285714285714, "path_recall": 1.0, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 1.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> common.topic.notable_types -> Film character\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced_by -> H. G. Wells\n# Answer:\nAldous Huxley", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Oliver Twist -> media_common.adapted_work.adaptations -> Oliver!\n# Answer:\nOliver Twist", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> film.film_character.portrayed_in_films -> m.010p_33b\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Alphonse Daudet -> influence.influence_node.influenced_by -> \u00c9mile Zola\n# Answer:\nAlphonse Daudet", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Oliver Twist -> common.topic.image -> \\\"Please, sir, I want some more.\\\" Illustration by George Cruikshank.\n# Answer:\nOliver Twist", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> common.topic.image -> Scrooges third visitor-John Leech,1843\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> base.kwebbase.kwtopic.has_sentences -> (1846) -> base.kwebbase.kwsentence.dates -> m.0c0z7kp\n# Answer:\n(1846)"], "ground_truth": ["A Christmas Carol (Whole Story)", "A Tale Of Two Cities (Adult Classics)", "A Tale of Two Cities (Illustrated Junior Library)", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Christmas Carol (Oxford Bookworms Library)", "A Tale of Two Cities (Classics Illustrated Notes)", "A Tale of Two Cities (Adopted Classic)", "A Christmas Carol (Dramascripts Classic Texts)", "The life and adventures of Nicholas Nickleby", "Little Dorrit", "A Christmas Carol (Pacemaker Classics)", "A Tale of Two Cities (Silver Classics)", "A Tale of Two Cities (Bookcassette(r) Edition)", "A Tale of Two Cities (Dover Thrift Editions)", "A Tale of Two Cities (Prentice Hall Science)", "A Tale of Two Cities (Penguin Popular Classics)", "A Tale of Two Cities (Signet Classics)", "A Tale of Two Cities (Everyman's Library (Paper))", "A Christmas Carol (Bantam Classic)", "A Tale of Two Cities (Longman Fiction)", "A Christmas Carol (Dramascripts)", "A Tale of Two Cities (Macmillan Students' Novels)", "A Christmas Carol (Tor Classics)", "A Christmas Carol (New Longman Literature)", "The old curiosity shop.", "Bleak House", "A Christmas Carol (Acting Edition)", "A Tale of Two Cities (Oxford Bookworms Library)", "A Tale of Two Cities (Clear Print)", "A Christmas Carol (Chrysalis Children's Classics Series)", "A Tale of Two Cities (Student's Novels)", "A Christmas Carol (Saddleback Classics)", "Dombey and Son.", "A Christmas Carol (Limited Editions)", "A Christmas Carol (Soundings)", "A Tale of Two Cities (Classics Illustrated)", "A Tale of Two Cities (The Classic Collection)", "Our mutual friend", "A Christmas Carol (Classic, Picture, Ladybird)", "Great Expectations.", "A CHRISTMAS CAROL", "Great expectations", "A Tale of Two Cities (Cyber Classics)", "A Tale of Two Cities (The Greatest Historical Novels)", "A Tale of Two Cities (Longman Classics, Stage 2)", "A Christmas Carol (Classic Fiction)", "A Christmas Carol (Thornes Classic Novels)", "A Tale of Two Cities (Isis Clear Type Classic)", "A Tale of Two Cities (Oxford Playscripts)", "A Tale of Two Cities (Illustrated Classics)", "A Tale of Two Cities (Masterworks)", "A Christmas Carol (Family Classics)", "A Christmas Carol (Classic Collection)", "The Old Curiosity Shop", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Christmas Carol (Enriched Classics)", "A Christmas Carol (Young Reading Series 2)", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "A Christmas Carol (Cover to Cover)", "A Christmas Carol (Audio Editions)", "A Christmas Carol (Puffin Choice)", "A Christmas Carol (Take Part)", "Hard times", "A Tale of Two Cities (Simple English)", "A Christmas Carol (Illustrated Classics)", "Bleak House.", "A Tale of Two Cities (40th Anniversary Edition)", "A Christmas Carol (Great Stories)", "A Tale of Two Cities (Collected Works of Charles Dickens)", "A Tale of Two Cities (Compact English Classics)", "A Christmas Carol (Apple Classics)", "A Tale of Two Cities (Dramascripts S.)", "Great expectations.", "A Tale of Two Cities (Cover to Cover Classics)", "A Tale of Two Cities (Everyman's Library Classics)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "Bleak house", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "A Tale of Two Cities (Barnes & Noble Classics Series)", "A Tale of Two Cities (10 Cassettes)", "A Christmas Carol (Penguin Student Editions)", "Dombey and son", "A Tale of Two Cities (Courage Literary Classics)", "A Tale of Two Cities (Paperback Classics)", "Dombey and Son", "A Christmas Carol (Large Print)", "A Christmas Carol (Puffin Classics)", "The Pickwick Papers", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "A Christmas Carol (Everyman's Library Children's Classics)", "A Christmas Carol (The Kennett Library)", "A Christmas Carol (Through the Magic Window Series)", "David Copperfield.", "The Mystery of Edwin Drood", "A Tale of Two Cities (Ladybird Children's Classics)", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "A Tale of Two Cities (Classic Literature with Classical Music)", "A Tale of Two Cities (Penguin Readers, Level 5)", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "Great Expectations", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "A Tale of Two Cities (Dodo Press)", "A Christmas Carol (Ladybird Children's Classics)", "A Tale Of Two Cities (Adult Classics in Audio)", "A Tale of Two Cities (Bantam Classic)", "A Christmas Carol (Value Books)", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "A Tale of Two Cities (Cassette (1 Hr).)", "A Christmas Carol (R)", "A Tale of Two Cities (Classic Fiction)", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (BBC Audio Series)", "A Tale of Two Cities (Naxos AudioBooks)", "A Christmas Carol (Usborne Young Reading)", "Sketches by Boz", "A Tale of Two Cities (Everyman Paperbacks)", "David Copperfield", "A Christmas Carol (Ladybird Classics)", "A Tale of Two Cities (Enriched Classic)", "A Tale of Two Cities (Dramatized)", "The old curiosity shop", "A Christmas Carol", "A Christmas Carol (Aladdin Classics)", "A Tale of Two Cities (Penguin Classics)", "A Christmas Carol (Scholastic Classics)", "A Tale of Two Cities (Acting Edition)", "A Christmas Carol (Cp 1135)", "A Christmas Carol (Clear Print)", "The mystery of Edwin Drood", "A Christmas Carol (Gollancz Children's Classics)", "A Christmas Carol (Nelson Graded Readers)", "A TALE OF TWO CITIES", "A Tale of Two Cities (Large Print Edition)", "A Christmas Carol (Reissue)", "A Christmas Carol. (Lernmaterialien)", "A Christmas Carol (Watermill Classic)", "Martin Chuzzlewit", "A Tale of Two Cities", "Our mutual friend.", "A Tale of Two Cities (Saddleback Classics)", "A Christmas Carol (Read & Listen Books)", "A Christmas Carol (Classics Illustrated)", "A Tale of Two Cities (Puffin Classics)", "A Tale of Two Cities (Ultimate Classics)", "A Christmas Carol (Classic Books on Cassettes Collection)", "A Christmas Carol (Classics for Young Adults and Adults)", "A Christmas Carol (Pacemaker Classic)", "A Tale of Two Cities (Pacemaker Classics)", "A Christmas Carol (Children's Classics)", "A Tale of Two Cities (Piccolo Books)", "A Christmas Carol (Radio Theatre)", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "A Christmas Carol (Watermill Classics)", "A Tale of Two Cities (Wordsworth Classics)", "A Tale of Two Cities (Progressive English)", "A Tale of Two Cities (Konemann Classics)", "The cricket on the hearth", "A Christmas Carol (Children's Theatre Playscript)", "A Tale of Two Cities (Tor Classics)", "A Christmas Carol (Penguin Readers, Level 2)", "A Tale of Two Cities (Collector's Library)", "Oliver Twist", "A Tale of Two Cities (Classic Retelling)", "The Pickwick papers", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "A Tale of Two Cities (Soundings)", "A Tale of Two Cities (Unabridged Classics)", "A Christmas Carol (Green Integer, 50)", "A Christmas Carol (Webster's Korean Thesaurus Edition)"], "ans_acc": 0.01775147928994083, "ans_hit": 1, "ans_f1": 0.011494252873563218, "ans_precission": 0.2, "ans_recall": 0.005917159763313609, "path_f1": 0.23225806451612901, "path_precision": 0.4, "path_recall": 0.16363636363636364, "path_ans_f1": 0.0339943342776204, "path_ans_precision": 0.4, "path_ans_recall": 0.01775147928994083}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.office_holder -> Nikita Khrushchev\n# Answer:\nNikita Khrushchev", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_commander -> Nikita Khrushchev\n# Answer:\nNikita Khrushchev", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_conflict -> Cuban Missile Crisis\n# Answer:\nCuban Missile Crisis", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf -> military.military_command.military_commander -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.049y3kf -> military.military_command.military_commander -> Ivan Konev\n# Answer:\nIvan Konev", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_6 -> government.government_position_held.office_holder -> Vladimir Ivashko\n# Answer:\nVladimir Ivashko", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.governmental_body -> Central Committee of the Communist Party of the Soviet Union\n# Answer:\nCentral Committee of the Communist Party of the Soviet Union", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_holder -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf -> military.military_command.military_conflict -> World War II\n# Answer:\nWorld War II", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.049y3kf -> military.military_command.military_conflict -> Battle of Berlin\n# Answer:\nBattle of Berlin"], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_formerly_used -> Cuba\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> Panama\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> American Samoa\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories -> location.country.first_level_divisions -> United States of America\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nPuerto Rico -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories -> base.aareas.schema.administrative_area.administrative_children -> United States of America\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> Bonaire\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.target_of_exchange -> m.04kq24y\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories -> base.aareas.schema.earth.sovereign_domain.sovereign_state -> United States of America\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nPuerto Rico -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories -> location.country.first_level_divisions -> American Samoa\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.target_of_exchange -> m.03x8_16\n# Answer:\nUnited States Dollar"], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.risk_factor.diseases -> Ptosis\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Radiation therapy\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Dementia\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease.medical_specialties -> Oncology\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Adjuvant\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Epilepsy\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.symptoms -> Air crescent sign\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Head pressing\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease.medical_specialties -> Neurology\n# Answer:\nBrain tumor"], "ground_truth": ["Lung cancer", "Brain tumor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4 -> tv.tv_series_season.episodes -> KITTnap\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4 -> tv.tv_series_season.regular_cast -> m.03lj4m5\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4 -> common.topic.notable_for -> g.125f8s1f9\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.languages -> English Language -> organization.organization_sector.organizations_in_this_sector -> dragon-class.com\n# Answer:\nEnglish Language", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4 -> tv.tv_series_season.episodes -> Burial Ground\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> common.topic.webpage -> m.09w0_mw -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4 -> tv.tv_series_season.episodes -> Deadly Knightshade\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4 -> tv.tv_series_season.regular_cast -> m.02h9cb0\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 0 -> tv.tv_series_season.episodes -> Behind the Scenes\n# Answer:\nKnight Rider - Season 0", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 0 -> common.topic.notable_for -> g.125fby47j\n# Answer:\nKnight Rider - Season 0"], "ground_truth": ["William Daniels"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.location.containedby -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.country -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.location.containedby -> Tennessee\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11b66dwnl4\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11x1ddsd6\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.location.containedby -> Nashville-Davidson--Murfreesboro--Franklin, TN Metropolitan Statistical Area\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11x1fr8dg\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.statistical_region.co2_emissions_total -> m.045hnmm\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.statistical_region.population -> g.11b674pwdr\n# Answer:\nWilliamson County"], "ground_truth": ["Williamson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Missouri\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.geolocation -> m.0kdmvf\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Jasper County\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_for -> g.1255959wd\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Newton County\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11b66mljn1\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> astronomy.extraterrestrial_location.on_celestial_object -> Moon\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11x1chmhk\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_types -> Extraterrestrial location\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver Academy -> location.location.geolocation -> m.0127s0kf\n# Answer:\nCarver Academy"], "ground_truth": ["Diamond"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.profession -> Actor\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Esm\u00e9 Annabelle Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nEsm\u00e9 Annabelle Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.sibling_s -> m.0j217jw\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> common.topic.notable_types -> Person\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.nationality -> United States of America\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.sibling_s -> m.0tjswcf\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvttg -> award.award_nomination.nominated_for -> Spin City\n# Answer:\nSpin City", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.sibling_s -> m.0tjswd3\n# Answer:\nSam Michael Fox"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333326, "path_precision": 0.3, "path_recall": 0.375, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04yvq68 -> military.military_command.military_conflict -> How Few Remain\n# Answer:\nHow Few Remain", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9q6 -> military.military_command.military_conflict -> Battle of McDowell\n# Answer:\nBattle of McDowell", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9nd -> military.military_command.military_conflict -> Battle of Port Republic\n# Answer:\nBattle of Port Republic", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Foot cavalry\n# Answer:\nFoot cavalry", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Abraham Lincoln\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> time.event.includes_event -> Battle of McDowell\n# Answer:\nJackson's Valley Campaign", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> book.book_subject.musical_compositions_about_this_topic -> the CIVIL warS: a tree is best measured when it is down\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Confederate States of America\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> visual_art.art_subject.artwork_on_the_subject -> Battle of Mobile Bay, 5 August 1864\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Antietam -> military.military_conflict.military_personnel_involved -> Albion P. Howe\n# Answer:\nBattle of Antietam"], "ground_truth": ["Battle of Chancellorsville", "Battle of Harpers Ferry", "Battle of Cedar Mountain", "Battle of Chantilly", "Manassas Station Operations", "Battle of Hoke's Run", "Battle of Front Royal", "Romney Expedition", "Jackson's Valley Campaign", "Battle of McDowell", "Battle of Hancock", "American Civil War", "Second Battle of Bull Run", "First Battle of Rappahannock Station", "First Battle of Winchester", "First Battle of Kernstown", "Battle of White Oak Swamp", "Battle of Port Republic", "How Few Remain"], "ans_acc": 0.2631578947368421, "ans_hit": 1, "ans_f1": 0.396039603960396, "ans_precission": 0.8, "ans_recall": 0.2631578947368421, "path_f1": 0.2445414847161572, "path_precision": 0.7, "path_recall": 0.14814814814814814, "path_ans_f1": 0.396039603960396, "path_ans_precision": 0.8, "path_ans_recall": 0.2631578947368421}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.parent -> Ongamo-Maa Group\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> common.topic.article -> m.064klf\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.document -> Maasai Phonology\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.document -> Maasai Detailed Description\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai -> common.image.size -> m.02bgrp_\n# Answer:\nMaasai", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.document -> Maasai Genesis Translation\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai women and children -> common.image.size -> m.03qy0z9\n# Answer:\nMaasai women and children", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai-jump -> common.image.size -> m.02br_p7\n# Answer:\nMaasai-jump"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Anne Harris\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> people.marriage.spouse -> Deborah Read\n# Answer:\nDeborah Read", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Ebenezer Franklin\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.place_of_birth -> Ecton\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger -> people.person.parents -> Peter Folger\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Elizabeth Douse\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.spouse_s -> m.0j4k0mt\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> people.marriage.type_of_union -> Common-law marriage\n# Answer:\nCommon-law marriage", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger -> people.person.parents -> Mary Morrill\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger -> people.deceased_person.place_of_burial -> Granary Burying Ground\n# Answer:\nAbiah Folger"], "ground_truth": ["Deborah Read"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9g\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Old age\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9q\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.notable_people_with_this_condition -> Marilyn Horne\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> African American\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.notable_people_with_this_condition -> Chuck Daly\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.includes_diseases -> Pancreatic Endocrine Carcinoma\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Chronic pancreatitis\n# Answer:\nPancreatic cancer"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Raphael and his age\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Leonardo da Vinci and the Art of Sculpture: Inspiration and Invention\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> people.profession.people_with_this_profession -> Sandro Botticelli\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> influence.influence_node.influenced_by -> Melozzo da Forl\u00ec\n# Answer:\nAntonio da Correggio", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper -> common.topic.article -> m.0jnzf\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Holy Infants Embracing -> common.topic.image -> Holy-infs\n# Answer:\nThe Holy Infants Embracing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper -> visual_art.artwork.media -> Pistacia lentiscus\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi -> visual_art.artwork.art_subject -> Biblical Magi\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Bernardino Luini -> visual_art.visual_artist.art_forms -> Painting\n# Answer:\nBernardino Luini"], "ground_truth": ["g.1213jb_b", "Portrait of Isabella d'Este", "g.120vt1gz", "Portrait of a Musician", "The Battle of Anghiari", "g.1239jd9p", "Medusa", "g.1219sb0g", "La belle ferronni\u00e8re", "Lucan portrait of Leonardo da Vinci", "Horse and Rider", "Lady with an Ermine", "The Last Supper", "Madonna Litta", "Madonna and Child with St Joseph", "The Baptism of Christ", "Head of a Woman", "Portrait of a man in red chalk", "Virgin of the Rocks", "St. Jerome in the Wilderness", "Madonna of the Carnation", "Portrait of a Young Fianc\u00e9e", "g.121yh91r", "Adoration of the Magi", "Bacchus", "g.12215rxg", "Leonardo's horse", "St. John the Baptist", "Vitruvian Man", "g.121wt37c", "Drapery for a Seated Figure", "The Virgin and Child with St. Anne", "Mona Lisa", "g.12314dm1", "Madonna of the Yarnwinder", "Benois Madonna", "g.1224tf0c", "The Virgin and Child with St Anne and St John the Baptist", "Ginevra de' Benci", "Annunciation", "Leda and the Swan", "Madonna of Laroque", "Salvator Mundi", "Sala delle Asse", "The Holy Infants Embracing"], "ans_acc": 0.06666666666666667, "ans_hit": 1, "ans_f1": 0.1142857142857143, "ans_precission": 0.4, "ans_recall": 0.06666666666666667, "path_f1": 0.11650485436893203, "path_precision": 0.4, "path_recall": 0.06818181818181818, "path_ans_f1": 0.1142857142857143, "path_ans_precision": 0.4, "path_ans_recall": 0.06666666666666667}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.capital -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Carinthia -> location.location.partially_contains -> Coglians\n# Answer:\nCarinthia", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Burgenland -> base.aareas.schema.administrative_area.administrative_area_type -> Austrian state\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Burgenland -> common.topic.notable_types -> Administrative Division\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Carinthia -> common.topic.notable_types -> Administrative Division\n# Answer:\nCarinthia", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Carinthia -> location.location.partially_contains -> Coglians\n# Answer:\nCarinthia", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Burgenland -> base.aareas.schema.administrative_area.administrative_area_type -> Austrian state\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Burgenland -> common.topic.notable_types -> Administrative Division\n# Answer:\nBurgenland"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.risk_factor.diseases -> Anemia\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04d7nvm\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.risk_factor.diseases -> Bacterial arthritis\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A Time to Mend\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjf_c\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.symptom.symptom_of -> Human papillomavirus infection\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.risk_factor.diseases -> Major depression\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjfp1\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A season in hell\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.person.gender -> Female -> biology.hybrid_parent_gender.hybrids -> m.0blp57t\n# Answer:\nFemale"], "ground_truth": ["Cervical cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Abhiyaza\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_for -> g.12582gf_5\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_types -> City/Town/Village\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> book.book_subject.works -> Remaking Buddhism for Medieval Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Nanda\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> R\u0101hula\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> location.location.time_zones -> Nepal Time Zone\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> location.location.geolocation -> m.0wmmlzp\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> common.topic.article -> m.0hznzjg\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Tooth Relic Temple and Museum -> common.topic.webpage -> m.0gb1hxk\n# Answer:\nBuddha Tooth Relic Temple and Museum"], "ground_truth": ["Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_trwk\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> Glass\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Lightning rod -> book.book_subject.works -> Stealing God's Thunder: Benjamin Franklin's Lightning Rod and the Invention of America\n# Answer:\nLightning rod", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_ty__\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> medicine.medical_treatment.used_to_treat -> Presbyopia\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_wj2d\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.instrument.family -> Crystallophone\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.image -> A bifocal corrective eyeglasses lens\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> An Armonica\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent"], "ground_truth": ["Glass harmonica", "Bifocals", "Franklin stove", "Lightning rod"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.8181818181818182, "path_precision": 0.9, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado -> location.location.containedby -> United States, with Territories\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado -> location.location.containedby -> United States of America\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpsq_\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.us_county.hud_county_place -> Aristocrat Ranchettes\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nGreeley -> location.location.contains -> Academy of Natural Therapy -> location.location.containedby -> Colorado\n# Answer:\nAcademy of Natural Therapy"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> people.profession.specialization_of -> Musician\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> music.group_member.instruments_played -> Piano\n# Answer:\nPiano", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> Music\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> people.profession.specialization_of -> Writer\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> base.descriptive_names.names.descriptive_name -> m.01260py_\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> common.topic.image -> Pietro Metastasio\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician -> people.profession.specializations -> Composer\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> common.topic.notable_for -> g.1259l_93p\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> Arranger\n# Answer:\nComposer"], "ground_truth": ["Composer", "Librettist", "Musician"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Eschen\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.country.first_level_divisions -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Andreas Bielau\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Gamprin\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.location.partially_contains -> Alps\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Germany -> location.country.languages_spoken -> Bavarian Language\n# Answer:\nGermany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.country.first_level_divisions -> Eschen\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Andreas Diebitz\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> olympics.olympic_participating_country.athletes -> m.04dq7vv\n# Answer:\nEast Germany"], "ground_truth": ["Switzerland", "Luxembourg", "Germany", "Liechtenstein", "Belgium", "Austria", "East Germany"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 1.0, "ans_recall": 0.42857142857142855, "path_f1": 0.4337349397590361, "path_precision": 0.9, "path_recall": 0.2857142857142857, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> common.topic.notable_types -> Musical genre\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> common.topic.notable_types -> Musical genre\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Pop rock\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.recordings -> Chasing a Feeling\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music -> common.topic.subject_of -> Stephen Melton\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis -> film.film.genre -> Short Film\n# Answer:\nApotheosis", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Rock and roll\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> music.genre.subgenre -> Math rock\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Rock music\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music -> common.topic.notable_types -> Musical genre\n# Answer:\nPop music"], "ground_truth": ["Experimental music", "Pop music", "Rock music", "Experimental rock", "Blues rock", "Psychedelic rock", "Pop rock", "Art rock", "Soft rock"], "ans_acc": 0.5555555555555556, "ans_hit": 1, "ans_f1": 0.48648648648648646, "ans_precission": 0.9, "ans_recall": 0.3333333333333333, "path_f1": 0.48648648648648646, "path_precision": 0.9, "path_recall": 0.3333333333333333, "path_ans_f1": 0.6870229007633587, "path_ans_precision": 0.9, "path_ans_recall": 0.5555555555555556}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.office_holder -> Mark Udall\n# Answer:\nMark Udall", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> government.government_position_held.office_holder -> Michael Bennet\n# Answer:\nMichael Bennet", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr -> government.government_position_held.office_holder -> Wayne Allard\n# Answer:\nWayne Allard", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.legislative_sessions -> 111th United States Congress\n# Answer:\n111th United States Congress", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2010 -> common.topic.image -> Michael Bennet Official Photo\n# Answer:\nUnited States Senate election in Colorado, 2010", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.legislative_sessions -> 112th United States Congress\n# Answer:\n112th United States Congress", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.legislative_sessions -> 113th United States Congress\n# Answer:\n113th United States Congress", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nBasic title", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator"], "ground_truth": ["Michael Bennet", "Mark Udall"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Europe\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries -> location.location.contains -> Denmark\n# Answer:\nNordic countries", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Kingdom of Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Capital Region of Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> g.1hhc37psk\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Nordic countries\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Central Denmark Region\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries -> location.location.contains -> Faroe Islands\n# Answer:\nNordic countries", "# Reasoning Path:\nGreenland -> location.location.containedby -> Kingdom of Denmark -> base.aareas.schema.administrative_area.administrative_children -> Denmark\n# Answer:\nKingdom of Denmark"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_for -> g.125h4fcxl\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98115 -> location.location.geometry -> m.0588rkt\n# Answer:\n98115", "# Reasoning Path:\nSeattle -> common.topic.notable_types -> Location -> type.type.properties -> Contained by\n# Answer:\nLocation", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_types -> Postal Code\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98119 -> location.location.containedby -> United States of America\n# Answer:\n98119", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.location.contains -> 1111 Third Avenue\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98115 -> common.topic.notable_types -> Postal Code\n# Answer:\n98115", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98115 -> location.location.contains -> A Sound Garden\n# Answer:\n98115", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98119 -> freebase.valuenotation.is_reviewed -> Country\n# Answer:\n98119", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.location.contains -> 1201 Third Avenue\n# Answer:\n98101"], "ground_truth": ["98102", "98158", "98113", "98181", "98131", "98116", "98118", "98190", "98184", "98178", "98134", "98103", "98154", "98175", "98106", "98174", "98136", "98111", "98133", "98114", "98199", "98195", "98165", "98126", "98160", "98125", "98129", "98101", "98177", "98191", "98105", "98117", "98122", "98155", "98104", "98145", "98198", "98194", "98168", "98171", "98164", "98119-4114", "98107", "98144", "98170", "98132", "98121", "98124", "98119", "98148", "98138", "98109", "98108", "98185", "98139", "98188", "98161", "98166", "98115", "98127", "98112", "98146", "98141"], "ans_acc": 0.047619047619047616, "ans_hit": 1, "ans_f1": 0.09045226130653267, "ans_precission": 0.9, "ans_recall": 0.047619047619047616, "path_f1": 0.09045226130653267, "path_precision": 0.9, "path_recall": 0.047619047619047616, "path_ans_f1": 0.09045226130653267, "path_ans_precision": 0.9, "path_ans_recall": 0.047619047619047616}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Adrienne Banfield-Jones\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Robsol Pinkett, Jr.\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> common.topic.image -> Pinkett-Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> tv.tv_actor.guest_roles -> m.09nq_ss\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.parents -> Caroline Bright\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B -> music.genre.subgenre -> Quiet Storm\n# Answer:\nContemporary R&B", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> tv.tv_actor.guest_roles -> m.09nq_sy\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.parents -> Willard Christopher Smith, Sr.\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> tv.tv_actor.guest_roles -> m.09nbq82\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> tv.tv_actor.guest_roles -> m.09nq_t2\n# Answer:\nJada Pinkett Smith"], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_types -> Language Writing System\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_for -> g.1258512fl\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> N\u00fcshu script -> common.topic.article -> m.014lbq\n# Answer:\nN\u00fcshu script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> language.language_writing_system.languages -> Mongolian language\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> language.language_writing_system.languages -> Chinese, Hakka Language\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> book.book_subject.works -> Flags of Our Fathers\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> location.location.contains -> Taiwan\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> common.topic.notable_for -> g.1259bftrt\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> base.schemastaging.context_name.pronunciation -> g.125_l82wv\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> education.field_of_study.students_majoring -> m.0104b7h1 -> education.education.degree -> PhD\n# Answer:\nPhD"], "ground_truth": ["'Phags-pa script", "N\u00fcshu script", "Chinese characters", "Traditional Chinese characters", "Simplified Chinese character"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.6, "ans_recall": 0.6, "path_f1": 0.6, "path_precision": 0.6, "path_recall": 0.6, "path_ans_f1": 0.6, "path_ans_precision": 0.6, "path_ans_recall": 0.6}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.spouse_s -> m.0j4ks8g\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.person.spouse_s -> m.0j4ks8g\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.spouse -> Pat Nixon\n# Answer:\nPat Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Arthur Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> freebase.valuenotation.has_value -> Place of death\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.location_of_ceremony -> The Mission Inn Hotel & Spa\n# Answer:\nThe Mission Inn Hotel & Spa", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Donald Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.person.parents -> Samuel Brady Nixon\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Edward Nixon\n# Answer:\nHannah Milhous Nixon"], "ground_truth": ["Pat Nixon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.11764705882352941, "path_precision": 0.1, "path_recall": 0.14285714285714285, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Supporting Actress in a Comedy Series\n# Answer:\nPrimetime Emmy Award for Outstanding Supporting Actress in a Comedy Series", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.award -> NAACP Image Award for Outstanding Actress in a Comedy Series\n# Answer:\nNAACP Image Award for Outstanding Actress in a Comedy Series", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.ceremony -> 37th Primetime Emmy Awards\n# Answer:\n37th Primetime Emmy Awards", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Supporting Actress in a Comedy Series\n# Answer:\nPrimetime Emmy Award for Outstanding Supporting Actress in a Comedy Series", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.ceremony -> 35th Primetime Emmy Awards\n# Answer:\n35th Primetime Emmy Awards", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.ceremony -> 16th NAACP Image Awards\n# Answer:\n16th NAACP Image Awards", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0_yczwx -> award.award_honor.award_winner -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 0 -> common.topic.notable_for -> g.1yl5pbtsv\n# Answer:\nThe Jeffersons - Season 0"], "ground_truth": ["Damon Evans", "Marla Gibbs", "Berlinda Tolbert", "Sherman Hemsley", "Paul Benedict", "Roxie Roker", "Mike Evans", "Isabel Sanford", "Franklin Cover", "Jay Hammer", "Zara Cully"], "ans_acc": 0.09090909090909091, "ans_hit": 1, "ans_f1": 0.13953488372093023, "ans_precission": 0.3, "ans_recall": 0.09090909090909091, "path_f1": 0.08695652173913043, "path_precision": 0.3, "path_recall": 0.05084745762711865, "path_ans_f1": 0.13953488372093023, "path_ans_precision": 0.3, "path_ans_recall": 0.09090909090909091}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Times -> book.periodical.frequency_or_issues_per_year -> m.09s58j6\n# Answer:\nSan Francisco Bay Times", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Times -> common.topic.notable_for -> g.125d1300_\n# Answer:\nSan Francisco Bay Times", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Times -> common.topic.webpage -> m.03kz_rr\n# Answer:\nSan Francisco Bay Times", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Street Sheet -> book.periodical.format -> m.02npbt7\n# Answer:\nStreet Sheet", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.periodical.first_issue_date -> m.02npn31\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.publication.contents -> m.0znkynv\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Street Sheet -> book.newspaper.owner -> Coalition on Homelessness\n# Answer:\nStreet Sheet", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94101 -> location.postal_code.country -> United States of America\n# Answer:\n94101", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> book.written_work.subjects -> California\n# Answer:\n1906", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> common.topic.notable_for -> g.1258179ql\n# Answer:\n1906"], "ground_truth": ["Synapse", "San Francisco Business Times", "San Francisco Chronicle", "Sing Tao Daily", "San Francisco Call", "Bay Area Reporter", "San Francisco Bay View", "San Francisco Bay Times", "San Francisco Foghorn", "The Daily Alta California", "California Star", "Free Society", "The Golden Era", "Street Sheet", "San Francisco Daily", "The San Francisco Examiner", "Dock of the Bay", "San Francisco News-Call Bulletin Newspaper", "AsianWeek", "San Francisco Bay Guardian"], "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0.24705882352941178, "ans_precission": 0.7, "ans_recall": 0.15, "path_f1": 0.24705882352941178, "path_precision": 0.7, "path_recall": 0.15, "path_ans_f1": 0.24705882352941178, "path_ans_precision": 0.7, "path_ans_recall": 0.15}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.partially_contains -> Didi Abuli\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Denmark\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> base.locations.continents.countries_within -> Denmark\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> location.location.partially_contains -> Didi Abuli\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.containedby -> Eurasia -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurasia", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Georgia\n# Answer:\nEurope"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w0hpl\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w4lwz\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> ACE inhibitor\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Epilepsy\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09wjtbj\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Diabetes mellitus\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> Acebutolol\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> heart attack\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> Anisindione\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Family History of Ischaemic Heart disease\n# Answer:\nheart attack"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 11: 1863\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 12: 1864\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin -> book.author.works_written -> The Autobiography of Charles Darwin\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Anne Darwin -> people.person.parents -> Emma Darwin\n# Answer:\nAnne Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nEngland"], "ground_truth": ["From Darwin's unpublished notebooks", "On the Movements and Habits of Climbing Plants", "vari\u00eberen der huisdieren en cultuurplanten", "Charles Darwin's natural selection", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "The portable Darwin", "The Correspondence of Charles Darwin, Volume 17: 1869", "Part I: Contributions to the Theory of Natural Selection / Part II", "Del Plata a Tierra del Fuego", "South American Geology", "Darwin-Wallace", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Reise eines Naturforschers um die Welt", "Geological Observations on the Volcanic Islands", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "The Orgin of Species", "The foundations of the Origin of species", "On Natural Selection", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Notes on the fertilization of orchids", "To the members of the Down Friendly Club", "The Correspondence of Charles Darwin, Volume 14: 1866", "Evolution by natural selection", "genese\u014ds t\u014dn eid\u014dn", "The Structure and Distribution of Coral Reefs", "From so simple a beginning", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "Volcanic Islands", "The geology of the voyage of H.M.S. Beagle", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Motsa ha-minim", "Evolution", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "A student's introduction to Charles Darwin", "Diary of the voyage of H.M.S. Beagle", "Die geschlechtliche Zuchtwahl", "Kleinere geologische Abhandlungen", "La vie et la correspondance de Charles Darwin", "More Letters of Charles Darwin", "Charles Darwin on the routes of male humble bees", "The education of Darwin", "Darwin and Henslow", "Reise um die Welt 1831 - 36", "Darwin's insects", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Gesammelte kleinere Schriften", "Darwin Darwin", "The living thoughts of Darwin", "The Correspondence of Charles Darwin, Volume 8: 1860", "Monographs of the fossil Lepadidae and the fossil Balanidae", "Resa kring jorden", "The Formation of Vegetable Mould through the Action of Worms", "Wu zhong qi yuan", "The Power of Movement in Plants", "Charles Darwin's marginalia", "On the tendency of species to form varieties", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "The Correspondence of Charles Darwin, Volume 13: 1865", "Darwin en Patagonia", "Diario del Viaje de Un Naturalista Alrededor", "Darwin", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The Voyage of the Beagle", "Tesakneri tsagume\u030c", "Origins", "Het uitdrukken van emoties bij mens en dier", "Evolutionary Writings: Including the Autobiographies", "Metaphysics, Materialism, & the evolution of mind", "Voyage d'un naturaliste autour du monde", "Die fundamente zur entstehung der arten", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "The Autobiography of Charles Darwin", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Proiskhozhdenie vidov", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Darwin's Ornithological notes", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "Works", "Geological Observations on South America", "Darwin from Insectivorous Plants to Worms", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Leben und Briefe von Charles Darwin", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The Essential Darwin", "Darwin on humus and the earthworm", "Beagle letters", "The Life and Letters of Charles Darwin Volume 2", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Cartas de Darwin 18251859", "The Correspondence of Charles Darwin, Volume 10: 1862", "Darwin for Today", "The Correspondence of Charles Darwin, Volume 15: 1867", "The Different Forms of Flowers on Plants of the Same Species", "Insectivorous Plants", "The Darwin Reader First Edition", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Charles Darwin's letters", "red notebook of Charles Darwin", "H.M.S. Beagle in South America", "The Life of Erasmus Darwin", "Rejse om jorden", "Charles Darwin", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Opsht\u0323amung fun menshen", "The\u0301orie de l'e\u0301volution", "Darwin's journal", "A Darwin Selection", "Les moyens d'expression chez les animaux", "Memorias y epistolario i\u0301ntimo", "Darwinism stated by Darwin himself", "The collected papers of Charles Darwin", "The Correspondence of Charles Darwin, Volume 9: 1861", "The Correspondence of Charles Darwin, Volume 16: 1868", "Questions about the breeding of animals", "Darwin's notebooks on transmutation of species", "monograph on the sub-class Cirripedia", "Human nature, Darwin's view", "The Correspondence of Charles Darwin, Volume 18: 1870", "La facult\u00e9 motrice dans les plantes", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "ontstaan der soorten door natuurlijke teeltkeus", "The principal works", "On the origin of species by means of natural selection", "The Expression of the Emotions in Man and Animals", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "The voyage of Charles Darwin", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The Darwin Reader Second Edition", "Les mouvements et les habitudes des plantes grimpantes", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "On evolution", "The action of carbonate of ammonia on the roots of certain plants", "Darwin Compendium", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Fertilisation of Orchids", "On a remarkable bar of sandstone off Pernambuco", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "The Life and Letters of Charles Darwin Volume 1", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Correspondence of Charles Darwin, Volume 12: 1864", "The Descent of Man, and Selection in Relation to Sex", "The Correspondence of Charles Darwin, Volume 11: 1863", "Evolution and natural selection", "Notebooks on transmutation of species", "The Variation of Animals and Plants under Domestication", "El Origin De Las Especies"], "ans_acc": 0.05228758169934641, "ans_hit": 1, "ans_f1": 0.025586353944562903, "ans_precission": 0.6, "ans_recall": 0.013071895424836602, "path_f1": 0.3636363636363636, "path_precision": 1.0, "path_recall": 0.2222222222222222, "path_ans_f1": 0.09937888198757763, "path_ans_precision": 1.0, "path_ans_recall": 0.05228758169934641}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> location.administrative_division.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.film -> End of the Road: How Money Became Worthless\n# Answer:\nEnd of the Road: How Money Became Worthless", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15 -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.0_0cs2_ -> film.personal_film_appearance.film -> Our Nixon\n# Answer:\nOur Nixon", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsgl5 -> tv.tv_guest_role.episodes_appeared_in -> The Future of the GOP\n# Answer:\nThe Future of the GOP", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglb -> tv.tv_guest_role.episodes_appeared_in -> The American Film Institute Salute to James Cagney\n# Answer:\nThe American Film Institute Salute to James Cagney", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15 -> film.personal_film_appearance.film -> Sicko\n# Answer:\nSicko", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglh -> tv.tv_guest_role.episodes_appeared_in -> Johnny Grant\n# Answer:\nJohnny Grant"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.size -> m.0kjrkq\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.appears_in_topic_gallery -> Grammy Legend Award\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.appears_in_topic_gallery -> Zubin Mehta\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> Little Miss Sweetness -> music.recording.releases -> Gettin' Ready\n# Answer:\nLittle Miss Sweetness", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.size -> m.02cljr8\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.subjects -> Bleona\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Music executive -> common.topic.article -> m.047rgq1\n# Answer:\nMusic executive", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Musician -> freebase.equivalent_topic.equivalent_type -> Musical Artist\n# Answer:\nMusician", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> I've Been Good to You -> music.composition.recordings -> I've Been Good To You\n# Answer:\nI've Been Good to You"], "ground_truth": ["Tears of a Sweet Free Clown", "You've Really Got a Hold on Me", "Love Letters", "Get Ready", "One Heartbeat", "Aqui Con Tigo (Being With You)", "Wanna Know My Mind", "Sweet Harmony", "Tea for Two", "Heavy On Pride (Light On Love)", "Hanging on by a Thread", "Little Girl Little Girl", "Easy", "The Agony and the Ecstasy", "My Guy", "Bad Girl", "If You Want My Love", "Pops, We Love You", "I Second That Emotions", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "Just Like You", "Come to Me Soon", "Nearness of You", "Noel", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "My Girl", "You Are Forever", "If You Can Want", "Train of Thought", "It's Christmas Time", "Will You Love Me Tomorrow", "Love So Fine", "No\u00ebl", "Cruisin'", "Be Kind to the Growing Mind", "Tell Me Tomorrow, Part 1", "Holly", "Driving Thru Life in the Fast Lane", "Be Who You Are", "Away in the Manger / Coventry Carol", "It's a Good Feeling", "Tell Me Tomorrow (12\\\" extended mix)", "More Love", "Let Me Be the Clock", "I Know You by Heart", "We\u2019ve Come Too Far to End It Now", "Fallin'", "I've Made Love To You A Thousand Times", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "Girlfriend", "Going to a Go-Go", "Be Careful What You Wish For (instrumental)", "The Hurt's On You", "Crusin", "Just Passing Through", "I Want You Back", "It's Fantastic", "That Place", "If You Wanna Make Love (Come 'round Here)", "And I Don't Love You (Larry Levan instrumental dub)", "Whatcha Gonna Do", "Shop Around", "Don't Play Another Love Song", "I've Got You Under My Skin", "I Can't Give You Anything but Love", "Blame It On Love (Duet with Barbara Mitchell)", "Yes It's You Lady", "Share It", "Why", "Rack Me Back", "(It's The) Same Old Love", "The Family Song", "Gang Bangin'", "Just a Touch Away", "Food For Thought", "Really Gonna Miss You", "We've Saved the Best for Last", "Unless You Do It Again", "What's Too Much", "Tears Of A Clown", "I Am, I Am", "Christmas Every Day", "I Can't Get Enough", "Te Quiero Como Si No Hubiera Un Manana", "Double Good Everything", "Jingle Bells", "Daylight & Darkness", "Going to a Gogo", "Will You Love Me Tomorrow?", "Time Flies", "More Than You Know", "A Silent Partner in a Three-Way Love Affair", "The Tracks of My Tears", "I Can\u2019t Stand to See You Cry (Commercial version)", "Jasmin", "Keep Me", "Some People Will Do Anything for Love", "Theme From the Big Time", "There Will Come a Day (I'm Gonna Happen to You)", "Walk on By", "I\u2019ve Got You Under My Skin", "Love Bath", "Happy (Love Theme From Lady Sings the Blues)", "The Tears of a Clown", "Love Is The Light", "Why Do Happy Memories Hurt So Bad", "You Made Me Feel Love", "I Have Prayed On It", "The Road to Damascus", "A Child Is Waiting", "Fulfill Your Need", "Ooo Baby Baby (live)", "Same Old Love", "Vitamin U", "She's Only a Baby Herself", "Be Careful What You Wish For", "With Your Love Came", "Crusin'", "Quiet Storm (Groove Boutique remix)", "I Can't Find", "Mickey's Monkey", "Ebony Eyes (Duet with Rick James)", "Ooh Baby Baby", "When A Woman Cries", "Tears of a Clown", "The Christmas Song", "Tracks Of My Tears (Live)", "Christmas Greeting", "Never My Love / Never Can Say Goodbye", "A Tattoo", "Wishful Thinking", "Being With You", "Love' n Life", "Sleepless Nights", "Take Me Through The Night", "Can't Fight Love", "I'm in the Mood for Love", "You've Really Go a Hold on Me", "I Praise & Worship You Father", "Please Don't Take Your Love (feat. Carlos Santana)", "Open", "The Tracks Of My Tears", "You Are So Beautiful (feat. Dave Koz)", "Santa Claus is Coming to Town", "I Love The Nearness Of You", "Will You Still Love Me Tomorrow", "There Will Come A Day ( I'm Gonna Happen To You )", "The Tracks of My Tears (live)", "I Like Your Face", "You Go to My Head", "Satisfy You", "Ain't That Peculiar", "Cruisin", "And I Don't Love You", "As You Do", "Quiet Storm", "One Time", "The Agony And The Ecstasy", "Who's Sad", "Blame It on Love", "Save Me", "Little Girl, Little Girl", "Let Me Be The Clock", "Don't Wanna Be Just Physical", "Photograph in My Mind", "Love Don' Give No Reason (12 Inch Club Mix)", "Baby Come Close", "I've Made Love to You a Thousand Times", "In My Corner", "I Love Your Face", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "And I Love Her", "Time After Time", "Be Kind To The Growing Mind (with The Temptations)", "He Can Fix Anything", "Tell Me Tomorrow", "Ebony Eyes", "You Don't Know What It's Like", "Just To See Her Again", "You're the One for Me (feat. Joss Stone)", "The Way You Do (The Things You Do)", "I Am I Am", "Asleep on My Love", "When Smokey Sings Tears Of A Clown", "Come by Here (Kum Ba Ya)", "Going to a Go Go", "Skid Row", "Hold on to Your Love", "It's Time to Stop Shoppin' Around", "Our Love Is Here to Stay", "Mother's Son", "I Care About Detroit", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "Just Another Kiss", "You Cannot Laugh Alone", "The Track of My Tears", "Season's Greetings from Smokey Robinson", "Love Don't Give No Reason", "If You Wanna Make Love", "It's Her Turn to Live", "God Rest Ye Merry Gentlemen", "Speak Low", "I'll Keep My Light In My Window", "Quiet Storm (Groove Boutique Chill Jazz mix)", "Christmas Everyday", "The Love Between Me and My Kids", "Ooo Baby Baby", "Baby That's Backatcha", "You Take Me Away", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "Shoe Soul", "Close Encounters of the First Kind", "Gone Forever", "The Tears Of A Clown", "Just to See Her", "Tracks of my Tears", "So Bad", "My World", "Just My Soul Responding", "Wedding Song", "We Are The Warriors", "Virgin Man", "Everything You Touch", "Girl I'm Standing There", "Let Your Light Shine On Me", "Love Brought Us Here", "Melody Man", "Everything for Christmas", "We've Saved The Best For Last (Kenny G with Smokey Robinson)", "You're Just My Life (feat. India.Arie)", "Tracks of My Tears", "It's A Good Night", "Night and Day", "Please Come Home for Christmas", "Did You Know (Berry's Theme)", "I'm Glad There Is You", "Pops, We Love You (disco)", "Jesus Told Me To Love You", "Medley: Never My Love / Never Can Say Goodbye", "Winter Wonderland", "Quiet Storm (single version)", "Coincidentally", "You Really Got a Hold on Me", "I Second That Emotion", "The Tracks of My Heart", "Ever Had A Dream", "No Time to Stop Believing", "Standing On Jesus", "I Hear The Children Singing", "Yester Love", "Deck the Halls", "Fly Me to the Moon (In Other Words)", "Because of You It's the Best It's Ever Been", "Why Are You Running From My Love", "Don't Know Why", "Rewind"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> base.americancomedy.impersonated_celebrity.impersonated_by -> Rich Little\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> Artwork -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nArtwork", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> Artwork -> freebase.type_hints.included_types -> Topic\n# Answer:\nArtwork", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of death\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> Artwork -> freebase.type_profile.published -> Published\n# Answer:\nArtwork", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> US President -> type.type.properties -> Vice president\n# Answer:\nUS President", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> US President -> type.type.expected_by -> President\n# Answer:\nUS President"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> South Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> common.topic.notable_types -> Human Language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Bosnia and Herzegovina\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> sports.multi_event_tournament.participating_countries -> United States of America\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Croatia\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> common.topic.article -> m.0l8p56r\n# Answer:\n2013 FIBA Under-19 World Championship"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
