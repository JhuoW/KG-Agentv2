{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Costa Rica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.region -> Americas\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Louisiana\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Mississippi\n# Answer:\n1812 Louisiana hurricane"], "ground_truth": ["Jamaican Creole English Language", "Jamaican English"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nUnited States Representative", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.district_represented -> Tennessee's 9th congressional district\n# Answer:\nTennessee's 9th congressional district", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8 -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> United States Representative\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j5sk8 -> government.government_position_held.office_position_or_title -> Governor of Tennessee\n# Answer:\nGovernor of Tennessee", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Representative\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> President of the United States\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Arkansas\n# Answer:\nUnited States of America"], "ground_truth": ["Speaker of the United States House of Representatives", "Governor of Tennessee", "United States Representative"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.30769230769230765, "ans_precission": 0.2, "ans_recall": 0.6666666666666666, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5, "path_ans_precision": 0.4, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole -> language.human_language.countries_spoken_in -> Dominican Republic\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.countries_spoken_in -> Dominican Republic\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole -> language.human_language.countries_spoken_in -> Cuba\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole -> language.human_language.main_country -> Puerto Rico\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French -> language.human_language.countries_spoken_in -> Martinique\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole -> language.human_language.countries_spoken_in -> Turks and Caicos Islands\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole -> language.human_language.region -> Americas\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.countries_spoken_in -> Cuba\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.main_country -> Puerto Rico\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> Martinique\n# Answer:\nFrench"], "ground_truth": ["Haitian Creole", "French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.0wyc2dy -> tv.regular_tv_appearance.actor -> Danielle Judovits\n# Answer:\nDanielle Judovits", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t8hv2 -> tv.regular_tv_appearance.actor -> Yvonne Craig\n# Answer:\nYvonne Craig", "# Reasoning Path:\nBarbara Gordon -> common.topic.notable_for -> g.12596ymdk\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.0wyc2dy -> tv.regular_tv_appearance.seasons -> The Batman - Season 5\n# Answer:\nThe Batman - Season 5", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t91b7 -> tv.regular_tv_appearance.actor -> Melissa Gilbert\n# Answer:\nMelissa Gilbert", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.0wyc2dy -> tv.regular_tv_appearance.seasons -> The Batman - Season 4\n# Answer:\nThe Batman - Season 4", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Politician -> common.topic.subjects -> Akinwunmi Ambode\n# Answer:\nPolitician", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> film.film_character.portrayed_in_films -> m.012hbb3h\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t91b7 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Politician -> common.topic.subjects -> Nathalie Kosciusko-Morizet\n# Answer:\nPolitician"], "ground_truth": ["Hannah Gunn", "Ilyssa Fradin", "Melinda McGraw"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qstvs -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qstvs -> basketball.basketball_player_stats.season -> 2002\u201303 NBA season\n# Answer:\n2002\u201303 NBA season", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.04ynxq7 -> sports.sports_award.award -> NBA All-Star Game Most Valuable Player Award\n# Answer:\nNBA All-Star Game Most Valuable Player Award", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.season -> 1992\u201393 NBA season\n# Answer:\n1992\u201393 NBA season", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player -> common.topic.notable_for -> g.125ctmqvt\n# Answer:\nPlayer", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.04ynxq7 -> sports.sports_award.season -> 2000\u201301 NBA season\n# Answer:\n2000\u201301 NBA season", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.season -> 2003\u201304 NBA season\n# Answer:\n2003\u201304 NBA season"], "ground_truth": ["Phoenix Suns", "Los Angeles Lakers", "Boston Celtics", "Cleveland Cavaliers", "LSU Tigers men's basketball", "Miami Heat", "Orlando Magic"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.3333333333333333, "ans_precission": 0.4, "ans_recall": 0.2857142857142857, "path_f1": 0.2, "path_precision": 0.4, "path_recall": 0.13333333333333333, "path_ans_f1": 0.3333333333333333, "path_ans_precision": 0.4, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> travel.travel_destination.tourist_attractions -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> location.location.contains -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9 -> award.award_honor.award -> Emmy Award for Outstanding Variety, Music or Comedy Series\n# Answer:\nEmmy Award for Outstanding Variety, Music or Comedy Series", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.image -> William paca\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.04lybjz\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> people.ethnicity.languages_spoken -> American English\n# Answer:\nScottish American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9 -> award.award_honor.honored_for -> The Tonight Show with Jay Leno\n# Answer:\nThe Tonight Show with Jay Leno", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.0n2qyfk -> award.award_honor.award -> People's Choice Award for Favorite Late Night Talk Show Host\n# Answer:\nPeople's Choice Award for Favorite Late Night Talk Show Host", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.0cqc3ql -> award.award_honor.award -> Primetime Emmy Award for Outstanding Variety Series - Musical\n# Answer:\nPrimetime Emmy Award for Outstanding Variety Series - Musical"], "ground_truth": ["New Rochelle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> location.country.languages_spoken -> Thai Language -> media_common.netflix_genre.titles -> Shutter\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Thai Language -> language.human_language.region -> Asia\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> media_common.netflix_genre.titles -> Shutter\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Thai Language -> common.topic.notable_types -> Human Language\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language -> common.topic.notable_types -> Human Language\n# Answer:\nVietnamese Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> language.human_language.region -> Asia\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yg\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language -> language.human_language.main_country -> Vietnam\n# Answer:\nVietnamese Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.main_country -> Myanmar\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> common.topic.notable_types -> Human Language\n# Answer:\nThai Language"], "ground_truth": ["Cham language", "Hmong language", "Phu Thai language", "Malay, Pattani Language", "Vietnamese Language", "Nyaw Language", "Khmer language", "Lao Language", "Mlabri Language", "Mon Language", "Saek language", "Thai Language", "Akha Language"], "ans_acc": 0.23076923076923078, "ans_hit": 1, "ans_f1": 0.3673469387755102, "ans_precission": 0.9, "ans_recall": 0.23076923076923078, "path_f1": 0.33333333333333337, "path_precision": 0.6, "path_recall": 0.23076923076923078, "path_ans_f1": 0.3673469387755102, "path_ans_precision": 0.9, "path_ans_recall": 0.23076923076923078}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Programmer\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Entrepreneur\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.parents -> Edward Zuckerberg\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Businessperson\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.owner -> Mark Zuckerberg\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.parents -> Karen Kempner\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> influence.influence_node.influenced -> Austin Cameron\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Social networking service -> common.topic.image -> Soc-net-paten-growth-chart\n# Answer:\nSocial networking service", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Social networking service -> common.topic.subject_of -> Krishna Mali\n# Answer:\nSocial networking service", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Social networking service -> computer.software_genre.software_in_genre -> 2go\n# Answer:\nSocial networking service"], "ground_truth": ["Cameron Winklevoss", "Tyler Winklevoss"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Statesman -> common.topic.notable_types -> Profession\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> common.topic.notable_types -> Profession\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Polly Jefferson\n# Answer:\nPolly Jefferson", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> fictional_universe.character_occupation.characters_with_this_occupation -> Indiana Jones\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Statesman -> people.profession.specialization_of -> Politician\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> people.profession.specialization_of -> Scientist\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Architect -> common.topic.notable_types -> Profession\n# Answer:\nArchitect", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Politician -> freebase.type_hints.included_types -> Topic\n# Answer:\nPolitician"], "ground_truth": ["Statesman", "Writer", "Inventor", "Architect", "Archaeologist", "Philosopher", "Lawyer", "Farmer", "Teacher", "Author"], "ans_acc": 0.3, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.6, "ans_recall": 0.3, "path_f1": 0.26666666666666666, "path_precision": 0.4, "path_recall": 0.2, "path_ans_f1": 0.4, "path_ans_precision": 0.6, "path_ans_recall": 0.3}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 3: 1844-1846\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin -> book.author.works_written -> The Autobiography of Charles Darwin\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Great Britain -> location.location.contains -> London\n# Answer:\nGreat Britain", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Anne Darwin -> people.person.parents -> Emma Darwin\n# Answer:\nAnne Darwin"], "ground_truth": ["The Correspondence of Charles Darwin, Volume 18: 1870", "The Expression Of The Emotions In Man And Animals", "The Correspondence of Charles Darwin, Volume 1", "Human nature, Darwin's view", "The foundations of the Origin of species", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The education of Darwin", "The Correspondence of Charles Darwin, Volume 15: 1867", "The Correspondence of Charles Darwin, Volume 8: 1860", "Voyage Of The Beagle", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Insectivorous Plants", "The Autobiography of Charles Darwin [EasyRead Edition]", "The Different Forms of Flowers on Plants of the Same Species", "Motsa ha-minim", "Voyage of the Beagle (Dover Value Editions)", "Darwinism stated by Darwin himself", "Opsht\u0323amung fun menshen", "A Darwin Selection", "Darwin on humus and the earthworm", "The Variation of Animals and Plants under Domestication", "Geological Observations on South America", "Leben und Briefe von Charles Darwin", "The Correspondence of Charles Darwin, Volume 9: 1861", "Charles Darwin's natural selection", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Gesammelte kleinere Schriften", "Darwin en Patagonia", "The expression of the emotions in man and animals", "Charles Darwin", "Charles Darwin on the routes of male humble bees", "Darwin for Today", "Evolution by natural selection", "The descent of man and selection in relation to sex.", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "The\u0301orie de l'e\u0301volution", "From so simple a beginning", "The Correspondence of Charles Darwin, Volume 12: 1864", "Voyage of the Beagle", "Darwin's notebooks on transmutation of species", "Questions about the breeding of animals", "Het uitdrukken van emoties bij mens en dier", "The Autobiography Of Charles Darwin", "The Formation of Vegetable Mould through the Action of Worms", "The Correspondence of Charles Darwin, Volume 6", "The Correspondence of Charles Darwin, Volume 3", "The Voyage of the Beagle (Adventure Classics)", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "The Origin of Species (Enriched Classics)", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "vari\u00eberen der huisdieren en cultuurplanten", "On a remarkable bar of sandstone off Pernambuco", "The Correspondence of Charles Darwin, Volume 10: 1862", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Les r\u00e9cifs de corail, leur structure et leur distribution", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "The autobiography of Charles Darwin, 1809-1882", "The Autobiography of Charles Darwin (Great Minds Series)", "The Correspondence of Charles Darwin, Volume 14: 1866", "Tesakneri tsagume\u030c", "The Correspondence of Charles Darwin, Volume 17: 1869", "The Correspondence of Charles Darwin, Volume 7", "The Origin of Species (Great Books : Learning Channel)", "The geology of the voyage of H.M.S. Beagle", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "A student's introduction to Charles Darwin", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "The Autobiography of Charles Darwin (Large Print)", "Works", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Voyage d'un naturaliste autour du monde", "The Correspondence of Charles Darwin, Volume 12", "Diario del Viaje de Un Naturalista Alrededor", "Darwin", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The Autobiography of Charles Darwin (Dodo Press)", "The portable Darwin", "ontstaan der soorten door natuurlijke teeltkeus", "The Voyage of the Beagle (Unabridged Classics)", "The Correspondence of Charles Darwin, Volume 2", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "Origin of Species (Harvard Classics, Part 11)", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "The Autobiography of Charles Darwin", "The Voyage of the Beagle (Everyman Paperbacks)", "Metaphysics, Materialism, & the evolution of mind", "Darwin's Ornithological notes", "monograph on the sub-class Cirripedia", "More Letters of Charles Darwin", "On the tendency of species to form varieties", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "Les mouvements et les habitudes des plantes grimpantes", "The Darwin Reader Second Edition", "Rejse om jorden", "The Correspondence of Charles Darwin, Volume 16: 1868", "The Power of Movement in Plants", "Part I: Contributions to the Theory of Natural Selection / Part II", "Voyage of the Beagle (Harvard Classics, Part 29)", "Cartas de Darwin 18251859", "The Origin of Species (Great Minds Series)", "The Correspondence of Charles Darwin, Volume 4", "Darwin Darwin", "Wu zhong qi yuan", "The Origin of Species (Oxford World's Classics)", "The Origin of Species (World's Classics)", "The Autobiography of Charles Darwin, and selected letters", "La facult\u00e9 motrice dans les plantes", "The Correspondence of Charles Darwin, Volume 11: 1863", "The Correspondence of Charles Darwin, Volume 5", "The Descent of Man and Selection in Relation to Sex", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The descent of man, and selection in relation to sex.", "La vie et la correspondance de Charles Darwin", "Fertilisation of Orchids", "The living thoughts of Darwin", "El Origin De Las Especies", "Die fundamente zur entstehung der arten", "The autobiography of Charles Darwin", "The Descent of Man, and Selection in Relation to Sex", "Del Plata a Tierra del Fuego", "Origin of Species (Everyman's University Paperbacks)", "The Origin of Species (Collector's Library)", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "The Correspondence of Charles Darwin, Volume 10", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Reise um die Welt 1831 - 36", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The Correspondence of Charles Darwin, Volume 13", "From Darwin's unpublished notebooks", "red notebook of Charles Darwin", "The Correspondence of Charles Darwin, Volume 15", "The Structure And Distribution of Coral Reefs", "Origins", "The action of carbonate of ammonia on the roots of certain plants", "The descent of man, and selection in relation to sex", "The Origin of Species", "The voyage of Charles Darwin", "Die geschlechtliche Zuchtwahl", "The structure and distribution of coral reefs", "The Correspondence of Charles Darwin, Volume 9", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Proiskhozhdenie vidov", "On Natural Selection", "Les moyens d'expression chez les animaux", "The Essential Darwin", "The Correspondence of Charles Darwin, Volume 13: 1865", "Charles Darwin's marginalia", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Darwin's insects", "Beagle letters", "To the members of the Down Friendly Club", "Darwin's journal", "Diary of the voyage of H.M.S. Beagle", "The Correspondence of Charles Darwin, Volume 11", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "The origin of species : complete and fully illustrated", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The expression of the emotions in man and animals.", "Evolution and natural selection", "Notebooks on transmutation of species", "The voyage of the Beagle.", "Darwin Compendium", "The collected papers of Charles Darwin", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "The Origin of Species (Mentor)", "H.M.S. Beagle in South America", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "The Darwin Reader First Edition", "On evolution", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "The Correspondence of Charles Darwin, Volume 14", "genese\u014ds t\u014dn eid\u014dn", "Voyage of the Beagle (NG Adventure Classics)", "Darwin and Henslow", "The Expression of the Emotions in Man And Animals", "The Origin Of Species", "Resa kring jorden", "On the origin of species by means of natural selection", "The Expression of the Emotions in Man and Animals", "The Structure and Distribution of Coral Reefs", "The origin of species", "The Correspondence of Charles Darwin, Volume 8", "Reise eines Naturforschers um die Welt", "Kleinere geologische Abhandlungen", "The Orgin of Species", "The structure and distribution of coral reefs.", "Memorias y epistolario i\u0301ntimo", "Evolution", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "The principal works", "On the Movements and Habits of Climbing Plants", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "The Voyage of the Beagle (Great Minds Series)", "Origin of Species", "The Life of Erasmus Darwin", "Volcanic Islands", "Charles Darwin's letters", "Autobiography of Charles Darwin", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "The Voyage of the Beagle (Mentor)", "Notes on the fertilization of orchids", "Darwin-Wallace", "The Voyage of the Beagle", "The Origin of Species (Variorum Reprint)", "From So Simple a Beginning"], "ans_acc": 0.08411214953271028, "ans_hit": 1, "ans_f1": 0.01840490797546012, "ans_precission": 0.6, "ans_recall": 0.009345794392523364, "path_f1": 0.4324324324324324, "path_precision": 1.0, "path_recall": 0.27586206896551724, "path_ans_f1": 0.15517241379310345, "path_ans_precision": 1.0, "path_ans_recall": 0.08411214953271028}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0vxk_df -> sports.sports_team_roster.team -> New England Patriots\n# Answer:\nNew England Patriots", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.team -> New York Jets\n# Answer:\nNew York Jets", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> book.written_work.author -> Nathan Whitaker\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0vxk_df -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nTim Tebow -> award.award_nominee.award_nominations -> m.0z87f99 -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.notable_types -> Book\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.article -> m.0j4d5g4\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> award.award_nominee.award_nominations -> m.0z87f99 -> award.award_nomination.ceremony -> 2012 Teen Choice Awards\n# Answer:\n2012 Teen Choice Awards"], "ground_truth": ["Florida Gators football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.09tcfsk -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.09tcfsk -> american_football.player_rushing_statistics.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.0791773 -> american_football.player_rushing_statistics.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.07mmh5w -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.0791773 -> american_football.player_rushing_statistics.season -> 2008 NFL season\n# Answer:\n2008 NFL season", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Sports played -> type.property.schema -> Athlete\n# Answer:\nSports played"], "ground_truth": ["Denver Broncos"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Eastern European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> United Kingdom\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Slovakia -> location.location.containedby -> Eurasia\n# Answer:\nSlovakia", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Ukraine -> location.location.containedby -> Eurasia\n# Answer:\nUkraine", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.partially_contains -> Russia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Slovakia -> base.locations.countries.continent -> Europe\n# Answer:\nSlovakia", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Slovakia -> base.uncommon.topic.exceptions -> City/Town/Village should pertain to Ghana, since City/Town/Village subdivides Ghanaian Municipal District.\n# Answer:\nSlovakia", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Ukraine -> base.locations.countries.continent -> Europe\n# Answer:\nUkraine", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Czech Republic -> location.location.containedby -> Eurasia\n# Answer:\nCzech Republic"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.notable_types -> Profession\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.article -> m.0knj9\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.image -> The Bard\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Bard\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Poet\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Cardiovascular disease\n# Answer:\nMale", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> book.book_subject.works -> Authors Frequently Mentioned on the Web\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.profession -> Writer\n# Answer:\nAnne Bront\u00eb", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Bob Dylan -> people.person.profession -> Writer\n# Answer:\nBob Dylan"], "ground_truth": ["Poet", "Writer", "Bard", "Author"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5833333333333334, "ans_precission": 0.7, "ans_recall": 0.5, "path_f1": 0.5833333333333334, "path_precision": 0.7, "path_recall": 0.5, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.actor -> Hayden Christensen\n# Answer:\nHayden Christensen", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.actor -> Hayden Christensen\n# Answer:\nHayden Christensen", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.film -> Star Wars Episode III: Revenge of the Sith\n# Answer:\nStar Wars Episode III: Revenge of the Sith", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.010wvb1d -> film.performance.actor -> Abraham Benrubi\n# Answer:\nAbraham Benrubi", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.film -> Star Wars Episode II: Attack of the Clones\n# Answer:\nStar Wars Episode II: Attack of the Clones", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> The Way of the Apprentice -> common.topic.notable_types -> Book\n# Answer:\nThe Way of the Apprentice", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.010wvb1d -> film.performance.film -> Robot Chicken: Star Wars Episode III\n# Answer:\nRobot Chicken: Star Wars Episode III", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> The Way of the Apprentice -> book.book.genre -> Science Fiction\n# Answer:\nThe Way of the Apprentice", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> The Changing of the Guard -> book.book.characters -> Obi-Wan Kenobi\n# Answer:\nThe Changing of the Guard"], "ground_truth": ["Hayden Christensen"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.season -> 2006 NFL season\n# Answer:\n2006 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_8 -> sports.sports_award.season -> 2003 NFL season\n# Answer:\n2003 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.award -> Super Bowl Most Valuable Player Award\n# Answer:\nSuper Bowl Most Valuable Player Award", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_j -> sports.sports_award.season -> 2004 NFL season\n# Answer:\n2004 NFL season", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_8 -> sports.sports_award.award -> National Football League Most Valuable Player Award\n# Answer:\nNational Football League Most Valuable Player Award", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_j -> sports.sports_award.award -> National Football League Most Valuable Player Award\n# Answer:\nNational Football League Most Valuable Player Award", "# Reasoning Path:\nPeyton Manning -> people.person.spouse_s -> m.0hpd4nj -> freebase.valuenotation.is_reviewed -> Spouse\n# Answer:\nSpouse", "# Reasoning Path:\nPeyton Manning -> people.person.spouse_s -> m.0hpd4nj -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Ontario\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Canada\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> base.wikipedia_infobox.settlement.area_code -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Ontario\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> common.topic.notable_types -> City/Town/Village\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Canada\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.people_born_here -> Pattie Mallette\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nJuno Awards of 2014"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z973l -> military.military_combatant_group.combatants -> Iraq\n# Answer:\nIraq", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> France\n# Answer:\nFrance", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.04fvd6y -> military.military_combatant_group.combatants -> Saudi Arabia\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> royalty.kingdom.rulers -> Abdullah of Saudi Arabia\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> location.country.official_language -> Arabic Language\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wphv -> military.casualties.combatant -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wphl -> military.casualties.combatant -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> base.locations.countries.continent -> Asia\n# Answer:\nSaudi Arabia"], "ground_truth": ["Argentina", "United States of America", "France", "Australia", "Saudi Arabia", "Iraq", "United Kingdom"], "ans_acc": 0.7142857142857143, "ans_hit": 1, "ans_f1": 0.8333333333333333, "ans_precission": 1.0, "ans_recall": 0.7142857142857143, "path_f1": 0.18604651162790697, "path_precision": 0.8, "path_recall": 0.10526315789473684, "path_ans_f1": 0.8333333333333333, "path_ans_precision": 1.0, "path_ans_recall": 0.7142857142857143}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.0tly67c -> tv.regular_tv_appearance.actor -> Doc Shaw\n# Answer:\nDoc Shaw", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.0tly67c -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 2\n# Answer:\nThe Suite Life on Deck - Season 2", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nBrenda Song", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sglsdh -> award.award_nomination.award_nominee -> Cole Sprouse\n# Answer:\nCole Sprouse", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.0tly67c -> tv.regular_tv_appearance.character -> Marcus Little\n# Answer:\nMarcus Little", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 2\n# Answer:\nThe Suite Life on Deck - Season 2", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.actor -> Phill Lewis\n# Answer:\nPhill Lewis", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sglsdh -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Actor\n# Answer:\nKids' Choice Award for Favorite TV Actor", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Show\n# Answer:\nKids' Choice Award for Favorite TV Show", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 1\n# Answer:\nThe Suite Life on Deck - Season 1"], "ground_truth": ["Brenda Song"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.office_holder -> Rob Portman\n# Answer:\nRob Portman", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.office_holder -> George Voinovich\n# Answer:\nGeorge Voinovich", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> Tennessee\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nUnited States Senate", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_holder -> Sherrod Brown\n# Answer:\nSherrod Brown", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.image -> The Rhodes State Office Tower adjacent to the Statehouse\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.tropical_cyclone_season -> 1982 Atlantic hurricane season\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> common.topic.notable_for -> g.1255tjcrg\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.notable_for -> g.125h3hwcp\n# Answer:\nGovernment of Ohio"], "ground_truth": ["Ted Strickland", "Return J. Meigs, Jr.", "John Kasich"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nPound sterling", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.02nr829 -> sports.sports_team_roster.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m -> soccer.football_player_stats.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.02nr829 -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nMidfielder", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9021c -> soccer.football_player_stats.team -> A.C. Milan\n# Answer:\nA.C. Milan", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar"], "ground_truth": ["LA Galaxy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.47058823529411764, "path_precision": 0.4, "path_recall": 0.5714285714285714, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Hidalgo\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> common.topic.image -> Die Plaza Mayor am Abend\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7t8559g\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Zacuto\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7tbpl3m\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado National Memorial -> base.usnationalparks.us_national_park.state -> Arizona\n# Answer:\nCoronado National Memorial", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Alfonso XI of Castile\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7vzj2hj\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> common.topic.image -> Salamanca 2008\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado National Memorial -> base.usnationalparks.us_national_park.classification -> National Memorial\n# Answer:\nCoronado National Memorial"], "ground_truth": ["Salamanca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Patrick Bouvier Kennedy -> freebase.valuenotation.has_no_value -> Spouse (or domestic partner)\n# Answer:\nPatrick Bouvier Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Patrick Bouvier Kennedy -> people.deceased_person.place_of_death -> Boston Children's Hospital\n# Answer:\nPatrick Bouvier Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Patrick Bouvier Kennedy -> freebase.valuenotation.has_no_value -> Children\n# Answer:\nPatrick Bouvier Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Patrick Bouvier Kennedy -> people.person.gender -> Male\n# Answer:\nPatrick Bouvier Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of death\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Patrick Bouvier Kennedy -> freebase.valuenotation.has_no_value -> Profession\n# Answer:\nPatrick Bouvier Kennedy"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.location.containedby -> Asia\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> Fukushima Prefecture\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 8 -> base.infrastructure.nuclear_power_plant.reactor_type -> Advanced boiling water reactor\n# Answer:\nFukushima I \u2013 8", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Aichi Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> Japan\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> common.topic.image -> Okuma town office\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.country.administrative_divisions -> Aichi Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> T\u014dhoku region\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 8 -> common.topic.notable_for -> g.1259_1vs2\n# Answer:\nFukushima I \u2013 8"], "ground_truth": ["Japan", "Okuma"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Tyrone\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> base.aareas.schema.administrative_area.administrative_children -> East Midlands\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Scotland -> location.country.first_level_divisions -> Aberdeen\n# Answer:\nScotland", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nChild pornography", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> base.aareas.schema.administrative_area.administrative_children -> East of England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> location.country.first_level_divisions -> East Midlands\n# Answer:\nEngland"], "ground_truth": ["England", "Northern Ireland", "Scotland", "Wales"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.6, "path_precision": 0.5, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Tickfaw River -> location.location.partially_contained_by -> m.0wg8__5\n# Answer:\nTickfaw River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Tickfaw River -> geography.river.origin -> Amite County\n# Answer:\nTickfaw River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> location.location.containedby -> United States of America\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Bartholomew -> location.location.partially_containedby -> Arkansas\n# Answer:\nBayou Bartholomew", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wjpmwn -> location.partial_containment_relationship.partially_contains -> Bayou Macon\n# Answer:\nBayou Macon", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wg8__0 -> location.partial_containment_relationship.partially_contains -> Sabine River\n# Answer:\nSabine River"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.profession -> Philosopher\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Albert Camus -> people.person.profession -> Philosopher\n# Answer:\nAlbert Camus", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.specializations -> Author\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.corresponding_type -> Author\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alvin Plantinga -> people.person.profession -> Philosopher\n# Answer:\nAlvin Plantinga", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> influence.influence_node.influenced_by -> Thomas Kuhn\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.specializations -> Novelist\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> influence.influence_node.influenced_by -> Thomas Aquinas\n# Answer:\nAlasdair MacIntyre"], "ground_truth": ["Physician", "Writer", "Philosopher"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> people.person.children -> Sean Cole\n# Answer:\nYvonne Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> people.person.spouse_s -> m.0n9hknn\n# Answer:\nYvonne Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> people.person.children -> Neffeteria Pugh\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> common.topic.notable_for -> g.125dlnswt\n# Answer:\nYvonne Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> people.person.children -> Sean Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan -> music.recording.featured_artists -> J. Holiday\n# Answer:\n#1 Fan", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> people.person.spouse_s -> m.0wq9452\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> people.person.spouse_s -> m.0n9hknn\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> award.award_nominee.award_nominations -> m.05cqdz0 -> award.award_nomination.nominated_for -> Heaven Sent\n# Answer:\nHeaven Sent", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan -> music.recording.releases -> Definition of Real\n# Answer:\n#1 Fan"], "ground_truth": ["Leon Cole", "Sal Gibson", "Francine Lons"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.4, "ans_recall": 0.6666666666666666, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5, "path_ans_precision": 0.4, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> France\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Portugal\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Syria\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3f_h6\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> common.topic.notable_types -> Governmental Body\n# Answer:\nParliament of Egypt"], "ground_truth": ["Semi-presidential system", "Provisional government"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.events -> Assassination of Martin Luther King, Jr.\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.containedby -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Alabama\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nAlpha Man: The Brotherhood of MLK", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> common.topic.article -> m.05nnh0\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0_714v2 -> film.personal_film_appearance.film -> Alice Walker: Beauty in Truth\n# Answer:\nAlice Walker: Beauty in Truth", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Montgomery\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0gbz10_ -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> Maryland\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime. -> media_common.quotation.subjects -> Christians and Christianity\n# Answer:\nA strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime. -> common.topic.image -> Edgar Allan Poe signature\n# Answer:\nA strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong. -> common.topic.article -> m.05chc0k\n# Answer:\nA wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong.", "# Reasoning Path:\nEdgar Allan Poe -> film.film_story_contributor.film_story_credits -> An Evening of Edgar Allan Poe -> film.film.genre -> Drama\n# Answer:\nAn Evening of Edgar Allan Poe", "# Reasoning Path:\nEdgar Allan Poe -> film.film_story_contributor.film_story_credits -> Beczka amontillado -> film.film.genre -> Short Film\n# Answer:\nBeczka amontillado", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong. -> media_common.quotation.subjects -> Revenge\n# Answer:\nA wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong.", "# Reasoning Path:\nEdgar Allan Poe -> film.film_story_contributor.film_story_credits -> An Evening of Edgar Allan Poe -> media_common.adaptation.adapted_from -> The Tell-Tale Heart\n# Answer:\nAn Evening of Edgar Allan Poe"], "ground_truth": ["Baltimore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola -> location.location.containedby -> Fulton County\n# Answer:\nWorld of Coca-Cola", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola -> common.topic.notable_types -> Museum\n# Answer:\nWorld of Coca-Cola", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> United States of America\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Men's rings\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> common.topic.notable_types -> Zoo\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola -> location.location.containedby -> 30313\n# Answer:\nWorld of Coca-Cola", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.containedby -> United States of America\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola -> location.location.containedby -> Georgia\n# Answer:\nWorld of Coca-Cola", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> travel.transport_terminus.travel_destinations_served -> m.052zwlt\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport"], "ground_truth": ["Variety Playhouse", "Center for Puppetry Arts", "Underground Atlanta", "Atlanta Ballet", "Atlanta Cyclorama & Civil War Museum", "Turner Field", "Georgia World Congress Center", "Georgia Aquarium", "Cobb Energy Performing Arts Centre", "Six Flags Over Georgia", "The Tabernacle", "Peachtree Road Race", "Fox Theatre", "Georgia Dome", "Atlanta Jewish Film Festival", "Atlanta History Center", "Fernbank Museum of Natural History", "Jimmy Carter Library and Museum", "Philips Arena", "World of Coca-Cola", "Six Flags White Water", "Zoo Atlanta", "Atlanta Symphony Orchestra", "Omni Coliseum", "Centennial Olympic Park", "Four Seasons Hotel Atlanta", "Masquerade", "Martin Luther King, Jr. National Historic Site", "Atlanta Marriott Marquis", "Arbor Place Mall", "Fernbank Science Center", "Woodruff Arts Center", "CNN Center", "Georgia State Capitol", "Margaret Mitchell House & Museum", "Hyatt Regency Atlanta"], "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0.15254237288135591, "ans_precission": 0.9, "ans_recall": 0.08333333333333333, "path_f1": 0.15254237288135591, "path_precision": 0.9, "path_recall": 0.08333333333333333, "path_ans_f1": 0.1978021978021978, "path_ans_precision": 0.9, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.containedby -> Queensland\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.containedby -> Australia\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> common.topic.notable_types -> City/Town/Village\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Adrienne Pickering\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.district_represented -> Electoral district of South Brisbane\n# Answer:\nElectoral district of South Brisbane", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.appears_in_topic_gallery -> Queensland state election, 2009\n# Answer:\nAnna Bligh crop", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Alan Marshal\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Alison Mosely\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.appears_in_topic_gallery -> Queensland state election, 2015\n# Answer:\nAnna Bligh crop"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ken Barlow\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> people.person.gender -> Male\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> David Barlow\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> film.actor.film -> m.0h0_mvx\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Albert Tatlock\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Soap opera -> common.topic.webpage -> m.09w1_hc\n# Answer:\nSoap opera", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Soap opera -> common.topic.article -> m.06q7y\n# Answer:\nSoap opera", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Soap opera -> common.topic.webpage -> m.09w1bxj\n# Answer:\nSoap opera", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Soap opera -> broadcast.genre.content -> A Woman of America\n# Answer:\nSoap opera", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Thursday 10th June 2010 -> common.topic.notable_types -> Multipart TV episode\n# Answer:\nThursday 10th June 2010"], "ground_truth": ["William Roache"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.administrative_division.second_level_division_of -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> base.biblioness.bibs_location.country -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> Scotland\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#domain -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Gender -> type.property.schema -> Person\n# Answer:\nGender"], "ground_truth": ["2005"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.country.form_of_government -> Parliamentary system\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.country.languages_spoken -> Malay Language\n# Answer:\nChristmas Island", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.country.form_of_government -> Federal monarchy\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc4pgdn\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands -> location.country.form_of_government -> Federal monarchy\n# Answer:\nCocos (Keeling) Islands", "# Reasoning Path:\nAustralian dollar -> base.ontologies.ontology_instance.equivalent_instances -> m.09knr56 -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nOpenCyc", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc4y0zl\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nCocos (Keeling) Islands", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands -> location.location.time_zones -> Cocos Islands\u00a0Time Zone\n# Answer:\nCocos (Keeling) Islands"], "ground_truth": ["AUD"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ll\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22zj\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc378pv\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc37hbq\n# Answer:\nlocation.statistical_region.part_time_employment_percent"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> common.topic.notable_for -> g.125dysc88\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0z23kt0 -> sports.sports_team_roster.team -> Auburn Tigers football\n# Answer:\nAuburn Tigers football", "# Reasoning Path:\nCam Newton -> people.person.parents -> Jackie Newton -> common.topic.notable_for -> g.125dzwcd6\n# Answer:\nJackie Newton", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.team -> Carolina Panthers\n# Answer:\nCarolina Panthers", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0z23kt0 -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nCam Newton -> people.person.parents -> Jackie Newton -> people.person.gender -> Female\n# Answer:\nJackie Newton", "# Reasoning Path:\nCam Newton -> people.person.parents -> Jackie Newton -> common.topic.notable_types -> Person\n# Answer:\nJackie Newton", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> common.topic.notable_types -> American football player\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["Carolina Panthers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.location.containedby -> Maryland\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.location.containedby -> Maryland\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.containedby -> United States of America\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Maryland\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.containedby -> United States, with Territories\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.statistical_region.rent50_1 -> m.05gcgl3\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.location.containedby -> Bethesda-Frederick-Gaithersburg, MD Metropolitan Division\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.location.containedby -> Bethesda-Frederick-Gaithersburg, MD Metropolitan Division\n# Answer:\nFrederick County"], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nHuntingdon College", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> University of Alabama School of Law\n# Answer:\nUniversity of Alabama School of Law", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138 -> education.education.institution -> University of Alabama\n# Answer:\nUniversity of Alabama", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.place_of_birth -> Los Angeles\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.gender -> Female\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_types -> Book Edition\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_for -> g.125920htw\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nHarper Seven Beckham"], "ground_truth": ["Monroe County High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> book.written_work.subjects -> History\n# Answer:\nUtah, the right place", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> book.written_work.author -> Thomas G. Alexander\n# Answer:\nUtah, the right place", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> common.topic.notable_for -> g.125dtp7bg\n# Answer:\nUtah, the right place", "# Reasoning Path:\nUtah -> book.book_subject.works -> Bound for Canaan -> book.written_work.author -> Margaret Blair Young\n# Answer:\nBound for Canaan", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> government.government_position_held.office_position_or_title -> Lieutenant Governor of Utah\n# Answer:\nLieutenant Governor of Utah", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010l3l5d -> freebase.valuenotation.has_no_value -> Appointed By (if Position is Appointed)\n# Answer:\nAppointed By (if Position is Appointed)"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> people.person.ethnicity -> White people\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> freebase.valuenotation.has_value -> Siblings\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> people.person.nationality -> United States of America\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointed_role -> Director of the Bureau of Counterterrorism\n# Answer:\nDirector of the Bureau of Counterterrorism", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pk -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointee -> Henry A. Crumpton\n# Answer:\nHenry A. Crumpton", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1 -> people.appointment.appointed_role -> United States Ambassador to Angola\n# Answer:\nUnited States Ambassador to Angola", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079q3lx -> people.appointment.appointed_role -> United States Ambassador to Brazil\n# Answer:\nUnited States Ambassador to Brazil", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pv -> film.personal_film_appearance.film -> All About Ann: Governor Richards of the Lone Star State\n# Answer:\nAll About Ann: Governor Richards of the Lone Star State", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1 -> people.appointment.appointee -> Dan Mozena\n# Answer:\nDan Mozena"], "ground_truth": ["Michael Peroutka", "John Kerry", "Ralph Nader", "Gene Amondson"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> people.marriage.spouse -> Sue Douglas\n# Answer:\nSue Douglas", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.spouse -> Ayaan Hirsi Ali\n# Answer:\nAyaan Hirsi Ali", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> freebase.valuenotation.has_value -> To\n# Answer:\nTo", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0pdthbn -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0pdthbn -> film.personal_film_appearance.film -> Valentino's Ghost\n# Answer:\nValentino's Ghost", "# Reasoning Path:\nNiall Ferguson -> book.author.works_written -> The world's banker -> book.book.editions -> The world's banker: the history of the House of Rothschild\n# Answer:\nThe world's banker", "# Reasoning Path:\nNiall Ferguson -> book.author.works_written -> The world's banker -> common.topic.notable_types -> Book\n# Answer:\nThe world's banker"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Pacific Ocean\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> common.topic.article -> m.03660h\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.containedby -> Americas\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> common.topic.notable_for -> g.1255fs0l4\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Pacific Ocean -> location.location.containedby -> World Ocean\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> g.12377byy\n# Answer:\ngeography.island_group.islands_in_group", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> base.aareas.schema.administrative_area.administrative_children -> Gal\u00e1pagos Province\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Baltra Island -> location.location.containedby -> Santa Cruz Canton, Ecuador\n# Answer:\nBaltra Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.primarily_containedby -> South America\n# Answer:\nEcuador"], "ground_truth": ["Ecuador", "Pacific Ocean", "Gal\u00e1pagos Province"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> Somebody to Love\n# Answer:\nSomebody to Love", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.featured_artists -> Ludacris\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> Under the Mistletoe -> freebase.valuenotation.is_reviewed -> Artist\n# Answer:\nUnder the Mistletoe", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> music.composition.composer -> Ryan Toby\n# Answer:\nRyan Toby", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.artist -> Ludacris\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nJuno Awards of 2014", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> Under the Mistletoe -> music.album.genre -> Pop music\n# Answer:\nUnder the Mistletoe", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> music.album.release_type -> Single\n# Answer:\nAll Bad", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> Under the Mistletoe -> freebase.valuenotation.is_reviewed -> Initial release date\n# Answer:\nUnder the Mistletoe"], "ground_truth": ["Eenie Meenie", "All That Matters", "Lolly", "Never Say Never", "Bad Day", "Somebody to Love", "Beauty And A Beat", "First Dance", "Recovery", "Right Here", "Thought Of You", "All Around The World", "#thatPower", "Home to Mama", "As Long as You Love Me", "Turn to You (Mother's Day Dedication)", "Baby", "Change Me", "Roller Coaster", "Never Let You Go", "Wait for a Minute", "Hold Tight", "Confident", "Heartbreaker", "Die in Your Arms", "Live My Life", "Pray", "Bigger", "Boyfriend", "PYD", "All Bad"], "ans_acc": 0.0967741935483871, "ans_hit": 1, "ans_f1": 0.15584415584415584, "ans_precission": 0.4, "ans_recall": 0.0967741935483871, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.16216216216216214, "path_ans_precision": 0.5, "path_ans_recall": 0.0967741935483871}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> common.topic.notable_types -> Profession\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> common.topic.article -> m.016fc9\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> common.topic.notable_types -> Profession\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Acetylene Lamp\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> people.profession.corresponding_type -> Author\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Physician -> common.topic.notable_types -> Profession\n# Answer:\nPhysician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> type.type.expected_by -> politician\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> base.descriptive_names.names.descriptive_name -> m.0101fnw4\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> base.descriptive_names.names.descriptive_name -> m.0105cq_d\n# Answer:\nJournalist"], "ground_truth": ["Statesman", "Writer", "Journalist", "Publisher", "Physician"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6461538461538462, "ans_precission": 0.7, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.6461538461538462, "path_ans_precision": 0.7, "path_ans_recall": 0.6}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.symbol -> Petrified wood\n# Answer:\nPetrified wood", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz -> location.location_symbol_relationship.symbol -> Mountain tree frog\n# Answer:\nMountain tree frog", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nSaguaro", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.Kind_of_symbol -> State flower\n# Answer:\nState flower", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.Kind_of_symbol -> State fossil\n# Answer:\nState fossil", "# Reasoning Path:\nArizona -> location.statistical_region.religions -> m.04403h9 -> location.religion_percentage.religion -> Pentecostalism\n# Answer:\nPentecostalism", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.resource -> ATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!\n# Answer:\nATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1l1s -> common.webpage.resource -> ATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!\n# Answer:\nATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz -> location.location_symbol_relationship.Kind_of_symbol -> State Amphibian\n# Answer:\nState Amphibian"], "ground_truth": ["Saguaro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> base.aareas.schema.administrative_area.administrative_parent -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.location.contains -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.064_ltw -> base.popstra.infidelity.participant -> Kanye West\n# Answer:\nKanye West", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.country.administrative_divisions -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> common.topic.notable_types -> Country\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> m.010_ydxv -> music.track_contribution.track -> If I Never See Your Face Again\n# Answer:\nIf I Never See Your Face Again"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> Harrison inauguration\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\ncommon.image.size", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.03fx87q -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.03qtjkt\n# Answer:\ncommon.image.size", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> 222px-Harrison_inauguration1841.jpg\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.0bfmkg2 -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.03fx87q -> government.government_position_held.basic_title -> President\n# Answer:\nPresident", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.04j5skj -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor"], "ground_truth": ["1841-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> United States of America\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> common.topic.image -> Ryan Braun\n# Answer:\nRyan Braun", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Ryan Braun\n# Answer:\nRyan Braun", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> California\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> Los Angeles\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.03lppm1 -> common.webpage.resource -> m.0bjrztw\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Alan Muraoka\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.0948qtd -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Chris Snail\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.03lppm1 -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website"], "ground_truth": ["Mission Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.administrative_division.country -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.containedby -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.citytown -> Suwon\n# Answer:\nSuwon", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> \u718a\u6d25\u30b0\u30eb\u30fc\u30d7\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.country -> South Korea\n# Answer:\nSouth Korea", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.statistical_region.population -> g.11b7tcbqv4\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.postal_code -> 443-742\n# Answer:\n443-742", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.statistical_region.population -> g.11b7tm_xvh\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Mahaka Media\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.statistical_region.population -> g.11b7tm_xwq\n# Answer:\nDaegu"], "ground_truth": ["Suwon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Sikhism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.beliefs -> End time\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.article -> m.055v4j\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Hinduism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.image -> Ramapir01\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> common.topic.article -> m.0jfq\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Jainism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> God -> common.topic.notable_for -> g.125d2mw4b\n# Answer:\nGod", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.image -> Ramapir22\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> religion.deity.deity_of -> Sunni Islam\n# Answer:\nAllah"], "ground_truth": ["Islamic view of angels", "Monotheism", "Tawhid", "Masih ad-Dajjal", "Islamic holy books", "Qiyamah", "Prophets in Islam", "Predestination in Islam", "Sharia", "\u1e6c\u016bb\u0101", "Entering Heaven alive", "God in Islam", "Mahdi"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.actor -> Jamie Dornan\n# Answer:\nJamie Dornan", "# Reasoning Path:\nChristian Grey -> common.topic.notable_for -> g.125czvn3w\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.film -> Fifty Shades of Grey\n# Answer:\nFifty Shades of Grey", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Males\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> common.topic.notable_for -> g.1255p_jgc\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Acute myeloid leukemia\n# Answer:\nMale"], "ground_truth": ["Jamie Dornan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Franz Kafka\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> people.cause_of_death.parent_cause_of_death -> Infectious disease\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Albert Camus\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Antibiotics\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Isoniazid\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Anton Chekhov\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Aldous Huxley -> people.deceased_person.cause_of_death -> Laryngeal cancer\n# Answer:\nAldous Huxley", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Pyrazinamide\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion. -> common.topic.notable_types -> Quotation\n# Answer:\nA dirty joke is a sort of mental rebellion.", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Arthur Koestler -> people.deceased_person.cause_of_death -> Parkinson's disease\n# Answer:\nArthur Koestler"], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.office_position_or_title -> F\u00fchrer\n# Answer:\nF\u00fchrer", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.basic_title -> F\u00fchrer\n# Answer:\nF\u00fchrer", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.office_position_or_title -> Chancellor of Germany\n# Answer:\nChancellor of Germany", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> base.schemastaging.context_name.pronunciation -> g.125_pt37m\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.place_founded -> Nazi Germany\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.founders -> Heinrich Himmler\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> 1st SS Panzer Division Leibstandarte SS Adolf Hitler -> organization.organization.place_founded -> Nazi Germany\n# Answer:\n1st SS Panzer Division Leibstandarte SS Adolf Hitler"], "ground_truth": ["Nazi Germany"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.380952380952381, "path_precision": 0.4, "path_recall": 0.36363636363636365, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> common.topic.notable_types -> Profession\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.specialization_of -> Musician\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.track_contributions -> g.11b7_lvdf2\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.specialization_of -> Musician\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> http://www.discogs.com/artist/Michael+Bubl%E9 -> common.webpage.category -> Topic Webpage\n# Answer:\nhttp://www.discogs.com/artist/Michael+Bubl%E9", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.03lpqjt -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> common.topic.notable_types -> Profession\n# Answer:\nSongwriter"], "ground_truth": ["Singer", "Songwriter", "Actor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas City, MO-KS Metropolitan Statistical Area\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.location.containedby -> Kansas City, MO-KS Metropolitan Statistical Area\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas City, MO-KS Metropolitan Statistical Area\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.statistical_region.co2_emissions_mobile -> m.045l26t\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.statistical_region.co2_emissions_mobile -> m.045l26t\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111 -> location.location.containedby -> Wyandotte County\n# Answer:\n66111", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Wyandotte County -> location.statistical_region.co2_emissions_mobile -> m.045l26t\n# Answer:\nWyandotte County"], "ground_truth": ["Wyandotte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> sports.sports_team_roster.team -> Montreal Royals\n# Answer:\nMontreal Royals", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0gggrzr -> sports.sports_team_roster.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> sports.sports_team_roster.position -> Shortstop\n# Answer:\nShortstop", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h -> sports.sports_team_roster.team -> UCLA Bruins football\n# Answer:\nUCLA Bruins football", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.season -> 1956 Major League Baseball Season\n# Answer:\n1956 Major League Baseball Season", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbp_k -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> sports.sports_team_roster.position -> Second baseman\n# Answer:\nSecond baseman", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0gggrzr -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nTeam"], "ground_truth": ["UCLA Bruins football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> people.person.place_of_birth -> Ossining\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> location.location.containedby -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> tv.tv_program_creator.programs_created -> Spaceballs: The Animated Series\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.story_by -> Harold Gray -> comic_strips.comic_strip_creator.comic_strips_written -> m.0gwbjrm\n# Answer:\nHarold Gray", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Play -> type.type.properties -> Composer\n# Answer:\nPlay", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Play -> type.type.properties -> Country of origin\n# Answer:\nPlay", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> location.location.containedby -> New York\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Choreographer\n# Answer:\nChoreographer", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> award.award_nominee.award_nominations -> m.010bvypw\n# Answer:\nThomas Meehan"], "ground_truth": ["1976-08-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> United States of America\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07ghzrz\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Alabama\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b66g7msm\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Area code 251\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b7tn_fd2\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.education -> m.03gkqtp -> education.education.institution -> Louisiana State University\n# Answer:\nLouisiana State University", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11btt6sf_l\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07919ln -> american_football.player_passing_statistics.team -> Oakland Raiders\n# Answer:\nOakland Raiders", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07919ln -> american_football.player_passing_statistics.season -> 2008 NFL season\n# Answer:\n2008 NFL season"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> base.aareas.schema.administrative_area.administrative_parent -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.administrative_division.country -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> John Aspinwall Roosevelt -> people.deceased_person.place_of_death -> New York City\n# Answer:\nJohn Aspinwall Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> You learn by living -> book.book_edition.book -> You Learn by Living\n# Answer:\nYou learn by living", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> What I hope to leave behind -> book.book_edition.book -> What I Hope to Leave Behind\n# Answer:\nWhat I hope to leave behind", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Anna Roosevelt Halsted -> people.deceased_person.place_of_death -> New York City\n# Answer:\nAnna Roosevelt Halsted", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> You learn by living -> common.topic.notable_for -> g.125fl6r_c\n# Answer:\nYou learn by living"], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf2_w -> location.religion_percentage.religion -> Islam\n# Answer:\nIslam", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60ptk2z\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf301 -> location.religion_percentage.religion -> Protestantism\n# Answer:\nProtestantism", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.12cp_j7n1\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.064szjw -> location.religion_percentage.religion -> Hinduism\n# Answer:\nHinduism", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_4m6h\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_67l9\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.1hhc378kt\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.1hhc38d0l\n# Answer:\nlocation.statistical_region.official_development_assistance"], "ground_truth": ["Protestantism", "Islam", "Catholicism", "Hinduism"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.46153846153846156, "ans_precission": 0.3333333333333333, "ans_recall": 0.75, "path_f1": 0.42857142857142855, "path_precision": 0.3333333333333333, "path_recall": 0.6, "path_ans_f1": 0.46153846153846156, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.75}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Countryway Gunshop\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Projectile weapon\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.image -> M&Prevolver\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Springfield Armory, Inc.\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> base.weapons.weapon.subtypes -> Shotgun\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.image -> HK USP 45\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.location.contains -> Wholesale Row\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> base.weapons.weapon.subtypes -> Handgun\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> base.fight.crime_type.crimes_of_this_type -> Assassination of Anwar Sadat\n# Answer:\nAssassination", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> base.disaster2.tactic.attacks_of_this_form -> m.065tkfr\n# Answer:\nAssassination"], "ground_truth": ["Firearm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> people.person.children -> Charles Hamlin\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> common.topic.article -> m.03mpv\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.04hdfss -> people.place_lived.location -> Illinois\n# Answer:\nIllinois", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.nationality -> United States of America\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.profession -> Politician\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pgr_5 -> people.place_lived.location -> Kentucky\n# Answer:\nKentucky", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pn4x_ -> people.place_lived.location -> Springfield\n# Answer:\nSpringfield"], "ground_truth": ["Andrew Johnson", "Hannibal Hamlin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7272727272727273, "ans_precission": 0.5714285714285714, "ans_recall": 1.0, "path_f1": 0.7272727272727273, "path_precision": 0.5714285714285714, "path_recall": 1.0, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 1.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> award.award_nominee.award_nominations -> m.011lncpm -> award.award_nomination.nominated_for -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> common.topic.notable_types -> Film character\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Oliver Twist -> theater.theater_character.plays_appears_in -> Oliver!\n# Answer:\nOliver Twist", "# Reasoning Path:\nCharles Dickens -> award.award_nominee.award_nominations -> m.011lncpm -> award.award_nomination.award_nominee -> Orson Welles\n# Answer:\nOrson Welles", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Oliver Twist -> common.topic.image -> \\\"Please, sir, I want some more.\\\" Illustration by George Cruikshank.\n# Answer:\nOliver Twist", "# Reasoning Path:\nCharles Dickens -> award.award_nominee.award_nominations -> m.011lncpm -> award.award_nomination.award -> Retro Hugo for Best Dramatic Presentation, Short Form\n# Answer:\nRetro Hugo for Best Dramatic Presentation, Short Form", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> film.film_character.portrayed_in_films -> m.010p_33b\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Oliver Twist -> tv.tv_character.appeared_in_tv_program -> m.0pdqx4p\n# Answer:\nOliver Twist"], "ground_truth": ["A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "A Tale of Two Cities (Clear Print)", "A Tale of Two Cities (Oxford Bookworms Library)", "A Christmas Carol (Young Reading Series 2)", "Our mutual friend", "A Tale of Two Cities (Penguin Readers, Level 5)", "A Christmas Carol (New Longman Literature)", "A Christmas Carol (Take Part)", "A Tale of Two Cities (BBC Audio Series)", "A Tale of Two Cities (Classic Retelling)", "A CHRISTMAS CAROL", "The Old Curiosity Shop", "A Tale of Two Cities (Cover to Cover Classics)", "A Tale of Two Cities (The Greatest Historical Novels)", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Silver Classics)", "A Christmas Carol (Watermill Classic)", "A Christmas Carol (Thornes Classic Novels)", "A Tale of Two Cities (Isis Clear Type Classic)", "Great Expectations.", "Martin Chuzzlewit", "A Tale of Two Cities (Illustrated Classics)", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "A Christmas Carol (Ladybird Children's Classics)", "A Christmas Carol (Scholastic Classics)", "A Tale of Two Cities (Pacemaker Classics)", "A Christmas Carol (Whole Story)", "A Christmas Carol (Family Classics)", "A Tale of Two Cities (Oxford Playscripts)", "A Christmas Carol (R)", "A Christmas Carol (Green Integer, 50)", "Great expectations", "A Tale of Two Cities (Classics Illustrated Notes)", "A Tale Of Two Cities (Adult Classics)", "A Christmas Carol (Illustrated Classics)", "A Tale of Two Cities (Unabridged Classics)", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "A Christmas Carol (Pacemaker Classic)", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "A Christmas Carol (Tor Classics)", "A Tale of Two Cities (Everyman Paperbacks)", "A Christmas Carol (Large Print)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "A Christmas Carol (Read & Listen Books)", "A Tale of Two Cities (Classic Literature with Classical Music)", "A Tale of Two Cities (Longman Fiction)", "A Christmas Carol", "A Christmas Carol (Everyman's Library Children's Classics)", "Hard times", "A Christmas Carol (Soundings)", "David Copperfield.", "A Tale of Two Cities (Piccolo Books)", "A Tale of Two Cities (Puffin Classics)", "A Christmas Carol (Penguin Student Editions)", "David Copperfield", "A Tale of Two Cities (Courage Literary Classics)", "A Christmas Carol (Watermill Classics)", "A Christmas Carol (Classics for Young Adults and Adults)", "A Christmas Carol (Cover to Cover)", "A Tale of Two Cities (Cassette (1 Hr).)", "A Tale of Two Cities (Ultimate Classics)", "A Tale of Two Cities (Konemann Classics)", "A Tale of Two Cities (Dover Thrift Editions)", "Bleak House", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "A Christmas Carol (Pacemaker Classics)", "A Tale of Two Cities (Dramatized)", "A Tale of Two Cities (Tor Classics)", "A Christmas Carol (Apple Classics)", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "The Mystery of Edwin Drood", "A Christmas Carol (Enriched Classics)", "A Tale of Two Cities (Classic Fiction)", "A Christmas Carol (Usborne Young Reading)", "A Tale of Two Cities (Paperback Classics)", "A Christmas Carol (Aladdin Classics)", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Cyber Classics)", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "Bleak house", "A Tale of Two Cities (Enriched Classic)", "A Christmas Carol (Radio Theatre)", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "A Tale of Two Cities (Large Print Edition)", "A Tale of Two Cities (Soundings)", "A Christmas Carol (Oxford Bookworms Library)", "A Tale of Two Cities (Prentice Hall Science)", "Oliver Twist", "A Tale Of Two Cities (Adult Classics in Audio)", "A Christmas Carol (Audio Editions)", "A Christmas Carol (Classic Books on Cassettes Collection)", "A Christmas Carol (Saddleback Classics)", "A Christmas Carol (Acting Edition)", "Dombey and son", "A Christmas Carol (Classics Illustrated)", "Dombey and Son.", "A Christmas Carol (Great Stories)", "Dombey and Son", "A Tale of Two Cities (Progressive English)", "A Tale of Two Cities (Signet Classics)", "A Tale of Two Cities (Bookcassette(r) Edition)", "The mystery of Edwin Drood", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "A TALE OF TWO CITIES", "Sketches by Boz", "The old curiosity shop.", "A Tale of Two Cities (40th Anniversary Edition)", "The cricket on the hearth", "A Christmas Carol (Nelson Graded Readers)", "A Christmas Carol (Clear Print)", "A Tale of Two Cities (Adopted Classic)", "A Tale of Two Cities (Simple English)", "A Tale of Two Cities (Dodo Press)", "Great expectations.", "A Christmas Carol (Limited Editions)", "A Tale of Two Cities (Everyman's Library (Paper))", "A Tale of Two Cities (Acting Edition)", "A Tale of Two Cities (Everyman's Library Classics)", "A Christmas Carol (Bantam Classic)", "A Christmas Carol (Ladybird Classics)", "A Tale of Two Cities (Collector's Library)", "A Christmas Carol (Dramascripts)", "A Tale of Two Cities (Illustrated Junior Library)", "A Christmas Carol (Gollancz Children's Classics)", "Bleak House.", "A Tale of Two Cities (Ladybird Children's Classics)", "A Christmas Carol (Puffin Classics)", "A Christmas Carol (Value Books)", "A Christmas Carol (Penguin Readers, Level 2)", "Our mutual friend.", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Tale of Two Cities (10 Cassettes)", "A Christmas Carol (Children's Classics)", "A Tale of Two Cities (Masterworks)", "A Tale of Two Cities (Dramascripts S.)", "A Tale of Two Cities", "A Tale of Two Cities (Penguin Classics)", "A Tale of Two Cities (Collected Works of Charles Dickens)", "A Christmas Carol (Puffin Choice)", "A Christmas Carol (Classic, Picture, Ladybird)", "A Tale of Two Cities (Wordsworth Classics)", "A Tale of Two Cities (Saddleback Classics)", "A Christmas Carol (Reissue)", "A Christmas Carol (Through the Magic Window Series)", "A Tale of Two Cities (The Classic Collection)", "A Christmas Carol (Dramascripts Classic Texts)", "A Tale of Two Cities (Compact English Classics)", "A Tale of Two Cities (Naxos AudioBooks)", "The Pickwick papers", "A Christmas Carol (Classic Collection)", "Great Expectations", "A Christmas Carol (Children's Theatre Playscript)", "A Tale of Two Cities (Barnes & Noble Classics Series)", "A Tale of Two Cities (Student's Novels)", "A Christmas Carol. (Lernmaterialien)", "A Tale of Two Cities (Penguin Popular Classics)", "A Tale of Two Cities (Classics Illustrated)", "A Christmas Carol (Cp 1135)", "A Tale of Two Cities (Bantam Classic)", "A Tale of Two Cities (Macmillan Students' Novels)", "Little Dorrit", "A Christmas Carol (Chrysalis Children's Classics Series)", "A Christmas Carol (Classic Fiction)", "The old curiosity shop", "A Tale of Two Cities (Longman Classics, Stage 2)", "The Pickwick Papers", "A Christmas Carol (The Kennett Library)", "The life and adventures of Nicholas Nickleby"], "ans_acc": 0.023668639053254437, "ans_hit": 1, "ans_f1": 0.0339943342776204, "ans_precission": 0.4, "ans_recall": 0.01775147928994083, "path_f1": 0.32, "path_precision": 0.6, "path_recall": 0.21818181818181817, "path_ans_f1": 0.034482758620689655, "path_ans_precision": 0.6, "path_ans_recall": 0.01775147928994083}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_commander -> Nikita Khrushchev\n# Answer:\nNikita Khrushchev", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.office_holder -> Nikita Khrushchev\n# Answer:\nNikita Khrushchev", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_conflict -> Cuban Missile Crisis\n# Answer:\nCuban Missile Crisis", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf -> military.military_command.military_commander -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_6 -> government.government_position_held.office_holder -> Vladimir Ivashko\n# Answer:\nVladimir Ivashko", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.049y3kf -> military.military_command.military_commander -> Ivan Konev\n# Answer:\nIvan Konev", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.governmental_body -> Central Committee of the Communist Party of the Soviet Union\n# Answer:\nCentral Committee of the Communist Party of the Soviet Union", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_holder -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf -> military.military_command.military_conflict -> World War II\n# Answer:\nWorld War II", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.049y3kf -> military.military_command.military_conflict -> Battle of Berlin\n# Answer:\nBattle of Berlin"], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_formerly_used -> Cuba\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> United States of America\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.statistical_region.gdp_deflator_change -> g.11b6ddwl64\n# Answer:\nlocation.statistical_region.gdp_deflator_change", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> American Samoa\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.target_of_exchange -> m.03x8_16\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Working People's Party of Puerto Rico -> common.topic.notable_types -> Political party\n# Answer:\nWorking People's Party of Puerto Rico", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> Bonaire\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.statistical_region.gdp_deflator_change -> g.124x8g_1y\n# Answer:\nlocation.statistical_region.gdp_deflator_change", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.target_of_exchange -> m.04kq24y\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Working People's Party of Puerto Rico -> common.topic.article -> m.0k0w3qq\n# Answer:\nWorking People's Party of Puerto Rico"], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Radiation therapy\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Epilepsy\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> people.cause_of_death.parent_cause_of_death -> Cancer\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Dementia\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease.medical_specialties -> Oncology\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Head pressing\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Adjuvant\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease.medical_specialties -> Neurology\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.symptoms -> Air crescent sign\n# Answer:\nLung cancer"], "ground_truth": ["Lung cancer", "Brain tumor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nWilliam Daniels", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.0kv9d2d -> tv.regular_tv_appearance.actor -> Peter Parros\n# Answer:\nPeter Parros", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 4\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4 -> tv.tv_series_season.regular_cast -> m.03lj4m5\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.02h9cb0 -> tv.regular_tv_appearance.actor -> David Hasselhoff\n# Answer:\nDavid Hasselhoff", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4 -> tv.tv_series_season.regular_cast -> m.0kv9d2d\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.0kv9d2d -> tv.regular_tv_appearance.character -> g.1213vfbl\n# Answer:\ntv.regular_tv_appearance.character", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.02h9cb0 -> tv.regular_tv_appearance.character -> Michael Knight\n# Answer:\nMichael Knight", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 3\n# Answer:\nKnight Rider - Season 3"], "ground_truth": ["William Daniels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.location.containedby -> Tennessee\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.location.containedby -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County -> location.location.containedby -> Tennessee\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.location.containedby -> Nashville-Davidson--Murfreesboro--Franklin, TN Metropolitan Statistical Area\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.statistical_region.co2_emissions_total -> m.045hnmm\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.statistical_region.population -> g.11b674pwdr\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.country -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11b66dwnl4\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County -> location.location.containedby -> Nashville-Davidson--Murfreesboro--Franklin, TN Metropolitan Statistical Area\n# Answer:\nWilliamson County"], "ground_truth": ["Williamson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> United States of America\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.geolocation -> m.0kdmvf\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Jasper County\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Missouri\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11b66mljn1\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_for -> g.1255959wd\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> USS George Washington Carver (SSBN-656) -> common.topic.notable_types -> Ship\n# Answer:\nUSS George Washington Carver (SSBN-656)", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11x1chmhk\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> astronomy.extraterrestrial_location.on_celestial_object -> Moon\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> USS George Washington Carver (SSBN-656) -> common.topic.article -> m.025436\n# Answer:\nUSS George Washington Carver (SSBN-656)"], "ground_truth": ["Diamond"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Schuyler Frances Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSchuyler Frances Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Schuyler Frances Fox -> common.topic.notable_types -> Person\n# Answer:\nSchuyler Frances Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Esm\u00e9 Annabelle Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nEsm\u00e9 Annabelle Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.spouse -> Tracy Pollan\n# Answer:\nTracy Pollan", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Schuyler Frances Fox -> people.person.sibling_s -> m.0j217k5\n# Answer:\nSchuyler Frances Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.location_of_ceremony -> Arlington\n# Answer:\nArlington", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Schuyler Frances Fox -> people.person.sibling_s -> m.0tjswcf\n# Answer:\nSchuyler Frances Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> common.topic.notable_types -> Person\n# Answer:\nAquinnah Kathleen Fox"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04yvq68 -> military.military_command.military_conflict -> How Few Remain\n# Answer:\nHow Few Remain", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9q6 -> military.military_command.military_conflict -> Battle of McDowell\n# Answer:\nBattle of McDowell", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9nd -> military.military_command.military_conflict -> Battle of Port Republic\n# Answer:\nBattle of Port Republic", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Foot cavalry\n# Answer:\nFoot cavalry", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Winfield Scott\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> time.event.includes_event -> Battle of McDowell\n# Answer:\nJackson's Valley Campaign", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> book.book_subject.musical_compositions_about_this_topic -> the CIVIL warS: a tree is best measured when it is down\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Abraham Lincoln\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> visual_art.art_subject.artwork_on_the_subject -> Battle of Mobile Bay, 5 August 1864\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Antietam -> military.military_conflict.military_personnel_involved -> Albion P. Howe\n# Answer:\nBattle of Antietam"], "ground_truth": ["Second Battle of Bull Run", "Battle of Hancock", "Battle of McDowell", "Battle of Port Republic", "First Battle of Winchester", "Battle of Front Royal", "Battle of Hoke's Run", "First Battle of Kernstown", "American Civil War", "Battle of White Oak Swamp", "Jackson's Valley Campaign", "First Battle of Rappahannock Station", "Manassas Station Operations", "Battle of Chantilly", "How Few Remain", "Battle of Harpers Ferry", "Battle of Chancellorsville", "Battle of Cedar Mountain", "Romney Expedition"], "ans_acc": 0.2631578947368421, "ans_hit": 1, "ans_f1": 0.396039603960396, "ans_precission": 0.8, "ans_recall": 0.2631578947368421, "path_f1": 0.2445414847161572, "path_precision": 0.7, "path_recall": 0.14814814814814814, "path_ans_f1": 0.396039603960396, "path_ans_precision": 0.8, "path_ans_recall": 0.2631578947368421}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.countries_spoken_in -> Tanzania\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.parent -> Ongamo-Maa Group\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> common.topic.article -> m.064klf\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.countries_spoken_in -> Kenya\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai -> common.image.size -> m.02bgrp_\n# Answer:\nMaasai", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai-jump -> common.image.size -> m.02br_p7\n# Answer:\nMaasai-jump", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai women and children -> common.image.size -> m.03qy0z9\n# Answer:\nMaasai women and children"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> common.topic.image -> WilliamFranklin\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Francis Folger Franklin -> people.person.parents -> Deborah Read\n# Answer:\nFrancis Folger Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> people.person.gender -> Male\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Sarah Franklin Bache -> people.person.children -> Benjamin Franklin Bache\n# Answer:\nSarah Franklin Bache", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Sarah Franklin Bache -> people.person.spouse_s -> m.0j4k6gy\n# Answer:\nSarah Franklin Bache", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> American literature\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Francis Folger Franklin -> people.deceased_person.place_of_burial -> Christ Church Burial Ground\n# Answer:\nFrancis Folger Franklin", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Francis Folger Franklin -> common.topic.notable_for -> g.125by3nfc\n# Answer:\nFrancis Folger Franklin", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> :Library and information science\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze"], "ground_truth": ["Deborah Read"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Old age\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Tobacco smoking\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> African American\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9g\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.risk_factors -> Old age\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9q\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.treatments -> Pancreatectomy\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Leonardo da Vinci and the Art of Sculpture: Inspiration and Invention\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Raphael and his age\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> people.profession.people_with_this_profession -> Sandro Botticelli\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper -> visual_art.artwork.art_subject -> Jesus Christ\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Holy Infants Embracing -> common.topic.image -> Holy-infs\n# Answer:\nThe Holy Infants Embracing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper -> common.topic.article -> m.0jnzf\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Walter Lowenfels -> book.author.works_written -> American voices\n# Answer:\nWalter Lowenfels", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Walter Lowenfels -> people.person.places_lived -> m.0b6wksb\n# Answer:\nWalter Lowenfels", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> influence.influence_node.influenced_by -> Melozzo da Forl\u00ec\n# Answer:\nAntonio da Correggio"], "ground_truth": ["Portrait of a Young Fianc\u00e9e", "g.121yh91r", "The Last Supper", "Ginevra de' Benci", "Bacchus", "Lady with an Ermine", "Head of a Woman", "Virgin of the Rocks", "The Virgin and Child with St Anne and St John the Baptist", "Drapery for a Seated Figure", "La belle ferronni\u00e8re", "Vitruvian Man", "g.1224tf0c", "Lucan portrait of Leonardo da Vinci", "Adoration of the Magi", "Horse and Rider", "g.1219sb0g", "g.1213jb_b", "The Virgin and Child with St. Anne", "g.120vt1gz", "Portrait of Isabella d'Este", "Madonna and Child with St Joseph", "Medusa", "Madonna of the Carnation", "Benois Madonna", "g.12215rxg", "g.1239jd9p", "St. John the Baptist", "St. Jerome in the Wilderness", "Madonna Litta", "The Baptism of Christ", "g.121wt37c", "Salvator Mundi", "The Holy Infants Embracing", "Leda and the Swan", "Portrait of a man in red chalk", "Portrait of a Musician", "Leonardo's horse", "Sala delle Asse", "g.12314dm1", "Annunciation", "The Battle of Anghiari", "Mona Lisa", "Madonna of Laroque", "Madonna of the Yarnwinder"], "ans_acc": 0.044444444444444446, "ans_hit": 1, "ans_f1": 0.07741935483870968, "ans_precission": 0.3, "ans_recall": 0.044444444444444446, "path_f1": 0.07894736842105263, "path_precision": 0.3, "path_recall": 0.045454545454545456, "path_ans_f1": 0.07741935483870968, "path_ans_precision": 0.3, "path_ans_recall": 0.044444444444444446}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vorarlberg -> base.aareas.schema.administrative_area.administrative_children -> Feldkirch District\n# Answer:\nVorarlberg", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vorarlberg -> base.aareas.schema.administrative_area.administrative_children -> Feldkirch District\n# Answer:\nVorarlberg", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vorarlberg -> base.aareas.schema.administrative_area.administrative_children -> Bludenz District\n# Answer:\nVorarlberg", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vorarlberg -> base.aareas.schema.administrative_area.administrative_children -> Bregenz District\n# Answer:\nVorarlberg", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vorarlberg -> base.aareas.schema.administrative_area.administrative_children -> Bludenz District\n# Answer:\nVorarlberg", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vorarlberg -> location.location.partially_contains -> Drei Schwestern\n# Answer:\nVorarlberg", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Burgenland -> location.administrative_division.capital -> m.0q2ddgw\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Carinthia -> location.location.partially_contains -> Coglians\n# Answer:\nCarinthia", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vorarlberg -> base.aareas.schema.administrative_area.administrative_children -> Bregenz District\n# Answer:\nVorarlberg", "# Reasoning Path:\nAustria -> location.country.capital -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.treatments -> Radiation therapy\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.treatments -> Cervical conization\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> common.topic.webpage -> m.04tnn4f\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> people.cause_of_death.people -> Yvette Wilson\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.person.gender -> Female -> medicine.risk_factor.diseases -> Ovarian cancer\n# Answer:\nFemale", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> common.topic.webpage -> m.09wbnt0\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.treatments -> Erlotinib\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> people.cause_of_death.people -> Yoko Kozakura\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> common.topic.webpage -> m.09ybml5\n# Answer:\nCervical cancer"], "ground_truth": ["Cervical cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> R\u0101hula\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_for -> g.12582gf_5\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> book.book_subject.works -> Remaking Buddhism for Medieval Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Abhiyaza\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_types -> City/Town/Village\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> location.location.time_zones -> Nepal Time Zone\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Nanda\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Thousand Buddha Mountain -> location.location.containedby -> China\n# Answer:\nThousand Buddha Mountain", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Thousand Buddha Mountain -> location.location.geolocation -> m.02_ttpj\n# Answer:\nThousand Buddha Mountain", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> location.location.geolocation -> m.0wmmlzp\n# Answer:\nBuddha Memorial Center"], "ground_truth": ["Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove -> common.topic.article -> m.029mn3\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_trwk\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> Glass\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> medicine.medical_treatment.used_to_treat -> Presbyopia\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_ty__\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.image -> A bifocal corrective eyeglasses lens\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> Waking up early\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_wj2d\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.instrument.family -> Crystallophone\n# Answer:\nGlass harmonica"], "ground_truth": ["Franklin stove", "Lightning rod", "Glass harmonica", "Bifocals"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.8, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.8, "path_ans_recall": 0.75}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpsq_\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado -> location.location.containedby -> United States, with Territories\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.contains -> Academy of Natural Therapy -> location.location.containedby -> United States of America\n# Answer:\nAcademy of Natural Therapy", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpst3\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County -> location.location.adjoin_s -> m.03jpsq_\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.us_county.hud_county_place -> Ault\n# Answer:\nWeld County"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> common.topic.notable_types -> Profession\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> people.profession.specialization_of -> Writer\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> music.group_member.instruments_played -> Piano\n# Answer:\nPiano", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> common.topic.image -> Pietro Metastasio\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> base.descriptive_names.names.descriptive_name -> m.01260py_\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> common.topic.notable_for -> g.1259l_93p\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician -> people.profession.specializations -> Composer\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> Arranger\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> Music\n# Answer:\nComposer"], "ground_truth": ["Librettist", "Composer", "Musician"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> West Germany -> location.country.currency_used -> Deutsche Mark\n# Answer:\nWest Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_used -> East German mark\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Vaduz\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_used -> Deutsche Mark\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Belgium\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> West Germany -> location.location.events -> Western Front\n# Answer:\nWest Germany", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0102xvg7 -> education.education.student -> Sigurd Burckhardt\n# Answer:\nSigurd Burckhardt", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> West Germany -> location.location.people_born_here -> Alexander Waske\n# Answer:\nWest Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Andreas Bielau\n# Answer:\nEast Germany"], "ground_truth": ["East Germany", "Switzerland", "Belgium", "Austria", "Luxembourg", "Liechtenstein", "Germany"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.5581395348837209, "ans_precission": 0.8, "ans_recall": 0.42857142857142855, "path_f1": 0.36363636363636365, "path_precision": 0.5, "path_recall": 0.2857142857142857, "path_ans_f1": 0.6990291262135921, "path_ans_precision": 0.9, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Rock music\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> common.topic.notable_types -> Musical genre\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> common.topic.notable_types -> Musical genre\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.recordings -> Chasing a Feeling\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music -> common.topic.subject_of -> Stephen Melton\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Pop rock\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> music.genre.subgenre -> Math rock\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Rock and roll\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music -> music.genre.parent_genre -> Rock music\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> .997 Radiostorm Oldies -> broadcast.content.genre -> Rock music\n# Answer:\n.997 Radiostorm Oldies"], "ground_truth": ["Pop music", "Blues rock", "Psychedelic rock", "Soft rock", "Experimental rock", "Experimental music", "Pop rock", "Art rock", "Rock music"], "ans_acc": 0.5555555555555556, "ans_hit": 1, "ans_f1": 0.48648648648648646, "ans_precission": 0.9, "ans_recall": 0.3333333333333333, "path_f1": 0.48648648648648646, "path_precision": 0.9, "path_recall": 0.3333333333333333, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 1.0, "path_ans_recall": 0.5555555555555556}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> government.government_position_held.office_holder -> Michael Bennet\n# Answer:\nMichael Bennet", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.office_holder -> Mark Udall\n# Answer:\nMark Udall", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr -> government.government_position_held.office_holder -> Wayne Allard\n# Answer:\nWayne Allard", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nBasic title", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.legislative_sessions -> 111th United States Congress\n# Answer:\n111th United States Congress", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008 -> common.topic.image -> Mark Udall\n# Answer:\nUnited States Senate election in Colorado, 2008", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> freebase.valuenotation.is_reviewed -> District represented (if position is district-related)\n# Answer:\nDistrict represented (if position is district-related)", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008 -> government.election.general_election -> United States Senate elections, 2008\n# Answer:\nUnited States Senate election in Colorado, 2008"], "ground_truth": ["Mark Udall", "Michael Bennet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Capital Region of Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Nordic countries\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries -> location.location.contains -> Denmark\n# Answer:\nNordic countries", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Central Denmark Region\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Northern Europe\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> g.1hhc37psk\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> North Denmark Region\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.form_of_government -> Parliamentary system\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries -> location.location.contains -> \u00c5land Islands\n# Answer:\nNordic countries", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Europe\n# Answer:\nDenmark"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_for -> g.125h4fcxl\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> common.topic.notable_for -> g.125f2tsfn\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> g.11b66b70n7\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_types -> Postal Code\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98103 -> common.topic.notable_types -> Postal Code\n# Answer:\n98103", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.location.contains -> 1111 Third Avenue\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> location.location.geometry -> m.055f4wk\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> g.11btt54h7d\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> location.location.geolocation -> m.03dyr0d\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98103 -> location.location.geometry -> m.055f4q4\n# Answer:\n98103"], "ground_truth": ["98148", "98138", "98121", "98124", "98144", "98139", "98160", "98158", "98102", "98164", "98115", "98125", "98101", "98136", "98126", "98104", "98132", "98178", "98181", "98195", "98145", "98103", "98122", "98119", "98113", "98198", "98134", "98105", "98155", "98117", "98131", "98119-4114", "98188", "98174", "98166", "98170", "98111", "98185", "98190", "98168", "98109", "98107", "98194", "98112", "98133", "98184", "98114", "98127", "98171", "98146", "98106", "98161", "98165", "98116", "98118", "98175", "98177", "98141", "98199", "98108", "98191", "98154", "98129"], "ans_acc": 0.047619047619047616, "ans_hit": 1, "ans_f1": 0.0898876404494382, "ans_precission": 0.8, "ans_recall": 0.047619047619047616, "path_f1": 0.0898876404494382, "path_precision": 0.8, "path_recall": 0.047619047619047616, "path_ans_f1": 0.0898876404494382, "path_ans_precision": 0.8, "path_ans_recall": 0.047619047619047616}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Robsol Pinkett, Jr.\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.children -> Jaden Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Adrienne Banfield-Jones\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.parents -> Willard Christopher Smith, Sr.\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> common.topic.image -> Pinkett-Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.parents -> Caroline Bright\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Pop music -> common.topic.subject_of -> Mamiboys\n# Answer:\nPop music", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Pop music -> base.schemastaging.music_genre_concept.artists -> Yves Bole\n# Answer:\nPop music", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> tv.tv_actor.guest_roles -> m.09nbq82\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Pop music -> common.topic.subject_of -> Felt Tip\n# Answer:\nPop music"], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Traditional Chinese characters -> language.language_writing_system.parent_writing_systems -> Chinese characters\n# Answer:\nTraditional Chinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_types -> Language Writing System\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Traditional Chinese characters -> language.language_writing_system.languages -> Yue Chinese\n# Answer:\nTraditional Chinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> language.language_writing_system.languages -> Uyghur Language\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> book.book_subject.works -> Flags of Our Fathers\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Traditional Chinese characters -> base.schemastaging.context_name.pronunciation -> g.125_q7_ld\n# Answer:\nTraditional Chinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_for -> g.1258512fl\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> language.language_writing_system.languages -> Chinese, Hakka Language\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> base.schemastaging.context_name.pronunciation -> g.125_l82wv\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> education.field_of_study.students_majoring -> m.0104b7h1 -> education.education.degree -> PhD\n# Answer:\nPhD"], "ground_truth": ["Simplified Chinese character", "N\u00fcshu script", "'Phags-pa script", "Traditional Chinese characters", "Chinese characters"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6461538461538462, "ans_precission": 0.7, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.6461538461538462, "path_ans_precision": 0.7, "path_ans_recall": 0.6}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> people.person.children -> Christopher Nixon Cox\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> people.person.spouse_s -> m.0j4kt_s\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.spouse_s -> m.0j4k1q4\n# Answer:\nJulie Nixon Eisenhower", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> film.person_or_entity_appearing_in_film.films -> m.0vpghfz\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.spouse -> Pat Nixon\n# Answer:\nPat Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.location_of_ceremony -> The Mission Inn Hotel & Spa\n# Answer:\nThe Mission Inn Hotel & Spa", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.children -> Alexander Richard Eisenhower\n# Answer:\nJulie Nixon Eisenhower", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.employment_history -> m.0k0dcyp\n# Answer:\nJulie Nixon Eisenhower", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage"], "ground_truth": ["Pat Nixon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.11764705882352941, "path_precision": 0.1, "path_recall": 0.14285714285714285, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.ceremony -> 37th Primetime Emmy Awards\n# Answer:\n37th Primetime Emmy Awards", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Supporting Actress in a Comedy Series\n# Answer:\nPrimetime Emmy Award for Outstanding Supporting Actress in a Comedy Series", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Supporting Actress in a Comedy Series\n# Answer:\nPrimetime Emmy Award for Outstanding Supporting Actress in a Comedy Series", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.ceremony -> 35th Primetime Emmy Awards\n# Answer:\n35th Primetime Emmy Awards", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.award -> NAACP Image Award for Outstanding Actress in a Comedy Series\n# Answer:\nNAACP Image Award for Outstanding Actress in a Comedy Series", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 0 -> tv.tv_series_season.episodes -> Entertainment Tonight: Whatever Happened To...\n# Answer:\nThe Jeffersons - Season 0", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.ceremony -> 16th NAACP Image Awards\n# Answer:\n16th NAACP Image Awards"], "ground_truth": ["Jay Hammer", "Franklin Cover", "Zara Cully", "Mike Evans", "Isabel Sanford", "Roxie Roker", "Berlinda Tolbert", "Paul Benedict", "Marla Gibbs", "Damon Evans", "Sherman Hemsley"], "ans_acc": 0.09090909090909091, "ans_hit": 1, "ans_f1": 0.13953488372093023, "ans_precission": 0.3, "ans_recall": 0.09090909090909091, "path_f1": 0.08695652173913043, "path_precision": 0.3, "path_recall": 0.05084745762711865, "path_ans_f1": 0.13953488372093023, "path_ans_precision": 0.3, "path_ans_recall": 0.09090909090909091}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.periodical.first_issue_date -> m.02npn31\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Street Sheet -> book.periodical.format -> m.02npbt7\n# Answer:\nStreet Sheet", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.publication.contents -> m.0znkynv\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> The Golden Era -> common.topic.article -> m.04zg6d\n# Answer:\nThe Golden Era", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Street Sheet -> book.newspaper.owner -> Coalition on Homelessness\n# Answer:\nStreet Sheet", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> book.written_work.subjects -> California\n# Answer:\n1906", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> The Golden Era -> common.topic.notable_for -> g.1258tnwlj\n# Answer:\nThe Golden Era", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94101 -> location.postal_code.country -> United States of America\n# Answer:\n94101", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> book.written_work.subjects -> Novel\n# Answer:\n1906", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> common.topic.notable_for -> g.1258179ql\n# Answer:\n1906"], "ground_truth": ["San Francisco Chronicle", "California Star", "San Francisco News-Call Bulletin Newspaper", "Free Society", "San Francisco Bay Times", "Synapse", "Bay Area Reporter", "San Francisco Daily", "San Francisco Foghorn", "Street Sheet", "San Francisco Bay View", "The San Francisco Examiner", "Dock of the Bay", "San Francisco Business Times", "San Francisco Call", "The Golden Era", "The Daily Alta California", "AsianWeek", "Sing Tao Daily", "San Francisco Bay Guardian"], "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0.24, "ans_precission": 0.6, "ans_recall": 0.15, "path_f1": 0.24, "path_precision": 0.6, "path_recall": 0.15, "path_ans_f1": 0.24, "path_ans_precision": 0.6, "path_ans_recall": 0.15}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.partially_contains -> Didi Abuli\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Denmark\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> base.locations.continents.countries_within -> Denmark\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> location.location.partially_contains -> Didi Abuli\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.containedby -> Eurasia -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurasia", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Georgia\n# Answer:\nEurope"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w0hpl\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w4lwz\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> Verapamil\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> heart attack\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09wjtbj\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> ACE inhibitor\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Tobacco smoking\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> Acebutolol\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Diabetes mellitus\n# Answer:\nheart attack"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 3: 1844-1846\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 5: 1851-1855\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin -> book.author.works_written -> The Autobiography of Charles Darwin\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Anne Darwin -> people.person.parents -> Emma Darwin\n# Answer:\nAnne Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nEngland"], "ground_truth": ["The Correspondence of Charles Darwin, Volume 18: 1870", "Human nature, Darwin's view", "The foundations of the Origin of species", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The education of Darwin", "The Correspondence of Charles Darwin, Volume 15: 1867", "The Correspondence of Charles Darwin, Volume 8: 1860", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Insectivorous Plants", "The Different Forms of Flowers on Plants of the Same Species", "Motsa ha-minim", "Darwinism stated by Darwin himself", "Opsht\u0323amung fun menshen", "A Darwin Selection", "The Variation of Animals and Plants under Domestication", "Geological Observations on South America", "Leben und Briefe von Charles Darwin", "The Correspondence of Charles Darwin, Volume 9: 1861", "Charles Darwin's natural selection", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Gesammelte kleinere Schriften", "Darwin en Patagonia", "Charles Darwin", "Charles Darwin on the routes of male humble bees", "Darwin for Today", "Evolution by natural selection", "The\u0301orie de l'e\u0301volution", "From so simple a beginning", "The Correspondence of Charles Darwin, Volume 12: 1864", "Darwin's notebooks on transmutation of species", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "Darwin from Insectivorous Plants to Worms", "Questions about the breeding of animals", "Het uitdrukken van emoties bij mens en dier", "The Formation of Vegetable Mould through the Action of Worms", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "vari\u00eberen der huisdieren en cultuurplanten", "On a remarkable bar of sandstone off Pernambuco", "The Correspondence of Charles Darwin, Volume 10: 1862", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Les r\u00e9cifs de corail, leur structure et leur distribution", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "The Correspondence of Charles Darwin, Volume 14: 1866", "The Correspondence of Charles Darwin, Volume 17: 1869", "Tesakneri tsagume\u030c", "The geology of the voyage of H.M.S. Beagle", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "A student's introduction to Charles Darwin", "Works", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Voyage d'un naturaliste autour du monde", "Diario del Viaje de Un Naturalista Alrededor", "Darwin", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The portable Darwin", "ontstaan der soorten door natuurlijke teeltkeus", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Autobiography of Charles Darwin", "Metaphysics, Materialism, & the evolution of mind", "Darwin's Ornithological notes", "monograph on the sub-class Cirripedia", "More Letters of Charles Darwin", "On the tendency of species to form varieties", "Les mouvements et les habitudes des plantes grimpantes", "The Darwin Reader Second Edition", "Rejse om jorden", "The Correspondence of Charles Darwin, Volume 16: 1868", "The Power of Movement in Plants", "Part I: Contributions to the Theory of Natural Selection / Part II", "Cartas de Darwin 18251859", "Wu zhong qi yuan", "Darwin Darwin", "La facult\u00e9 motrice dans les plantes", "The Correspondence of Charles Darwin, Volume 11: 1863", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The Life and Letters of Charles Darwin Volume 2", "La vie et la correspondance de Charles Darwin", "Fertilisation of Orchids", "The living thoughts of Darwin", "El Origin De Las Especies", "Die fundamente zur entstehung der arten", "The Descent of Man, and Selection in Relation to Sex", "Del Plata a Tierra del Fuego", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Reise um die Welt 1831 - 36", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "From Darwin's unpublished notebooks", "red notebook of Charles Darwin", "Origins", "The action of carbonate of ammonia on the roots of certain plants", "Darwin-Wallace", "The voyage of Charles Darwin", "Die geschlechtliche Zuchtwahl", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Proiskhozhdenie vidov", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "On Natural Selection", "Les moyens d'expression chez les animaux", "The Essential Darwin", "The Correspondence of Charles Darwin, Volume 13: 1865", "Charles Darwin's marginalia", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Darwin's insects", "Beagle letters", "To the members of the Down Friendly Club", "Darwin's journal", "Diary of the voyage of H.M.S. Beagle", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Evolutionary Writings: Including the Autobiographies", "South American Geology", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Evolution and natural selection", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "Darwin Compendium", "The collected papers of Charles Darwin", "H.M.S. Beagle in South America", "The Darwin Reader First Edition", "On evolution", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "genese\u014ds t\u014dn eid\u014dn", "Darwin and Henslow", "Resa kring jorden", "On the origin of species by means of natural selection", "Geological Observations on the Volcanic Islands", "The Structure and Distribution of Coral Reefs", "The Expression of the Emotions in Man and Animals", "Kleinere geologische Abhandlungen", "Reise eines Naturforschers um die Welt", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "The Orgin of Species", "Memorias y epistolario i\u0301ntimo", "Evolution", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "The principal works", "On the Movements and Habits of Climbing Plants", "The Life and Letters of Charles Darwin Volume 1", "The Life of Erasmus Darwin", "Volcanic Islands", "Charles Darwin's letters", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Notes on the fertilization of orchids", "Notebooks on transmutation of species", "The Voyage of the Beagle", "Darwin on humus and the earthworm"], "ans_acc": 0.05228758169934641, "ans_hit": 1, "ans_f1": 0.025586353944562903, "ans_precission": 0.6, "ans_recall": 0.013071895424836602, "path_f1": 0.3636363636363636, "path_precision": 1.0, "path_recall": 0.2222222222222222, "path_ans_f1": 0.09937888198757763, "path_ans_precision": 1.0, "path_ans_recall": 0.05228758169934641}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> location.administrative_division.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.0gmjl4l -> tv.tv_guest_role.episodes_appeared_in -> Day of the Moon (2)\n# Answer:\nDay of the Moon (2)", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.film -> End of the Road: How Money Became Worthless\n# Answer:\nEnd of the Road: How Money Became Worthless", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15 -> film.personal_film_appearance.film -> Sicko\n# Answer:\nSicko", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsgl5 -> tv.tv_guest_role.episodes_appeared_in -> The Future of the GOP\n# Answer:\nThe Future of the GOP", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15 -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglb -> tv.tv_guest_role.episodes_appeared_in -> The American Film Institute Salute to James Cagney\n# Answer:\nThe American Film Institute Salute to James Cagney", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.0_0cs2_ -> film.personal_film_appearance.film -> Our Nixon\n# Answer:\nOur Nixon"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.size -> m.0kjrkq\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.appears_in_topic_gallery -> Zubin Mehta\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.appears_in_topic_gallery -> Grammy Legend Award\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.subjects -> Bleona\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.size -> m.02cljr8\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> Little Miss Sweetness -> music.recording.releases -> Gettin' Ready\n# Answer:\nLittle Miss Sweetness", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Music executive -> common.topic.article -> m.047rgq1\n# Answer:\nMusic executive", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> I've Been Good to You -> music.composition.recordings -> I've Been Good to You (extended mix)\n# Answer:\nI've Been Good to You", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> After All -> music.composition.recordings -> After All (stereo mix)\n# Answer:\nAfter All"], "ground_truth": ["Everything for Christmas", "I Hear The Children Singing", "Bad Girl", "I Can't Get Enough", "Come by Here (Kum Ba Ya)", "Love Bath", "It's a Good Feeling", "Theme From the Big Time", "Quiet Storm (Groove Boutique Chill Jazz mix)", "Virgin Man", "The Love Between Me and My Kids", "Why", "Be Careful What You Wish For", "One Heartbeat", "If You Want My Love", "Let Me Be the Clock", "You Really Got a Hold on Me", "Take Me Through The Night", "Christmas Every Day", "Share It", "Come to Me Soon", "Gone Forever", "Will You Love Me Tomorrow", "You Don't Know What It's Like", "She's Only a Baby Herself", "Ever Had A Dream", "When A Woman Cries", "Cruisin", "Love So Fine", "There Will Come a Day (I'm Gonna Happen to You)", "Hanging on by a Thread", "Just My Soul Responding", "Being With You", "Walk on By", "I Second That Emotion", "Tears Of A Clown", "My Guy", "Close Encounters of the First Kind", "Let Me Be The Clock", "Why Are You Running From My Love", "Yester Love", "I Can't Give You Anything but Love", "Crusin'", "We\u2019ve Come Too Far to End It Now", "Whatcha Gonna Do", "Deck the Halls", "You Go to My Head", "A Child Is Waiting", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "Our Love Is Here to Stay", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "Wishful Thinking", "Girlfriend", "A Silent Partner in a Three-Way Love Affair", "Away in the Manger / Coventry Carol", "We Are The Warriors", "And I Don't Love You (Larry Levan instrumental dub)", "Be Kind To The Growing Mind (with The Temptations)", "Cruisin'", "Love Don' Give No Reason (12 Inch Club Mix)", "Heavy On Pride (Light On Love)", "Don't Play Another Love Song", "Why Do Happy Memories Hurt So Bad", "Going to a Gogo", "It's Fantastic", "I Am, I Am", "Ooo Baby Baby (live)", "Be Careful What You Wish For (instrumental)", "The Tracks of My Tears (live)", "Save Me", "Ebony Eyes (Duet with Rick James)", "Going to a Go-Go", "The Tracks of My Heart", "Be Who You Are", "The Tears Of A Clown", "I Love Your Face", "Blame It On Love (Duet with Barbara Mitchell)", "God Rest Ye Merry Gentlemen", "Will You Love Me Tomorrow?", "I Am I Am", "Vitamin U", "Don't Wanna Be Just Physical", "You Take Me Away", "Tracks Of My Tears (Live)", "Will You Still Love Me Tomorrow", "The Tears of a Clown", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "Melody Man", "Please Come Home for Christmas", "The Track of My Tears", "I Can't Find", "(It's The) Same Old Love", "The Agony And The Ecstasy", "Really Gonna Miss You", "Driving Thru Life in the Fast Lane", "Skid Row", "Little Girl, Little Girl", "Wanna Know My Mind", "Baby Come Close", "He Can Fix Anything", "Did You Know (Berry's Theme)", "I\u2019ve Got You Under My Skin", "The Agony and the Ecstasy", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "Winter Wonderland", "And I Don't Love You", "The Hurt's On You", "I Like Your Face", "If You Wanna Make Love (Come 'round Here)", "I've Made Love To You A Thousand Times", "Quiet Storm (single version)", "Tracks of my Tears", "Just Passing Through", "I Praise & Worship You Father", "I Know You by Heart", "With Your Love Came", "Gang Bangin'", "Speak Low", "One Time", "You've Really Go a Hold on Me", "A Tattoo", "There Will Come A Day ( I'm Gonna Happen To You )", "Christmas Everyday", "I Can\u2019t Stand to See You Cry (Commercial version)", "We've Saved The Best For Last (Kenny G with Smokey Robinson)", "Fallin'", "Hold on to Your Love", "Let Your Light Shine On Me", "Baby That's Backatcha", "Tears of a Sweet Free Clown", "I Want You Back", "Shop Around", "Ooo Baby Baby", "You're Just My Life (feat. India.Arie)", "Girl I'm Standing There", "Don't Know Why", "You Are Forever", "Tell Me Tomorrow (12\\\" extended mix)", "Tell Me Tomorrow, Part 1", "It's Time to Stop Shoppin' Around", "Who's Sad", "Tracks of My Tears", "Little Girl Little Girl", "The Way You Do (The Things You Do)", "If You Can Want", "Can't Fight Love", "Holly", "You've Really Got a Hold on Me", "Tea for Two", "Pops, We Love You", "Jingle Bells", "It's Her Turn to Live", "I've Got You Under My Skin", "Yes It's You Lady", "Wedding Song", "If You Wanna Make Love", "In My Corner", "I'll Keep My Light In My Window", "Photograph in My Mind", "More Love", "Sweet Harmony", "Te Quiero Como Si No Hubiera Un Manana", "I'm Glad There Is You", "Love Brought Us Here", "Please Don't Take Your Love (feat. Carlos Santana)", "Tell Me Tomorrow", "Ooh Baby Baby", "Time After Time", "Never My Love / Never Can Say Goodbye", "The Tracks of My Tears", "My Girl", "When Smokey Sings Tears Of A Clown", "Unless You Do It Again", "Crusin", "More Than You Know", "You Are So Beautiful (feat. Dave Koz)", "Christmas Greeting", "I Care About Detroit", "Some People Will Do Anything for Love", "No\u00ebl", "Everything You Touch", "Medley: Never My Love / Never Can Say Goodbye", "Pops, We Love You (disco)", "Coincidentally", "Santa Claus is Coming to Town", "Get Ready", "As You Do", "Noel", "Asleep on My Love", "It's Christmas Time", "Double Good Everything", "My World", "Standing On Jesus", "Ain't That Peculiar", "Jasmin", "Just Another Kiss", "No Time to Stop Believing", "I've Made Love to You a Thousand Times", "And I Love Her", "Ebony Eyes", "Happy (Love Theme From Lady Sings the Blues)", "Time Flies", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "I'm in the Mood for Love", "The Christmas Song", "Just a Touch Away", "Fulfill Your Need", "Night and Day", "Mickey's Monkey", "Love' n Life", "Be Kind to the Growing Mind", "Rack Me Back", "You Made Me Feel Love", "Love Letters", "Train of Thought", "We've Saved the Best for Last", "I Have Prayed On It", "Jesus Told Me To Love You", "Just to See Her", "Shoe Soul", "That Place", "Going to a Go Go", "The Road to Damascus", "Tears of a Clown", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "Because of You It's the Best It's Ever Been", "Just To See Her Again", "Just Like You", "Food For Thought", "You're the One for Me (feat. Joss Stone)", "It's A Good Night", "The Tracks Of My Tears", "Love Don't Give No Reason", "Quiet Storm", "Sleepless Nights", "Open", "Fly Me to the Moon (In Other Words)", "Easy", "Keep Me", "You Cannot Laugh Alone", "Season's Greetings from Smokey Robinson", "Aqui Con Tigo (Being With You)", "Rewind", "Quiet Storm (Groove Boutique remix)", "I Love The Nearness Of You", "What's Too Much", "Daylight & Darkness", "Mother's Son", "Satisfy You", "Blame It on Love", "I Second That Emotions", "Love Is The Light", "The Family Song", "Same Old Love", "So Bad", "Nearness of You"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Yad Kennedy -> common.topic.notable_types -> Structure\n# Answer:\nYad Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> White people -> medicine.risk_factor.diseases -> Breast cancer\n# Answer:\nWhite people", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Yad Kennedy -> common.topic.article -> m.0262gn5\n# Answer:\nYad Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of death\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Yad Kennedy -> location.location.geolocation -> m.0cr0pmw\n# Answer:\nYad Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School -> common.topic.notable_for -> g.125583hnb\n# Answer:\nBloomington Kennedy High School", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nIrish American"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> South Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Serbia and Montenegro\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> common.topic.notable_types -> Human Language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Bosnia and Herzegovina\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Torlakian dialect -> common.topic.image -> Torlak\n# Answer:\nTorlakian dialect", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> time.event.locations -> Czech Republic\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Torlakian dialect -> common.topic.image -> Balkan dialects belic 1914\n# Answer:\nTorlakian dialect"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
