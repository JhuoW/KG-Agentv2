{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Costa Rica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.region -> Americas\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Louisiana\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1900 Galveston hurricane -> meteorology.tropical_cyclone.affected_areas -> Atlantic Canada\n# Answer:\n1900 Galveston hurricane"], "ground_truth": ["Jamaican Creole English Language", "Jamaican English"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nm.04j60kh", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.district_represented -> Tennessee's 9th congressional district\n# Answer:\nm.04j60kh", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8 -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nm.04469y8", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j5sk8 -> government.government_position_held.office_position_or_title -> Governor of Tennessee\n# Answer:\nm.04j5sk8", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> United States Representative\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> freebase.type_profile.kind -> Title\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Representative\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> freebase.type_hints.included_types -> Topic\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8 -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nm.04469y8"], "ground_truth": ["Speaker of the United States House of Representatives", "Governor of Tennessee", "United States Representative"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5, "path_ans_precision": 0.4, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.main_country -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.countries_spoken_in -> Cuba\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_1j97\n# Answer:\ng.1245_1j97", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> Martinique\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French -> language.human_language.main_country -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.countries_spoken_in -> Dominican Republic\n# Answer:\nHaitian Creole"], "ground_truth": ["French", "Haitian Creole"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> common.topic.notable_for -> g.12596ymdk\n# Answer:\ng.12596ymdk", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t8hv2 -> tv.regular_tv_appearance.actor -> Yvonne Craig\n# Answer:\nm.02t8hv2", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t91b7 -> tv.regular_tv_appearance.actor -> Melissa Gilbert\n# Answer:\nm.02t91b7", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> film.film_character.portrayed_in_films -> m.012hbb3h\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t91b7 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nm.02t91b7", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.03ljbs2 -> tv.regular_tv_appearance.actor -> Angie Harmon\n# Answer:\nm.03ljbs2", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> book.book_subject.works -> A Cold Christmas\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer -> base.descriptive_names.names.descriptive_name -> m.0101pjk9\n# Answer:\nLawyer", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> fictional_universe.fictional_job_title.fictional_characters_with_this_job -> m.0110s7g9\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> book.book_subject.works -> A Darkness More Than Night\n# Answer:\nDetective"], "ground_truth": ["Hannah Gunn", "Ilyssa Fradin", "Melinda McGraw"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nm.04qjyzm", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qpksh -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nm.04qpksh", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.season -> 1992\u201393 NBA season\n# Answer:\nm.04qjyzm", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nm.04q9532", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.team -> Orlando Magic\n# Answer:\nm.02kbc3w", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player -> common.topic.notable_for -> g.125ctmqvt\n# Answer:\nPlayer", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qpksh -> basketball.basketball_player_stats.season -> 1994\u201395 NBA season\n# Answer:\nm.04qpksh", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.season -> 2003\u201304 NBA season\n# Answer:\nm.04q9532", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc58 -> sports.sports_award.team -> Los Angeles Lakers\n# Answer:\nm.02kbc58", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player -> music.recording.releases -> You cant STOP the REIGN\n# Answer:\nPlayer"], "ground_truth": ["Cleveland Cavaliers", "Phoenix Suns", "LSU Tigers men's basketball", "Boston Celtics", "Miami Heat", "Orlando Magic", "Los Angeles Lakers"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.5, "path_recall": 0.16666666666666666, "path_ans_f1": 0.36363636363636365, "path_ans_precision": 0.5, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> travel.travel_destination.tourist_attractions -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> location.location.contains -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9 -> award.award_honor.award -> Emmy Award for Outstanding Variety, Music or Comedy Series\n# Answer:\nm.04fk_g9", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.04lybjz\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Chandler Muriel Bing\n# Answer:\nScottish American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9 -> award.award_honor.honored_for -> The Tonight Show with Jay Leno\n# Answer:\nm.04fk_g9", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.0n2qyfk -> award.award_honor.award -> People's Choice Award for Favorite Late Night Talk Show Host\n# Answer:\nm.0n2qyfk", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.image -> Mulberry Street NYC c1900 LOC 3g04637u edit\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.0cqc3ql -> award.award_honor.award_winner -> Bill Royce\n# Answer:\nm.0cqc3ql"], "ground_truth": ["New Rochelle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.main_country -> Myanmar\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> media_common.netflix_genre.titles -> Shutter\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.region -> Asia\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Lao Language -> language.human_language.countries_spoken_in -> Laos\n# Answer:\nLao Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language -> common.topic.notable_types -> Human Language\n# Answer:\nVietnamese Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> language.human_language.region -> Asia\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yg\n# Answer:\ng.12tb6f6yg", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language -> language.human_language.main_country -> Vietnam\n# Answer:\nVietnamese Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Lao Language -> language.human_language.region -> Asia\n# Answer:\nLao Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> common.topic.notable_types -> Human Language\n# Answer:\nThai Language"], "ground_truth": ["Malay, Pattani Language", "Nyaw Language", "Mon Language", "Hmong language", "Khmer language", "Lao Language", "Akha Language", "Thai Language", "Saek language", "Phu Thai language", "Cham language", "Vietnamese Language", "Mlabri Language"], "ans_acc": 0.3076923076923077, "ans_hit": 1, "ans_f1": 0.45859872611464964, "ans_precission": 0.9, "ans_recall": 0.3076923076923077, "path_f1": 0.33333333333333337, "path_precision": 0.6, "path_recall": 0.23076923076923078, "path_ans_f1": 0.45859872611464964, "path_ans_precision": 0.9, "path_ans_recall": 0.3076923076923077}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Programmer\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Entrepreneur\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.owner -> Mark Zuckerberg\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.parents -> Edward Zuckerberg\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Businessperson\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.parents -> Karen Kempner\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> influence.influence_node.influenced -> Austin Cameron\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.owner -> Facebook, Inc.\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.category -> Social networking service\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> influence.influence_node.influenced -> Paul Scolnick\n# Answer:\nMark Zuckerberg"], "ground_truth": ["Tyler Winklevoss", "Cameron Winklevoss"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Statesman -> common.topic.notable_types -> Profession\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> common.topic.notable_types -> Profession\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> fictional_universe.character_occupation.characters_with_this_occupation -> Indiana Jones\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Farmer -> common.topic.notable_types -> Profession\n# Answer:\nFarmer", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Statesman -> people.profession.specialization_of -> Politician\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> people.profession.specialization_of -> Scientist\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Philosopher -> common.topic.notable_types -> Profession\n# Answer:\nPhilosopher", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Polly Jefferson\n# Answer:\nPolly Jefferson", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician"], "ground_truth": ["Philosopher", "Inventor", "Teacher", "Lawyer", "Writer", "Archaeologist", "Architect", "Farmer", "Author", "Statesman"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.509090909090909, "ans_precission": 0.7, "ans_recall": 0.4, "path_f1": 0.37499999999999994, "path_precision": 0.5, "path_recall": 0.3, "path_ans_f1": 0.509090909090909, "path_ans_precision": 0.7, "path_ans_recall": 0.4}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 11: 1863\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Great Britain -> location.location.contains -> London\n# Answer:\nGreat Britain", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Darwin -> book.written_work.subjects -> Biology\n# Answer:\nDarwin"], "ground_truth": ["The Autobiography of Charles Darwin [EasyRead Edition]", "Tesakneri tsagume\u030c", "On the origin of species by means of natural selection", "The origin of species : complete and fully illustrated", "The voyage of the Beagle.", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The structure and distribution of coral reefs.", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "Cartas de Darwin 18251859", "Darwin Darwin", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "The Power of Movement in Plants", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Questions about the breeding of animals", "Part I: Contributions to the Theory of Natural Selection / Part II", "The Voyage of the Beagle (Adventure Classics)", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "Human nature, Darwin's view", "The Correspondence of Charles Darwin, Volume 9: 1861", "The Correspondence of Charles Darwin, Volume 2", "Darwin", "Fertilisation of Orchids", "Autobiography of Charles Darwin", "Darwin's notebooks on transmutation of species", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Darwin and Henslow", "On the Movements and Habits of Climbing Plants", "The Correspondence of Charles Darwin, Volume 8", "Beagle letters", "From So Simple a Beginning", "Notes on the fertilization of orchids", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "Opsht\u0323amung fun menshen", "The Descent of Man and Selection in Relation to Sex", "The education of Darwin", "The Expression of the Emotions in Man And Animals", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "A student's introduction to Charles Darwin", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "monograph on the sub-class Cirripedia", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "The Origin of Species (Great Books : Learning Channel)", "Darwin Compendium", "The Voyage of the Beagle (Mentor)", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Gesammelte kleinere Schriften", "Insectivorous Plants", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "The Origin of Species (Great Minds Series)", "The Correspondence of Charles Darwin, Volume 18: 1870", "The Origin of Species (World's Classics)", "The Voyage of the Beagle (Great Minds Series)", "Charles Darwin's letters", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Darwinism stated by Darwin himself", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "The Origin of Species (Collector's Library)", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The Correspondence of Charles Darwin, Volume 11: 1863", "The Darwin Reader First Edition", "The Expression of the Emotions in Man and Animals", "The Autobiography of Charles Darwin", "The Origin of Species (Enriched Classics)", "The Correspondence of Charles Darwin, Volume 11", "Voyage Of The Beagle", "El Origin De Las Especies", "Kleinere geologische Abhandlungen", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "The Origin Of Species", "Origin of Species", "Notebooks on transmutation of species", "The Correspondence of Charles Darwin, Volume 15", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "The Correspondence of Charles Darwin, Volume 7", "Charles Darwin's marginalia", "From Darwin's unpublished notebooks", "Reise eines Naturforschers um die Welt", "Les r\u00e9cifs de corail, leur structure et leur distribution", "Del Plata a Tierra del Fuego", "Darwin's Ornithological notes", "The Correspondence of Charles Darwin, Volume 9", "Resa kring jorden", "The Correspondence of Charles Darwin, Volume 1", "The Autobiography of Charles Darwin (Great Minds Series)", "The Correspondence of Charles Darwin, Volume 13", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "Darwin-Wallace", "Metaphysics, Materialism, & the evolution of mind", "La facult\u00e9 motrice dans les plantes", "Evolution by natural selection", "On evolution", "The geology of the voyage of H.M.S. Beagle", "The foundations of the Origin of species", "The Expression Of The Emotions In Man And Animals", "The Formation of Vegetable Mould through the Action of Worms", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "The voyage of Charles Darwin", "Memorias y epistolario i\u0301ntimo", "Evolution", "The Autobiography of Charles Darwin (Dodo Press)", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "The Correspondence of Charles Darwin, Volume 12: 1864", "Die fundamente zur entstehung der arten", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "The Variation of Animals and Plants under Domestication", "A Darwin Selection", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "The expression of the emotions in man and animals.", "The Correspondence of Charles Darwin, Volume 6", "The\u0301orie de l'e\u0301volution", "genese\u014ds t\u014dn eid\u014dn", "Darwin for Today", "The Structure and Distribution of Coral Reefs", "The collected papers of Charles Darwin", "Reise um die Welt 1831 - 36", "Origins", "Geological Observations on South America", "Les mouvements et les habitudes des plantes grimpantes", "Rejse om jorden", "vari\u00eberen der huisdieren en cultuurplanten", "The living thoughts of Darwin", "The Correspondence of Charles Darwin, Volume 14", "Volcanic Islands", "The Origin of Species (Variorum Reprint)", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "The Correspondence of Charles Darwin, Volume 17: 1869", "The descent of man, and selection in relation to sex", "The Correspondence of Charles Darwin, Volume 13: 1865", "The Structure And Distribution of Coral Reefs", "Origin of Species (Harvard Classics, Part 11)", "The expression of the emotions in man and animals", "Leben und Briefe von Charles Darwin", "The Darwin Reader Second Edition", "Die geschlechtliche Zuchtwahl", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Motsa ha-minim", "Voyage of the Beagle (Dover Value Editions)", "Origin of Species (Everyman's University Paperbacks)", "Darwin's journal", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The Correspondence of Charles Darwin, Volume 15: 1867", "Charles Darwin", "The descent of man, and selection in relation to sex.", "The Correspondence of Charles Darwin, Volume 10: 1862", "The Autobiography of Charles Darwin, and selected letters", "The Descent of Man, and Selection in Relation to Sex", "More Letters of Charles Darwin", "Works", "The principal works", "The Voyage of the Beagle (Unabridged Classics)", "The descent of man and selection in relation to sex.", "Evolution and natural selection", "Voyage d'un naturaliste autour du monde", "Charles Darwin on the routes of male humble bees", "The Origin of Species (Mentor)", "The origin of species", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "Wu zhong qi yuan", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The action of carbonate of ammonia on the roots of certain plants", "On a remarkable bar of sandstone off Pernambuco", "The Correspondence of Charles Darwin, Volume 5", "The Correspondence of Charles Darwin, Volume 3", "The Correspondence of Charles Darwin, Volume 10", "The Origin of Species", "The autobiography of Charles Darwin", "Charles Darwin's natural selection", "The Correspondence of Charles Darwin, Volume 4", "ontstaan der soorten door natuurlijke teeltkeus", "The Correspondence of Charles Darwin, Volume 14: 1866", "The Correspondence of Charles Darwin, Volume 8: 1860", "The Correspondence of Charles Darwin, Volume 12", "On Natural Selection", "The Voyage of the Beagle", "On the tendency of species to form varieties", "From so simple a beginning", "Les moyens d'expression chez les animaux", "The Life of Erasmus Darwin", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "The Autobiography Of Charles Darwin", "Darwin en Patagonia", "Voyage of the Beagle (NG Adventure Classics)", "red notebook of Charles Darwin", "Het uitdrukken van emoties bij mens en dier", "The autobiography of Charles Darwin, 1809-1882", "The Correspondence of Charles Darwin, Volume 16: 1868", "The Origin of Species (Oxford World's Classics)", "H.M.S. Beagle in South America", "La vie et la correspondance de Charles Darwin", "The Orgin of Species", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "Proiskhozhdenie vidov", "The Essential Darwin", "The Different Forms of Flowers on Plants of the Same Species", "The portable Darwin", "To the members of the Down Friendly Club", "Diary of the voyage of H.M.S. Beagle", "Diario del Viaje de Un Naturalista Alrededor", "Voyage of the Beagle", "The Autobiography of Charles Darwin (Large Print)", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "The structure and distribution of coral reefs", "The Voyage of the Beagle (Everyman Paperbacks)", "Voyage of the Beagle (Harvard Classics, Part 29)", "Darwin's insects", "Darwin on humus and the earthworm"], "ans_acc": 0.06542056074766354, "ans_hit": 1, "ans_f1": 0.018348623853211007, "ans_precission": 0.5, "ans_recall": 0.009345794392523364, "path_f1": 0.29411764705882354, "path_precision": 1.0, "path_recall": 0.1724137931034483, "path_ans_f1": 0.12280701754385964, "path_ans_precision": 1.0, "path_ans_recall": 0.06542056074766354}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nm.0hpc6gc", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hpc6gc", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.team -> New York Jets\n# Answer:\nm.04nb7z0", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0j5dkn6 -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nm.0j5dkn6", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> book.written_work.author -> Nathan Whitaker\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.notable_types -> Book\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.04nb7z0", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.article -> m.0j4d5g4\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0j5dkn6 -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0j5dkn6", "# Reasoning Path:\nTim Tebow -> award.award_nominee.award_nominations -> m.0513_pw -> award.award_nomination.award -> Heisman Trophy\n# Answer:\nm.0513_pw"], "ground_truth": ["Florida Gators football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nm.0jh9fcj", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0jh9fcj", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.07mmh5w -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\nm.07mmh5w", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0zs5mvy", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.07mmh5w -> american_football.player_rushing_statistics.team -> Indianapolis Colts\n# Answer:\nm.07mmh5w", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.0791773 -> american_football.player_rushing_statistics.season -> 2008 NFL season\n# Answer:\nm.0791773", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nm.0zs5mvy", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.07sgy3b -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\nm.07sgy3b", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.0791773 -> american_football.player_rushing_statistics.team -> Indianapolis Colts\n# Answer:\nm.0791773"], "ground_truth": ["Denver Broncos"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Serbia -> location.location.containedby -> Eurasia\n# Answer:\nSerbia", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Eastern European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.partially_contains -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Czech Republic -> location.location.containedby -> Eurasia\n# Answer:\nCzech Republic", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Babia G\u00f3ra -> location.location.containedby -> Europe\n# Answer:\nBabia G\u00f3ra", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Czech Republic\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Hungary -> location.location.containedby -> Eurasia\n# Answer:\nHungary", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Georgia\n# Answer:\nEurope"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.notable_types -> Profession\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.article -> m.0knj9\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Bard\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Poet -> media_common.quotation_subject.quotations_about_this_subject -> As imagination bodies forth The forms of things unknown, the poet's pen Turns them to shapes\n# Answer:\nPoet", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.profession -> Governess\n# Answer:\nAnne Bront\u00eb", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.image -> Hope-coventina01a\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Poet\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Cardiovascular disease\n# Answer:\nMale", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> book.book_subject.works -> Authors Frequently Mentioned on the Web\n# Answer:\nWriter"], "ground_truth": ["Writer", "Poet", "Bard", "Author"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.8, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.actor -> Hayden Christensen\n# Answer:\nm.02sg5ry", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.actor -> Hayden Christensen\n# Answer:\nm.02sg5s6", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.film -> Star Wars Episode III: Revenge of the Sith\n# Answer:\nm.02sg5ry", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.film -> Star Wars Episode II: Attack of the Clones\n# Answer:\nm.02sg5s6", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.part_of_series -> Jedi Quest\n# Answer:\nPath to Truth", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.author -> Judy Blundell\n# Answer:\nPath to Truth"], "ground_truth": ["Hayden Christensen"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.season -> 2006 NFL season\n# Answer:\nm.04kcv8b", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_8 -> sports.sports_award.season -> 2003 NFL season\n# Answer:\nm.04kg9_8", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_j -> sports.sports_award.season -> 2004 NFL season\n# Answer:\nm.04kg9_j", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.award -> Super Bowl Most Valuable Player Award\n# Answer:\nm.04kcv8b", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_8 -> sports.sports_award.award -> National Football League Most Valuable Player Award\n# Answer:\nm.04kg9_8", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_j -> sports.sports_award.award -> National Football League Most Valuable Player Award\n# Answer:\nm.04kg9_j", "# Reasoning Path:\nPeyton Manning -> people.person.spouse_s -> m.0hpd4nj -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nm.0hpd4nj", "# Reasoning Path:\nPeyton Manning -> people.person.spouse_s -> m.0hpd4nj -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0hpd4nj", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Canada\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> base.wikipedia_infobox.settlement.area_code -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Canada\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Ontario\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.administrative_divisions -> Ontario\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> common.topic.notable_types -> City/Town/Village\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Ontario\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> common.topic.notable_types -> Country\n# Answer:\nCanada"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.06vz4t9 -> military.military_combatant_group.combatants -> Argentina\n# Answer:\nm.06vz4t9", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> Australia\n# Answer:\nm.03z96c5", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z973l -> military.military_combatant_group.combatants -> Iraq\n# Answer:\nm.03z973l", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> royalty.kingdom.rulers -> Abdullah of Saudi Arabia\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> France\n# Answer:\nm.03z96c5", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wphv -> military.casualties.combatant -> United States of America\n# Answer:\nm.043wphv", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wphl -> military.casualties.combatant -> United States of America\n# Answer:\nm.043wphl", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> United Kingdom\n# Answer:\nm.03z96c5", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wpj2 -> military.casualties.combatant -> France\n# Answer:\nm.043wpj2", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> location.country.official_language -> Arabic Language\n# Answer:\nSaudi Arabia"], "ground_truth": ["France", "Saudi Arabia", "Iraq", "Argentina", "Australia", "United States of America", "United Kingdom"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.16666666666666666, "ans_precission": 0.2, "ans_recall": 0.14285714285714285, "path_f1": 0.2428330522765599, "path_precision": 0.9, "path_recall": 0.14035087719298245, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nm.040p0ym", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.actor -> Phill Lewis\n# Answer:\nm.040p0ys", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 1\n# Answer:\nm.040p0ym", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Show\n# Answer:\nm.0sgkxsb", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 2\n# Answer:\nm.040p0ym", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> award.award_nomination.ceremony -> 2011 Kids' Choice Awards\n# Answer:\nm.0sgkxsb", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkyvs -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Actor\n# Answer:\nm.0sgkyvs", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.05v3ngr -> tv.regular_tv_appearance.actor -> Erin Cardillo\n# Answer:\nm.05v3ngr", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 2\n# Answer:\nm.040p0ys", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.05v3ngr -> tv.regular_tv_appearance.character -> Emma Tutweiller\n# Answer:\nm.05v3ngr"], "ground_truth": ["Brenda Song"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.office_holder -> Rob Portman\n# Answer:\nm.0gm5f32", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nm.0gm5f32", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.office_holder -> George Voinovich\n# Answer:\nm.05kfccr", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nm.0gm5f32", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_holder -> Sherrod Brown\n# Answer:\nm.05kg_6s", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.image -> The Rhodes State Office Tower adjacent to the Statehouse\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.notable_for -> g.125h3hwcp\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> meteorology.tropical_cyclone.affected_areas -> Alabama\n# Answer:\nHurricane Bob", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nm.05kg_6s", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> common.topic.notable_for -> g.1259xs2jv\n# Answer:\nHurricane Bob"], "ground_truth": ["Return J. Meigs, Jr.", "Ted Strickland", "John Kasich"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nm.0w9plqp", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m -> soccer.football_player_stats.team -> LA Galaxy\n# Answer:\nm.0w8w79m", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9021c -> soccer.football_player_stats.team -> A.C. Milan\n# Answer:\nm.0w9021c", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nm.0j2zzj_", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nm.0k4ytw5", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0j2zzj_", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0k4ytw5", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.team -> Paris Saint-Germain F.C.\n# Answer:\nm.0qzkj58", "# Reasoning Path:\nDavid Beckham -> tv.tv_actor.guest_roles -> m.0_z851f -> tv.tv_guest_role.special_performance_type -> Him/Herself\n# Answer:\nm.0_z851f", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nm.0qzkj58"], "ground_truth": ["LA Galaxy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3529411764705882, "path_precision": 0.3, "path_recall": 0.42857142857142855, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> H\u00e9ctor Ladero\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Hidalgo\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7t8559g\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Zacuto\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> common.topic.image -> Die Plaza Mayor am Abend\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7tbpl3m\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.statistical_region.population -> g.11b674hjl7\n# Answer:\nCoronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> common.topic.article -> m.0r4xz\n# Answer:\nCoronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7vzj2hj\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> common.topic.image -> Salamanca 2008\n# Answer:\nSalamanca"], "ground_truth": ["Salamanca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nIrish American", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> Oswald's Tale\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> common.topic.notable_types -> Event\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School -> common.topic.notable_for -> g.125583hnb\n# Answer:\nBloomington Kennedy High School", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> base.culturalevent.event.entity_involved -> Lee Harvey Oswald\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Cape Canaveral -> common.topic.article -> m.0f76p\n# Answer:\nCape Canaveral", "# Reasoning Path:\nJohn F. Kennedy -> book.author.works_written -> A Nation of Immigrants -> common.topic.notable_types -> Book\n# Answer:\nA Nation of Immigrants", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> 11/22/63\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School -> education.educational_institution.school_type -> Public school\n# Answer:\nBloomington Kennedy High School", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nIrish American"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.location.containedby -> Asia\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nm.0clv1h_", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> common.topic.image -> Okuma town office\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_for -> g.125btmfy0\n# Answer:\nFukushima I \u2013 1", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Aichi Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.country.administrative_divisions -> Aichi Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> common.topic.notable_types -> City/Town/Village\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_types -> Structure\n# Answer:\nFukushima I \u2013 1", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Akita Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Aomori Prefecture\n# Answer:\nJapan"], "ground_truth": ["Okuma", "Japan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> base.aareas.schema.administrative_area.administrative_children -> Anglesey\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> base.aareas.schema.administrative_area.administrative_children -> Blaenau Gwent\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales -> location.country.first_level_divisions -> Anglesey\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Wales -> location.country.first_level_divisions -> Anglesey\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Tyrone\n# Answer:\nNorthern Ireland"], "ground_truth": ["Northern Ireland", "England", "Wales", "Scotland"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> geography.river.cities -> Denham Springs\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wg8__0 -> location.partial_containment_relationship.partially_contains -> Sabine River\n# Answer:\nm.0wg8__0", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Macon -> location.location.partially_containedby -> Arkansas\n# Answer:\nBayou Macon", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> geography.river.mouth -> Lake Maurepas\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Bartholomew -> location.location.partially_contained_by -> m.0wjpmv2\n# Answer:\nBayou Bartholomew", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Bartholomew -> common.topic.notable_for -> g.125bn2xvk\n# Answer:\nBayou Bartholomew"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.specializations -> Author\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.corresponding_type -> Author\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.specializations -> Novelist\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.nationality -> United Kingdom\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.specializations -> Poet\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.profession -> Philosopher\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> people.person.quotations -> A thing is not necessarily true because badly uttered, nor false because spoken magnificently. -> common.topic.notable_types -> Quotation\n# Answer:\nA thing is not necessarily true because badly uttered, nor false because spoken magnificently.", "# Reasoning Path:\nAugustine of Hippo -> people.person.quotations -> By faithfulness we are collected and wound up into unity within ourselves, whereas we had been scattered abroad in multiplicity. -> media_common.quotation.subjects -> Faithfulness\n# Answer:\nBy faithfulness we are collected and wound up into unity within ourselves, whereas we had been scattered abroad in multiplicity."], "ground_truth": ["Writer", "Physician", "Philosopher"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Parents\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> people.person.children -> Sean Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> freebase.valuenotation.has_value -> Parents\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> common.topic.notable_types -> Person\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> people.person.children -> Neffeteria Pugh\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> common.topic.notable_types -> Person\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Sal Gibson -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nSal Gibson", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Sal Gibson -> common.topic.notable_types -> Deceased Person\n# Answer:\nSal Gibson"], "ground_truth": ["Sal Gibson", "Francine Lons", "Leon Cole"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.location.containedby -> Middle East -> location.location.contains -> Saudi Arabia\n# Answer:\nMiddle East", "# Reasoning Path:\nEgypt -> location.location.containedby -> Middle East -> travel.travel_destination.tour_operators -> Bunnik Tours\n# Answer:\nMiddle East", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> location.location.containedby -> Middle East -> location.location.contains -> Iraq\n# Answer:\nMiddle East", "# Reasoning Path:\nEgypt -> location.location.containedby -> Middle East -> location.location.events -> Six-Day War\n# Answer:\nMiddle East", "# Reasoning Path:\nEgypt -> location.location.containedby -> Arab world -> book.book_subject.works -> As Egypt Howls and History Tweets\n# Answer:\nArab world", "# Reasoning Path:\nEgypt -> location.location.containedby -> Middle East -> location.location.contains -> Israel\n# Answer:\nMiddle East", "# Reasoning Path:\nEgypt -> location.location.containedby -> Middle East -> location.location.events -> Yom Kippur War\n# Answer:\nMiddle East", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> location.location.containedby -> Middle East -> location.location.events -> Middle East Theatre of World War II\n# Answer:\nMiddle East"], "ground_truth": ["Provisional government", "Semi-presidential system"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.16666666666666669, "ans_precission": 0.1, "ans_recall": 0.5, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 0.1, "path_ans_recall": 0.5}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.events -> Assassination of Martin Luther King, Jr.\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.containedby -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Alabama\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nm.011j_4sh", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> common.topic.article -> m.05nnh0\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.011j_4sh", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0_714v2 -> film.personal_film_appearance.film -> Alice Walker: Beauty in Truth\n# Answer:\nm.0_714v2", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Montgomery\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0gbz10_ -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.0gbz10_"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> Maryland\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Jorge Luis Borges -> influence.influence_node.influenced_by -> Ambrose Bierce\n# Answer:\nJorge Luis Borges", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Jorge Luis Borges -> influence.influence_node.influenced_by -> Arthur Schopenhauer\n# Answer:\nJorge Luis Borges", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime. -> media_common.quotation.subjects -> Christians and Christianity\n# Answer:\nA strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Jorge Luis Borges -> influence.influence_node.influenced_by -> Charles Baudelaire\n# Answer:\nJorge Luis Borges", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Jorge Luis Borges -> people.person.profession -> Author\n# Answer:\nJorge Luis Borges", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Agatha Christie -> people.person.profession -> Novelist\n# Answer:\nAgatha Christie", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong. -> common.topic.article -> m.05chc0k\n# Answer:\nA wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong."], "ground_truth": ["Baltimore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> sports.sports_facility.teams -> Atlanta Falcons\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> common.topic.notable_types -> Zoo\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Men's rings\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Women's artistic team all-around\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.geolocation -> m.0clwfck\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Zoo Atlanta -> zoos.zoo.notable_animals -> m.0h_jk68\n# Answer:\nZoo Atlanta", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> travel.transport_terminus.travel_destinations_served -> m.052zwlt\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Zoo Atlanta -> location.location.street_address -> m.010bd6fj\n# Answer:\nZoo Atlanta", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Zoo Atlanta -> zoos.zoo.notable_animals -> m.0h_jk6d\n# Answer:\nZoo Atlanta"], "ground_truth": ["Atlanta Marriott Marquis", "Atlanta Ballet", "Atlanta History Center", "Fernbank Science Center", "Georgia State Capitol", "CNN Center", "Arbor Place Mall", "Four Seasons Hotel Atlanta", "Hyatt Regency Atlanta", "Atlanta Symphony Orchestra", "Woodruff Arts Center", "Georgia World Congress Center", "Martin Luther King, Jr. National Historic Site", "Six Flags Over Georgia", "Philips Arena", "Masquerade", "Six Flags White Water", "Atlanta Jewish Film Festival", "Underground Atlanta", "Omni Coliseum", "Georgia Aquarium", "Jimmy Carter Library and Museum", "Zoo Atlanta", "Center for Puppetry Arts", "Variety Playhouse", "Atlanta Cyclorama & Civil War Museum", "Fox Theatre", "The Tabernacle", "Fernbank Museum of Natural History", "Georgia Dome", "World of Coca-Cola", "Peachtree Road Race", "Margaret Mitchell House & Museum", "Turner Field", "Cobb Energy Performing Arts Centre", "Centennial Olympic Park"], "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0.15254237288135591, "ans_precission": 0.9, "ans_recall": 0.08333333333333333, "path_f1": 0.15254237288135591, "path_precision": 0.9, "path_recall": 0.08333333333333333, "path_ans_f1": 0.15254237288135591, "path_ans_precision": 0.9, "path_ans_recall": 0.08333333333333333}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nm.0cr320w", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Adrienne Pickering\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> common.topic.notable_types -> City/Town/Village\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Alan Marshal\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.district_represented -> Electoral district of South Brisbane\n# Answer:\nm.0cr320w", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr301h -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nm.0cr301h", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Alison Mosely\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.contains -> Assumption College, Warwick\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.size -> m.05t654b\n# Answer:\nAnna Bligh crop", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr301h -> government.government_position_held.office_position_or_title -> Premier of Queensland\n# Answer:\nm.0cr301h"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ken Barlow\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> people.person.gender -> Male\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> David Barlow\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> film.actor.film -> m.0h0_mvx\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Albert Tatlock\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> common.topic.notable_for -> g.1255pdp4f\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> tv.tv_genre.programs -> 100% Senorita\n# Answer:\nChivalric romance", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> common.topic.notable_types -> Multipart TV episode\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> tv.tv_genre.programs -> Alakdana\n# Answer:\nChivalric romance", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> media_common.literary_genre.books_in_this_genre -> Amis and Amiloun\n# Answer:\nChivalric romance"], "ground_truth": ["William Roache"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.administrative_division.second_level_division_of -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> base.biblioness.bibs_location.country -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> Scotland\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#domain -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Gender -> type.property.schema -> Person\n# Answer:\nGender"], "ground_truth": ["2005"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc4pgdn\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc4y0zl\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Bunyip\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> common.topic.webpage -> m.09wby4y\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Yowie\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.statistical_region.population -> g.11bymn224r\n# Answer:\nChristmas Island", "# Reasoning Path:\nAustralian dollar -> base.ontologies.ontology_instance.equivalent_instances -> m.09knr56 -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nm.09knr56", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> common.topic.webpage -> m.09x1v2_\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.country.internet_tld -> cx\n# Answer:\nChristmas Island", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands -> location.location.time_zones -> Cocos Islands\u00a0Time Zone\n# Answer:\nCocos (Keeling) Islands"], "ground_truth": ["AUD"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\ng.11b60n70x2", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\ng.12cp_k2s4", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ll\n# Answer:\ng.1245_22ll", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22zj\n# Answer:\ng.1245_22zj", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc378pv\n# Answer:\ng.1hhc378pv", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc37hbq\n# Answer:\ng.1hhc37hbq"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> common.topic.notable_for -> g.125dysc88\n# Answer:\ng.125dysc88", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> common.topic.notable_types -> American football player\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.team -> Carolina Panthers\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0z23kt0 -> sports.sports_team_roster.team -> Auburn Tigers football\n# Answer:\nm.0z23kt0", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> people.person.children -> Cecil Newton\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hpgnsj", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> people.person.gender -> Male\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> people.person.parents -> Jackie Newton -> common.topic.notable_for -> g.125dzwcd6\n# Answer:\nJackie Newton"], "ground_truth": ["Carolina Panthers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.location.containedby -> Bethesda-Frederick-Gaithersburg, MD Metropolitan Division\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.location.containedby -> Maryland\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.containedby -> United States of America\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.rent50_1 -> m.05gcgl3\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.location.containedby -> Bethesda-Frederick-Gaithersburg, MD Metropolitan Division\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.population -> g.11b66h2c0w\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.containedby -> United States, with Territories\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.location.containedby -> Maryland\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.population -> g.11x1cfntl\n# Answer:\nFrederick County"], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nm.0lwxmy1", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> University of Alabama School of Law\n# Answer:\nm.0n1l46h", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_types -> Book Edition\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.place_of_birth -> Los Angeles\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138 -> education.education.institution -> University of Alabama\n# Answer:\nm.04hx138", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.gender -> Female\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_for -> g.125920htw\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nHarper Seven Beckham"], "ground_truth": ["Monroe County High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> book.written_work.author -> Thomas G. Alexander\n# Answer:\nUtah, the right place", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.010flwmg", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> common.topic.notable_for -> g.125dtp7bg\n# Answer:\nUtah, the right place", "# Reasoning Path:\nUtah -> book.book_subject.works -> Blossoms of faith -> common.topic.notable_for -> g.125fjpyr7\n# Answer:\nBlossoms of faith", "# Reasoning Path:\nUtah -> book.book_subject.works -> Bound for Canaan -> book.written_work.author -> Margaret Blair Young\n# Answer:\nBound for Canaan", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> government.government_position_held.office_position_or_title -> Lieutenant Governor of Utah\n# Answer:\nm.010flwmg", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010l3l5d -> freebase.valuenotation.has_no_value -> Appointed By (if Position is Appointed)\n# Answer:\nm.010l3l5d"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> freebase.valuenotation.has_value -> Siblings\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> people.person.nationality -> United States of America\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> government.politician.government_positions_held -> m.0b6b7y1\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointed_role -> Director of the Bureau of Counterterrorism\n# Answer:\nm.010p95d2", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pk -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.010l29pk", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointee -> Henry A. Crumpton\n# Answer:\nm.010p95d2", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1 -> people.appointment.appointed_role -> United States Ambassador to Angola\n# Answer:\nm.079pxt1", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079q3lx -> people.appointment.appointed_role -> United States Ambassador to Brazil\n# Answer:\nm.079q3lx", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pv -> film.personal_film_appearance.film -> All About Ann: Governor Richards of the Lone Star State\n# Answer:\nm.010l29pv", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1 -> people.appointment.appointee -> Dan Mozena\n# Answer:\nm.079pxt1"], "ground_truth": ["Gene Amondson", "Michael Peroutka", "John Kerry", "Ralph Nader"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> people.person.parents -> Ayaan Hirsi Ali\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> people.person.gender -> Male\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.spouse -> Ayaan Hirsi Ali\n# Answer:\nm.0j4jq57", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.0j4jq57", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> people.marriage.spouse -> Sue Douglas\n# Answer:\nm.0pdtjg5", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> freebase.valuenotation.has_value -> Place of birth\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0j4jq57", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nm.0pdtjg5", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.0pdtjg5"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Pacific Ocean\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> common.topic.article -> m.03660h\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.primarily_containedby -> South America\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> common.topic.notable_for -> g.1255fs0l4\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.time_zones -> Gal\u00e1pagos Time Zone\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.partially_containedby -> South America\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Pacific Ocean -> location.location.adjoin_s -> m.02_4t96\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tourist_attractions -> Gal\u00e1pagos National Park -> protected_sites.protected_site.iucn_category -> National park\n# Answer:\nGal\u00e1pagos National Park", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.partially_containedby -> Pacific Ocean\n# Answer:\nEcuador"], "ground_truth": ["Gal\u00e1pagos Province", "Ecuador", "Pacific Ocean"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber's Believe -> film.film.personal_appearances -> m.0101fs_z\n# Answer:\nJustin Bieber's Believe", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0115qhzk -> award.award_honor.award -> MTV Europe Music Voices Award\n# Answer:\nm.0115qhzk", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> freebase.valuenotation.is_reviewed -> Award category\n# Answer:\nm.010lkp2z", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> award.award_honor.ceremony -> 2014 Billboard Music Awards\n# Answer:\nm.010lkp2z", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber's Believe -> film.film.personal_appearances -> m.0101fsyr\n# Answer:\nJustin Bieber's Believe", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber's Believe -> film.film.other_crew -> m.0101ftk6\n# Answer:\nJustin Bieber's Believe", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> Eenie Meenie -> music.recording.artist -> Sean Kingston\n# Answer:\nEenie Meenie", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> award.award_honor.award -> Billboard Music Award for Top Social Artist\n# Answer:\nm.010lkp2z"], "ground_truth": ["Roller Coaster", "Never Say Never", "First Dance", "Eenie Meenie", "Change Me", "Boyfriend", "Pray", "Heartbreaker", "Right Here", "PYD", "As Long as You Love Me", "#thatPower", "Live My Life", "Thought Of You", "Never Let You Go", "Hold Tight", "All Around The World", "Bigger", "Confident", "Recovery", "Turn to You (Mother's Day Dedication)", "All Bad", "Somebody to Love", "Home to Mama", "Lolly", "Beauty And A Beat", "Wait for a Minute", "Baby", "Bad Day", "All That Matters", "Die in Your Arms"], "ans_acc": 0.03225806451612903, "ans_hit": 1, "ans_f1": 0.04878048780487805, "ans_precission": 0.1, "ans_recall": 0.03225806451612903, "path_f1": 0.04878048780487805, "path_precision": 0.1, "path_recall": 0.03225806451612903, "path_ans_f1": 0.04878048780487805, "path_ans_precision": 0.1, "path_ans_recall": 0.03225806451612903}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> common.topic.article -> m.016fc9\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> common.topic.notable_types -> Profession\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Acetylene Lamp\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> base.descriptive_names.names.descriptive_name -> m.0101fnw4\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> people.profession.corresponding_type -> Author\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> type.type.expected_by -> politician\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> fictional_universe.character_occupation.characters_with_this_occupation -> Arthur Wellesley, 1st Duke of Wellington\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Ali G\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> base.descriptive_names.names.descriptive_name -> m.0105cq_d\n# Answer:\nJournalist"], "ground_truth": ["Publisher", "Journalist", "Physician", "Writer", "Statesman"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.509090909090909, "ans_precission": 0.7, "ans_recall": 0.4, "path_f1": 0.509090909090909, "path_precision": 0.7, "path_recall": 0.4, "path_ans_f1": 0.509090909090909, "path_ans_precision": 0.7, "path_ans_recall": 0.4}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nm.0hz8v06", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.Kind_of_symbol -> State flower\n# Answer:\nm.0hz8v06", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.symbol -> Petrified wood\n# Answer:\nm.04l0cxp", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.Kind_of_symbol -> State fossil\n# Answer:\nm.04l0cxp", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w1gvc", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz -> location.location_symbol_relationship.symbol -> Mountain tree frog\n# Answer:\nm.04st6lz", "# Reasoning Path:\nArizona -> location.statistical_region.religions -> m.04403h9 -> location.religion_percentage.religion -> Pentecostalism\n# Answer:\nm.04403h9", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.resource -> ATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!\n# Answer:\nm.09w1gvc", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1l1s -> common.webpage.resource -> ATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!\n# Answer:\nm.09w1l1s", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz -> location.location_symbol_relationship.Kind_of_symbol -> State Amphibian\n# Answer:\nm.04st6lz"], "ground_truth": ["Saguaro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.location.contains -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> base.aareas.schema.administrative_area.administrative_parent -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\ng.11bv1mttmr", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.country.administrative_divisions -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> common.topic.notable_types -> Country\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\ng.11bv383dbd", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.063y0bl -> base.popstra.infidelity.victim -> Beyonc\u00e9 Knowles\n# Answer:\nm.063y0bl", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> m.010_ydxv -> music.track_contribution.track -> If I Never See Your Face Again\n# Answer:\nm.010_ydxv"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> Harrison inauguration\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\nm.02bk8zt", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Illinois\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.03qtjkt\n# Answer:\nm.03qtjkt", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> 222px-Harrison_inauguration1841.jpg\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Illinois\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Indiana\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Iowa\n# Answer:\nUnited States of America"], "ground_truth": ["1841-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> common.topic.image -> Ryan Braun\n# Answer:\nRyan Braun", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Alan Muraoka\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> California\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Chris Snail\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.03lppm1 -> common.webpage.resource -> m.0bjrztw\n# Answer:\nm.03lppm1", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Deborah Lurie\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> Los Angeles\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.0948qtd -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.0948qtd", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> Los Angeles County\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> film.person_or_entity_appearing_in_film.films -> m.0cs3c3w -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.0cs3c3w"], "ground_truth": ["Mission Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.citytown -> Suwon\n# Answer:\nm.03l9ynf", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.administrative_division.country -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.country -> South Korea\n# Answer:\nm.03l9ynf", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.containedby -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.postal_code -> 443-742\n# Answer:\nm.03l9ynf", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.contains -> 700-160\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Mahaka Media\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.contains -> 702-701\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Textron\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.contains -> 702-721\n# Answer:\nDaegu"], "ground_truth": ["Suwon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Hinduism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> common.topic.article -> m.0jfq\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.beliefs -> End time\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.article -> m.055v4j\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Jainism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.image -> Ramapir01\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> religion.deity.deity_of -> Sunni Islam\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Sikhism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> God -> common.topic.notable_for -> g.125d2mw4b\n# Answer:\nGod", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.image -> Ramapir22\n# Answer:\nRamdev Pir"], "ground_truth": ["Monotheism", "Tawhid", "Islamic view of angels", "Predestination in Islam", "Prophets in Islam", "Qiyamah", "Islamic holy books", "Mahdi", "\u1e6c\u016bb\u0101", "Masih ad-Dajjal", "God in Islam", "Sharia", "Entering Heaven alive"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.actor -> Jamie Dornan\n# Answer:\nm.0ydn3r2", "# Reasoning Path:\nChristian Grey -> common.topic.notable_for -> g.125czvn3w\n# Answer:\ng.125czvn3w", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.film -> Fifty Shades of Grey\n# Answer:\nm.0ydn3r2", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Males\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> common.topic.notable_for -> g.1255p_jgc\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Acute myeloid leukemia\n# Answer:\nMale"], "ground_truth": ["Jamie Dornan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Albert Camus\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> people.cause_of_death.parent_cause_of_death -> Infectious disease\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Anton Chekhov\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Isoniazid\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Antibiotics\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Franz Kafka\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Pyrazinamide\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion. -> common.topic.notable_types -> Quotation\n# Answer:\nA dirty joke is a sort of mental rebellion.", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> \u00c9mile Zola -> influence.influence_node.influenced -> Alexandru Macedonski\n# Answer:\n\u00c9mile Zola", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Arthur Koestler -> people.deceased_person.cause_of_death -> Parkinson's disease\n# Answer:\nArthur Koestler"], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nm.08_ksdj", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nm.0pz073c", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.office_position_or_title -> Chancellor of Germany\n# Answer:\nm.08_ksdj", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ng.1254zbtvv", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.office_position_or_title -> F\u00fchrer\n# Answer:\nm.0pz073c", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.basic_title -> F\u00fchrer\n# Answer:\nm.0pz073c", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> base.schemastaging.context_name.pronunciation -> g.125_pt37m\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.place_founded -> Nazi Germany\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.founders -> Heinrich Himmler\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> book.book_subject.works -> Anne Frank and the Children of the Holocaust\n# Answer:\nNazi Party"], "ground_truth": ["Nazi Germany"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.28571428571428564, "path_precision": 0.3, "path_recall": 0.2727272727272727, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.notable_types -> Musical Artist -> freebase.type_hints.included_types -> Topic\n# Answer:\nMusical Artist", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> common.topic.notable_types -> Profession\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.specialization_of -> Musician\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.notable_types -> Musical Artist -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nMusical Artist", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.specialization_of -> Musician\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.notable_types -> Musical Artist -> freebase.type_profile.equivalent_topic -> Musician\n# Answer:\nMusical Artist", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> common.topic.notable_types -> Profession\n# Answer:\nSongwriter"], "ground_truth": ["Songwriter", "Singer", "Actor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> location.location.containedby -> United States of America\n# Answer:\nKansas", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.statistical_region.co2_emissions_mobile -> m.045l26t\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111 -> location.location.containedby -> Wyandotte County\n# Answer:\n66111", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.location.containedby -> Kansas City, MO-KS Metropolitan Statistical Area\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nKansas", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas City, MO-KS Metropolitan Statistical Area\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.statistical_region.co2_emissions_mobile -> m.045l26t\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.statistical_region.population -> g.11b674pwdp\n# Answer:\nWyandotte County"], "ground_truth": ["Wyandotte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h -> sports.sports_team_roster.team -> UCLA Bruins football\n# Answer:\nm.0hpgh_h", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nm.06sbpzt", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nm.06sbq0l", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.season -> 1954 Major League Baseball season\n# Answer:\nm.06sbpzt", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nm.06sbpz2", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.season -> 1953 Major League Baseball season\n# Answer:\nm.06sbpz2", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0gggrzr -> sports.sports_team_roster.team -> Brooklyn Dodgers\n# Answer:\nm.0gggrzr", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.season -> 1956 Major League Baseball Season\n# Answer:\nm.06sbq0l", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgj2z -> sports.sports_team_roster.team -> Los Angeles Bulldogs\n# Answer:\nm.0hpgj2z", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hpgh_h"], "ground_truth": ["UCLA Bruins football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> location.location.containedby -> New York\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> base.militaryinfiction.location_in_fiction.contained_by -> New York\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> location.location.containedby -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Play -> type.type.expected_by -> Appears in plays\n# Answer:\nPlay", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Choreographer\n# Answer:\nChoreographer", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Play -> type.type.expected_by -> Play Lyrics Written\n# Answer:\nPlay", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Play -> base.descriptive_names.names.descriptive_name -> m.010h7q1p\n# Answer:\nPlay", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Composer -> freebase.type_hints.included_types -> Topic\n# Answer:\nComposer", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Play -> type.type.expected_by -> Plays Appears In\n# Answer:\nPlay", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Play -> type.type.properties -> Composer\n# Answer:\nPlay"], "ground_truth": ["1976-08-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Alabama\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> United States of America\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07ghzrz\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b66g7msm\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.education -> m.03gkqtp -> education.education.institution -> Louisiana State University\n# Answer:\nm.03gkqtp", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Area code 251\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b7tn_fd2\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11btt6sf_l\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07919ln -> american_football.player_passing_statistics.team -> Oakland Raiders\n# Answer:\nm.07919ln", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07mmjx4 -> american_football.player_passing_statistics.team -> Oakland Raiders\n# Answer:\nm.07mmjx4"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> base.aareas.schema.administrative_area.administrative_parent -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.administrative_division.country -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Anna Roosevelt Halsted -> people.deceased_person.place_of_death -> New York City\n# Answer:\nAnna Roosevelt Halsted", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Elliott Roosevelt -> people.deceased_person.place_of_death -> Scottsdale\n# Answer:\nElliott Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Autobiography of Eleanor Roosevelt -> common.topic.notable_types -> Book Edition\n# Answer:\nAutobiography of Eleanor Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Courage in a dangerous world -> book.book_edition.isbn -> 9780231111812\n# Answer:\nCourage in a dangerous world", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Autobiography of Eleanor Roosevelt -> common.topic.notable_for -> g.125b58fny\n# Answer:\nAutobiography of Eleanor Roosevelt"], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf2_w -> location.religion_percentage.religion -> Islam\n# Answer:\nm.03xf2_w", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf301 -> location.religion_percentage.religion -> Protestantism\n# Answer:\nm.03xf301", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.064szjw -> location.religion_percentage.religion -> Hinduism\n# Answer:\nm.064szjw", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60ptk2z\n# Answer:\ng.11b60ptk2z", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.12cp_j7n1\n# Answer:\ng.12cp_j7n1", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_4m6h\n# Answer:\ng.1245_4m6h", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_67l9\n# Answer:\ng.1245_67l9", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.1hhc378kt\n# Answer:\ng.1hhc378kt", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.1hhc38d0l\n# Answer:\ng.1hhc38d0l"], "ground_truth": ["Islam", "Catholicism", "Protestantism", "Hinduism"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.42857142857142855, "path_precision": 0.3333333333333333, "path_recall": 0.6, "path_ans_f1": 0.46153846153846156, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.75}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Countryway Gunshop\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Projectile weapon\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.image -> HK USP 45\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> people.cause_of_death.includes_causes_of_death -> Assassination by firearm\n# Answer:\nAssassination", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Springfield Armory, Inc.\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> base.weapons.weapon.subtypes -> Handgun\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> people.cause_of_death.includes_causes_of_death -> Assassination in ways which appear natural\n# Answer:\nAssassination", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.location.contains -> American College of Technology\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.image -> M&Prevolver\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> base.weapons.weapon.subtypes -> Rifle\n# Answer:\nFirearm"], "ground_truth": ["Firearm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.nationality -> United States of America\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.profession -> Politician\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> common.topic.article -> m.03mpv\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pgr_5 -> people.place_lived.location -> Kentucky\n# Answer:\nm.03pgr_5", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> people.person.gender -> Male\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pn4x_ -> people.place_lived.location -> Springfield\n# Answer:\nm.03pn4x_", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.04hdfss -> people.place_lived.location -> Illinois\n# Answer:\nm.04hdfss"], "ground_truth": ["Andrew Johnson", "Hannibal Hamlin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7272727272727273, "ans_precission": 0.5714285714285714, "ans_recall": 1.0, "path_f1": 0.7272727272727273, "path_precision": 0.5714285714285714, "path_recall": 1.0, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 1.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Fezziwig -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nFezziwig", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> common.topic.notable_types -> Film character\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced_by -> H. G. Wells\n# Answer:\nAldous Huxley", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> film.film_character.portrayed_in_films -> m.010p_33b\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> common.topic.image -> Scrooges third visitor-John Leech,1843\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> base.kwebbase.kwtopic.has_sentences -> (1846) -> base.kwebbase.kwsentence.dates -> m.0c0z7kp\n# Answer:\n(1846)", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> film.film_character.portrayed_in_films -> m.011cd25b\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Alphonse Daudet -> influence.influence_node.influenced_by -> Gustave Flaubert\n# Answer:\nAlphonse Daudet"], "ground_truth": ["Our mutual friend", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "A Christmas Carol (Saddleback Classics)", "A Tale of Two Cities (Puffin Classics)", "A Christmas Carol (Audio Editions)", "A Christmas Carol (Take Part)", "A Tale of Two Cities (Paperback Classics)", "A Tale of Two Cities (Penguin Classics)", "A Christmas Carol (Whole Story)", "A Christmas Carol (Classics for Young Adults and Adults)", "The Old Curiosity Shop", "A Christmas Carol (Classic Fiction)", "A Christmas Carol (Children's Theatre Playscript)", "A Christmas Carol (Bantam Classic)", "A Christmas Carol (Large Print)", "A TALE OF TWO CITIES", "A Christmas Carol (Watermill Classic)", "Dombey and son", "A Tale of Two Cities (Simple English)", "A Christmas Carol (Classic Collection)", "A Christmas Carol (Nelson Graded Readers)", "A Tale of Two Cities (Dodo Press)", "A Tale of Two Cities (Dramascripts S.)", "A Tale of Two Cities (Barnes & Noble Classics Series)", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (BBC Audio Series)", "A Christmas Carol (New Longman Literature)", "Bleak House.", "A Christmas Carol (Puffin Choice)", "A Tale of Two Cities (Oxford Bookworms Library)", "The life and adventures of Nicholas Nickleby", "The cricket on the hearth", "A Tale of Two Cities (10 Cassettes)", "A Christmas Carol (Great Stories)", "Dombey and Son", "A Christmas Carol (Puffin Classics)", "A Tale of Two Cities (Isis Clear Type Classic)", "A Tale of Two Cities (40th Anniversary Edition)", "A Christmas Carol (Penguin Readers, Level 2)", "A Tale Of Two Cities (Adult Classics in Audio)", "A Christmas Carol (Classics Illustrated)", "A Christmas Carol (Ladybird Children's Classics)", "A Christmas Carol (Dramascripts Classic Texts)", "A Christmas Carol. (Lernmaterialien)", "A Christmas Carol (Classic Books on Cassettes Collection)", "A Christmas Carol (Thornes Classic Novels)", "A Christmas Carol (Classic, Picture, Ladybird)", "A Tale of Two Cities (Large Print Edition)", "A Tale of Two Cities (Everyman's Library Classics)", "A Christmas Carol (Enriched Classics)", "A Tale of Two Cities (Cyber Classics)", "The mystery of Edwin Drood", "A Tale of Two Cities (Classic Retelling)", "A Tale of Two Cities (Longman Classics, Stage 2)", "A Christmas Carol", "A Tale of Two Cities (Classic Fiction)", "A Tale of Two Cities (Longman Fiction)", "A Christmas Carol (Aladdin Classics)", "A Tale of Two Cities (Naxos AudioBooks)", "A Christmas Carol (Penguin Student Editions)", "A Tale of Two Cities (Tor Classics)", "A Christmas Carol (Everyman's Library Children's Classics)", "The Pickwick papers", "A Tale of Two Cities (Adopted Classic)", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "A Christmas Carol (Chrysalis Children's Classics Series)", "A Tale of Two Cities (Clear Print)", "A Tale of Two Cities (Enriched Classic)", "A Tale of Two Cities", "A Christmas Carol (Family Classics)", "A CHRISTMAS CAROL", "Sketches by Boz", "A Christmas Carol (Usborne Young Reading)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "A Christmas Carol (Through the Magic Window Series)", "A Tale of Two Cities (Konemann Classics)", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "A Tale of Two Cities (Collected Works of Charles Dickens)", "A Tale of Two Cities (Pacemaker Classics)", "A Tale of Two Cities (Progressive English)", "A Tale of Two Cities (Collector's Library)", "A Tale of Two Cities (Everyman Paperbacks)", "A Christmas Carol (Tor Classics)", "Hard times", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "A Christmas Carol (Radio Theatre)", "A Tale of Two Cities (Cover to Cover Classics)", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Christmas Carol (Cover to Cover)", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "A Tale of Two Cities (Wordsworth Classics)", "A Tale of Two Cities (Prentice Hall Science)", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "A Christmas Carol (Value Books)", "A Christmas Carol (Pacemaker Classic)", "A Tale of Two Cities (The Classic Collection)", "Great Expectations", "A Tale of Two Cities (Signet Classics)", "The Mystery of Edwin Drood", "The old curiosity shop.", "Bleak house", "A Christmas Carol (Illustrated Classics)", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "The old curiosity shop", "A Christmas Carol (Soundings)", "A Tale of Two Cities (Classic Literature with Classical Music)", "A Christmas Carol (The Kennett Library)", "A Christmas Carol (Acting Edition)", "A Tale of Two Cities (Acting Edition)", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Dover Thrift Editions)", "A Tale of Two Cities (Silver Classics)", "A Christmas Carol (Scholastic Classics)", "A Christmas Carol (Children's Classics)", "A Tale of Two Cities (The Greatest Historical Novels)", "A Christmas Carol (R)", "A Tale of Two Cities (Dramatized)", "A Christmas Carol (Pacemaker Classics)", "A Christmas Carol (Watermill Classics)", "David Copperfield.", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "A Tale of Two Cities (Everyman's Library (Paper))", "A Tale of Two Cities (Classics Illustrated)", "A Tale of Two Cities (Soundings)", "A Christmas Carol (Gollancz Children's Classics)", "A Tale of Two Cities (Student's Novels)", "Great Expectations.", "A Tale of Two Cities (Bookcassette(r) Edition)", "Bleak House", "A Tale of Two Cities (Courage Literary Classics)", "A Christmas Carol (Clear Print)", "A Tale of Two Cities (Compact English Classics)", "Oliver Twist", "A Christmas Carol (Limited Editions)", "Martin Chuzzlewit", "A Tale of Two Cities (Illustrated Classics)", "David Copperfield", "A Tale of Two Cities (Saddleback Classics)", "A Christmas Carol (Dramascripts)", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "A Christmas Carol (Read & Listen Books)", "A Tale of Two Cities (Piccolo Books)", "A Christmas Carol (Young Reading Series 2)", "A Tale of Two Cities (Macmillan Students' Novels)", "A Tale of Two Cities (Classics Illustrated Notes)", "Little Dorrit", "A Christmas Carol (Reissue)", "Dombey and Son.", "A Tale of Two Cities (Ultimate Classics)", "A Tale of Two Cities (Unabridged Classics)", "The Pickwick Papers", "A Christmas Carol (Green Integer, 50)", "Our mutual friend.", "A Christmas Carol (Apple Classics)", "Great expectations", "A Tale of Two Cities (Masterworks)", "Great expectations.", "A Tale of Two Cities (Penguin Popular Classics)", "A Christmas Carol (Oxford Bookworms Library)", "A Tale of Two Cities (Bantam Classic)", "A Christmas Carol (Cp 1135)", "A Tale of Two Cities (Illustrated Junior Library)", "A Tale Of Two Cities (Adult Classics)", "A Christmas Carol (Ladybird Classics)", "A Tale of Two Cities (Penguin Readers, Level 5)", "A Tale of Two Cities (Cassette (1 Hr).)", "A Tale of Two Cities (Oxford Playscripts)", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "A Tale of Two Cities (Ladybird Children's Classics)"], "ans_acc": 0.011834319526627219, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.21176470588235294, "path_precision": 0.3, "path_recall": 0.16363636363636364, "path_ans_f1": 0.022770398481973434, "path_ans_precision": 0.3, "path_ans_recall": 0.011834319526627219}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_commander -> Nikita Khrushchev\n# Answer:\nm.0vmx6nr", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.office_holder -> Nikita Khrushchev\n# Answer:\nm.049x6zw", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_conflict -> Cuban Missile Crisis\n# Answer:\nm.0vmx6nr", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf -> military.military_command.military_commander -> Joseph Stalin\n# Answer:\nm.02h7nmf", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.049y3kf -> military.military_command.military_commander -> Ivan Konev\n# Answer:\nm.049y3kf", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_6 -> government.government_position_held.office_holder -> Vladimir Ivashko\n# Answer:\nm.049x6_6", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.governmental_body -> Central Committee of the Communist Party of the Soviet Union\n# Answer:\nm.049x6zw", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_holder -> Joseph Stalin\n# Answer:\nm.049x6_k", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf -> military.military_command.military_conflict -> World War II\n# Answer:\nm.02h7nmf", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.049y3kf -> military.military_command.military_conflict -> Battle of Berlin\n# Answer:\nm.049y3kf"], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> base.mystery.cryptid_area_of_occurrence.cryptid_s_found_here -> Chupacabra -> base.folklore.mythical_creature.similar_mythical_creature_s -> m.05d0ybv\n# Answer:\nChupacabra", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> Panama\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> common.topic.article -> m.09nqr\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> American Samoa\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.target_of_exchange -> m.03x8_16\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> base.mystery.cryptid_area_of_occurrence.cryptid_s_found_here -> Chupacabra -> common.topic.notable_types -> Literature Subject\n# Answer:\nChupacabra", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> Bonaire\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> base.mystery.cryptid_area_of_occurrence.cryptid_s_found_here -> Chupacabra -> award.ranked_item.appears_in_ranked_lists -> m.0h7g_lj\n# Answer:\nChupacabra", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.target_of_exchange -> m.04kq24y\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Frontpoint Security Solutions -> organization.organization.geographic_scope -> United States of America\n# Answer:\nFrontpoint Security Solutions"], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Dementia\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> people.cause_of_death.parent_cause_of_death -> Cancer\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Epilepsy\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease.medical_specialties -> Oncology\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Radiation therapy\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Head pressing\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Cholecalciferol\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease.medical_specialties -> Neurology\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.symptoms -> Air crescent sign\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease.medical_specialties -> Neurosurgery\n# Answer:\nBrain tumor"], "ground_truth": ["Brain tumor", "Lung cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.02h9cb0 -> tv.regular_tv_appearance.actor -> David Hasselhoff\n# Answer:\nm.02h9cb0", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4 -> tv.tv_series_season.regular_cast -> m.03lj4m5\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4lk -> tv.regular_tv_appearance.actor -> Patricia McPherson\n# Answer:\nm.03lj4lk", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 4\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 3\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.02h9cb0 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 4\n# Answer:\nm.02h9cb0", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4 -> tv.tv_series_season.regular_cast -> m.02h9cb0\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4 -> tv.tv_series_season.regular_cast -> m.03lj4lk\n# Answer:\nKnight Rider - Season 4"], "ground_truth": ["William Daniels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.location.containedby -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County -> location.location.containedby -> Tennessee\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.country -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.location.containedby -> Tennessee\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11b66dwnl4\n# Answer:\ng.11b66dwnl4", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County -> location.location.containedby -> Nashville-Davidson--Murfreesboro--Franklin, TN Metropolitan Statistical Area\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.location.containedby -> Nashville-Davidson--Murfreesboro--Franklin, TN Metropolitan Statistical Area\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County -> location.statistical_region.co2_emissions_total -> m.045hnmm\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11x1ddsd6\n# Answer:\ng.11x1ddsd6"], "ground_truth": ["Williamson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.geolocation -> m.0kdmvf\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11b66mljn1\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_for -> g.1255959wd\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11x1chmhk\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> astronomy.extraterrestrial_location.on_celestial_object -> Moon\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Missouri\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11x1ct3wq\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Jasper County\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_types -> Extraterrestrial location\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver Academy -> location.location.geolocation -> m.0127s0kf\n# Answer:\nCarver Academy"], "ground_truth": ["Diamond"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.location_of_ceremony -> Arlington\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.profession -> Actor\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Esm\u00e9 Annabelle Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nEsm\u00e9 Annabelle Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.sibling_s -> m.0j217jw\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> common.topic.notable_types -> Person\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.spouse -> Tracy Pollan\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.nationality -> United States of America\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.02kkmrn"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04yvq68 -> military.military_command.military_conflict -> How Few Remain\n# Answer:\nm.04yvq68", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9q6 -> military.military_command.military_conflict -> Battle of McDowell\n# Answer:\nm.04fv9q6", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9nd -> military.military_command.military_conflict -> Battle of Port Republic\n# Answer:\nm.04fv9nd", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> time.event.includes_event -> Battle of McDowell\n# Answer:\nJackson's Valley Campaign", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Foot cavalry\n# Answer:\nFoot cavalry", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Abraham Lincoln\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> time.event.includes_event -> Battle of Port Republic\n# Answer:\nJackson's Valley Campaign", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> book.book_subject.musical_compositions_about_this_topic -> the CIVIL warS: a tree is best measured when it is down\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Confederate States of America\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> visual_art.art_subject.artwork_on_the_subject -> Battle of Mobile Bay, 5 August 1864\n# Answer:\nAmerican Civil War"], "ground_truth": ["Battle of Harpers Ferry", "First Battle of Rappahannock Station", "Jackson's Valley Campaign", "Battle of McDowell", "Romney Expedition", "Second Battle of Bull Run", "First Battle of Kernstown", "Battle of Chancellorsville", "Battle of Hancock", "How Few Remain", "Battle of Port Republic", "First Battle of Winchester", "American Civil War", "Battle of Cedar Mountain", "Battle of Chantilly", "Battle of Hoke's Run", "Manassas Station Operations", "Battle of White Oak Swamp", "Battle of Front Royal"], "ans_acc": 0.2631578947368421, "ans_hit": 1, "ans_f1": 0.17910447761194026, "ans_precission": 0.6, "ans_recall": 0.10526315789473684, "path_f1": 0.2445414847161572, "path_precision": 0.7, "path_recall": 0.14814814814814814, "path_ans_f1": 0.40723981900452483, "path_ans_precision": 0.9, "path_ans_recall": 0.2631578947368421}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.parent -> Ongamo-Maa Group\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ng.1256fv3pv", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> common.topic.article -> m.064klf\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.document -> Maasai Phonology\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.document -> Maasai Detailed Description\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai -> common.image.size -> m.02bgrp_\n# Answer:\nMaasai", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.document -> Maasai Genesis Translation\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai women and children -> common.image.size -> m.03qy0z9\n# Answer:\nMaasai women and children", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai-jump -> common.image.size -> m.02br_p7\n# Answer:\nMaasai-jump"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Anne Harris\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Ebenezer Franklin\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.place_of_birth -> Ecton\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Elizabeth Douse\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger -> people.person.parents -> Mary Morrill\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> common.topic.image -> WilliamFranklin\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Francis Folger Franklin -> people.person.parents -> Deborah Read\n# Answer:\nFrancis Folger Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.spouse_s -> m.0j4k0ll\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> people.person.gender -> Male\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Sarah Franklin Bache -> people.person.children -> Benjamin Franklin Bache\n# Answer:\nSarah Franklin Bache"], "ground_truth": ["Deborah Read"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9g\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> African American\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9q\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Chronic pancreatitis\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Diabetes mellitus\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.treatments -> Pancreatectomy\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9g\n# Answer:\nPancreatic cancer"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Raphael and his age\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Leonardo da Vinci and the Art of Sculpture: Inspiration and Invention\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> people.profession.people_with_this_profession -> Sandro Botticelli\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> visual_art.visual_artist.art_forms -> Fresco\n# Answer:\nAntonio da Correggio", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper -> common.topic.article -> m.0jnzf\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Holy Infants Embracing -> common.topic.image -> Holy-infs\n# Answer:\nThe Holy Infants Embracing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi -> visual_art.artwork.art_subject -> Biblical Magi\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> visual_art.visual_artist.artworks -> The Education of Cupid\n# Answer:\nAntonio da Correggio"], "ground_truth": ["g.1224tf0c", "Medusa", "Lucan portrait of Leonardo da Vinci", "Portrait of a man in red chalk", "Annunciation", "Madonna Litta", "Ginevra de' Benci", "Leda and the Swan", "The Last Supper", "The Virgin and Child with St. Anne", "Vitruvian Man", "g.1213jb_b", "g.120vt1gz", "Madonna of the Yarnwinder", "g.1219sb0g", "The Virgin and Child with St Anne and St John the Baptist", "Madonna of the Carnation", "The Battle of Anghiari", "St. John the Baptist", "Horse and Rider", "Portrait of Isabella d'Este", "Head of a Woman", "La belle ferronni\u00e8re", "Adoration of the Magi", "Mona Lisa", "g.12215rxg", "St. Jerome in the Wilderness", "Sala delle Asse", "Portrait of a Musician", "Lady with an Ermine", "Benois Madonna", "Drapery for a Seated Figure", "g.12314dm1", "Virgin of the Rocks", "The Baptism of Christ", "Leonardo's horse", "Madonna of Laroque", "g.121yh91r", "g.121wt37c", "Madonna and Child with St Joseph", "Portrait of a Young Fianc\u00e9e", "Salvator Mundi", "The Holy Infants Embracing", "g.1239jd9p", "Bacchus"], "ans_acc": 0.06666666666666667, "ans_hit": 1, "ans_f1": 0.1090909090909091, "ans_precission": 0.3, "ans_recall": 0.06666666666666667, "path_f1": 0.11111111111111109, "path_precision": 0.3, "path_recall": 0.06818181818181818, "path_ans_f1": 0.1090909090909091, "path_ans_precision": 0.3, "path_ans_recall": 0.06666666666666667}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Salzburg -> base.aareas.schema.administrative_area.administrative_children -> Hallein District\n# Answer:\nSalzburg", "# Reasoning Path:\nAustria -> location.country.capital -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Salzburg -> base.aareas.schema.administrative_area.administrative_children -> Salzburg-Umgebung District\n# Answer:\nSalzburg", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Salzburg -> base.aareas.schema.administrative_area.administrative_children -> St. Johann im Pongau District\n# Answer:\nSalzburg", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Salzburg -> location.location.containedby -> Euregio Salzburg-Berchtesgadener Land-Traunstein\n# Answer:\nSalzburg", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Salzburg -> base.aareas.schema.administrative_area.administrative_area_type -> Austrian municipality\n# Answer:\nSalzburg", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Burgenland -> base.aareas.schema.administrative_area.administrative_area_type -> Austrian state\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Carinthia -> location.location.partially_contains -> Coglians\n# Answer:\nCarinthia"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.treatments -> Erlotinib\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04d7nvm\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A Time to Mend\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjf_c\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> common.topic.webpage -> m.04tnn4f\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.treatments -> Radiation therapy\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjfp1\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A season in hell\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.person.gender -> Female -> biology.hybrid_parent_gender.hybrids -> m.0blp57t\n# Answer:\nFemale"], "ground_truth": ["Cervical cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Abhiyaza\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_for -> g.12582gf_5\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> book.book_subject.works -> Remaking Buddhism for Medieval Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_types -> City/Town/Village\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Nanda\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> location.location.time_zones -> Nepal Time Zone\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> R\u0101hula\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> location.location.geolocation -> m.0wmmlzp\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> common.topic.article -> m.0hznzjg\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Tooth Relic Temple and Museum -> common.topic.webpage -> m.0gb1hxk\n# Answer:\nBuddha Tooth Relic Temple and Museum"], "ground_truth": ["Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove -> common.topic.article -> m.029mn3\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_trwk\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> Glass\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nm.012zbkk5", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.instrument.family -> Other instruments\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_ty__\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> medicine.medical_treatment.used_to_treat -> Presbyopia\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> :Library and information science\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.size -> m.02bc9fn\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_wj2d\n# Answer:\nGlass harmonica"], "ground_truth": ["Franklin stove", "Lightning rod", "Glass harmonica", "Bifocals"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7241379310344827, "ans_precission": 0.7, "ans_recall": 0.75, "path_f1": 0.7241379310344827, "path_precision": 0.7, "path_recall": 0.75, "path_ans_f1": 0.7241379310344827, "path_ans_precision": 0.7, "path_ans_recall": 0.75}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado -> location.location.containedby -> United States, with Territories\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpsq_\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado -> location.location.containedby -> United States of America\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.us_county.hud_county_place -> Aristocrat Ranchettes\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nGreeley -> location.location.contains -> Academy of Natural Therapy -> location.location.containedby -> Colorado\n# Answer:\nAcademy of Natural Therapy"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> common.topic.notable_types -> Profession\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician -> people.profession.specializations -> Composer\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> music.group_member.instruments_played -> Piano\n# Answer:\nPiano", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> Music\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician -> common.topic.notable_types -> Profession\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> common.topic.notable_types -> Profession\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> base.descriptive_names.names.descriptive_name -> m.01260py_\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> people.profession.specialization_of -> Writer\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> common.topic.image -> Pietro Metastasio\n# Answer:\nLibrettist"], "ground_truth": ["Composer", "Librettist", "Musician"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.5, "path_recall": 0.6666666666666666, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Eschen\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.country.first_level_divisions -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_used -> East German mark\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Gamprin\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.location.partially_contains -> Alps\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.contains -> Borna, Leipzig\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Germany -> location.country.languages_spoken -> Bavarian Language\n# Answer:\nGermany", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Switzerland\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.country.first_level_divisions -> Eschen\n# Answer:\nLiechtenstein"], "ground_truth": ["Switzerland", "Liechtenstein", "Luxembourg", "Austria", "Germany", "Belgium", "East Germany"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.5806451612903225, "ans_precission": 0.9, "ans_recall": 0.42857142857142855, "path_f1": 0.4210526315789473, "path_precision": 0.8, "path_recall": 0.2857142857142857, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 1.0, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> common.topic.notable_types -> Musical genre\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> common.topic.notable_types -> Musical genre\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Pop rock\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.recordings -> Chasing a Feeling\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music -> common.topic.subject_of -> Stephen Melton\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis -> film.film.genre -> Short Film\n# Answer:\nApotheosis", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Rock and roll\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> music.genre.subgenre -> Math rock\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Rock music\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music -> common.topic.notable_types -> Musical genre\n# Answer:\nPop music"], "ground_truth": ["Rock music", "Art rock", "Soft rock", "Pop music", "Psychedelic rock", "Experimental rock", "Pop rock", "Experimental music", "Blues rock"], "ans_acc": 0.5555555555555556, "ans_hit": 1, "ans_f1": 0.48648648648648646, "ans_precission": 0.9, "ans_recall": 0.3333333333333333, "path_f1": 0.48648648648648646, "path_precision": 0.9, "path_recall": 0.3333333333333333, "path_ans_f1": 0.6870229007633587, "path_ans_precision": 0.9, "path_ans_recall": 0.5555555555555556}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> government.government_position_held.office_holder -> Michael Bennet\n# Answer:\nm.05kh0nn", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.office_holder -> Mark Udall\n# Answer:\nm.05kfbwl", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr -> government.government_position_held.office_holder -> Wayne Allard\n# Answer:\nm.02_wstr", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nm.05kh0nn", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.05kfbwl", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.legislative_sessions -> 111th United States Congress\n# Answer:\nm.05kfbwl", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008 -> government.election.winner -> Mark Udall\n# Answer:\nUnited States Senate election in Colorado, 2008", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> freebase.valuenotation.is_reviewed -> District represented (if position is district-related)\n# Answer:\nm.05kh0nn", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nm.02_wstr", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008 -> government.election.general_election -> Colorado state elections, 2008\n# Answer:\nUnited States Senate election in Colorado, 2008"], "ground_truth": ["Mark Udall", "Michael Bennet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Nordic countries\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Europe\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Capital Region of Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> g.1hhc37psk\n# Answer:\ng.1hhc37psk", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Kingdom of Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries -> location.location.contains -> Denmark\n# Answer:\nNordic countries", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Central Denmark Region\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Kingdom of Denmark -> location.location.contains -> Denmark\n# Answer:\nKingdom of Denmark", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> g.1hhc38qmq\n# Answer:\ng.1hhc38qmq"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_for -> g.125h4fcxl\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98174 -> location.postal_code.country -> United States of America\n# Answer:\n98174", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> g.11b66b70n7\n# Answer:\ng.11b66b70n7", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.location.contains -> Fifteen Twenty-One Second Avenue\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98174 -> location.location.contains -> 1000 Second Avenue\n# Answer:\n98174", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> common.topic.notable_for -> g.125f2tsfn\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point -> location.neighborhood.neighborhood_of -> Seattle-Tacoma-Bellevue, WA Metropolitan Statistical Area\n# Answer:\nAlki Point", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98174 -> common.topic.notable_types -> Postal Code\n# Answer:\n98174", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> g.11btt54h7d\n# Answer:\ng.11btt54h7d", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.location.contains -> 1111 Third Avenue\n# Answer:\n98101"], "ground_truth": ["98101", "98144", "98129", "98148", "98139", "98127", "98136", "98113", "98108", "98116", "98111", "98188", "98199", "98164", "98184", "98104", "98170", "98195", "98154", "98158", "98119-4114", "98160", "98114", "98107", "98124", "98185", "98122", "98146", "98190", "98198", "98112", "98166", "98131", "98125", "98171", "98118", "98121", "98161", "98168", "98178", "98181", "98175", "98102", "98115", "98133", "98126", "98109", "98174", "98134", "98138", "98145", "98119", "98165", "98117", "98155", "98106", "98191", "98105", "98141", "98177", "98132", "98194", "98103"], "ans_acc": 0.047619047619047616, "ans_hit": 1, "ans_f1": 0.08917197452229297, "ans_precission": 0.7, "ans_recall": 0.047619047619047616, "path_f1": 0.08917197452229297, "path_precision": 0.7, "path_recall": 0.047619047619047616, "path_ans_f1": 0.08917197452229297, "path_ans_precision": 0.7, "path_ans_recall": 0.047619047619047616}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Adrienne Banfield-Jones\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Robsol Pinkett, Jr.\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> common.topic.image -> Pinkett-Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> tv.tv_actor.guest_roles -> m.09nq_ss\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.parents -> Caroline Bright\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B -> music.genre.subgenre -> Quiet Storm\n# Answer:\nContemporary R&B", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> tv.tv_actor.guest_roles -> m.09nq_sy\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.parents -> Willard Christopher Smith, Sr.\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> tv.tv_actor.guest_roles -> m.09nbq82\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> tv.tv_actor.guest_roles -> m.09nq_t2\n# Answer:\nJada Pinkett Smith"], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_types -> Language Writing System\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_for -> g.1258512fl\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> N\u00fcshu script -> common.topic.article -> m.014lbq\n# Answer:\nN\u00fcshu script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> language.language_writing_system.languages -> Mongolian language\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> language.language_writing_system.languages -> Chinese, Hakka Language\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> language.language_writing_system.type_of_writing -> Ideogram\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> book.book_subject.works -> Flags of Our Fathers\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> location.location.contains -> China\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> common.topic.notable_for -> g.1259bftrt\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> N\u00fcshu script -> common.topic.image -> NushuGarden\n# Answer:\nN\u00fcshu script"], "ground_truth": ["Chinese characters", "'Phags-pa script", "Traditional Chinese characters", "Simplified Chinese character", "N\u00fcshu script"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6857142857142857, "ans_precission": 0.8, "ans_recall": 0.6, "path_f1": 0.6857142857142857, "path_precision": 0.8, "path_recall": 0.6, "path_ans_f1": 0.6857142857142857, "path_ans_precision": 0.8, "path_ans_recall": 0.6}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.spouse -> Pat Nixon\n# Answer:\nm.02h98gq", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.person.parents -> Samuel Brady Nixon\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> common.topic.notable_for -> g.125b3gc_h\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> freebase.valuenotation.has_value -> Place of death\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.type_of_union -> Marriage\n# Answer:\nm.02h98gq", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.deceased_person.place_of_death -> La Habra\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.location_of_ceremony -> The Mission Inn Hotel & Spa\n# Answer:\nm.02h98gq", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.person.parents -> Sarah Ann Nixon\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Arthur Nixon\n# Answer:\nHannah Milhous Nixon"], "ground_truth": ["Pat Nixon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.11764705882352941, "path_precision": 0.1, "path_recall": 0.14285714285714285, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nm.07t6_k3", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Supporting Actress in a Comedy Series\n# Answer:\nm.07t6_mc", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.award -> NAACP Image Award for Outstanding Actress in a Comedy Series\n# Answer:\nm.0_yc_3q", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.ceremony -> 37th Primetime Emmy Awards\n# Answer:\nm.07t6_k3", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Supporting Actress in a Comedy Series\n# Answer:\nm.07t6_k3", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nm.07t6_mc", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.ceremony -> 35th Primetime Emmy Awards\n# Answer:\nm.07t6_mc", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.ceremony -> 16th NAACP Image Awards\n# Answer:\nm.0_yc_3q", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0_yczwx -> award.award_honor.award_winner -> Marla Gibbs\n# Answer:\nm.0_yczwx", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 0 -> common.topic.notable_for -> g.1yl5pbtsv\n# Answer:\nThe Jeffersons - Season 0"], "ground_truth": ["Mike Evans", "Paul Benedict", "Roxie Roker", "Zara Cully", "Damon Evans", "Franklin Cover", "Jay Hammer", "Marla Gibbs", "Isabel Sanford", "Sherman Hemsley", "Berlinda Tolbert"], "ans_acc": 0.09090909090909091, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.08695652173913043, "path_precision": 0.3, "path_recall": 0.05084745762711865, "path_ans_f1": 0.13953488372093023, "path_ans_precision": 0.3, "path_ans_recall": 0.09090909090909091}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Dock of the Bay -> common.topic.notable_types -> Newspaper\n# Answer:\nDock of the Bay", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> The Golden Era -> common.topic.article -> m.04zg6d\n# Answer:\nThe Golden Era", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Dock of the Bay -> book.periodical.first_issue_date -> m.0_pbpv1\n# Answer:\nDock of the Bay", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> The Golden Era -> common.topic.notable_for -> g.1258tnwlj\n# Answer:\nThe Golden Era", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.periodical.first_issue_date -> m.02npn31\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> book.written_work.subjects -> California\n# Answer:\n1906", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.publication.contents -> m.0znkynv\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94101 -> location.postal_code.country -> United States of America\n# Answer:\n94101", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> common.topic.notable_for -> g.1258179ql\n# Answer:\n1906", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> book.written_work.original_language -> English Language\n# Answer:\n1906"], "ground_truth": ["San Francisco Business Times", "California Star", "The San Francisco Examiner", "San Francisco Bay Guardian", "The Daily Alta California", "San Francisco News-Call Bulletin Newspaper", "San Francisco Call", "AsianWeek", "Sing Tao Daily", "San Francisco Daily", "San Francisco Chronicle", "San Francisco Bay Times", "San Francisco Foghorn", "Synapse", "The Golden Era", "Street Sheet", "Free Society", "Dock of the Bay", "San Francisco Bay View", "Bay Area Reporter"], "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0.24, "ans_precission": 0.6, "ans_recall": 0.15, "path_f1": 0.24, "path_precision": 0.6, "path_recall": 0.15, "path_ans_f1": 0.24, "path_ans_precision": 0.6, "path_ans_recall": 0.15}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.partially_contains -> Didi Abuli\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Denmark\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> base.locations.continents.countries_within -> Denmark\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> location.location.partially_contains -> Didi Abuli\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.containedby -> Eurasia -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurasia", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Georgia\n# Answer:\nEurope"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w0hpl\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w4lwz\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> ACE inhibitor\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> heart attack\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09wjtbj\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Diabetes mellitus\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> Acebutolol\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> Anisindione\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Family History of Ischaemic Heart disease\n# Answer:\nheart attack"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 11: 1863\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin -> book.author.works_written -> The Autobiography of Charles Darwin\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Anne Darwin -> people.person.parents -> Emma Darwin\n# Answer:\nAnne Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nEngland"], "ground_truth": ["Tesakneri tsagume\u030c", "On the origin of species by means of natural selection", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "Cartas de Darwin 18251859", "Darwin Darwin", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "The Power of Movement in Plants", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Questions about the breeding of animals", "Part I: Contributions to the Theory of Natural Selection / Part II", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "Human nature, Darwin's view", "The Correspondence of Charles Darwin, Volume 9: 1861", "Darwin", "Fertilisation of Orchids", "Darwin's notebooks on transmutation of species", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Darwin and Henslow", "On the Movements and Habits of Climbing Plants", "Beagle letters", "Notes on the fertilization of orchids", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "Opsht\u0323amung fun menshen", "The education of Darwin", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "A student's introduction to Charles Darwin", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "monograph on the sub-class Cirripedia", "Darwin Compendium", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Gesammelte kleinere Schriften", "Insectivorous Plants", "The Correspondence of Charles Darwin, Volume 18: 1870", "Charles Darwin's letters", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Darwinism stated by Darwin himself", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The Correspondence of Charles Darwin, Volume 11: 1863", "The Darwin Reader First Edition", "The Expression of the Emotions in Man and Animals", "The Autobiography of Charles Darwin", "El Origin De Las Especies", "Kleinere geologische Abhandlungen", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Notebooks on transmutation of species", "Charles Darwin's marginalia", "From Darwin's unpublished notebooks", "Reise eines Naturforschers um die Welt", "Les r\u00e9cifs de corail, leur structure et leur distribution", "Del Plata a Tierra del Fuego", "Darwin's Ornithological notes", "Resa kring jorden", "Geological Observations on the Volcanic Islands", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "Darwin-Wallace", "Metaphysics, Materialism, & the evolution of mind", "La facult\u00e9 motrice dans les plantes", "Evolution by natural selection", "On evolution", "The geology of the voyage of H.M.S. Beagle", "The foundations of the Origin of species", "The Formation of Vegetable Mould through the Action of Worms", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "The voyage of Charles Darwin", "Evolutionary Writings: Including the Autobiographies", "Memorias y epistolario i\u0301ntimo", "Evolution", "The Correspondence of Charles Darwin, Volume 12: 1864", "Die fundamente zur entstehung der arten", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "The Variation of Animals and Plants under Domestication", "South American Geology", "A Darwin Selection", "The\u0301orie de l'e\u0301volution", "genese\u014ds t\u014dn eid\u014dn", "Darwin for Today", "The Structure and Distribution of Coral Reefs", "The collected papers of Charles Darwin", "Reise um die Welt 1831 - 36", "Origins", "Geological Observations on South America", "Les mouvements et les habitudes des plantes grimpantes", "Rejse om jorden", "vari\u00eberen der huisdieren en cultuurplanten", "The living thoughts of Darwin", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "Volcanic Islands", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "The Correspondence of Charles Darwin, Volume 17: 1869", "The Correspondence of Charles Darwin, Volume 13: 1865", "Leben und Briefe von Charles Darwin", "The Darwin Reader Second Edition", "Die geschlechtliche Zuchtwahl", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "Darwin's journal", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The Correspondence of Charles Darwin, Volume 15: 1867", "Charles Darwin", "The Correspondence of Charles Darwin, Volume 10: 1862", "More Letters of Charles Darwin", "The Descent of Man, and Selection in Relation to Sex", "Works", "The principal works", "Evolution and natural selection", "Voyage d'un naturaliste autour du monde", "Charles Darwin on the routes of male humble bees", "Wu zhong qi yuan", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The action of carbonate of ammonia on the roots of certain plants", "On a remarkable bar of sandstone off Pernambuco", "The Life and Letters of Charles Darwin Volume 1", "Charles Darwin's natural selection", "ontstaan der soorten door natuurlijke teeltkeus", "The Correspondence of Charles Darwin, Volume 14: 1866", "The Correspondence of Charles Darwin, Volume 8: 1860", "On Natural Selection", "The Voyage of the Beagle", "Motsa ha-minim", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "On the tendency of species to form varieties", "From so simple a beginning", "Les moyens d'expression chez les animaux", "The Life of Erasmus Darwin", "Darwin en Patagonia", "red notebook of Charles Darwin", "Het uitdrukken van emoties bij mens en dier", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "The Correspondence of Charles Darwin, Volume 16: 1868", "The Life and Letters of Charles Darwin Volume 2", "La vie et la correspondance de Charles Darwin", "The Orgin of Species", "H.M.S. Beagle in South America", "Proiskhozhdenie vidov", "The Essential Darwin", "Darwin from Insectivorous Plants to Worms", "The Different Forms of Flowers on Plants of the Same Species", "The portable Darwin", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "To the members of the Down Friendly Club", "Diary of the voyage of H.M.S. Beagle", "Diario del Viaje de Un Naturalista Alrededor", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "Darwin's insects", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "Darwin on humus and the earthworm"], "ans_acc": 0.05228758169934641, "ans_hit": 1, "ans_f1": 0.025586353944562903, "ans_precission": 0.6, "ans_recall": 0.013071895424836602, "path_f1": 0.3636363636363636, "path_precision": 1.0, "path_recall": 0.2222222222222222, "path_ans_f1": 0.09937888198757763, "path_ans_precision": 1.0, "path_ans_recall": 0.05228758169934641}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> location.administrative_division.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.film -> End of the Road: How Money Became Worthless\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15 -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.02shm15", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.0_0cs2_ -> film.personal_film_appearance.film -> Our Nixon\n# Answer:\nm.0_0cs2_", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsgl5 -> tv.tv_guest_role.episodes_appeared_in -> The Future of the GOP\n# Answer:\nm.09nsgl5", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglb -> tv.tv_guest_role.episodes_appeared_in -> The American Film Institute Salute to James Cagney\n# Answer:\nm.09nsglb", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15 -> film.personal_film_appearance.film -> Sicko\n# Answer:\nm.02shm15", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglh -> tv.tv_guest_role.episodes_appeared_in -> Johnny Grant\n# Answer:\nm.09nsglh"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.size -> m.0kjrkq\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.appears_in_topic_gallery -> Grammy Legend Award\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.appears_in_topic_gallery -> Zubin Mehta\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> Little Miss Sweetness -> music.recording.releases -> Gettin' Ready\n# Answer:\nLittle Miss Sweetness", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.size -> m.02cljr8\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.subjects -> Bleona\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Music executive -> common.topic.article -> m.047rgq1\n# Answer:\nMusic executive", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Musician -> freebase.equivalent_topic.equivalent_type -> Musical Artist\n# Answer:\nMusician", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> After All -> music.composition.recordings -> After All (stereo mix)\n# Answer:\nAfter All"], "ground_truth": ["Tears of a Clown", "Quiet Storm (Groove Boutique remix)", "The Way You Do (The Things You Do)", "We\u2019ve Come Too Far to End It Now", "The Tracks of My Tears (live)", "Going to a Go Go", "If You Want My Love", "With Your Love Came", "In My Corner", "Rack Me Back", "I Know You by Heart", "Driving Thru Life in the Fast Lane", "Baby Come Close", "I'm in the Mood for Love", "Just to See Her", "A Silent Partner in a Three-Way Love Affair", "Medley: Never My Love / Never Can Say Goodbye", "Love Is The Light", "Everything for Christmas", "Vitamin U", "Yes It's You Lady", "Love Don' Give No Reason (12 Inch Club Mix)", "Blame It on Love", "And I Don't Love You", "I Can't Get Enough", "Mother's Son", "Tears Of A Clown", "Photograph in My Mind", "It's Christmas Time", "Will You Love Me Tomorrow", "Tell Me Tomorrow, Part 1", "Walk on By", "I've Got You Under My Skin", "Quiet Storm", "Open", "Holly", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "The Tracks of My Heart", "Be Kind to the Growing Mind", "I Hear The Children Singing", "A Tattoo", "Season's Greetings from Smokey Robinson", "I Want You Back", "Time Flies", "God Rest Ye Merry Gentlemen", "Why", "Santa Claus is Coming to Town", "Ebony Eyes (Duet with Rick James)", "Hold on to Your Love", "Love' n Life", "Blame It On Love (Duet with Barbara Mitchell)", "Let Me Be the Clock", "As You Do", "Love Letters", "You Take Me Away", "Yester Love", "So Bad", "Virgin Man", "Ain't That Peculiar", "Keep Me", "Tracks of my Tears", "Fulfill Your Need", "My Girl", "You Really Got a Hold on Me", "Some People Will Do Anything for Love", "Be Careful What You Wish For (instrumental)", "Christmas Greeting", "Fly Me to the Moon (In Other Words)", "Ooo Baby Baby", "The Tracks of My Tears", "She's Only a Baby Herself", "Noel", "Love Don't Give No Reason", "Let Your Light Shine On Me", "Just My Soul Responding", "Pops, We Love You", "Be Careful What You Wish For", "Tell Me Tomorrow (12\\\" extended mix)", "I\u2019ve Got You Under My Skin", "I Love The Nearness Of You", "There Will Come A Day ( I'm Gonna Happen To You )", "Who's Sad", "We've Saved the Best for Last", "Standing On Jesus", "You've Really Got a Hold on Me", "I Second That Emotions", "Love Brought Us Here", "The Road to Damascus", "It's Fantastic", "Don't Know Why", "Away in the Manger / Coventry Carol", "Get Ready", "My Guy", "A Child Is Waiting", "Will You Love Me Tomorrow?", "I Can't Give You Anything but Love", "It's a Good Feeling", "One Heartbeat", "Close Encounters of the First Kind", "More Love", "What's Too Much", "I Have Prayed On It", "Please Come Home for Christmas", "Little Girl Little Girl", "I Second That Emotion", "Whatcha Gonna Do", "You Made Me Feel Love", "And I Don't Love You (Larry Levan instrumental dub)", "Cruisin", "The Love Between Me and My Kids", "Nearness of You", "Double Good Everything", "It's Time to Stop Shoppin' Around", "It's A Good Night", "You're the One for Me (feat. Joss Stone)", "The Family Song", "I'm Glad There Is You", "Mickey's Monkey", "Quiet Storm (single version)", "Take Me Through The Night", "If You Wanna Make Love (Come 'round Here)", "Quiet Storm (Groove Boutique Chill Jazz mix)", "Gone Forever", "Deck the Halls", "It's Her Turn to Live", "You've Really Go a Hold on Me", "Come to Me Soon", "Sweet Harmony", "Train of Thought", "Going to a Gogo", "I Can't Find", "Tears of a Sweet Free Clown", "Little Girl, Little Girl", "Can't Fight Love", "Crusin'", "Cruisin'", "Don't Play Another Love Song", "Going to a Go-Go", "The Hurt's On You", "Wedding Song", "Tea for Two", "Everything You Touch", "Ebony Eyes", "Aqui Con Tigo (Being With You)", "Gang Bangin'", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "Will You Still Love Me Tomorrow", "Ooo Baby Baby (live)", "Sleepless Nights", "When A Woman Cries", "Asleep on My Love", "He Can Fix Anything", "Share It", "No Time to Stop Believing", "I've Made Love To You A Thousand Times", "Because of You It's the Best It's Ever Been", "Love So Fine", "You Are So Beautiful (feat. Dave Koz)", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "Save Me", "We Are The Warriors", "Ooh Baby Baby", "Wishful Thinking", "Jasmin", "Crusin", "Never My Love / Never Can Say Goodbye", "You're Just My Life (feat. India.Arie)", "Being With You", "Shoe Soul", "More Than You Know", "Christmas Every Day", "I Am, I Am", "The Tears Of A Clown", "(It's The) Same Old Love", "The Track of My Tears", "Bad Girl", "One Time", "I'll Keep My Light In My Window", "Don't Wanna Be Just Physical", "I Like Your Face", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "You Are Forever", "No\u00ebl", "If You Wanna Make Love", "Tell Me Tomorrow", "Baby That's Backatcha", "Rewind", "Shop Around", "My World", "I Praise & Worship You Father", "Wanna Know My Mind", "There Will Come a Day (I'm Gonna Happen to You)", "Girl I'm Standing There", "Te Quiero Como Si No Hubiera Un Manana", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "Please Don't Take Your Love (feat. Carlos Santana)", "The Tears of a Clown", "When Smokey Sings Tears Of A Clown", "Pops, We Love You (disco)", "Heavy On Pride (Light On Love)", "Speak Low", "Just Another Kiss", "Melody Man", "Jingle Bells", "If You Can Want", "Same Old Love", "Night and Day", "Let Me Be The Clock", "I Care About Detroit", "The Christmas Song", "We've Saved The Best For Last (Kenny G with Smokey Robinson)", "I Can\u2019t Stand to See You Cry (Commercial version)", "You Go to My Head", "And I Love Her", "I Love Your Face", "Satisfy You", "Time After Time", "Just To See Her Again", "Hanging on by a Thread", "That Place", "Unless You Do It Again", "Skid Row", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "Girlfriend", "Daylight & Darkness", "Be Who You Are", "I've Made Love to You a Thousand Times", "The Agony And The Ecstasy", "Coincidentally", "Just a Touch Away", "Happy (Love Theme From Lady Sings the Blues)", "Jesus Told Me To Love You", "Tracks of My Tears", "Why Do Happy Memories Hurt So Bad", "Ever Had A Dream", "Be Kind To The Growing Mind (with The Temptations)", "Easy", "Come by Here (Kum Ba Ya)", "The Agony and the Ecstasy", "Just Like You", "Why Are You Running From My Love", "Love Bath", "Did You Know (Berry's Theme)", "Just Passing Through", "Winter Wonderland", "You Don't Know What It's Like", "You Cannot Laugh Alone", "Really Gonna Miss You", "The Tracks Of My Tears", "Theme From the Big Time", "I Am I Am", "Christmas Everyday", "Fallin'", "Our Love Is Here to Stay", "Tracks Of My Tears (Live)", "Food For Thought"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> US President -> type.type.properties -> President number\n# Answer:\nUS President", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> US President -> type.type.expected_by -> President\n# Answer:\nUS President", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> Artwork -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nArtwork", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> US President -> type.type.properties -> Vice president\n# Answer:\nUS President", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> Artwork -> freebase.type_hints.included_types -> Topic\n# Answer:\nArtwork", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> US President -> freebase.type_profile.kind -> Significant\n# Answer:\nUS President"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Montenegro\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> South Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> common.topic.notable_types -> Human Language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Bosnia and Herzegovina\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> sports.multi_event_tournament.participating_countries -> United States of America\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> common.topic.article -> m.0l8p56r\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Hungarian language -> common.topic.notable_types -> Human Language\n# Answer:\nHungarian language"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
