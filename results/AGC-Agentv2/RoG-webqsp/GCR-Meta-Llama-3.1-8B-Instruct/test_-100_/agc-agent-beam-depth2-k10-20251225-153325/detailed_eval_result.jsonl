{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Costa Rica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.region -> Americas\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_for -> g.1255wtfxx\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.notable_for -> g.1255wtfxx\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1900 Galveston hurricane -> meteorology.tropical_cyclone.affected_areas -> Atlantic Canada\n# Answer:\n1900 Galveston hurricane"], "ground_truth": ["Jamaican Creole English Language", "Jamaican English"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nm.04j60kh", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> United States Representative\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.district_represented -> Tennessee's 9th congressional district\n# Answer:\nm.04j60kh", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j5sk8 -> government.government_position_held.office_position_or_title -> Governor of Tennessee\n# Answer:\nm.04j5sk8", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Farmer\n# Answer:\nFarmer", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> President of the United States\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8 -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nm.04469y8", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8 -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nm.04469y8", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j5sk8 -> government.government_position_held.basic_title -> Governor\n# Answer:\nm.04j5sk8"], "ground_truth": ["Governor of Tennessee", "Speaker of the United States House of Representatives", "United States Representative"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.41379310344827586, "path_ans_precision": 0.3, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> Martinique\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French -> language.human_language.countries_spoken_in -> Martinique\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.main_country -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_1j97\n# Answer:\ng.1245_1j97"], "ground_truth": ["Haitian Creole", "French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.03ljbs2\n# Answer:\nm.03ljbs2", "# Reasoning Path:\nBarbara Gordon -> common.topic.notable_for -> g.12596ymdk\n# Answer:\ng.12596ymdk", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t8hv2 -> tv.regular_tv_appearance.actor -> Yvonne Craig\n# Answer:\nm.02t8hv2", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer\n# Answer:\nLawyer", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t91b7 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nm.02t91b7", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t91b7 -> tv.regular_tv_appearance.actor -> Melissa Gilbert\n# Answer:\nm.02t91b7"], "ground_truth": ["Melinda McGraw", "Ilyssa Fradin", "Hannah Gunn"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nm.04qjyzm", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qpksh -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nm.04qpksh", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.season -> 1992\u201393 NBA season\n# Answer:\nm.04qjyzm", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nm.04q9532", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qpksh -> basketball.basketball_player_stats.season -> 1994\u201395 NBA season\n# Answer:\nm.04qpksh", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player -> common.topic.notable_for -> g.125ctmqvt\n# Answer:\nPlayer", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.team -> Orlando Magic\n# Answer:\nm.02kbc3w", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.season -> 2003\u201304 NBA season\n# Answer:\nm.04q9532", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc5c -> sports.sports_award.team -> Los Angeles Lakers\n# Answer:\nm.02kbc5c", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player -> music.recording.releases -> You cant STOP the REIGN\n# Answer:\nPlayer"], "ground_truth": ["Cleveland Cavaliers", "Miami Heat", "LSU Tigers men's basketball", "Phoenix Suns", "Orlando Magic", "Boston Celtics", "Los Angeles Lakers"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.5, "path_recall": 0.16666666666666666, "path_ans_f1": 0.36363636363636365, "path_ans_precision": 0.5, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9\n# Answer:\nm.04fk_g9", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American\n# Answer:\nScottish American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.0cqc3ql\n# Answer:\nm.0cqc3ql", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.0n2qyfk -> award.award_honor.award -> People's Choice Award for Favorite Late Night Talk Show Host\n# Answer:\nm.0n2qyfk", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.0n2qyfk -> award.award_honor.honored_for -> The Tonight Show with Jay Leno\n# Answer:\nm.0n2qyfk", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.0n2qyfk -> award.award_honor.ceremony -> 32nd People's Choice Awards\n# Answer:\nm.0n2qyfk"], "ground_truth": ["New Rochelle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language\n# Answer:\nVietnamese Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Lao Language\n# Answer:\nLao Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> media_common.netflix_genre.titles -> Shutter\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> language.human_language.region -> Asia\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yg\n# Answer:\ng.12tb6f6yg", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> common.topic.notable_types -> Human Language\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6gh2z\n# Answer:\ng.12tb6gh2z", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.1hhc37pvk\n# Answer:\ng.1hhc37pvk"], "ground_truth": ["Phu Thai language", "Khmer language", "Lao Language", "Nyaw Language", "Akha Language", "Malay, Pattani Language", "Cham language", "Hmong language", "Mlabri Language", "Saek language", "Vietnamese Language", "Mon Language", "Thai Language"], "ans_acc": 0.3076923076923077, "ans_hit": 1, "ans_f1": 0.42105263157894735, "ans_precission": 0.6666666666666666, "ans_recall": 0.3076923076923077, "path_f1": 0.27272727272727276, "path_precision": 0.3333333333333333, "path_recall": 0.23076923076923078, "path_ans_f1": 0.42105263157894735, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.3076923076923077}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.owner -> Facebook, Inc.\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> award.award_nominated_work.award_nominations -> m.0_rfq07\n# Answer:\nm.0_rfq07", "# Reasoning Path:\nThe Social Network -> award.award_winning_work.awards_won -> m.0fpkghb\n# Answer:\nm.0fpkghb", "# Reasoning Path:\nThe Social Network -> award.award_nominated_work.award_nominations -> m.0dlskcl\n# Answer:\nm.0dlskcl", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.owner -> Mark Zuckerberg\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> award.award_nominated_work.award_nominations -> m.0fpnkz_\n# Answer:\nm.0fpnkz_", "# Reasoning Path:\nThe Social Network -> award.award_winning_work.awards_won -> m.0fpkgmk\n# Answer:\nm.0fpkgmk", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Social networking service -> common.topic.image -> Growth in Social Network Patent Applications\n# Answer:\nSocial networking service", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Social networking service -> computer.software_genre.software_in_genre -> 2go\n# Answer:\nSocial networking service"], "ground_truth": ["Tyler Winklevoss", "Cameron Winklevoss"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Statesman\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Farmer\n# Answer:\nFarmer", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Polly Jefferson\n# Answer:\nPolly Jefferson", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson"], "ground_truth": ["Inventor", "Lawyer", "Writer", "Architect", "Philosopher", "Author", "Archaeologist", "Statesman", "Teacher", "Farmer"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.47058823529411764, "ans_precission": 0.5714285714285714, "ans_recall": 0.4, "path_f1": 0.3529411764705882, "path_precision": 0.42857142857142855, "path_recall": 0.3, "path_ans_f1": 0.47058823529411764, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 0.4}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 11: 1863\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Great Britain\n# Answer:\nGreat Britain", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Charles Waring Darwin\n# Answer:\nCharles Waring Darwin"], "ground_truth": ["The Essential Darwin", "Charles Darwin's marginalia", "The Autobiography of Charles Darwin (Dodo Press)", "To the members of the Down Friendly Club", "The Power of Movement in Plants", "Charles Darwin on the routes of male humble bees", "Motsa ha-minim", "The Correspondence of Charles Darwin, Volume 9", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Charles Darwin's letters", "The Descent of Man, and Selection in Relation to Sex", "The voyage of Charles Darwin", "The action of carbonate of ammonia on the roots of certain plants", "Works", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The Correspondence of Charles Darwin, Volume 15: 1867", "The expression of the emotions in man and animals.", "The Variation of Animals and Plants under Domestication", "On the tendency of species to form varieties", "The principal works", "Kleinere geologische Abhandlungen", "A Darwin Selection", "Evolution", "The origin of species", "The education of Darwin", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The Expression of the Emotions in Man And Animals", "The Voyage of the Beagle", "Insectivorous Plants", "The foundations of the Origin of species", "The Voyage of the Beagle (Unabridged Classics)", "The Correspondence of Charles Darwin, Volume 8", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "The Correspondence of Charles Darwin, Volume 8: 1860", "Les moyens d'expression chez les animaux", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Darwin's insects", "The Correspondence of Charles Darwin, Volume 13: 1865", "Voyage of the Beagle (NG Adventure Classics)", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "The Correspondence of Charles Darwin, Volume 12", "Les mouvements et les habitudes des plantes grimpantes", "The Correspondence of Charles Darwin, Volume 14", "The Correspondence of Charles Darwin, Volume 3", "The geology of the voyage of H.M.S. Beagle", "Origin of Species", "More Letters of Charles Darwin", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "The Orgin of Species", "Diary of the voyage of H.M.S. Beagle", "Rejse om jorden", "Part I: Contributions to the Theory of Natural Selection / Part II", "Darwin Compendium", "Autobiography of Charles Darwin", "Darwinism stated by Darwin himself", "Gesammelte kleinere Schriften", "Het uitdrukken van emoties bij mens en dier", "Evolution by natural selection", "Evolution and natural selection", "Memorias y epistolario i\u0301ntimo", "The Origin of Species (Oxford World's Classics)", "The structure and distribution of coral reefs.", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "vari\u00eberen der huisdieren en cultuurplanten", "The Origin of Species (Enriched Classics)", "El Origin De Las Especies", "Darwin Darwin", "genese\u014ds t\u014dn eid\u014dn", "La vie et la correspondance de Charles Darwin", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "Charles Darwin's natural selection", "The Voyage of the Beagle (Mentor)", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "From so simple a beginning", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Cartas de Darwin 18251859", "red notebook of Charles Darwin", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The Correspondence of Charles Darwin, Volume 14: 1866", "On Natural Selection", "The living thoughts of Darwin", "Origin of Species (Harvard Classics, Part 11)", "The collected papers of Charles Darwin", "Darwin en Patagonia", "Darwin and Henslow", "The Structure and Distribution of Coral Reefs", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "The Autobiography of Charles Darwin (Great Minds Series)", "The Correspondence of Charles Darwin, Volume 15", "Origins", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Voyage of the Beagle (Dover Value Editions)", "The Different Forms of Flowers on Plants of the Same Species", "Resa kring jorden", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "The Voyage of the Beagle (Great Minds Series)", "The Correspondence of Charles Darwin, Volume 11: 1863", "The Voyage of the Beagle (Everyman Paperbacks)", "H.M.S. Beagle in South America", "Reise eines Naturforschers um die Welt", "The Origin of Species (Collector's Library)", "The Correspondence of Charles Darwin, Volume 10: 1862", "The Autobiography of Charles Darwin (Large Print)", "The Life of Erasmus Darwin", "Questions about the breeding of animals", "The Correspondence of Charles Darwin, Volume 17: 1869", "ontstaan der soorten door natuurlijke teeltkeus", "monograph on the sub-class Cirripedia", "The autobiography of Charles Darwin, 1809-1882", "The Origin of Species", "Darwin's journal", "The Correspondence of Charles Darwin, Volume 12: 1864", "The descent of man and selection in relation to sex.", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "The Correspondence of Charles Darwin, Volume 4", "La facult\u00e9 motrice dans les plantes", "The\u0301orie de l'e\u0301volution", "The Autobiography of Charles Darwin", "Voyage of the Beagle (Harvard Classics, Part 29)", "Fertilisation of Orchids", "The Correspondence of Charles Darwin, Volume 11", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "On evolution", "Reise um die Welt 1831 - 36", "The Correspondence of Charles Darwin, Volume 7", "The Correspondence of Charles Darwin, Volume 18: 1870", "Notebooks on transmutation of species", "The origin of species : complete and fully illustrated", "The structure and distribution of coral reefs", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "The Origin Of Species", "From So Simple a Beginning", "Die fundamente zur entstehung der arten", "Notes on the fertilization of orchids", "The Correspondence of Charles Darwin, Volume 1", "Proiskhozhdenie vidov", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The Origin of Species (World's Classics)", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The Origin of Species (Mentor)", "Die geschlechtliche Zuchtwahl", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "On a remarkable bar of sandstone off Pernambuco", "Wu zhong qi yuan", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "The Correspondence of Charles Darwin, Volume 2", "The autobiography of Charles Darwin", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Les r\u00e9cifs de corail, leur structure et leur distribution", "Voyage of the Beagle", "The portable Darwin", "Darwin-Wallace", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The Correspondence of Charles Darwin, Volume 6", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Darwin's Ornithological notes", "The Expression Of The Emotions In Man And Animals", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "The Correspondence of Charles Darwin, Volume 5", "The Autobiography of Charles Darwin [EasyRead Edition]", "The Correspondence of Charles Darwin, Volume 10", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Diario del Viaje de Un Naturalista Alrededor", "Voyage d'un naturaliste autour du monde", "The Darwin Reader First Edition", "A student's introduction to Charles Darwin", "The Structure And Distribution of Coral Reefs", "The Voyage of the Beagle (Adventure Classics)", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "Darwin", "The Correspondence of Charles Darwin, Volume 9: 1861", "Charles Darwin", "Del Plata a Tierra del Fuego", "The expression of the emotions in man and animals", "The descent of man, and selection in relation to sex", "On the origin of species by means of natural selection", "Darwin for Today", "The Origin of Species (Great Books : Learning Channel)", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "The descent of man, and selection in relation to sex.", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "The Correspondence of Charles Darwin, Volume 13", "Voyage Of The Beagle", "Leben und Briefe von Charles Darwin", "The Darwin Reader Second Edition", "Metaphysics, Materialism, & the evolution of mind", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Human nature, Darwin's view", "The Origin of Species (Variorum Reprint)", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "Darwin's notebooks on transmutation of species", "The Autobiography Of Charles Darwin", "Volcanic Islands", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "The voyage of the Beagle.", "The Descent of Man and Selection in Relation to Sex", "Opsht\u0323amung fun menshen", "From Darwin's unpublished notebooks", "Beagle letters", "The Formation of Vegetable Mould through the Action of Worms", "The Correspondence of Charles Darwin, Volume 16: 1868", "Geological Observations on South America", "The Expression of the Emotions in Man and Animals", "The Origin of Species (Great Minds Series)", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "The Autobiography of Charles Darwin, and selected letters", "Darwin on humus and the earthworm", "Tesakneri tsagume\u030c", "Origin of Species (Everyman's University Paperbacks)", "On the Movements and Habits of Climbing Plants"], "ans_acc": 0.06542056074766354, "ans_hit": 1, "ans_f1": 0.01840490797546012, "ans_precission": 0.6, "ans_recall": 0.009345794392523364, "path_f1": 0.29411764705882354, "path_precision": 1.0, "path_recall": 0.1724137931034483, "path_ans_f1": 0.12280701754385964, "path_ans_precision": 1.0, "path_ans_recall": 0.06542056074766354}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nm.0hpc6gc", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hpc6gc", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.team -> New York Jets\n# Answer:\nm.04nb7z0", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0j5dkn6 -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nm.0j5dkn6", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> book.written_work.author -> Nathan Whitaker\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.notable_types -> Book\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.04nb7z0", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.article -> m.0j4d5g4\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0j5dkn6 -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0j5dkn6", "# Reasoning Path:\nTim Tebow -> award.award_nominee.award_nominations -> m.0513_pw -> award.award_nomination.award -> Heisman Trophy\n# Answer:\nm.0513_pw"], "ground_truth": ["Florida Gators football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.07mmh5w\n# Answer:\nm.07mmh5w", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0jh9fcj", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.0791773\n# Answer:\nm.0791773", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.07sgy3b\n# Answer:\nm.07sgy3b", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nm.0jh9fcj", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0zs5mvy", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nDate of birth", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nm.0zs5mvy", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nGender"], "ground_truth": ["Denver Broncos"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Slovakia\n# Answer:\nSlovakia", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Hungary\n# Answer:\nHungary", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Babia G\u00f3ra\n# Answer:\nBabia G\u00f3ra", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Czech Republic\n# Answer:\nCzech Republic", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Brebeneskul\n# Answer:\nBrebeneskul", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Bucura Dumbrav\u0103 -> location.location.containedby -> Europe\n# Answer:\nBucura Dumbrav\u0103", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Bucura Dumbrav\u0103 -> location.location.containedby -> Bucegi Mountains\n# Answer:\nBucura Dumbrav\u0103", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Bucura Dumbrav\u0103 -> common.topic.notable_for -> g.125d3kbg5\n# Answer:\nBucura Dumbrav\u0103", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Bucura Dumbrav\u0103 -> location.location.containedby -> Romania\n# Answer:\nBucura Dumbrav\u0103"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Author\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> people.person.gender -> Male\n# Answer:\nMale", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.profession -> Writer\n# Answer:\nAnne Bront\u00eb", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Bob Dylan\n# Answer:\nBob Dylan", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Charlotte Bront\u00eb -> people.person.profession -> Writer\n# Answer:\nCharlotte Bront\u00eb", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.profession -> Governess\n# Answer:\nAnne Bront\u00eb", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.ethnicity -> Irish people in Great Britain\n# Answer:\nAnne Bront\u00eb", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.profession -> Novelist\n# Answer:\nAnne Bront\u00eb"], "ground_truth": ["Poet", "Writer", "Author", "Bard"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.4285714285714285, "ans_precission": 0.3, "ans_recall": 0.75, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.6, "path_ans_precision": 0.5, "path_ans_recall": 0.75}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.film -> Star Wars Episode III: Revenge of the Sith\n# Answer:\nm.02sg5ry", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.actor -> Hayden Christensen\n# Answer:\nm.02sg5s6", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.actor -> Hayden Christensen\n# Answer:\nm.02sg5ry", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.film -> Star Wars Episode II: Attack of the Clones\n# Answer:\nm.02sg5s6", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nm.0j7zstf", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.part_of_series -> Jedi Quest\n# Answer:\nPath to Truth", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.author -> Judy Blundell\n# Answer:\nPath to Truth"], "ground_truth": ["Hayden Christensen"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.award -> Super Bowl Most Valuable Player Award\n# Answer:\nm.04kcv8b", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_8 -> sports.sports_award.season -> 2003 NFL season\n# Answer:\nm.04kg9_8", "# Reasoning Path:\nPeyton Manning -> people.person.spouse_s -> m.0hpd4nj\n# Answer:\nm.0hpd4nj", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.season -> 2006 NFL season\n# Answer:\nm.04kcv8b", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_j -> sports.sports_award.award -> National Football League Most Valuable Player Award\n# Answer:\nm.04kg9_j", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nDate of birth", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nGender", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_8 -> sports.sports_award.award -> National Football League Most Valuable Player Award\n# Answer:\nm.04kg9_8", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_j -> sports.sports_award.season -> 2004 NFL season\n# Answer:\nm.04kg9_j"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.administrative_divisions -> Ontario\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.languages_spoken -> English Language\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> common.topic.notable_types -> Country\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.languages_spoken -> Spanish Language\n# Answer:\nCanada"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5\n# Answer:\nm.03z96c5", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.06vz4t9 -> military.military_combatant_group.combatants -> Argentina\n# Answer:\nm.06vz4t9", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z973l\n# Answer:\nm.03z973l", "# Reasoning Path:\nGulf War -> time.event.locations -> Israel\n# Answer:\nIsrael", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> royalty.kingdom.rulers -> Abdullah of Saudi Arabia\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.locations -> Arabian Peninsula\n# Answer:\nArabian Peninsula", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> location.country.official_language -> Arabic Language\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wpj2\n# Answer:\nm.043wpj2", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wphl -> military.casualties.combatant -> United States of America\n# Answer:\nm.043wphl", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> base.locations.countries.continent -> Asia\n# Answer:\nSaudi Arabia"], "ground_truth": ["Iraq", "Argentina", "France", "Saudi Arabia", "United Kingdom", "United States of America", "Australia"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.19354838709677416, "ans_precission": 0.3, "ans_recall": 0.14285714285714285, "path_f1": 0.09523809523809525, "path_precision": 0.5, "path_recall": 0.05263157894736842, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.5, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym\n# Answer:\nm.040p0ym", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys\n# Answer:\nm.040p0ys", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.05v3ngr\n# Answer:\nm.05v3ngr", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkyvs\n# Answer:\nm.0sgkyvs", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Show\n# Answer:\nm.0sgkxsb", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.episodes -> A London Carol\n# Answer:\nA London Carol", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> award.award_nomination.ceremony -> 2011 Kids' Choice Awards\n# Answer:\nm.0sgkxsb", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.episodes -> Ala-ka-scram!\n# Answer:\nAla-ka-scram!", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> freebase.valuenotation.has_value -> Award Nominee\n# Answer:\nm.0sgkxsb", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkz86 -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Sidekick\n# Answer:\nm.0sgkz86"], "ground_truth": ["Brenda Song"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.office_holder -> George Voinovich\n# Answer:\nm.05kfccr", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_holder -> Sherrod Brown\n# Answer:\nm.05kg_6s", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0bfmhl0 -> government.government_position_held.office_holder -> Mike DeWine\n# Answer:\nm.0bfmhl0", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.image -> The Rhodes State Office Tower adjacent to the Statehouse\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.notable_for -> g.125h3hwcp\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.legislative_sessions -> 106th United States Congress\n# Answer:\nm.05kfccr", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nm.05kg_6s", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> meteorology.tropical_cyclone.affected_areas -> Alabama\n# Answer:\nHurricane Bob", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> common.topic.notable_for -> g.1259xs2jv\n# Answer:\nHurricane Bob", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Frances -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\nHurricane Frances"], "ground_truth": ["John Kasich", "Return J. Meigs, Jr.", "Ted Strickland"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nm.0w9plqp", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m -> soccer.football_player_stats.team -> LA Galaxy\n# Answer:\nm.0w8w79m", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9021c\n# Answer:\nm.0w9021c", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nm.0j2zzj_", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0k4ytw5", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0j2zzj_", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nm.0k4ytw5", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.team -> Paris Saint-Germain F.C.\n# Answer:\nm.0qzkj58", "# Reasoning Path:\nDavid Beckham -> tv.tv_actor.guest_roles -> m.0_z851f -> tv.tv_guest_role.special_performance_type -> Him/Herself\n# Answer:\nm.0_z851f", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nm.0qzkj58"], "ground_truth": ["LA Galaxy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3529411764705882, "path_precision": 0.3, "path_recall": 0.42857142857142855, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado\n# Answer:\nCoronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Hidalgo\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Zacuto\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7t8559g\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Alfonso XI of Castile\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> common.topic.image -> Die Plaza Mayor am Abend\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado Historic Site\n# Answer:\nCoronado Historic Site", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7tbpl3m\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado National Forest\n# Answer:\nCoronado National Forest", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7vzj2hj\n# Answer:\nSalamanca"], "ground_truth": ["Salamanca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.namesake.named_after -> John F. Fitzgerald\n# Answer:\nJohn F. Fitzgerald", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> base.americancomedy.impersonated_celebrity.impersonated_by -> Rich Little\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School\n# Answer:\nBloomington Kennedy High School", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of death\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Cape Canaveral\n# Answer:\nCape Canaveral", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> 11/22/63\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> common.topic.notable_types -> Event\n# Answer:\nAssassination of John F. Kennedy"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nm.0clv1h_", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_for -> g.125btmfy0\n# Answer:\nFukushima I \u2013 1", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_types -> Structure\n# Answer:\nFukushima I \u2013 1", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 3 -> common.topic.notable_types -> Structure\n# Answer:\nFukushima I \u2013 3", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 2 -> common.topic.notable_for -> g.1257z60_h\n# Answer:\nFukushima I \u2013 2", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 2 -> base.infrastructure.nuclear_power_plant.reactor_type -> Boiling water reactor\n# Answer:\nFukushima I \u2013 2", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 3 -> common.topic.notable_for -> g.125bty6m2\n# Answer:\nFukushima I \u2013 3", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 3 -> base.infrastructure.nuclear_power_plant.coolant -> Water\n# Answer:\nFukushima I \u2013 3"], "ground_truth": ["Japan", "Okuma"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Wales\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h\n# Answer:\nm.0ghc35h", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> England\n# Answer:\nEngland"], "ground_truth": ["Northern Ireland", "Wales", "Scotland", "England"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Bartholomew\n# Answer:\nBayou Bartholomew", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> geography.river.cities -> Denham Springs\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Macon\n# Answer:\nBayou Macon", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wg8__0 -> location.partial_containment_relationship.partially_contains -> Sabine River\n# Answer:\nm.0wg8__0", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wg8___\n# Answer:\nm.0wg8___", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> geography.river.mouth -> Lake Maurepas\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> location.location.partially_contained_by -> m.0wg8__h\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wg8__5 -> location.partial_containment_relationship.partially_contains -> Tickfaw River\n# Answer:\nm.0wg8__5", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> geography.river.mouth -> Livingston Parish\n# Answer:\nAmite River"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.profession -> Philosopher\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> people.person.quotations -> A thing is not necessarily true because badly uttered, nor false because spoken magnificently.\n# Answer:\nA thing is not necessarily true because badly uttered, nor false because spoken magnificently.", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Albert Camus\n# Answer:\nAlbert Camus", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alvin Plantinga -> people.person.profession -> Physician\n# Answer:\nAlvin Plantinga", "# Reasoning Path:\nAugustine of Hippo -> people.person.quotations -> By faithfulness we are collected and wound up into unity within ourselves, whereas we had been scattered abroad in multiplicity. -> media_common.quotation.subjects -> Faithfulness\n# Answer:\nBy faithfulness we are collected and wound up into unity within ourselves, whereas we had been scattered abroad in multiplicity.", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.nationality -> United Kingdom\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> book.author.works_written -> A short history of ethics\n# Answer:\nAlasdair MacIntyre"], "ground_truth": ["Physician", "Writer", "Philosopher"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> people.person.children -> Sean Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Sal Gibson\n# Answer:\nSal Gibson", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> freebase.valuenotation.has_value -> Parents\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> common.topic.notable_types -> Person\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan\n# Answer:\n#1 Fan", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> people.person.children -> Neffeteria Pugh\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> award.award_nominee.award_nominations -> m.05cqdz0\n# Answer:\nm.05cqdz0", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> common.topic.notable_types -> Person\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> (When You Gonna) Give It Up To Me\n# Answer:\n(When You Gonna) Give It Up To Me"], "ground_truth": ["Francine Lons", "Sal Gibson", "Leon Cole"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> France\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\ng.12cp_k6sh", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Portugal\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\ng.1hhc3_4cn", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Syria\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> common.topic.notable_types -> Governmental Body\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3f_h6\n# Answer:\ng.1hhc3f_h6", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nShura Council"], "ground_truth": ["Provisional government", "Semi-presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.events -> Assassination of Martin Luther King, Jr.\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.containedby -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh\n# Answer:\nm.011j_4sh", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0_714v2\n# Answer:\nm.0_714v2", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0gbz10_\n# Answer:\nm.0gbz10_", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dr. Martin Luther King, Jr. Library -> location.location.geolocation -> m.0wmmldf\n# Answer:\nDr. Martin Luther King, Jr. Library", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dr. Martin Luther King, Jr. Library -> common.topic.image -> 2008-0817-SJSU-MLKlib\n# Answer:\nDr. Martin Luther King, Jr. Library", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dr. Martin Luther King, Jr. Library -> common.topic.image -> Southeast entrance of the King Library\n# Answer:\nDr. Martin Luther King, Jr. Library"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Agatha Christie\n# Answer:\nAgatha Christie", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> Maryland\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Alan Moore\n# Answer:\nAlan Moore", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.\n# Answer:\nA strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong.\n# Answer:\nA wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong.", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> After reading all that has been written, and after thinking all that can be thought, on the topics of God and the soul, the man who has a right to say that he thinks at all, will find himself face to face with the conclusion that, on these topics, the most profound thought is that which can be the least easily distinguished from the most superficial sentiment.\n# Answer:\nAfter reading all that has been written, and after thinking all that can be thought, on the topics of God and the soul, the man who has a right to say that he thinks at all, will find himself face to face with the conclusion that, on these topics, the most profound thought is that which can be the least easily distinguished from the most superficial sentiment.", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Albrecht Behmel -> book.author.school_or_movement -> Modernism\n# Answer:\nAlbrecht Behmel", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Albrecht Behmel -> people.person.gender -> Male\n# Answer:\nAlbrecht Behmel"], "ground_truth": ["Baltimore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall\n# Answer:\nArbor Place Mall", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.03ldb41\n# Answer:\nm.03ldb41", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> travel.transport_terminus.travel_destinations_served -> m.052zwlt\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> location.location.containedby -> Georgia\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Cobb County Airport\n# Answer:\nCobb County Airport", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> common.topic.notable_types -> Airport\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Atlanta Regional Airport -> aviation.airport.serves -> Peachtree City\n# Answer:\nAtlanta Regional Airport", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.0542hwn -> common.webpage.resource -> Visitor Information\n# Answer:\nm.0542hwn"], "ground_truth": ["Georgia State Capitol", "Turner Field", "Margaret Mitchell House & Museum", "Arbor Place Mall", "Underground Atlanta", "Atlanta Marriott Marquis", "Center for Puppetry Arts", "Atlanta Cyclorama & Civil War Museum", "Zoo Atlanta", "Centennial Olympic Park", "Cobb Energy Performing Arts Centre", "Philips Arena", "Jimmy Carter Library and Museum", "The Tabernacle", "Peachtree Road Race", "Atlanta Symphony Orchestra", "Woodruff Arts Center", "Georgia World Congress Center", "Georgia Dome", "Hyatt Regency Atlanta", "Variety Playhouse", "Masquerade", "Four Seasons Hotel Atlanta", "Atlanta Jewish Film Festival", "World of Coca-Cola", "Fernbank Science Center", "Atlanta Ballet", "Six Flags Over Georgia", "Georgia Aquarium", "CNN Center", "Fox Theatre", "Fernbank Museum of Natural History", "Martin Luther King, Jr. National Historic Site", "Atlanta History Center", "Omni Coliseum", "Six Flags White Water"], "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0.13043478260869565, "ans_precission": 0.3, "ans_recall": 0.08333333333333333, "path_f1": 0.13043478260869565, "path_precision": 0.3, "path_recall": 0.08333333333333333, "path_ans_f1": 0.13043478260869565, "path_ans_precision": 0.3, "path_ans_recall": 0.08333333333333333}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nm.0cr320w", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Adrienne Pickering\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> common.topic.notable_types -> City/Town/Village\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Alan Marshal\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Alison Mosely\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.contains -> Assumption College, Warwick\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.district_represented -> Electoral district of South Brisbane\n# Answer:\nm.0cr320w", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr301h -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nm.0cr301h", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.size -> m.05t654b\n# Answer:\nAnna Bligh crop", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.contains -> Australian Rodeo Heritage Centre\n# Answer:\nWarwick"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> people.person.gender -> Male\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> film.actor.film -> m.0h0_mvx\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Albert Tatlock\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Alf Roberts\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Annie Walker\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> common.topic.notable_for -> g.1255pdp4f\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Soap opera -> common.topic.webpage -> m.09w1_hc\n# Answer:\nSoap opera", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> common.topic.notable_types -> Multipart TV episode\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> tv.tv_genre.programs -> 100% Senorita\n# Answer:\nChivalric romance", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Soap opera -> common.topic.article -> m.06q7y\n# Answer:\nSoap opera"], "ground_truth": ["William Roache"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> tennis.tennis_player.matches_won -> m.010pyd6z\n# Answer:\nm.010pyd6z", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nDate of birth", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nGender", "# Reasoning Path:\nAndy Murray -> tennis.tennis_player.matches_won -> m.010q5kls\n# Answer:\nm.010q5kls", "# Reasoning Path:\nAndy Murray -> tennis.tennis_player.matches_won -> m.010qb6kd\n# Answer:\nm.010qb6kd"], "ground_truth": ["2005"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island\n# Answer:\nChristmas Island", "# Reasoning Path:\nAustralian dollar -> base.ontologies.ontology_instance.equivalent_instances -> m.09knr56\n# Answer:\nm.09knr56", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands\n# Answer:\nCocos (Keeling) Islands", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gf\n# Answer:\nm.04lt3gf", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gl\n# Answer:\nm.04lt3gl", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gr\n# Answer:\nm.04lt3gr"], "ground_truth": ["AUD"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\ng.11b60n70x2", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\ng.12cp_k2s4", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ll\n# Answer:\ng.1245_22ll", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22zj\n# Answer:\ng.1245_22zj", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc378pv\n# Answer:\ng.1hhc378pv", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc37hbq\n# Answer:\ng.1hhc37hbq"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> common.topic.notable_for -> g.125dysc88\n# Answer:\ng.125dysc88", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr.\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> people.person.parents -> Jackie Newton\n# Answer:\nJackie Newton", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hpgnsj", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0z23kt0 -> sports.sports_team_roster.team -> Auburn Tigers football\n# Answer:\nm.0z23kt0", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.0hpgnsj", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0z23kt0 -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.0z23kt0"], "ground_truth": ["Carolina Panthers"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.containedby -> United States, with Territories\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.people_born_here -> Alfred G. Mayer\n# Answer:\nAlfred G. Mayer", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.containedby -> United States of America\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.location.people_born_here -> Bazabeel Norman\n# Answer:\nBazabeel Norman", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.location.people_born_here -> Bert Myers\n# Answer:\nBert Myers"], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1\n# Answer:\nm.0lwxmy1", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h\n# Answer:\nm.0n1l46h", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138\n# Answer:\nm.04hx138", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_types -> Book Edition\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.place_of_birth -> Los Angeles\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.gender -> Female\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_for -> g.125920htw\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nHarper Seven Beckham"], "ground_truth": ["Monroe County High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> book.written_work.author -> Thomas G. Alexander\n# Answer:\nUtah, the right place", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.010flwmg", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> common.topic.notable_for -> g.125dtp7bg\n# Answer:\nUtah, the right place", "# Reasoning Path:\nUtah -> book.book_subject.works -> Blossoms of faith -> common.topic.notable_for -> g.125fjpyr7\n# Answer:\nBlossoms of faith", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.04kcmn6\n# Answer:\nm.04kcmn6", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> book.written_work.subjects -> History\n# Answer:\nUtah, the right place", "# Reasoning Path:\nUtah -> book.book_subject.works -> Bound for Canaan -> book.written_work.author -> Margaret Blair Young\n# Answer:\nBound for Canaan"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointed_role -> Director of the Bureau of Counterterrorism\n# Answer:\nm.010p95d2", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pk -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.010l29pk", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointee -> Henry A. Crumpton\n# Answer:\nm.010p95d2", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1 -> people.appointment.appointed_role -> United States Ambassador to Angola\n# Answer:\nm.079pxt1", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079q3lx -> people.appointment.appointed_role -> United States Ambassador to Brazil\n# Answer:\nm.079q3lx", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pv -> film.personal_film_appearance.film -> All About Ann: Governor Richards of the Lone Star State\n# Answer:\nm.010l29pv", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1 -> people.appointment.appointee -> Dan Mozena\n# Answer:\nm.079pxt1", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pk -> film.personal_film_appearance.film -> All About Ann: Governor Richards of the Lone Star State\n# Answer:\nm.010l29pk", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.0115sdhb -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.0115sdhb"], "ground_truth": ["Michael Peroutka", "Ralph Nader", "Gene Amondson", "John Kerry"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57\n# Answer:\nm.0j4jq57", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5\n# Answer:\nm.0pdtjg5", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0h4nmc7\n# Answer:\nm.0h4nmc7", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0h4nq4f\n# Answer:\nm.0h4nq4f", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0pdthbn\n# Answer:\nm.0pdthbn"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Pacific Ocean\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Pacific Ocean\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tourist_attractions -> Floreana Island\n# Answer:\nFloreana Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Floreana Island\n# Answer:\nFloreana Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> common.topic.article -> m.03660h\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> common.topic.notable_for -> g.1255fs0l4\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Baltra Island\n# Answer:\nBaltra Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tourist_attractions -> Gal\u00e1pagos National Park\n# Answer:\nGal\u00e1pagos National Park"], "ground_truth": ["Pacific Ocean", "Gal\u00e1pagos Province", "Ecuador"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> Mistletoe\n# Answer:\nMistletoe", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z\n# Answer:\nm.010lkp2z", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> music.composition.composer -> Andre Harris\n# Answer:\nAll Bad", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0115qhzk\n# Answer:\nm.0115qhzk", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> music.album.release_type -> Single\n# Answer:\nAll Bad", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> music.composition.composer -> Dre & Vidal\n# Answer:\nAll Bad", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> common.topic.notable_for -> g.1yl5r3f4x\n# Answer:\nAll Bad"], "ground_truth": ["Live My Life", "Baby", "Never Say Never", "Never Let You Go", "Change Me", "Home to Mama", "Heartbreaker", "Die in Your Arms", "All That Matters", "First Dance", "Thought Of You", "#thatPower", "Hold Tight", "Lolly", "PYD", "Bad Day", "Bigger", "Right Here", "As Long as You Love Me", "Roller Coaster", "All Bad", "Wait for a Minute", "All Around The World", "Pray", "Confident", "Somebody to Love", "Recovery", "Boyfriend", "Beauty And A Beat", "Turn to You (Mother's Day Dedication)", "Eenie Meenie"], "ans_acc": 0.06451612903225806, "ans_hit": 1, "ans_f1": 0.1142857142857143, "ans_precission": 0.5, "ans_recall": 0.06451612903225806, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.1142857142857143, "path_ans_precision": 0.5, "path_ans_recall": 0.06451612903225806}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> common.topic.subjects -> Ciro Pellegrino\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Publisher\n# Answer:\nPublisher", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Acetylene Lamp\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> base.descriptive_names.names.descriptive_name -> m.0105cq_d\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> type.type.expected_by -> politician\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Ali G\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Physician -> media_common.quotation_subject.quotations_about_this_subject -> A doctor, like anyone else who has to deal with human beings, each of them unique, cannot be a scientist; he is either, like the surgeon, a craftsman, or, like the physician and the psychologist, an artist. This means that in order to be a good doctor a man must also have a good character, that is to say, whatever weaknesses and foibles he may have, he must love his fellow human beings in the concrete and desire their good before his own.\n# Answer:\nPhysician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> freebase.type_profile.kind -> Title\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> April Carver\n# Answer:\nJournalist"], "ground_truth": ["Writer", "Publisher", "Journalist", "Physician", "Statesman"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6461538461538462, "ans_precission": 0.7, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.6461538461538462, "path_ans_precision": 0.7, "path_ans_recall": 0.6}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nm.0hz8v06", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.Kind_of_symbol -> State flower\n# Answer:\nm.0hz8v06", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.symbol -> Petrified wood\n# Answer:\nm.04l0cxp", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz\n# Answer:\nm.04st6lz", "# Reasoning Path:\nArizona -> common.topic.notable_types -> US State -> freebase.type_profile.strict_included_types -> Administrative Division\n# Answer:\nUS State", "# Reasoning Path:\nArizona -> common.topic.notable_types -> US State -> type.type.expected_by -> Indiana\n# Answer:\nUS State", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc\n# Answer:\nm.09w1gvc", "# Reasoning Path:\nArizona -> common.topic.notable_types -> US State -> freebase.type_profile.equivalent_topic -> U.S. state\n# Answer:\nUS State", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.Kind_of_symbol -> State fossil\n# Answer:\nm.04l0cxp", "# Reasoning Path:\nArizona -> common.topic.notable_types -> US State -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nUS State"], "ground_truth": ["Saguaro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\ng.11bv1mttmr", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\ng.11bv383dbd", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> m.010_ydxv\n# Answer:\nm.010_ydxv", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.064_ltw\n# Answer:\nm.064_ltw", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.063y0bl -> base.popstra.infidelity.victim -> Beyonc\u00e9 Knowles\n# Answer:\nm.063y0bl", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.063y0bl -> base.popstra.infidelity.perpetrator -> Jay-Z\n# Answer:\nm.063y0bl"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> Harrison inauguration\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\nm.02bk8zt", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Illinois\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.03qtjkt\n# Answer:\nm.03qtjkt", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> 222px-Harrison_inauguration1841.jpg\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Illinois\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Indiana\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Iowa\n# Answer:\nUnited States of America"], "ground_truth": ["1841-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.03lppm1\n# Answer:\nm.03lppm1", "# Reasoning Path:\nGeorge Lopez -> film.person_or_entity_appearing_in_film.films -> m.0cs3c3w\n# Answer:\nm.0cs3c3w", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.0948qtd\n# Answer:\nm.0948qtd", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.094cr65\n# Answer:\nm.094cr65", "# Reasoning Path:\nGeorge Lopez -> film.person_or_entity_appearing_in_film.films -> m.0cs8ydd\n# Answer:\nm.0cs8ydd", "# Reasoning Path:\nGeorge Lopez -> film.person_or_entity_appearing_in_film.films -> m.0djcy_v\n# Answer:\nm.0djcy_v"], "ground_truth": ["Mission Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf\n# Answer:\nm.03l9ynf", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate\n# Answer:\nConglomerate"], "ground_truth": ["Suwon"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.deities -> God\n# Answer:\nGod", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Barat Night\n# Answer:\nBarat Night", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Day of Ashura\n# Answer:\nDay of Ashura", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Day of Arafat -> common.topic.article -> m.027cs4d\n# Answer:\nDay of Arafat", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Day of Arafat -> common.topic.notable_for -> g.125dtpf6f\n# Answer:\nDay of Arafat", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Day of Arafat -> time.day_of_year.calendar_system -> Islamic calendar\n# Answer:\nDay of Arafat"], "ground_truth": ["Masih ad-Dajjal", "Mahdi", "Islamic view of angels", "Sharia", "Prophets in Islam", "\u1e6c\u016bb\u0101", "Predestination in Islam", "God in Islam", "Tawhid", "Entering Heaven alive", "Islamic holy books", "Monotheism", "Qiyamah"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2\n# Answer:\nm.0ydn3r2", "# Reasoning Path:\nChristian Grey -> common.topic.notable_for -> g.125czvn3w\n# Answer:\ng.125czvn3w", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Males\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> common.topic.notable_for -> g.1255p_jgc\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Acute myeloid leukemia\n# Answer:\nMale"], "ground_truth": ["Jamie Dornan"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Antibiotics\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> people.cause_of_death.parent_cause_of_death -> Infectious disease\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Isoniazid\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Pyrazinamide\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.prevention_factors -> Isoniazid\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Aldous Huxley -> people.deceased_person.cause_of_death -> Laryngeal cancer\n# Answer:\nAldous Huxley", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion. -> common.topic.notable_types -> Quotation\n# Answer:\nA dirty joke is a sort of mental rebellion.", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Arthur Koestler\n# Answer:\nArthur Koestler", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.prevention_factors -> Pyrazinamide\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion. -> common.topic.notable_for -> g.12599cgh4\n# Answer:\nA dirty joke is a sort of mental rebellion."], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.office_position_or_title -> Chancellor of Germany\n# Answer:\nm.08_ksdj", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nm.0pz073c", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nm.08_ksdj", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ng.1254zbtvv", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.office_position_or_title -> F\u00fchrer\n# Answer:\nm.0pz073c", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.basic_title -> F\u00fchrer\n# Answer:\nm.0pz073c", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> 1st SS Panzer Division Leibstandarte SS Adolf Hitler -> organization.organization.place_founded -> Nazi Germany\n# Answer:\n1st SS Panzer Division Leibstandarte SS Adolf Hitler", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> 1st SS Panzer Division Leibstandarte SS Adolf Hitler -> base.schemastaging.context_name.pronunciation -> m.01314fx8\n# Answer:\n1st SS Panzer Division Leibstandarte SS Adolf Hitler"], "ground_truth": ["Nazi Germany"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.28571428571428564, "path_precision": 0.3, "path_recall": 0.2727272727272727, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Actor\n# Answer:\nActor", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.notable_types -> Musical Artist\n# Answer:\nMusical Artist", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> common.topic.notable_types -> Profession\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> http://www.discogs.com/artist/Michael+Bubl%E9\n# Answer:\nhttp://www.discogs.com/artist/Michael+Bubl%E9", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.specialization_of -> Musician\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.specialization_of -> Musician\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.03lpqjt\n# Answer:\nm.03lpqjt", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.09w699t\n# Answer:\nm.09w699t"], "ground_truth": ["Singer", "Songwriter", "Actor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas City\n# Answer:\nKansas City", "# Reasoning Path:\nKansas Speedway\n# Answer:\nKansas Speedway"], "ground_truth": ["Wyandotte County"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h\n# Answer:\nm.0hpgh_h", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbp_k\n# Answer:\nm.06sbp_k", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0gggrzr\n# Answer:\nm.0gggrzr", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l\n# Answer:\nm.06sbq0l", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgj2z\n# Answer:\nm.0hpgj2z", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt\n# Answer:\nm.06sbpzt", "# Reasoning Path:\nJackie Robinson -> common.topic.notable_types -> Baseball Player -> type.type.properties -> Baseball Almanac ID\n# Answer:\nBaseball Player", "# Reasoning Path:\nJackie Robinson -> common.topic.notable_types -> Baseball Player -> type.type.expected_by -> Player\n# Answer:\nBaseball Player", "# Reasoning Path:\nJackie Robinson -> common.topic.notable_types -> Film character -> freebase.type_profile.published -> Published\n# Answer:\nFilm character", "# Reasoning Path:\nJackie Robinson -> common.topic.notable_types -> Baseball Player -> type.type.properties -> Bats\n# Answer:\nBaseball Player"], "ground_truth": ["UCLA Bruins football"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> location.location.containedby -> New York\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Play\n# Answer:\nPlay", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> base.militaryinfiction.location_in_fiction.contained_by -> New York\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> film.film.story_by -> Harold Gray\n# Answer:\nHarold Gray", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Composer\n# Answer:\nComposer", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> location.location.containedby -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Choreographer\n# Answer:\nChoreographer"], "ground_truth": ["1976-08-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.education -> m.03gkqtp\n# Answer:\nm.03gkqtp", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07919ln\n# Answer:\nm.07919ln", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07mmjx4\n# Answer:\nm.07mmjx4", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07sg_z8\n# Answer:\nm.07sg_z8"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> This I remember\n# Answer:\nThis I remember", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Anna Roosevelt Halsted\n# Answer:\nAnna Roosevelt Halsted", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Autobiography of Eleanor Roosevelt\n# Answer:\nAutobiography of Eleanor Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Courage in a dangerous world\n# Answer:\nCourage in a dangerous world", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Elliott Roosevelt\n# Answer:\nElliott Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Franklin D. Roosevelt, Jr.\n# Answer:\nFranklin D. Roosevelt, Jr."], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf2_w\n# Answer:\nm.03xf2_w", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf301\n# Answer:\nm.03xf301", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.064szjw\n# Answer:\nm.064szjw", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60ptk2z\n# Answer:\ng.11b60ptk2z", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.12cp_j7n1\n# Answer:\ng.12cp_j7n1", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_4m6h\n# Answer:\ng.1245_4m6h", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_67l9\n# Answer:\ng.1245_67l9", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.1hhc378kt\n# Answer:\ng.1hhc378kt", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.1hhc38d0l\n# Answer:\ng.1hhc38d0l"], "ground_truth": ["Catholicism", "Protestantism", "Hinduism", "Islam"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination\n# Answer:\nAssassination", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Countryway Gunshop\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Projectile weapon\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.image -> HK USP 45\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Springfield Armory, Inc.\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> base.weapons.weapon.subtypes -> Handgun\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.image -> M&Prevolver\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> base.weapons.weapon.subtypes -> Rifle\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> base.weapons.weapon.subtypes -> Semi-automatic firearm\n# Answer:\nFirearm"], "ground_truth": ["Firearm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.nationality -> United States of America\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.profession -> Politician\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> common.topic.notable_types -> US President\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pgr_5\n# Answer:\nm.03pgr_5", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> common.topic.article -> m.03mpv\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pn4x_\n# Answer:\nm.03pn4x_", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.04hdfss\n# Answer:\nm.04hdfss", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> people.person.gender -> Male\n# Answer:\nHannibal Hamlin"], "ground_truth": ["Andrew Johnson", "Hannibal Hamlin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Oliver Twist -> common.topic.image -> \\\"Please, sir, I want some more.\\\" Illustration by George Cruikshank.\n# Answer:\nOliver Twist", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced_by -> H. G. Wells\n# Answer:\nAldous Huxley", "# Reasoning Path:\nCharles Dickens -> base.kwebbase.kwtopic.has_sentences -> (1846)\n# Answer:\n(1846)", "# Reasoning Path:\nCharles Dickens -> base.kwebbase.kwtopic.has_sentences -> Ada Lovelace asked to see him when she was dying in 1847.\n# Answer:\nAda Lovelace asked to see him when she was dying in 1847.", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Oliver Twist -> tv.tv_character.appeared_in_tv_program -> m.0pdqx4p\n# Answer:\nOliver Twist", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Oliver Twist -> tv.tv_character.appeared_in_tv_program -> m.0wh4clp\n# Answer:\nOliver Twist", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> people.person.nationality -> United Kingdom\n# Answer:\nAldous Huxley", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced -> Anthony Burgess\n# Answer:\nAldous Huxley"], "ground_truth": ["Oliver Twist", "A Christmas Carol (The Kennett Library)", "A Tale of Two Cities (Longman Fiction)", "A Christmas Carol (Chrysalis Children's Classics Series)", "A Tale of Two Cities (Macmillan Students' Novels)", "The life and adventures of Nicholas Nickleby", "A Tale of Two Cities (Isis Clear Type Classic)", "A Tale of Two Cities (Wordsworth Classics)", "David Copperfield", "A Tale of Two Cities (Bantam Classic)", "A Christmas Carol (Oxford Bookworms Library)", "A Tale of Two Cities (Puffin Classics)", "A Tale of Two Cities (Simple English)", "A Christmas Carol (Read & Listen Books)", "A Tale of Two Cities (Dodo Press)", "A Christmas Carol (Cover to Cover)", "The old curiosity shop.", "Great expectations", "A Tale of Two Cities (Masterworks)", "A Tale of Two Cities (Dover Thrift Editions)", "The mystery of Edwin Drood", "A Tale of Two Cities (Ladybird Children's Classics)", "A Christmas Carol (Children's Classics)", "A Christmas Carol (R)", "A Tale of Two Cities (Acting Edition)", "A Tale of Two Cities", "A Tale of Two Cities (Silver Classics)", "A Tale of Two Cities (Penguin Readers, Level 5)", "A CHRISTMAS CAROL", "A Tale of Two Cities (Compact English Classics)", "A Tale of Two Cities (Collector's Library)", "A Tale of Two Cities (The Classic Collection)", "A Christmas Carol (Classic Fiction)", "A Christmas Carol (Usborne Young Reading)", "A Christmas Carol. (Lernmaterialien)", "A Tale of Two Cities (Classic Retelling)", "A Tale of Two Cities (Everyman's Library Classics)", "A Christmas Carol (Young Reading Series 2)", "David Copperfield.", "Our mutual friend.", "A Christmas Carol (Scholastic Classics)", "A Christmas Carol (Bantam Classic)", "A Christmas Carol (Whole Story)", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Tale of Two Cities (Student's Novels)", "A Christmas Carol (Nelson Graded Readers)", "A Tale of Two Cities (Classic Fiction)", "A Tale of Two Cities (Oxford Bookworms Library)", "A Christmas Carol (Watermill Classic)", "Dombey and son", "A Tale of Two Cities (Penguin Popular Classics)", "Sketches by Boz", "A Tale of Two Cities (Large Print Edition)", "A Tale of Two Cities (Paperback Classics)", "A Tale of Two Cities (Cover to Cover Classics)", "Great Expectations", "The Pickwick papers", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "A Christmas Carol (Classics for Young Adults and Adults)", "A Tale of Two Cities (Prentice Hall Science)", "A Tale of Two Cities (The Greatest Historical Novels)", "Bleak House", "A Tale of Two Cities (Pacemaker Classics)", "A Christmas Carol (Ladybird Children's Classics)", "A Tale of Two Cities (Unabridged Classics)", "A Tale of Two Cities (Bookcassette(r) Edition)", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "A Christmas Carol (Cp 1135)", "A Tale Of Two Cities (Adult Classics in Audio)", "A Tale of Two Cities (Courage Literary Classics)", "A Tale of Two Cities (Konemann Classics)", "The cricket on the hearth", "The old curiosity shop", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "A Tale of Two Cities (Signet Classics)", "Hard times", "Martin Chuzzlewit", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "A Tale of Two Cities (Adopted Classic)", "A Christmas Carol (Classic Books on Cassettes Collection)", "A Christmas Carol (Gollancz Children's Classics)", "A Tale of Two Cities (Naxos AudioBooks)", "A Tale of Two Cities (Saddleback Classics)", "A Tale of Two Cities (10 Cassettes)", "A Christmas Carol (Pacemaker Classic)", "A Christmas Carol (Everyman's Library Children's Classics)", "A Tale of Two Cities (Classics Illustrated Notes)", "A Christmas Carol (Radio Theatre)", "A Christmas Carol (Classic, Picture, Ladybird)", "A Christmas Carol (Puffin Classics)", "A Tale Of Two Cities (Adult Classics)", "A Tale of Two Cities (Barnes & Noble Classics Series)", "A Christmas Carol (Family Classics)", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "A Tale of Two Cities (Cassette (1 Hr).)", "A Christmas Carol (Through the Magic Window Series)", "A Christmas Carol (Reissue)", "A Christmas Carol (Take Part)", "A Tale of Two Cities (Cyber Classics)", "A Christmas Carol (Thornes Classic Novels)", "A Christmas Carol (Soundings)", "A Tale of Two Cities (Piccolo Books)", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "A Christmas Carol (Value Books)", "A Christmas Carol (Classics Illustrated)", "A Tale of Two Cities (Dramascripts S.)", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "A Tale of Two Cities (Everyman Paperbacks)", "A Tale of Two Cities (Classic Literature with Classical Music)", "A Christmas Carol (Ladybird Classics)", "The Old Curiosity Shop", "Great Expectations.", "A Tale of Two Cities (BBC Audio Series)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "A Tale of Two Cities (Enriched Classic)", "A Tale of Two Cities (Oxford Playscripts)", "A Tale of Two Cities (Ultimate Classics)", "A Christmas Carol (Dramascripts Classic Texts)", "A Christmas Carol (Acting Edition)", "A Tale of Two Cities (Longman Classics, Stage 2)", "A TALE OF TWO CITIES", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "Bleak house", "A Christmas Carol (Enriched Classics)", "A Tale of Two Cities (Collected Works of Charles Dickens)", "A Christmas Carol (Large Print)", "A Christmas Carol (Puffin Choice)", "A Christmas Carol (Saddleback Classics)", "A Christmas Carol (Apple Classics)", "A Tale of Two Cities (Classics Illustrated)", "A Christmas Carol (Clear Print)", "A Christmas Carol (Tor Classics)", "A Christmas Carol", "A Tale of Two Cities (Clear Print)", "A Christmas Carol (New Longman Literature)", "A Christmas Carol (Aladdin Classics)", "A Christmas Carol (Penguin Readers, Level 2)", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "The Pickwick Papers", "Our mutual friend", "Bleak House.", "A Tale of Two Cities (Penguin Classics)", "A Christmas Carol (Children's Theatre Playscript)", "Dombey and Son", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Illustrated Junior Library)", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "Dombey and Son.", "A Christmas Carol (Classic Collection)", "A Christmas Carol (Illustrated Classics)", "A Christmas Carol (Limited Editions)", "A Christmas Carol (Dramascripts)", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "A Christmas Carol (Penguin Student Editions)", "A Christmas Carol (Great Stories)", "A Christmas Carol (Green Integer, 50)", "A Tale of Two Cities (Progressive English)", "A Tale of Two Cities (40th Anniversary Edition)", "A Christmas Carol (Watermill Classics)", "A Christmas Carol (Pacemaker Classics)", "Great expectations.", "A Tale of Two Cities (Tor Classics)", "A Christmas Carol (Audio Editions)", "The Mystery of Edwin Drood", "A Tale of Two Cities (Everyman's Library (Paper))", "A Tale of Two Cities (Soundings)", "A Tale of Two Cities (Dramatized)", "Little Dorrit", "A Tale of Two Cities (Illustrated Classics)"], "ans_acc": 0.005917159763313609, "ans_hit": 1, "ans_f1": 0.01160541586073501, "ans_precission": 0.3, "ans_recall": 0.005917159763313609, "path_f1": 0.0923076923076923, "path_precision": 0.3, "path_recall": 0.05454545454545454, "path_ans_f1": 0.01160541586073501, "path_ans_precision": 0.3, "path_ans_recall": 0.005917159763313609}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_commander -> Nikita Khrushchev\n# Answer:\nm.0vmx6nr", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.office_holder -> Nikita Khrushchev\n# Answer:\nm.049x6zw", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_conflict -> Cuban Missile Crisis\n# Answer:\nm.0vmx6nr", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf -> military.military_command.military_commander -> Joseph Stalin\n# Answer:\nm.02h7nmf", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_6 -> government.government_position_held.office_holder -> Vladimir Ivashko\n# Answer:\nm.049x6_6", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.049y3kf -> military.military_command.military_commander -> Ivan Konev\n# Answer:\nm.049y3kf", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.governmental_body -> Central Committee of the Communist Party of the Soviet Union\n# Answer:\nm.049x6zw", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_holder -> Joseph Stalin\n# Answer:\nm.049x6_k", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1977 Moscow bombings\n# Answer:\n1977 Moscow bombings", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf -> military.military_command.military_conflict -> World War II\n# Answer:\nm.02h7nmf"], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp.\n# Answer:\nDDR Corp.", "# Reasoning Path:\nPuerto Rico -> location.statistical_region.gdp_deflator_change -> g.11b6ddwl64\n# Answer:\ng.11b6ddwl64", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Frontpoint Security Solutions\n# Answer:\nFrontpoint Security Solutions", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Hair Club\n# Answer:\nHair Club", "# Reasoning Path:\nPuerto Rico -> location.statistical_region.gdp_deflator_change -> g.124x8g_1y\n# Answer:\ng.124x8g_1y", "# Reasoning Path:\nPuerto Rico -> location.statistical_region.gdp_deflator_change -> g.124x8g_5d\n# Answer:\ng.124x8g_5d"], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> music.artist.track -> Young Blood -> music.recording.releases -> Youngblood\n# Answer:\nYoung Blood", "# Reasoning Path:\nCarl Wilson -> music.artist.track -> A Scottish Soldier -> music.recording.releases -> Scotland the Brave\n# Answer:\nA Scottish Soldier", "# Reasoning Path:\nCarl Wilson -> music.artist.track -> Aul Lang Syne -> common.topic.notable_for -> g.126t2bpvr\n# Answer:\nAul Lang Syne", "# Reasoning Path:\nCarl Wilson -> music.artist.track -> Aul Lang Syne -> common.topic.notable_types -> Musical Recording\n# Answer:\nAul Lang Syne"], "ground_truth": ["Brain tumor", "Lung cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.02h9cb0\n# Answer:\nm.02h9cb0", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4lk\n# Answer:\nm.03lj4lk", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 3\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> common.topic.webpage -> m.09w0_mw\n# Answer:\nm.09w0_mw", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 4\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4 -> tv.tv_series_season.episodes -> Burial Ground\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4 -> common.topic.notable_for -> g.125f8s1f9\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4 -> tv.tv_series_season.episodes -> Deadly Knightshade\n# Answer:\nKnight Rider - Season 4"], "ground_truth": ["William Daniels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County -> location.location.containedby -> Tennessee\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11b66dwnl4\n# Answer:\ng.11b66dwnl4", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County -> location.location.containedby -> Nashville-Davidson--Murfreesboro--Franklin, TN Metropolitan Statistical Area\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County -> location.statistical_region.co2_emissions_total -> m.045hnmm\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11x1ddsd6\n# Answer:\ng.11x1ddsd6", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County -> location.statistical_region.population -> g.11b674pwdr\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11x1fr8dg\n# Answer:\ng.11x1fr8dg", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County -> location.statistical_region.population -> g.11x1cc95y\n# Answer:\nWilliamson County"], "ground_truth": ["Williamson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.profession -> Botanist\n# Answer:\nBotanist", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver Academy\n# Answer:\nCarver Academy", "# Reasoning Path:\nGeorge Washington Carver -> people.person.profession -> Chemist\n# Answer:\nChemist", "# Reasoning Path:\nGeorge Washington Carver -> people.person.profession -> Inventor\n# Answer:\nInventor", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver Center for Arts and Technology -> education.educational_institution.school_type -> High school\n# Answer:\nCarver Center for Arts and Technology", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver Center for Arts and Technology -> location.location.geolocation -> m.01259rzq\n# Answer:\nCarver Center for Arts and Technology", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver Center for Arts and Technology -> education.educational_institution.school_type -> Magnet school\n# Answer:\nCarver Center for Arts and Technology", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver Center for Arts and Technology -> education.educational_institution.school_type -> Public school\n# Answer:\nCarver Center for Arts and Technology"], "ground_truth": ["Diamond"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Esm\u00e9 Annabelle Fox\n# Answer:\nEsm\u00e9 Annabelle Fox", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvttg\n# Answer:\nm.07nvttg", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvtvt\n# Answer:\nm.07nvtvt", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvtwp\n# Answer:\nm.07nvtwp"], "ground_truth": ["Tracy Pollan"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04yvq68 -> military.military_command.military_conflict -> How Few Remain\n# Answer:\nm.04yvq68", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9q6 -> military.military_command.military_conflict -> Battle of McDowell\n# Answer:\nm.04fv9q6", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9nd -> military.military_command.military_conflict -> Battle of Port Republic\n# Answer:\nm.04fv9nd", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign\n# Answer:\nJackson's Valley Campaign", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Foot cavalry\n# Answer:\nFoot cavalry", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Cedar Mountain -> time.event.included_in_event -> Northern Virginia Campaign\n# Answer:\nBattle of Cedar Mountain", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Cedar Mountain -> base.schemastaging.context_name.pronunciation -> g.125_ks95v\n# Answer:\nBattle of Cedar Mountain", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Antietam -> military.military_conflict.military_personnel_involved -> Albion P. Howe\n# Answer:\nBattle of Antietam", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Antietam -> base.culturalevent.event.entity_involved -> Union\n# Answer:\nBattle of Antietam"], "ground_truth": ["First Battle of Rappahannock Station", "Battle of Hancock", "Battle of Port Republic", "Battle of Chantilly", "Battle of McDowell", "Battle of Front Royal", "Battle of Chancellorsville", "Battle of Harpers Ferry", "Romney Expedition", "First Battle of Kernstown", "Battle of Cedar Mountain", "Jackson's Valley Campaign", "Second Battle of Bull Run", "First Battle of Winchester", "American Civil War", "Battle of White Oak Swamp", "Manassas Station Operations", "Battle of Hoke's Run", "How Few Remain"], "ans_acc": 0.3157894736842105, "ans_hit": 1, "ans_f1": 0.22641509433962267, "ans_precission": 0.4, "ans_recall": 0.15789473684210525, "path_f1": 0.2830188679245283, "path_precision": 0.6, "path_recall": 0.18518518518518517, "path_ans_f1": 0.43523316062176165, "path_ans_precision": 0.7, "path_ans_recall": 0.3157894736842105}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.parent -> Ongamo-Maa Group\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ng.1256fv3pv", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> common.topic.article -> m.064klf\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai\n# Answer:\nMaasai", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.document -> Maasai Phonology\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.document -> Maasai Detailed Description\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai-jump\n# Answer:\nMaasai-jump", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.document -> Maasai Genesis Translation\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai women and children -> common.image.size -> m.03qy0z9\n# Answer:\nMaasai women and children"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46\n# Answer:\nm.0j4kb46", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Abiah Folger\n# Answer:\nAbiah Folger", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5\n# Answer:\nm.012zbkk5"], "ground_truth": ["Deborah Read"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9g\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> African American\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9q\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Chronic pancreatitis\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Diabetes mellitus\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.treatments -> Pancreatectomy\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9g\n# Answer:\nPancreatic cancer"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Holy Infants Embracing\n# Answer:\nThe Holy Infants Embracing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Portrait of a Young Fianc\u00e9e\n# Answer:\nPortrait of a Young Fianc\u00e9e", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio\n# Answer:\nAntonio da Correggio", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Bernardino Luini\n# Answer:\nBernardino Luini", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Bob Kane\n# Answer:\nBob Kane"], "ground_truth": ["Portrait of a man in red chalk", "Portrait of Isabella d'Este", "Horse and Rider", "Vitruvian Man", "St. Jerome in the Wilderness", "The Baptism of Christ", "Madonna of the Carnation", "Madonna and Child with St Joseph", "Madonna Litta", "g.1239jd9p", "Portrait of a Young Fianc\u00e9e", "Virgin of the Rocks", "Madonna of Laroque", "Ginevra de' Benci", "The Holy Infants Embracing", "Bacchus", "g.121wt37c", "g.1219sb0g", "The Virgin and Child with St Anne and St John the Baptist", "Annunciation", "g.12215rxg", "Sala delle Asse", "Medusa", "Head of a Woman", "g.120vt1gz", "Portrait of a Musician", "The Battle of Anghiari", "g.12314dm1", "Mona Lisa", "Lucan portrait of Leonardo da Vinci", "The Virgin and Child with St. Anne", "Lady with an Ermine", "Salvator Mundi", "g.1213jb_b", "Madonna of the Yarnwinder", "La belle ferronni\u00e8re", "Leda and the Swan", "Leonardo's horse", "Drapery for a Seated Figure", "St. John the Baptist", "g.1224tf0c", "Adoration of the Magi", "Benois Madonna", "g.121yh91r", "The Last Supper"], "ans_acc": 0.06666666666666667, "ans_hit": 1, "ans_f1": 0.1111111111111111, "ans_precission": 0.3333333333333333, "ans_recall": 0.06666666666666667, "path_f1": 0.11320754716981131, "path_precision": 0.3333333333333333, "path_recall": 0.06818181818181818, "path_ans_f1": 0.1111111111111111, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.06666666666666667}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.capital -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Burgenland\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Carinthia\n# Answer:\nCarinthia", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Burgenland\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Carinthia\n# Answer:\nCarinthia"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04d7nvm\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.person.gender -> Female\n# Answer:\nFemale", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjf_c\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A Time to Mend\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjfp1\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Anthony Casso\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> book.author.works_written -> Essays\n# Answer:\nEssays", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A season in hell\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> common.topic.webpage -> m.04tnn4f\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> Anticancer\n# Answer:\nCancer"], "ground_truth": ["Cervical cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> book.book_subject.works -> Remaking Buddhism for Medieval Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> location.location.time_zones -> Nepal Time Zone\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> location.location.geolocation -> m.0wmmlzp\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> common.topic.article -> m.0hznzjg\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Tooth Relic Temple and Museum -> common.topic.webpage -> m.0gb1hxk\n# Answer:\nBuddha Tooth Relic Temple and Museum", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> common.topic.notable_types -> Tourist attraction\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Tooth Relic Temple and Museum -> common.topic.article -> m.0c3tv_n\n# Answer:\nBuddha Tooth Relic Temple and Museum", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Tooth Relic Temple and Museum -> common.topic.notable_types -> Museum\n# Answer:\nBuddha Tooth Relic Temple and Museum", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism -> religion.religion.practices -> Astrology\n# Answer:\nBuddhism"], "ground_truth": ["Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5\n# Answer:\nm.012zbkk5", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_trwk\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> Glass\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove -> common.topic.article -> m.029mn3\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Lightning rod -> book.book_subject.works -> Stealing God's Thunder: Benjamin Franklin's Lightning Rod and the Invention of America\n# Answer:\nLightning rod", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_ty__\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_wj2d\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.instrument.family -> Crystallophone\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> An Armonica\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> BenFranklinDuplessis\n# Answer:\nBenFranklinDuplessis"], "ground_truth": ["Lightning rod", "Bifocals", "Glass harmonica", "Franklin stove"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.8, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.8, "path_ans_recall": 0.75}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpsq_\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGreeley -> location.location.contains -> Academy of Natural Therapy\n# Answer:\nAcademy of Natural Therapy", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.us_county.hud_county_place -> Aristocrat Ranchettes\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpst3\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.contains -> Aims Community College\n# Answer:\nAims Community College", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpt45\n# Answer:\nWeld County"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> music.group_member.instruments_played -> Piano\n# Answer:\nPiano", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician -> people.profession.specializations -> Composer\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Charode\u01d0ka\n# Answer:\nCharode\u01d0ka", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician -> common.topic.notable_types -> Profession\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Andante cantabile from quartet in D major, op. 11 -> common.topic.notable_for -> g.12599cm43\n# Answer:\nAndante cantabile from quartet in D major, op. 11", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Andante cantabile from quartet in D major, op. 11 -> common.topic.notable_types -> Book\n# Answer:\nAndante cantabile from quartet in D major, op. 11", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Classic Bits & Pieces -> common.topic.notable_types -> Book\n# Answer:\nClassic Bits & Pieces", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Classic Bits & Pieces -> common.topic.notable_for -> g.125d8f763\n# Answer:\nClassic Bits & Pieces"], "ground_truth": ["Musician", "Librettist", "Composer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.41379310344827586, "path_precision": 0.3, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Germany\n# Answer:\nGermany", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0102xvg7\n# Answer:\nm.0102xvg7", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0103v73t\n# Answer:\nm.0103v73t", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0127rg0f\n# Answer:\nm.0127rg0f"], "ground_truth": ["Germany", "Belgium", "Austria", "Liechtenstein", "Switzerland", "Luxembourg", "East Germany"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.42857142857142855, "ans_precission": 0.42857142857142855, "ans_recall": 0.42857142857142855, "path_f1": 0.2857142857142857, "path_precision": 0.2857142857142857, "path_recall": 0.2857142857142857, "path_ans_f1": 0.42857142857142855, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Rock music\n# Answer:\nRock music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop rock\n# Answer:\nPop rock", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> .997 Radiostorm Oldies\n# Answer:\n.997 Radiostorm Oldies", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis\n# Answer:\nApotheosis", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 181-greatoldies\n# Answer:\n181-greatoldies", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Clock\n# Answer:\nClock", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Erection\n# Answer:\nErection", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 1Club.FM: 70s (Lite) -> broadcast.content.genre -> 70's\n# Answer:\n1Club.FM: 70s (Lite)", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 1Club.FM: 70s (Lite) -> broadcast.content.location -> Chicago\n# Answer:\n1Club.FM: 70s (Lite)"], "ground_truth": ["Experimental rock", "Experimental music", "Pop music", "Art rock", "Rock music", "Pop rock", "Blues rock", "Psychedelic rock", "Soft rock"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.3157894736842105, "ans_precission": 0.3, "ans_recall": 0.3333333333333333, "path_f1": 0.3157894736842105, "path_precision": 0.3, "path_recall": 0.3333333333333333, "path_ans_f1": 0.3157894736842105, "path_ans_precision": 0.3, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn\n# Answer:\nm.05kh0nn", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl\n# Answer:\nm.05kfbwl", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr\n# Answer:\nm.02_wstr", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008\n# Answer:\nUnited States Senate election in Colorado, 2008", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2010 -> common.topic.image -> Michael Bennet Official Photo\n# Answer:\nUnited States Senate election in Colorado, 2010", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Cavalry Regiment -> common.topic.notable_types -> Military unit\n# Answer:\n1st Colorado Cavalry Regiment", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Cavalry Regiment -> military.military_unit.unit_size -> Regiment\n# Answer:\n1st Colorado Cavalry Regiment", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Infantry Regiment -> common.topic.notable_types -> Military unit\n# Answer:\n1st Colorado Infantry Regiment", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Cavalry Regiment -> common.topic.article -> m.07fjvw\n# Answer:\n1st Colorado Cavalry Regiment", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Infantry Regiment -> military.military_unit.armed_force -> Union Army\n# Answer:\n1st Colorado Infantry Regiment"], "ground_truth": ["Michael Bennet", "Mark Udall"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 0.1, "path_ans_recall": 0.5}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries\n# Answer:\nNordic countries", "# Reasoning Path:\nGreenland -> location.location.containedby -> North America\n# Answer:\nNorth America", "# Reasoning Path:\nGreenland -> location.location.containedby -> Kingdom of Denmark\n# Answer:\nKingdom of Denmark", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> g.1hhc37psk\n# Answer:\ng.1hhc37psk", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> g.1hhc38qmq\n# Answer:\ng.1hhc38qmq", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> g.1hhc390kr\n# Answer:\ng.1hhc390kr"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.2857142857142857, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.location.contains -> 1111 Third Avenue\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point\n# Answer:\nAlki Point", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.location.contains -> 1201 Third Avenue\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_for -> g.125h4fcxl\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> common.topic.notable_for -> g.125f2tsfn\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> g.11b66b70n7\n# Answer:\ng.11b66b70n7", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.location.contains -> 1918 Eighth Avenue\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_types -> Postal Code\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98103 -> common.topic.notable_types -> Postal Code\n# Answer:\n98103", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> location.location.geometry -> m.055f4wk\n# Answer:\n98102"], "ground_truth": ["98101", "98124", "98134", "98108", "98109", "98191", "98168", "98144", "98145", "98178", "98112", "98170", "98198", "98116", "98129", "98136", "98104", "98111", "98174", "98138", "98188", "98194", "98107", "98121", "98131", "98103", "98126", "98160", "98115", "98139", "98181", "98113", "98155", "98119-4114", "98125", "98102", "98158", "98119", "98154", "98105", "98127", "98190", "98148", "98166", "98161", "98118", "98122", "98133", "98177", "98184", "98165", "98141", "98106", "98175", "98132", "98199", "98171", "98185", "98195", "98114", "98146", "98164", "98117"], "ans_acc": 0.047619047619047616, "ans_hit": 1, "ans_f1": 0.0898876404494382, "ans_precission": 0.8, "ans_recall": 0.047619047619047616, "path_f1": 0.0898876404494382, "path_precision": 0.8, "path_recall": 0.047619047619047616, "path_ans_f1": 0.0898876404494382, "path_ans_precision": 0.8, "path_ans_recall": 0.047619047619047616}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Adrienne Banfield-Jones\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Robsol Pinkett, Jr.\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> common.topic.image -> Pinkett-Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B\n# Answer:\nContemporary R&B", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> tv.tv_actor.guest_roles -> m.09nq_ss\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Dance music\n# Answer:\nDance music", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Dance-pop\n# Answer:\nDance-pop", "# Reasoning Path:\nWillow Smith -> common.topic.webpage -> m.0d_tv_c\n# Answer:\nm.0d_tv_c", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> tv.tv_actor.guest_roles -> m.09nq_sy\n# Answer:\nJada Pinkett Smith"], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> N\u00fcshu script\n# Answer:\nN\u00fcshu script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> language.language_writing_system.languages -> Chinese, Hakka Language\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> education.field_of_study.students_majoring -> m.02t0c71\n# Answer:\nm.02t0c71", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> common.topic.notable_for -> g.1259bftrt\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> education.field_of_study.students_majoring -> m.0104b7h1 -> education.education.degree -> PhD\n# Answer:\nm.0104b7h1", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> language.language_writing_system.languages -> Fuzhou dialect\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> language.language_writing_system.type_of_writing -> Ideogram\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> education.field_of_study.students_majoring -> m.02wn10k\n# Answer:\nm.02wn10k"], "ground_truth": ["Simplified Chinese character", "N\u00fcshu script", "'Phags-pa script", "Chinese characters", "Traditional Chinese characters"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.6, "ans_recall": 0.6, "path_f1": 0.6, "path_precision": 0.6, "path_recall": 0.6, "path_ans_f1": 0.6, "path_ans_precision": 0.6, "path_ans_recall": 0.6}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq\n# Answer:\nm.02h98gq", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Arthur Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Donald Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> freebase.valuenotation.has_value -> Place of death\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Edward Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.person.parents -> Samuel Brady Nixon\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> common.topic.notable_for -> g.125b3gc_h\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15\n# Answer:\nm.02shm15", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.deceased_person.place_of_death -> La Habra\n# Answer:\nFrancis A. Nixon"], "ground_truth": ["Pat Nixon"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nm.07t6_k3", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Supporting Actress in a Comedy Series\n# Answer:\nm.07t6_mc", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.award -> NAACP Image Award for Outstanding Actress in a Comedy Series\n# Answer:\nm.0_yc_3q", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.ceremony -> 37th Primetime Emmy Awards\n# Answer:\nm.07t6_k3", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Supporting Actress in a Comedy Series\n# Answer:\nm.07t6_k3", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nm.07t6_mc", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.ceremony -> 35th Primetime Emmy Awards\n# Answer:\nm.07t6_mc", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.ceremony -> 16th NAACP Image Awards\n# Answer:\nm.0_yc_3q", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 0 -> common.topic.notable_for -> g.1yl5pbtsv\n# Answer:\nThe Jeffersons - Season 0", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 0 -> common.topic.notable_types -> TV Season\n# Answer:\nThe Jeffersons - Season 0"], "ground_truth": ["Paul Benedict", "Mike Evans", "Marla Gibbs", "Damon Evans", "Sherman Hemsley", "Zara Cully", "Roxie Roker", "Isabel Sanford", "Jay Hammer", "Berlinda Tolbert", "Franklin Cover"], "ans_acc": 0.09090909090909091, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.05797101449275363, "path_precision": 0.2, "path_recall": 0.03389830508474576, "path_ans_f1": 0.12500000000000003, "path_ans_precision": 0.2, "path_ans_recall": 0.09090909090909091}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Street Sheet -> book.periodical.format -> m.02npbt7\n# Answer:\nStreet Sheet", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> The Golden Era -> common.topic.article -> m.04zg6d\n# Answer:\nThe Golden Era", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Street Sheet -> book.newspaper.owner -> Coalition on Homelessness\n# Answer:\nStreet Sheet", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.periodical.first_issue_date -> m.02npn31\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906\n# Answer:\n1906", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94101\n# Answer:\n94101", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> The Golden Era -> common.topic.notable_for -> g.1258tnwlj\n# Answer:\nThe Golden Era", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.publication.contents -> m.0znkynv\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.newspaper.issues -> San Francisco Bay Guardian, 24 Nov 1999\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94102\n# Answer:\n94102"], "ground_truth": ["The Daily Alta California", "San Francisco Bay Guardian", "Sing Tao Daily", "Free Society", "San Francisco Daily", "San Francisco Bay Times", "San Francisco Foghorn", "San Francisco Chronicle", "Dock of the Bay", "California Star", "San Francisco Bay View", "The Golden Era", "San Francisco Business Times", "Bay Area Reporter", "Street Sheet", "San Francisco News-Call Bulletin Newspaper", "AsianWeek", "Synapse", "San Francisco Call", "The San Francisco Examiner"], "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0.24705882352941178, "ans_precission": 0.7, "ans_recall": 0.15, "path_f1": 0.24705882352941178, "path_precision": 0.7, "path_recall": 0.15, "path_ans_f1": 0.24705882352941178, "path_ans_precision": 0.7, "path_ans_recall": 0.15}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.containedby -> Eurasia\n# Answer:\nEurasia", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Asia\n# Answer:\nAsia", "# Reasoning Path:\nArmenia -> location.location.containedby -> Roman Catholic Archdiocese of Manizales -> location.location.geolocation -> m.012p0bp1\n# Answer:\nRoman Catholic Archdiocese of Manizales", "# Reasoning Path:\nArmenia -> location.location.containedby -> Roman Catholic Archdiocese of Manizales -> religion.religious_leadership_jurisdiction.size_or_type -> Archdiocese\n# Answer:\nRoman Catholic Archdiocese of Manizales", "# Reasoning Path:\nArmenia -> location.location.containedby -> Roman Catholic Archdiocese of Manizales -> common.topic.article -> m.03qkh47\n# Answer:\nRoman Catholic Archdiocese of Manizales"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.2857142857142857, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 1.0}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> heart attack\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> biology.hybrid_parent_gender.hybrids -> m.0blp580\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> music.artist.track -> Feel the Madness -> common.topic.notable_types -> Canonical Version\n# Answer:\nFeel the Madness", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_98hd\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> music.artist.track -> Be a Man -> music.album.genre -> Hip hop music\n# Answer:\nBe a Man", "# Reasoning Path:\nRandy Savage -> music.artist.track -> Get Back -> music.recording.releases -> Be a Man\n# Answer:\nGet Back"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 11: 1863\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Great Britain\n# Answer:\nGreat Britain", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Charles Waring Darwin\n# Answer:\nCharles Waring Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> award.award_discipline.awards_in_this_discipline -> Copley Medal\n# Answer:\nBiology"], "ground_truth": ["The Essential Darwin", "Charles Darwin's marginalia", "To the members of the Down Friendly Club", "The Power of Movement in Plants", "Charles Darwin on the routes of male humble bees", "Motsa ha-minim", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "South American Geology", "Charles Darwin's letters", "The Descent of Man, and Selection in Relation to Sex", "The voyage of Charles Darwin", "The action of carbonate of ammonia on the roots of certain plants", "Works", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The Correspondence of Charles Darwin, Volume 15: 1867", "The Variation of Animals and Plants under Domestication", "On the tendency of species to form varieties", "The principal works", "Kleinere geologische Abhandlungen", "A Darwin Selection", "Evolution", "The education of Darwin", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The Voyage of the Beagle", "Insectivorous Plants", "The foundations of the Origin of species", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "The Correspondence of Charles Darwin, Volume 8: 1860", "Les moyens d'expression chez les animaux", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Darwin's insects", "The Correspondence of Charles Darwin, Volume 13: 1865", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Les mouvements et les habitudes des plantes grimpantes", "More Letters of Charles Darwin", "The geology of the voyage of H.M.S. Beagle", "The Orgin of Species", "Diary of the voyage of H.M.S. Beagle", "Rejse om jorden", "Part I: Contributions to the Theory of Natural Selection / Part II", "Darwin Compendium", "Darwinism stated by Darwin himself", "Gesammelte kleinere Schriften", "Het uitdrukken van emoties bij mens en dier", "Evolution by natural selection", "Evolution and natural selection", "Memorias y epistolario i\u0301ntimo", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "vari\u00eberen der huisdieren en cultuurplanten", "El Origin De Las Especies", "Darwin Darwin", "genese\u014ds t\u014dn eid\u014dn", "La vie et la correspondance de Charles Darwin", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "Charles Darwin's natural selection", "The Life and Letters of Charles Darwin Volume 1", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "From so simple a beginning", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Cartas de Darwin 18251859", "red notebook of Charles Darwin", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The Correspondence of Charles Darwin, Volume 14: 1866", "On Natural Selection", "The living thoughts of Darwin", "The collected papers of Charles Darwin", "Darwin en Patagonia", "Darwin and Henslow", "The Structure and Distribution of Coral Reefs", "Evolutionary Writings: Including the Autobiographies", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "Origins", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "The Different Forms of Flowers on Plants of the Same Species", "Resa kring jorden", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "The Correspondence of Charles Darwin, Volume 11: 1863", "H.M.S. Beagle in South America", "Reise eines Naturforschers um die Welt", "The Correspondence of Charles Darwin, Volume 10: 1862", "The Life of Erasmus Darwin", "The Correspondence of Charles Darwin, Volume 17: 1869", "Questions about the breeding of animals", "ontstaan der soorten door natuurlijke teeltkeus", "monograph on the sub-class Cirripedia", "Darwin's journal", "The Correspondence of Charles Darwin, Volume 12: 1864", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "La facult\u00e9 motrice dans les plantes", "The\u0301orie de l'e\u0301volution", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "The Autobiography of Charles Darwin", "Fertilisation of Orchids", "On evolution", "Reise um die Welt 1831 - 36", "The Correspondence of Charles Darwin, Volume 18: 1870", "Notebooks on transmutation of species", "Die fundamente zur entstehung der arten", "Notes on the fertilization of orchids", "Monographs of the fossil Lepadidae and the fossil Balanidae", "Proiskhozhdenie vidov", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "Die geschlechtliche Zuchtwahl", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The Life and Letters of Charles Darwin Volume 2", "On a remarkable bar of sandstone off Pernambuco", "Wu zhong qi yuan", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "The portable Darwin", "Les r\u00e9cifs de corail, leur structure et leur distribution", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Darwin's Ornithological notes", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Darwin-Wallace", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Geological Observations on the Volcanic Islands", "Voyage d'un naturaliste autour du monde", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Diario del Viaje de Un Naturalista Alrededor", "The Darwin Reader First Edition", "A student's introduction to Charles Darwin", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "Darwin", "The Correspondence of Charles Darwin, Volume 9: 1861", "Charles Darwin", "Del Plata a Tierra del Fuego", "On the origin of species by means of natural selection", "Darwin for Today", "Leben und Briefe von Charles Darwin", "The Darwin Reader Second Edition", "Metaphysics, Materialism, & the evolution of mind", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Human nature, Darwin's view", "Darwin from Insectivorous Plants to Worms", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "Darwin's notebooks on transmutation of species", "Volcanic Islands", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Opsht\u0323amung fun menshen", "From Darwin's unpublished notebooks", "Beagle letters", "The Formation of Vegetable Mould through the Action of Worms", "The Correspondence of Charles Darwin, Volume 16: 1868", "Geological Observations on South America", "The Expression of the Emotions in Man and Animals", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Darwin on humus and the earthworm", "Tesakneri tsagume\u030c", "On the Movements and Habits of Climbing Plants"], "ans_acc": 0.0392156862745098, "ans_hit": 1, "ans_f1": 0.025586353944562903, "ans_precission": 0.6, "ans_recall": 0.013071895424836602, "path_f1": 0.25806451612903225, "path_precision": 1.0, "path_recall": 0.14814814814814814, "path_ans_f1": 0.07547169811320754, "path_ans_precision": 1.0, "path_ans_recall": 0.0392156862745098}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsgl5\n# Answer:\nm.09nsgl5", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15\n# Answer:\nm.02shm15", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.0_0cs2_\n# Answer:\nm.0_0cs2_", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglb\n# Answer:\nm.09nsglb", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglh\n# Answer:\nm.09nsglh"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> Little Miss Sweetness\n# Answer:\nLittle Miss Sweetness", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.appears_in_topic_gallery -> Zubin Mehta\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Musician\n# Answer:\nMusician", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> After All\n# Answer:\nAfter All", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.size -> m.02cljr8\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Music executive -> common.topic.article -> m.047rgq1\n# Answer:\nMusic executive", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> Come Spy with Me\n# Answer:\nCome Spy with Me", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Music executive -> people.profession.specializations -> Record producer\n# Answer:\nMusic executive"], "ground_truth": ["Daylight & Darkness", "Same Old Love", "There Will Come a Day (I'm Gonna Happen to You)", "Gang Bangin'", "Sleepless Nights", "I Am, I Am", "Love Brought Us Here", "Be Kind To The Growing Mind (with The Temptations)", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "The Hurt's On You", "Will You Love Me Tomorrow", "I Can't Get Enough", "What's Too Much", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "I\u2019ve Got You Under My Skin", "I Second That Emotions", "A Silent Partner in a Three-Way Love Affair", "You Cannot Laugh Alone", "Just Another Kiss", "My Guy", "The Tracks of My Tears (live)", "Medley: Never My Love / Never Can Say Goodbye", "Yes It's You Lady", "Just My Soul Responding", "Coincidentally", "The Tracks of My Heart", "If You Can Want", "We\u2019ve Come Too Far to End It Now", "Yester Love", "Why", "Te Quiero Como Si No Hubiera Un Manana", "No Time to Stop Believing", "You Don't Know What It's Like", "Hold on to Your Love", "That Place", "Mother's Son", "Little Girl Little Girl", "Pops, We Love You", "And I Don't Love You (Larry Levan instrumental dub)", "Baby That's Backatcha", "Blame It On Love (Duet with Barbara Mitchell)", "Crusin'", "Ebony Eyes (Duet with Rick James)", "I Love Your Face", "Please Come Home for Christmas", "I Am I Am", "Will You Love Me Tomorrow?", "Going to a Go-Go", "Why Are You Running From My Love", "Tears of a Clown", "It's Time to Stop Shoppin' Around", "Heavy On Pride (Light On Love)", "The Road to Damascus", "Be Kind to the Growing Mind", "I Hear The Children Singing", "You're the One for Me (feat. Joss Stone)", "The Tears of a Clown", "And I Don't Love You", "Noel", "Tell Me Tomorrow (12\\\" extended mix)", "A Tattoo", "She's Only a Baby Herself", "Love Is The Light", "Girl I'm Standing There", "No\u00ebl", "Walk on By", "Let Me Be the Clock", "Everything You Touch", "Sweet Harmony", "More Love", "Train of Thought", "I'm Glad There Is You", "Bad Girl", "Holly", "You're Just My Life (feat. India.Arie)", "As You Do", "Blame It on Love", "Ooo Baby Baby", "Let Your Light Shine On Me", "Wanna Know My Mind", "Just a Touch Away", "Quiet Storm (Groove Boutique Chill Jazz mix)", "Virgin Man", "And I Love Her", "I Praise & Worship You Father", "Why Do Happy Memories Hurt So Bad", "Happy (Love Theme From Lady Sings the Blues)", "I Can't Find", "Time After Time", "Ain't That Peculiar", "Please Don't Take Your Love (feat. Carlos Santana)", "I Can't Give You Anything but Love", "Nearness of You", "Love Don' Give No Reason (12 Inch Club Mix)", "I've Got You Under My Skin", "Share It", "Come by Here (Kum Ba Ya)", "Just Passing Through", "Ebony Eyes", "Christmas Every Day", "Food For Thought", "You Really Got a Hold on Me", "Cruisin'", "I Have Prayed On It", "I've Made Love to You a Thousand Times", "Asleep on My Love", "Deck the Halls", "We Are The Warriors", "The Agony And The Ecstasy", "Fallin'", "You Take Me Away", "There Will Come A Day ( I'm Gonna Happen To You )", "Theme From the Big Time", "Cruisin", "Satisfy You", "You Are So Beautiful (feat. Dave Koz)", "You Go to My Head", "Jingle Bells", "With Your Love Came", "Take Me Through The Night", "Love Don't Give No Reason", "Who's Sad", "Whatcha Gonna Do", "So Bad", "Double Good Everything", "Tracks Of My Tears (Live)", "The Tears Of A Clown", "Just Like You", "Night and Day", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "Some People Will Do Anything for Love", "It's Her Turn to Live", "Going to a Gogo", "I'm in the Mood for Love", "Save Me", "Wedding Song", "Ooo Baby Baby (live)", "The Christmas Song", "Hanging on by a Thread", "Mickey's Monkey", "One Heartbeat", "Skid Row", "Girlfriend", "Really Gonna Miss You", "If You Wanna Make Love (Come 'round Here)", "Baby Come Close", "Fly Me to the Moon (In Other Words)", "Season's Greetings from Smokey Robinson", "Just To See Her Again", "Aqui Con Tigo (Being With You)", "God Rest Ye Merry Gentlemen", "Close Encounters of the First Kind", "The Family Song", "Vitamin U", "I Second That Emotion", "I Know You by Heart", "Quiet Storm (Groove Boutique remix)", "Going to a Go Go", "Keep Me", "Let Me Be The Clock", "I Can\u2019t Stand to See You Cry (Commercial version)", "The Agony and the Ecstasy", "If You Want My Love", "Open", "Rack Me Back", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "Rewind", "Christmas Greeting", "My World", "Standing On Jesus", "Did You Know (Berry's Theme)", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "Driving Thru Life in the Fast Lane", "He Can Fix Anything", "Being With You", "Quiet Storm", "It's a Good Feeling", "I Want You Back", "Speak Low", "Love So Fine", "Melody Man", "I Love The Nearness Of You", "You Are Forever", "It's A Good Night", "The Tracks Of My Tears", "Will You Still Love Me Tomorrow", "Shop Around", "Tracks of My Tears", "Time Flies", "It's Christmas Time", "Santa Claus is Coming to Town", "I'll Keep My Light In My Window", "You've Really Go a Hold on Me", "Little Girl, Little Girl", "I Like Your Face", "Be Careful What You Wish For", "Everything for Christmas", "Tell Me Tomorrow", "Fulfill Your Need", "Tracks of my Tears", "Winter Wonderland", "Gone Forever", "You've Really Got a Hold on Me", "(It's The) Same Old Love", "Unless You Do It Again", "Love Bath", "Be Who You Are", "Pops, We Love You (disco)", "Because of You It's the Best It's Ever Been", "My Girl", "Love Letters", "One Time", "Never My Love / Never Can Say Goodbye", "The Tracks of My Tears", "Ever Had A Dream", "Tears Of A Clown", "Tea for Two", "Photograph in My Mind", "I Care About Detroit", "Come to Me Soon", "Tell Me Tomorrow, Part 1", "Jesus Told Me To Love You", "Get Ready", "It's Fantastic", "Ooh Baby Baby", "Christmas Everyday", "Shoe Soul", "A Child Is Waiting", "Our Love Is Here to Stay", "We've Saved the Best for Last", "Crusin", "More Than You Know", "Easy", "When A Woman Cries", "Don't Know Why", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "Can't Fight Love", "Tears of a Sweet Free Clown", "You Made Me Feel Love", "Just to See Her", "Jasmin", "When Smokey Sings Tears Of A Clown", "If You Wanna Make Love", "I've Made Love To You A Thousand Times", "Don't Play Another Love Song", "Be Careful What You Wish For (instrumental)", "The Way You Do (The Things You Do)", "The Track of My Tears", "In My Corner", "The Love Between Me and My Kids", "Away in the Manger / Coventry Carol", "Don't Wanna Be Just Physical", "Quiet Storm (single version)", "Love' n Life", "Wishful Thinking", "We've Saved The Best For Last (Kenny G with Smokey Robinson)"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> base.americancomedy.impersonated_celebrity.impersonated_by -> Rich Little\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> White people\n# Answer:\nWhite people", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School\n# Answer:\nBloomington Kennedy High School", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Cape Canaveral\n# Answer:\nCape Canaveral", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American\n# Answer:\nIrish American", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of death\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> 11/22/63\n# Answer:\nAssassination of John F. Kennedy"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Torlakian dialect\n# Answer:\nTorlakian dialect", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> sports.multi_event_tournament.participating_countries -> United States of America\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Hungarian language\n# Answer:\nHungarian language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Montenegro\n# Answer:\nAlbanian language", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> common.topic.article -> m.0l8p56r\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> sports.multi_event_tournament.sports -> Basketball\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nAlbanian language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.main_country -> Albania\n# Answer:\nAlbanian language"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
