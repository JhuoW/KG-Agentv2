{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Costa Rica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.region -> Americas\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Louisiana\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Mississippi\n# Answer:\n1812 Louisiana hurricane"], "ground_truth": ["Jamaican Creole English Language", "Jamaican English"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nUnited States Representative", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.district_represented -> Tennessee's 9th congressional district\n# Answer:\nTennessee's 9th congressional district", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8 -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> United States Representative\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j5sk8 -> government.government_position_held.office_position_or_title -> Governor of Tennessee\n# Answer:\nGovernor of Tennessee", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Representative\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> President of the United States\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Arkansas\n# Answer:\nUnited States of America"], "ground_truth": ["Governor of Tennessee", "Speaker of the United States House of Representatives", "United States Representative"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.30769230769230765, "ans_precission": 0.2, "ans_recall": 0.6666666666666666, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5, "path_ans_precision": 0.4, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.countries_spoken_in -> Dominican Republic\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole -> language.human_language.countries_spoken_in -> Dominican Republic\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.countries_spoken_in -> Cuba\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.main_country -> Puerto Rico\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> Martinique\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.countries_spoken_in -> Turks and Caicos Islands\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.region -> Americas\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole -> language.human_language.countries_spoken_in -> Cuba\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole -> language.human_language.main_country -> Puerto Rico\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French -> language.human_language.countries_spoken_in -> Martinique\n# Answer:\nFrench"], "ground_truth": ["Haitian Creole", "French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1000", "prediction": ["# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.location.contains -> Phoenix Islands\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.location.containedby -> Pacific Ocean\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.location.contains -> Tarawa\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.country.languages_spoken -> English Language\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Australia -> location.country.currency_used -> Australian dollar\n# Answer:\nAustralia", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.location.contains -> Line Islands\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.containedby -> DVD Region 4 -> location.location.contains -> South America\n# Answer:\nDVD Region 4", "# Reasoning Path:\nOceania -> location.location.contains -> Australia -> sports.sport_country.multi_event_tournaments_participated_in -> 2010 Commonwealth Games\n# Answer:\nAustralia", "# Reasoning Path:\nOceania -> location.location.contains -> Australia -> olympics.olympic_participating_country.olympics_participated_in -> 2000 Summer Olympics\n# Answer:\nAustralia", "# Reasoning Path:\nOceania -> location.location.containedby -> Pacific Ocean -> location.location.contains -> Kiribati\n# Answer:\nPacific Ocean"], "ground_truth": ["Kiribati", "Northern Mariana Islands", "Papua New Guinea", "Vanuatu", "Fiji", "Tonga", "Cook Islands", "Australia", "New Caledonia", "New Zealand", "Marshall Islands", "Solomon Islands", "Indonesia", "Guam", "Samoa"], "ans_acc": 0.13333333333333333, "ans_hit": 1, "ans_f1": 0.2285714285714286, "ans_precission": 0.8, "ans_recall": 0.13333333333333333, "path_f1": 0.2285714285714286, "path_precision": 0.8, "path_recall": 0.13333333333333333, "path_ans_f1": 0.232258064516129, "path_ans_precision": 0.9, "path_ans_recall": 0.13333333333333333}
{"id": "WebQTest-1001", "prediction": ["# Reasoning Path:\nNicki Minaj -> people.person.nationality -> United States of America -> location.country.currency_used -> United States Dollar\n# Answer:\nUnited States of America", "# Reasoning Path:\nNicki Minaj -> people.person.nationality -> United States of America -> location.country.languages_spoken -> Spanish Language\n# Answer:\nUnited States of America", "# Reasoning Path:\nNicki Minaj -> people.person.place_of_birth -> Saint James, Trinidad and Tobago -> location.location.people_born_here -> Siobhan Cropper\n# Answer:\nSaint James, Trinidad and Tobago", "# Reasoning Path:\nNicki Minaj -> people.person.place_of_birth -> Saint James, Trinidad and Tobago -> common.topic.image -> Music Truck In Trinidad Carnival\n# Answer:\nSaint James, Trinidad and Tobago", "# Reasoning Path:\nNicki Minaj -> people.person.place_of_birth -> Saint James, Trinidad and Tobago -> common.topic.notable_for -> g.125532q8s\n# Answer:\nSaint James, Trinidad and Tobago", "# Reasoning Path:\nNicki Minaj -> people.person.ethnicity -> African American -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nAfrican American", "# Reasoning Path:\nNicki Minaj -> people.person.place_of_birth -> Saint James, Trinidad and Tobago -> location.location.people_born_here -> Lincoln Phillips\n# Answer:\nSaint James, Trinidad and Tobago", "# Reasoning Path:\nNicki Minaj -> people.person.nationality -> United States of America -> location.country.languages_spoken -> Guyanese Creole English Language\n# Answer:\nUnited States of America", "# Reasoning Path:\nNicki Minaj -> people.person.ethnicity -> African American -> common.topic.notable_types -> Ethnicity\n# Answer:\nAfrican American", "# Reasoning Path:\nNicki Minaj -> people.person.ethnicity -> African American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nAfrican American"], "ground_truth": ["United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1002", "prediction": ["# Reasoning Path:\nProfessor Albus Dumbledore -> film.film_character.portrayed_in_films -> m.03bvlq0 -> film.performance.actor -> Richard Harris\n# Answer:\nRichard Harris", "# Reasoning Path:\nProfessor Albus Dumbledore -> film.film_character.portrayed_in_films -> m.0jz6c8 -> film.performance.actor -> Richard Harris\n# Answer:\nRichard Harris", "# Reasoning Path:\nProfessor Albus Dumbledore -> film.film_character.portrayed_in_films -> m.02tb7yy -> film.performance.actor -> Michael Gambon\n# Answer:\nMichael Gambon", "# Reasoning Path:\nProfessor Albus Dumbledore -> film.film_character.portrayed_in_films -> m.03bvlq0 -> film.performance.film -> Harry Potter and the Philosopher's Stone\n# Answer:\nHarry Potter and the Philosopher's Stone", "# Reasoning Path:\nProfessor Albus Dumbledore -> film.film_character.portrayed_in_films -> m.0jz6c8 -> film.performance.film -> Harry Potter and the Chamber of Secrets\n# Answer:\nHarry Potter and the Chamber of Secrets", "# Reasoning Path:\nProfessor Albus Dumbledore -> cvg.game_character.games -> m.0hhjwzp -> cvg.game_performance.performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nProfessor Albus Dumbledore -> cvg.game_character.games -> m.0hhjwzp -> cvg.game_performance.voice_actor -> Lewis MacLeod\n# Answer:\nLewis MacLeod", "# Reasoning Path:\nProfessor Albus Dumbledore -> common.topic.notable_types -> Film character -> freebase.type_profile.kind -> Annotation\n# Answer:\nFilm character", "# Reasoning Path:\nProfessor Albus Dumbledore -> common.topic.notable_types -> Film character -> type.type.expected_by -> Character\n# Answer:\nFilm character", "# Reasoning Path:\nProfessor Albus Dumbledore -> cvg.game_character.games -> m.0h57m_z -> cvg.game_performance.game -> Harry Potter and the Goblet of Fire\n# Answer:\nHarry Potter and the Goblet of Fire"], "ground_truth": ["Richard Harris", "Toby Regbo", "Dane Farwell", "Michael Gambon"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.37499999999999994, "ans_precission": 0.3, "ans_recall": 0.5, "path_f1": 0.28571428571428564, "path_precision": 0.3, "path_recall": 0.2727272727272727, "path_ans_f1": 0.37499999999999994, "path_ans_precision": 0.3, "path_ans_recall": 0.5}
{"id": "WebQTest-1003", "prediction": ["# Reasoning Path:\nPrime Minister of Pakistan -> government.government_office_or_title.office_holders -> m.088yy7q -> government.government_position_held.office_holder -> Nawaz Sharif\n# Answer:\nNawaz Sharif", "# Reasoning Path:\nPrime Minister of Pakistan -> government.government_office_or_title.office_holders -> m.088yy7z -> government.government_position_held.office_holder -> Nawaz Sharif\n# Answer:\nNawaz Sharif", "# Reasoning Path:\nPrime Minister of Pakistan -> government.government_office_or_title.office_holders -> m.0k_kpqx -> government.government_position_held.office_holder -> Nawaz Sharif\n# Answer:\nNawaz Sharif", "# Reasoning Path:\nPrime Minister of Pakistan -> government.government_office_or_title.office_holders -> m.088yy7q -> government.government_position_held.basic_title -> Prime minister\n# Answer:\nPrime minister", "# Reasoning Path:\nPrime Minister of Pakistan -> common.topic.notable_for -> g.1255j1742\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nPrime Minister of Pakistan -> government.government_office_or_title.office_holders -> m.088yy7q -> government.government_position_held.jurisdiction_of_office -> Pakistan\n# Answer:\nPakistan", "# Reasoning Path:\nPrime Minister of Pakistan -> government.government_office_or_title.office_holders -> m.088yy7z -> government.government_position_held.basic_title -> Prime minister\n# Answer:\nPrime minister", "# Reasoning Path:\nPrime Minister of Pakistan -> common.topic.notable_types -> Government Office or Title -> type.type.domain -> Government\n# Answer:\nGovernment Office or Title", "# Reasoning Path:\nPrime Minister of Pakistan -> government.government_office_or_title.office_holders -> m.0k_kpqx -> government.government_position_held.jurisdiction_of_office -> Pakistan\n# Answer:\nPakistan", "# Reasoning Path:\nPrime Minister of Pakistan -> common.topic.notable_types -> Government Office or Title -> common.topic.article -> m.02xlj1v\n# Answer:\nGovernment Office or Title"], "ground_truth": ["Moeenuddin Ahmad Qureshi", "Nawaz Sharif"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.37499999999999994, "ans_precission": 0.3, "ans_recall": 0.5, "path_f1": 0.4, "path_precision": 0.3, "path_recall": 0.6, "path_ans_f1": 0.37499999999999994, "path_ans_precision": 0.3, "path_ans_recall": 0.5}
{"id": "WebQTest-1004", "prediction": ["# Reasoning Path:\nChristopher Plummer -> award.award_nominee.award_nominations -> m.010nysm3 -> award.award_nomination.award -> Primetime Emmy Award for Best Actor in a Single Performance\n# Answer:\nPrimetime Emmy Award for Best Actor in a Single Performance", "# Reasoning Path:\nChristopher Plummer -> award.award_nominee.award_nominations -> m.07cyt_y -> award.award_nomination.nominated_for -> Our Fathers\n# Answer:\nOur Fathers", "# Reasoning Path:\nChristopher Plummer -> common.resource.annotations -> m.0944d05 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nChristopher Plummer -> award.award_nominee.award_nominations -> m.07shq4d -> award.award_nomination.nominated_for -> The Big Event\n# Answer:\nThe Big Event", "# Reasoning Path:\nChristopher Plummer -> award.award_nominee.award_nominations -> m.010nysm3 -> freebase.valuenotation.has_value -> Nominated work\n# Answer:\nNominated work", "# Reasoning Path:\nChristopher Plummer -> tv.tv_personality.tv_regular_appearances -> m.0103cmz2 -> tv.tv_regular_personal_appearance.appearance_type -> Narrator\n# Answer:\nNarrator", "# Reasoning Path:\nChristopher Plummer -> award.award_nominee.award_nominations -> m.07cyt_y -> award.award_nomination.ceremony -> 12th Screen Actors Guild Awards\n# Answer:\n12th Screen Actors Guild Awards", "# Reasoning Path:\nChristopher Plummer -> award.award_nominee.award_nominations -> m.07cyt_y -> award.award_nomination.award -> Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Miniseries or Television Movie\n# Answer:\nScreen Actors Guild Award for Outstanding Performance by a Male Actor in a Miniseries or Television Movie", "# Reasoning Path:\nChristopher Plummer -> tv.tv_personality.tv_regular_appearances -> m.0103cmz2 -> tv.tv_regular_personal_appearance.program -> Moguls and Movie Stars\n# Answer:\nMoguls and Movie Stars", "# Reasoning Path:\nChristopher Plummer -> tv.tv_personality.tv_regular_appearances -> m.06jv4_f -> tv.tv_regular_personal_appearance.program -> The Final Chapter?\n# Answer:\nThe Final Chapter?"], "ground_truth": ["The Gnomes' Great Adventure", "12 Monkeys", "Vampire in Venice", "The Boy in Blue", "Barrymore", "Money", "The New World", "Inside Man", "Our Fathers", "Harrison Bergeron", "Firehead", "Impolite", "The Legend of Sarila", "Cold Creek Manor", "Possessed", "Wolf", "Star Trek VI: The Undiscovered Country", "Riel", "Somewhere in Time", "Eyewitness", "The Conspiracy of Fear", "Heidi", "Malcolm X", "Full Disclosure", "The Man Who Planted Trees", "Starcrash", "Gandahar", "Alexander", "Nicholas Nickleby", "Where the Heart Is", "Little Gloria... Happy at Last", "Hector and the Search for Happiness", "The Night of the Generals", "The Pyx", "When the Circus Came to Town", "A Hazard of Hearts", "The Tempest", "Imagine", "Silver Blaze", "The Last Station", "Closing the Ring", "Triple Cross", "The Return of the Pink Panther", "The Man Who Would Be King", "Rumpelstiltskin", "An American Tail", "Dreamscape", "The Scarlet and the Black", "The Fall of the Roman Empire", "A Marriage: Georgia O'Keeffe and Alfred Stieglitz", "The Royal Hunt of the Sun", "Battle of Britain", "Blizzard", "The Imaginarium of Doctor Parnassus", "Liar's Edge", "On Golden Pond", "Inside Daisy Clover", "The Amateur", "The Lake House", "Aces High", "Dragnet", "A Beautiful Mind", "The Assignment", "Waterloo", "The Clown at Midnight", "Priest", "Ararat", "Night Flight", "Man in the Chair", "The First Circle", "Five Good Years", "The Spiral Staircase", "Woman Wanted", "Four Minutes", "The Gospel of John", "Nobody Runs Forever", "Rock-a-Doodle", "Oedipus the King", "My Dog Tulip", "National Treasure", "The Day That Shook the World", "Hamlet at Elsinore", "Conduct Unbecoming", "Elsa & Fred", "Hidden Agenda", "Murder by Decree", "The Boss' Wife", "Emotional Arithmetic", "Babes in Toyland", "The Sound of Music", "Up", "The Silent Partner", "American Tragedy", "Lucky Break", "Souvenir", "Dark Descent of the Forgotten Empress", "Lock Up Your Daughters", "Remember", "Beginners", "I Love N.Y.", "Shadow Dancing", "Must Love Dogs", "Dolores Claiborne", "Skeletons", "Caesar and Cleopatra", "Already Dead", "Dracula 2000", "Lily in Love", "Wind Across the Everglades", "Kali the Little Vampire", "The Girl with the Dragon Tattoo", "Winchell", "International Velvet", "9", "Ordeal by Innocence", "Blackheart", "A Doll's House", "Madeline: Lost in Paris", "The Insider", "Syriana", "The Forger", "Muhammad Ali's Greatest Fight", "Highpoint", "Secrets", "Hanover Street", "Prototype", "Stage Struck", "Young Catherine", "The Dinosaur Hunter"], "ans_acc": 0.015503875968992248, "ans_hit": 1, "ans_f1": 0.014388489208633093, "ans_precission": 0.1, "ans_recall": 0.007751937984496124, "path_f1": 0.03636363636363636, "path_precision": 0.1, "path_recall": 0.022222222222222223, "path_ans_f1": 0.028776978417266185, "path_ans_precision": 0.2, "path_ans_recall": 0.015503875968992248}
{"id": "WebQTest-1005", "prediction": ["# Reasoning Path:\nMorocco -> location.country.capital -> Rabat -> location.location.containedby -> Rabat-Sal\u00e9-Zemmour-Zaer\n# Answer:\nRabat", "# Reasoning Path:\nMorocco -> location.country.capital -> Rabat -> travel.travel_destination.tourist_attractions -> Mausoleum of Mohammed V\n# Answer:\nRabat", "# Reasoning Path:\nMorocco -> location.country.capital -> Rabat -> common.topic.notable_types -> City/Town/Village\n# Answer:\nRabat", "# Reasoning Path:\nMorocco -> location.country.administrative_divisions -> Western Sahara -> location.country.capital -> Laayoune\n# Answer:\nWestern Sahara", "# Reasoning Path:\nMorocco -> location.country.administrative_divisions -> Western Sahara -> location.location.containedby -> Greater Maghreb\n# Answer:\nWestern Sahara", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Taza-Al Hoceima-Taounate -> common.topic.article -> m.0dgr09\n# Answer:\nTaza-Al Hoceima-Taounate", "# Reasoning Path:\nMorocco -> location.country.administrative_divisions -> Western Sahara -> location.location.contains -> Laayoune\n# Answer:\nWestern Sahara", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Taza-Al Hoceima-Taounate -> location.location.time_zones -> Western European Time Zone\n# Answer:\nTaza-Al Hoceima-Taounate", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Chaouia-Ouardigha -> location.location.time_zones -> Western European Time Zone\n# Answer:\nChaouia-Ouardigha", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Doukkala-Abda -> location.location.people_born_here -> Bouchaib Rmail\n# Answer:\nDoukkala-Abda"], "ground_truth": ["Rabat"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1006", "prediction": ["# Reasoning Path:\nOmar Epps -> film.actor.film -> m.0k66h_ -> film.performance.film -> The Program\n# Answer:\nThe Program", "# Reasoning Path:\nOmar Epps -> film.actor.film -> m.0k0669 -> film.performance.film -> Love & Basketball\n# Answer:\nLove & Basketball", "# Reasoning Path:\nOmar Epps -> film.actor.film -> m.0k66h_ -> film.performance.character -> Darnell Jefferson\n# Answer:\nDarnell Jefferson", "# Reasoning Path:\nOmar Epps -> film.actor.film -> m.02vbbx0 -> film.performance.film -> The Mod Squad\n# Answer:\nThe Mod Squad", "# Reasoning Path:\nOmar Epps -> award.award_nominee.award_nominations -> m.0b3v06c -> award.award_nomination.nominated_for -> Love & Basketball\n# Answer:\nLove & Basketball", "# Reasoning Path:\nOmar Epps -> award.award_nominee.award_nominations -> m.0z9x7cw -> award.award_nomination.nominated_for -> Love & Basketball\n# Answer:\nLove & Basketball", "# Reasoning Path:\nOmar Epps -> film.actor.film -> m.0k0669 -> film.performance.character -> Quincy McCall\n# Answer:\nQuincy McCall", "# Reasoning Path:\nOmar Epps -> award.award_nominee.award_nominations -> m.0b3v06c -> award.award_nomination.ceremony -> 2001 MTV Movie Awards\n# Answer:\n2001 MTV Movie Awards", "# Reasoning Path:\nOmar Epps -> award.award_nominee.award_nominations -> m.0_r73q0 -> award.award_nomination.nominated_for -> House\n# Answer:\nHouse", "# Reasoning Path:\nOmar Epps -> award.award_nominee.award_nominations -> m.0z9x7cw -> award.award_nomination.award -> Teen Choice Award for Film - Choice Actor\n# Answer:\nTeen Choice Award for Film - Choice Actor"], "ground_truth": ["Love & Basketball", "In Too Deep", "Higher Learning", "MTV 20: Jams", "The Program", "Alfie", "Dracula 2000", "Major League II", "Brother", "Don't Be a Menace to South Central While Drinking Your Juice in the Hood", "Scream 2", "Big Trouble", "Conviction", "The Wood", "Breakfast of Champions", "Perfume", "Deadly Voyage", "Against the Ropes", "Juice", "A Day in the Life", "The Mod Squad", "First Time Felon", "Daybreak"], "ans_acc": 0.13043478260869565, "ans_hit": 1, "ans_f1": 0.20689655172413793, "ans_precission": 0.5, "ans_recall": 0.13043478260869565, "path_f1": 0.25, "path_precision": 0.5, "path_recall": 0.16666666666666666, "path_ans_f1": 0.20689655172413793, "path_ans_precision": 0.5, "path_ans_recall": 0.13043478260869565}
{"id": "WebQTest-1007", "prediction": ["# Reasoning Path:\nNancy Pelosi -> people.person.place_of_birth -> Baltimore -> location.location.containedby -> Maryland\n# Answer:\nBaltimore", "# Reasoning Path:\nNancy Pelosi -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Maryland\n# Answer:\nUnited States of America", "# Reasoning Path:\nNancy Pelosi -> people.person.place_of_birth -> Baltimore -> location.location.containedby -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nNancy Pelosi -> people.person.place_of_birth -> Baltimore -> location.location.containedby -> Area codes 410, 443, and 667\n# Answer:\nBaltimore", "# Reasoning Path:\nNancy Pelosi -> government.politician.government_positions_held -> m.0239jrx -> government.government_position_held.district_represented -> California\u2019s 5th congressional district\n# Answer:\nCalifornia\u2019s 5th congressional district", "# Reasoning Path:\nNancy Pelosi -> government.politician.government_positions_held -> m.03nbxkd -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nNancy Pelosi -> government.politician.government_positions_held -> m.0239jrx -> government.government_position_held.legislative_sessions -> 100th United States Congress\n# Answer:\n100th United States Congress", "# Reasoning Path:\nNancy Pelosi -> government.politician.government_positions_held -> m.03hcgxq -> government.government_position_held.legislative_sessions -> 110th United States Congress\n# Answer:\n110th United States Congress", "# Reasoning Path:\nNancy Pelosi -> government.politician.government_positions_held -> m.03hcgxq -> government.government_position_held.governmental_body -> United States House of Representatives\n# Answer:\nUnited States House of Representatives", "# Reasoning Path:\nNancy Pelosi -> government.politician.government_positions_held -> m.0239jrx -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nBasic title"], "ground_truth": ["California\u2019s 12th congressional district", "California\u2019s 8th congressional district", "California\u2019s 5th congressional district"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.15384615384615383, "ans_precission": 0.1, "ans_recall": 0.3333333333333333, "path_f1": 0.13333333333333333, "path_precision": 0.1, "path_recall": 0.2, "path_ans_f1": 0.15384615384615383, "path_ans_precision": 0.1, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1008", "prediction": ["# Reasoning Path:\nWalton -> location.location.containedby -> Indiana -> location.location.containedby -> United States, with Territories\n# Answer:\nIndiana", "# Reasoning Path:\nWalton -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nWalton -> location.location.containedby -> Indiana -> location.location.containedby -> United States of America\n# Answer:\nIndiana", "# Reasoning Path:\nWalton -> location.location.containedby -> Indiana -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nIndiana", "# Reasoning Path:\nWalton -> location.location.containedby -> Cass County -> location.location.containedby -> Indiana\n# Answer:\nCass County", "# Reasoning Path:\nWalton -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Indiana\n# Answer:\nUnited States of America", "# Reasoning Path:\nWalton -> location.location.containedby -> Indiana -> location.location.containedby -> Contiguous United States\n# Answer:\nIndiana", "# Reasoning Path:\nWalton -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Indiana\n# Answer:\nUnited States of America", "# Reasoning Path:\nWalton -> location.statistical_region.population -> g.11b66g7n0c\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nWalton -> location.location.containedby -> Indiana -> fictional_universe.fictional_setting.contains -> Eagleton\n# Answer:\nIndiana"], "ground_truth": ["Tipton Township", "Indiana", "United States of America", "Cass County"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.6153846153846154, "path_precision": 0.8, "path_recall": 0.5, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-1009", "prediction": ["# Reasoning Path:\nGeorge H. W. Bush -> people.person.education -> m.0sw3d4k -> education.education.institution -> Davenport College\n# Answer:\nDavenport College", "# Reasoning Path:\nGeorge H. W. Bush -> people.person.education -> m.03k_sf2 -> education.education.institution -> University Yale\n# Answer:\nUniversity Yale", "# Reasoning Path:\nGeorge H. W. Bush -> people.person.education -> m.04hdfwr -> education.education.institution -> Phillips Academy\n# Answer:\nPhillips Academy", "# Reasoning Path:\nGeorge H. W. Bush -> people.person.profession -> Military aviator -> people.profession.people_with_this_profession -> Bruce Evans\n# Answer:\nMilitary aviator", "# Reasoning Path:\nGeorge H. W. Bush -> people.person.profession -> Military aviator -> people.profession.people_with_this_profession -> Chuck Yeager\n# Answer:\nMilitary aviator", "# Reasoning Path:\nGeorge H. W. Bush -> people.person.profession -> Military aviator -> people.profession.specializations -> United States Naval Aviator\n# Answer:\nMilitary aviator", "# Reasoning Path:\nGeorge H. W. Bush -> people.person.profession -> Military aviator -> people.profession.people_with_this_profession -> Dami\u00e1n Peterson\n# Answer:\nMilitary aviator", "# Reasoning Path:\nGeorge H. W. Bush -> people.person.profession -> Military aviator -> fictional_universe.character_occupation.characters_with_this_occupation -> Chuck Yeager\n# Answer:\nMilitary aviator", "# Reasoning Path:\nGeorge H. W. Bush -> people.person.profession -> Entrepreneur -> common.topic.subject_of -> Krishna Mali\n# Answer:\nEntrepreneur", "# Reasoning Path:\nGeorge H. W. Bush -> government.political_appointer.appointees -> m.048zk8w -> government.government_position_held.office_holder -> James Baker\n# Answer:\nJames Baker"], "ground_truth": ["University Yale"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1010", "prediction": ["# Reasoning Path:\nCaitlyn Jenner -> olympics.olympic_athlete.medals_won -> m.07ygz5n -> olympics.olympic_medal_honor.event -> Athletics at the 1976 Summer Olympics - Men's Decathlon\n# Answer:\nAthletics at the 1976 Summer Olympics - Men's Decathlon", "# Reasoning Path:\nCaitlyn Jenner -> olympics.olympic_athlete.medals_won -> m.07ygz5n -> olympics.olympic_medal_honor.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nCaitlyn Jenner -> film.actor.film -> m.01z02gc -> film.performance.film -> Can't Stop the Music\n# Answer:\nCan't Stop the Music", "# Reasoning Path:\nCaitlyn Jenner -> film.actor.film -> m.0_vcp39 -> film.performance.film -> The Hungover Games\n# Answer:\nThe Hungover Games", "# Reasoning Path:\nCaitlyn Jenner -> people.person.children -> Kylie Jenner -> common.topic.webpage -> m.0fq945m\n# Answer:\nKylie Jenner", "# Reasoning Path:\nCaitlyn Jenner -> film.actor.film -> m.0cgnzpc -> film.performance.film -> The Big Tease\n# Answer:\nThe Big Tease", "# Reasoning Path:\nCaitlyn Jenner -> film.actor.film -> m.0_vcp39 -> film.performance.character -> Skip Bayflick\n# Answer:\nSkip Bayflick", "# Reasoning Path:\nCaitlyn Jenner -> people.person.children -> Kylie Jenner -> award.award_winner.awards_won -> m.0wj9z_x\n# Answer:\nKylie Jenner", "# Reasoning Path:\nCaitlyn Jenner -> people.person.children -> Brandon Jenner -> people.person.sibling_s -> m.010_srbf\n# Answer:\nBrandon Jenner", "# Reasoning Path:\nCaitlyn Jenner -> people.person.children -> Brody Jenner -> tv.tv_actor.guest_roles -> m.09nm66k\n# Answer:\nBrody Jenner"], "ground_truth": ["Athletics at the 1976 Summer Olympics - Men's Decathlon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1011", "prediction": ["# Reasoning Path:\nStandard Tibetan -> language.human_language.countries_spoken_in -> China -> location.country.administrative_divisions -> Tibet\n# Answer:\nChina", "# Reasoning Path:\nStandard Tibetan -> language.human_language.countries_spoken_in -> China -> location.country.languages_spoken -> Zhuang languages\n# Answer:\nChina", "# Reasoning Path:\nStandard Tibetan -> base.schemastaging.context_name.pronunciation -> m.013160gt\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nStandard Tibetan -> language.human_language.countries_spoken_in -> China -> location.country.administrative_divisions -> Zhejiang\n# Answer:\nChina", "# Reasoning Path:\nStandard Tibetan -> language.human_language.main_country -> China -> location.country.administrative_divisions -> Tibet\n# Answer:\nChina", "# Reasoning Path:\nStandard Tibetan -> language.human_language.countries_spoken_in -> China -> location.country.administrative_divisions -> Anhui\n# Answer:\nChina", "# Reasoning Path:\nStandard Tibetan -> language.human_language.countries_spoken_in -> China -> location.country.languages_spoken -> Chinese language\n# Answer:\nChina", "# Reasoning Path:\nStandard Tibetan -> language.human_language.countries_spoken_in -> China -> biology.breed_origin.breeds_originating_here -> Tibetan Terrier\n# Answer:\nChina", "# Reasoning Path:\nStandard Tibetan -> language.human_language.countries_spoken_in -> China -> location.country.languages_spoken -> Esperanto Language\n# Answer:\nChina", "# Reasoning Path:\nStandard Tibetan -> language.human_language.countries_spoken_in -> China -> biology.breed_origin.breeds_originating_here -> Yili horse\n# Answer:\nChina"], "ground_truth": ["Tibet", "China"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6428571428571429, "ans_precission": 0.9, "ans_recall": 0.5, "path_f1": 0.6153846153846154, "path_precision": 0.8, "path_recall": 0.5, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1012", "prediction": ["# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Universal Studios Hollywood -> location.location.containedby -> Universal City\n# Answer:\nUniversal Studios Hollywood", "# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Universal Studios Hollywood -> amusement_parks.park.rides -> Curious George Goes to Town\n# Answer:\nUniversal Studios Hollywood", "# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Universal Studios Hollywood -> location.location.events -> 16th People's Choice Awards\n# Answer:\nUniversal Studios Hollywood", "# Reasoning Path:\nHollywood -> book.book_subject.works -> American Lightning: Terror, Mystery, the Birth of Hollywood, and the Crime of the Century -> book.written_work.subjects -> James McNamara\n# Answer:\nAmerican Lightning: Terror, Mystery, the Birth of Hollywood, and the Crime of the Century", "# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Universal Studios Hollywood -> location.location.containedby -> California\n# Answer:\nUniversal Studios Hollywood", "# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Universal Studios Hollywood -> location.location.events -> 17th People's Choice Awards\n# Answer:\nUniversal Studios Hollywood", "# Reasoning Path:\nHollywood -> book.book_subject.works -> American Lightning: Terror, Mystery, the Birth of Hollywood, and the Crime of the Century -> book.written_work.subjects -> John McNamara\n# Answer:\nAmerican Lightning: Terror, Mystery, the Birth of Hollywood, and the Crime of the Century", "# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Universal Studios Hollywood -> location.location.containedby -> Los Angeles County\n# Answer:\nUniversal Studios Hollywood", "# Reasoning Path:\nHollywood -> location.location.contains -> Hollywood High School -> location.location.containedby -> United States of America\n# Answer:\nHollywood High School", "# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Universal Studios Hollywood -> location.location.events -> 18th People's Choice Awards\n# Answer:\nUniversal Studios Hollywood"], "ground_truth": ["Dolby Theatre", "Universal Studios Hollywood", "Grauman's Egyptian Theatre", "Hollywood Walk of Fame", "Hollywood Wax Museum", "Griffith Observatory", "Hollywood Heritage Museum", "Hollywood Sign", "TCL Chinese Theatre", "Universal CityWalk"], "ans_acc": 0.1, "ans_hit": 1, "ans_f1": 0.175, "ans_precission": 0.7, "ans_recall": 0.1, "path_f1": 0.175, "path_precision": 0.7, "path_recall": 0.1, "path_ans_f1": 0.175, "path_ans_precision": 0.7, "path_ans_recall": 0.1}
{"id": "WebQTest-1014", "prediction": ["# Reasoning Path:\nKim Kardashian -> people.person.place_of_birth -> Los Angeles -> base.popstra.location.arrestee -> m.064jg0c\n# Answer:\nLos Angeles", "# Reasoning Path:\nKim Kardashian -> people.person.parents -> Robert Kardashian -> people.person.place_of_birth -> Los Angeles\n# Answer:\nRobert Kardashian", "# Reasoning Path:\nKim Kardashian -> people.person.place_of_birth -> Los Angeles -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nLos Angeles", "# Reasoning Path:\nKim Kardashian -> people.person.parents -> Robert Kardashian -> people.person.parents -> Helen Kardashian\n# Answer:\nRobert Kardashian", "# Reasoning Path:\nKim Kardashian -> people.person.parents -> Kris Jenner -> people.person.place_of_birth -> San Diego\n# Answer:\nKris Jenner", "# Reasoning Path:\nKim Kardashian -> people.person.parents -> Robert Kardashian -> people.person.gender -> Male\n# Answer:\nRobert Kardashian", "# Reasoning Path:\nKim Kardashian -> people.person.parents -> Robert Kardashian -> people.person.parents -> Arthur Kardashian\n# Answer:\nRobert Kardashian", "# Reasoning Path:\nKim Kardashian -> base.popstra.celebrity.dated -> m.064hndx -> base.popstra.dated.participant -> Reggie Bush\n# Answer:\nReggie Bush", "# Reasoning Path:\nKim Kardashian -> people.person.parents -> Kris Jenner -> people.person.parents -> Robert Houghton\n# Answer:\nKris Jenner", "# Reasoning Path:\nKim Kardashian -> base.popstra.celebrity.dated -> m.065pvw3 -> base.popstra.dated.participant -> Ben Roethlisberger\n# Answer:\nBen Roethlisberger"], "ground_truth": ["Los Angeles"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1015", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Ontario\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> base.wikipedia_infobox.settlement.area_code -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Canada\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Canada\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> common.topic.notable_types -> City/Town/Village\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Ontario\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.people_born_here -> Pattie Mallette\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nJuno Awards of 2014"], "ground_truth": ["Canada", "Stratford"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.4, "ans_recall": 0.5, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1017", "prediction": ["# Reasoning Path:\nThe Santa Clause -> film.film.starring -> m.0k6ckh -> film.performance.actor -> Eric Lloyd\n# Answer:\nEric Lloyd", "# Reasoning Path:\nThe Santa Clause -> film.film.starring -> m.0k6ckh -> film.performance.character -> Charlie Calvin\n# Answer:\nCharlie Calvin", "# Reasoning Path:\nThe Santa Clause -> film.film.starring -> m.02tbbmx -> film.performance.actor -> Peter Boyle\n# Answer:\nPeter Boyle", "# Reasoning Path:\nThe Santa Clause -> film.film.starring -> m.0dltq_0 -> film.performance.actor -> Jesse Collins\n# Answer:\nJesse Collins", "# Reasoning Path:\nThe Santa Clause -> media_common.netflix_title.netflix_genres -> Ages 11-12\n# Answer:\nAges 11-12", "# Reasoning Path:\nThe Santa Clause -> common.topic.webpage -> m.09syg4y -> common.webpage.resource -> Review of Santa Clause, The by Hollis Chacona at Austin Chronicle\n# Answer:\nReview of Santa Clause, The by Hollis Chacona at Austin Chronicle", "# Reasoning Path:\nThe Santa Clause -> media_common.netflix_title.netflix_genres -> Comedies\n# Answer:\nComedies", "# Reasoning Path:\nThe Santa Clause -> film.film.starring -> m.02tbbmx -> film.performance.character -> Mr. Whittle\n# Answer:\nMr. Whittle", "# Reasoning Path:\nThe Santa Clause -> film.film.starring -> m.0dltq_0 -> film.performance.character -> Ad Executive\n# Answer:\nAd Executive", "# Reasoning Path:\nThe Santa Clause -> common.topic.webpage -> m.09syg4y -> common.webpage.category -> Review\n# Answer:\nReview"], "ground_truth": ["Eric Lloyd"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t8hv2 -> tv.regular_tv_appearance.actor -> Yvonne Craig\n# Answer:\nYvonne Craig", "# Reasoning Path:\nBarbara Gordon -> common.topic.notable_for -> g.12596ymdk\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.03ljbs2 -> tv.regular_tv_appearance.actor -> Angie Harmon\n# Answer:\nAngie Harmon", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t91b7 -> tv.regular_tv_appearance.actor -> Melissa Gilbert\n# Answer:\nMelissa Gilbert", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t91b7 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Politician -> common.topic.subjects -> Akinwunmi Ambode\n# Answer:\nPolitician", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> film.film_character.portrayed_in_films -> m.012hbb3h\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Politician -> common.topic.subjects -> Nathalie Kosciusko-Morizet\n# Answer:\nPolitician", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdn_\n# Answer:\nPolitician", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer -> base.descriptive_names.names.descriptive_name -> m.0101pjk9\n# Answer:\nLawyer"], "ground_truth": ["Hannah Gunn", "Melinda McGraw", "Ilyssa Fradin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1020", "prediction": ["# Reasoning Path:\nR. Kelly -> people.person.education -> m.0h3355s -> education.education.institution -> Kenwood Academy\n# Answer:\nKenwood Academy", "# Reasoning Path:\nR. Kelly -> music.artist.track_contributions -> g.11bypnyn2g\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nR. Kelly -> music.artist.track_contributions -> m.010_sxwg -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nR. Kelly -> music.artist.track_contributions -> m.01100cnr -> music.track_contribution.track -> Hotel\n# Answer:\nHotel", "# Reasoning Path:\nR. Kelly -> music.producer.tracks_produced -> Age Ain't Nothing But a Number -> music.single.versions -> Age Ain't Nothing but a Number\n# Answer:\nAge Ain't Nothing But a Number", "# Reasoning Path:\nR. Kelly -> music.producer.tracks_produced -> Age Ain't Nothing But a Number -> music.recording.releases -> Age Ain't Nothing but a Number\n# Answer:\nAge Ain't Nothing But a Number", "# Reasoning Path:\nR. Kelly -> music.artist.track_contributions -> m.010_sxwg -> music.track_contribution.track -> I'm Your Angel\n# Answer:\nI'm Your Angel", "# Reasoning Path:\nR. Kelly -> music.producer.tracks_produced -> Age Ain't Nothing But a Number -> music.recording.contributions -> m.0rqjwzt\n# Answer:\nAge Ain't Nothing But a Number", "# Reasoning Path:\nR. Kelly -> music.producer.tracks_produced -> At Your Best -> music.recording.releases -> Age Ain't Nothing but a Number\n# Answer:\nAt Your Best", "# Reasoning Path:\nR. Kelly -> music.artist.track_contributions -> m.01100cnr -> music.track_contribution.role -> Vocals\n# Answer:\nVocals"], "ground_truth": ["Kenwood Academy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1021", "prediction": ["# Reasoning Path:\nBenghazi -> location.location.events -> 2013 Benghazi conflict -> common.topic.article -> m.0w1f1w0\n# Answer:\n2013 Benghazi conflict", "# Reasoning Path:\nBenghazi -> location.location.events -> 2012 U.S. diplomatic missions attacks -> time.event.locations -> Tunis\n# Answer:\n2012 U.S. diplomatic missions attacks", "# Reasoning Path:\nBenghazi -> location.location.events -> 2013 Benghazi conflict -> time.event.included_in_event -> Factional violence in Libya\n# Answer:\n2013 Benghazi conflict", "# Reasoning Path:\nBenghazi -> location.location.events -> Second Battle of Benghazi -> base.culturalevent.event.entity_involved -> Anti-Gaddafi forces\n# Answer:\nSecond Battle of Benghazi", "# Reasoning Path:\nBenghazi -> location.location.events -> 2012 U.S. diplomatic missions attacks -> common.topic.article -> m.0m2t_23\n# Answer:\n2012 U.S. diplomatic missions attacks", "# Reasoning Path:\nBenghazi -> location.location.events -> 2012 U.S. diplomatic missions attacks -> time.event.included_in_event -> 2012 Benghazi attack\n# Answer:\n2012 U.S. diplomatic missions attacks", "# Reasoning Path:\nBenghazi -> location.location.events -> Second Battle of Benghazi -> common.topic.notable_types -> Military Conflict\n# Answer:\nSecond Battle of Benghazi", "# Reasoning Path:\nBenghazi -> location.location.events -> 2012 U.S. diplomatic missions attacks -> time.event.locations -> Antwerp\n# Answer:\n2012 U.S. diplomatic missions attacks", "# Reasoning Path:\nBenghazi -> location.location.events -> Second Battle of Benghazi -> common.topic.image -> Palmaria bengasi 1903 0612 b1\n# Answer:\nSecond Battle of Benghazi", "# Reasoning Path:\nBenghazi -> location.location.events -> 2012 U.S. diplomatic missions attacks -> time.event.locations -> Cairo\n# Answer:\n2012 U.S. diplomatic missions attacks"], "ground_truth": ["Second Battle of Benghazi", "First Battle of Benghazi", "2012 Benghazi attack", "2013 Benghazi conflict", "2012 U.S. diplomatic missions attacks", "Battle of Benina Airport", "Operation Dignity Battle"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 1.0, "ans_recall": 0.42857142857142855, "path_f1": 0.6, "path_precision": 1.0, "path_recall": 0.42857142857142855, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 1.0, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-1022", "prediction": ["# Reasoning Path:\nNike, Inc. -> organization.organization.headquarters -> m.09rzrss -> location.mailing_address.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nNike, Inc. -> organization.organization.headquarters -> m.09rzrss -> location.mailing_address.citytown -> Beaverton\n# Answer:\nBeaverton", "# Reasoning Path:\nNike, Inc. -> organization.organization.phone_number -> m.0my29ql -> common.phone_number.service_location -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nNike, Inc. -> organization.organization.headquarters -> m.09rzrss -> location.mailing_address.postal_code -> 97005\n# Answer:\n97005", "# Reasoning Path:\nNike, Inc. -> organization.organization.phone_number -> m.0my29ql -> common.phone_number.category -> Customer Service\n# Answer:\nCustomer Service", "# Reasoning Path:\nNike, Inc. -> business.sponsor.sponsorship -> m.0113y1fk -> business.sponsorship.sponsored_recipient -> Manchester United F.C.\n# Answer:\nManchester United F.C.", "# Reasoning Path:\nNike, Inc. -> business.sponsor.sponsorship -> m.03n9_nh -> business.sponsorship.sponsored_recipient -> Michael Jordan\n# Answer:\nMichael Jordan", "# Reasoning Path:\nNike, Inc. -> business.sponsor.sponsorship -> m.03n9_n7 -> business.sponsorship.sponsored_recipient -> LeBron James\n# Answer:\nLeBron James"], "ground_truth": ["United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.25, "path_recall": 0.25, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1024", "prediction": ["# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_0s31\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.11b60v4rn6\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_38p7\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_569j\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.12tb6hbx9\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.1hhc378k5\n# Answer:\nlocation.statistical_region.cpi_inflation_rate"], "ground_truth": ["Mexican peso"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1025", "prediction": ["# Reasoning Path:\nSteve Spurrier -> sports.pro_athlete.teams -> m.0hpcpht -> sports.sports_team_roster.team -> San Francisco 49ers\n# Answer:\nSan Francisco 49ers", "# Reasoning Path:\nSteve Spurrier -> sports.pro_athlete.teams -> m.0hpcphg -> sports.sports_team_roster.team -> Tampa Bay Buccaneers\n# Answer:\nTampa Bay Buccaneers", "# Reasoning Path:\nSteve Spurrier -> sports.pro_athlete.teams -> m.0hpcpht -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nSteve Spurrier -> sports.pro_athlete.teams -> m.0hpcn0z -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nSteve Spurrier -> people.person.education -> m.03p7x9l -> education.education.institution -> University of Florida\n# Answer:\nUniversity of Florida", "# Reasoning Path:\nSteve Spurrier -> sports.pro_athlete.teams -> m.0hpcphg -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nSteve Spurrier -> people.person.profession -> Athlete -> common.topic.notable_types -> Profession\n# Answer:\nAthlete", "# Reasoning Path:\nSteve Spurrier -> people.person.education -> m.0n1lxwh -> education.education.institution -> Science Hill High School\n# Answer:\nScience Hill High School", "# Reasoning Path:\nSteve Spurrier -> sports.pro_athlete.teams -> m.0hpcphg -> sports.sports_team_roster.position -> Punter\n# Answer:\nPunter", "# Reasoning Path:\nSteve Spurrier -> sports.pro_athlete.teams -> m.0hpcn0z -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback"], "ground_truth": ["San Francisco 49ers", "Tampa Bay Buccaneers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1027", "prediction": ["# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> Chicago Stadium -> location.location.containedby -> 60612\n# Answer:\nChicago Stadium", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> Chicago Stadium -> location.location.containedby -> Cook County\n# Answer:\nChicago Stadium", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> Chicago Stadium -> base.politicalconventions.convention_venue.presidential_nomination_conventions_hosted -> 1932 Democratic National Convention\n# Answer:\nChicago Stadium", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> United Center -> location.location.containedby -> Chicago\n# Answer:\nUnited Center", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> Chicago Stadium -> sports.sports_facility.teams -> Chicago Sting\n# Answer:\nChicago Stadium", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> United Center -> sports.sports_facility.teams -> Chicago Bulls\n# Answer:\nUnited Center", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.venue -> m.0wz229c -> sports.team_venue_relationship.venue -> Chicago Stadium\n# Answer:\nChicago Stadium", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> Chicago Stadium -> base.politicalconventions.convention_venue.presidential_nomination_conventions_hosted -> 1932 Republican National Convention\n# Answer:\nChicago Stadium", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> Chicago Stadium -> base.politicalconventions.convention_venue.presidential_nomination_conventions_hosted -> 1940 Democratic National Convention\n# Answer:\nChicago Stadium", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> Chicago Stadium -> sports.sports_facility.teams -> Chicago Cougars\n# Answer:\nChicago Stadium"], "ground_truth": ["United Center"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1028", "prediction": ["# Reasoning Path:\nJohn Connor -> film.film_character.portrayed_in_films -> m.04m5b3q -> film.performance.actor -> Christian Bale\n# Answer:\nChristian Bale", "# Reasoning Path:\nJohn Connor -> film.film_character.portrayed_in_films -> m.02vbxnf -> film.performance.actor -> Edward Furlong\n# Answer:\nEdward Furlong", "# Reasoning Path:\nJohn Connor -> film.film_character.portrayed_in_films -> m.04m5b3q -> film.performance.film -> Terminator Salvation\n# Answer:\nTerminator Salvation", "# Reasoning Path:\nJohn Connor -> film.film_character.portrayed_in_films -> m.03n9_pb -> film.performance.actor -> Nick Stahl\n# Answer:\nNick Stahl", "# Reasoning Path:\nJohn Connor -> film.film_character.portrayed_in_films -> m.02vbxnf -> film.performance.film -> T2 3-D:Battle Across Time\n# Answer:\nT2 3-D:Battle Across Time", "# Reasoning Path:\nJohn Connor -> film.film_character.portrayed_in_films -> m.03n9_pb -> film.performance.film -> Terminator 3: Rise of the Machines\n# Answer:\nTerminator 3: Rise of the Machines", "# Reasoning Path:\nJohn Connor -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Terminator -> people.person.gender -> Male\n# Answer:\nTerminator", "# Reasoning Path:\nJohn Connor -> common.topic.webpage -> m.09yfxs8 -> common.webpage.resource -> Clip du Jour: Bunny Terminator!\n# Answer:\nClip du Jour: Bunny Terminator!", "# Reasoning Path:\nJohn Connor -> common.topic.webpage -> m.09w5r38 -> common.webpage.resource -> Christian Bale to star in 'Terminator'\n# Answer:\nChristian Bale to star in 'Terminator'", "# Reasoning Path:\nJohn Connor -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Terminator -> fictional_universe.fictional_universe.works_set_here -> Skynet\n# Answer:\nTerminator"], "ground_truth": ["Michael Edwards", "Edward Furlong", "Dalton Abbott", "Christian Bale", "Nick Stahl"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.48, "ans_precission": 0.4, "ans_recall": 0.6, "path_f1": 0.3529411764705882, "path_precision": 0.3, "path_recall": 0.42857142857142855, "path_ans_f1": 0.48, "path_ans_precision": 0.4, "path_ans_recall": 0.6}
{"id": "WebQTest-1029", "prediction": ["# Reasoning Path:\nMexico -> location.country.languages_spoken -> Maya, Yucat\u00e1n Language -> language.human_language.main_country -> Guatemala\n# Answer:\nMaya, Yucat\u00e1n Language", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Maya, Yucat\u00e1n Language -> common.topic.notable_types -> Human Language\n# Answer:\nMaya, Yucat\u00e1n Language", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Maya, Yucat\u00e1n Language -> language.human_language.language_family -> Mayan languages\n# Answer:\nMaya, Yucat\u00e1n Language", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Awakateko Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nAwakateko Language", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_0s31\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.11b60v4rn6\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Spanish Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Awakateko Language -> common.topic.notable_types -> Human Language\n# Answer:\nAwakateko Language"], "ground_truth": ["Mazahua language", "Kickapoo Language", "Yaqui Language", "Mazatecan languages", "Tzotzil language", "Pame language", "Mayo Language", "Huichol Language", "Pur\u00e9pecha language", "Tataltepec Chatino Language", "Tzeltal language", "Huamelula Language", "Tepehua languages", "Maya, Yucat\u00e1n Language", "Tepehuan, Southeastern Language", "Guerrero Amuzgo language", "Totonac, Highland Language", "Lacandon Language", "Tarahumara language", "Tlapanec, Azoy\u00fa Language", "O'odham language", "Cocopah Language", "Huave language", "Mocho Language", "Tojolabal Language", "Chochotec Language", "Chinantecan languages", "Ixcatec Language", "Ixil, San Juan Cotzal Language", "Awakateko Language", "Chichimeca Jonaz language", "Paipai Language", "Italian Language", "Seri Language", "Kumiai Language", "Chuj language", "Otomi language", "Q'eqchi' Language", "Kiliwa Language", "Texistepec language", "Cuicatec, Teutila Language", "Trique language", "Mixe languages", "Cochimi Language", "Q\u2019anjob\u2019al language", "Matlatzinca language", "Huastec, Veracruz Language", "Cora, El Nayar Language", "Mixtecan languages", "Nahuatl languages", "Spanish Language", "Ch\u2019ol language", "Pima Bajo Language", "Chontal, Tabasco Language", "Chicomuceltec Language", "Jakaltek language", "Huarijio Language", "K'iche' language"], "ans_acc": 0.05172413793103448, "ans_hit": 1, "ans_f1": 0.09716599190283401, "ans_precission": 0.8, "ans_recall": 0.05172413793103448, "path_f1": 0.09716599190283401, "path_precision": 0.8, "path_recall": 0.05172413793103448, "path_ans_f1": 0.09716599190283401, "path_ans_precision": 0.8, "path_ans_recall": 0.05172413793103448}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qstvs -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qpksh -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qstvs -> basketball.basketball_player_stats.season -> 2002\u201303 NBA season\n# Answer:\n2002\u201303 NBA season", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.season -> 1992\u201393 NBA season\n# Answer:\n1992\u201393 NBA season", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc58 -> sports.sports_award.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player -> common.topic.notable_for -> g.125ctmqvt\n# Answer:\nPlayer", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qpksh -> basketball.basketball_player_stats.season -> 1994\u201395 NBA season\n# Answer:\n1994\u201395 NBA season", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc5c -> sports.sports_award.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers"], "ground_truth": ["Los Angeles Lakers", "LSU Tigers men's basketball", "Boston Celtics", "Phoenix Suns", "Orlando Magic", "Miami Heat", "Cleveland Cavaliers"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.3870967741935483, "ans_precission": 0.6, "ans_recall": 0.2857142857142857, "path_f1": 0.3, "path_precision": 0.6, "path_recall": 0.2, "path_ans_f1": 0.3870967741935483, "path_ans_precision": 0.6, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-1030", "prediction": ["# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Europeans -> people.ethnicity.included_in_group -> White people\n# Answer:\nEuropeans", "# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Europeans -> people.ethnicity.includes_groups -> Irish people\n# Answer:\nEuropeans", "# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Europeans -> people.ethnicity.languages_spoken -> Italian Language\n# Answer:\nEuropeans", "# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Europeans -> people.ethnicity.includes_groups -> Czech American\n# Answer:\nEuropeans", "# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Latin European peoples -> common.topic.image -> LatinEuropeCountries\n# Answer:\nLatin European peoples", "# Reasoning Path:\nItalian people -> common.topic.image -> Italian -> base.rosetta.local_name.languoid -> Italian Language\n# Answer:\nItalian", "# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Europeans -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nEuropeans", "# Reasoning Path:\nItalian people -> people.ethnicity.languages_spoken -> Italian Language -> language.human_language.region -> Europe\n# Answer:\nItalian Language", "# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Europeans -> people.ethnicity.includes_groups -> Danes\n# Answer:\nEuropeans", "# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Latin European peoples -> people.ethnicity.includes_groups -> French people\n# Answer:\nLatin European peoples"], "ground_truth": ["Latin European peoples", "Europeans"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1031", "prediction": ["# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Voice Actor -> people.profession.specialization_of -> Actor\n# Answer:\nVoice Actor", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Voice Actor -> common.topic.notable_types -> Profession\n# Answer:\nVoice Actor", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Film director -> freebase.type_profile.published -> Published\n# Answer:\nFilm director", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Film director -> common.topic.subject_of -> Mervin Praison\n# Answer:\nFilm director", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Film director -> freebase.type_hints.included_types -> Person\n# Answer:\nFilm director", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Actor -> people.profession.specializations -> Voice Actor\n# Answer:\nActor", "# Reasoning Path:\nAngelina Jolie -> book.author.book_editions_published -> Notes from My Travels -> common.topic.notable_types -> Book\n# Answer:\nNotes from My Travels", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Film director -> common.topic.subject_of -> Eugenio Polgovsky\n# Answer:\nFilm director", "# Reasoning Path:\nAngelina Jolie -> award.award_nominee.award_nominations -> m.010wr37v -> award.award_nomination.nominated_for -> Maleficent\n# Answer:\nMaleficent", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Film director -> common.topic.subject_of -> Lew M. Parry\n# Answer:\nFilm director"], "ground_truth": ["Voice Actor", "Model", "Writer", "Film director", "Author", "Film Producer", "Screenwriter", "Actor"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5106382978723405, "ans_precission": 0.8, "ans_recall": 0.375, "path_f1": 0.5106382978723405, "path_precision": 0.8, "path_recall": 0.375, "path_ans_f1": 0.6428571428571429, "path_ans_precision": 0.9, "path_ans_recall": 0.5}
{"id": "WebQTest-1032", "prediction": ["# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0hhzbby -> film.performance.actor -> Chris Pine\n# Answer:\nChris Pine", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8g09 -> film.performance.actor -> William Shatner\n# Answer:\nWilliam Shatner", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8ffj -> film.performance.actor -> William Shatner\n# Answer:\nWilliam Shatner", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0hhzbby -> film.performance.film -> Star Trek Into Darkness\n# Answer:\nStar Trek Into Darkness", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8g09 -> film.performance.film -> Star Trek VI: The Undiscovered Country\n# Answer:\nStar Trek VI: The Undiscovered Country", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8ffj -> film.performance.film -> Star Trek: The Motion Picture\n# Answer:\nStar Trek: The Motion Picture", "# Reasoning Path:\nJames T. Kirk -> common.topic.notable_types -> Film character -> freebase.type_profile.kind -> Annotation\n# Answer:\nFilm character", "# Reasoning Path:\nJames T. Kirk -> common.topic.notable_types -> Film character -> type.type.expected_by -> Character\n# Answer:\nFilm character", "# Reasoning Path:\nJames T. Kirk -> common.topic.webpage -> m.09wcz0z -> common.webpage.resource -> Wolverines! Chris Hemsworth nabs lead in 'Red Dawn'\n# Answer:\nWolverines! Chris Hemsworth nabs lead in 'Red Dawn'", "# Reasoning Path:\nJames T. Kirk -> common.topic.webpage -> m.09wcz0z -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["William Shatner"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.1904761904761905, "path_precision": 0.2, "path_recall": 0.18181818181818182, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1033", "prediction": ["# Reasoning Path:\nDana Scully -> film.film_character.portrayed_in_films -> m.04f8_rv -> film.performance.actor -> Gillian Anderson\n# Answer:\nGillian Anderson", "# Reasoning Path:\nDana Scully -> film.film_character.portrayed_in_films -> m.0k6zn6 -> film.performance.actor -> Gillian Anderson\n# Answer:\nGillian Anderson", "# Reasoning Path:\nDana Scully -> film.film_character.portrayed_in_films -> m.04f8_rv -> film.performance.film -> The X-Files: I Want to Believe\n# Answer:\nThe X-Files: I Want to Believe", "# Reasoning Path:\nDana Scully -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The X-Files Universe -> common.topic.notable_types -> Fictional Universe\n# Answer:\nThe X-Files Universe", "# Reasoning Path:\nDana Scully -> film.film_character.portrayed_in_films -> m.0k6zn6 -> film.performance.film -> The X-Files\n# Answer:\nThe X-Files", "# Reasoning Path:\nDana Scully -> tv.tv_character.appeared_in_tv_episodes -> m.06zyt5d -> tv.tv_guest_role.actor -> Gillian Anderson\n# Answer:\nGillian Anderson", "# Reasoning Path:\nDana Scully -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The X-Files Universe -> fictional_universe.fictional_universe.species -> Manitou\n# Answer:\nThe X-Files Universe", "# Reasoning Path:\nDana Scully -> tv.tv_character.appeared_in_tv_episodes -> m.06zyt5d -> tv.tv_guest_role.episodes_appeared_in -> Ghost in the Machine\n# Answer:\nGhost in the Machine", "# Reasoning Path:\nDana Scully -> tv.tv_character.appeared_in_tv_episodes -> m.06zyy_x -> tv.tv_guest_role.episodes_appeared_in -> War of the Coprophages\n# Answer:\nWar of the Coprophages", "# Reasoning Path:\nDana Scully -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The X-Files Universe -> fictional_universe.fictional_universe.works_set_here -> The Lone Gunmen\n# Answer:\nThe X-Files Universe"], "ground_truth": ["Gillian Anderson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.10714285714285714, "path_precision": 0.3, "path_recall": 0.06521739130434782, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1035", "prediction": ["# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Novelist -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nNovelist", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Novelist -> people.profession.specialization_of -> Writer\n# Answer:\nNovelist", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Novelist -> common.topic.notable_types -> Profession\n# Answer:\nNovelist", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Poet -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nPoet", "# Reasoning Path:\nF. Scott Fitzgerald -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Male\n# Answer:\nheart attack", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Writer -> people.profession.specializations -> Screenwriter\n# Answer:\nWriter", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Poet -> common.topic.notable_types -> Profession\n# Answer:\nPoet", "# Reasoning Path:\nF. Scott Fitzgerald -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.parent_disease -> Cardiovascular disease\n# Answer:\nheart attack", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Poet -> people.profession.specialization_of -> Writer\n# Answer:\nPoet", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Writer -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nWriter"], "ground_truth": ["Novelist", "Poet", "Writer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1036", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Costa Rica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.region -> Americas\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Louisiana\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Mississippi\n# Answer:\n1812 Louisiana hurricane"], "ground_truth": ["Jamaican English"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1037", "prediction": ["# Reasoning Path:\nNew York Knicks -> sports.sports_team.coaches -> m.0qt2p64 -> sports.sports_team_coach_tenure.coach -> Mike Woodson\n# Answer:\nMike Woodson", "# Reasoning Path:\nNew York Knicks -> basketball.basketball_team.head_coach -> Derek Fisher -> basketball.basketball_player.player_statistics -> m.04qkqr7\n# Answer:\nDerek Fisher", "# Reasoning Path:\nNew York Knicks -> basketball.basketball_team.head_coach -> Derek Fisher -> basketball.basketball_player.player_statistics -> m.04q9tdk\n# Answer:\nDerek Fisher", "# Reasoning Path:\nNew York Knicks -> basketball.basketball_team.head_coach -> Derek Fisher -> tv.tv_actor.guest_roles -> m.09nch_2\n# Answer:\nDerek Fisher", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.coaches -> m.0qt2p64 -> sports.sports_team_coach_tenure.position -> Head coach\n# Answer:\nHead coach", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.coaches -> m.0112zxyh -> sports.sports_team_coach_tenure.coach -> Kurt Rambis\n# Answer:\nKurt Rambis", "# Reasoning Path:\nNew York Knicks -> basketball.basketball_team.head_coach -> Derek Fisher -> basketball.basketball_player.player_statistics -> m.04qc3sv\n# Answer:\nDerek Fisher", "# Reasoning Path:\nNew York Knicks -> basketball.basketball_team.head_coach -> Derek Fisher -> tv.tv_actor.guest_roles -> m.09nch_7\n# Answer:\nDerek Fisher", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.coaches -> m.0112zxyh -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.coaches -> m.010r7q18 -> sports.sports_team_coach_tenure.coach -> Derek Fisher\n# Answer:\nDerek Fisher"], "ground_truth": ["Derek Fisher"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1038", "prediction": ["# Reasoning Path:\nWesley Snipes -> people.person.profession -> Martial Artist -> people.profession.specialization_of -> Athlete\n# Answer:\nMartial Artist", "# Reasoning Path:\nWesley Snipes -> people.person.profession -> Dancer -> people.profession.specialization_of -> Artist\n# Answer:\nDancer", "# Reasoning Path:\nWesley Snipes -> common.topic.notable_for -> g.1254x6509\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nWesley Snipes -> people.person.profession -> Martial Artist -> base.descriptive_names.names.descriptive_name -> m.0105yc8_\n# Answer:\nMartial Artist", "# Reasoning Path:\nWesley Snipes -> people.person.profession -> Martial Artist -> common.topic.notable_for -> g.12565kr7j\n# Answer:\nMartial Artist", "# Reasoning Path:\nWesley Snipes -> people.person.profession -> Martial Artist -> base.descriptive_names.names.descriptive_name -> m.0108mspk\n# Answer:\nMartial Artist", "# Reasoning Path:\nWesley Snipes -> people.person.profession -> Dancer -> common.topic.notable_for -> g.125h67_9m\n# Answer:\nDancer", "# Reasoning Path:\nWesley Snipes -> people.person.profession -> Dancer -> base.descriptive_names.names.descriptive_name -> m.0102lxg3\n# Answer:\nDancer", "# Reasoning Path:\nWesley Snipes -> tv.tv_actor.guest_roles -> m.0bvvfwz -> tv.tv_guest_role.episodes_appeared_in -> March 29, 2002\n# Answer:\nMarch 29, 2002", "# Reasoning Path:\nWesley Snipes -> people.person.profession -> Martial Artist -> base.descriptive_names.names.descriptive_name -> m.0108mtdk\n# Answer:\nMartial Artist"], "ground_truth": ["Dancer", "Businessperson", "Singer", "Film Producer", "Martial Artist", "Actor"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.47058823529411764, "ans_precission": 0.8, "ans_recall": 0.3333333333333333, "path_f1": 0.47058823529411764, "path_precision": 0.8, "path_recall": 0.3333333333333333, "path_ans_f1": 0.6428571428571429, "path_ans_precision": 0.9, "path_ans_recall": 0.5}
{"id": "WebQTest-1039", "prediction": ["# Reasoning Path:\nSherrod Brown -> people.person.places_lived -> m.0wk6dss -> people.place_lived.location -> Avon\n# Answer:\nAvon", "# Reasoning Path:\nSherrod Brown -> people.person.place_of_birth -> Mansfield -> location.statistical_region.population -> g.11b66g3p9g\n# Answer:\nMansfield", "# Reasoning Path:\nSherrod Brown -> people.person.place_of_birth -> Mansfield -> location.hud_foreclosure_area.total_residential_addresses -> m.07dcp5y\n# Answer:\nMansfield", "# Reasoning Path:\nSherrod Brown -> people.person.place_of_birth -> Mansfield -> location.statistical_region.population -> g.11bc8563lx\n# Answer:\nMansfield", "# Reasoning Path:\nSherrod Brown -> people.person.place_of_birth -> Mansfield -> location.location.people_born_here -> Amy Douglass\n# Answer:\nMansfield", "# Reasoning Path:\nSherrod Brown -> people.person.places_lived -> m.04hcytr -> people.place_lived.location -> Mansfield\n# Answer:\nMansfield", "# Reasoning Path:\nSherrod Brown -> people.person.place_of_birth -> Mansfield -> location.statistical_region.population -> g.11x1clr1_\n# Answer:\nMansfield", "# Reasoning Path:\nSherrod Brown -> people.person.place_of_birth -> Mansfield -> location.location.people_born_here -> Antonio Graves\n# Answer:\nMansfield", "# Reasoning Path:\nSherrod Brown -> people.person.place_of_birth -> Mansfield -> location.location.people_born_here -> Bentley B. Gilbert\n# Answer:\nMansfield", "# Reasoning Path:\nSherrod Brown -> people.person.spouse_s -> m.0j4ky8c -> freebase.valuenotation.is_reviewed -> Spouse\n# Answer:\nSpouse"], "ground_truth": ["Avon", "Mansfield"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> travel.travel_destination.tourist_attractions -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> location.location.containedby -> Westchester County\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> location.location.contains -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> location.location.containedby -> New York\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9 -> award.award_honor.award -> Emmy Award for Outstanding Variety, Music or Comedy Series\n# Answer:\nEmmy Award for Outstanding Variety, Music or Comedy Series", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.04lybjz\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> people.ethnicity.languages_spoken -> American English\n# Answer:\nScottish American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9 -> award.award_honor.honored_for -> The Tonight Show with Jay Leno\n# Answer:\nThe Tonight Show with Jay Leno", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.0n2qyfk -> award.award_honor.award -> People's Choice Award for Favorite Late Night Talk Show Host\n# Answer:\nPeople's Choice Award for Favorite Late Night Talk Show Host"], "ground_truth": ["New Rochelle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1040", "prediction": ["# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Achawa language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nAchawa language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Belize\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Achawa language -> language.human_language.region -> Americas\n# Answer:\nAchawa language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Chile\n# Answer:\nSpanish Language"], "ground_truth": ["Guahibo language", "Cumeral Language", "Siona Language", "Cubeo Language", "Cuiba language", "Desano Language", "Andoque Language", "Nukak language", "Ember\u00e1, Northern Language", "Cof\u00e1n Language", "Uwa language", "Awa-Cuaiquer Language", "Minica Huitoto", "Cams\u00e1 Language", "Totoro Language", "Tunebo, Central Language", "Malayo Language", "Nonuya language", "Piapoco Language", "Cabiyar\u00ed Language", "Romani, Vlax Language", "Quechua, Napo Lowland Language", "Carijona Language", "Bora Language", "Chipiajes Language", "Piratapuyo Language", "Yucuna Language", "Guanano Language", "Siriano Language", "Baudo language", "Anserma Language", "Wayuu Language", "Ocaina Language", "Ponares Language", "Macagu\u00e1n Language", "Barasana Language", "Arhuaco Language", "Yukpa Language", "S\u00e1liba Language", "Inga, Jungle Language", "Achawa language", "Koreguaje Language", "Spanish Language", "Tinigua language", "Puinave Language", "Pijao Language", "Runa Language", "Guambiano Language", "Tama Language", "Macaguaje Language", "Piaroa Language", "Murui Huitoto language", "Cagua Language", "Curripaco Language", "Palenquero Language", "Kogi Language", "Ticuna language", "Tunebo, Western Language", "Macuna Language", "Andaqui Language", "Tuyuca language", "Muinane Language", "Carabayo Language", "Providencia Sign Language", "Nheengatu language", "Natagaimas Language", "Playero language", "Coxima Language", "P\u00e1ez language", "Tucano Language", "Coyaima Language", "Colombian Sign Language", "Cocama language", "Hupd\u00eb Language", "Inga Language", "Islander Creole English", "Kuna, Border Language", "Waimaj\u00e3 Language", "Tomedes Language", "Bar\u00ed Language", "Tanimuca-Retuar\u00e3 Language", "Tunebo, Barro Negro Language", "Guayabero Language", "Omejes Language", "Catio language", "Tunebo, Angosturas Language"], "ans_acc": 0.023255813953488372, "ans_hit": 1, "ans_f1": 0.04545454545454545, "ans_precission": 1.0, "ans_recall": 0.023255813953488372, "path_f1": 0.04552845528455284, "path_precision": 0.7, "path_recall": 0.023529411764705882, "path_ans_f1": 0.04545454545454545, "path_ans_precision": 1.0, "path_ans_recall": 0.023255813953488372}
{"id": "WebQTest-1041", "prediction": ["# Reasoning Path:\nUnited Kingdom -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> England\n# Answer:\nPound sterling", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> location.country.currency_used -> Pound sterling\n# Answer:\nPound sterling", "# Reasoning Path:\nUnited Kingdom -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Northern Ireland\n# Answer:\nPound sterling", "# Reasoning Path:\nEngland -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> United Kingdom\n# Answer:\nPound sterling", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0njwhw5\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nEngland -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Northern Ireland\n# Answer:\nPound sterling", "# Reasoning Path:\nUnited Kingdom -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Kingdom of England\n# Answer:\nPound sterling", "# Reasoning Path:\nEngland -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Kingdom of England\n# Answer:\nPound sterling", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> base.aareas.schema.administrative_area.administrative_children -> Cambridgeshire\n# Answer:\nCambridgeshire", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.currency_used -> Pound sterling\n# Answer:\nNorthern Ireland"], "ground_truth": ["Pound sterling"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1042", "prediction": ["# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.0k6pxpv -> tv.regular_tv_appearance.actor -> Lacey Chabert\n# Answer:\nLacey Chabert", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.04d4q86 -> tv.regular_tv_appearance.actor -> Lacey Chabert\n# Answer:\nLacey Chabert", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.0k6pxpv -> tv.regular_tv_appearance.series -> Family Guy\n# Answer:\nFamily Guy", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntq_4 -> tv.regular_tv_appearance.actor -> Mila Kunis\n# Answer:\nMila Kunis", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.04d4q86 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nMeg Griffin -> cvg.game_character.games -> m.0ypsv3n -> cvg.game_performance.voice_actor -> Mila Kunis\n# Answer:\nMila Kunis", "# Reasoning Path:\nMeg Griffin -> cvg.game_character.games -> m.0ypsv3n -> cvg.game_performance.performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nMeg Griffin -> film.film_character.portrayed_in_films -> m.0k3ssw -> film.performance.actor -> Mila Kunis\n# Answer:\nMila Kunis", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntq_4 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nMeg Griffin -> film.film_character.portrayed_in_films -> m.010x342y -> film.performance.actor -> Mila Kunis\n# Answer:\nMila Kunis"], "ground_truth": ["Lacey Chabert"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1043", "prediction": ["# Reasoning Path:\nMali -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> United Arab Republic\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nMali -> location.country.form_of_government -> Semi-presidential system -> common.topic.notable_types -> Form of Government\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nMali -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nMali -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d0tc16\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nMali -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Algeria\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nMali -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Arab Republic\n# Answer:\nUnitary state", "# Reasoning Path:\nMali -> location.country.form_of_government -> Unitary state -> common.topic.notable_types -> Form of Government\n# Answer:\nUnitary state", "# Reasoning Path:\nMali -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6flnr\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nMali -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Burkina Faso\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nMali -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6ntbzfs\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita"], "ground_truth": ["Unitary state", "Republic", "Semi-presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6829268292682926, "ans_precission": 0.7, "ans_recall": 0.6666666666666666, "path_f1": 0.6829268292682926, "path_precision": 0.7, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1044", "prediction": ["# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w9gdcb -> soccer.football_player_stats.team -> Sporting-soc Desportiva De\n# Answer:\nSporting-soc Desportiva De", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.match -> 2014 Real Madrid CF vs. CA Osasuna football match\n# Answer:\n2014 Real Madrid CF vs. CA Osasuna football match", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzrlj -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w8w78v -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w8w78v -> soccer.football_player_stats.team -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w8w7f0 -> soccer.football_player_stats.team -> Portugal national football team\n# Answer:\nPortugal national football team", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010p4jhc -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzrlj -> soccer.football_goal.match -> 2014 Real Madrid CF vs. CA Osasuna football match\n# Answer:\n2014 Real Madrid CF vs. CA Osasuna football match", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010p4jhc -> soccer.football_goal.match -> 2014 UEFA Champions League Final\n# Answer:\n2014 UEFA Champions League Final"], "ground_truth": ["Real Madrid C.F.", "Portugal national football team"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.3333333333333333, "path_precision": 0.5, "path_recall": 0.25, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1045", "prediction": ["# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26508 -> location.location.geometry -> m.058x11n\n# Answer:\n26508", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26508 -> common.topic.notable_types -> Postal Code\n# Answer:\n26508", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26504 -> common.topic.notable_types -> Postal Code\n# Answer:\n26504", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26504 -> location.postal_code.country -> United States of America\n# Answer:\n26504", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26507 -> location.postal_code.country -> United States of America\n# Answer:\n26507", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26508 -> location.location.contains -> Chestnut Ridge Church\n# Answer:\n26508", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26504 -> common.topic.notable_for -> g.1258skzt9\n# Answer:\n26504", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26507 -> common.topic.notable_for -> g.125dzdth5\n# Answer:\n26507", "# Reasoning Path:\nMorgantown -> location.location.people_born_here -> Alan Henderson -> sports.pro_athlete.sports_played_professionally -> m.0c54chs\n# Answer:\nAlan Henderson", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26507 -> common.topic.notable_types -> Postal Code\n# Answer:\n26507"], "ground_truth": ["26508", "26502", "26505", "26504", "26501", "26507", "26506"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5806451612903225, "ans_precission": 0.9, "ans_recall": 0.42857142857142855, "path_f1": 0.5806451612903225, "path_precision": 0.9, "path_recall": 0.42857142857142855, "path_ans_f1": 0.5806451612903225, "path_ans_precision": 0.9, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1046", "prediction": ["# Reasoning Path:\nJacksonville -> location.location.time_zones -> Eastern Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nJacksonville -> location.location.time_zones -> Eastern Time Zone -> common.topic.notable_for -> g.125bn2wcf\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nJacksonville -> location.location.time_zones -> Eastern Time Zone -> common.topic.image -> UnionSquareAtomicClock\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nJacksonville -> location.location.time_zones -> Eastern Time Zone -> common.topic.article -> m.02hcvg\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nJacksonville -> location.citytown.postal_codes -> 32099 -> location.postal_code.country -> United States of America\n# Answer:\n32099", "# Reasoning Path:\nJacksonville -> location.location.time_zones -> Eastern Time Zone -> common.topic.article -> m.0j44rkj\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nJacksonville -> location.location.adjoin_s -> m.02xjbkj -> location.adjoining_relationship.adjoins -> St. Johns County\n# Answer:\nSt. Johns County", "# Reasoning Path:\nJacksonville -> location.citytown.postal_codes -> 32099 -> common.topic.notable_for -> g.1259f0k4c\n# Answer:\n32099", "# Reasoning Path:\nJacksonville -> location.citytown.postal_codes -> 32201 -> common.topic.notable_for -> g.125dnh3dj\n# Answer:\n32201", "# Reasoning Path:\nJacksonville -> location.location.time_zones -> Eastern Time Zone -> common.topic.article -> m.0j62c65\n# Answer:\nEastern Time Zone"], "ground_truth": ["Eastern Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1047", "prediction": ["# Reasoning Path:\nDodgeBall: A True Underdog Story -> film.film.starring -> m.02kdl29 -> film.performance.character -> Steve the Pirate\n# Answer:\nSteve the Pirate", "# Reasoning Path:\nDodgeBall: A True Underdog Story -> film.film.starring -> m.03jp9kl -> film.performance.character -> Dwight\n# Answer:\nDwight", "# Reasoning Path:\nAmy Stiller -> film.actor.film -> m.063wc2g -> film.performance.character -> Flower Girl/Carol\n# Answer:\nFlower Girl/Carol", "# Reasoning Path:\nAmy Stiller -> people.person.sibling_s -> m.04dq69h -> people.sibling_relationship.sibling -> Ben Stiller\n# Answer:\nBen Stiller", "# Reasoning Path:\nDodgeBall: A True Underdog Story -> film.film.starring -> m.03jqjf8 -> film.performance.character -> Blazer\n# Answer:\nBlazer", "# Reasoning Path:\nDodgeBall: A True Underdog Story -> film.film.starring -> m.02kdl29 -> film.performance.actor -> Alan Tudyk\n# Answer:\nAlan Tudyk", "# Reasoning Path:\nAmy Stiller -> film.actor.film -> m.063wc2m -> film.performance.character -> Amy Corinne Fairbright-Lebow\n# Answer:\nAmy Corinne Fairbright-Lebow", "# Reasoning Path:\nDodgeBall: A True Underdog Story -> film.film.language -> English Language -> organization.organization_sector.organizations_in_this_sector -> dragon-class.com\n# Answer:\nEnglish Language", "# Reasoning Path:\nAmy Stiller -> people.person.profession -> Comedian -> fictional_universe.character_occupation.characters_with_this_occupation -> Bernie McCullough\n# Answer:\nComedian", "# Reasoning Path:\nAmy Stiller -> film.actor.film -> m.063wc2s -> film.performance.character -> Psychic Phone Partner\n# Answer:\nPsychic Phone Partner"], "ground_truth": ["Keno Waitress"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1048", "prediction": ["# Reasoning Path:\nHenry VII of England -> people.person.children -> Arthur, Prince of Wales -> people.person.parents -> Elizabeth of York\n# Answer:\nArthur, Prince of Wales", "# Reasoning Path:\nHenry VII of England -> people.person.children -> Arthur, Prince of Wales -> people.person.place_of_birth -> Winchester Cathedral Priory\n# Answer:\nArthur, Prince of Wales", "# Reasoning Path:\nHenry VII of England -> common.topic.notable_for -> g.125c04cnp\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nHenry VII of England -> people.person.children -> Arthur, Prince of Wales -> people.person.sibling_s -> m.04jt6xb\n# Answer:\nArthur, Prince of Wales", "# Reasoning Path:\nHenry VII of England -> people.person.children -> Edmund Tudor, Duke of Somerset -> people.person.place_of_birth -> Palace of Placentia\n# Answer:\nEdmund Tudor, Duke of Somerset", "# Reasoning Path:\nHenry VII of England -> people.person.children -> Henry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset\n# Answer:\nHenry VIII of England", "# Reasoning Path:\nHenry VII of England -> people.person.children -> Edmund Tudor, Duke of Somerset -> common.topic.notable_for -> g.125fhsz99\n# Answer:\nEdmund Tudor, Duke of Somerset", "# Reasoning Path:\nHenry VII of England -> people.person.children -> Edmund Tudor, Duke of Somerset -> people.family_member.family -> Tudor dynasty\n# Answer:\nEdmund Tudor, Duke of Somerset", "# Reasoning Path:\nHenry VII of England -> people.person.children -> Henry VIII of England -> people.person.children -> Edward VI of England\n# Answer:\nHenry VIII of England", "# Reasoning Path:\nHenry VII of England -> organization.organization_founder.organizations_founded -> Reading School -> organization.organization.headquarters -> m.05nndfv\n# Answer:\nReading School"], "ground_truth": ["Edmund Tudor, Duke of Somerset", "Arthur, Prince of Wales", "Elizabeth Tudor", "Edward Tudor", "Mary Tudor, Queen of France", "Katherine Tudor", "Henry VIII of England", "Margaret Tudor", "Roland de Velville"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.47058823529411764, "ans_precission": 0.8, "ans_recall": 0.3333333333333333, "path_f1": 0.4363636363636363, "path_precision": 0.8, "path_recall": 0.3, "path_ans_f1": 0.47058823529411764, "path_ans_precision": 0.8, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> location.country.languages_spoken -> Thai Language -> media_common.netflix_genre.titles -> Shutter\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Thai Language -> language.human_language.region -> Asia\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> media_common.netflix_genre.titles -> Shutter\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Thai Language -> common.topic.notable_types -> Human Language\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language -> common.topic.notable_types -> Human Language\n# Answer:\nVietnamese Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> language.human_language.region -> Asia\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yg\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language -> language.human_language.main_country -> Vietnam\n# Answer:\nVietnamese Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.main_country -> Myanmar\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> common.topic.notable_types -> Human Language\n# Answer:\nThai Language"], "ground_truth": ["Lao Language", "Cham language", "Mlabri Language", "Vietnamese Language", "Mon Language", "Nyaw Language", "Phu Thai language", "Akha Language", "Malay, Pattani Language", "Hmong language", "Thai Language", "Khmer language", "Saek language"], "ans_acc": 0.23076923076923078, "ans_hit": 1, "ans_f1": 0.3673469387755102, "ans_precission": 0.9, "ans_recall": 0.23076923076923078, "path_f1": 0.33333333333333337, "path_precision": 0.6, "path_recall": 0.23076923076923078, "path_ans_f1": 0.3673469387755102, "path_ans_precision": 0.9, "path_ans_recall": 0.23076923076923078}
{"id": "WebQTest-1050", "prediction": ["# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> Mickey McFinnegan\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> God\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> Meg Griffin\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> common.topic.notable_for -> g.1255sxsvl\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.works_set_here -> Family Guy Video Game!\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.0k6pxpv -> tv.regular_tv_appearance.character -> Meg Griffin\n# Answer:\nMeg Griffin", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.02ntq_4 -> tv.regular_tv_appearance.character -> Meg Griffin\n# Answer:\nMeg Griffin", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.0k6pxpv -> tv.regular_tv_appearance.actor -> Lacey Chabert\n# Answer:\nLacey Chabert", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.010btxt8 -> tv.regular_tv_appearance.actor -> Brandon Richardson\n# Answer:\nBrandon Richardson", "# Reasoning Path:\nFamily Guy -> common.topic.webpage -> m.02sfx_3 -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage"], "ground_truth": ["Meg Griffin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1051", "prediction": ["# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> location.location.containedby -> United States of America\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> travel.travel_destination.tourist_attractions -> Yerba Buena Center for the Arts\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> base.schemastaging.context_name.pronunciation -> g.125_nh8xf\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> location.location.containedby -> San Francisco Bay Area\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> travel.travel_destination.tourist_attractions -> Golden Gate Park\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> location.location.containedby -> Area codes 415 and 628\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> common.topic.notable_types -> Disaster -> type.type.domain -> Event\n# Answer:\nDisaster", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> travel.travel_destination.tourist_attractions -> Alcatraz Island\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> common.topic.notable_types -> Disaster -> freebase.type_hints.included_types -> Event\n# Answer:\nDisaster"], "ground_truth": ["1906-04-18"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1052", "prediction": ["# Reasoning Path:\nThe Omen -> film.film.starring -> m.0k610p -> film.performance.actor -> Seamus Davey-Fitzpatrick\n# Answer:\nSeamus Davey-Fitzpatrick", "# Reasoning Path:\nThe Omen -> film.film.starring -> m.05h9v78 -> film.performance.actor -> Tomas Wooler\n# Answer:\nTomas Wooler", "# Reasoning Path:\nThe Omen -> film.film.starring -> m.0k610p -> film.performance.character -> Damien Thorn\n# Answer:\nDamien Thorn", "# Reasoning Path:\nThe Omen -> film.film.sequel -> Damien: Omen II -> film.film.story_by -> David Seltzer\n# Answer:\nDamien: Omen II", "# Reasoning Path:\nThe Omen -> film.film.starring -> m.05h9v78 -> film.performance.character -> Damien Thorn\n# Answer:\nDamien Thorn", "# Reasoning Path:\nThe Omen -> film.film.starring -> m.02tb9t2 -> film.performance.character -> Keith Jennings\n# Answer:\nKeith Jennings", "# Reasoning Path:\nThe Omen -> film.film.sequel -> Damien: Omen II -> film.film.sequel -> Omen III: The Final Conflict\n# Answer:\nDamien: Omen II", "# Reasoning Path:\nThe Omen -> film.film.sequel -> Damien: Omen II -> media_common.netflix_title.netflix_genres -> Thriller\n# Answer:\nDamien: Omen II", "# Reasoning Path:\nThe Omen -> common.resource.annotations -> m.094p5d1 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nThe Omen -> film.film.sequel -> Damien: Omen II -> media_common.netflix_title.netflix_genres -> Cult Horror\n# Answer:\nDamien: Omen II"], "ground_truth": ["Seamus Davey-Fitzpatrick", "Tomas Wooler"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1054", "prediction": ["# Reasoning Path:\nFour Christmases -> film.film.starring -> m.06483k3 -> film.performance.actor -> Jon Favreau filmography\n# Answer:\nJon Favreau filmography", "# Reasoning Path:\nFour Christmases -> film.film.starring -> m.06483k3 -> film.performance.character -> Denver\n# Answer:\nDenver", "# Reasoning Path:\nFour Christmases -> film.film.starring -> m.06483jz -> film.performance.actor -> Dwight Yoakam\n# Answer:\nDwight Yoakam", "# Reasoning Path:\nFour Christmases -> film.film.starring -> m.04ddq2f -> film.performance.actor -> Reese Witherspoon\n# Answer:\nReese Witherspoon", "# Reasoning Path:\nFour Christmases -> media_common.netflix_title.netflix_genres -> Comedies\n# Answer:\nComedies", "# Reasoning Path:\nFour Christmases -> film.film.starring -> m.06483jz -> film.performance.character -> Pastor Phil\n# Answer:\nPastor Phil", "# Reasoning Path:\nFour Christmases -> film.film.film_set_decoration_by -> Jan Pascale -> film.film_set_designer.film_sets_designed -> You Again\n# Answer:\nJan Pascale", "# Reasoning Path:\nFour Christmases -> film.film.film_set_decoration_by -> Jan Pascale -> common.topic.notable_types -> Organization leader\n# Answer:\nJan Pascale", "# Reasoning Path:\nFour Christmases -> film.film.film_set_decoration_by -> Jan Pascale -> film.film_set_designer.film_sets_designed -> A Gathering of Old Men\n# Answer:\nJan Pascale", "# Reasoning Path:\nFour Christmases -> film.film.film_set_decoration_by -> Jan Pascale -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nJan Pascale"], "ground_truth": ["Jon Favreau filmography"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1055", "prediction": ["# Reasoning Path:\nEssex -> travel.travel_destination.tourist_attractions -> Colchester Zoo -> travel.tourist_attraction.near_travel_destination -> Colchester\n# Answer:\nColchester Zoo", "# Reasoning Path:\nEssex -> travel.travel_destination.tourist_attractions -> Colchester Zoo -> common.topic.article -> m.05zjnk\n# Answer:\nColchester Zoo", "# Reasoning Path:\nEssex -> base.aareas.schema.administrative_area.capital -> Chelmsford -> travel.travel_destination.tourist_attractions -> Riverside Ice & Leisure Centre\n# Answer:\nChelmsford", "# Reasoning Path:\nEssex -> travel.travel_destination.tourist_attractions -> Colchester Zoo -> common.topic.image -> A Wolf at Colchester Zoo\n# Answer:\nColchester Zoo", "# Reasoning Path:\nEssex -> travel.travel_destination.tourist_attractions -> Beth Chatto Gardens -> travel.tourist_attraction.near_travel_destination -> Colchester\n# Answer:\nBeth Chatto Gardens", "# Reasoning Path:\nEssex -> travel.travel_destination.tourist_attractions -> Waltham Abbey Royal Gunpowder Mills -> location.location.containedby -> United Kingdom\n# Answer:\nWaltham Abbey Royal Gunpowder Mills", "# Reasoning Path:\nEssex -> travel.travel_destination.tourist_attractions -> Waltham Abbey Royal Gunpowder Mills -> common.topic.image -> Napoleonic battle reenactment at Waltham Abbey Royal Gunpowder Mills\n# Answer:\nWaltham Abbey Royal Gunpowder Mills", "# Reasoning Path:\nEssex -> base.aareas.schema.administrative_area.capital -> Chelmsford -> travel.travel_destination.tourist_attractions -> Tropical Wings Zoo\n# Answer:\nChelmsford", "# Reasoning Path:\nEssex -> base.aareas.schema.administrative_area.administrative_area_type -> English non metropolitan county -> base.aareas.schema.administrative_area_type.pertains_to -> England\n# Answer:\nEnglish non metropolitan county", "# Reasoning Path:\nEssex -> travel.travel_destination.tourist_attractions -> Beth Chatto Gardens -> symbols.namesake.named_after -> Beth Chatto\n# Answer:\nBeth Chatto Gardens"], "ground_truth": ["Thorndon Country Park", "Waltham Abbey Royal Gunpowder Mills", "Beth Chatto Gardens", "RHS Garden, Hyde Hall", "Marsh Farm Country Park", "Cudmore Grove Country Park", "Mistley Place Park", "Green Island Gardens", "Stansted Mountfitchet Castle", "Colchester Zoo"], "ans_acc": 0.3, "ans_hit": 1, "ans_f1": 0.42, "ans_precission": 0.7, "ans_recall": 0.3, "path_f1": 0.42, "path_precision": 0.7, "path_recall": 0.3, "path_ans_f1": 0.42, "path_ans_precision": 0.7, "path_ans_recall": 0.3}
{"id": "WebQTest-1056", "prediction": ["# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.roster -> m.011461z_ -> sports.sports_team_roster.player -> LeBron James\n# Answer:\nLeBron James", "# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.roster -> m.011461z_ -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.roster -> m.0102thh3 -> sports.sports_team_roster.player -> Scotty Hopson\n# Answer:\nScotty Hopson", "# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.roster -> m.0114813g -> sports.sports_team_roster.player -> Brendan Haywood\n# Answer:\nBrendan Haywood", "# Reasoning Path:\nCleveland Cavaliers -> sports.professional_sports_team.draft_picks -> m.04_byvx -> sports.sports_league_draft_pick.player -> JJ Hickson\n# Answer:\nJJ Hickson", "# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.roster -> m.0102thh3 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nCleveland Cavaliers -> sports.professional_sports_team.draft_picks -> m.04_bzkk -> sports.sports_league_draft_pick.player -> Shannon Brown\n# Answer:\nShannon Brown", "# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.roster -> m.0114813g -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.coaches -> m.0_qrjgt -> sports.sports_team_coach_tenure.coach -> Mike Brown\n# Answer:\nMike Brown", "# Reasoning Path:\nCleveland Cavaliers -> sports.professional_sports_team.draft_picks -> m.04_byvx -> sports.sports_league_draft_pick.draft -> 2008 NBA draft\n# Answer:\n2008 NBA draft"], "ground_truth": ["Anthony Bennett", "Dion Waiters", "Shannon Brown", "JJ Hickson", "Andrew Wiggins", "Luke Jackson", "LeBron James"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.3529411764705882, "ans_precission": 0.3, "ans_recall": 0.42857142857142855, "path_f1": 0.2608695652173913, "path_precision": 0.3, "path_recall": 0.23076923076923078, "path_ans_f1": 0.3529411764705882, "path_ans_precision": 0.3, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1057", "prediction": ["# Reasoning Path:\nAnne Frank -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Bergen-Belsen concentration camp -> people.place_of_interment.interred_here -> Abel Herzberg\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Bergen-Belsen concentration camp -> common.topic.article -> m.0139dt\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Bergen-Belsen concentration camp -> people.place_of_interment.interred_here -> Abraham Asscher\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Bergen-Belsen concentration camp -> common.topic.image -> Mass graves at Bergen Belsen\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Bergen-Belsen concentration camp -> people.place_of_interment.interred_here -> Adrien de Noailles\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> people.deceased_person.place_of_death -> Bergen-Belsen concentration camp -> people.place_of_interment.interred_here -> Abel Herzberg\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Bergen-Belsen concentration camp -> common.topic.image -> 450px-Anne_frank_memorial_bergen_belsen.jpg\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> people.deceased_person.place_of_death -> Bergen-Belsen concentration camp -> common.topic.article -> m.0139dt\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Bergen-Belsen concentration camp -> common.topic.image -> 610px-Bergen_Belsen_Liberation_05.jpg\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> people.deceased_person.place_of_death -> Bergen-Belsen concentration camp -> people.place_of_interment.interred_here -> Abraham Asscher\n# Answer:\nBergen-Belsen concentration camp"], "ground_truth": ["Bergen-Belsen concentration camp"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1059", "prediction": ["# Reasoning Path:\nTimor-Leste -> location.country.languages_spoken -> Bekais -> common.topic.notable_types -> Human Language\n# Answer:\nBekais", "# Reasoning Path:\nTimor-Leste -> location.country.official_language -> Tetun Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nTetun Language", "# Reasoning Path:\nTimor-Leste -> location.country.languages_spoken -> Bekais -> common.topic.notable_for -> g.125b9xx03\n# Answer:\nBekais", "# Reasoning Path:\nTimor-Leste -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Indonesia\n# Answer:\nEnglish Language", "# Reasoning Path:\nTimor-Leste -> location.country.languages_spoken -> Makalero dialect -> common.topic.notable_for -> g.12561xxm2\n# Answer:\nMakalero dialect", "# Reasoning Path:\nTimor-Leste -> location.country.official_language -> Tetun Language -> base.rosetta.languoid.parent -> East Nuclear Timor Group\n# Answer:\nTetun Language", "# Reasoning Path:\nTimor-Leste -> location.statistical_region.internet_users_percent_population -> g.11b60w2nsc\n# Answer:\nlocation.statistical_region.internet_users_percent_population", "# Reasoning Path:\nTimor-Leste -> location.country.languages_spoken -> Makalero dialect -> common.topic.notable_types -> Human Language\n# Answer:\nMakalero dialect", "# Reasoning Path:\nTimor-Leste -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nTimor-Leste -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language"], "ground_truth": ["Habu Language", "Galoli Language", "Indonesian Language", "Wetarese", "Tukudede Language", "Mambai Language", "Makasae Language", "Dawan", "Tetun-Terik", "Tetun Language", "Fataluku Language", "Bekais", "Portuguese Language", "English Language", "Makalero dialect", "Makuv'a Language", "Bunak Language", "Kawaimina languages", "Idalaka", "Kemak Language"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.32727272727272727, "ans_precission": 0.9, "ans_recall": 0.2, "path_f1": 0.32727272727272727, "path_precision": 0.9, "path_recall": 0.2, "path_ans_f1": 0.32727272727272727, "path_ans_precision": 0.9, "path_ans_recall": 0.2}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Programmer\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Entrepreneur\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Businessperson\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.owner -> Mark Zuckerberg\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> influence.influence_node.influenced -> Austin Cameron\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Social networking service -> common.topic.image -> Soc-net-paten-growth-chart\n# Answer:\nSocial networking service", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Social networking service -> common.topic.subject_of -> Krishna Mali\n# Answer:\nSocial networking service", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Social networking service -> computer.software_genre.software_in_genre -> 2go\n# Answer:\nSocial networking service", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> influence.influence_node.influenced -> Paul Scolnick\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.owner -> Facebook, Inc.\n# Answer:\nFacebook"], "ground_truth": ["Tyler Winklevoss", "Cameron Winklevoss"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1060", "prediction": ["# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Vatican City\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> common.topic.notable_types -> Currency\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Belgium\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37cxr\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Tenerani -> base.thoroughbredracing.thoroughbred_racehorse.foal_if_this_horse_is_sire -> Ribot\n# Answer:\nTenerani", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Tenerani -> biology.organism.sex -> Male\n# Answer:\nTenerani", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Nearco -> biology.organism.sex -> Male\n# Answer:\nNearco"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1061", "prediction": ["# Reasoning Path:\nGreek Language -> language.human_language.language_family -> Indo-European languages -> language.language_family.sub_families -> Goidelic languages\n# Answer:\nIndo-European languages", "# Reasoning Path:\nGreek Language -> language.human_language.language_family -> Indo-European languages -> common.topic.article -> m.03v0m\n# Answer:\nIndo-European languages", "# Reasoning Path:\nGreek Language -> base.rosetta.languoid.parent -> Attic Group -> base.rosetta.languoid.parent -> Greek Group\n# Answer:\nAttic Group", "# Reasoning Path:\nGreek Language -> language.human_language.language_family -> Indo-European languages -> base.schemastaging.context_name.pronunciation -> g.125_r38g5\n# Answer:\nIndo-European languages", "# Reasoning Path:\nGreek Language -> language.human_language.language_family -> Hellenic languages -> common.topic.article -> m.05pd8n2\n# Answer:\nHellenic languages", "# Reasoning Path:\nGreek Language -> language.human_language.language_family -> Indo-European languages -> language.language_family.sub_families -> Balto-Slavic languages\n# Answer:\nIndo-European languages", "# Reasoning Path:\nGreek Language -> base.rosetta.languoid.parent -> Attic Group -> base.rosetta.languoid.languoid_class -> Group\n# Answer:\nAttic Group", "# Reasoning Path:\nGreek Language -> language.human_language.language_family -> Indo-European languages -> language.language_family.sub_families -> Bihari languages\n# Answer:\nIndo-European languages", "# Reasoning Path:\nGreek Language -> base.rosetta.languoid.parent -> Attic Group -> base.rosetta.languoid.child -> Yevanic language\n# Answer:\nAttic Group", "# Reasoning Path:\nGreek Language -> language.human_language.language_family -> Hellenic languages -> language.language_family.sub_families -> Ancient Greek dialects\n# Answer:\nHellenic languages"], "ground_truth": ["Attic Group"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1062", "prediction": ["# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Vatican City\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> common.topic.notable_types -> Currency\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Belgium\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37cxr\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Tenerani -> base.thoroughbredracing.thoroughbred_racehorse.foal_if_this_horse_is_sire -> Ribot\n# Answer:\nTenerani", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Tenerani -> biology.organism.sex -> Male\n# Answer:\nTenerani", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Nearco -> biology.organism.sex -> Male\n# Answer:\nNearco"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1063", "prediction": ["# Reasoning Path:\nAudrey Hepburn -> people.person.place_of_birth -> Ixelles -> location.location.people_born_here -> Ad\u00e8le Christiaens\n# Answer:\nIxelles", "# Reasoning Path:\nAudrey Hepburn -> people.person.place_of_birth -> Ixelles -> location.location.contains -> La Cambre Abbey\n# Answer:\nIxelles", "# Reasoning Path:\nAudrey Hepburn -> people.person.place_of_birth -> Ixelles -> location.location.people_born_here -> Agn\u00e8s Varda\n# Answer:\nIxelles", "# Reasoning Path:\nAudrey Hepburn -> people.person.place_of_birth -> Ixelles -> common.topic.image -> La Cambre Abbey in Ixelles.\n# Answer:\nIxelles", "# Reasoning Path:\nAudrey Hepburn -> people.person.place_of_birth -> Ixelles -> location.location.people_born_here -> Alfred Bastien\n# Answer:\nIxelles", "# Reasoning Path:\nAudrey Hepburn -> people.person.place_of_birth -> Ixelles -> common.topic.image -> Ixelles (Elsene) municipality in the Brussels-Capital Region\n# Answer:\nIxelles", "# Reasoning Path:\nAudrey Hepburn -> award.ranked_item.appears_in_ranked_lists -> m.09p5yb2 -> award.ranking.list -> AFI's 100 Years...100 Stars\n# Answer:\nAFI's 100 Years...100 Stars", "# Reasoning Path:\nAudrey Hepburn -> award.award_winner.awards_won -> m.059wntr -> award.award_honor.ceremony -> 47th Golden Globe Awards\n# Answer:\n47th Golden Globe Awards", "# Reasoning Path:\nAudrey Hepburn -> award.award_winner.awards_won -> m.06zyf62 -> award.award_honor.award -> Presidential Medal of Freedom\n# Answer:\nPresidential Medal of Freedom", "# Reasoning Path:\nAudrey Hepburn -> award.award_winner.awards_won -> m.059wntr -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nWinning work"], "ground_truth": ["Ixelles"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1065", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zsqt -> film.performance.actor -> David Prowse\n# Answer:\nDavid Prowse", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nThe Making of Star Wars", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.film -> Star Wars\n# Answer:\nStar Wars", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Uncredited\n# Answer:\nUncredited", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zsqt -> film.performance.film -> The Making of Star Wars\n# Answer:\nThe Making of Star Wars", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice"], "ground_truth": ["Matt Lanter", "Zac Efron", "Dr. Smoov", "James Earl Jones", "Abraham Benrubi"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.20000000000000004, "ans_precission": 0.2, "ans_recall": 0.2, "path_f1": 0.1904761904761905, "path_precision": 0.2, "path_recall": 0.18181818181818182, "path_ans_f1": 0.20000000000000004, "path_ans_precision": 0.2, "path_ans_recall": 0.2}
{"id": "WebQTest-1067", "prediction": ["# Reasoning Path:\nDouglas MacArthur -> people.person.education -> m.04hdcyk -> education.education.institution -> United States Military Academy\n# Answer:\nUnited States Military Academy", "# Reasoning Path:\nDouglas MacArthur -> people.person.education -> m.0n0mx8r -> education.education.institution -> TMI \u2014 The Episcopal School of Texas\n# Answer:\nTMI \u2014 The Episcopal School of Texas", "# Reasoning Path:\nDouglas MacArthur -> symbols.name_source.namesakes -> Douglas MacArthur High School -> education.educational_institution.school_type -> High school\n# Answer:\nDouglas MacArthur High School", "# Reasoning Path:\nDouglas MacArthur -> symbols.name_source.namesakes -> Douglas MacArthur High School -> organization.organization.headquarters -> m.0dhdwr7\n# Answer:\nDouglas MacArthur High School", "# Reasoning Path:\nDouglas MacArthur -> symbols.name_source.namesakes -> MacArthur Causeway -> common.topic.article -> m.09tmvf\n# Answer:\nMacArthur Causeway", "# Reasoning Path:\nDouglas MacArthur -> people.person.quotations -> A general is just as good or just as bad as the troops under his command make him. -> common.topic.notable_types -> Quotation\n# Answer:\nA general is just as good or just as bad as the troops under his command make him.", "# Reasoning Path:\nDouglas MacArthur -> people.person.quotations -> Age wrinkles the body. Quitting wrinkles the soul. -> common.topic.notable_types -> Quotation\n# Answer:\nAge wrinkles the body. Quitting wrinkles the soul.", "# Reasoning Path:\nDouglas MacArthur -> symbols.name_source.namesakes -> Douglas MacArthur High School -> education.educational_institution.students_graduates -> m.012dxr1b\n# Answer:\nDouglas MacArthur High School", "# Reasoning Path:\nDouglas MacArthur -> symbols.name_source.namesakes -> MacArthur Bridge -> transportation.bridge.body_of_water_spanned -> Detroit River\n# Answer:\nMacArthur Bridge", "# Reasoning Path:\nDouglas MacArthur -> people.person.quotations -> A general is just as good or just as bad as the troops under his command make him. -> common.topic.notable_for -> g.125b9dw19\n# Answer:\nA general is just as good or just as bad as the troops under his command make him."], "ground_truth": ["United States Military Academy", "TMI \u2014 The Episcopal School of Texas"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1068", "prediction": ["# Reasoning Path:\nBarbados -> location.location.contains -> Saint Michael Parish -> location.location.people_born_here -> Rihanna\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nBarbados -> location.location.contains -> Saint Michael Parish -> location.location.people_born_here -> Anderson Sealy\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nBarbados -> location.location.contains -> Saint Michael Parish -> common.topic.webpage -> m.0b47dqc\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nBarbados -> location.location.contains -> Saint Michael Parish -> location.location.people_born_here -> Barto Bartlett\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nBarbados -> location.location.contains -> Saint Michael Parish -> common.topic.image -> Map of Barbados showing the Saint Michael parish\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nBarbados -> location.statistical_region.official_development_assistance -> g.1hhc37pj3\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBarbados -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc37ppn\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nBarbados -> location.statistical_region.official_development_assistance -> g.1hhc382cm\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBarbados -> location.location.contains -> 3Ws Oval -> common.topic.article -> m.02656zh\n# Answer:\n3Ws Oval", "# Reasoning Path:\nBarbados -> location.statistical_region.official_development_assistance -> g.1hhc38m3z\n# Answer:\nlocation.statistical_region.official_development_assistance"], "ground_truth": ["Prof Edwards", "Kerry Holder", "Clennell Wickham", "Ricardo Morris", "Hugh Gordon Cummins", "Wendell White", "Tony White", "Jim Hendy", "Omari Eastmond", "Alison Sealy-Smith", "Riviere Williams", "Kandy Fenty", "Dadrian Forde", "John Goddard", "Hadan Holligan", "Pamela Lavine", "Roger Blades", "Kadeem Latham", "George Moe", "Richard Lavine", "Dave Wilkins", "Zeeteah Massiah", "Sylvester Braithwaite", "Brandy Fenty", "Karen Lord", "Omar Archer", "Javon Searles", "George Blackman", "William Cleeve", "Romell Brathwaite", "Jason Holder", "Andy Straughn", "Wilfred Wood", "Samuel Jackman Prescod", "Jonathan Straker", "Shakera Selman", "Winston Reid", "Ricky Hoyte", "Nevada Phillips", "G. Aubrey Goodman", "Renn Dickson Hampden", "Michael Stoute", "Jason Boxhill", "Float Woods", "George Codrington", "Colin Forde", "Richard Pile", "Kyle Hope", "Robert Bailey", "James Wedderburn", "Ronald Fenty", "Marita Payne", "Gordon Greenidge", "Dennis Archer", "Nick Nanton", "Lunden De'Leon", "Jackie Roberts", "Hugh Springer", "John Parris", "Hal Padmore", "Owen Arthur", "Norman Forde", "Billie Miller", "William Maynard Gomm", "Horace Stoute", "Arturo Tappin", "Menelik Shabazz", "Renaldo Gilkes", "Bentley Springer", "Tom Adams", "Jamal Chandler", "Tony Cordle", "Kyle Gibson", "Carlos Brathwaite", "Chai Lloyd", "Colin Young", "Shawn Terry Cox", "Jomo Brathwaite", "Jason Carter", "Shai Hope", "Philo Wallace", "William Perkins", "Emmerson Trotman", "Nita Barrow", "Barry Skeete", "Denys Williams", "Peter Lashley", "Xavier Marcus", "James Waithe", "Tim Thorogood", "Murr", "Rashidi Boucher", "Lloyd Erskine Sandiford", "Redd Pepper", "Adriel Brathwaite", "Jaicko", "Robert Callender", "Shaquana Quintyne", "Carl Joseph", "Greg Armstrong", "Sir William Randolph Douglas", "Anthony Kellman", "Robin Bynoe", "Trevor W. Payne", "Curtis Odle", "John Lucas", "Goodridge Roberts", "Anderson Cummins", "Henry Doorly", "Cecil Foster", "Jon Rubin", "Charlie Griffith", "Seymour Nurse", "Jennifer Gibbons", "Renaldo Fenty", "Ryan Hinds", "Alvin Rouse", "Sheridan Grosvenor", "Ryan Wiggins", "Arthur Hendy", "Edwin Lascelles, 1st Baron Harewood", "John Holder", "Tony Reid", "Kat Flint", "Richard Moody", "Ramuel Miller", "Austin Clarke", "Sam Seale", "Martin Nurse", "Hal Linton", "Shakera Reece", "Kycia Knight", "George Alleyne", "Gregory Goodridge", "Hartley Alleyne", "Kirk Edwards", "Frank L. White", "Hilary Beckles", "Chris Jordan", "Monica Braithwaite", "Frank Leslie Walcott", "Rashida Williams", "Bryan Neblett", "Edward Evelyn Greaves", "Raheim Sargeant", "Anthony Forde", "Wayne Sobers", "Adrian Chase", "David Comissiong", "Ashley Bickerton", "Armando Lashley", "Odimumba Kwamdela", "Ron King", "Clyde Mascoll", "Kensley Reece", "Glenville Lovell", "Kyle Mayers", "Annalee Davis", "Kirk Corbin", "Lionel Paul", "Mabel Keaton Staupers", "Mia Mottley", "Ricardo Ellcock", "Malcolm Marshall", "Agymah Kamau", "Arnold Josiah Ford", "Dwayne Stanford", "Alana Shipp", "Tony Cozier", "Don Kinch", "John Shepherd", "Magnet Man", "Chris Braithwaite", "Conrad Hunte", "Douglas Dummett", "Hugh Laing", "Tristan Parris", "William T. Shorey", "Diquan Adamson", "Keith Griffith", "June Gibbons", "Rickey A. Walcott", "Kemar Headley", "Frank Collymore", "Woodie Blackman", "Jabarry Chandler", "Ivor Germain", "Denzil H. Hurley", "Ella Jane New", "Dwight James", "Craig Worrell", "Branford Taitt", "Wyndham Gittens", "Rayvon", "Earl Maynard", "Micky Welch", "Lloyd A. Thompson", "Thomas Jordan", "Christopher Codrington", "Shane Dowrich", "John Richard Farre", "Louis Eugene King", "Cynthia Rosalina La Touche Andersen", "Dwayne Griffith", "Henry Honychurch Gorringe", "Lene Hall", "Anne Cools", "Campbell Foster", "Roy Callender", "Deandra Dottin", "Sylvesta Stuart", "Crystal Fenty", "Mario Harte", "Richard B. Moore", "Romario Harewood", "Anthony Alleyne", "Alan Emtage", "Priya Balachandran", "Samuel Hinds", "Avinash Persaud", "Kyshona Knight", "Richard Roett", "Neville Goddard", "Albert Beckles", "Eyre Sealy", "Seibert Straughn", "Andre Bourne", "David Holford", "Kyle Corbin", "Stephen Griffith", "Percy Tarilton"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1069", "prediction": ["# Reasoning Path:\nChina -> location.country.currency_used -> Renminbi -> common.topic.notable_types -> Currency\n# Answer:\nRenminbi", "# Reasoning Path:\nChina -> location.location.partially_contains -> Teram Kangri -> base.schemastaging.disputed_location.claimed_by -> Pakistan\n# Answer:\nTeram Kangri", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Yili horse -> common.topic.notable_types -> Organism Classification\n# Answer:\nYili horse", "# Reasoning Path:\nChina -> location.location.partially_contains -> Teram Kangri -> location.location.partially_containedby -> Xinjiang\n# Answer:\nTeram Kangri", "# Reasoning Path:\nChina -> location.location.partially_contains -> Amur River -> location.location.partially_containedby -> Russia\n# Answer:\nAmur River", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Balikun horse -> common.topic.notable_types -> Organism Classification\n# Answer:\nBalikun horse", "# Reasoning Path:\nChina -> location.location.partially_contains -> Teram Kangri -> location.location.containedby -> Karakoram\n# Answer:\nTeram Kangri", "# Reasoning Path:\nChina -> location.location.partially_contains -> Argun River -> location.location.partially_containedby -> Russia\n# Answer:\nArgun River", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Black Telescope -> biology.animal_breed.breed_of -> Goldfish\n# Answer:\nBlack Telescope", "# Reasoning Path:\nChina -> location.location.partially_contains -> Teram Kangri -> base.schemastaging.disputed_location.claimed_by -> India\n# Answer:\nTeram Kangri"], "ground_truth": ["Renminbi"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Teacher -> common.topic.notable_types -> Profession\n# Answer:\nTeacher", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> common.topic.notable_types -> Profession\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Architect -> common.topic.notable_types -> Profession\n# Answer:\nArchitect", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> fictional_universe.character_occupation.characters_with_this_occupation -> Indiana Jones\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Statesman -> common.topic.notable_types -> Profession\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Polly Jefferson\n# Answer:\nPolly Jefferson", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> people.profession.specialization_of -> Scientist\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Politician -> freebase.type_hints.included_types -> Topic\n# Answer:\nPolitician"], "ground_truth": ["Inventor", "Statesman", "Writer", "Teacher", "Archaeologist", "Architect", "Philosopher", "Author", "Farmer", "Lawyer"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.48, "ans_precission": 0.6, "ans_recall": 0.4, "path_f1": 0.37499999999999994, "path_precision": 0.5, "path_recall": 0.3, "path_ans_f1": 0.48, "path_ans_precision": 0.6, "path_ans_recall": 0.4}
{"id": "WebQTest-1070", "prediction": ["# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.gender -> Male\n# Answer:\nMichael Joseph Jackson, Jr.", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.nationality -> United States of America\n# Answer:\nMichael Joseph Jackson, Jr.", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.profession -> Actor\n# Answer:\nMichael Joseph Jackson, Jr.", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Paris-Michael Katherine Jackson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nParis-Michael Katherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Paris-Michael Katherine Jackson -> common.topic.article -> m.0j38d_7\n# Answer:\nParis-Michael Katherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Paris-Michael Katherine Jackson -> common.topic.subject_of -> Diamond Ranch Academy\n# Answer:\nParis-Michael Katherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Paris-Michael Katherine Jackson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nParis-Michael Katherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.profession -> TV Personality\n# Answer:\nMichael Joseph Jackson, Jr.", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Paris-Michael Katherine Jackson -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nParis-Michael Katherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Prince Michael Jackson II -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nPrince Michael Jackson II"], "ground_truth": ["Paris-Michael Katherine Jackson", "Michael Joseph Jackson, Jr.", "Prince Michael Jackson II"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1072", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y46 -> government.government_position_held.office_holder -> Paul Fannin\n# Answer:\nPaul Fannin", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y42 -> government.government_position_held.office_holder -> Ernest McFarland\n# Answer:\nErnest McFarland", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.0hz834l -> government.government_position_held.office_holder -> Jan Brewer\n# Answer:\nJan Brewer", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y46 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y42 -> government.government_position_held.office_position_or_title -> Governor of Arizona\n# Answer:\nGovernor of Arizona", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.0hz834l -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.0hz834l -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nBasic title", "# Reasoning Path:\nArizona -> location.statistical_region.religions -> m.04403h9 -> location.religion_percentage.religion -> Pentecostalism\n# Answer:\nPentecostalism", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.resource -> ATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!\n# Answer:\nATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!"], "ground_truth": ["Jan Brewer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1074", "prediction": ["# Reasoning Path:\nLamar Odom -> base.schemastaging.athlete_extra.salary -> m.0j2l84k -> base.schemastaging.athlete_salary.team -> Dallas Mavericks\n# Answer:\nDallas Mavericks", "# Reasoning Path:\nLamar Odom -> basketball.basketball_player.player_statistics -> m.04qrr58 -> basketball.basketball_player_stats.team -> Los Angeles Clippers\n# Answer:\nLos Angeles Clippers", "# Reasoning Path:\nLamar Odom -> base.schemastaging.athlete_extra.salary -> m.0_w1nkg -> base.schemastaging.athlete_salary.team -> Los Angeles Clippers\n# Answer:\nLos Angeles Clippers", "# Reasoning Path:\nLamar Odom -> sports.pro_athlete.teams -> m.0j2gtf_ -> sports.sports_team_roster.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nLamar Odom -> base.schemastaging.athlete_extra.salary -> m.0j2l84k -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nLamar Odom -> basketball.basketball_player.player_statistics -> m.04qrr58 -> basketball.basketball_player_stats.season -> 1999\u20132000 NBA season\n# Answer:\n1999\u20132000 NBA season", "# Reasoning Path:\nLamar Odom -> basketball.basketball_player.player_statistics -> m.04q8xv5 -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nLamar Odom -> base.schemastaging.athlete_extra.salary -> m.0_w1nkg -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nLamar Odom -> sports.pro_athlete.teams -> m.0j2gtf_ -> sports.sports_team_roster.position -> Power forward\n# Answer:\nPower forward", "# Reasoning Path:\nLamar Odom -> sports.pro_athlete.teams -> m.010b9r96 -> sports.sports_team_roster.team -> New York Knicks\n# Answer:\nNew York Knicks"], "ground_truth": ["New York Knicks", "Los Angeles Lakers", "Saski Baskonia", "Los Angeles Clippers", "Dallas Mavericks"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.6857142857142857, "ans_precission": 0.6, "ans_recall": 0.8, "path_f1": 0.4615384615384615, "path_precision": 0.6, "path_recall": 0.375, "path_ans_f1": 0.6857142857142857, "path_ans_precision": 0.6, "path_ans_recall": 0.8}
{"id": "WebQTest-1075", "prediction": ["# Reasoning Path:\nGreat Pyramid of Giza -> location.location.containedby -> Egypt -> location.country.administrative_divisions -> Giza Governorate\n# Answer:\nEgypt", "# Reasoning Path:\nGreat Pyramid of Giza -> location.location.containedby -> Giza Plateau -> location.location.containedby -> Giza\n# Answer:\nGiza Plateau", "# Reasoning Path:\nGreat Pyramid of Giza -> architecture.building.building_complex -> Giza Plateau -> location.location.containedby -> Giza\n# Answer:\nGiza Plateau", "# Reasoning Path:\nGreat Pyramid of Giza -> location.location.containedby -> Egypt -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nEgypt", "# Reasoning Path:\nGreat Pyramid of Giza -> location.location.containedby -> Egypt -> location.location.events -> Greco-Persian Wars\n# Answer:\nEgypt", "# Reasoning Path:\nGreat Pyramid of Giza -> location.location.containedby -> Giza Plateau -> location.location.contains -> Pyramid of Menkaure\n# Answer:\nGiza Plateau", "# Reasoning Path:\nGreat Pyramid of Giza -> architecture.building.building_complex -> Giza Plateau -> location.location.contains -> Pyramid of Menkaure\n# Answer:\nGiza Plateau", "# Reasoning Path:\nGreat Pyramid of Giza -> location.location.containedby -> Giza Plateau -> common.topic.notable_for -> g.125bpgq0f\n# Answer:\nGiza Plateau", "# Reasoning Path:\nGreat Pyramid of Giza -> architecture.building.building_complex -> Giza Plateau -> common.topic.notable_for -> g.125bpgq0f\n# Answer:\nGiza Plateau", "# Reasoning Path:\nGreat Pyramid of Giza -> location.location.containedby -> Egypt -> location.location.events -> Battle of al-Babein\n# Answer:\nEgypt"], "ground_truth": ["Giza Plateau"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1077", "prediction": ["# Reasoning Path:\nRussia -> location.country.languages_spoken -> Kumyk Language -> language.human_language.language_family -> Altaic languages\n# Answer:\nKumyk Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> language.human_language.countries_spoken_in -> Ukraine\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Kumyk Language -> common.topic.notable_types -> Human Language\n# Answer:\nKumyk Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Yiddish Language -> language.human_language.countries_spoken_in -> Sweden\n# Answer:\nYiddish Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Kumyk Language -> language.human_language.language_family -> Turkic languages\n# Answer:\nKumyk Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Yiddish Language -> common.topic.notable_types -> Human Language\n# Answer:\nYiddish Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> common.topic.notable_types -> Human Language\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.11b60vv5th\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Yiddish Language -> language.human_language.main_country -> United States of America\n# Answer:\nYiddish Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Abaza Language -> language.human_language.region -> Europe\n# Answer:\nAbaza Language"], "ground_truth": ["Altai language", "Abaza Language", "Russian Language", "Komi language", "Yakut Language", "Tuvin Language", "Yiddish Language", "Nogai Language", "Tabassaran Language", "Dargwa Language", "Aghul language", "Azerbaijani language", "Lak Language", "Bashkir Language", "Ukrainian Language", "Kalmyk-Oirat Language", "Rutul language", "Avar Language", "Khakas Language", "Erzya Language", "Adyghe Language", "Tsakhur Language", "Buryat language", "Osetin Language", "Crimean Turkish Language", "Lezgi Language", "Kumyk Language", "Udmurt Language", "Kabardian Language", "Moksha Language", "Chechen Language", "Karachay-Balkar Language", "Tatar Language", "Mari language", "Ingush Language"], "ans_acc": 0.11428571428571428, "ans_hit": 1, "ans_f1": 0.2028169014084507, "ans_precission": 0.9, "ans_recall": 0.11428571428571428, "path_f1": 0.2028169014084507, "path_precision": 0.9, "path_recall": 0.11428571428571428, "path_ans_f1": 0.2028169014084507, "path_ans_precision": 0.9, "path_ans_recall": 0.11428571428571428}
{"id": "WebQTest-1078", "prediction": ["# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> Japan\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars"], "ground_truth": ["Esperanto Language", "English Language", "Lojban"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1079", "prediction": ["# Reasoning Path:\nKV62 -> location.location.containedby -> Valley of the Kings -> location.location.containedby -> Theban Necropolis\n# Answer:\nValley of the Kings", "# Reasoning Path:\nKV62 -> location.location.containedby -> Valley of the Kings -> location.location.containedby -> Egypt\n# Answer:\nValley of the Kings", "# Reasoning Path:\nKV62 -> location.location.containedby -> Valley of the Kings -> travel.tourist_attraction.near_travel_destination -> Luxor\n# Answer:\nValley of the Kings", "# Reasoning Path:\nKV62 -> location.location.containedby -> Egypt -> location.country.languages_spoken -> Modern Standard Arabic\n# Answer:\nEgypt", "# Reasoning Path:\nKV62 -> common.topic.article -> m.01qg6v\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nKV62 -> location.location.containedby -> Valley of the Kings -> location.location.contains -> KV6\n# Answer:\nValley of the Kings", "# Reasoning Path:\nKV62 -> location.location.containedby -> Egypt -> location.country.administrative_divisions -> Cairo\n# Answer:\nEgypt", "# Reasoning Path:\nKV62 -> location.location.containedby -> Egypt -> location.country.administrative_divisions -> Cairo Governorate\n# Answer:\nEgypt", "# Reasoning Path:\nKV62 -> location.location.containedby -> Egypt -> location.country.administrative_divisions -> Suez Governorate\n# Answer:\nEgypt", "# Reasoning Path:\nKV62 -> location.location.containedby -> Valley of the Kings -> location.location.contains -> KV1\n# Answer:\nValley of the Kings"], "ground_truth": ["Egypt"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 3: 1844-1846\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 5: 1851-1855\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin -> book.author.works_written -> The Autobiography of Charles Darwin\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Anne Darwin -> people.person.parents -> Emma Darwin\n# Answer:\nAnne Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nEngland"], "ground_truth": ["Cartas de Darwin 18251859", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "The Structure And Distribution of Coral Reefs", "The descent of man and selection in relation to sex.", "The voyage of the Beagle.", "To the members of the Down Friendly Club", "The Origin of Species (World's Classics)", "Les moyens d'expression chez les animaux", "The action of carbonate of ammonia on the roots of certain plants", "The origin of species : complete and fully illustrated", "The Autobiography of Charles Darwin, and selected letters", "The Origin of Species (Mentor)", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "The Correspondence of Charles Darwin, Volume 12: 1864", "Notebooks on transmutation of species", "The Correspondence of Charles Darwin, Volume 1", "The origin of species", "Part I: Contributions to the Theory of Natural Selection / Part II", "Darwin Darwin", "The Correspondence of Charles Darwin, Volume 13", "Charles Darwin's marginalia", "The Correspondence of Charles Darwin, Volume 8: 1860", "Darwin's notebooks on transmutation of species", "The geology of the voyage of H.M.S. Beagle", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "The Correspondence of Charles Darwin, Volume 4", "Origin of Species (Everyman's University Paperbacks)", "The Correspondence of Charles Darwin, Volume 2", "Evolution by natural selection", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "vari\u00eberen der huisdieren en cultuurplanten", "The Correspondence of Charles Darwin, Volume 11: 1863", "Charles Darwin", "The Different Forms of Flowers on Plants of the Same Species", "The Correspondence of Charles Darwin, Volume 15", "The Origin of Species (Collector's Library)", "The Correspondence of Charles Darwin, Volume 16: 1868", "Tesakneri tsagume\u030c", "The\u0301orie de l'e\u0301volution", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The Correspondence of Charles Darwin, Volume 15: 1867", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "Geological Observations on South America", "On evolution", "Diary of the voyage of H.M.S. Beagle", "The Origin Of Species", "Autobiography of Charles Darwin", "The expression of the emotions in man and animals.", "Darwin Compendium", "The Voyage of the Beagle", "Leben und Briefe von Charles Darwin", "The Orgin of Species", "The Correspondence of Charles Darwin, Volume 3", "The Correspondence of Charles Darwin, Volume 8", "Darwin", "Origin of Species (Harvard Classics, Part 11)", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The structure and distribution of coral reefs.", "La vie et la correspondance de Charles Darwin", "Questions about the breeding of animals", "The Life of Erasmus Darwin", "Darwinism stated by Darwin himself", "The Correspondence of Charles Darwin, Volume 10", "Voyage of the Beagle (Dover Value Editions)", "The Autobiography of Charles Darwin (Great Minds Series)", "Resa kring jorden", "Gesammelte kleinere Schriften", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "The Correspondence of Charles Darwin, Volume 12", "The autobiography of Charles Darwin", "From Darwin's unpublished notebooks", "Voyage of the Beagle", "Origin of Species", "Del Plata a Tierra del Fuego", "Evolution and natural selection", "Voyage of the Beagle (Harvard Classics, Part 29)", "On the origin of species by means of natural selection", "The Correspondence of Charles Darwin, Volume 14", "On the tendency of species to form varieties", "Die fundamente zur entstehung der arten", "Darwin's journal", "The Correspondence of Charles Darwin, Volume 17: 1869", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The Darwin Reader First Edition", "The portable Darwin", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "The Formation of Vegetable Mould through the Action of Worms", "genese\u014ds t\u014dn eid\u014dn", "The Correspondence of Charles Darwin, Volume 10: 1862", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "The Correspondence of Charles Darwin, Volume 14: 1866", "The Descent of Man, and Selection in Relation to Sex", "Reise um die Welt 1831 - 36", "The living thoughts of Darwin", "Notes on the fertilization of orchids", "The Voyage of the Beagle (Mentor)", "Rejse om jorden", "The Voyage of the Beagle (Everyman Paperbacks)", "Beagle letters", "A student's introduction to Charles Darwin", "Charles Darwin's natural selection", "Darwin and Henslow", "The Darwin Reader Second Edition", "Fertilisation of Orchids", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The autobiography of Charles Darwin, 1809-1882", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Diario del Viaje de Un Naturalista Alrededor", "The Voyage of the Beagle (Great Minds Series)", "Insectivorous Plants", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "The Autobiography of Charles Darwin (Dodo Press)", "From So Simple a Beginning", "Voyage Of The Beagle", "The Expression of the Emotions in Man And Animals", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "Proiskhozhdenie vidov", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "The Correspondence of Charles Darwin, Volume 9: 1861", "The Autobiography of Charles Darwin (Large Print)", "Darwin for Today", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "The Correspondence of Charles Darwin, Volume 5", "Darwin's insects", "The expression of the emotions in man and animals", "La facult\u00e9 motrice dans les plantes", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The Autobiography of Charles Darwin [EasyRead Edition]", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Structure and Distribution of Coral Reefs", "The voyage of Charles Darwin", "The Correspondence of Charles Darwin, Volume 11", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "The Origin of Species (Enriched Classics)", "The Origin of Species (Variorum Reprint)", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "The Power of Movement in Plants", "The principal works", "Origins", "Works", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "The Autobiography of Charles Darwin", "The Voyage of the Beagle (Adventure Classics)", "The Autobiography Of Charles Darwin", "A Darwin Selection", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The descent of man, and selection in relation to sex", "The foundations of the Origin of species", "Darwin on humus and the earthworm", "Volcanic Islands", "Motsa ha-minim", "Darwin's Ornithological notes", "The Correspondence of Charles Darwin, Volume 13: 1865", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "Kleinere geologische Abhandlungen", "Charles Darwin's letters", "The Origin of Species (Great Books : Learning Channel)", "Opsht\u0323amung fun menshen", "The Descent of Man and Selection in Relation to Sex", "Metaphysics, Materialism, & the evolution of mind", "The Origin of Species (Great Minds Series)", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Reise eines Naturforschers um die Welt", "More Letters of Charles Darwin", "From so simple a beginning", "ontstaan der soorten door natuurlijke teeltkeus", "Die geschlechtliche Zuchtwahl", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "On Natural Selection", "Darwin en Patagonia", "H.M.S. Beagle in South America", "The structure and distribution of coral reefs", "On a remarkable bar of sandstone off Pernambuco", "monograph on the sub-class Cirripedia", "Darwin-Wallace", "El Origin De Las Especies", "Wu zhong qi yuan", "On the Movements and Habits of Climbing Plants", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "Evolution", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "The descent of man, and selection in relation to sex.", "The collected papers of Charles Darwin", "The Correspondence of Charles Darwin, Volume 18: 1870", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "red notebook of Charles Darwin", "The Expression of the Emotions in Man and Animals", "Voyage d'un naturaliste autour du monde", "Charles Darwin on the routes of male humble bees", "The Correspondence of Charles Darwin, Volume 6", "Memorias y epistolario i\u0301ntimo", "The Origin of Species (Oxford World's Classics)", "Human nature, Darwin's view", "The education of Darwin", "The Correspondence of Charles Darwin, Volume 7", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "The Essential Darwin", "Les mouvements et les habitudes des plantes grimpantes", "The Voyage of the Beagle (Unabridged Classics)", "The Correspondence of Charles Darwin, Volume 9", "The Variation of Animals and Plants under Domestication", "Voyage of the Beagle (NG Adventure Classics)", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "The Origin of Species", "Het uitdrukken van emoties bij mens en dier", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "The Expression Of The Emotions In Man And Animals"], "ans_acc": 0.08411214953271028, "ans_hit": 1, "ans_f1": 0.01840490797546012, "ans_precission": 0.6, "ans_recall": 0.009345794392523364, "path_f1": 0.4324324324324324, "path_precision": 1.0, "path_recall": 0.27586206896551724, "path_ans_f1": 0.15517241379310345, "path_ans_precision": 1.0, "path_ans_recall": 0.08411214953271028}
{"id": "WebQTest-1080", "prediction": ["# Reasoning Path:\nAbraham -> people.person.place_of_birth -> Mesopotamia -> location.location.people_born_here -> Ziryab\n# Answer:\nMesopotamia", "# Reasoning Path:\nAbraham -> people.person.place_of_birth -> Mesopotamia -> location.location.people_born_here -> Enheduanna\n# Answer:\nMesopotamia", "# Reasoning Path:\nAbraham -> people.person.place_of_birth -> Mesopotamia -> book.book_subject.works -> The Sacred Band\n# Answer:\nMesopotamia", "# Reasoning Path:\nAbraham -> people.person.place_of_birth -> Mesopotamia -> location.location.people_born_here -> Noah\n# Answer:\nMesopotamia", "# Reasoning Path:\nAbraham -> people.person.place_of_birth -> Mesopotamia -> location.location.events -> Anastasian War\n# Answer:\nMesopotamia", "# Reasoning Path:\nAbraham -> people.person.places_lived -> m.048b8qt -> people.place_lived.location -> Canaan\n# Answer:\nCanaan", "# Reasoning Path:\nAbraham -> people.person.place_of_birth -> Mesopotamia -> book.book_subject.works -> A History of the Ancient Near East\n# Answer:\nMesopotamia", "# Reasoning Path:\nAbraham -> people.person.place_of_birth -> Mesopotamia -> book.book_subject.works -> Ancient Iraq\n# Answer:\nMesopotamia", "# Reasoning Path:\nAbraham -> people.person.place_of_birth -> Mesopotamia -> location.location.events -> Antony's Parthian War\n# Answer:\nMesopotamia", "# Reasoning Path:\nAbraham -> people.person.places_lived -> m.0zh1zv9 -> people.place_lived.location -> Mesopotamia\n# Answer:\nMesopotamia"], "ground_truth": ["Mesopotamia", "Canaan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.6153846153846154, "path_precision": 0.8, "path_recall": 0.5, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1081", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.education -> m.0n1916z -> education.education.institution -> Simpson College\n# Answer:\nSimpson College", "# Reasoning Path:\nGeorge Washington Carver -> people.person.education -> m.0ztr7gt -> education.education.institution -> Iowa State University\n# Answer:\nIowa State University", "# Reasoning Path:\nGeorge Washington Carver -> people.person.education -> m.04hdfv4 -> education.education.institution -> Iowa State University\n# Answer:\nIowa State University", "# Reasoning Path:\nGeorge Washington Carver -> people.person.education -> m.0ztr7gt -> education.education.degree -> Master's Degree\n# Answer:\nMaster's Degree", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_for -> g.1255959wd\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.education -> m.0n1916z -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nGeorge Washington Carver -> people.person.education -> m.04hdfv4 -> education.education.degree -> Bachelor of Science\n# Answer:\nBachelor of Science", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> astronomy.extraterrestrial_location.on_celestial_object -> Moon\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.education -> m.0n1916z -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_types -> Extraterrestrial location\n# Answer:\nCarver"], "ground_truth": ["Simpson College", "Iowa State University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1082", "prediction": ["# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Baloncesto M\u00e1laga -> sports.sports_team.sport -> Basketball\n# Answer:\nBaloncesto M\u00e1laga", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Baloncesto M\u00e1laga -> sports.sports_team.location -> M\u00e1laga\n# Answer:\nBaloncesto M\u00e1laga", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Valencia BC -> sports.sports_team.sport -> Basketball\n# Answer:\nValencia BC", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Baloncesto M\u00e1laga -> common.topic.notable_types -> Basketball Team\n# Answer:\nBaloncesto M\u00e1laga", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> CB Sevilla -> sports.sports_team.sport -> Basketball\n# Answer:\nCB Sevilla", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Valencia BC -> common.topic.notable_types -> Basketball Team\n# Answer:\nValencia BC", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60xs9d2\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Valencia BC -> sports.sports_team.location -> Valencia\n# Answer:\nValencia BC", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> CB Sevilla -> sports.sports_team.location -> Seville\n# Answer:\nCB Sevilla"], "ground_truth": ["Spain women's national volleyball team", "CB 1939 Canarias", "CB Murcia", "Ros Casares Valencia", "San Sebasti\u00e1n Gipuzkoa BC", "CB Sevilla", "Spain women's national handball team", "Baloncesto M\u00e1laga", "Spain national handball team", "Valencia BC", "Joventut Badalona", "CB Estudiantes", "B\u00e0squet Manresa", "CB Avenida", "Obradoiro CAB", "Spain national football team", "Spain men's national volleyball team", "CB Breog\u00e1n", "Spain national beach handball team", "Saski Baskonia", "Real Madrid Baloncesto", "CB Sant Josep", "Basket Zaragoza 2002"], "ans_acc": 0.13043478260869565, "ans_hit": 1, "ans_f1": 0.22429906542056074, "ans_precission": 0.8, "ans_recall": 0.13043478260869565, "path_f1": 0.22429906542056074, "path_precision": 0.8, "path_recall": 0.13043478260869565, "path_ans_f1": 0.22429906542056074, "path_ans_precision": 0.8, "path_ans_recall": 0.13043478260869565}
{"id": "WebQTest-1083", "prediction": ["# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> location.location.containedby -> United States of America\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> base.biblioness.bibs_location.state -> Hawaii\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> location.location.containedby -> Hawaii\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> Yes We Can: A Biography of President Barack Obama -> book.written_work.subjects -> United States of America\n# Answer:\nYes We Can: A Biography of President Barack Obama", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Writer's Journey to Selma, Alabama -> book.written_work.subjects -> Selma\n# Answer:\nA Writer's Journey to Selma, Alabama", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> Yes We Can: A Biography of President Barack Obama -> book.written_work.subjects -> History of the United States\n# Answer:\nYes We Can: A Biography of President Barack Obama", "# Reasoning Path:\nBarack Obama -> people.person.ethnicity -> English American -> people.ethnicity.geographic_distribution -> California\n# Answer:\nEnglish American", "# Reasoning Path:\nBarack Obama -> people.person.ethnicity -> Irish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nIrish American", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> Yes We Can: A Biography of President Barack Obama -> common.topic.notable_types -> Book\n# Answer:\nYes We Can: A Biography of President Barack Obama"], "ground_truth": ["Honolulu"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1084", "prediction": ["# Reasoning Path:\nPoland -> sports.sports_team_location.teams -> Basket Zielona G\u00f3ra -> sports.sports_team.sport -> Basketball\n# Answer:\nBasket Zielona G\u00f3ra", "# Reasoning Path:\nPoland -> sports.sports_team_location.teams -> Basket Zielona G\u00f3ra -> sports.sports_team.colors -> White\n# Answer:\nBasket Zielona G\u00f3ra", "# Reasoning Path:\nPoland -> sports.sports_team_location.teams -> Basket Zielona G\u00f3ra -> common.topic.notable_types -> Basketball Team\n# Answer:\nBasket Zielona G\u00f3ra", "# Reasoning Path:\nPoland -> sports.sports_team_location.teams -> GT\u017b Grudzi\u0105dz -> sports.sports_team.sport -> Motorcycle speedway\n# Answer:\nGT\u017b Grudzi\u0105dz", "# Reasoning Path:\nPoland -> location.statistical_region.size_of_armed_forces -> g.12cp_j3lx\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nPoland -> sports.sports_team_location.teams -> Asseco Gdynia -> common.topic.notable_types -> Basketball Team\n# Answer:\nAsseco Gdynia", "# Reasoning Path:\nPoland -> sports.sports_team_location.teams -> Asseco Gdynia -> sports.sports_team.sport -> Basketball\n# Answer:\nAsseco Gdynia", "# Reasoning Path:\nPoland -> location.statistical_region.size_of_armed_forces -> g.1hhc385gk\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nPoland -> sports.sports_team_location.teams -> GT\u017b Grudzi\u0105dz -> sports.sports_team.colors -> Blue\n# Answer:\nGT\u017b Grudzi\u0105dz", "# Reasoning Path:\nPoland -> location.statistical_region.size_of_armed_forces -> g.1hhc39krm\n# Answer:\nlocation.statistical_region.size_of_armed_forces"], "ground_truth": ["Asseco Gdynia", "Poland national football team", "Stal Gorz\u00f3w Wielkopolski", "Basket Zielona G\u00f3ra", "Poland men's national volleyball team", "KK W\u0142oc\u0142awek", "\u015al\u0105sk Wroc\u0142aw", "Poland women's national handball team", "Poland national handball team", "GT\u017b Grudzi\u0105dz", "Tur\u00f3w Zgorzelec", "Poland women's national volleyball team"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.3684210526315789, "ans_precission": 0.7, "ans_recall": 0.25, "path_f1": 0.3684210526315789, "path_precision": 0.7, "path_recall": 0.25, "path_ans_f1": 0.3684210526315789, "path_ans_precision": 0.7, "path_ans_recall": 0.25}
{"id": "WebQTest-1085", "prediction": ["# Reasoning Path:\nDemi Lovato -> music.composer.compositions -> Behind Enemy Lines -> music.composition.composer -> Joe Jonas\n# Answer:\nBehind Enemy Lines", "# Reasoning Path:\nDemi Lovato -> base.popstra.celebrity.dated -> m.065q3fy -> base.popstra.dated.participant -> Joe Jonas\n# Answer:\nJoe Jonas", "# Reasoning Path:\nDemi Lovato -> music.composer.compositions -> Don't Forget -> music.composition.composer -> Joe Jonas\n# Answer:\nDon't Forget", "# Reasoning Path:\nDemi Lovato -> music.artist.album -> Demi -> common.topic.article -> m.0sghhtd\n# Answer:\nDemi", "# Reasoning Path:\nDemi Lovato -> music.artist.album -> Don't Forget -> music.composition.composer -> Joe Jonas\n# Answer:\nDon't Forget", "# Reasoning Path:\nDemi Lovato -> base.popstra.celebrity.dated -> m.063gd6q -> base.popstra.dated.participant -> Cody Linley\n# Answer:\nCody Linley", "# Reasoning Path:\nDemi Lovato -> music.composer.compositions -> Behind Enemy Lines -> music.composition.composer -> Kevin Jonas\n# Answer:\nBehind Enemy Lines", "# Reasoning Path:\nDemi Lovato -> music.composer.compositions -> Behind Enemy Lines -> music.composition.language -> English Language\n# Answer:\nBehind Enemy Lines", "# Reasoning Path:\nDemi Lovato -> base.popstra.celebrity.dated -> m.064cvl8 -> base.popstra.dated.participant -> Nicholas Braun\n# Answer:\nNicholas Braun", "# Reasoning Path:\nDemi Lovato -> music.composer.compositions -> Behind Enemy Lines -> music.composition.composer -> Nick Jonas\n# Answer:\nBehind Enemy Lines"], "ground_truth": ["Nicholas Braun", "Joe Jonas", "Wilmer Valderrama", "Cody Linley"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.4285714285714285, "ans_precission": 0.3, "ans_recall": 0.75, "path_f1": 0.37037037037037035, "path_precision": 0.5, "path_recall": 0.29411764705882354, "path_ans_f1": 0.6666666666666665, "path_ans_precision": 0.6, "path_ans_recall": 0.75}
{"id": "WebQTest-1087", "prediction": ["# Reasoning Path:\nScarlett Johansson -> award.award_nominee.award_nominations -> m.0z83g54 -> award.award_nomination.nominated_for -> We Bought a Zoo\n# Answer:\nWe Bought a Zoo", "# Reasoning Path:\nScarlett Johansson -> award.award_nominee.award_nominations -> m.0z9m97f -> award.award_nomination.nominated_for -> The Black Dahlia\n# Answer:\nThe Black Dahlia", "# Reasoning Path:\nScarlett Johansson -> award.award_nominee.award_nominations -> m.0gm4_rg -> award.award_nomination.nominated_for -> The Perfect Score\n# Answer:\nThe Perfect Score", "# Reasoning Path:\nScarlett Johansson -> award.award_nominee.award_nominations -> m.0z83g54 -> award.award_nomination.ceremony -> 2012 Teen Choice Awards\n# Answer:\n2012 Teen Choice Awards", "# Reasoning Path:\nScarlett Johansson -> award.award_nominee.award_nominations -> m.0gm4_rg -> award.award_nomination.nominated_for -> Lost in Translation\n# Answer:\nLost in Translation", "# Reasoning Path:\nScarlett Johansson -> award.award_nominee.award_nominations -> m.0z9m97f -> award.award_nomination.nominated_for -> The Prestige\n# Answer:\nThe Prestige", "# Reasoning Path:\nScarlett Johansson -> award.award_nominee.award_nominations -> m.0z9m97f -> award.award_nomination.ceremony -> 2007 Teen Choice Awards\n# Answer:\n2007 Teen Choice Awards", "# Reasoning Path:\nScarlett Johansson -> award.award_nominee.award_nominations -> m.0gm4_rg -> award.award_nomination.award -> Teen Choice Award for Choice Movie Breakout Star - Female\n# Answer:\nTeen Choice Award for Choice Movie Breakout Star - Female", "# Reasoning Path:\nScarlett Johansson -> award.award_nominee.award_nominations -> m.0gm4_rg -> award.award_nomination.ceremony -> 2004 Teen Choice Awards\n# Answer:\n2004 Teen Choice Awards", "# Reasoning Path:\nScarlett Johansson -> tv.tv_actor.guest_roles -> m.0bvtz__ -> tv.tv_guest_role.episodes_appeared_in -> Ryan Reynolds / Lady Gaga\n# Answer:\nRyan Reynolds / Lady Gaga"], "ground_truth": ["Ghost World", "Scoop", "Her", "Don Jon", "A Good Woman", "A Love Song for Bobby Long", "Captain America: The Winter Soldier", "The Horse Whisperer", "North", "We Bought a Zoo", "Home Alone 3", "Under the Skin", "Captain America: Civil War", "Iron Man 2", "The Perfect Score", "Lucy", "An American Rhapsody", "The Island", "Hitchcock", "He's Just Not That Into You", "If Lucy Fell", "Manny & Lo", "My Brother the Pig", "Fall", "Vicky Cristina Barcelona", "Match Point", "The Avengers", "Lost in Translation", "The Spirit", "The Black Dahlia", "The SpongeBob SquarePants Movie", "Buck", "Girl with a Pearl Earring", "The Avengers: Age of Ultron", "The Nanny Diaries", "Chef", "The Other Boleyn Girl", "In Good Company", "The Jungle Book", "Just Cause", "Eight Legged Freaks", "The Prestige", "The Man Who Wasn't There"], "ans_acc": 0.11627906976744186, "ans_hit": 1, "ans_f1": 0.18867924528301888, "ans_precission": 0.5, "ans_recall": 0.11627906976744186, "path_f1": 0.09345794392523363, "path_precision": 0.5, "path_recall": 0.05154639175257732, "path_ans_f1": 0.18867924528301888, "path_ans_precision": 0.5, "path_ans_recall": 0.11627906976744186}
{"id": "WebQTest-1088", "prediction": ["# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.0k6pxpv -> tv.regular_tv_appearance.actor -> Lacey Chabert\n# Answer:\nLacey Chabert", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.02ntq_4 -> tv.regular_tv_appearance.actor -> Mila Kunis\n# Answer:\nMila Kunis", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.0k6pxpv -> tv.regular_tv_appearance.character -> Meg Griffin\n# Answer:\nMeg Griffin", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.010btxt8 -> tv.regular_tv_appearance.actor -> Brandon Richardson\n# Answer:\nBrandon Richardson", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.02ntq_4 -> tv.regular_tv_appearance.seasons -> Family Guy - Season 9\n# Answer:\nFamily Guy - Season 9", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.02ntq_4 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nFamily Guy -> common.topic.webpage -> m.02sfx_3 -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.010btxt8 -> tv.regular_tv_appearance.seasons -> Family Guy - Season 12\n# Answer:\nFamily Guy - Season 12", "# Reasoning Path:\nFamily Guy -> tv.tv_program.seasons -> Family Guy - Season 0 -> tv.tv_series_season.episodes -> 200 Episodes Later\n# Answer:\nFamily Guy - Season 0", "# Reasoning Path:\nFamily Guy -> tv.tv_program.seasons -> Family Guy - Season 0 -> common.topic.notable_types -> TV Season\n# Answer:\nFamily Guy - Season 0"], "ground_truth": ["Lacey Chabert"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1089", "prediction": ["# Reasoning Path:\nBad Teacher -> film.film.starring -> m.0b798jt -> film.performance.actor -> Lucy Punch\n# Answer:\nLucy Punch", "# Reasoning Path:\nBad Teacher -> film.film.starring -> m.0b798j0 -> film.performance.actor -> Jason Segel\n# Answer:\nJason Segel", "# Reasoning Path:\nBad Teacher -> film.film.starring -> m.0b798jt -> film.performance.character -> Amy Squirrel\n# Answer:\nAmy Squirrel", "# Reasoning Path:\nBad Teacher -> film.film.starring -> m.0b798j5 -> film.performance.actor -> Cameron Diaz\n# Answer:\nCameron Diaz", "# Reasoning Path:\nBad Teacher -> film.film.starring -> m.0b798j0 -> film.performance.character -> Russell Gettis\n# Answer:\nRussell Gettis", "# Reasoning Path:\nBad Teacher -> film.film.other_crew -> m.0glzm9q -> film.film_crew_gig.crewmember -> Laurene Alvarado\n# Answer:\nLaurene Alvarado", "# Reasoning Path:\nBad Teacher -> film.film.other_crew -> m.0glzm9v -> film.film_crew_gig.film_crew_role -> Key Hair Stylist\n# Answer:\nKey Hair Stylist", "# Reasoning Path:\nBad Teacher -> film.film.starring -> m.0b798j5 -> film.performance.character -> Elizabeth Halsey\n# Answer:\nElizabeth Halsey", "# Reasoning Path:\nBad Teacher -> film.film.other_crew -> m.0glzm9q -> film.film_crew_gig.film_crew_role -> Makeup Artist\n# Answer:\nMakeup Artist", "# Reasoning Path:\nBad Teacher -> film.film.other_crew -> m.0glzmb2 -> film.film_crew_gig.film_crew_role -> Key Makeup Artist\n# Answer:\nKey Makeup Artist"], "ground_truth": ["Lucy Punch"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0vxk_df -> sports.sports_team_roster.team -> New England Patriots\n# Answer:\nNew England Patriots", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.team -> New York Jets\n# Answer:\nNew York Jets", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0vxk_df -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> book.written_work.author -> Nathan Whitaker\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> people.person.education -> m.05v4mnf -> education.education.institution -> University of Florida\n# Answer:\nUniversity of Florida", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.notable_types -> Book\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nTim Tebow -> people.person.education -> m.05v4mnf -> education.education.major_field_of_study -> Family, Youth and Community Sciences\n# Answer:\nFamily, Youth and Community Sciences"], "ground_truth": ["Florida Gators football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1090", "prediction": ["# Reasoning Path:\nBoeing -> organization.organization.headquarters -> m.02_6pm_ -> location.mailing_address.citytown -> Chicago\n# Answer:\nChicago", "# Reasoning Path:\nBoeing -> organization.organization.headquarters -> m.02_6pm_ -> location.mailing_address.state_province_region -> Illinois\n# Answer:\nIllinois", "# Reasoning Path:\nBoeing -> organization.organization.place_founded -> Seattle -> location.location.containedby -> United States of America\n# Answer:\nSeattle", "# Reasoning Path:\nBoeing -> organization.organization.headquarters -> m.02_6pm_ -> freebase.valuenotation.has_no_value -> Street Address 2\n# Answer:\nStreet Address 2", "# Reasoning Path:\nBoeing -> organization.organization.place_founded -> Seattle -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nSeattle", "# Reasoning Path:\nBoeing -> business.business_operation.current_liabilities -> m.090887v -> measurement_unit.dated_money_value.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nBoeing -> organization.organization.headquarters -> m.02_6pm_ -> freebase.valuenotation.has_no_value -> Street Address 3\n# Answer:\nStreet Address 3", "# Reasoning Path:\nBoeing -> organization.organization.headquarters -> m.02_6pm_ -> freebase.valuenotation.has_no_value -> Street Address 4\n# Answer:\nStreet Address 4"], "ground_truth": ["Chicago"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1091", "prediction": ["# Reasoning Path:\nUnited States of America -> location.statistical_region.religions -> m.03q1lvq\n# Answer:\nlocation.statistical_region.religions", "# Reasoning Path:\nUnited States of America -> location.statistical_region.religions -> m.03q1lvh\n# Answer:\nlocation.statistical_region.religions", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Upper Midwest American English -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nUpper Midwest American English", "# Reasoning Path:\nUnited States of America -> location.statistical_region.religions -> m.03q1lvy\n# Answer:\nlocation.statistical_region.religions", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Abenaki language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nAbenaki language", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Abenaki language -> language.human_language.region -> Americas\n# Answer:\nAbenaki language", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Abenaki language -> language.human_language.main_country -> Canada\n# Answer:\nAbenaki language", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Guyanese Creole English Language -> language.human_language.region -> Americas\n# Answer:\nGuyanese Creole English Language"], "ground_truth": ["Christianity", "Judaism", "Buddhism", "Atheism", "Unitarian Universalism", "Hinduism", "Islam"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1092", "prediction": ["# Reasoning Path:\nViggo Mortensen -> film.actor.film -> m.0k5s9k -> film.performance.character -> Aragorn\n# Answer:\nAragorn", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> film.film.starring -> m.0k5s9q -> film.performance.character -> Arwen\n# Answer:\nArwen", "# Reasoning Path:\nViggo Mortensen -> film.actor.film -> g.11b6bnm19y\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nViggo Mortensen -> people.person.profession -> Actor\n# Answer:\nActor", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> film.film.starring -> m.0k5s9k -> film.performance.character -> Aragorn\n# Answer:\nAragorn", "# Reasoning Path:\nViggo Mortensen -> film.actor.film -> g.11b6ntbrrh\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nViggo Mortensen -> film.actor.film -> m.0k5s9k -> film.performance.film -> The Lord of the Rings: The Fellowship of the Ring\n# Answer:\nfilm.performance.film", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> film.film.starring -> m.02vc8t6 -> film.performance.character -> Old Bilbo\n# Answer:\nOld Bilbo", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> film.film.starring -> m.02vbl_t -> film.performance.character -> Gollum\n# Answer:\nGollum", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> film.film.starring -> m.0k5s9q -> film.performance.actor -> Liv Tyler\n# Answer:\nLiv Tyler"], "ground_truth": ["Aragorn"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.1818181818181818, "path_precision": 0.2, "path_recall": 0.16666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1094", "prediction": ["# Reasoning Path:\nIsaac Newton -> people.person.profession -> Chemist -> base.descriptive_names.names.descriptive_name -> m.01066g6n\n# Answer:\nChemist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Chemist -> base.schemastaging.context_name.pronunciation -> g.125_m09_2\n# Answer:\nChemist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Philosopher -> people.profession.specializations -> Logician\n# Answer:\nPhilosopher", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Chemist -> common.topic.notable_types -> Profession\n# Answer:\nChemist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Scientist -> common.topic.notable_types -> Profession\n# Answer:\nScientist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Chemist -> base.descriptive_names.names.descriptive_name -> m.0106_ymb\n# Answer:\nChemist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Philosopher -> fictional_universe.character_rank.characters_of_this_rank -> Heraclitus\n# Answer:\nPhilosopher", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Chemist -> base.descriptive_names.names.descriptive_name -> m.010f2qwv\n# Answer:\nChemist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Philosopher -> common.topic.notable_types -> Profession\n# Answer:\nPhilosopher", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Scientist -> common.topic.subject_of -> Biography\n# Answer:\nScientist"], "ground_truth": ["Astronomer", "Philosopher", "Chemist", "Scientist", "Physicist", "Mathematician"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-1095", "prediction": ["# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Judith Quiney -> people.person.parents -> Anne Hathaway\n# Answer:\nJudith Quiney", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Hamnet Shakespeare -> people.person.parents -> Anne Hathaway\n# Answer:\nHamnet Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.spouse_s -> m.02wtqtm -> people.marriage.spouse -> Anne Hathaway\n# Answer:\nAnne Hathaway", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Judith Quiney -> people.person.nationality -> United Kingdom\n# Answer:\nJudith Quiney", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Susanna Hall -> people.person.place_of_birth -> Stratford-upon-Avon\n# Answer:\nSusanna Hall", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Judith Quiney -> freebase.valuenotation.has_value -> Cause of death\n# Answer:\nJudith Quiney", "# Reasoning Path:\nWilliam Shakespeare -> people.person.spouse_s -> m.02wtqtm -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nWilliam Shakespeare -> people.person.spouse_s -> m.02wtqtm -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Judith Quiney -> freebase.valuenotation.has_value -> Place of death\n# Answer:\nJudith Quiney", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Hamnet Shakespeare -> freebase.valuenotation.has_value -> Place of death\n# Answer:\nHamnet Shakespeare"], "ground_truth": ["Anne Hathaway"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1096", "prediction": ["# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0z9rhy8 -> award.award_nomination.nominated_for -> Last Holiday\n# Answer:\nLast Holiday", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0b3t_h2 -> award.award_nomination.nominated_for -> Chicago\n# Answer:\nChicago", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0x0zxs3 -> award.award_nomination.nominated_for -> Chicago\n# Answer:\nChicago", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0z9rhy8 -> award.award_nomination.ceremony -> 2006 Teen Choice Awards\n# Answer:\n2006 Teen Choice Awards", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0z9rhy8 -> award.award_nomination.award_nominee -> LL Cool J\n# Answer:\nLL Cool J", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0b3t_h2 -> award.award_nomination.ceremony -> 2003 MTV Movie Awards\n# Answer:\n2003 MTV Movie Awards", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0x0zxs3 -> award.award_nomination.nominated_for -> Bringing Down the House\n# Answer:\nBringing Down the House", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0x0zxs3 -> award.award_nomination.nominated_for -> Brown Sugar\n# Answer:\nBrown Sugar", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0x0zxs3 -> award.award_nomination.award -> BET Award for Best Actress\n# Answer:\nBET Award for Best Actress", "# Reasoning Path:\nQueen Latifah -> award.award_winner.awards_won -> m.05bl1y_ -> award.award_honor.honored_for -> U.N.I.T.Y.\n# Answer:\nU.N.I.T.Y."], "ground_truth": ["Bessie", "Living with the Dead", "We Are Family", "Kung Faux: Vol. 4", "Kung Faux: Vol. 3", "One Love: The Bob Marley All-Star Tribute", "Stranger than Fiction", "Brown Sugar", "The Making of What's Going On: An All-Star Tribute", "House Party 2", "Kung Faux: Vol. 5", "Mama Flora's Family", "Mad Money", "What Happens in Vegas", "Bringing Down the House", "Zac Efron's Pool Party", "Ice Age: Dawn of the Dinosaurs", "Scary Movie 3", "Barbershop 3", "The Country Bears", "Hairspray", "The Muppets' Wizard of Oz", "Ice Age: A Mammoth Christmas", "Hairspray 2: White Lipstick", "Beauty Shop", "Ice Age: Continental Drift", "Jungle Fever", "We Are One: The Obama Inaugural Celebration at the Lincoln Memorial", "Sphere", "My Life", "Bringing Out the Dead", "Set It Off", "Life in the 310: Vol. 3", "22 Jump Street", "Last Holiday", "Barbershop 2: Back in Business", "Chicago", "The Secret Life of Bees", "Juice", "Joyful Noise", "Living Out Loud", "Arctic Tale", "Kung Faux: Vol. 1", "Valentine's Day", "Life Support", "Through the Years of Hip-Hop: Vol. 1: Graffiti", "Hoodlum", "The Dilemma", "Queen Latifah - Unauthorized", "Kung Faux: Vol. 2", "Ice Age: The Meltdown", "Taxi", "Just Wright", "Steel Magnolias", "The Bone Collector", "The Perfect Holiday", "Breaking Out: The Alcatraz Concert", "g.11bc95kg2q", "The Cookout"], "ans_acc": 0.06779661016949153, "ans_hit": 1, "ans_f1": 0.11940298507462686, "ans_precission": 0.5, "ans_recall": 0.06779661016949153, "path_f1": 0.07207207207207207, "path_precision": 0.4, "path_recall": 0.039603960396039604, "path_ans_f1": 0.11940298507462686, "path_ans_precision": 0.5, "path_ans_recall": 0.06779661016949153}
{"id": "WebQTest-1097", "prediction": ["# Reasoning Path:\nMichelle Obama -> people.person.education -> m.02wn0g2 -> education.education.institution -> Princeton University\n# Answer:\nPrinceton University", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.0n1db15 -> education.education.institution -> Whitney M. Young Magnet High School\n# Answer:\nWhitney M. Young Magnet High School", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.02wn0g2 -> education.education.major_field_of_study -> Sociology\n# Answer:\nSociology", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.02wpt7p -> education.education.institution -> Harvard Law School\n# Answer:\nHarvard Law School", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.02wn0g2 -> freebase.valuenotation.has_value -> Specialization\n# Answer:\nSpecialization", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.02wn0g2 -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.0n1db15 -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.0n1db15 -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nMichelle Obama -> common.topic.webpage -> m.09sfllx -> common.webpage.resource -> m.0bk40pl\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nMichelle Obama -> common.topic.webpage -> m.09w8vkr -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Whitney M. Young Magnet High School", "Harvard Law School", "Princeton University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1098", "prediction": ["# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Classical music -> radio.radio_subject.programs_with_this_subject -> The Collector\u2019s Shelf\n# Answer:\nClassical music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Romantic music -> music.genre.parent_genre -> Classical music\n# Answer:\nRomantic music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.compositional_form.superforms -> Vocal music\n# Answer:\nArt song", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Classical music -> radio.radio_subject.programs_with_this_subject -> Adventures in Good Music\n# Answer:\nClassical music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Classical music -> broadcast.genre.content -> 1.FM Otto's Opera House\n# Answer:\nClassical music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Romantic music -> music.genre.artists -> Sergei Rachmaninoff\n# Answer:\nRomantic music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Classical music -> radio.radio_subject.programs_with_this_subject -> Music 'Til Dawn\n# Answer:\nClassical music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Classical music -> book.book_subject.works -> Franz Liszt, Vol. 1: The Virtuoso Years, 1811-1847\n# Answer:\nClassical music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Romantic music -> common.topic.notable_types -> Musical genre\n# Answer:\nRomantic music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Classical music -> broadcast.genre.content -> Adventures in Good Music\n# Answer:\nClassical music"], "ground_truth": ["Opera", "Classical music", "Art song", "Romantic music", "Chamber music"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 1.0, "ans_recall": 0.6, "path_f1": 0.7499999999999999, "path_precision": 1.0, "path_recall": 0.6, "path_ans_f1": 0.888888888888889, "path_ans_precision": 1.0, "path_ans_recall": 0.8}
{"id": "WebQTest-1099", "prediction": ["# Reasoning Path:\nDavid Luiz -> people.person.nationality -> Brazil -> sports.sports_team_location.teams -> Brazil national football team\n# Answer:\nBrazil", "# Reasoning Path:\nDavid Luiz -> soccer.football_player.statistics -> m.0w8zrhb -> soccer.football_player_stats.team -> S.L. Benfica\n# Answer:\nS.L. Benfica", "# Reasoning Path:\nDavid Luiz -> soccer.football_player.statistics -> m.0w9d9nx -> soccer.football_player_stats.team -> Brazil national football team\n# Answer:\nBrazil national football team", "# Reasoning Path:\nDavid Luiz -> people.person.nationality -> Brazil -> base.aareas.schema.administrative_area.administrative_children -> Amazonas\n# Answer:\nBrazil", "# Reasoning Path:\nDavid Luiz -> soccer.football_player.statistics -> m.0w9dq88 -> soccer.football_player_stats.team -> Chelsea F.C.\n# Answer:\nChelsea F.C.", "# Reasoning Path:\nDavid Luiz -> sports.pro_athlete.teams -> m.010p28b1 -> sports.sports_team_roster.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Luiz -> people.person.nationality -> Brazil -> base.aareas.schema.administrative_area.administrative_children -> Acre\n# Answer:\nBrazil", "# Reasoning Path:\nDavid Luiz -> people.person.nationality -> Brazil -> location.country.first_level_divisions -> Amazonas\n# Answer:\nBrazil", "# Reasoning Path:\nDavid Luiz -> people.person.nationality -> Brazil -> base.aareas.schema.administrative_area.administrative_children -> Alagoas\n# Answer:\nBrazil", "# Reasoning Path:\nDavid Luiz -> people.person.nationality -> Brazil -> location.country.first_level_divisions -> Acre\n# Answer:\nBrazil"], "ground_truth": ["Brazil national football team"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.09tcfsk -> american_football.player_rushing_statistics.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.09tcfsk -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.0791773 -> american_football.player_rushing_statistics.season -> 2008 NFL season\n# Answer:\n2008 NFL season", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.07mmh5w -> american_football.player_rushing_statistics.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.0791773 -> american_football.player_rushing_statistics.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts"], "ground_truth": ["Denver Broncos"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1100", "prediction": ["# Reasoning Path:\nThomas Jefferson -> book.author.book_editions_published -> The Jefferson Bible -> book.book_edition.book -> Jefferson Bible\n# Answer:\nThe Jefferson Bible", "# Reasoning Path:\nThomas Jefferson -> book.author.book_editions_published -> A Manual of Parliamentary Practice Composed for the Use of the Senate of the United States -> book.book_edition.book -> A Manual of Parliamentary Practice for the Use of the Senate of the United States\n# Answer:\nA Manual of Parliamentary Practice Composed for the Use of the Senate of the United States", "# Reasoning Path:\nThomas Jefferson -> book.author.book_editions_published -> The Jefferson Bible -> common.topic.notable_types -> Book Edition\n# Answer:\nThe Jefferson Bible", "# Reasoning Path:\nThomas Jefferson -> book.author.book_editions_published -> A manual of parliamentary practice for the use of the Senate of the United States -> book.book_edition.book -> A Manual of Parliamentary Practice for the Use of the Senate of the United States\n# Answer:\nA manual of parliamentary practice for the use of the Senate of the United States", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Polly Jefferson\n# Answer:\nPolly Jefferson", "# Reasoning Path:\nThomas Jefferson -> book.author.book_editions_published -> A Manual of Parliamentary Practice Composed for the Use of the Senate of the United States -> book.book_edition.isbn -> 9781417905218\n# Answer:\nA Manual of Parliamentary Practice Composed for the Use of the Senate of the United States", "# Reasoning Path:\nThomas Jefferson -> book.author.book_editions_published -> A Manual of Parliamentary Practice Composed for the Use of the Senate of the United States -> common.topic.notable_for -> g.1255cyj5v\n# Answer:\nA Manual of Parliamentary Practice Composed for the Use of the Senate of the United States", "# Reasoning Path:\nThomas Jefferson -> book.author.book_editions_published -> A manual of parliamentary practice for the use of the Senate of the United States -> book.book_edition.isbn -> 9781557092021\n# Answer:\nA manual of parliamentary practice for the use of the Senate of the United States", "# Reasoning Path:\nThomas Jefferson -> influence.influence_node.influenced -> Ayn Rand -> influence.influence_node.influenced_by -> Ludwig von Mises\n# Answer:\nAyn Rand"], "ground_truth": ["The wisdom of Thomas Jefferson", "Thomas Jefferson Travels", "Account of Louisiana", "The Papers of Thomas Jefferson, Volume 31: 1 February 1799 to 31 May 1800", "The living thoughts of Thomas Jefferson", "The Papers of Thomas Jefferson, Volume 32: 1 June 1800 to 16 February 1801", "The life and letters of Thomas Jefferson", "The Papers of Thomas Jefferson, Volume 34: 1 May to 31 July 1801", "The family letters of Thomas Jefferson", "The complete Jefferson", "The inaugural speeches and messages of Thomas Jefferson, Esq", "Thomas Jefferson, his words and vision", "The Papers of Thomas Jefferson, Volume 5: February 1781 to May 1781", "The Papers of Thomas Jefferson, Volume 17: July 1790 to November 1790", "The quotable Jefferson", "The Papers of Thomas Jefferson, Volume 8: February 1785 to October 1785", "State of the Union Addresses of Thomas Jefferson", "Jefferson and Madison on the Separation of Church and State", "The Papers of Thomas Jefferson, Volume 22: 6 August to 31 December 1791", "Master thoughts of Thomas Jefferson", "The Papers of Thomas Jefferson, Retirement Series: Volume 4: 18 June 1811 to 30 April 1812", "The Papers of Thomas Jefferson, Volume 14: October 1788 to March 1789", "The Papers of Thomas Jefferson, Volume 2: January 1777 to 18 June 1779", "A supplementary note on the mould board described in a letter to Sir John Sinclair, of March 23, 1798", "Il pensiero politico e sociale di Thomas Jefferson", "Manual de pra\u0301ctica parlamentaria", "The Papers of Thomas Jefferson, Volume 23: 1 January to 31 May 1792", "The address of Thomas Jefferson", "The Papers of Thomas Jefferson, Volume 36: 1 December 1801 to 3 March 1802", "The life & morals of Jesus Christ of Nazareth", "The Papers of Thomas Jefferson, Volume 16: November 1789 to July 1790", "Responsibility Skills", "Speech of Thomas Jefferson, president of the United States, delivered at his inauguration, March 4, 1801", "Jefferson on Jefferson", "The anas of Thomas Jefferson", "The Papers of Thomas Jefferson, Volume 7: March 1784 to February 1785", "Letters of Thomas Jefferson", "The Papers of Thomas Jefferson, Volume 9: November 1785 to June 1786", "Unpublished correspondence between Thomas Jefferson and some American Jews", "Thomas Jefferson correspondence", "The writings of Thomas Jefferson", "Catalogue", "To the girls and boys", "The best letters of Thomas Jefferson", "The Papers of Thomas Jefferson, Volume 21: Index, Vols. 1-20", "Jefferson's parliamentary writings", "The President's speech", "Jefferson's literary commonplace book", "Memoir, correspondence, and miscellanies", "The Papers of Thomas Jefferson, Volume 10: June 1786 to December 1786", "Speech of Thomas Jefferson, president of the United States", "Notes on the State of Virginia", "The Papers of Thomas Jefferson, Volume 33: 17 February to 30 April 1801", "The proceedings of the government of the United States, in maintaining the public right to the beach of the Missisipi", "Autobiography of Thomas Jefferson", "Jefferson's ideas on a university library", "An appendix to the Notes on Virginia relative to the murder of Logan's family", "The Papers of Thomas Jefferson, Volume 3: June 1779 to September 1780", "The inaugural addresses of President Thomas Jefferson, 1801 and 1805", "The essence of Jefferson", "Jefferson in love", "The speech of Logan", "Jefferson himself", "The Papers of Thomas Jefferson, Volume 28: 1 January 1794 to 29 February 1796", "The Jefferson-Dunglison letters", "The Papers of Thomas Jefferson, Volume 29: 1 March 1796 to 31 December 1797", "The Papers of Thomas Jefferson, Volume 11: January 1787 to August 1787", "The Commonplace book of Thomas Jefferson", "The Papers of Thomas Jefferson, Volume 27: 1 September to 31 December 1793", "The Statute of Virginia for Religious Freedom", "Jeffersonian principles", "The Papers of Thomas Jefferson, Volume 30: 1 January 1798 to 31 January 1799", "The Papers of Thomas Jefferson, Volume 1: 14 January 1760 to 6 December 1776", "A Manual of Parliamentary Practice for the Use of the Senate of the United States", "Documents Relating To The Purchase And Exploration Of Louisiana", "A Jefferson profile as revealed in his letters", "The Papers of Thomas Jefferson, Retirement Series: Volume 5: 1 May 1812 to 10 March 1813", "The republic of letters", "The political writings of Thomas Jefferson", "John Dewey presents the living thoughts of Thomas Jefferson", "Basic writings of Thomas Jefferson", "The papers of Thomas Jefferson. Index", "Light and liberty", "The Papers of Thomas Jefferson, Volume 12: August 1787 to March 1788", "The Papers of Thomas Jefferson, Volume 4: October 1780 to February 1781", "Thomas Jefferson's architectural drawings", "Minor Vocabularies of Nanticoke-Conoy", "Calendar of the correspondence of Thomas Jefferson", "Foundations of Freedom", "United States Declaration of Independence", "The Papers of Thomas Jefferson, Volume 19: January 1791 to March 1791", "Papers", "Jefferson Bible", "The life and selected writings of Thomas Jefferson", "Memorandums taken on a journey from Paris into the southern parts of France and Northern Italy, in the year 1787", "Speech of Thomas Jefferson, president of the United States, delivered at his instalment, March 4, 1801, at the city of Washington", "An American Christian Bible", "A Summary View of the Rights of British America", "The Papers of Thomas Jefferson, Volume 25: 1 January to 10 May 1793", "Thomas Jefferson, political writings", "Jefferson's proposed instructions to the Virginia delegates, 1744", "An essay towards facilitating instruction in the Anglo-Saxon and modern dialects of the English language. For the use of the University of Virginia", "Speech of Thomas Jefferson, president of the United States, delivered in the Senate chamber, March 4th, 1801", "Jefferson's Germantown letters", "The Papers of Thomas Jefferson, Volume 24: 1 June to 31 December 1792", "Letters", "The four versions of Jefferson's letter to Mazzei", "The Papers of Thomas Jefferson, Volume 6: May 1781 to March 1784", "Citizen Jefferson", "The Literary Bible of Thomas Jefferson", "The Papers of Thomas Jefferson, Retirement Series: Volume 6: 11 March to 27 November 1813", "Thomas Jefferson's Farm book", "The Papers of Thomas Jefferson, Volume 35: 1 August to 30 November 1801", "The portable Thomas Jefferson", "Junior Fact Summer 2007 Bundle", "The Papers of Thomas Jefferson, Retirement Series: Volume 2: 16 November 1809 to 11 August 1810", "Thomas Jefferson", "The Papers of Thomas Jefferson, Retirement Series: Volume 1: 4 March 1809 to 15 November 1809", "The correspondence of Jefferson and Du Pont de Nemours", "The Papers of Thomas Jefferson, Volume 18: 4 November 1790 to 24 January 1791", "Jefferson's extracts from the Gospels", "Letters and addresses of Thomas Jefferson", "The Papers of Thomas Jefferson, Retirement Series: Volume 3: 12 August 1810 to 17 June 1811", "Republican notes on religion ; and, An act establishing religious freedom, passed in the assembly of Virginia, in the year 1786", "The essential Jefferson", "The Papers of Thomas Jefferson, Volume 15: March 1789 to November 1789", "Correspondence between His Excellency Thomas Jefferson, President of the United States and James Monroe, Esq., late American ambassador to the Court of St. James", "Jefferson abroad", "Jefferson the man", "Crusade against ignorance", "Elementary School Support Kit/Bulletin Boards", "The Papers of Thomas Jefferson, Volume 26: 11 May to 31 August 1793", "The Papers of Thomas Jefferson, Volume 20: April 1791 to August 1791", "\\\"Ye will say I am no Christian\\\"", "Public and private papers", "The Papers of Thomas Jefferson, Volume 13: March 1788 to October 1788", "Revolutionary Philosopher", "The Works of Thomas Jefferson", "The religious and moral wisdom of Thomas Jefferson"], "ans_acc": 0.02158273381294964, "ans_hit": 1, "ans_f1": 0.04095563139931741, "ans_precission": 0.4, "ans_recall": 0.02158273381294964, "path_f1": 0.08080808080808081, "path_precision": 1.0, "path_recall": 0.042105263157894736, "path_ans_f1": 0.04225352112676056, "path_ans_precision": 1.0, "path_ans_recall": 0.02158273381294964}
{"id": "WebQTest-1101", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.activism.activism_issue.activists -> Thomas Nkobi\n# Answer:\nAnti-Apartheid", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.nobelprizes.nobel_subject_area.nobel_awards -> m.063k0hx\n# Answer:\nAnti-Apartheid", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.profession -> Humanitarian -> common.topic.notable_for -> g.1256fnzxx\n# Answer:\nHumanitarian", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.activism.activism_issue.activists -> Abbie Hoffman\n# Answer:\nAnti-Apartheid", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> African Americans' rights -> base.activism.activism_issue.activists -> Abbie Hoffman\n# Answer:\nAfrican Americans' rights", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.profession -> Humanitarian -> common.topic.notable_types -> Profession\n# Answer:\nHumanitarian", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.profession -> Writer -> people.profession.specializations -> Author\n# Answer:\nWriter", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.activism.activism_issue.activists -> Albert Lutuli\n# Answer:\nAnti-Apartheid", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.activism.activism_issue.supporting_political_parties -> African National Congress\n# Answer:\nAnti-Apartheid", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> African Americans' rights -> base.activism.activism_issue.activists -> Abraham Osheroff\n# Answer:\nAfrican Americans' rights"], "ground_truth": ["Civil rights activist", "Minister", "Writer", "Pastor", "Clergy", "Humanitarian"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.3157894736842105, "ans_precission": 0.3, "ans_recall": 0.3333333333333333, "path_f1": 0.125, "path_precision": 0.1, "path_recall": 0.16666666666666666, "path_ans_f1": 0.3157894736842105, "path_ans_precision": 0.3, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1102", "prediction": ["# Reasoning Path:\nVictoria -> location.location.containedby -> British Columbia -> location.location.containedby -> Canada\n# Answer:\nBritish Columbia", "# Reasoning Path:\nVictoria -> location.location.containedby -> British Columbia -> base.aareas.schema.administrative_area.administrative_area_type -> Canadian province\n# Answer:\nBritish Columbia", "# Reasoning Path:\nVictoria -> location.location.containedby -> Canada -> location.location.containedby -> North America\n# Answer:\nCanada", "# Reasoning Path:\nVictoria -> common.image.size -> m.02dtr85\n# Answer:\ncommon.image.size", "# Reasoning Path:\nVictoria -> location.location.containedby -> Canada -> base.aareas.schema.administrative_area.administrative_children -> British Columbia\n# Answer:\nCanada", "# Reasoning Path:\nVictoria -> location.location.containedby -> British Columbia -> book.book_subject.works -> A Ghost in the Water\n# Answer:\nBritish Columbia", "# Reasoning Path:\nVictoria -> location.location.containedby -> British Columbia -> book.book_subject.works -> A child is not a toy\n# Answer:\nBritish Columbia", "# Reasoning Path:\nVictoria -> location.location.containedby -> Canada -> base.locations.countries.continent -> North America\n# Answer:\nCanada", "# Reasoning Path:\nVictoria -> location.location.containedby -> British Columbia -> book.book_subject.works -> Beloved land\n# Answer:\nBritish Columbia", "# Reasoning Path:\nVictoria -> location.location.events -> 1995 Victoria Film Festival -> common.topic.notable_for -> g.1q5bfp9t9\n# Answer:\n1995 Victoria Film Festival"], "ground_truth": ["British Columbia", "Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1103", "prediction": ["# Reasoning Path:\nMary Mary -> music.artist.album -> A Mary Mary Christmas -> common.topic.notable_for -> g.1258xk87v\n# Answer:\nA Mary Mary Christmas", "# Reasoning Path:\nMary Mary -> music.artist.album -> Go Get It -> music.composition.composer -> Warryn Campbell\n# Answer:\nGo Get It", "# Reasoning Path:\nMary Mary -> music.artist.album -> A Mary Mary Christmas -> music.album.release_type -> Album\n# Answer:\nA Mary Mary Christmas", "# Reasoning Path:\nMary Mary -> music.artist.album -> Go Get It -> music.composition.composer -> Erica Campbell\n# Answer:\nGo Get It", "# Reasoning Path:\nMary Mary -> award.award_winner.awards_won -> m.0_r9jdp -> freebase.valuenotation.has_value -> Winning work\n# Answer:\nWinning work", "# Reasoning Path:\nMary Mary -> music.artist.album -> Go Get It -> music.album.album_content_type -> Compilation album\n# Answer:\nGo Get It", "# Reasoning Path:\nMary Mary -> music.artist.album -> Go Get It -> music.composition.composer -> Tina Campbell\n# Answer:\nGo Get It", "# Reasoning Path:\nMary Mary -> music.artist.album -> Help 2.0 -> music.album.genre -> Contemporary R&B\n# Answer:\nHelp 2.0", "# Reasoning Path:\nMary Mary -> award.award_winner.awards_won -> m.0_r9jdp -> award.award_honor.award -> NAACP Image Award for Outstanding Duo or Group\n# Answer:\nNAACP Image Award for Outstanding Duo or Group", "# Reasoning Path:\nMary Mary -> award.award_winner.awards_won -> m.0_r9tvb -> award.award_honor.honored_for -> Go Get It\n# Answer:\nGo Get It"], "ground_truth": ["Erica Campbell", "Tina Campbell"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25, "path_precision": 0.2, "path_recall": 0.3333333333333333, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1104", "prediction": ["# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> French Polynesia\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> French Polynesia\n# Answer:\nFrench franc", "# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> New Caledonia\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> Wallis and Futuna\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> Andorra\n# Answer:\nFrench franc", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.11b60tv_69\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nFrance -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Spain\n# Answer:\nEuro", "# Reasoning Path:\nFrance -> location.country.currency_used -> Euro -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nEuro", "# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> Monaco\n# Answer:\nFrench franc", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6gn9g\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita"], "ground_truth": ["CFP franc", "Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1106", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m -> soccer.football_player_stats.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9021c -> soccer.football_player_stats.team -> A.C. Milan\n# Answer:\nA.C. Milan", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nPound sterling", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.02nr829 -> sports.sports_team_roster.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.02nr829 -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nMidfielder", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.05ckhjp -> sports.sports_team_roster.team -> A.C. Milan\n# Answer:\nA.C. Milan", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy"], "ground_truth": ["LA Galaxy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.47058823529411764, "path_precision": 0.4, "path_recall": 0.5714285714285714, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1107", "prediction": ["# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0z9v3c6 -> award.award_nomination.nominated_for -> Love Don't Cost a Thing\n# Answer:\nLove Don't Cost a Thing", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0b4d586 -> award.award_nomination.nominated_for -> Bobby\n# Answer:\nBobby", "# Reasoning Path:\nNick Cannon -> film.actor.film -> m.062wcnd -> film.performance.film -> Ball Don't Lie\n# Answer:\nBall Don't Lie", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0z9v3c6 -> award.award_nomination.ceremony -> 2004 Teen Choice Awards\n# Answer:\n2004 Teen Choice Awards", "# Reasoning Path:\nNick Cannon -> film.actor.film -> m.0ncf55j -> film.performance.film -> Men in Black II\n# Answer:\nMen in Black II", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0z9v3c6 -> award.award_nomination.award -> Teen Choice Award for Choice Movie: Chemistry\n# Answer:\nTeen Choice Award for Choice Movie: Chemistry", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.010g2z6m -> award.award_nomination.nominated_for -> Disney Parks Christmas Day Parade\n# Answer:\nDisney Parks Christmas Day Parade", "# Reasoning Path:\nNick Cannon -> film.actor.film -> g.11bttggxl1\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0b4d586 -> award.award_nomination.award -> Screen Actors Guild Award for Outstanding Performance by a Cast in a Motion Picture\n# Answer:\nScreen Actors Guild Award for Outstanding Performance by a Cast in a Motion Picture", "# Reasoning Path:\nNick Cannon -> film.actor.film -> m.062wcnd -> film.performance.character -> Mico\n# Answer:\nMico"], "ground_truth": ["American Son", "The Adventures of Brer Rabbit", "Underclassman", "Drumline: A New Beat", "Goal II: Living the Dream", "Weapons", "Even Money", "Love Don't Cost a Thing", "Ball Don't Lie", "Drumline", "Day of the Dead", "Whatever It Takes", "Chiraq", "Shall We Dance?", "Roll Bounce", "The Killing Room", "Monster House", "A Very School Gyrls Holla-Day", "Men in Black II", "Garfield: The Movie", "Bobby"], "ans_acc": 0.19047619047619047, "ans_hit": 1, "ans_f1": 0.25806451612903225, "ans_precission": 0.4, "ans_recall": 0.19047619047619047, "path_f1": 0.1951219512195122, "path_precision": 0.4, "path_recall": 0.12903225806451613, "path_ans_f1": 0.25806451612903225, "path_ans_precision": 0.4, "path_ans_recall": 0.19047619047619047}
{"id": "WebQTest-1108", "prediction": ["# Reasoning Path:\nNPR -> organization.organization.headquarters -> m.02hr995 -> location.mailing_address.citytown -> Washington, D.C.\n# Answer:\nWashington, D.C.", "# Reasoning Path:\nNPR -> organization.organization.headquarters -> m.02hr995 -> common.topic.notable_for -> g.1258r1rck\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nNPR -> organization.organization.headquarters -> m.02hr995 -> location.location.geolocation -> m.04nm5hc\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nNPR -> award.award_winner.awards_won -> m.0103rsl0 -> award.award_honor.ceremony -> 2014 Peabody Awards Ceremony\n# Answer:\n2014 Peabody Awards Ceremony", "# Reasoning Path:\nNPR -> broadcast.radio_network.affiliates -> m.010gb2_0 -> broadcast.radio_affiliation_duration.station -> WDET-FM\n# Answer:\nWDET-FM", "# Reasoning Path:\nNPR -> award.award_winner.awards_won -> m.0103rsl0 -> award.award_honor.award_winner -> Michele Norris\n# Answer:\nMichele Norris", "# Reasoning Path:\nNPR -> award.award_winner.awards_won -> m.03wllq5 -> award.award_honor.award -> National Medal of Arts\n# Answer:\nNational Medal of Arts", "# Reasoning Path:\nNPR -> award.award_winner.awards_won -> m.0103rsl0 -> award.award_honor.honored_for -> The Race Card Project\n# Answer:\nThe Race Card Project", "# Reasoning Path:\nNPR -> broadcast.radio_network.affiliates -> m.010gb2ws -> broadcast.radio_affiliation_duration.station -> WBHM\n# Answer:\nWBHM", "# Reasoning Path:\nNPR -> award.award_winner.awards_won -> m.05v0spb -> freebase.valuenotation.is_reviewed -> Ceremony\n# Answer:\nCeremony"], "ground_truth": ["WNYC"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1109", "prediction": ["# Reasoning Path:\nTom Landry -> sports.pro_athlete.teams -> m.0hpglgc -> sports.sports_team_roster.team -> New York Giants\n# Answer:\nNew York Giants", "# Reasoning Path:\nTom Landry -> sports.pro_athlete.teams -> m.0hph6vx -> sports.sports_team_roster.team -> Texas Longhorns football\n# Answer:\nTexas Longhorns football", "# Reasoning Path:\nTom Landry -> sports.pro_athlete.teams -> m.0hpglgc -> sports.sports_team_roster.position -> Cornerback\n# Answer:\nCornerback", "# Reasoning Path:\nTom Landry -> sports.pro_athlete.teams -> m.0hpglgp -> sports.sports_team_roster.team -> New York Yankees\n# Answer:\nNew York Yankees", "# Reasoning Path:\nTom Landry -> sports.pro_athlete.teams -> m.0hph6vx -> freebase.valuenotation.has_value -> To\n# Answer:\nTo", "# Reasoning Path:\nTom Landry -> symbols.name_source.namesakes -> Landry Jones -> sports.pro_athlete.teams -> m.0h_j38w\n# Answer:\nLandry Jones", "# Reasoning Path:\nTom Landry -> symbols.name_source.namesakes -> Landry Jones -> sports.pro_athlete.teams -> m.0vyl_2v\n# Answer:\nLandry Jones", "# Reasoning Path:\nTom Landry -> symbols.name_source.namesakes -> Landry Jones -> common.topic.image -> Jones winding up to pass in a game againt Tulsa in 2009.\n# Answer:\nLandry Jones", "# Reasoning Path:\nTom Landry -> sports.pro_athlete.teams -> m.0hpglgp -> sports.sports_team_roster.position -> Defensive back\n# Answer:\nDefensive back", "# Reasoning Path:\nTom Landry -> symbols.name_source.namesakes -> Landry Jones -> american_football.football_player.position_s -> Quarterback\n# Answer:\nLandry Jones"], "ground_truth": ["New York Yankees", "Texas Longhorns football", "New York Giants"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Eastern European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> United Kingdom\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Slovakia -> location.location.containedby -> Eurasia\n# Answer:\nSlovakia", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Hungary -> location.location.containedby -> Eurasia\n# Answer:\nHungary", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.partially_contains -> Russia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Babia G\u00f3ra -> location.location.containedby -> Europe\n# Answer:\nBabia G\u00f3ra", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Hungary -> base.locations.countries.continent -> Europe\n# Answer:\nHungary", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Czech Republic -> location.location.containedby -> Eurasia\n# Answer:\nCzech Republic"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1110", "prediction": ["# Reasoning Path:\nJohn Jay -> people.person.place_of_birth -> New York City -> travel.travel_destination.tourist_attractions -> Staten Island Ferry\n# Answer:\nNew York City", "# Reasoning Path:\nJohn Jay -> people.person.place_of_birth -> New York City -> media_common.quotation_subject.quotations_about_this_subject -> \\\"History dressed up in the glow of love\u2019s kiss turned grief into beauty.\\\"\n# Answer:\nNew York City", "# Reasoning Path:\nJohn Jay -> people.person.place_of_birth -> New York City -> travel.travel_destination.tourist_attractions -> Metropolitan Museum of Art\n# Answer:\nNew York City", "# Reasoning Path:\nJohn Jay -> government.politician.party -> m.0lr1zhl -> government.political_party_tenure.party -> Federalist Party\n# Answer:\nFederalist Party", "# Reasoning Path:\nJohn Jay -> common.topic.image -> John Jay, einer der Gr\u00fcnderv\u00e4ter der Vereinigten Staaten, war direkter Vorfahre Trotts. -> common.image.appears_in_topic_gallery -> Federalist No. 64\n# Answer:\nJohn Jay, einer der Gr\u00fcnderv\u00e4ter der Vereinigten Staaten, war direkter Vorfahre Trotts.", "# Reasoning Path:\nJohn Jay -> common.topic.image -> John Jay, einer der Gr\u00fcnderv\u00e4ter der Vereinigten Staaten, war direkter Vorfahre Trotts. -> common.image.appears_in_topic_gallery -> Adam von Trott zu Solz\n# Answer:\nJohn Jay, einer der Gr\u00fcnderv\u00e4ter der Vereinigten Staaten, war direkter Vorfahre Trotts.", "# Reasoning Path:\nJohn Jay -> common.topic.image -> John Jay, einer der Gr\u00fcnderv\u00e4ter der Vereinigten Staaten, war direkter Vorfahre Trotts. -> common.image.size -> m.02bg03x\n# Answer:\nJohn Jay, einer der Gr\u00fcnderv\u00e4ter der Vereinigten Staaten, war direkter Vorfahre Trotts.", "# Reasoning Path:\nJohn Jay -> common.topic.image -> John Jay, einer der Gr\u00fcnderv\u00e4ter der Vereinigten Staaten, war direkter Vorfahre Trotts. -> common.image.appears_in_topic_gallery -> Federalist No. 2\n# Answer:\nJohn Jay, einer der Gr\u00fcnderv\u00e4ter der Vereinigten Staaten, war direkter Vorfahre Trotts."], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1111", "prediction": ["# Reasoning Path:\nMount Fuji -> location.location.containedby -> Honshu -> location.location.containedby -> Japan\n# Answer:\nHonshu", "# Reasoning Path:\nMount Fuji -> location.location.containedby -> Asia -> location.location.containedby -> Northern Hemisphere\n# Answer:\nAsia", "# Reasoning Path:\nMount Fuji -> base.schemastaging.context_name.pronunciation -> g.125_ptxg0\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nMount Fuji -> location.location.containedby -> Honshu -> location.location.geolocation -> m.0dbffb_\n# Answer:\nHonshu", "# Reasoning Path:\nMount Fuji -> location.location.containedby -> Ch\u016bbu region -> location.location.containedby -> Japan\n# Answer:\nCh\u016bbu region", "# Reasoning Path:\nMount Fuji -> location.location.containedby -> Honshu -> location.location.contains -> Abu\n# Answer:\nHonshu", "# Reasoning Path:\nMount Fuji -> geography.mountain.mountain_type -> Volcano -> common.topic.image -> dds24167_L.jpg\n# Answer:\nVolcano", "# Reasoning Path:\nMount Fuji -> location.location.containedby -> Honshu -> location.location.contains -> Aichi Prefecture\n# Answer:\nHonshu", "# Reasoning Path:\nMount Fuji -> geography.mountain.mountain_type -> Volcano -> book.book_subject.works -> Volcano Adventure\n# Answer:\nVolcano", "# Reasoning Path:\nMount Fuji -> location.location.containedby -> Honshu -> location.location.contains -> Akita Prefecture\n# Answer:\nHonshu"], "ground_truth": ["Honshu", "Japan", "Ch\u016bbu region", "Fuji-Hakone-Izu National Park", "Asia"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.6461538461538462, "ans_precission": 0.7, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.7466666666666666, "path_ans_precision": 0.7, "path_ans_recall": 0.8}
{"id": "WebQTest-1113", "prediction": ["# Reasoning Path:\nToussaint Louverture -> people.person.place_of_birth -> Saint-Domingue -> location.location.people_born_here -> Vincent Og\u00e9\n# Answer:\nSaint-Domingue", "# Reasoning Path:\nToussaint Louverture -> people.person.place_of_birth -> Saint-Domingue -> common.topic.notable_types -> Location\n# Answer:\nSaint-Domingue", "# Reasoning Path:\nToussaint Louverture -> people.person.place_of_birth -> Saint-Domingue -> location.location.people_born_here -> Dominique You\n# Answer:\nSaint-Domingue", "# Reasoning Path:\nToussaint Louverture -> people.person.children -> Saint-Jean Louverture -> people.person.gender -> Male\n# Answer:\nSaint-Jean Louverture", "# Reasoning Path:\nToussaint Louverture -> people.person.place_of_birth -> Saint-Domingue -> location.location.people_born_here -> Jean Lafitte\n# Answer:\nSaint-Domingue", "# Reasoning Path:\nToussaint Louverture -> people.person.place_of_birth -> Saint-Domingue -> location.location.events -> Haitian Revolution\n# Answer:\nSaint-Domingue", "# Reasoning Path:\nToussaint Louverture -> military.military_person.service -> m.0410mhn -> military.military_service.rank -> General officer\n# Answer:\nGeneral officer", "# Reasoning Path:\nToussaint Louverture -> people.person.children -> Saint-Jean Louverture -> common.topic.notable_types -> Person\n# Answer:\nSaint-Jean Louverture", "# Reasoning Path:\nToussaint Louverture -> people.person.place_of_birth -> Saint-Domingue -> location.location.events -> Action of 1 January 1800\n# Answer:\nSaint-Domingue", "# Reasoning Path:\nToussaint Louverture -> people.person.children -> Saint-Jean Louverture -> common.topic.notable_for -> g.12564t_87\n# Answer:\nSaint-Jean Louverture"], "ground_truth": ["Saint-Domingue"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1114", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Tyrone\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> base.aareas.schema.administrative_area.administrative_children -> East Midlands\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Scotland -> location.country.first_level_divisions -> Aberdeen\n# Answer:\nScotland", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nChild pornography", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> base.aareas.schema.administrative_area.administrative_children -> East of England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> location.country.first_level_divisions -> East Midlands\n# Answer:\nEngland"], "ground_truth": ["England", "Northern Ireland", "Scotland", "Wales"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.6, "path_precision": 0.5, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-1115", "prediction": ["# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73505 -> location.location.geolocation -> m.03dw8k7\n# Answer:\n73505", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73505 -> location.postal_code.country -> United States of America\n# Answer:\n73505", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73507 -> location.location.containedby -> Comanche County\n# Answer:\n73507", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73505 -> location.location.containedby -> Comanche County\n# Answer:\n73505", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73502 -> common.topic.notable_types -> Postal Code\n# Answer:\n73502", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73507 -> common.topic.notable_types -> Postal Code\n# Answer:\n73507", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73507 -> location.location.geolocation -> m.03dw8kf\n# Answer:\n73507", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73502 -> location.postal_code.country -> United States of America\n# Answer:\n73502", "# Reasoning Path:\nLawton -> location.location.people_born_here -> Ada Leonard -> music.artist.genre -> Jazz\n# Answer:\nAda Leonard", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73502 -> common.topic.notable_for -> g.1258s3mpy\n# Answer:\n73502"], "ground_truth": ["73506", "73507", "73501", "73505", "73502"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7200000000000001, "ans_precission": 0.9, "ans_recall": 0.6, "path_f1": 0.7200000000000001, "path_precision": 0.9, "path_recall": 0.6, "path_ans_f1": 0.7200000000000001, "path_ans_precision": 0.9, "path_ans_recall": 0.6}
{"id": "WebQTest-1116", "prediction": ["# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> location.location.containedby -> Iberian Peninsula\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> location.location.containedby -> Community of Madrid\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> periodicals.newspaper_circulation_area.newspapers -> P\u00fablico\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_children -> Chafarinas Islands -> location.location.contains_major_portion_of -> Morocco\n# Answer:\nChafarinas Islands", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_children -> Chafarinas Islands -> base.aareas.schema.administrative_area.administrative_area_type -> Spanish place of sovereignty\n# Answer:\nChafarinas Islands", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc38hhk\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> periodicals.newspaper_circulation_area.newspapers -> ADN\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_children -> Chafarinas Islands -> location.location.containedby -> Mediterranean Sea\n# Answer:\nChafarinas Islands", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_children -> Aragon -> base.aareas.schema.administrative_area.administrative_children -> Zaragoza\n# Answer:\nAragon"], "ground_truth": ["Madrid"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1117", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_used -> East German mark\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> West Germany -> location.country.currency_used -> Deutsche Mark\n# Answer:\nWest Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Vaduz\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.country.first_level_divisions -> Vaduz\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Switzerland\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_used -> Deutsche Mark\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Andreas Bielau\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0102xvg7 -> education.education.student -> Sigurd Burckhardt\n# Answer:\nSigurd Burckhardt", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> West Germany -> location.location.containedby -> Western Europe\n# Answer:\nWest Germany"], "ground_truth": ["Denmark", "Canada", "Austria", "Liechtenstein", "Germany", "Czech Republic", "Vatican City", "Luxembourg", "West Germany", "Second Polish Republic", "Switzerland", "Cyprus", "East Germany", "Belgium", "South Africa"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.8, "ans_recall": 0.26666666666666666, "path_f1": 0.32000000000000006, "path_precision": 0.8, "path_recall": 0.2, "path_ans_f1": 0.48648648648648646, "path_ans_precision": 0.9, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1118", "prediction": ["# Reasoning Path:\nMark McGwire -> baseball.baseball_player.batting_stats -> m.06rkgnq -> baseball.batting_statistics.season -> 2001 Major League Baseball Season\n# Answer:\n2001 Major League Baseball Season", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.batting_stats -> m.06rkf_h -> baseball.batting_statistics.season -> 1988 Major League Baseball Season\n# Answer:\n1988 Major League Baseball Season", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.batting_stats -> m.06rkfx7 -> baseball.batting_statistics.season -> 1986 Major League Baseball Season\n# Answer:\n1986 Major League Baseball Season", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.batting_stats -> m.06rkgnq -> baseball.batting_statistics.team -> St. Louis Cardinals\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.batting_stats -> m.06rkf_h -> baseball.batting_statistics.team -> Oakland Athletics\n# Answer:\nOakland Athletics", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.position_s -> First baseman -> sports.sports_position.players -> g.11bxv84dh5\n# Answer:\nFirst baseman", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.position_s -> First baseman -> common.topic.notable_types -> Baseball Position\n# Answer:\nFirst baseman", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.position_s -> First baseman -> sports.sports_position.sport -> Baseball\n# Answer:\nFirst baseman", "# Reasoning Path:\nMark McGwire -> award.award_nominee.award_nominations -> m.0sgpgnt -> award.award_nomination.award -> Kids' Choice Award for Favorite Male Athlete\n# Answer:\nKids' Choice Award for Favorite Male Athlete", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.position_s -> First baseman -> sports.sports_position.players -> g.11bxwkc1bd\n# Answer:\nFirst baseman"], "ground_truth": ["2001 Major League Baseball Season"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1119", "prediction": ["# Reasoning Path:\nColorado Technical University -> education.educational_institution.phone_number -> m.0nbc0rl\n# Answer:\neducation.educational_institution.phone_number", "# Reasoning Path:\nColorado Technical University -> education.university.number_of_undergraduates -> m.0hpxh03\n# Answer:\neducation.university.number_of_undergraduates", "# Reasoning Path:\nColorado Technical University -> education.educational_institution.phone_number -> m.0hprv5g\n# Answer:\neducation.educational_institution.phone_number", "# Reasoning Path:\nColorado Technical University -> education.university.number_of_undergraduates -> m.0h783sd\n# Answer:\neducation.university.number_of_undergraduates"], "ground_truth": ["College/University"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Bard\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.notable_types -> Profession\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Poet\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> book.book_subject.works -> Authors Frequently Mentioned on the Web\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.article -> m.0knj9\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Author\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.image -> The Bard\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Cardiovascular disease\n# Answer:\nMale", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> book.book_subject.works -> Contemporary Authors: A Bio-Bibliographical Guide to Current Writers in Fiction, General Nonfiction, Poetry, Journalism, Drama, Motion Pictures, Television. Vol. 249\n# Answer:\nWriter"], "ground_truth": ["Author", "Poet", "Writer", "Bard"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6428571428571429, "ans_precission": 0.9, "ans_recall": 0.5, "path_f1": 0.6428571428571429, "path_precision": 0.9, "path_recall": 0.5, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1120", "prediction": ["# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> French Polynesia\n# Answer:\nFrench franc", "# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> Andorra\n# Answer:\nFrench franc", "# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> Monaco\n# Answer:\nFrench franc", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.11b60tv_69\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nFrance -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Spain\n# Answer:\nEuro", "# Reasoning Path:\nFrance -> location.country.currency_used -> Euro -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nEuro", "# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> French Polynesia\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Andorra\n# Answer:\nEuro", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6gn9g\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> New Caledonia\n# Answer:\nCFP franc"], "ground_truth": ["French franc", "Assignat"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.37499999999999994, "ans_precission": 0.3, "ans_recall": 0.5, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.37499999999999994, "path_ans_precision": 0.3, "path_ans_recall": 0.5}
{"id": "WebQTest-1121", "prediction": ["# Reasoning Path:\nKentucky -> symbols.flag_referent.flag -> m.0jsrj2m -> symbols.flag_use.flag -> Flag of Kentucky\n# Answer:\nFlag of Kentucky", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.0hz9xtw -> location.location_symbol_relationship.symbol -> Cardinal\n# Answer:\nCardinal", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.0hz9xtw -> location.location_symbol_relationship.Kind_of_symbol -> State bird\n# Answer:\nState bird", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0d65 -> location.location_symbol_relationship.symbol -> Brachiopod\n# Answer:\nBrachiopod", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0nqp -> location.location_symbol_relationship.symbol -> Liriodendron\n# Answer:\nLiriodendron", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0d65 -> location.location_symbol_relationship.Kind_of_symbol -> State fossil\n# Answer:\nState fossil", "# Reasoning Path:\nKentucky -> military.military_unit_place_of_origin.military_units -> 10th Kentucky Cavalry Regiment (Diamond's) -> common.topic.notable_types -> Military unit\n# Answer:\n10th Kentucky Cavalry Regiment (Diamond's)", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0nqp -> location.location_symbol_relationship.Kind_of_symbol -> State tree\n# Answer:\nState tree", "# Reasoning Path:\nKentucky -> military.military_unit_place_of_origin.military_units -> 10th Kentucky Cavalry Regiment (Diamond's) -> military.military_unit.armed_force -> Confederate States Army\n# Answer:\n10th Kentucky Cavalry Regiment (Diamond's)", "# Reasoning Path:\nKentucky -> military.military_unit_place_of_origin.military_units -> 10th Kentucky Partisan Rangers Regiment (Johnson's) -> military.military_unit.armed_force -> Confederate States Army\n# Answer:\n10th Kentucky Partisan Rangers Regiment (Johnson's)"], "ground_truth": ["United we stand, divided we fall"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1124", "prediction": ["# Reasoning Path:\nBewitched -> tv.tv_program.regular_cast -> m.0kdhbl1 -> tv.regular_tv_appearance.actor -> Maurice Evans\n# Answer:\nMaurice Evans", "# Reasoning Path:\nBewitched -> tv.tv_program.regular_cast -> m.02nrqs4 -> tv.regular_tv_appearance.actor -> Dick York\n# Answer:\nDick York", "# Reasoning Path:\nBewitched -> tv.tv_program.regular_cast -> m.02nrqt1 -> tv.regular_tv_appearance.actor -> Agnes Moorehead\n# Answer:\nAgnes Moorehead", "# Reasoning Path:\nBewitched -> tv.tv_program.regular_cast -> m.02nrqs4 -> tv.regular_tv_appearance.character -> Darrin\n# Answer:\nDarrin", "# Reasoning Path:\nBewitched -> tv.tv_program.regular_cast -> m.02nrqt1 -> tv.regular_tv_appearance.character -> Endora\n# Answer:\nEndora", "# Reasoning Path:\nBewitched -> common.topic.webpage -> m.0941_05 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nBewitched -> common.topic.webpage -> m.0941vyc -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Elizabeth Montgomery"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1125", "prediction": ["# Reasoning Path:\nIsaac Newton -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.0101ddrk\n# Answer:\nPhysicist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Physicist -> people.profession.part_of_professional_field -> Physics\n# Answer:\nPhysicist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Scientist -> common.topic.notable_types -> Profession\n# Answer:\nScientist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.01066smw\n# Answer:\nPhysicist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Daniel Faraday\n# Answer:\nPhysicist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Chemist -> base.descriptive_names.names.descriptive_name -> m.01066g6n\n# Answer:\nChemist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Chemist -> base.schemastaging.context_name.pronunciation -> g.125_m09_2\n# Answer:\nChemist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.01066sxg\n# Answer:\nPhysicist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Chemist -> common.topic.notable_types -> Profession\n# Answer:\nChemist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Scientist -> common.topic.subject_of -> Biography\n# Answer:\nScientist"], "ground_truth": ["Astronomer", "Philosopher", "Chemist", "Scientist", "Physicist", "Mathematician"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-1126", "prediction": ["# Reasoning Path:\nSan Diego International Airport -> location.location.containedby -> United States of America -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nSan Diego -> location.location.containedby -> United States of America -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nSan Diego International Airport -> location.location.containedby -> San Diego County -> location.location.containedby -> California\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego -> location.location.containedby -> San Diego County -> location.location.containedby -> California\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego International Airport -> aviation.airport.serves -> San Diego -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nSan Diego International Airport -> location.location.containedby -> San Diego County -> location.location.containedby -> United States of America\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego -> location.location.containedby -> San Diego County -> location.location.containedby -> United States of America\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego International Airport -> location.location.containedby -> United States of America -> location.location.primarily_containedby -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nSan Diego -> location.location.containedby -> United States of America -> location.location.primarily_containedby -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nSan Diego -> location.location.containedby -> 92101 -> location.location.containedby -> San Diego County\n# Answer:\n92101"], "ground_truth": ["San Diego County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.28571428571428575, "path_precision": 0.2, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1127", "prediction": ["# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> United States of America -> location.country.first_level_divisions -> Mississippi\n# Answer:\nUnited States of America", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> United States of America -> location.country.first_level_divisions -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> United States of America -> location.country.first_level_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> Antigua and Barbuda -> location.country.languages_spoken -> English Language\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Alaska\n# Answer:\nUnited States of America", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> United States of America -> base.locations.countries.states_provinces_within -> Mississippi\n# Answer:\nUnited States of America", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> Bahamas -> location.country.languages_spoken -> English Language\n# Answer:\nBahamas", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> Antigua and Barbuda -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAntigua and Barbuda"], "ground_truth": ["Canada"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1130", "prediction": ["# Reasoning Path:\nSanta Claus -> film.film_character.portrayed_in_films -> m.010rvdmk -> film.performance.actor -> Googy Gress\n# Answer:\nGoogy Gress", "# Reasoning Path:\nSanta Claus -> film.film.genre -> Cult film\n# Answer:\nCult film", "# Reasoning Path:\nSanta Claus -> film.film_character.portrayed_in_films -> m.0111f_4n -> film.performance.actor -> Jim Broadbent\n# Answer:\nJim Broadbent", "# Reasoning Path:\nSanta Claus -> film.film_character.portrayed_in_films -> m.010rvdmk -> film.performance.film -> Naughty or Nice\n# Answer:\nNaughty or Nice", "# Reasoning Path:\nSanta Claus -> common.topic.webpage -> m.09w47wj -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nSanta Claus -> common.topic.webpage -> m.09w85m2 -> common.webpage.resource -> Uma Thurman, Jesse L. Martin to appear in NBC's Muppets holiday special\n# Answer:\nUma Thurman, Jesse L. Martin to appear in NBC's Muppets holiday special", "# Reasoning Path:\nSanta Claus -> film.film_character.portrayed_in_films -> m.01143_m2 -> film.performance.actor -> Boris Martemyanov\n# Answer:\nBoris Martemyanov", "# Reasoning Path:\nSanta Claus -> common.topic.webpage -> m.09w47wj -> common.webpage.resource -> Exclusive: Look what Santa Claus brought me!\n# Answer:\nExclusive: Look what Santa Claus brought me!", "# Reasoning Path:\nSanta Claus -> common.topic.webpage -> m.09w9dng -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nSanta Claus -> film.film.genre -> Drama -> base.kinometric.genre.incompatible_genres -> Comedy\n# Answer:\nDrama"], "ground_truth": ["Saint Nicholas"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1131", "prediction": ["# Reasoning Path:\nKelly Slater -> people.person.profession -> Voice Actor -> fictional_universe.character_occupation.characters_with_this_occupation -> Jenny Larson\n# Answer:\nVoice Actor", "# Reasoning Path:\nKelly Slater -> people.person.profession -> Voice Actor -> people.profession.specialization_of -> Actor\n# Answer:\nVoice Actor", "# Reasoning Path:\nKelly Slater -> people.person.profession -> Actor -> common.topic.subject_of -> Michael Palance\n# Answer:\nActor", "# Reasoning Path:\nKelly Slater -> people.person.profession -> Film Producer -> base.descriptive_names.names.descriptive_name -> m.0106b5xy\n# Answer:\nFilm Producer", "# Reasoning Path:\nKelly Slater -> tv.tv_actor.guest_roles -> m.0dkzsbw -> tv.tv_guest_role.episodes_appeared_in -> Sky Rider\n# Answer:\nSky Rider", "# Reasoning Path:\nKelly Slater -> people.person.profession -> Film Producer -> common.topic.subject_of -> Mervin Praison\n# Answer:\nFilm Producer", "# Reasoning Path:\nKelly Slater -> people.person.profession -> Film Producer -> common.topic.subject_of -> Billy Sorrentino\n# Answer:\nFilm Producer", "# Reasoning Path:\nKelly Slater -> people.person.profession -> Film Producer -> award.award_discipline.awards_in_this_discipline -> David di Donatello for Best Producer\n# Answer:\nFilm Producer", "# Reasoning Path:\nKelly Slater -> people.person.profession -> Actor -> common.topic.subjects -> Michael Palance\n# Answer:\nActor", "# Reasoning Path:\nKelly Slater -> award.award_nominee.award_nominations -> m.0z9ljgz -> award.award_nomination.award -> Teen Choice Award for Choice Male Action Sports Star\n# Answer:\nTeen Choice Award for Choice Male Action Sports Star"], "ground_truth": ["1990"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1132", "prediction": ["# Reasoning Path:\nCoretta Scott King -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.includes_causes_of_death -> Choking\n# Answer:\nRespiratory failure", "# Reasoning Path:\nCoretta Scott King -> people.deceased_person.cause_of_death -> Respiratory failure -> common.topic.notable_types -> Symptom\n# Answer:\nRespiratory failure", "# Reasoning Path:\nCoretta Scott King -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nRespiratory failure", "# Reasoning Path:\nCoretta Scott King -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.includes_causes_of_death -> Erotic asphyxiation\n# Answer:\nRespiratory failure", "# Reasoning Path:\nCoretta Scott King -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.includes_causes_of_death -> Strangling\n# Answer:\nRespiratory failure", "# Reasoning Path:\nCoretta Scott King -> people.person.ethnicity -> African American -> medicine.risk_factor.diseases -> Sickle cell disease\n# Answer:\nAfrican American", "# Reasoning Path:\nCoretta Scott King -> people.person.children -> Yolanda King -> people.deceased_person.cause_of_death -> heart attack\n# Answer:\nYolanda King", "# Reasoning Path:\nCoretta Scott King -> people.person.ethnicity -> African American -> medicine.risk_factor.diseases -> Scleroderma\n# Answer:\nAfrican American", "# Reasoning Path:\nCoretta Scott King -> people.person.ethnicity -> African American -> common.topic.webpage -> m.09w10n8\n# Answer:\nAfrican American", "# Reasoning Path:\nCoretta Scott King -> people.person.ethnicity -> African American -> medicine.risk_factor.diseases -> Uterine fibroid\n# Answer:\nAfrican American"], "ground_truth": ["Respiratory failure"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1133", "prediction": ["# Reasoning Path:\nWilliam Henry Smith -> common.topic.notable_types -> Politician -> common.topic.article -> m.02xlj10\n# Answer:\nPolitician", "# Reasoning Path:\nWilliam Henry Smith -> people.person.place_of_birth -> London -> travel.travel_destination.tourist_attractions -> Smithfield, London\n# Answer:\nLondon", "# Reasoning Path:\nWilliam Henry Smith -> people.person.spouse_s -> m.0j4mrzc -> people.marriage.spouse -> Emily Smith, 1st Viscountess Hambleden\n# Answer:\nEmily Smith, 1st Viscountess Hambleden", "# Reasoning Path:\nWilliam Henry Smith -> people.person.place_of_birth -> London -> symbols.name_source.namesakes -> 8837 London\n# Answer:\nLondon", "# Reasoning Path:\nWilliam Henry Smith -> common.topic.notable_types -> Politician -> type.type.expected_by -> politician\n# Answer:\nPolitician", "# Reasoning Path:\nWilliam Henry Smith -> people.person.place_of_birth -> London -> location.location.partially_contains -> River Thames\n# Answer:\nLondon", "# Reasoning Path:\nWilliam Henry Smith -> people.person.spouse_s -> m.0j4mrzc -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nWilliam Henry Smith -> people.person.place_of_birth -> London -> symbols.name_source.namesakes -> New London\n# Answer:\nLondon", "# Reasoning Path:\nWilliam Henry Smith -> people.person.place_of_birth -> London -> travel.travel_destination.tourist_attractions -> Apsley House\n# Answer:\nLondon", "# Reasoning Path:\nWilliam Henry Smith -> common.topic.notable_types -> Politician -> type.type.expected_by -> Candidate\n# Answer:\nPolitician"], "ground_truth": ["WHSmith"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1135", "prediction": ["# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Spiro Agnew -> people.person.quotations -> A spirit of national masochism prevails, encouraged by an effete corps of impudent snobs who characterize themselves as intellectuals.\n# Answer:\nSpiro Agnew", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Gerald Ford -> base.americancomedy.impersonated_celebrity.impersonated_by -> Rich Little\n# Answer:\nGerald Ford", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Spiro Agnew -> people.person.quotations -> Freedom of speech is useless without freedom of thought. And I fear that the politics of protest is shutting out the process of thought, so necessary to rational discussion. We are faced with the Ten Commandments of Protest:Thou Shalt Not Allow Thy Opponent to Speak. Thou Shalt Not Set Forth a Program of Thine Own. Thou Shalt Not Trust Anybody Over Thirty. Thou Shalt Not Honor Thy Father or Thy Mother. Thou Shalt Not Heed the Lessons of History. Thou Shalt Not Write Anything Longer than a Slogan. Thou Shalt Not Present a Negotiable Demand. Thou Shalt Not Accept Any Establishment Idea. Thou Shalt Not Revere Any but Totalitarian Heroes. Thou Shalt Not Ask Forgiveness for Thy Transgressions, Rather Thou Shalt Demand Amnesty for Them.\n# Answer:\nSpiro Agnew", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Spiro Agnew -> film.person_or_entity_appearing_in_film.films -> m.0114dk_n\n# Answer:\nSpiro Agnew", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Spiro Agnew -> people.person.quotations -> In the United States today, we have more than our share of the nattering nabobs of negativism. They have formed their own 4H Clubthe hopeless, hysterical hypochondriacs of history.\n# Answer:\nSpiro Agnew", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Spiro Agnew -> people.person.education -> m.02wpd1f\n# Answer:\nSpiro Agnew", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Gerald Ford -> people.person.profession -> Politician\n# Answer:\nGerald Ford", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Spiro Agnew -> film.person_or_entity_appearing_in_film.films -> m.0cv_zwx\n# Answer:\nSpiro Agnew", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Spiro Agnew -> film.person_or_entity_appearing_in_film.films -> m.0w1mtml\n# Answer:\nSpiro Agnew"], "ground_truth": ["Gerald Ford", "Spiro Agnew"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1136", "prediction": ["# Reasoning Path:\nAdolf Hitler -> people.person.religion -> Catholicism -> religion.religion.is_part_of -> Christianity\n# Answer:\nCatholicism", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAdolf Hitler -> people.person.religion -> Catholicism -> religion.religion.branched_from -> Judaism\n# Answer:\nCatholicism", "# Reasoning Path:\nAdolf Hitler -> people.person.religion -> Christianity -> religion.religion.branched_from -> Judaism\n# Answer:\nChristianity", "# Reasoning Path:\nAdolf Hitler -> people.person.religion -> Christianity -> religion.religion.includes -> Catholicism\n# Answer:\nChristianity", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.resource -> Another Megan Fox letter -- and this one sings her praises!\n# Answer:\nAnother Megan Fox letter -- and this one sings her praises!", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wgn1j -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wgn1j -> common.webpage.resource -> Charles Manson: 40 years later, the movie about him you have to see\n# Answer:\nCharles Manson: 40 years later, the movie about him you have to see", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wqfb3 -> common.webpage.resource -> Erykah Badu teases 'New Amerykah Part Two' with trippy, funky clip\n# Answer:\nErykah Badu teases 'New Amerykah Part Two' with trippy, funky clip"], "ground_truth": ["Christianity", "Catholicism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1137", "prediction": ["# Reasoning Path:\nIllinois River -> location.location.containedby -> Illinois -> location.location.containedby -> United States of America\n# Answer:\nIllinois", "# Reasoning Path:\nIllinois River -> location.location.containedby -> Illinois -> location.location.containedby -> United States, with Territories\n# Answer:\nIllinois", "# Reasoning Path:\nIllinois River -> location.location.containedby -> Illinois -> location.location.containedby -> Contiguous United States\n# Answer:\nIllinois", "# Reasoning Path:\nIllinois River -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nIllinois River -> location.location.containedby -> Illinois -> location.statistical_region.religions -> m.04403jk\n# Answer:\nIllinois", "# Reasoning Path:\nIllinois River -> geography.body_of_water.bridges -> Abraham Lincoln Memorial Bridge -> location.location.containedby -> LaSalle County\n# Answer:\nAbraham Lincoln Memorial Bridge", "# Reasoning Path:\nIllinois River -> location.location.containedby -> Illinois -> common.topic.webpage -> m.04lstq9\n# Answer:\nIllinois", "# Reasoning Path:\nIllinois River -> location.location.containedby -> United States of America -> location.location.containedby -> Americas\n# Answer:\nUnited States of America", "# Reasoning Path:\nIllinois River -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Illinois\n# Answer:\nUnited States of America", "# Reasoning Path:\nIllinois River -> location.location.containedby -> North America -> location.location.containedby -> Northern Hemisphere\n# Answer:\nNorth America"], "ground_truth": ["Illinois", "North America", "United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1138", "prediction": ["# Reasoning Path:\nHonduras -> base.locations.countries.continent -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nHonduras -> location.location.containedby -> Latin America -> location.location.containedby -> Americas\n# Answer:\nLatin America", "# Reasoning Path:\nHonduras -> base.locations.countries.continent -> North America -> base.locations.continents.countries_within -> United States of America\n# Answer:\nNorth America", "# Reasoning Path:\nHonduras -> base.locations.countries.continent -> North America -> location.location.contains_major_portion_of -> United States of America\n# Answer:\nNorth America", "# Reasoning Path:\nHonduras -> location.location.containedby -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nHonduras -> location.location.containedby -> Latin America -> location.location.time_zones -> Central Time Zone\n# Answer:\nLatin America", "# Reasoning Path:\nHonduras -> base.locations.countries.continent -> North America -> base.locations.continents.countries_within -> Bahamas\n# Answer:\nNorth America", "# Reasoning Path:\nHonduras -> base.locations.countries.continent -> North America -> base.locations.continents.countries_within -> Belize\n# Answer:\nNorth America", "# Reasoning Path:\nHonduras -> location.location.containedby -> Latin America -> location.location.contains -> Venezuela\n# Answer:\nLatin America", "# Reasoning Path:\nHonduras -> location.location.containedby -> North America -> base.locations.continents.countries_within -> United States of America\n# Answer:\nNorth America"], "ground_truth": ["Latin America", "Central America", "Americas", "North America"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6153846153846154, "path_precision": 0.8, "path_recall": 0.5, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.actor -> Hayden Christensen\n# Answer:\nHayden Christensen", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.actor -> Hayden Christensen\n# Answer:\nHayden Christensen", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.film -> Star Wars Episode III: Revenge of the Sith\n# Answer:\nStar Wars Episode III: Revenge of the Sith", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.film -> Star Wars Episode II: Attack of the Clones\n# Answer:\nStar Wars Episode II: Attack of the Clones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nThe Making of Star Wars", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.part_of_series -> Jedi Quest\n# Answer:\nPath to Truth", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.author -> Judy Blundell\n# Answer:\nPath to Truth"], "ground_truth": ["Hayden Christensen"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1142", "prediction": ["# Reasoning Path:\nRonaldinho -> soccer.football_player.statistics -> m.0w9hk0b -> soccer.football_player_stats.team -> FC Barcelona\n# Answer:\nFC Barcelona", "# Reasoning Path:\nRonaldinho -> sports.pro_athlete.teams -> m.0j_7v43 -> sports.sports_team_roster.team -> FC Barcelona\n# Answer:\nFC Barcelona", "# Reasoning Path:\nRonaldinho -> soccer.football_player.statistics -> m.0w9db76 -> soccer.football_player_stats.team -> Brazil national football team\n# Answer:\nBrazil national football team", "# Reasoning Path:\nRonaldinho -> sports.pro_athlete.teams -> m.0j3_kkz -> sports.sports_team_roster.team -> Brazil national football team\n# Answer:\nBrazil national football team", "# Reasoning Path:\nRonaldinho -> sports.pro_athlete.teams -> m.0j_7v43 -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nMidfielder", "# Reasoning Path:\nRonaldinho -> people.person.nationality -> Brazil -> sports.sports_team_location.teams -> Brazil national football team\n# Answer:\nBrazil", "# Reasoning Path:\nRonaldinho -> soccer.football_player.statistics -> m.0w9b586 -> soccer.football_player_stats.team -> Clube de Regatas do Flamengo\n# Answer:\nClube de Regatas do Flamengo", "# Reasoning Path:\nRonaldinho -> sports.pro_athlete.teams -> m.011qx677 -> sports.sports_team_roster.team -> Quer\u00e9taro F.C.\n# Answer:\nQuer\u00e9taro F.C.", "# Reasoning Path:\nRonaldinho -> people.person.nationality -> Brazil -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\nBrazil", "# Reasoning Path:\nRonaldinho -> people.person.nationality -> Brazil -> location.country.currency_used -> Brazilian real\n# Answer:\nBrazil"], "ground_truth": ["Brazil national football team", "Clube de Regatas do Flamengo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1143", "prediction": ["# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Astronomer -> people.profession.specialization_of -> Scientist\n# Answer:\nAstronomer", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Werner Heisenberg\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Daniel Faraday\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.0101ddrk\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Mathematician -> base.descriptive_names.names.descriptive_name -> m.01066gkh\n# Answer:\nMathematician", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Astronomer -> common.topic.image -> Johannes Hevelius\n# Answer:\nAstronomer", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Emmett Brown\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> common.topic.image -> Albert Einstein 1947\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.01066smw\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Mathematician -> base.descriptive_names.names.descriptive_name -> m.010f3g_r\n# Answer:\nMathematician"], "ground_truth": ["Solar System", "Kinematics", "Heliocentrism"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1144", "prediction": ["# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Czech Republic\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Austria\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Austria\n# Answer:\nFederal republic", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Poland\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.statistical_region.places_exported_to -> m.049374f -> location.imports_and_exports.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nGermany -> location.statistical_region.places_imported_from -> m.049375c -> location.imports_and_exports.currency -> United States Dollar\n# Answer:\nUnited States Dollar"], "ground_truth": ["Albania", "Tuvalu", "Scotland", "United States of America", "Afghanistan", "Nepal", "Algeria", "Tanzania", "Antigua and Barbuda", "Madagascar"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1145", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.institution -> Harvard Law School\n# Answer:\nHarvard Law School", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkf4 -> education.education.institution -> Stanford University\n# Answer:\nStanford University", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.major_field_of_study -> Law\n# Answer:\nLaw", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkf4 -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.0123vxrw -> education.educational_institution.students_graduates -> m.0123vxqw\n# Answer:\neducation.educational_institution.students_graduates", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0tk1pyb -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0tk1pyb -> film.personal_film_appearance.film -> 8: The Mormon Proposition\n# Answer:\n8: The Mormon Proposition", "# Reasoning Path:\nMitt Romney -> tv.tv_actor.guest_roles -> m.0bvvym_ -> tv.tv_guest_role.episodes_appeared_in -> The Comeback of President Bush\n# Answer:\nThe Comeback of President Bush", "# Reasoning Path:\nMitt Romney -> tv.tv_actor.guest_roles -> m.0bvw9my -> tv.tv_guest_role.episodes_appeared_in -> Mitt Romney, Ryan Sheckler, Jonas Brothers\n# Answer:\nMitt Romney, Ryan Sheckler, Jonas Brothers", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0_gvz4k -> film.personal_film_appearance.film -> Mitt\n# Answer:\nMitt"], "ground_truth": ["Brigham Young University"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1146", "prediction": ["# Reasoning Path:\nCorpse Bride -> film.film.starring -> m.0jwq1q -> film.performance.character -> Victor Van Dort\n# Answer:\nVictor Van Dort", "# Reasoning Path:\nJohnny Depp -> film.actor.film -> m.0jwq1q -> film.performance.character -> Victor Van Dort\n# Answer:\nVictor Van Dort", "# Reasoning Path:\nCorpse Bride -> film.film.starring -> m.03jsyd7 -> film.performance.character -> Elder Gutknecht\n# Answer:\nElder Gutknecht", "# Reasoning Path:\nCorpse Bride -> film.film.starring -> m.0jwq1q -> film.performance.actor -> Johnny Depp\n# Answer:\nfilm.performance.actor", "# Reasoning Path:\nCorpse Bride -> film.film.starring -> m.03l4b_9 -> film.performance.character -> Nell Van Dort\n# Answer:\nNell Van Dort", "# Reasoning Path:\nJohnny Depp -> film.actor.film -> g.11b6tp52w8\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nJohnny Depp -> film.actor.film -> m.0jwq1q -> film.performance.film -> Corpse Bride\n# Answer:\nfilm.performance.film", "# Reasoning Path:\nJohnny Depp -> film.actor.film -> m.0107gr6b -> film.performance.character -> Guy LaPointe\n# Answer:\nGuy LaPointe", "# Reasoning Path:\nCorpse Bride -> film.film.starring -> m.03jsyd7 -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nCorpse Bride -> film.film.starring -> m.03jsyd7 -> film.performance.actor -> Michael Gough\n# Answer:\nMichael Gough"], "ground_truth": ["Victor Van Dort"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1147", "prediction": ["# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72401 -> location.location.contains -> 1211 Stone\n# Answer:\n72401", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72401 -> location.postal_code.country -> United States of America\n# Answer:\n72401", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72401 -> location.location.containedby -> Craighead County\n# Answer:\n72401", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72403 -> common.topic.notable_for -> g.1259zdl3v\n# Answer:\n72403", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72401 -> location.location.contains -> 1222 W Monroe @ West Elementary\n# Answer:\n72401", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72404 -> location.location.contains -> CYS on Stadium\n# Answer:\n72404", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72404 -> location.location.geolocation -> m.03dn5ct\n# Answer:\n72404", "# Reasoning Path:\nJonesboro -> base.wikipedia_infobox.settlement.area_code -> Area code 870 -> location.location.geolocation -> m.0_8fvhm\n# Answer:\nArea code 870", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72401 -> location.location.contains -> 1st Care Occupational Clinic on Highland\n# Answer:\n72401", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72404 -> location.postal_code.country -> United States of America\n# Answer:\n72404"], "ground_truth": ["72403", "72404", "72402", "72401"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.8181818181818182, "path_precision": 0.9, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-1148", "prediction": ["# Reasoning Path:\nS. E. Hinton -> people.person.education -> m.04hd46b -> education.education.institution -> University of Tulsa\n# Answer:\nUniversity of Tulsa", "# Reasoning Path:\nS. E. Hinton -> people.person.education -> m.0n1jntg -> education.education.institution -> Will Rogers High School\n# Answer:\nWill Rogers High School", "# Reasoning Path:\nS. E. Hinton -> book.author.book_editions_published -> The Outsiders (Penguin Classics) -> book.book_edition.book -> The Outsiders\n# Answer:\nThe Outsiders (Penguin Classics)", "# Reasoning Path:\nS. E. Hinton -> book.author.book_editions_published -> The Outsiders (Penguin Classics) -> book.book_edition.number_of_pages -> m.0b3snbp\n# Answer:\nThe Outsiders (Penguin Classics)", "# Reasoning Path:\nS. E. Hinton -> film.actor.film -> m.0t_lwd8 -> film.performance.film -> Rumble Fish\n# Answer:\nRumble Fish", "# Reasoning Path:\nS. E. Hinton -> film.actor.film -> m.0t_lwd8 -> film.performance.character -> Hooker on Strip\n# Answer:\nHooker on Strip", "# Reasoning Path:\nS. E. Hinton -> book.author.book_editions_published -> Hawkes Harbor -> book.book_edition.publisher -> Tor Books\n# Answer:\nHawkes Harbor", "# Reasoning Path:\nS. E. Hinton -> film.actor.film -> m.0vn2n_r -> film.performance.film -> Tex\n# Answer:\nTex", "# Reasoning Path:\nS. E. Hinton -> book.author.book_editions_published -> Hawkes Harbor -> book.book.genre -> Speculative fiction\n# Answer:\nHawkes Harbor", "# Reasoning Path:\nS. E. Hinton -> film.actor.film -> m.0vn2n_r -> film.performance.character -> Mrs. Barnes\n# Answer:\nMrs. Barnes"], "ground_truth": ["University of Tulsa"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1149", "prediction": ["# Reasoning Path:\nFearless Tour -> music.concert_tour.artist -> Taylor Swift -> music.artist.concert_tours -> Taylor Swift Fearless 2009 Tour\n# Answer:\nTaylor Swift", "# Reasoning Path:\nFearless Tour -> music.concert_tour.artist -> Taylor Swift -> music.artist.album -> Speak Now\n# Answer:\nTaylor Swift", "# Reasoning Path:\nFearless Tour -> music.concert_tour.artist -> Taylor Swift -> music.composer.compositions -> Fearless\n# Answer:\nTaylor Swift", "# Reasoning Path:\nFearless Tour -> music.concert_tour.artist -> Taylor Swift -> music.artist.concert_tours -> The 1989 World Tour\n# Answer:\nTaylor Swift", "# Reasoning Path:\nFearless Tour -> music.concert_tour.artist -> Taylor Swift -> music.artist.album -> Fearless\n# Answer:\nTaylor Swift", "# Reasoning Path:\nFearless Tour -> music.concert_tour.artist -> Taylor Swift -> music.artist.concert_tours -> The Red Tour\n# Answer:\nTaylor Swift", "# Reasoning Path:\nFearless Tour -> music.concert_tour.artist -> Taylor Swift -> music.artist.album -> Should've Said No\n# Answer:\nTaylor Swift", "# Reasoning Path:\nFearless Tour -> common.topic.notable_types -> Concert tour -> type.type.expected_by -> Concert Tour\n# Answer:\nConcert tour", "# Reasoning Path:\nFearless Tour -> music.concert_tour.artist -> Taylor Swift -> music.composer.compositions -> Speak Now\n# Answer:\nTaylor Swift", "# Reasoning Path:\nFearless Tour -> common.topic.notable_types -> Concert tour -> type.type.properties -> Concerts\n# Answer:\nConcert tour"], "ground_truth": ["2009-04-23"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.season -> 2006 NFL season\n# Answer:\n2006 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_8 -> sports.sports_award.season -> 2003 NFL season\n# Answer:\n2003 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_j -> sports.sports_award.season -> 2004 NFL season\n# Answer:\n2004 NFL season", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.award -> Super Bowl Most Valuable Player Award\n# Answer:\nSuper Bowl Most Valuable Player Award", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_8 -> sports.sports_award.award -> National Football League Most Valuable Player Award\n# Answer:\nNational Football League Most Valuable Player Award", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_j -> sports.sports_award.award -> National Football League Most Valuable Player Award\n# Answer:\nNational Football League Most Valuable Player Award", "# Reasoning Path:\nPeyton Manning -> people.person.spouse_s -> m.0hpd4nj -> freebase.valuenotation.is_reviewed -> Spouse\n# Answer:\nSpouse", "# Reasoning Path:\nPeyton Manning -> people.person.spouse_s -> m.0hpd4nj -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1151", "prediction": ["# Reasoning Path:\nCharles Darwin -> influence.influence_node.influenced -> Karl Marx -> people.deceased_person.place_of_death -> London\n# Answer:\nKarl Marx", "# Reasoning Path:\nCharles Darwin -> influence.influence_node.influenced -> John Bowlby -> influence.influence_node.influenced -> Mary Ainsworth\n# Answer:\nJohn Bowlby", "# Reasoning Path:\nCharles Darwin -> influence.influence_node.influenced -> Karl Marx -> people.person.profession -> Philosopher\n# Answer:\nKarl Marx", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_death -> Down House -> common.topic.notable_for -> g.12578zv1w\n# Answer:\nDown House", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_death -> Down House -> common.topic.article -> m.02_67b\n# Answer:\nDown House", "# Reasoning Path:\nCharles Darwin -> influence.influence_node.influenced -> Karl Marx -> influence.influence_node.influenced_by -> Adam Smith\n# Answer:\nKarl Marx", "# Reasoning Path:\nCharles Darwin -> influence.influence_node.influenced -> John Bowlby -> influence.influence_node.influenced -> Jenny Aubry\n# Answer:\nJohn Bowlby", "# Reasoning Path:\nCharles Darwin -> influence.influence_node.influenced -> John Bowlby -> people.person.profession -> Psychologist\n# Answer:\nJohn Bowlby", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_death -> Down House -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.0djwmxs\n# Answer:\nDown House", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland"], "ground_truth": ["London", "England", "Shrewsbury", "Downe, Kent"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.14285714285714288, "ans_precission": 0.1, "ans_recall": 0.25, "path_f1": 0.18750000000000003, "path_precision": 0.2, "path_recall": 0.17647058823529413, "path_ans_f1": 0.28571428571428575, "path_ans_precision": 0.2, "path_ans_recall": 0.5}
{"id": "WebQTest-1152", "prediction": ["# Reasoning Path:\nKITT -> film.film_character.portrayed_in_films -> m.0j7ph8m -> film.performance.actor -> William Daniels\n# Answer:\nWilliam Daniels", "# Reasoning Path:\nKITT -> tv.tv_character.appeared_in_tv_program -> m.07spfgl -> tv.regular_tv_appearance.actor -> Val Kilmer\n# Answer:\nVal Kilmer", "# Reasoning Path:\nKITT -> tv.tv_character.appeared_in_tv_program -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nWilliam Daniels", "# Reasoning Path:\nKITT -> film.film_character.portrayed_in_films -> m.0j7ph8m -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nKITT -> tv.tv_character.appeared_in_tv_program -> m.07spfgl -> tv.regular_tv_appearance.series -> Knight Rider\n# Answer:\nKnight Rider", "# Reasoning Path:\nKITT -> tv.tv_character.appeared_in_tv_episodes -> m.09nfwd3 -> tv.tv_guest_role.actor -> William Daniels\n# Answer:\nWilliam Daniels", "# Reasoning Path:\nKITT -> tv.tv_character.appeared_in_tv_program -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 3\n# Answer:\nKnight Rider - Season 3", "# Reasoning Path:\nKITT -> tv.tv_character.appeared_in_tv_program -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 4\n# Answer:\nKnight Rider - Season 4"], "ground_truth": ["William Daniels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1153", "prediction": ["# Reasoning Path:\nEminem -> people.person.ethnicity -> Scotch-Irish American -> people.ethnicity.people -> Miley Cyrus\n# Answer:\nScotch-Irish American", "# Reasoning Path:\nEminem -> people.person.ethnicity -> Scotch-Irish American -> people.ethnicity.people -> John McCain\n# Answer:\nScotch-Irish American", "# Reasoning Path:\nEminem -> music.artist.origin -> Detroit -> location.location.containedby -> United States of America\n# Answer:\nDetroit", "# Reasoning Path:\nEminem -> people.person.ethnicity -> Scotch-Irish American -> people.ethnicity.people -> George W. Bush\n# Answer:\nScotch-Irish American", "# Reasoning Path:\nEminem -> music.artist.origin -> Detroit -> travel.travel_destination.tourist_attractions -> Comerica Park\n# Answer:\nDetroit", "# Reasoning Path:\nEminem -> music.artist.origin -> Warren -> location.location.containedby -> United States of America\n# Answer:\nWarren", "# Reasoning Path:\nEminem -> music.producer.releases_produced -> The Marshall Mathers LP -> music.multipart_release.components -> The Marshall Mathers LP (bonus disc)\n# Answer:\nThe Marshall Mathers LP", "# Reasoning Path:\nEminem -> music.producer.releases_produced -> All or Nothing -> common.topic.notable_for -> g.1258d_62k\n# Answer:\nAll or Nothing", "# Reasoning Path:\nEminem -> music.producer.releases_produced -> The Marshall Mathers LP -> common.topic.notable_types -> Consumer product\n# Answer:\nThe Marshall Mathers LP", "# Reasoning Path:\nEminem -> music.producer.releases_produced -> B.o.B. Presents The Adventures of Bobby Ray -> common.topic.notable_for -> g.125761_vk\n# Answer:\nB.o.B. Presents The Adventures of Bobby Ray"], "ground_truth": ["Marshall Bruce Mathers III"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1154", "prediction": ["# Reasoning Path:\nAmy Grant -> people.person.spouse_s -> m.0w2ygvd -> people.marriage.spouse -> Gary Chapman\n# Answer:\nGary Chapman", "# Reasoning Path:\nAmy Grant -> people.person.spouse_s -> m.0w2ygvd -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nAmy Grant -> people.person.spouse_s -> m.0h4q7j_ -> people.marriage.spouse -> Vince Gill\n# Answer:\nVince Gill", "# Reasoning Path:\nAmy Grant -> people.person.spouse_s -> m.0w2ygvd -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nAmy Grant -> people.person.children -> Corrina Grant Gill -> people.person.parents -> Vince Gill\n# Answer:\nCorrina Grant Gill", "# Reasoning Path:\nAmy Grant -> people.person.children -> Gloria Mills Chapman -> people.person.parents -> Gary Chapman\n# Answer:\nGloria Mills Chapman", "# Reasoning Path:\nAmy Grant -> people.person.spouse_s -> m.0h4q7j_ -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nAmy Grant -> people.person.spouse_s -> m.0h4q7j_ -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nAmy Grant -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAmy Grant -> people.person.children -> Corrina Grant Gill -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nCorrina Grant Gill"], "ground_truth": ["Gary Chapman"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1155", "prediction": ["# Reasoning Path:\nCentral America -> location.location.contains -> La Amistad International Park -> location.location.contains -> Talamancan montane forests\n# Answer:\nLa Amistad International Park", "# Reasoning Path:\nCentral America -> location.location.contains -> Panama -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nPanama", "# Reasoning Path:\nCentral America -> location.location.contains -> La Amistad International Park -> common.topic.notable_for -> g.12596bzjx\n# Answer:\nLa Amistad International Park", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.location.contains -> Belize District\n# Answer:\nBelize", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.location.containedby -> North America\n# Answer:\nBelize", "# Reasoning Path:\nCentral America -> location.location.contains -> Panama -> location.location.partiallycontains -> m.0wg9hnr\n# Answer:\nPanama", "# Reasoning Path:\nCentral America -> location.location.contains -> Panama -> location.location.containedby -> North America\n# Answer:\nPanama", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.location.contains -> Cayo District\n# Answer:\nBelize", "# Reasoning Path:\nCentral America -> meteorology.cyclone_affected_area.cyclones -> 1924 Cuba hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\n1924 Cuba hurricane", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.location.contains -> Corozal District\n# Answer:\nBelize"], "ground_truth": ["El Salvador", "Honduras", "Gran Colombia", "Costa Rica", "Belize", "Guatemala", "Panama"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.40579710144927533, "ans_precission": 0.7, "ans_recall": 0.2857142857142857, "path_f1": 0.40579710144927533, "path_precision": 0.7, "path_recall": 0.2857142857142857, "path_ans_f1": 0.40579710144927533, "path_ans_precision": 0.7, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-1156", "prediction": ["# Reasoning Path:\nJamie Lynn Spears -> people.person.children -> Maddie Briann Aldridge -> people.person.parents -> Casey Aldridge\n# Answer:\nMaddie Briann Aldridge", "# Reasoning Path:\nJamie Lynn Spears -> people.person.children -> Maddie Briann Aldridge -> common.topic.notable_for -> g.125g6_bhr\n# Answer:\nMaddie Briann Aldridge", "# Reasoning Path:\nJamie Lynn Spears -> people.person.children -> Maddie Briann Aldridge -> people.person.nationality -> United States of America\n# Answer:\nMaddie Briann Aldridge", "# Reasoning Path:\nJamie Lynn Spears -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#range -> Person\n# Answer:\nChildren", "# Reasoning Path:\nJamie Lynn Spears -> people.person.place_of_birth -> McComb -> location.location.people_born_here -> Maddie Briann Aldridge\n# Answer:\nMcComb", "# Reasoning Path:\nJamie Lynn Spears -> freebase.valuenotation.is_reviewed -> Children -> type.property.schema -> Person\n# Answer:\nChildren", "# Reasoning Path:\nJamie Lynn Spears -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nJamie Lynn Spears -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#domain -> Person\n# Answer:\nChildren", "# Reasoning Path:\nJamie Lynn Spears -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nJamie Lynn Spears -> people.person.place_of_birth -> McComb -> location.location.people_born_here -> Tra'Vez Taylor\n# Answer:\nMcComb"], "ground_truth": ["Maddie Briann Aldridge"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Canada\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Ontario\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> base.wikipedia_infobox.settlement.area_code -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Canada\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> common.topic.notable_types -> City/Town/Village\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Ontario\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.people_born_here -> Pattie Mallette\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nJuno Awards of 2014"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1160", "prediction": ["# Reasoning Path:\nMichael Jackson -> people.person.parents -> Katherine Jackson -> people.person.nationality -> United States of America\n# Answer:\nKatherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Katherine Jackson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nKatherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Katherine Jackson -> people.person.children -> Tito Jackson\n# Answer:\nKatherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Katherine Jackson -> people.person.children -> Brandon Jackson\n# Answer:\nKatherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Katherine Jackson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nKatherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Katherine Jackson -> people.person.children -> Jackie Jackson\n# Answer:\nKatherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Katherine Jackson -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nKatherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Joe Jackson -> people.family_member.family -> Jackson family\n# Answer:\nJoe Jackson", "# Reasoning Path:\nMichael Jackson -> film.person_or_entity_appearing_in_film.films -> m.010wz0tg -> film.personal_film_appearance.film -> Live at Wembley July 16, 1988\n# Answer:\nLive at Wembley July 16, 1988", "# Reasoning Path:\nMichael Jackson -> music.producer.releases_produced -> Yours Faithfully -> common.topic.notable_types -> Musical Release\n# Answer:\nYours Faithfully"], "ground_truth": ["Katherine Jackson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1161", "prediction": ["# Reasoning Path:\nClemson University -> education.educational_institution.mascot -> Clemson University The Tiger -> common.topic.notable_for -> g.12568xvfh\n# Answer:\nClemson University The Tiger", "# Reasoning Path:\nClemson University -> education.educational_institution.mascot -> Clemson University The Tiger -> common.topic.notable_types -> School mascot\n# Answer:\nClemson University The Tiger", "# Reasoning Path:\nClemson University -> education.educational_institution.mascot -> The Tiger and the Cub -> common.topic.notable_types -> School mascot\n# Answer:\nThe Tiger and the Cub", "# Reasoning Path:\nClemson University -> business.employer.employees -> m.010h0sdv -> business.employment_tenure.person -> James P. Clements\n# Answer:\nJames P. Clements", "# Reasoning Path:\nClemson University -> education.educational_institution.mascot -> The Tiger and the Cub -> common.topic.notable_for -> g.1258wsn3q\n# Answer:\nThe Tiger and the Cub", "# Reasoning Path:\nClemson University -> award.ranked_item.appears_in_ranked_lists -> m.0h6f984 -> award.ranking.list -> US News & World Report: National University Rankings\n# Answer:\nUS News & World Report: National University Rankings", "# Reasoning Path:\nClemson University -> business.employer.employees -> m.05bqvj_ -> business.employment_tenure.person -> Joe L. Kincheloe\n# Answer:\nJoe L. Kincheloe", "# Reasoning Path:\nClemson University -> business.employer.employees -> m.05nf2mp -> business.employment_tenure.person -> Shaka Smart\n# Answer:\nShaka Smart", "# Reasoning Path:\nClemson University -> award.ranked_item.appears_in_ranked_lists -> m.0_6sgsn -> award.ranking.list -> US News & World Report: National University Rankings\n# Answer:\nUS News & World Report: National University Rankings", "# Reasoning Path:\nClemson University -> business.employer.employees -> m.05nf2mp -> business.employment_tenure.title -> Director of Basketball Operations\n# Answer:\nDirector of Basketball Operations"], "ground_truth": ["Clemson University The Tiger", "The Tiger and the Cub"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1163", "prediction": ["# Reasoning Path:\nSouth America -> book.book_subject.works -> Bogota 360 -> book.written_work.subjects -> Colombia\n# Answer:\nBogota 360", "# Reasoning Path:\nSouth America -> location.location.time_zones -> Amazon Time Zone -> time.time_zone.day_dst_begins -> Third Sunday in October\n# Answer:\nAmazon Time Zone", "# Reasoning Path:\nSouth America -> location.location.partially_contains -> Ecuador -> location.location.containedby -> Americas\n# Answer:\nEcuador", "# Reasoning Path:\nSouth America -> location.location.time_zones -> Amazon Time Zone -> time.time_zone.locations_in_this_time_zone -> Quer\u00eancia\n# Answer:\nAmazon Time Zone", "# Reasoning Path:\nSouth America -> location.location.time_zones -> Amazon Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nAmazon Time Zone", "# Reasoning Path:\nSouth America -> location.location.partially_contains -> Overseas France -> location.location.containedby -> France\n# Answer:\nOverseas France", "# Reasoning Path:\nSouth America -> book.book_subject.works -> Bogota 360 -> book.written_work.subjects -> Latin America\n# Answer:\nBogota 360", "# Reasoning Path:\nSouth America -> book.book_subject.works -> Bogota 360 -> common.topic.notable_for -> g.1256hz4vy\n# Answer:\nBogota 360", "# Reasoning Path:\nSouth America -> location.location.time_zones -> Amazon Time Zone -> time.time_zone.locations_in_this_time_zone -> Americas\n# Answer:\nAmazon Time Zone", "# Reasoning Path:\nSouth America -> location.location.time_zones -> Argentina Time Zone -> freebase.valuenotation.has_no_value -> DST offset from UTC\n# Answer:\nArgentina Time Zone"], "ground_truth": ["Amazon rainforest"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1164", "prediction": ["# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nModern art", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Impressionism -> book.book_subject.works -> \u00c9douard Manet\n# Answer:\nImpressionism", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> The Body in Pieces\n# Answer:\nModern art", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Eyewitness: Reports from an Art World in Crisis\n# Answer:\nModern art", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Impressionism -> book.book_subject.works -> Monet\n# Answer:\nImpressionism", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Impressionism -> book.written_work.subjects -> France\n# Answer:\nImpressionism", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Origins, Imitation, Conventions\n# Answer:\nModern art", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> visual_art.art_period_movement.associated_artworks -> Elemental: The Power of Illuminated Love\n# Answer:\nModern art", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Impressionism -> base.schemastaging.context_name.pronunciation -> g.125_m6dv2\n# Answer:\nImpressionism", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.art_forms -> Painting -> base.descriptive_names.names.descriptive_name -> m.0109p390\n# Answer:\nPainting"], "ground_truth": ["Modern art", "Impressionism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1166", "prediction": ["# Reasoning Path:\nIraq War -> base.argumentmaps.motivation_s.motivation -> m.05z179k -> base.argumentmaps.motivation.intention -> Withdrawal of U.S. troops from Iraq\n# Answer:\nWithdrawal of U.S. troops from Iraq", "# Reasoning Path:\nIraq War -> book.book_subject.works -> Iraq -> military.military_combatant.military_conflicts -> m.043wpnt\n# Answer:\nIraq", "# Reasoning Path:\nIraq War -> base.fight.subject_of_protest.protests_about_this -> 20 March 2003 anti-war protest -> time.event.locations -> Global\n# Answer:\n20 March 2003 anti-war protest", "# Reasoning Path:\nIraq War -> base.argumentmaps.motivation_s.motivation -> m.05z179k -> base.argumentmaps.motivation.type_of_motivation -> Resistance movement\n# Answer:\nResistance movement", "# Reasoning Path:\nIraq War -> book.book_subject.works -> Iraq -> military.military_combatant.military_conflicts -> m.04ftyvf\n# Answer:\nIraq", "# Reasoning Path:\nIraq War -> book.book_subject.works -> Iraq -> book.written_work.subjects -> United States of America\n# Answer:\nIraq", "# Reasoning Path:\nIraq War -> base.fight.subject_of_protest.protests_about_this -> 20 March 2003 anti-war protest -> common.topic.article -> m.082shh\n# Answer:\n20 March 2003 anti-war protest", "# Reasoning Path:\nIraq War -> base.fight.subject_of_protest.protests_about_this -> Berkeley Marine Corps Recruiting Center protests -> common.topic.notable_for -> g.1255gcll3\n# Answer:\nBerkeley Marine Corps Recruiting Center protests", "# Reasoning Path:\nIraq War -> book.book_subject.works -> Iraq -> common.topic.notable_types -> Book\n# Answer:\nIraq", "# Reasoning Path:\nIraq War -> base.fight.subject_of_protest.protests_about_this -> Bring Them Home Now Tour -> common.topic.notable_types -> Event\n# Answer:\nBring Them Home Now Tour"], "ground_truth": ["2003-03-20"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1168", "prediction": ["# Reasoning Path:\nKurt Cobain -> music.guitarist.guitars_played -> Fender Jag-Stang -> common.topic.article -> m.047q4q\n# Answer:\nFender Jag-Stang", "# Reasoning Path:\nKurt Cobain -> music.guitarist.guitars_played -> Fender Jag-Stang -> music.guitar.brand -> Fender Musical Instruments Corporation\n# Answer:\nFender Jag-Stang", "# Reasoning Path:\nKurt Cobain -> music.group_member.instruments_played -> Guitar -> music.instrument.variation -> Bass guitar\n# Answer:\nGuitar", "# Reasoning Path:\nKurt Cobain -> music.group_member.instruments_played -> Electric guitar -> music.instrument.family -> Guitar\n# Answer:\nElectric guitar", "# Reasoning Path:\nKurt Cobain -> music.group_member.instruments_played -> Guitar -> music.instrument.variation -> Fretless guitar\n# Answer:\nGuitar", "# Reasoning Path:\nKurt Cobain -> music.group_member.instruments_played -> Guitar -> common.topic.subject_of -> Sonic Sense\n# Answer:\nGuitar", "# Reasoning Path:\nKurt Cobain -> music.group_member.instruments_played -> Electric guitar -> interests.hobby.people_with_this_hobby -> Tony Edward\n# Answer:\nElectric guitar", "# Reasoning Path:\nKurt Cobain -> common.topic.webpage -> m.09wjrjt -> common.webpage.resource -> Eddie Vedder's solo tour: What other frontmen do you want to see without their bands?\n# Answer:\nEddie Vedder's solo tour: What other frontmen do you want to see without their bands?", "# Reasoning Path:\nKurt Cobain -> music.group_member.instruments_played -> Guitar -> book.book_subject.works -> Clapton's Guitar: Watching Wayne Henderson Build the Perfect Instrument\n# Answer:\nGuitar", "# Reasoning Path:\nKurt Cobain -> music.group_member.instruments_played -> Electric guitar -> common.topic.notable_types -> Interest\n# Answer:\nElectric guitar"], "ground_truth": ["Fender Jag-Stang"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1169", "prediction": ["# Reasoning Path:\nJennifer Lawrence -> film.actor.film -> m.0s912lb -> film.performance.character -> Raven / Mystique\n# Answer:\nRaven / Mystique", "# Reasoning Path:\nJennifer Lawrence -> film.actor.film -> m.010sqn6t -> film.performance.character -> Raven / Mystique\n# Answer:\nRaven / Mystique", "# Reasoning Path:\nJennifer Lawrence -> film.actor.film -> m.0cgr66b -> film.performance.character -> Raven / Mystique\n# Answer:\nRaven / Mystique", "# Reasoning Path:\nJennifer Lawrence -> film.actor.film -> m.0s912lb -> film.performance.film -> X-Men: Days of Future Past\n# Answer:\nX-Men: Days of Future Past", "# Reasoning Path:\nJennifer Lawrence -> film.actor.film -> m.010sqn6t -> film.performance.film -> X-Men: Apocalypse\n# Answer:\nX-Men: Apocalypse", "# Reasoning Path:\nJennifer Lawrence -> film.actor.film -> m.0cgr66b -> film.performance.film -> X-Men: First Class\n# Answer:\nX-Men: First Class", "# Reasoning Path:\nJennifer Lawrence -> award.award_winner.awards_won -> m.0101mk_y -> award.award_honor.ceremony -> 2014 Kids' Choice Awards\n# Answer:\n2014 Kids' Choice Awards", "# Reasoning Path:\nJennifer Lawrence -> award.award_winner.awards_won -> m.0101mk_y -> award.award_honor.honored_for -> The Hunger Games: Catching Fire\n# Answer:\nThe Hunger Games: Catching Fire", "# Reasoning Path:\nJennifer Lawrence -> award.award_winner.awards_won -> m.0101mld6 -> award.award_honor.honored_for -> The Hunger Games: Catching Fire\n# Answer:\nThe Hunger Games: Catching Fire", "# Reasoning Path:\nJennifer Lawrence -> award.award_winner.awards_won -> m.0101mk_y -> award.award_honor.award -> Kids' Choice Award for Favorite Movie Actress\n# Answer:\nKids' Choice Award for Favorite Movie Actress"], "ground_truth": ["Raven / Mystique"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1170", "prediction": ["# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> government.government_position_held.office_holder -> Tim Kaine\n# Answer:\nTim Kaine", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.04g7cb8 -> government.government_position_held.office_holder -> Gerald L. Baliles\n# Answer:\nGerald L. Baliles", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nUnited States Senate", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.04g7cmm -> government.government_position_held.office_holder -> Gerald L. Baliles\n# Answer:\nGerald L. Baliles", "# Reasoning Path:\nVirginia -> location.location.containedby -> United States of America -> meteorology.cyclone_affected_area.cyclones -> Hurricane Isabel\n# Answer:\nUnited States of America", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.04g7cb8 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.04g7cmm -> government.government_position_held.basic_title -> Assistant Attorney General\n# Answer:\nAssistant Attorney General", "# Reasoning Path:\nVirginia -> location.location.containedby -> Charles Irving Thornton Tombstone -> location.location.geolocation -> m.0zjvs72\n# Answer:\nCharles Irving Thornton Tombstone", "# Reasoning Path:\nVirginia -> location.location.partiallycontains -> m.0w_v1r2 -> location.partial_containment_relationship.partially_contains -> Skeet Rock Knob\n# Answer:\nSkeet Rock Knob"], "ground_truth": ["Bob McDonnell"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1171", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.parents -> Shmi Skywalker -> film.film_character.portrayed_in_films -> m.04kc989\n# Answer:\nShmi Skywalker", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.parents -> Shmi Skywalker -> fictional_universe.fictional_character.gender -> Female\n# Answer:\nShmi Skywalker", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.children -> Leia Organa -> fictional_universe.fictional_character.parents -> Padm\u00e9 Amidala\n# Answer:\nLeia Organa", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nThe Making of Star Wars", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zsqt -> film.performance.film -> The Making of Star Wars\n# Answer:\nThe Making of Star Wars", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.parents -> Shmi Skywalker -> film.film_character.portrayed_in_films -> m.0h0y2bb\n# Answer:\nShmi Skywalker", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.children -> Leia Organa -> book.book_character.appears_in_book -> The Truce at Bakura\n# Answer:\nLeia Organa"], "ground_truth": ["Matt Lanter", "Zac Efron", "Dr. Smoov", "James Earl Jones", "Abraham Benrubi"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.20000000000000004, "ans_precission": 0.2, "ans_recall": 0.2, "path_f1": 0.1904761904761905, "path_precision": 0.2, "path_recall": 0.18181818181818182, "path_ans_f1": 0.20000000000000004, "path_ans_precision": 0.2, "path_ans_recall": 0.2}
{"id": "WebQTest-1173", "prediction": ["# Reasoning Path:\nWalton County -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nWalton County -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Georgia\n# Answer:\nUnited States of America", "# Reasoning Path:\nWalton County -> location.location.containedby -> Georgia -> location.location.containedby -> United States, with Territories\n# Answer:\nGeorgia", "# Reasoning Path:\nWalton County -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Georgia\n# Answer:\nUnited States of America", "# Reasoning Path:\nWalton County -> location.location.containedby -> Georgia -> location.location.containedby -> United States of America\n# Answer:\nGeorgia", "# Reasoning Path:\nWalton County -> location.location.containedby -> Georgia -> location.us_state.capital -> Atlanta\n# Answer:\nGeorgia", "# Reasoning Path:\nWalton County -> location.location.containedby -> Atlanta-Sandy Springs-Marietta, GA Metropolitan Statistical Area -> location.location.containedby -> Georgia\n# Answer:\nAtlanta-Sandy Springs-Marietta, GA Metropolitan Statistical Area", "# Reasoning Path:\nWalton County -> location.statistical_region.population -> g.11b674j1z2\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nWalton County -> location.statistical_region.population -> g.11x1cgp3h\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nWalton County -> location.statistical_region.population -> g.11x1d4n0p\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["Georgia", "Atlanta-Sandy Springs-Marietta, GA Metropolitan Statistical Area"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1174", "prediction": ["# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.medical_specialties -> Oncology\n# Answer:\nCancer", "# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease_cause.diseases -> Erythema nodosum\n# Answer:\nCancer", "# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Cancer -> medicine.risk_factor.diseases -> Anemia\n# Answer:\nCancer", "# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Lymphoma -> medicine.risk_factor.diseases -> Intussusception\n# Answer:\nLymphoma", "# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease_cause.diseases -> Kyphosis\n# Answer:\nCancer", "# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Lymphoma -> common.topic.notable_types -> Cause Of Death\n# Answer:\nLymphoma", "# Reasoning Path:\nJacqueline Kennedy Onassis -> opera.opera_character.operas_appears_in -> m.0dm2d1m -> opera.opera_character_voice.opera -> Jackie O\n# Answer:\nJackie O", "# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Lymphoma -> medicine.disease.symptoms -> Thrombocytopenia\n# Answer:\nLymphoma", "# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Lymphoma -> medicine.disease.symptoms -> Abdominal mass\n# Answer:\nLymphoma", "# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Lymphoma -> medicine.disease.symptoms -> Anemia\n# Answer:\nLymphoma"], "ground_truth": ["Lymphoma"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1175", "prediction": ["# Reasoning Path:\nLauren Conrad -> people.person.education -> m.0gx3gdb -> education.education.institution -> Laguna Beach High School\n# Answer:\nLaguna Beach High School", "# Reasoning Path:\nLauren Conrad -> people.person.education -> m.040zfb0 -> education.education.major_field_of_study -> Fashion design\n# Answer:\nFashion design", "# Reasoning Path:\nLauren Conrad -> people.person.education -> m.04hyzwc -> education.education.major_field_of_study -> Fashion design\n# Answer:\nFashion design", "# Reasoning Path:\nLauren Conrad -> base.popstra.celebrity.friendship -> m.063g5hk -> base.popstra.friendship.participant -> Brody Jenner\n# Answer:\nBrody Jenner", "# Reasoning Path:\nLauren Conrad -> people.person.education -> m.040zfb0 -> education.education.institution -> Fashion Institute of Design & Merchandising\n# Answer:\nFashion Institute of Design & Merchandising", "# Reasoning Path:\nLauren Conrad -> base.popstra.celebrity.dated -> m.063t50c -> base.popstra.dated.participant -> Cisco Adler\n# Answer:\nCisco Adler", "# Reasoning Path:\nLauren Conrad -> base.popstra.celebrity.friendship -> m.063tm27 -> base.popstra.friendship.participant -> Audrina Patridge\n# Answer:\nAudrina Patridge", "# Reasoning Path:\nLauren Conrad -> people.person.education -> m.04hyzwc -> education.education.institution -> Academy of Art University\n# Answer:\nAcademy of Art University", "# Reasoning Path:\nLauren Conrad -> base.popstra.celebrity.friendship -> m.064v2hh -> base.popstra.friendship.participant -> Heidi Montag\n# Answer:\nHeidi Montag", "# Reasoning Path:\nLauren Conrad -> base.popstra.celebrity.dated -> m.064v2gv -> base.popstra.dated.participant -> Kyle Howard\n# Answer:\nKyle Howard"], "ground_truth": ["Fashion design"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1176", "prediction": ["# Reasoning Path:\nPennsylvania -> location.location.time_zones -> Eastern Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nPennsylvania -> location.location.time_zones -> UTC\u221205:00 -> common.topic.notable_types -> Time Zone\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.title -> Director, Office of Health Care Reform\n# Answer:\nDirector, Office of Health Care Reform", "# Reasoning Path:\nPennsylvania -> location.location.time_zones -> UTC\u221205:00 -> common.topic.notable_for -> g.125fnb_r1\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.person -> Rosemarie B. Greco\n# Answer:\nRosemarie B. Greco", "# Reasoning Path:\nPennsylvania -> location.location.time_zones -> UTC\u221205:00 -> common.topic.image -> Timezones2008 UTC-5\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nPennsylvania -> common.topic.webpage -> m.04lsvtk -> common.webpage.resource -> m.0blz_lk\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nPennsylvania -> common.topic.webpage -> m.09wcxfl -> common.webpage.resource -> Exclusive: Jon Gosselin tells EW 'I am not guilty of cheating on my wife'\n# Answer:\nExclusive: Jon Gosselin tells EW 'I am not guilty of cheating on my wife'", "# Reasoning Path:\nPennsylvania -> common.topic.webpage -> m.04lsvtk -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nPennsylvania -> common.topic.webpage -> m.09wcyd_ -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Eastern Time Zone", "UTC\u221205:00"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1177", "prediction": ["# Reasoning Path:\nMario Lopez -> people.person.education -> m.0g_9bvg -> education.education.institution -> Chula Vista High School\n# Answer:\nChula Vista High School", "# Reasoning Path:\nMario Lopez -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nMario Lopez -> film.actor.film -> m.03jq9tb -> film.performance.film -> Colors\n# Answer:\nColors", "# Reasoning Path:\nMario Lopez -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nMario Lopez -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nMale", "# Reasoning Path:\nMario Lopez -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Acute myeloid leukemia\n# Answer:\nMale", "# Reasoning Path:\nMario Lopez -> people.person.gender -> Male -> biology.hybrid_parent_gender.hybrids -> m.0blp580\n# Answer:\nMale", "# Reasoning Path:\nMario Lopez -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_98hd\n# Answer:\nMale", "# Reasoning Path:\nMario Lopez -> film.actor.film -> m.03jq9tb -> film.performance.character -> Felipe's Friend\n# Answer:\nFelipe's Friend", "# Reasoning Path:\nMario Lopez -> film.actor.film -> m.03l8jfc -> film.performance.film -> A Crack in the Floor\n# Answer:\nA Crack in the Floor"], "ground_truth": ["Chula Vista High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1178", "prediction": ["# Reasoning Path:\nTupac Shakur -> people.person.place_of_birth -> East Harlem -> location.location.containedby -> New York City\n# Answer:\nEast Harlem", "# Reasoning Path:\nTupac Shakur -> people.person.place_of_birth -> East Harlem -> location.location.people_born_here -> Lumidee\n# Answer:\nEast Harlem", "# Reasoning Path:\nTupac Shakur -> people.person.place_of_birth -> East Harlem -> location.location.containedby -> Manhattan\n# Answer:\nEast Harlem", "# Reasoning Path:\nTupac Shakur -> people.person.place_of_birth -> East Harlem -> location.neighborhood.neighborhood_of -> New York City\n# Answer:\nEast Harlem", "# Reasoning Path:\nTupac Shakur -> people.person.place_of_birth -> East Harlem -> location.location.containedby -> New York metropolitan area\n# Answer:\nEast Harlem", "# Reasoning Path:\nTupac Shakur -> people.person.place_of_birth -> East Harlem -> location.neighborhood.neighborhood_of -> Manhattan\n# Answer:\nEast Harlem", "# Reasoning Path:\nTupac Shakur -> people.person.place_of_birth -> East Harlem -> location.neighborhood.neighborhood_of -> New York metropolitan area\n# Answer:\nEast Harlem", "# Reasoning Path:\nTupac Shakur -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nTupac Shakur -> people.person.education -> m.0h35g85 -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nTupac Shakur -> people.person.education -> m.0h35g85 -> education.education.institution -> Baltimore School for the Arts\n# Answer:\nBaltimore School for the Arts"], "ground_truth": ["East Harlem"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1179", "prediction": ["# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Darth Vader -> film.film_character.portrayed_in_films -> m.02nv74t\n# Answer:\nDarth Vader", "# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Darth Vader -> film.film_character.portrayed_in_films -> m.01xpnt2\n# Answer:\nDarth Vader", "# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Darth Vader -> film.film_character.portrayed_in_films -> m.0235q84\n# Answer:\nDarth Vader", "# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Darth Vader -> fictional_universe.fictional_character.powers_or_abilities -> Force\n# Answer:\nDarth Vader", "# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Anoat -> common.topic.notable_for -> g.1yl5yk50h\n# Answer:\nAnoat", "# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Darth Vader -> book.book_character.appears_in_book -> Path to Truth\n# Answer:\nDarth Vader", "# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ansion -> common.topic.image -> Ansion_02.JPG\n# Answer:\nAnsion", "# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Darth Vader -> fictional_universe.fictional_character.powers_or_abilities -> Levitation\n# Answer:\nDarth Vader", "# Reasoning Path:\nGeorge Lucas -> award.award_winner.awards_won -> m.010x01cz -> award.award_honor.award -> Daytime Emmy Award for Outstanding Special Class Animated Program\n# Answer:\nDaytime Emmy Award for Outstanding Special Class Animated Program", "# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Darth Vader -> fictional_universe.fictional_character.powers_or_abilities -> Telepathy\n# Answer:\nDarth Vader"], "ground_truth": ["Baron Papanoida"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z973l -> military.military_combatant_group.combatants -> Iraq\n# Answer:\nIraq", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> France\n# Answer:\nFrance", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.04fvd6y -> military.military_combatant_group.combatants -> Saudi Arabia\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> military.military_combatant.military_conflicts -> m.04fvd6y\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> military.military_combatant.military_conflicts -> m.04ywz3n\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> royalty.kingdom.rulers -> Abdullah of Saudi Arabia\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wphl -> military.casualties.combatant -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> military.military_combatant.military_conflicts -> m.05cklfk\n# Answer:\nSaudi Arabia"], "ground_truth": ["Argentina", "United States of America", "Saudi Arabia", "France", "Australia", "United Kingdom", "Iraq"], "ans_acc": 0.7142857142857143, "ans_hit": 1, "ans_f1": 0.8333333333333333, "ans_precission": 1.0, "ans_recall": 0.7142857142857143, "path_f1": 0.15810276679841895, "path_precision": 0.8, "path_recall": 0.08771929824561403, "path_ans_f1": 0.8333333333333333, "path_ans_precision": 1.0, "path_ans_recall": 0.7142857142857143}
{"id": "WebQTest-1180", "prediction": ["# Reasoning Path:\nSarah Dessen -> film.film_story_contributor.film_story_credits -> How to Deal -> common.topic.notable_types -> Film\n# Answer:\nHow to Deal", "# Reasoning Path:\nSarah Dessen -> film.film_story_contributor.film_story_credits -> How to Deal -> film.film.runtime -> m.0jzmqx\n# Answer:\nHow to Deal", "# Reasoning Path:\nSarah Dessen -> book.author.works_written -> The Moon and More -> common.topic.notable_types -> Book\n# Answer:\nThe Moon and More", "# Reasoning Path:\nSarah Dessen -> book.author.works_written -> Dreamland -> book.written_work.subjects -> Identity\n# Answer:\nDreamland", "# Reasoning Path:\nSarah Dessen -> book.author.works_written -> Along for the Ride -> book.book.characters -> Adam\n# Answer:\nAlong for the Ride", "# Reasoning Path:\nSarah Dessen -> book.author.book_editions_published -> Crazy Moon. -> book.book_edition.publisher -> Dtv\n# Answer:\nCrazy Moon.", "# Reasoning Path:\nSarah Dessen -> book.author.works_written -> Along for the Ride -> book.written_work.original_language -> English Language\n# Answer:\nAlong for the Ride", "# Reasoning Path:\nSarah Dessen -> book.author.book_editions_published -> Crazy Moon. -> book.publication.contents -> m.011k7lfy\n# Answer:\nCrazy Moon.", "# Reasoning Path:\nSarah Dessen -> book.author.works_written -> Along for the Ride -> book.book.characters -> Auden Penelope West\n# Answer:\nAlong for the Ride", "# Reasoning Path:\nSarah Dessen -> book.author.book_editions_published -> Dreamland -> book.book.genre -> Young-adult fiction\n# Answer:\nDreamland"], "ground_truth": ["How to Deal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1181", "prediction": ["# Reasoning Path:\nHawaii -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nKauai County -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nKauai -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nHawaiian Islands -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nHawaiian Islands -> location.location.contains -> Hawaii -> location.location.time_zones -> Hawaii-Aleutian Time Zone\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nHawaiian Islands -> travel.travel_destination.tourist_attractions -> USS Missouri (BB-63) -> location.location.containedby -> Pearl Harbor\n# Answer:\nUSS Missouri (BB-63)", "# Reasoning Path:\nKauai County -> location.us_county.hud_county_place -> Wailua -> location.location.time_zones -> Hawaii-Aleutian Time Zone\n# Answer:\nWailua", "# Reasoning Path:\nKauai -> common.topic.image -> Kauai from space oriented -> common.image.size -> m.02brp1k\n# Answer:\nKauai from space oriented", "# Reasoning Path:\nHawaiian Islands -> travel.travel_destination.tourist_attractions -> USS Missouri (BB-63) -> boats.ship.ship_class -> Iowa-class battleship\n# Answer:\nUSS Missouri (BB-63)"], "ground_truth": ["Hawaii-Aleutian Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-1183", "prediction": ["# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.place_of_burial -> Westwood Village Memorial Park Cemetery -> location.location.geolocation -> m.0cqx58y\n# Answer:\nWestwood Village Memorial Park Cemetery", "# Reasoning Path:\nFarrah Fawcett -> common.topic.notable_for -> g.125f8rws1\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.place_of_burial -> Westwood Village Memorial Park Cemetery -> common.topic.notable_for -> g.1256km_vm\n# Answer:\nWestwood Village Memorial Park Cemetery", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.place_of_burial -> Westwood Village Memorial Park Cemetery -> common.topic.article -> m.018mmb\n# Answer:\nWestwood Village Memorial Park Cemetery", "# Reasoning Path:\nFarrah Fawcett -> common.topic.webpage -> m.0773vxj -> common.webpage.resource -> TV and Movie Trivia Tribute\n# Answer:\nTV and Movie Trivia Tribute", "# Reasoning Path:\nFarrah Fawcett -> common.topic.webpage -> m.0773vxj -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nFarrah Fawcett -> common.topic.webpage -> m.08wwplb -> common.webpage.resource -> Farrah Fawcett: 16 highlights\n# Answer:\nFarrah Fawcett: 16 highlights", "# Reasoning Path:\nFarrah Fawcett -> common.topic.webpage -> m.094039t -> common.webpage.resource -> '70s Comebacks of the Week\n# Answer:\n'70s Comebacks of the Week", "# Reasoning Path:\nFarrah Fawcett -> common.topic.webpage -> m.08wwplb -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nFarrah Fawcett -> common.topic.webpage -> m.094039t -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Westwood Village Memorial Park Cemetery"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1185", "prediction": ["# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> influence.influence_node.influenced_by -> Charles Dickens\n# Answer:\nWilliam Faulkner", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> influence.influence_node.influenced_by -> Edgar Allan Poe\n# Answer:\nWilliam Faulkner", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> influence.influence_node.influenced -> Toni Morrison\n# Answer:\nWilliam Faulkner", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> Robert Burns -> influence.influence_node.influenced -> J. D. Salinger\n# Answer:\nRobert Burns", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> influence.influence_node.influenced_by -> Herman Melville\n# Answer:\nWilliam Faulkner", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> people.person.gender -> Male\n# Answer:\nWilliam Faulkner", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> Sherwood Anderson -> influence.influence_node.influenced_by -> Mark Twain\n# Answer:\nSherwood Anderson", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> Robert Burns -> influence.influence_node.influenced -> James Joyce\n# Answer:\nRobert Burns", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> Sherwood Anderson -> influence.influence_node.influenced_by -> Walt Whitman\n# Answer:\nSherwood Anderson", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> Sherwood Anderson -> influence.influence_node.influenced -> William Faulkner\n# Answer:\nSherwood Anderson"], "ground_truth": ["Sherwood Anderson", "Robert Burns", "Thomas Malory", "William Faulkner"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.8571428571428571, "path_precision": 1.0, "path_recall": 0.75, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-1186", "prediction": ["# Reasoning Path:\nCuba -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Venezuela\n# Answer:\nSocialist state", "# Reasoning Path:\nCuba -> location.country.form_of_government -> Socialist state -> common.topic.notable_types -> Form of Government\n# Answer:\nSocialist state", "# Reasoning Path:\nCuba -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Sri Lanka\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nCuba -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Soviet Union\n# Answer:\nSocialist state", "# Reasoning Path:\nCuba -> location.country.form_of_government -> Semi-presidential system -> common.topic.notable_types -> Form of Government\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nCuba -> book.book_subject.works -> A simple Habana melody -> book.written_work.subjects -> History\n# Answer:\nA simple Habana melody", "# Reasoning Path:\nCuba -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc38hmp\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nCuba -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Martinique\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nCuba -> location.country.form_of_government -> Republic -> common.topic.notable_types -> Form of Government\n# Answer:\nRepublic", "# Reasoning Path:\nCuba -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc3d68y\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita"], "ground_truth": ["Unitary state", "Republic", "Socialist state", "Semi-presidential system"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7241379310344827, "ans_precission": 0.7, "ans_recall": 0.75, "path_f1": 0.7241379310344827, "path_precision": 0.7, "path_recall": 0.75, "path_ans_f1": 0.7241379310344827, "path_ans_precision": 0.7, "path_ans_recall": 0.75}
{"id": "WebQTest-1187", "prediction": ["# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> United States of America -> base.locations.countries.continent -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.location.containedby -> North America\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Kingdom of Great Britain -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nKingdom of Great Britain", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Kingdom of Great Britain -> location.country.capital -> London\n# Answer:\nKingdom of Great Britain", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> New Zealand -> location.country.languages_spoken -> Standard Chinese\n# Answer:\nNew Zealand", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Kingdom of Great Britain -> location.location.containedby -> British Isles\n# Answer:\nKingdom of Great Britain", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Australia -> sports.sport_country.multi_event_tournaments_participated_in -> 2010 Commonwealth Games\n# Answer:\nAustralia", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> South Africa -> location.location.containedby -> Africa\n# Answer:\nSouth Africa", "# Reasoning Path:\nEnglish Language -> education.school_category.schools_of_this_kind -> Piers Midwinter -> people.person.quotations -> Learn with a NEST. Please be my GUEST. I will give you my BEST,\n# Answer:\nPiers Midwinter"], "ground_truth": ["Montserrat", "Mandatory Palestine", "Hong Kong", "Republic of Ireland", "Brunei", "South Africa", "South Yemen", "Cura\u00e7ao", "Sierra Leone", "Kenya", "Honduras", "Gazankulu", "Pakistan", "Cameroon", "Tuvalu", "Transkei", "Cyprus", "Isle of Man", "Saint Lucia", "Vatican City", "China", "Japan", "Vanuatu", "Sri Lanka", "Bonaire", "Rwanda", "Indonesia", "Samoa", "Papua New Guinea", "Fiji", "Territory of Papua and New Guinea", "Antigua and Barbuda", "Zimbabwe", "Swaziland", "England", "Ethiopia", "Uganda", "Nigeria", "Tanzania", "Turks and Caicos Islands", "Marshall Islands", "Belize", "Guam", "Gibraltar", "Australia", "Philippines", "United Kingdom", "Qatar", "Laos", "Malaysia", "Cook Islands", "New Zealand", "Saint Vincent and the Grenadines", "Kiribati", "Kingdom of Great Britain", "United States of America", "Bahamas", "Sudan", "India", "Lesotho", "Guyana", "Israel", "Puerto Rico", "Bangladesh", "Jersey", "Jordan", "Bermuda", "Malta", "Singapore", "Cayman Islands", "Canada", "Gambia", "Saint Kitts and Nevis", "Namibia", "Dominica", "Nauru", "Ghana", "Liberia", "Timor-Leste", "Zambia", "Barbados", "Tokelau", "State of Palestine", "Grenada", "Botswana", "Wales", "Territory of New Guinea"], "ans_acc": 0.06896551724137931, "ans_hit": 1, "ans_f1": 0.12811387900355872, "ans_precission": 0.9, "ans_recall": 0.06896551724137931, "path_f1": 0.10723860589812331, "path_precision": 0.8, "path_recall": 0.05747126436781609, "path_ans_f1": 0.12811387900355872, "path_ans_precision": 0.9, "path_ans_recall": 0.06896551724137931}
{"id": "WebQTest-1189", "prediction": ["# Reasoning Path:\nItaly -> location.country.capital -> Rome -> travel.travel_destination.tourist_attractions -> Vatican City\n# Answer:\nRome", "# Reasoning Path:\nItaly -> location.country.capital -> Rome -> location.location.time_zones -> Central European Time Zone\n# Answer:\nRome", "# Reasoning Path:\nItaly -> location.country.capital -> Rome -> periodicals.newspaper_circulation_area.newspapers -> Liberazione\n# Answer:\nRome", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.capital -> Rome -> periodicals.newspaper_circulation_area.newspapers -> Il Messaggero\n# Answer:\nRome", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37cxr\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.capital -> Rome -> periodicals.newspaper_circulation_area.newspapers -> L'Unit\u00e0\n# Answer:\nRome", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Nearco -> biology.organism.sex -> Male\n# Answer:\nNearco", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Tenerani -> base.thoroughbredracing.thoroughbred_racehorse.foal_if_this_horse_is_sire -> Ribot\n# Answer:\nTenerani"], "ground_truth": ["Rome"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.0tly67c -> tv.regular_tv_appearance.actor -> Doc Shaw\n# Answer:\nDoc Shaw", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nBrenda Song", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.0tly67c -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 2\n# Answer:\nThe Suite Life on Deck - Season 2", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sglsdh -> award.award_nomination.award_nominee -> Cole Sprouse\n# Answer:\nCole Sprouse", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.0tly67c -> tv.regular_tv_appearance.character -> Marcus Little\n# Answer:\nMarcus Little", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.actor -> Phill Lewis\n# Answer:\nPhill Lewis", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 2\n# Answer:\nThe Suite Life on Deck - Season 2", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sglsdh -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Actor\n# Answer:\nKids' Choice Award for Favorite TV Actor", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgl70y -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Show\n# Answer:\nKids' Choice Award for Favorite TV Show", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 1\n# Answer:\nThe Suite Life on Deck - Season 1"], "ground_truth": ["Brenda Song"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1191", "prediction": ["# Reasoning Path:\nBahamas -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Barbados\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Belize\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Bermuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.country.languages_spoken -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Barbados\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.statistical_region.co2_emissions_per_capita -> g.1245_22wy\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nBahamas -> location.country.languages_spoken -> Bahamas Creole English Language -> common.topic.notable_types -> Human Language\n# Answer:\nBahamas Creole English Language", "# Reasoning Path:\nBahamas -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Belize\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.country.official_language -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language"], "ground_truth": ["English Language", "Bahamas Creole English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1192", "prediction": ["# Reasoning Path:\nGeorge Michael -> people.person.education -> m.0h3d9q4 -> education.education.institution -> Bushey Meads School\n# Answer:\nBushey Meads School", "# Reasoning Path:\nGeorge Michael -> music.artist.track_contributions -> m.010__qx2 -> music.track_contribution.track -> The First Time Ever I Saw Your Face\n# Answer:\nThe First Time Ever I Saw Your Face", "# Reasoning Path:\nGeorge Michael -> music.lyricist.lyrics_written -> A Last Request (I Want Your Sex, Part III) -> common.topic.notable_for -> g.1z2sq43ys\n# Answer:\nA Last Request (I Want Your Sex, Part III)", "# Reasoning Path:\nGeorge Michael -> music.artist.track_contributions -> m.010__qx2 -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nGeorge Michael -> music.artist.track_contributions -> m.010_v1d8 -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nGeorge Michael -> music.lyricist.lyrics_written -> A Different Corner -> common.topic.notable_for -> g.125blwbtm\n# Answer:\nA Different Corner", "# Reasoning Path:\nGeorge Michael -> music.lyricist.lyrics_written -> A Last Request (I Want Your Sex, Part III) -> common.topic.notable_types -> Composition\n# Answer:\nA Last Request (I Want Your Sex, Part III)", "# Reasoning Path:\nGeorge Michael -> music.lyricist.lyrics_written -> A Ray of Sunshine -> music.composition.language -> English Language\n# Answer:\nA Ray of Sunshine", "# Reasoning Path:\nGeorge Michael -> music.artist.track_contributions -> m.010_t6xt -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nGeorge Michael -> music.lyricist.lyrics_written -> A Ray of Sunshine -> common.topic.notable_for -> g.1yp3dsy7d\n# Answer:\nA Ray of Sunshine"], "ground_truth": ["Bushey Meads School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1194", "prediction": ["# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.0k061tt -> soccer.football_goal.point_awarded_to -> Portugal national football team\n# Answer:\nPortugal national football team", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w9gdcb -> soccer.football_player_stats.team -> Sporting-soc Desportiva De\n# Answer:\nSporting-soc Desportiva De", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.0k061tt -> soccer.football_goal.match -> UEFA Euro 2012 Quarter-final I\n# Answer:\nUEFA Euro 2012 Quarter-final I", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzrlj -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w8w78v -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.match -> 2014 Real Madrid CF vs. CA Osasuna football match\n# Answer:\n2014 Real Madrid CF vs. CA Osasuna football match", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w8w78v -> soccer.football_player_stats.team -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w8w7f0 -> soccer.football_player_stats.team -> Portugal national football team\n# Answer:\nPortugal national football team", "# Reasoning Path:\nCristiano Ronaldo -> award.award_winner.awards_won -> m.010nx4bl -> award.award_honor.award -> UEFA Champions League Top Goalscorer\n# Answer:\nUEFA Champions League Top Goalscorer"], "ground_truth": ["Real Madrid C.F.", "Portugal national football team"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.3333333333333333, "path_precision": 0.5, "path_recall": 0.25, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1195", "prediction": ["# Reasoning Path:\nBahamas -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nBahamas -> location.location.time_zones -> UTC\u221205:00\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nBahamas -> location.statistical_region.co2_emissions_per_capita -> g.1245_22wy\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nBahamas -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6ddy727\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nBahamas -> location.statistical_region.co2_emissions_per_capita -> g.1245_7s_2\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nBahamas -> location.statistical_region.co2_emissions_per_capita -> g.1245_b1hf\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nBahamas -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6nxttlh\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nBahamas -> location.statistical_region.renewable_freshwater_per_capita -> g.1hhc38m4x\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita"], "ground_truth": ["Eastern Time Zone", "UTC\u221205:00"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1197", "prediction": ["# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Portuguese Language -> language.human_language.dialects -> Brazilian Portuguese\n# Answer:\nPortuguese Language", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Brazilian Portuguese -> language.language_dialect.language -> Portuguese Language\n# Answer:\nBrazilian Portuguese", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc39sjp\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fm01\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc385hl\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp"], "ground_truth": ["Portuguese Language", "Brazilian Portuguese", "Italian Language"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.36363636363636365, "ans_precission": 0.25, "ans_recall": 0.6666666666666666, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 0.36363636363636365, "path_ans_precision": 0.25, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1198", "prediction": ["# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced_by -> Algernon Charles Swinburne -> influence.influence_node.influenced_by -> Robert Browning\n# Answer:\nAlgernon Charles Swinburne", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced_by -> Algernon Charles Swinburne -> influence.influence_node.influenced_by -> Elizabeth Barrett Browning\n# Answer:\nAlgernon Charles Swinburne", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced_by -> Algernon Charles Swinburne -> people.person.religion -> Atheism\n# Answer:\nAlgernon Charles Swinburne", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced_by -> Algernon Charles Swinburne -> influence.influence_node.influenced_by -> Alfred Tennyson, 1st Baron Tennyson\n# Answer:\nAlgernon Charles Swinburne", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced_by -> Baruch Spinoza -> influence.influence_node.influenced_by -> Thomas Hobbes\n# Answer:\nBaruch Spinoza", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced_by -> William Shakespeare -> influence.influence_node.influenced_by -> Edmund Spenser\n# Answer:\nWilliam Shakespeare", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced_by -> Baruch Spinoza -> influence.influence_node.influenced_by -> Aristotle\n# Answer:\nBaruch Spinoza", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced_by -> Baruch Spinoza -> base.kwebbase.kwtopic.connections_from -> baruch spinoza influenced frederick wilhelm joseph von schelling\n# Answer:\nBaruch Spinoza", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced_by -> Algernon Charles Swinburne -> award.award_nominee.award_nominations -> m.0g4xqff\n# Answer:\nAlgernon Charles Swinburne", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced_by -> Baruch Spinoza -> influence.influence_node.influenced_by -> Epicurus\n# Answer:\nBaruch Spinoza"], "ground_truth": ["Thomas Browne", "Robert Burns", "Baruch Spinoza", "Friedrich Wilhelm Joseph Schelling", "Emanuel Swedenborg", "Giambattista Vico", "Algernon Charles Swinburne", "Immanuel Kant", "William Shakespeare", "John Milton"], "ans_acc": 0.3, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 1.0, "ans_recall": 0.3, "path_f1": 0.4615384615384615, "path_precision": 1.0, "path_recall": 0.3, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 1.0, "path_ans_recall": 0.3}
{"id": "WebQTest-1199", "prediction": ["# Reasoning Path:\nKim Kardashian -> people.person.profession -> TV Personality -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nTV Personality", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> Television producer -> common.topic.subject_of -> Michael Palance\n# Answer:\nTelevision producer", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> TV Personality -> base.descriptive_names.names.descriptive_name -> m.0106hk42\n# Answer:\nTV Personality", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> Television producer -> common.topic.subjects -> Michael Palance\n# Answer:\nTelevision producer", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> Television producer -> common.topic.notable_types -> Profession\n# Answer:\nTelevision producer", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> Socialite -> base.descriptive_names.names.descriptive_name -> m.010fn4s1\n# Answer:\nSocialite", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> TV Personality -> base.descriptive_names.names.descriptive_name -> m.0106hqv_\n# Answer:\nTV Personality", "# Reasoning Path:\nKim Kardashian -> base.popstra.celebrity.dated -> m.064hndx -> base.popstra.dated.participant -> Reggie Bush\n# Answer:\nReggie Bush", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> TV Personality -> fictional_universe.character_occupation.characters_with_this_occupation -> Misa Amane\n# Answer:\nTV Personality", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> Socialite -> common.topic.notable_for -> g.1256dwvkv\n# Answer:\nSocialite"], "ground_truth": ["Model", "Fashion designer", "Businessperson", "TV Personality", "Socialite", "Actor", "Television producer", "Entrepreneur"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.5294117647058825, "ans_precission": 0.9, "ans_recall": 0.375, "path_f1": 0.5294117647058825, "path_precision": 0.9, "path_recall": 0.375, "path_ans_f1": 0.5294117647058825, "path_ans_precision": 0.9, "path_ans_recall": 0.375}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.office_holder -> Rob Portman\n# Answer:\nRob Portman", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.office_holder -> George Voinovich\n# Answer:\nGeorge Voinovich", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nUnited States Senate", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_holder -> Sherrod Brown\n# Answer:\nSherrod Brown", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> Tennessee\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.image -> The Rhodes State Office Tower adjacent to the Statehouse\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.tropical_cyclone_season -> 1982 Atlantic hurricane season\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> common.topic.notable_for -> g.1255tjcrg\n# Answer:\nTropical Storm Chris"], "ground_truth": ["Ted Strickland", "John Kasich", "Return J. Meigs, Jr."], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1200", "prediction": ["# Reasoning Path:\nMississippi River -> geography.river.mouth -> Gulf of Mexico -> base.aubreymaturin.place.mentions -> m.05pwg4d\n# Answer:\nGulf of Mexico", "# Reasoning Path:\nMississippi River -> geography.river.mouth -> Gulf of Mexico -> common.topic.notable_for -> g.12559b43k\n# Answer:\nGulf of Mexico", "# Reasoning Path:\nMississippi River -> geography.river.mouth -> Gulf of Mexico -> location.statistical_region.population -> m.0hyq8n7\n# Answer:\nGulf of Mexico", "# Reasoning Path:\nMississippi River -> location.location.partially_containedby -> Wisconsin -> location.location.containedby -> United States of America\n# Answer:\nWisconsin", "# Reasoning Path:\nMississippi River -> location.location.partially_containedby -> Wisconsin -> location.location.containedby -> United States, with Territories\n# Answer:\nWisconsin", "# Reasoning Path:\nMississippi River -> book.book_subject.works -> Adventures of Huckleberry Finn -> book.written_work.subjects -> Adventure\n# Answer:\nAdventures of Huckleberry Finn", "# Reasoning Path:\nMississippi River -> geography.river.mouth -> Gulf of Mexico -> location.statistical_region.population -> m.0t4sg4g\n# Answer:\nGulf of Mexico", "# Reasoning Path:\nMississippi River -> location.location.partially_containedby -> Mississippi -> location.location.containedby -> United States of America\n# Answer:\nMississippi", "# Reasoning Path:\nMississippi River -> location.location.partially_containedby -> Wisconsin -> location.location.containedby -> Contiguous United States\n# Answer:\nWisconsin", "# Reasoning Path:\nMississippi River -> location.location.partially_containedby -> Wisconsin -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nWisconsin"], "ground_truth": ["Gulf of Mexico"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1201", "prediction": ["# Reasoning Path:\nPsych -> tv.tv_program.regular_cast -> m.02wsgt5 -> tv.regular_tv_appearance.actor -> Timothy Omundson\n# Answer:\nTimothy Omundson", "# Reasoning Path:\nPsych -> tv.tv_program.regular_cast -> m.02wsgt5 -> tv.regular_tv_appearance.character -> Carlton Lassiter\n# Answer:\nCarlton Lassiter", "# Reasoning Path:\nPsych -> tv.tv_program.regular_cast -> m.02wsgl5 -> tv.regular_tv_appearance.actor -> James Roday\n# Answer:\nJames Roday", "# Reasoning Path:\nPsych -> award.award_nominated_work.award_nominations -> m.0_ln542 -> award.award_nomination.award_nominee -> James Roday\n# Answer:\nJames Roday", "# Reasoning Path:\nPsych -> tv.tv_program.regular_cast -> m.011j_6wj -> tv.regular_tv_appearance.character -> Beautiful Woman\n# Answer:\nBeautiful Woman", "# Reasoning Path:\nPsych -> award.award_nominated_work.award_nominations -> m.0_lq_93 -> award.award_nomination.award_nominee -> James Roday\n# Answer:\nJames Roday", "# Reasoning Path:\nPsych -> tv.tv_program.regular_cast -> m.02wsgl5 -> tv.regular_tv_appearance.seasons -> Psych - Season 8\n# Answer:\nPsych - Season 8", "# Reasoning Path:\nPsych -> award.award_nominated_work.award_nominations -> m.0_ln542 -> award.award_nomination.ceremony -> 13th ALMA Awards\n# Answer:\n13th ALMA Awards", "# Reasoning Path:\nPsych -> award.award_nominated_work.award_nominations -> m.0_ls2dd -> award.award_nomination.ceremony -> 11th ALMA Awards\n# Answer:\n11th ALMA Awards", "# Reasoning Path:\nPsych -> common.topic.notable_types -> Award-Winning Work -> freebase.documented_object.documentation -> m.02nqvch\n# Answer:\nAward-Winning Work"], "ground_truth": ["Timothy Omundson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1203", "prediction": ["# Reasoning Path:\nIsrael -> government.governmental_jurisdiction.governing_officials -> m.0j5wkf1 -> government.government_position_held.office_holder -> Shimon Peres\n# Answer:\nShimon Peres", "# Reasoning Path:\nIsrael -> government.governmental_jurisdiction.governing_officials -> m.0j5wkf1 -> government.government_position_held.office_position_or_title -> President of Israel\n# Answer:\nPresident of Israel", "# Reasoning Path:\nIsrael -> government.governmental_jurisdiction.governing_officials -> m.0114vpvq -> government.government_position_held.appointed_by -> Benjamin Netanyahu\n# Answer:\nBenjamin Netanyahu", "# Reasoning Path:\nIsrael -> government.governmental_jurisdiction.governing_officials -> m.010gg9lp -> government.government_position_held.office_holder -> Ehud Olmert\n# Answer:\nEhud Olmert", "# Reasoning Path:\nIsrael -> location.statistical_region.health_expenditure_as_percent_of_gdp -> g.12cp_j7nj\n# Answer:\nlocation.statistical_region.health_expenditure_as_percent_of_gdp", "# Reasoning Path:\nIsrael -> location.statistical_region.cpi_inflation_rate -> g.11b60st8pp\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nIsrael -> government.governmental_jurisdiction.governing_officials -> m.0114vpvq -> government.government_position_held.governmental_body -> Israeli Ministry of Defense\n# Answer:\nIsraeli Ministry of Defense", "# Reasoning Path:\nIsrael -> location.statistical_region.health_expenditure_as_percent_of_gdp -> g.1hhc38hll\n# Answer:\nlocation.statistical_region.health_expenditure_as_percent_of_gdp", "# Reasoning Path:\nIsrael -> location.statistical_region.health_expenditure_as_percent_of_gdp -> g.1hhc3_l0d\n# Answer:\nlocation.statistical_region.health_expenditure_as_percent_of_gdp", "# Reasoning Path:\nIsrael -> location.statistical_region.cpi_inflation_rate -> g.12tb6fss0\n# Answer:\nlocation.statistical_region.cpi_inflation_rate"], "ground_truth": ["Reuven Rivlin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1204", "prediction": ["# Reasoning Path:\nAncestral Puebloans -> people.ethnicity.geographic_distribution -> Four Corners -> common.topic.article -> m.027v6q\n# Answer:\nFour Corners", "# Reasoning Path:\nAncestral Puebloans -> people.ethnicity.geographic_distribution -> Four Corners -> common.topic.image -> The Four Corners region is in the red area on this map\n# Answer:\nFour Corners", "# Reasoning Path:\nAncestral Puebloans -> common.topic.article -> m.0ly_\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nAncestral Puebloans -> people.ethnicity.geographic_distribution -> Four Corners -> common.topic.notable_for -> g.1257_prs_\n# Answer:\nFour Corners", "# Reasoning Path:\nAncestral Puebloans -> common.topic.notable_types -> Ethnicity -> base.descriptive_names.names.descriptive_name -> m.011vfn5d\n# Answer:\nEthnicity", "# Reasoning Path:\nAncestral Puebloans -> common.topic.notable_types -> Ethnicity -> freebase.type_hints.included_types -> Topic\n# Answer:\nEthnicity", "# Reasoning Path:\nAncestral Puebloans -> common.topic.notable_types -> Ethnicity -> type.type.expected_by -> Applies to\n# Answer:\nEthnicity", "# Reasoning Path:\nAncestral Puebloans -> common.topic.notable_types -> Ethnicity -> type.type.expected_by -> Ethnic population\n# Answer:\nEthnicity", "# Reasoning Path:\nAncestral Puebloans -> common.topic.notable_types -> Ethnicity -> type.type.expected_by -> Included in group(s)\n# Answer:\nEthnicity"], "ground_truth": ["Four Corners"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-1205", "prediction": ["# Reasoning Path:\nHarley Quinn -> tv.tv_character.appeared_in_tv_program -> m.0wz39vs -> tv.regular_tv_appearance.actor -> Arleen Sorkin\n# Answer:\nArleen Sorkin", "# Reasoning Path:\nHarley Quinn -> tv.tv_character.appeared_in_tv_program -> m.02wm17r -> tv.regular_tv_appearance.actor -> Hynden Walch\n# Answer:\nHynden Walch", "# Reasoning Path:\nHarley Quinn -> tv.tv_character.appeared_in_tv_program -> m.0wz39vs -> tv.regular_tv_appearance.seasons -> Batman: The Animated Series - Season 3\n# Answer:\nBatman: The Animated Series - Season 3", "# Reasoning Path:\nHarley Quinn -> tv.tv_character.appeared_in_tv_program -> m.02wm18b -> tv.regular_tv_appearance.actor -> Mia Sara\n# Answer:\nMia Sara", "# Reasoning Path:\nHarley Quinn -> fictional_universe.fictional_character.romantically_involved_with -> m.030rldt -> fictional_universe.romantic_involvement.partner -> Joker\n# Answer:\nJoker", "# Reasoning Path:\nHarley Quinn -> fictional_universe.fictional_character.employers -> m.02wm19d -> fictional_universe.fictional_employment_tenure.title -> Internship\n# Answer:\nInternship", "# Reasoning Path:\nHarley Quinn -> tv.tv_character.appeared_in_tv_program -> m.02wm18b -> tv.regular_tv_appearance.series -> Birds of Prey\n# Answer:\nBirds of Prey", "# Reasoning Path:\nHarley Quinn -> fictional_universe.fictional_character.employers -> m.02wm19d -> fictional_universe.fictional_employment_tenure.employer -> Arkham Asylum\n# Answer:\nArkham Asylum"], "ground_truth": ["Mia Sara", "Hynden Walch", "Arleen Sorkin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.375, "path_recall": 0.75, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1206", "prediction": ["# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> common.topic.subject_of -> Book\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Statesman -> common.topic.article -> m.016fc9\n# Answer:\nStatesman", "# Reasoning Path:\nFrancis Bacon -> book.author.works_written -> Bacon\n# Answer:\nBacon", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> tv.tv_subject.tv_programs -> The Secret Life of Scientists & Engineers\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Visual Artist -> freebase.type_hints.included_types -> Person\n# Answer:\nVisual Artist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> common.topic.subject_of -> Autobiography\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> base.descriptive_names.names.descriptive_name -> m.01053kvm\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Emphysema\n# Answer:\nMale", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> common.topic.subject_of -> Biography\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Statesman -> fictional_universe.character_occupation.characters_with_this_occupation -> Thomas Jefferson\n# Answer:\nStatesman"], "ground_truth": ["Statesman", "Spy", "Philosopher", "Author", "Scientist"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.509090909090909, "ans_precission": 0.7, "ans_recall": 0.4, "path_f1": 0.5833333333333334, "path_precision": 0.7, "path_recall": 0.5, "path_ans_f1": 0.6857142857142857, "path_ans_precision": 0.8, "path_ans_recall": 0.6}
{"id": "WebQTest-1209", "prediction": ["# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 0 -> tv.tv_series_season.episodes -> 10 Items or Less: 10 Random Looks at NCIS\n# Answer:\nNCIS - Season 0", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 0 -> tv.tv_series_season.episodes -> 10 Years Aft\n# Answer:\nNCIS - Season 0", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 0 -> common.topic.notable_for -> g.125505_2h\n# Answer:\nNCIS - Season 0", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 0 -> tv.tv_series_season.episodes -> A Death in the Family\n# Answer:\nNCIS - Season 0", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 1 -> tv.tv_series_season.episodes -> A Weak Link\n# Answer:\nNCIS - Season 1", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 10 -> tv.tv_series_season.episodes -> Berlin\n# Answer:\nNCIS - Season 10", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 1 -> tv.tv_series_season.episodes -> B\u00eate Noire\n# Answer:\nNCIS - Season 1", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 1 -> tv.tv_series_season.episodes -> Dead Man Talking\n# Answer:\nNCIS - Season 1", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 1 -> common.topic.image -> NCISSeason01.jpg\n# Answer:\nNCIS - Season 1", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 10 -> tv.tv_series_season.episodes -> Canary\n# Answer:\nNCIS - Season 10"], "ground_truth": ["NCIS - Season 3"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nPound sterling", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.02nr829 -> sports.sports_team_roster.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m -> soccer.football_player_stats.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.02nr829 -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nMidfielder", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9021c -> soccer.football_player_stats.team -> A.C. Milan\n# Answer:\nA.C. Milan", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar"], "ground_truth": ["LA Galaxy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.47058823529411764, "path_precision": 0.4, "path_recall": 0.5714285714285714, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1210", "prediction": ["# Reasoning Path:\nCanada -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.country.official_language -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Bermuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.country.official_language -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> Abenaki language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nAbenaki language", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language"], "ground_truth": ["O Canada"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1212", "prediction": ["# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> education.education.major_field_of_study -> Computer programming\n# Answer:\nComputer programming", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.02n93cn -> education.education.major_field_of_study -> Computer Science\n# Answer:\nComputer Science", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.02n93cn -> education.education.major_field_of_study -> Psychology\n# Answer:\nPsychology", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.02n93cn -> education.education.institution -> Harvard University\n# Answer:\nHarvard University", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nMinor", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.02n93cn -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nMark Zuckerberg -> influence.influence_node.influenced -> Austin Cameron -> influence.influence_node.influenced_by -> Tim Berners-Lee\n# Answer:\nAustin Cameron", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.04hc7zn -> education.education.institution -> Phillips Exeter Academy\n# Answer:\nPhillips Exeter Academy"], "ground_truth": ["Psychology", "Computer programming", "Computer Science"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1213", "prediction": ["# Reasoning Path:\nMichael Jackson -> people.person.place_of_birth -> Gary -> location.location.containedby -> United States of America\n# Answer:\nGary", "# Reasoning Path:\nMichael Jackson -> music.artist.origin -> Gary -> location.location.containedby -> United States of America\n# Answer:\nGary", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> WNEW -> broadcast.content.location -> New York City\n# Answer:\nWNEW", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1.FM Absolute  90's -> common.topic.notable_types -> Broadcast Content\n# Answer:\n1.FM Absolute  90's", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> .977 The '80s Channel -> broadcast.content.genre -> Pop music\n# Answer:\n.977 The '80s Channel", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> .977 The '80s Channel -> common.topic.notable_types -> Broadcast Content\n# Answer:\n.977 The '80s Channel", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> WNEW -> broadcast.content.artist -> Usher\n# Answer:\nWNEW", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> WNEW -> common.topic.notable_types -> Broadcast Content\n# Answer:\nWNEW", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> WNEW -> broadcast.content.artist -> Alicia Keys\n# Answer:\nWNEW", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> .977 The '80s Channel -> broadcast.content.genre -> 70's\n# Answer:\n.977 The '80s Channel"], "ground_truth": ["United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1215", "prediction": ["# Reasoning Path:\nStephen Covey -> people.person.profession -> Consultant -> business.product_line.brand -> Resilient Solution\n# Answer:\nConsultant", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Consultant -> common.topic.subjects -> Ben Galley\n# Answer:\nConsultant", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Consultant -> business.industry.companies -> Corporate Citizenship\n# Answer:\nConsultant", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Author -> tv.tv_subject.tv_episodes -> Author David Kerr - The Ol' Turkey Hunter\n# Answer:\nAuthor", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Manager -> sports.coaching_position.coaches -> m.010g0m_x\n# Answer:\nManager", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Author -> tv.tv_subject.tv_episodes -> J. J. Woods \u2013 Author - The Forgotten Covenant\n# Answer:\nAuthor", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Author -> tv.tv_subject.tv_episodes -> Leisha Kelly \u2013 author- \\\"House on Malcolm Street\\\"\n# Answer:\nAuthor", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Manager -> sports.coaching_position.coaches -> m.010g0nd_\n# Answer:\nManager", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Manager -> base.schemastaging.sports_team_manager_position.managers -> m.010gh3kt\n# Answer:\nManager"], "ground_truth": ["Writer", "Motivational speaker", "Manager", "Consultant", "Author", "Professor"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.42857142857142855, "path_precision": 0.6, "path_recall": 0.3333333333333333, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1216", "prediction": ["# Reasoning Path:\nJoseph Stalin -> people.person.nationality -> Soviet Union -> location.country.capital -> Moscow\n# Answer:\nSoviet Union", "# Reasoning Path:\nJoseph Stalin -> people.person.nationality -> Soviet Union -> government.governmental_jurisdiction.government_positions -> General Secretary of the Communist Party of the Soviet Union\n# Answer:\nSoviet Union", "# Reasoning Path:\nJoseph Stalin -> government.politician.government_positions_held -> m.049x6_k -> government.government_position_held.jurisdiction_of_office -> Soviet Union\n# Answer:\nSoviet Union", "# Reasoning Path:\nJoseph Stalin -> people.person.nationality -> Soviet Union -> base.rivalries.rival.rivals -> Space Race\n# Answer:\nSoviet Union", "# Reasoning Path:\nJoseph Stalin -> people.person.nationality -> Russian Empire -> location.country.official_language -> Russian Language\n# Answer:\nRussian Empire", "# Reasoning Path:\nJoseph Stalin -> government.politician.government_positions_held -> m.049x6_k -> government.government_position_held.office_position_or_title -> General Secretary of the Communist Party of the Soviet Union\n# Answer:\nGeneral Secretary of the Communist Party of the Soviet Union", "# Reasoning Path:\nJoseph Stalin -> government.politician.government_positions_held -> m.049x6_k -> government.government_position_held.governmental_body -> Central Committee of the Communist Party of the Soviet Union\n# Answer:\nCentral Committee of the Communist Party of the Soviet Union", "# Reasoning Path:\nJoseph Stalin -> government.politician.government_positions_held -> m.0c9_j12 -> government.government_position_held.jurisdiction_of_office -> Soviet Union\n# Answer:\nSoviet Union", "# Reasoning Path:\nJoseph Stalin -> people.person.nationality -> Russian Empire -> location.country.capital -> Saint Petersburg\n# Answer:\nRussian Empire", "# Reasoning Path:\nJoseph Stalin -> symbols.name_source.namesakes -> Country Joe McDonald -> people.person.nationality -> United States of America\n# Answer:\nCountry Joe McDonald"], "ground_truth": ["Soviet Union"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1217", "prediction": ["# Reasoning Path:\nMonta Ellis -> sports.pro_athlete.sports_played_professionally -> m.0c54277 -> sports.pro_sports_played.sport -> Basketball\n# Answer:\nBasketball", "# Reasoning Path:\nMonta Ellis -> basketball.basketball_player.player_statistics -> m.04qq_57 -> basketball.basketball_player_stats.season -> 2006\u201307 NBA season\n# Answer:\n2006\u201307 NBA season", "# Reasoning Path:\nMonta Ellis -> common.topic.article -> m.06rsnr\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nMonta Ellis -> basketball.basketball_player.player_statistics -> m.04qq_57 -> basketball.basketball_player_stats.team -> Golden State Warriors\n# Answer:\nGolden State Warriors", "# Reasoning Path:\nMonta Ellis -> basketball.basketball_player.player_statistics -> m.04qds38 -> basketball.basketball_player_stats.season -> 2005\u201306 NBA season\n# Answer:\n2005\u201306 NBA season", "# Reasoning Path:\nMonta Ellis -> basketball.basketball_player.player_statistics -> m.04qhmvs -> basketball.basketball_player_stats.season -> 2007\u201308 NBA season\n# Answer:\n2007\u201308 NBA season", "# Reasoning Path:\nMonta Ellis -> basketball.basketball_player.player_statistics -> m.04qds38 -> basketball.basketball_player_stats.team -> Golden State Warriors\n# Answer:\nGolden State Warriors", "# Reasoning Path:\nMonta Ellis -> basketball.basketball_player.player_statistics -> m.04qhmvs -> basketball.basketball_player_stats.team -> Golden State Warriors\n# Answer:\nGolden State Warriors"], "ground_truth": ["NBA Most Improved Player Award"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1219", "prediction": ["# Reasoning Path:\nRihanna -> people.person.nationality -> Barbados -> location.location.contains -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> people.person.nationality -> Barbados -> location.country.administrative_divisions -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> people.person.nationality -> Barbados -> common.topic.notable_types -> Country\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> base.aareas.schema.administrative_area.administrative_parent -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.country.official_language -> English Language\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.location.contains -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\nmusic.artist.track_contributions"], "ground_truth": ["Barbados"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Hidalgo\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.containedby -> Spain\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Zacuto\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.location.containedby -> United States of America\n# Answer:\nCoronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Alfonso XI of Castile\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.containedby -> Province of Salamanca\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7t8559g\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7tbpl3m\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> common.topic.article -> m.0r4xz\n# Answer:\nCoronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> book.author.works_written -> Coronado's letter to Mendoza, August 3, 1540 -> book.book.editions -> Coronado's letter to Mendoza, August 3, 1540. The relation of Francis Vasquez de Coronado, captaine generall of the people which were sent in the name of the Emperours Maiestie to the countrey of Cibola newly discouered, which he sent to don Antonio de Mendoca, viceroy of Mexico, of such things as happened in his voyage from the 22. of Aprill in the yeere 1540. Which departed from Culiacan forward, and of such things as hee found in the countrey which he passed\n# Answer:\nCoronado's letter to Mendoza, August 3, 1540"], "ground_truth": ["Salamanca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1220", "prediction": ["# Reasoning Path:\nShannen Doherty -> film.actor.film -> m.0118pdct -> film.performance.film -> Bless All the Dear Children\n# Answer:\nBless All the Dear Children", "# Reasoning Path:\nShannen Doherty -> film.actor.film -> m.012zhxmp -> film.performance.film -> Bukowski\n# Answer:\nBukowski", "# Reasoning Path:\nShannen Doherty -> tv.tv_actor.starring_roles -> m.0220tfg -> tv.regular_tv_appearance.series -> Beverly Hills, 90210\n# Answer:\nBeverly Hills, 90210", "# Reasoning Path:\nShannen Doherty -> film.actor.film -> m.03l0kfx -> film.performance.film -> Nowhere\n# Answer:\nNowhere", "# Reasoning Path:\nShannen Doherty -> film.actor.film -> m.0118pdct -> film.performance.character -> Jenny Wilder\n# Answer:\nJenny Wilder", "# Reasoning Path:\nShannen Doherty -> tv.tv_actor.starring_roles -> m.03ltlbv -> tv.regular_tv_appearance.series -> North Shore\n# Answer:\nNorth Shore", "# Reasoning Path:\nShannen Doherty -> tv.tv_actor.starring_roles -> m.0220tfg -> tv.regular_tv_appearance.character -> Brenda Walsh\n# Answer:\nBrenda Walsh", "# Reasoning Path:\nShannen Doherty -> tv.tv_director.episodes_directed -> All Hell Breaks Loose -> common.topic.notable_for -> g.125dwbgbl\n# Answer:\nAll Hell Breaks Loose", "# Reasoning Path:\nShannen Doherty -> tv.tv_actor.starring_roles -> m.0220tfg -> tv.regular_tv_appearance.seasons -> Beverly Hills, 90210 - Season 1\n# Answer:\nBeverly Hills, 90210 - Season 1", "# Reasoning Path:\nShannen Doherty -> film.actor.film -> m.03l0kfx -> film.performance.character -> Val-Chick 2\n# Answer:\nVal-Chick 2"], "ground_truth": ["Nightlight", "Jailbreakers", "The Secret of NIMH", "Bless All the Dear Children", "The Delphi Effect", "Another Day", "Night Shift", "Blindfold: Acts of Obsession", "Almost Dead", "Sleeping with the Devil", "The Lost Treasure of the Grand Canyon", "Striking Poses", "Heathers", "Witchslayer Gretl", "The Rendering", "Bukowski", "The Ticket", "Gone in the Night", "Category 7: The End of the World", "Friends 'Til the End", "Jay and Silent Bob Strike Back", "The Other Lover", "Christmas Caper", "Nowhere", "Satan's School for Girls", "Hell on Heels: The Battle of Mary Kay", "Freeze Frame", "A Burning Passion: The Margaret Mitchell Story", "Girls Just Want to Have Fun", "Burning Palms", "Obsessed", "Mallrats", "Growing the Big One"], "ans_acc": 0.09090909090909091, "ans_hit": 1, "ans_f1": 0.13953488372093023, "ans_precission": 0.3, "ans_recall": 0.09090909090909091, "path_f1": 0.13636363636363635, "path_precision": 0.3, "path_recall": 0.08823529411764706, "path_ans_f1": 0.13953488372093023, "path_ans_precision": 0.3, "path_ans_recall": 0.09090909090909091}
{"id": "WebQTest-1221", "prediction": ["# Reasoning Path:\nJoakim Noah -> people.person.education -> m.03p7z1w -> education.education.institution -> University of Florida\n# Answer:\nUniversity of Florida", "# Reasoning Path:\nJoakim Noah -> sports.drafted_athlete.drafted -> m.02nqxbv -> sports.sports_league_draft_pick.school -> University of Florida\n# Answer:\nUniversity of Florida", "# Reasoning Path:\nJoakim Noah -> people.person.education -> m.0b6vrkl -> education.education.institution -> Poly Prep Country Day School\n# Answer:\nPoly Prep Country Day School", "# Reasoning Path:\nJoakim Noah -> people.person.education -> m.040rzsq -> education.education.institution -> Lawrenceville School\n# Answer:\nLawrenceville School", "# Reasoning Path:\nJoakim Noah -> sports.drafted_athlete.drafted -> m.02nqxbv -> sports.sports_league_draft_pick.team -> Chicago Bulls\n# Answer:\nChicago Bulls", "# Reasoning Path:\nJoakim Noah -> sports.drafted_athlete.drafted -> m.02nqxbv -> sports.sports_league_draft_pick.draft -> 2007 NBA draft\n# Answer:\n2007 NBA draft", "# Reasoning Path:\nJoakim Noah -> people.person.gender -> Male -> biology.hybrid_parent_gender.hybrids -> m.0blp580\n# Answer:\nMale", "# Reasoning Path:\nJoakim Noah -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nJoakim Noah -> people.person.gender -> Male -> biology.hybrid_parent_gender.hybrids -> m.0blp5sr\n# Answer:\nMale", "# Reasoning Path:\nJoakim Noah -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nMale"], "ground_truth": ["University of Florida"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1223", "prediction": ["# Reasoning Path:\nBilly Elliot -> film.film.starring -> m.0y81fs9 -> film.performance.actor -> Adam Cooper\n# Answer:\nAdam Cooper", "# Reasoning Path:\nBilly Elliot -> film.film.starring -> m.02tbgcw -> film.performance.actor -> Gary Lewis\n# Answer:\nGary Lewis", "# Reasoning Path:\nBilly Elliot -> film.film.starring -> m.0by4690 -> film.performance.actor -> Joe Renton\n# Answer:\nJoe Renton", "# Reasoning Path:\nBilly Elliot -> award.award_nominated_work.award_nominations -> m.0b455xy -> award.award_nomination.award_nominee -> Jamie Bell\n# Answer:\nJamie Bell", "# Reasoning Path:\nBilly Elliot -> film.film.starring -> m.02tbgcw -> film.performance.character -> Jackie Elliot\n# Answer:\nJackie Elliot", "# Reasoning Path:\nBilly Elliot -> film.film.starring -> m.0by4690 -> film.performance.character -> Gary Poulson\n# Answer:\nGary Poulson", "# Reasoning Path:\nBilly Elliot -> common.topic.webpage -> m.03l6gmg -> common.webpage.resource -> m.0bkz0lq\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nBilly Elliot -> award.award_nominated_work.award_nominations -> m.0b455xy -> award.award_nomination.award -> Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Leading Role\n# Answer:\nScreen Actors Guild Award for Outstanding Performance by a Male Actor in a Leading Role", "# Reasoning Path:\nBilly Elliot -> award.award_nominated_work.award_nominations -> m.021ykh9 -> award.award_nomination.award_nominee -> Stephen Daldry\n# Answer:\nStephen Daldry", "# Reasoning Path:\nBilly Elliot -> award.award_nominated_work.award_nominations -> m.05bm4dh -> award.award_nomination.award_nominee -> Lee Hall\n# Answer:\nLee Hall"], "ground_truth": ["Adam Cooper", "Jamie Bell"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.08695652173913043, "path_precision": 0.1, "path_recall": 0.07692307692307693, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1225", "prediction": ["# Reasoning Path:\nKourtney Kardashian -> base.popstra.celebrity.dated -> m.065px1n -> base.popstra.dated.participant -> Scott Disick\n# Answer:\nScott Disick", "# Reasoning Path:\nKourtney Kardashian -> award.award_winner.awards_won -> m.0y76gjw -> award.award_honor.award_winner -> Kim Kardashian\n# Answer:\nKim Kardashian", "# Reasoning Path:\nKourtney Kardashian -> award.award_winner.awards_won -> m.0y76gjw -> award.award_honor.ceremony -> 2012 Teen Choice Awards\n# Answer:\n2012 Teen Choice Awards", "# Reasoning Path:\nKourtney Kardashian -> award.award_winner.awards_won -> m.0y76gjw -> award.award_honor.honored_for -> Keeping Up with the Kardashians\n# Answer:\nKeeping Up with the Kardashians", "# Reasoning Path:\nKourtney Kardashian -> common.topic.image -> Kourtney Kardashian 2 2009 -> common.image.size -> m.0632364\n# Answer:\nKourtney Kardashian 2 2009", "# Reasoning Path:\nKourtney Kardashian -> award.award_winner.awards_won -> m.0y76gjw -> award.award_honor.award_winner -> Khlo\u00e9 Kardashian\n# Answer:\nKhlo\u00e9 Kardashian", "# Reasoning Path:\nKourtney Kardashian -> award.award_winner.awards_won -> m.0y66_42 -> award.award_honor.honored_for -> Keeping Up with the Kardashians\n# Answer:\nKeeping Up with the Kardashians", "# Reasoning Path:\nKourtney Kardashian -> award.award_winner.awards_won -> m.0wj9z_x -> award.award_honor.award_winner -> Kendall\n# Answer:\nKendall", "# Reasoning Path:\nKourtney Kardashian -> award.award_winner.awards_won -> m.0wj9z_x -> award.award_honor.ceremony -> 2013 Teen Choice Awards\n# Answer:\n2013 Teen Choice Awards", "# Reasoning Path:\nKourtney Kardashian -> award.award_winner.awards_won -> m.0wj9z_x -> award.award_honor.award -> Teen Choice Award for Choice TV Female Reality Star\n# Answer:\nTeen Choice Award for Choice TV Female Reality Star"], "ground_truth": ["Scott Disick"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1226", "prediction": ["# Reasoning Path:\nAustralia -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> United Kingdom\n# Answer:\nParliamentary system", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Canada\n# Answer:\nParliamentary system", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_55mr\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_2hqk\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Japan\n# Answer:\nParliamentary system", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Federal monarchy -> government.form_of_government.countries -> Cocos (Keeling) Islands\n# Answer:\nFederal monarchy"], "ground_truth": ["Parliamentary system", "Constitutional monarchy", "Federal monarchy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1227", "prediction": ["# Reasoning Path:\nHowie Long -> sports.pro_athlete.teams -> m.0hqqf18 -> sports.sports_team_roster.team -> Oakland Raiders\n# Answer:\nOakland Raiders", "# Reasoning Path:\nHowie Long -> sports.pro_athlete.teams -> m.0hqqf0t -> sports.sports_team_roster.team -> Villanova Wildcats football\n# Answer:\nVillanova Wildcats football", "# Reasoning Path:\nHowie Long -> sports.pro_athlete.teams -> m.0hqqf18 -> sports.sports_team_roster.position -> Defensive end\n# Answer:\nDefensive end", "# Reasoning Path:\nHowie Long -> sports.pro_athlete.teams -> m.0hqqf0t -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nHowie Long -> common.topic.image -> Howie Long - American Football Player TV host -> common.image.size -> m.02d_35f\n# Answer:\nHowie Long - American Football Player TV host", "# Reasoning Path:\nHowie Long -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nHowie Long -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nHowie Long -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nHowie Long -> freebase.valuenotation.is_reviewed -> Gender -> type.property.schema -> Person\n# Answer:\nGender", "# Reasoning Path:\nHowie Long -> freebase.valuenotation.is_reviewed -> Date of birth -> type.property.expected_type -> Date/Time\n# Answer:\nDate of birth"], "ground_truth": ["Oakland Raiders", "Villanova Wildcats football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1228", "prediction": ["# Reasoning Path:\nGreek alphabet -> language.language_writing_system.parent_writing_systems -> Phoenician alphabet -> language.language_writing_system.parent_writing_systems -> Proto-Canaanite alphabet\n# Answer:\nPhoenician alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.parent_writing_systems -> Phoenician alphabet -> language.language_writing_system.languages -> Phoenician\n# Answer:\nPhoenician alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.child_writing_systems -> Gothic alphabet -> language.language_writing_system.parent_writing_systems -> Latin alphabet\n# Answer:\nGothic alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.parent_writing_systems -> Phoenician alphabet -> base.schemastaging.context_name.pronunciation -> g.125_pyj6_\n# Answer:\nPhoenician alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.parent_writing_systems -> Phoenician alphabet -> language.language_writing_system.languages -> Araona Language\n# Answer:\nPhoenician alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.child_writing_systems -> Gothic alphabet -> language.language_writing_system.languages -> Gothic Language\n# Answer:\nGothic alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.parent_writing_systems -> Phoenician alphabet -> language.language_writing_system.languages -> Biblical Hebrew\n# Answer:\nPhoenician alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.child_writing_systems -> Gothic alphabet -> base.schemastaging.context_name.pronunciation -> m.01314m6_\n# Answer:\nGothic alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.child_writing_systems -> Gothic alphabet -> language.language_writing_system.languages -> Crimean Gothic\n# Answer:\nGothic alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.child_writing_systems -> Lycian alphabet -> common.topic.notable_types -> Language Writing System\n# Answer:\nLycian alphabet"], "ground_truth": ["Phoenician alphabet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1229", "prediction": ["# Reasoning Path:\nElizabethtown -> location.location.time_zones -> Eastern Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nElizabethtown -> location.location.time_zones -> Eastern Time Zone -> common.topic.notable_for -> g.125bn2wcf\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nElizabethtown -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07c9g7m\n# Answer:\nlocation.hud_foreclosure_area.bls_unemployment_rate", "# Reasoning Path:\nElizabethtown -> location.statistical_region.unemployment_rate -> g.11b60rwz93\n# Answer:\nlocation.statistical_region.unemployment_rate", "# Reasoning Path:\nElizabethtown -> location.location.time_zones -> Eastern Time Zone -> common.topic.image -> UnionSquareAtomicClock\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nElizabethtown -> location.location.time_zones -> Eastern Time Zone -> common.topic.article -> m.02hcvg\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nElizabethtown -> location.statistical_region.unemployment_rate -> g.11b60thpzf\n# Answer:\nlocation.statistical_region.unemployment_rate", "# Reasoning Path:\nElizabethtown -> location.statistical_region.unemployment_rate -> g.11b66dqlpc\n# Answer:\nlocation.statistical_region.unemployment_rate", "# Reasoning Path:\nElizabethtown -> location.location.time_zones -> Eastern Time Zone -> common.topic.article -> m.0j44rkj\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nElizabethtown -> location.location.time_zones -> Eastern Time Zone -> common.topic.article -> m.0j62c65\n# Answer:\nEastern Time Zone"], "ground_truth": ["Eastern Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-123", "prediction": ["# Reasoning Path:\nGoogle -> organization.organization_partnership.members -> Acxiom Corporation -> business.customer.supplier -> IBM\n# Answer:\nAcxiom Corporation", "# Reasoning Path:\nGoogle -> organization.organization_partnership.members -> Cleverly Engaged Marketing -> organization.organization.sectors -> Digital marketing\n# Answer:\nCleverly Engaged Marketing", "# Reasoning Path:\nGoogle -> organization.organization_partnership.members -> Cleverly Engaged Marketing -> organization.organization.email -> m.012hr_p8\n# Answer:\nCleverly Engaged Marketing", "# Reasoning Path:\nGoogle -> common.topic.subjects -> Google Apps for Work -> organization.organization_partnership.members -> Redcorus\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> organization.organization_partnership.members -> Cleverly Engaged Marketing -> organization.organization.sectors -> Internet Marketing\n# Answer:\nCleverly Engaged Marketing", "# Reasoning Path:\nGoogle -> common.topic.subjects -> Google Apps for Work -> common.topic.subjects -> Gmail\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> organization.organization_partnership.members -> Cleverly Engaged Marketing -> organization.organization.sectors -> Marketing strategy\n# Answer:\nCleverly Engaged Marketing", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> organization.organization_partnership.members -> Redcorus\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Chrome -> common.topic.subjects -> Google Apps for Work\n# Answer:\nGoogle Chrome", "# Reasoning Path:\nGoogle -> common.topic.subjects -> Google Apps for Work -> common.topic.subjects -> App Store\n# Answer:\nGoogle Apps for Work"], "ground_truth": ["Google Maps", "Chromecast", "Google Glass", "Nexus Q", "Google Classroom", "Nexus 10", "Nexus 7", "Google Drive", "Nexus S", "Google Earth", "Google Docs, Sheets, and Slides", "Apache Wave", "Google Buzz"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1230", "prediction": ["# Reasoning Path:\nChristchurch -> travel.travel_destination.tourist_attractions -> Canterbury Museum, Christchurch -> common.topic.notable_types -> Museum\n# Answer:\nCanterbury Museum, Christchurch", "# Reasoning Path:\nChristchurch -> travel.travel_destination.tourist_attractions -> Canterbury Museum, Christchurch -> architecture.structure.architectural_style -> Gothic Revival architecture\n# Answer:\nCanterbury Museum, Christchurch", "# Reasoning Path:\nChristchurch -> location.location.contains -> ChristChurch Cathedral, Christchurch -> location.location.containedby -> New Zealand\n# Answer:\nChristChurch Cathedral, Christchurch", "# Reasoning Path:\nChristchurch -> travel.travel_destination.tourist_attractions -> Canterbury Museum, Christchurch -> common.topic.image -> Canterbury Museum1\n# Answer:\nCanterbury Museum, Christchurch", "# Reasoning Path:\nChristchurch -> travel.travel_destination.tourist_attractions -> Christchurch Botanic Gardens -> common.topic.notable_for -> g.1256xmszx\n# Answer:\nChristchurch Botanic Gardens", "# Reasoning Path:\nChristchurch -> location.statistical_region.population -> g.11bcdlk0kf\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nChristchurch -> travel.travel_destination.tourist_attractions -> Christchurch Botanic Gardens -> common.topic.notable_types -> Tourist attraction\n# Answer:\nChristchurch Botanic Gardens", "# Reasoning Path:\nChristchurch -> location.statistical_region.population -> g.11btt59cj5\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nChristchurch -> travel.travel_destination.tourist_attractions -> Canterbury Museum, Christchurch -> common.topic.image -> CanterburyMuseum gobeirne\n# Answer:\nCanterbury Museum, Christchurch", "# Reasoning Path:\nChristchurch -> travel.travel_destination.tourist_attractions -> Air Force Museum of New Zealand -> location.location.geolocation -> m.0wgdwgn\n# Answer:\nAir Force Museum of New Zealand"], "ground_truth": ["Ferrymead Heritage Park", "Lyttelton Timeball Station", "Canterbury Museum, Christchurch", "TranzAlpine", "Christchurch Arts Centre", "Cathedral Square, Christchurch", "International Antarctic Centre", "Christchurch Botanic Gardens", "Air Force Museum of New Zealand", "Willowbank Wildlife Reserve, Christchurch"], "ans_acc": 0.3, "ans_hit": 1, "ans_f1": 0.42, "ans_precission": 0.7, "ans_recall": 0.3, "path_f1": 0.28571428571428575, "path_precision": 0.5, "path_recall": 0.2, "path_ans_f1": 0.42, "path_ans_precision": 0.7, "path_ans_recall": 0.3}
{"id": "WebQTest-1231", "prediction": ["# Reasoning Path:\nChris Paul -> people.person.place_of_birth -> Winston-Salem -> base.biblioness.bibs_location.state -> North Carolina\n# Answer:\nWinston-Salem", "# Reasoning Path:\nChris Paul -> common.topic.article -> m.0cymlx\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nChris Paul -> people.person.place_of_birth -> Winston-Salem -> common.topic.notable_types -> City/Town/Village\n# Answer:\nWinston-Salem", "# Reasoning Path:\nChris Paul -> people.person.place_of_birth -> Winston-Salem -> location.citytown.postal_codes -> 27199\n# Answer:\nWinston-Salem", "# Reasoning Path:\nChris Paul -> sports.pro_athlete.sports_played_professionally -> m.0c54blq -> sports.pro_sports_played.sport -> Basketball\n# Answer:\nBasketball", "# Reasoning Path:\nChris Paul -> people.person.place_of_birth -> Winston-Salem -> location.citytown.postal_codes -> 27101\n# Answer:\nWinston-Salem", "# Reasoning Path:\nChris Paul -> people.person.place_of_birth -> Winston-Salem -> location.citytown.postal_codes -> 27101-2755\n# Answer:\nWinston-Salem"], "ground_truth": ["United States of America"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1232", "prediction": ["# Reasoning Path:\nAlbert Pujols -> sports.sports_award_winner.awards -> m.04ynyvj -> sports.sports_award.season -> 2001 Major League Baseball Season\n# Answer:\n2001 Major League Baseball Season", "# Reasoning Path:\nAlbert Pujols -> sports.sports_award_winner.awards -> m.04ynyvj -> sports.sports_award.award -> National League Rookie of the Year\n# Answer:\nNational League Rookie of the Year", "# Reasoning Path:\nAlbert Pujols -> sports.sports_award_winner.awards -> m.04xwlnb -> sports.sports_award.season -> 2008 Major League Baseball season\n# Answer:\n2008 Major League Baseball season", "# Reasoning Path:\nAlbert Pujols -> sports.drafted_athlete.drafted -> m.0461q4y -> sports.sports_league_draft_pick.draft -> 1999 Major League Baseball Draft\n# Answer:\n1999 Major League Baseball Draft", "# Reasoning Path:\nAlbert Pujols -> sports.sports_award_winner.awards -> m.04xwlnk -> sports.sports_award.season -> 2005 Major League Baseball season\n# Answer:\n2005 Major League Baseball season", "# Reasoning Path:\nAlbert Pujols -> sports.drafted_athlete.drafted -> m.0461q4y -> sports.sports_league_draft_pick.team -> St. Louis Cardinals\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\nAlbert Pujols -> sports.drafted_athlete.drafted -> m.0461q4y -> sports.sports_league_draft_pick.school -> Maple Woods Community College\n# Answer:\nMaple Woods Community College", "# Reasoning Path:\nAlbert Pujols -> sports.sports_award_winner.awards -> m.04xwlnb -> sports.sports_award.award -> National League Most Valuable Player Award\n# Answer:\nNational League Most Valuable Player Award", "# Reasoning Path:\nAlbert Pujols -> sports.sports_award_winner.awards -> m.04xwlnk -> sports.sports_award.award -> National League Most Valuable Player Award\n# Answer:\nNational League Most Valuable Player Award", "# Reasoning Path:\nAlbert Pujols -> baseball.baseball_player.batting_stats -> m.06s6m0p -> baseball.batting_statistics.season -> 2001 Major League Baseball Season\n# Answer:\n2001 Major League Baseball Season"], "ground_truth": ["2001 Major League Baseball Season"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1233", "prediction": ["# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> White -> base.roses.rose_color.roses_of_this_color -> Rosa 'Ambiente'\n# Answer:\nWhite", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> White -> base.roses.rose_color.roses_of_this_color -> Rosa 'Annapurna'\n# Answer:\nWhite", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> White -> base.schemastaging.visual_color_extra.srgb -> m.010q1h63\n# Answer:\nWhite", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> White -> base.roses.rose_color.roses_of_this_color -> Rosa 'Cabaret'\n# Answer:\nWhite", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> White -> base.petbreeds.dog_coat_color.dog_breeds -> Border Collie\n# Answer:\nWhite", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> Blue -> base.schemastaging.visual_color_extra.hsv -> m.010nmgqr\n# Answer:\nBlue", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> Blue -> book.book_subject.works -> Blue: The History of a Color\n# Answer:\nBlue", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> Blue -> common.topic.notable_for -> g.125gl7_n4\n# Answer:\nBlue", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> Grey -> business.product_category.products -> Pewter Faux Leather (D8564)\n# Answer:\nGrey", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> Grey -> base.fashionmodels.eye_color.fashion_models_with_this_eye_color -> Karen Mulder\n# Answer:\nGrey"], "ground_truth": ["Blue", "Grey", "Red", "White"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.8571428571428571, "path_precision": 1.0, "path_recall": 0.75, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-1234", "prediction": ["# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Business magnate -> base.descriptive_names.names.descriptive_name -> m.0106b2k8\n# Answer:\nBusiness magnate", "# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Business magnate -> common.topic.article -> m.025sq3\n# Answer:\nBusiness magnate", "# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Business magnate -> book.book_subject.works -> Masters of Enterprise: Giants of American Business from John Jacob Astor and J.P. Morgan to Bill Gates and Oprah Winfrey\n# Answer:\nBusiness magnate", "# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Business magnate -> base.descriptive_names.names.descriptive_name -> m.0106b2kr\n# Answer:\nBusiness magnate", "# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Businessperson -> common.topic.subjects -> Krishna Mali\n# Answer:\nBusinessperson", "# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Business magnate -> base.descriptive_names.names.descriptive_name -> m.010fn7xm\n# Answer:\nBusiness magnate", "# Reasoning Path:\nAndrew Carnegie -> common.topic.notable_types -> Book -> freebase.type_profile.published -> Published\n# Answer:\nBook", "# Reasoning Path:\nAndrew Carnegie -> symbols.name_source.namesakes -> Carnegie Steel Company -> common.topic.image -> Carnegie-Illinois Steel furnaces\n# Answer:\nCarnegie Steel Company", "# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Businessperson -> common.topic.notable_types -> Profession\n# Answer:\nBusinessperson", "# Reasoning Path:\nAndrew Carnegie -> symbols.name_source.namesakes -> Carnegie Library of Reims -> location.location.geolocation -> m.0wmj18f\n# Answer:\nCarnegie Library of Reims"], "ground_truth": ["steel", "financier"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.16666666666666669, "ans_precission": 0.1, "ans_recall": 0.5, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 0.1, "path_ans_recall": 0.5}
{"id": "WebQTest-1235", "prediction": ["# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Reflecting telescope -> base.onephylogeny.type_of_thing.includes -> Eyepiece\n# Answer:\nReflecting telescope", "# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Reflecting telescope -> astronomy.telescope_principle_type.telescopic_classifications_of_this_technique -> Optical telescope\n# Answer:\nReflecting telescope", "# Reasoning Path:\nIsaac Newton -> base.argumentmaps.innovator.original_ideas -> Newton's law of universal gravitation -> common.topic.article -> m.01kffv\n# Answer:\nNewton's law of universal gravitation", "# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Reflecting telescope -> base.onephylogeny.type_of_thing.includes -> Objective\n# Answer:\nReflecting telescope", "# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Reflecting telescope -> astronomy.telescope_type.telescopes_of_this_type -> 60 inch telescope\n# Answer:\nReflecting telescope", "# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Pet door -> common.topic.notable_types -> Invention\n# Answer:\nPet door", "# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Pet door -> common.topic.article -> m.079vqs\n# Answer:\nPet door", "# Reasoning Path:\nIsaac Newton -> base.argumentmaps.innovator.original_ideas -> Newton's law of universal gravitation -> common.topic.image -> Isaacnewton\n# Answer:\nNewton's law of universal gravitation", "# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Reflecting telescope -> astronomy.telescope_type.telescopes_of_this_type -> Astrophysical Research Consortium Telescope\n# Answer:\nReflecting telescope", "# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Reflecting telescope -> astronomy.telescope_type.telescopes_of_this_type -> Gran Telescopio Canarias\n# Answer:\nReflecting telescope"], "ground_truth": ["Reflecting telescope", "Pet door"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1237", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> sports.drafted_athlete.drafted -> m.04fw77r -> sports.sports_league_draft_pick.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qstvs -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> sports.drafted_athlete.drafted -> m.04fw77r -> sports.sports_league_draft_pick.draft -> 1992 NBA draft\n# Answer:\n1992 NBA draft", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qstvs -> basketball.basketball_player_stats.season -> 2002\u201303 NBA season\n# Answer:\n2002\u201303 NBA season", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.0j2dtly -> sports.sports_team_roster.team -> Boston Celtics\n# Answer:\nBoston Celtics", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.0j2dtly -> sports.sports_team_roster.position -> Center\n# Answer:\nCenter", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.04fw72b -> sports.sports_team_roster.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.season -> 1992\u201393 NBA season\n# Answer:\n1992\u201393 NBA season"], "ground_truth": ["Orlando Magic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.3529411764705882, "path_precision": 0.3, "path_recall": 0.42857142857142855, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1238", "prediction": ["# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.character_created_by -> Walt Disney -> award.award_winner.awards_won -> m.010qccn6\n# Answer:\nWalt Disney", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.character_created_by -> Walt Disney -> award.award_winner.awards_won -> m.09dznhz\n# Answer:\nWalt Disney", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.character_created_by -> Walt Disney -> award.award_nominee.award_nominations -> m.05bs963\n# Answer:\nWalt Disney", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.character_created_by -> Walt Disney -> award.award_winner.awards_won -> m.0_gjwtk\n# Answer:\nWalt Disney", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.character_created_by -> Walt Disney -> film.director.film -> The Band Concert\n# Answer:\nWalt Disney", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.character_created_by -> Walt Disney -> award.award_nominee.award_nominations -> m.097k9k3\n# Answer:\nWalt Disney", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.character_created_by -> Ub Iwerks -> film.director.film -> The Reluctant Dragon\n# Answer:\nUb Iwerks", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.character_created_by -> Ub Iwerks -> common.topic.notable_types -> Film director\n# Answer:\nUb Iwerks", "# Reasoning Path:\nMickey Mouse -> film.film_character.portrayed_in_films -> m.0116m9z7 -> film.performance.film -> Mickey's Mechanical House\n# Answer:\nMickey's Mechanical House", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.character_created_by -> Walt Disney -> award.award_nominee.award_nominations -> m.0gkdxvp\n# Answer:\nWalt Disney"], "ground_truth": ["Walt Disney", "Ub Iwerks"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1239", "prediction": ["# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94520 -> location.location.geolocation -> m.03dnk1k\n# Answer:\n94520", "# Reasoning Path:\nConcord -> common.topic.article -> m.0qyn0\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94520 -> location.location.geometry -> m.058s25t\n# Answer:\n94520", "# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94520 -> common.topic.notable_types -> Postal Code\n# Answer:\n94520", "# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94529 -> common.topic.notable_types -> Postal Code\n# Answer:\n94529", "# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94518 -> location.location.geometry -> m.056d_lq\n# Answer:\n94518", "# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94518 -> location.location.containedby -> Contra Costa County\n# Answer:\n94518", "# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94529 -> location.postal_code.country -> United States of America\n# Answer:\n94529", "# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94529 -> common.topic.notable_for -> g.1257lx6mw\n# Answer:\n94529", "# Reasoning Path:\nConcord -> location.location.people_born_here -> Alex Sanchez -> common.topic.notable_for -> g.1258jb99z\n# Answer:\nAlex Sanchez"], "ground_truth": ["94529", "94527", "94521", "94519", "94520", "94518", "94524", "94522"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.5106382978723405, "ans_precission": 0.8, "ans_recall": 0.375, "path_f1": 0.5106382978723405, "path_precision": 0.8, "path_recall": 0.375, "path_ans_f1": 0.5106382978723405, "path_ans_precision": 0.8, "path_ans_recall": 0.375}
{"id": "WebQTest-124", "prediction": ["# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.region -> Africa\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> base.rosetta.languoid.parent -> Arabic Group\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.writing_system -> Arabic alphabet\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Nobiin Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nNobiin Language", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Nobiin Language -> common.topic.notable_types -> Human Language\n# Answer:\nNobiin Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.writing_system -> Arabic alphabet\n# Answer:\nArabic Language", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Sudan\n# Answer:\nArabic Language"], "ground_truth": ["Modern Standard Arabic"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1240", "prediction": ["# Reasoning Path:\nMichael Vick -> american_football.football_player.rushing -> m.07sh2b2 -> american_football.player_rushing_statistics.team -> Philadelphia Eagles\n# Answer:\nPhiladelphia Eagles", "# Reasoning Path:\nMichael Vick -> american_football.football_player.rushing -> m.07tdd11 -> american_football.player_rushing_statistics.team -> Philadelphia Eagles\n# Answer:\nPhiladelphia Eagles", "# Reasoning Path:\nMichael Vick -> american_football.football_player.rushing -> m.07vzkbw -> american_football.player_rushing_statistics.team -> Philadelphia Eagles\n# Answer:\nPhiladelphia Eagles", "# Reasoning Path:\nMichael Vick -> american_football.football_player.rushing -> m.07sh2b2 -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nMichael Vick -> american_football.football_player.rushing -> m.07tdd11 -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nMichael Vick -> people.person.education -> m.0n1k_2y -> education.education.institution -> Warwick High School\n# Answer:\nWarwick High School", "# Reasoning Path:\nMichael Vick -> american_football.football_player.rushing -> m.07vzkbw -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nMichael Vick -> common.topic.image -> VICKpb crop -> common.image.size -> m.03sv987\n# Answer:\nVICKpb crop", "# Reasoning Path:\nMichael Vick -> people.person.education -> m.0hpcsnj -> education.education.institution -> Virginia Tech\n# Answer:\nVirginia Tech", "# Reasoning Path:\nMichael Vick -> common.topic.image -> Vick 6 -> common.image.size -> m.0bct9py\n# Answer:\nVick 6"], "ground_truth": ["New York Jets"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1241", "prediction": ["# Reasoning Path:\nColumbia River -> geography.river.origin -> Columbia Lake -> common.topic.notable_types -> Lake\n# Answer:\nColumbia Lake", "# Reasoning Path:\nColumbia River -> geography.river.origin -> Columbia Lake -> location.location.containedby -> British Columbia\n# Answer:\nColumbia Lake", "# Reasoning Path:\nColumbia River -> location.location.events -> Robert Gray's Columbia River expedition -> time.event.locations -> North America\n# Answer:\nRobert Gray's Columbia River expedition", "# Reasoning Path:\nColumbia River -> geography.river.origin -> Columbia Lake -> location.location.containedby -> Canada\n# Answer:\nColumbia Lake", "# Reasoning Path:\nColumbia River -> location.location.events -> Robert Gray's Columbia River expedition -> common.topic.image -> Capt Robert Gray\n# Answer:\nRobert Gray's Columbia River expedition", "# Reasoning Path:\nColumbia River -> location.location.events -> Robert Gray's Columbia River expedition -> common.topic.notable_for -> g.1256p97hd\n# Answer:\nRobert Gray's Columbia River expedition", "# Reasoning Path:\nColumbia River -> geography.body_of_water.bridges -> Vernita Bridge -> common.topic.notable_for -> g.12564bfbm\n# Answer:\nVernita Bridge", "# Reasoning Path:\nColumbia River -> geography.body_of_water.bridges -> Vantage Bridge -> freebase.valuenotation.has_value -> Structural Height\n# Answer:\nVantage Bridge", "# Reasoning Path:\nColumbia River -> geography.body_of_water.bridges -> Vantage Bridge -> common.topic.image -> Columbia River 1\n# Answer:\nVantage Bridge", "# Reasoning Path:\nColumbia River -> geography.body_of_water.bridges -> Vernita Bridge -> freebase.valuenotation.has_value -> Clearance Below\n# Answer:\nVernita Bridge"], "ground_truth": ["Columbia Lake"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1242", "prediction": ["# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Deathly Hallows -> book.written_work.previous_in_series -> Harry Potter and the Half-Blood Prince\n# Answer:\nHarry Potter and the Deathly Hallows", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Deathly Hallows -> book.book.editions -> Harry Potter and the Deathly Hallows (Book 7) (Deluxe Edition)\n# Answer:\nHarry Potter and the Deathly Hallows", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Chamber of Secrets -> book.written_work.previous_in_series -> Harry Potter and the Philosopher's Stone\n# Answer:\nHarry Potter and the Chamber of Secrets", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Deathly Hallows -> book.book_edition.author_editor -> J. K. Rowling\n# Answer:\nHarry Potter and the Deathly Hallows", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.locations -> Azkaban\n# Answer:\nAzkaban", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Deathly Hallows -> book.book.editions -> Harry Potter and the Deathly Hallows (Book 7)\n# Answer:\nHarry Potter and the Deathly Hallows", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Fantastic Beasts and Where to Find Them\n# Answer:\nFantastic Beasts and Where to Find Them", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Deathly Hallows -> book.book.editions -> Harry Potter and the Deathly Hallows (Harry Potter)\n# Answer:\nHarry Potter and the Deathly Hallows", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Chamber of Secrets -> book.written_work.subjects -> England\n# Answer:\nHarry Potter and the Chamber of Secrets", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_character.has_possessed -> Deathly Hallows -> common.topic.image -> HP books\n# Answer:\nDeathly Hallows"], "ground_truth": ["Harry Potter and the Chamber of Secrets", "Harry Potter and the Goblet of Fire", "Harry Potter and the Philosopher's Stone", "Harry Potter and the Order of the Phoenix", "Harry Potter and the Deathly Hallows", "Harry Potter and the Prisoner of Azkaban", "Harry Potter and the Half-Blood Prince"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.40579710144927533, "ans_precission": 0.7, "ans_recall": 0.2857142857142857, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.6292134831460674, "path_ans_precision": 0.7, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-1244", "prediction": ["# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Courtier -> people.profession.people_with_this_profession -> Andrew Newport\n# Answer:\nCourtier", "# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Courtier -> base.descriptive_names.names.descriptive_name -> m.010b1tg9\n# Answer:\nCourtier", "# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Courtier -> common.topic.notable_for -> g.1258nrk6f\n# Answer:\nCourtier", "# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Courtier -> people.profession.people_with_this_profession -> Anne Lennard, Countess of Sussex\n# Answer:\nCourtier", "# Reasoning Path:\nWalter Raleigh -> common.topic.notable_types -> Deceased Person -> freebase.type_profile.strict_included_types -> Person\n# Answer:\nDeceased Person", "# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Courtier -> people.profession.people_with_this_profession -> Anthony Lee\n# Answer:\nCourtier", "# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Poet -> media_common.quotation_subject.quotations_about_this_subject -> As imagination bodies forth The forms of things unknown, the poet's pen Turns them to shapes\n# Answer:\nPoet", "# Reasoning Path:\nWalter Raleigh -> common.topic.notable_types -> Deceased Person -> freebase.type_hints.included_types -> Person\n# Answer:\nDeceased Person", "# Reasoning Path:\nWalter Raleigh -> common.topic.notable_types -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Sailor -> fictional_universe.character_occupation.characters_with_this_occupation -> Jack Aubrey\n# Answer:\nSailor"], "ground_truth": ["Soldier", "Poet", "Writer", "Courtier", "Sailor"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6461538461538462, "ans_precission": 0.7, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.6461538461538462, "path_ans_precision": 0.7, "path_ans_recall": 0.6}
{"id": "WebQTest-1245", "prediction": ["# Reasoning Path:\nEuro -> finance.currency.countries_used -> \u00c5land Islands -> location.administrative_division.country -> Finland\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nEuro -> finance.currency.countries_used -> Netherlands -> location.country.currency_used -> United States Dollar\n# Answer:\nNetherlands", "# Reasoning Path:\nEuro -> finance.currency.countries_used -> \u00c5land Islands -> location.location.primarily_containedby -> Finland\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nEuro -> finance.currency.countries_used -> \u00c5land Islands -> location.location.people_born_here -> Paul Gustavson\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nEuro -> finance.currency.countries_used -> Andorra -> location.country.form_of_government -> Unitary state\n# Answer:\nAndorra", "# Reasoning Path:\nEuro -> finance.currency.countries_formerly_used -> Serbia and Montenegro -> location.country.currency_formerly_used -> Yugoslav dinar\n# Answer:\nSerbia and Montenegro", "# Reasoning Path:\nEuro -> finance.currency.countries_used -> \u00c5land Islands -> location.location.people_born_here -> Gabriel Fliflet\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nEuro -> finance.currency.countries_used -> Netherlands -> location.location.events -> 2010 European Summer Time\n# Answer:\nNetherlands", "# Reasoning Path:\nEuro -> common.topic.webpage -> m.04lst2j -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nEuro -> finance.currency.countries_used -> \u00c5land Islands -> location.location.people_born_here -> Georg August Wallin\n# Answer:\n\u00c5land Islands"], "ground_truth": ["Austria", "Guadeloupe", "Monaco", "Slovakia", "Germany", "Vatican City", "Spain", "Saint Pierre and Miquelon", "Italy", "\u00c5land Islands", "Malta", "France", "Republic of Ireland", "Andorra", "Estonia", "Finland", "Kingdom of the Netherlands", "Province of Varese", "Saint Barth\u00e9lemy", "Montenegro", "Collectivity of Saint Martin", "Martinique", "Varese", "Greece", "Caribbean special municipalities of the Netherlands", "Luxembourg", "Republic of Kosovo", "San Marino", "Lithuania", "Slovenia", "Netherlands", "Latvia", "Mayotte", "Portugal", "Cyprus", "Belgium", "Zimbabwe"], "ans_acc": 0.13513513513513514, "ans_hit": 1, "ans_f1": 0.19302949061662197, "ans_precission": 0.9, "ans_recall": 0.10810810810810811, "path_f1": 0.14723926380368096, "path_precision": 0.8, "path_recall": 0.08108108108108109, "path_ans_f1": 0.23498694516971277, "path_ans_precision": 0.9, "path_ans_recall": 0.13513513513513514}
{"id": "WebQTest-1246", "prediction": ["# Reasoning Path:\nBarbados -> location.country.currency_used -> Barbadian dollar -> common.topic.notable_for -> g.125b468qw\n# Answer:\nBarbadian dollar", "# Reasoning Path:\nBarbados -> location.country.currency_used -> Barbadian dollar -> common.topic.article -> m.05hy7y\n# Answer:\nBarbadian dollar", "# Reasoning Path:\nBarbados -> location.statistical_region.official_development_assistance -> g.1hhc37pj3\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBarbados -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc37ppn\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nBarbados -> location.statistical_region.official_development_assistance -> g.1hhc382cm\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBarbados -> location.statistical_region.official_development_assistance -> g.1hhc38m3z\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBarbados -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc37x3k\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nBarbados -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc3b5td\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp"], "ground_truth": ["Barbadian dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1247", "prediction": ["# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Voice Actor -> people.profession.specialization_of -> Actor\n# Answer:\nVoice Actor", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Voice Actor -> common.topic.notable_types -> Profession\n# Answer:\nVoice Actor", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Film director -> freebase.type_profile.published -> Published\n# Answer:\nFilm director", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Film director -> common.topic.subject_of -> Mervin Praison\n# Answer:\nFilm director", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Film director -> freebase.type_hints.included_types -> Person\n# Answer:\nFilm director", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Actor -> people.profession.specializations -> Voice Actor\n# Answer:\nActor", "# Reasoning Path:\nAngelina Jolie -> book.author.book_editions_published -> Notes from My Travels -> common.topic.notable_types -> Book\n# Answer:\nNotes from My Travels", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Film director -> common.topic.subject_of -> Eugenio Polgovsky\n# Answer:\nFilm director", "# Reasoning Path:\nAngelina Jolie -> award.award_nominee.award_nominations -> m.010wr37v -> award.award_nomination.nominated_for -> Maleficent\n# Answer:\nMaleficent", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Film director -> common.topic.subject_of -> Lew M. Parry\n# Answer:\nFilm director"], "ground_truth": ["Voice Actor", "Model", "Writer", "Film director", "Author", "Film Producer", "Screenwriter", "Actor"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5106382978723405, "ans_precission": 0.8, "ans_recall": 0.375, "path_f1": 0.5106382978723405, "path_precision": 0.8, "path_recall": 0.375, "path_ans_f1": 0.6428571428571429, "path_ans_precision": 0.9, "path_ans_recall": 0.5}
{"id": "WebQTest-1248", "prediction": ["# Reasoning Path:\nNineveh -> location.location.containedby -> Nineveh Governorate -> location.location.containedby -> Iraq\n# Answer:\nNineveh Governorate", "# Reasoning Path:\nNineveh -> location.location.containedby -> Nineveh Governorate -> location.administrative_division.capital -> Mosul\n# Answer:\nNineveh Governorate", "# Reasoning Path:\nNineveh -> location.location.containedby -> Nineveh Governorate -> location.location.contains -> Baiji\n# Answer:\nNineveh Governorate", "# Reasoning Path:\nNineveh -> location.location.containedby -> Iraq -> location.location.containedby -> Eurasia\n# Answer:\nIraq", "# Reasoning Path:\nNineveh -> location.location.containedby -> Nineveh Governorate -> location.location.contains -> Bakhdida\n# Answer:\nNineveh Governorate", "# Reasoning Path:\nNineveh -> location.location.containedby -> Iraq -> location.location.containedby -> Arab world\n# Answer:\nIraq", "# Reasoning Path:\nNineveh -> location.location.containedby -> Iraq -> location.country.administrative_divisions -> Nineveh Governorate\n# Answer:\nIraq", "# Reasoning Path:\nNineveh -> common.topic.image -> A proposed flag for the Chaldean people. -> common.image.appears_in_topic_gallery -> Nimrud\n# Answer:\nA proposed flag for the Chaldean people.", "# Reasoning Path:\nNineveh -> location.location.containedby -> Nineveh Governorate -> location.location.contains -> Hatra\n# Answer:\nNineveh Governorate", "# Reasoning Path:\nNineveh -> location.location.containedby -> Iraq -> location.location.containedby -> Asia\n# Answer:\nIraq"], "ground_truth": ["Nineveh Governorate", "Iraq"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1249", "prediction": ["# Reasoning Path:\nNew Mexico -> government.governmental_jurisdiction.official_symbols -> m.0hzbf17 -> location.location_symbol_relationship.symbol -> Yucca flower\n# Answer:\nYucca flower", "# Reasoning Path:\nNew Mexico -> government.governmental_jurisdiction.official_symbols -> m.04st6n2 -> location.location_symbol_relationship.symbol -> New Mexico spadefoot toad\n# Answer:\nNew Mexico spadefoot toad", "# Reasoning Path:\nNew Mexico -> government.governmental_jurisdiction.official_symbols -> m.04st870 -> location.location_symbol_relationship.symbol -> Greater roadrunner\n# Answer:\nGreater roadrunner", "# Reasoning Path:\nNew Mexico -> government.governmental_jurisdiction.official_symbols -> m.0hzbf17 -> location.location_symbol_relationship.Kind_of_symbol -> State flower\n# Answer:\nState flower", "# Reasoning Path:\nNew Mexico -> book.book_subject.works -> ...And Now Miguel -> book.written_work.subjects -> United States of America\n# Answer:\n...And Now Miguel", "# Reasoning Path:\nNew Mexico -> government.governmental_jurisdiction.official_symbols -> m.04st6n2 -> location.location_symbol_relationship.Kind_of_symbol -> State Amphibian\n# Answer:\nState Amphibian", "# Reasoning Path:\nNew Mexico -> book.book_subject.works -> ...And Now Miguel -> book.book.genre -> Children's literature\n# Answer:\n...And Now Miguel", "# Reasoning Path:\nNew Mexico -> government.governmental_jurisdiction.official_symbols -> m.04st870 -> location.location_symbol_relationship.Kind_of_symbol -> State bird\n# Answer:\nState bird", "# Reasoning Path:\nNew Mexico -> book.book_subject.works -> ...And Now Miguel -> book.written_work.subjects -> Men\n# Answer:\n...And Now Miguel", "# Reasoning Path:\nNew Mexico -> book.book_subject.works -> ...And Now Miguel -> book.written_work.original_language -> English Language\n# Answer:\n...And Now Miguel"], "ground_truth": ["Yucca flower"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-125", "prediction": ["# Reasoning Path:\nAbraham -> people.person.children -> Ishmael -> people.person.parents -> Hagar\n# Answer:\nIshmael", "# Reasoning Path:\nAbraham -> people.person.children -> Isaac -> people.person.children -> Jacob\n# Answer:\nIsaac", "# Reasoning Path:\nAbraham -> people.person.children -> Jokshan -> people.person.sibling_s -> m.048b8rb\n# Answer:\nJokshan", "# Reasoning Path:\nAbraham -> people.person.children -> Ishmael -> common.topic.webpage -> m.04m0j0k\n# Answer:\nIshmael", "# Reasoning Path:\nAbraham -> people.person.children -> Isaac -> people.person.children -> Esau\n# Answer:\nIsaac", "# Reasoning Path:\nAbraham -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#range -> Person\n# Answer:\nChildren", "# Reasoning Path:\nAbraham -> people.person.children -> Ishmael -> common.topic.image -> Expulsion of Ishmael and His Mother\n# Answer:\nIshmael", "# Reasoning Path:\nAbraham -> people.person.children -> Isaac -> common.topic.image -> AbrahamIsaac\n# Answer:\nIsaac", "# Reasoning Path:\nAbraham -> people.person.children -> Jokshan -> common.topic.notable_for -> g.125bxhcfr\n# Answer:\nJokshan", "# Reasoning Path:\nAbraham -> people.person.children -> Ishmael -> common.topic.webpage -> m.09ytykz\n# Answer:\nIshmael"], "ground_truth": ["Midian", "Isaac", "Zimran", "Shuah", "Medan", "Jokshan", "Ishmael", "Ishbak"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.5294117647058825, "ans_precission": 0.9, "ans_recall": 0.375, "path_f1": 0.5294117647058825, "path_precision": 0.9, "path_recall": 0.375, "path_ans_f1": 0.5294117647058825, "path_ans_precision": 0.9, "path_ans_recall": 0.375}
{"id": "WebQTest-1250", "prediction": ["# Reasoning Path:\nNational FFA Organization -> organization.organization.headquarters -> m.0wkkztl -> location.mailing_address.citytown -> Alexandria\n# Answer:\nAlexandria", "# Reasoning Path:\nNational FFA Organization -> organization.non_profit_organization.registered_with -> m.0pqg9g3 -> organization.non_profit_registration.registering_agency -> Internal Revenue Service\n# Answer:\nInternal Revenue Service", "# Reasoning Path:\nNational FFA Organization -> organization.organization.headquarters -> m.0wkkztl -> location.mailing_address.state_province_region -> Virginia\n# Answer:\nVirginia", "# Reasoning Path:\nNational FFA Organization -> organization.organization.headquarters -> m.0wkkztl -> location.mailing_address.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nNational FFA Organization -> organization.non_profit_organization.registered_with -> m.0pqg9g3 -> organization.non_profit_registration.registered_as -> 501(c)(3)\n# Answer:\n501(c)(3)", "# Reasoning Path:\nNational FFA Organization -> organization.organization.sectors -> Agriculture -> business.industry.companies -> AMERICAN CRYSTAL SUGAR CO /MN/\n# Answer:\nAgriculture", "# Reasoning Path:\nNational FFA Organization -> organization.organization.sectors -> Agriculture -> common.topic.article -> m.0hkr\n# Answer:\nAgriculture", "# Reasoning Path:\nNational FFA Organization -> organization.organization.sectors -> Agriculture -> freebase.valuenotation.has_value -> NAICS 2007 code\n# Answer:\nAgriculture", "# Reasoning Path:\nNational FFA Organization -> organization.organization.sectors -> Agriculture -> business.industry.companies -> AVEBE\n# Answer:\nAgriculture", "# Reasoning Path:\nNational FFA Organization -> organization.organization.sectors -> Agriculture -> business.industry.companies -> Adler Seeds\n# Answer:\nAgriculture"], "ground_truth": ["Alexandria"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1251", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> West Germany -> location.country.currency_used -> Deutsche Mark\n# Answer:\nWest Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Vaduz\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria -> location.country.languages_spoken -> Bavarian Language\n# Answer:\nAustria", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.country.first_level_divisions -> Vaduz\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> West Germany -> location.location.containedby -> Western Europe\n# Answer:\nWest Germany", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Switzerland\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0102xvg7 -> education.education.student -> Sigurd Burckhardt\n# Answer:\nSigurd Burckhardt", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Eschen\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.country.first_level_divisions -> Balzers\n# Answer:\nLiechtenstein"], "ground_truth": ["Austria", "Liechtenstein", "Germany", "Luxembourg", "Switzerland", "East Germany", "Belgium"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.5581395348837209, "ans_precission": 0.8, "ans_recall": 0.42857142857142855, "path_f1": 0.3870967741935483, "path_precision": 0.6, "path_recall": 0.2857142857142857, "path_ans_f1": 0.6990291262135921, "path_ans_precision": 0.9, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-1252", "prediction": ["# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_winning_work.awards_won -> m.09k3pgy -> award.award_honor.award_winner -> Miranda Otto\n# Answer:\nMiranda Otto", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_nominated_work.award_nominations -> m.09k3q0p -> award.award_nomination.award_nominee -> Miranda Otto\n# Answer:\nMiranda Otto", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_winning_work.awards_won -> m.09k3pgy -> award.award_honor.ceremony -> 9th Critics' Choice Awards\n# Answer:\n9th Critics' Choice Awards", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_winning_work.awards_won -> m.09k3pgy -> award.award_honor.award -> Critics' Choice Movie Award for Best Acting Ensemble\n# Answer:\nCritics' Choice Movie Award for Best Acting Ensemble", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_winning_work.awards_won -> m.0n7xsws -> award.award_honor.award_winner -> Miranda Otto\n# Answer:\nMiranda Otto", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_nominated_work.award_nominations -> m.0b4d5rz -> award.award_nomination.award_nominee -> Miranda Otto\n# Answer:\nMiranda Otto", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_winning_work.awards_won -> m.09k3pgy -> award.award_honor.award_winner -> Andy Serkis\n# Answer:\nAndy Serkis", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_nominated_work.award_nominations -> m.09k3q0p -> award.award_nomination.award_nominee -> Andy Serkis\n# Answer:\nAndy Serkis", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_nominated_work.award_nominations -> m.09k3q0p -> award.award_nomination.ceremony -> 9th Critics' Choice Awards\n# Answer:\n9th Critics' Choice Awards", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_winning_work.awards_won -> m.09k3pgy -> award.award_honor.award_winner -> Bernard Hill\n# Answer:\nBernard Hill"], "ground_truth": ["Miranda Otto"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5333333333333333, "path_precision": 0.4, "path_recall": 0.8, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1254", "prediction": ["# Reasoning Path:\n2012 Masters Tournament -> time.event.locations -> Augusta National Golf Course -> location.location.events -> 2013 Masters Tournament\n# Answer:\nAugusta National Golf Course", "# Reasoning Path:\n2012 Masters Tournament -> time.event.locations -> Augusta National Golf Course -> common.topic.notable_for -> g.12556r5p0\n# Answer:\nAugusta National Golf Course", "# Reasoning Path:\n2012 Masters Tournament -> time.event.locations -> Augusta National Golf Course -> location.location.events -> 2007 Masters Tournament\n# Answer:\nAugusta National Golf Course", "# Reasoning Path:\n2012 Masters Tournament -> time.event.locations -> Augusta National Golf Course -> award.ranked_item.appears_in_ranked_lists -> m.04tk4kl\n# Answer:\nAugusta National Golf Course", "# Reasoning Path:\n2012 Masters Tournament -> time.event.locations -> Augusta National Golf Course -> location.location.events -> 2008 Masters Tournament\n# Answer:\nAugusta National Golf Course", "# Reasoning Path:\n2012 Masters Tournament -> time.event.locations -> Augusta National Golf Course -> award.ranked_item.appears_in_ranked_lists -> m.04v6l04\n# Answer:\nAugusta National Golf Course", "# Reasoning Path:\n2012 Masters Tournament -> freebase.valuenotation.is_reviewed -> End date -> type.property.schema -> Event\n# Answer:\nEnd date", "# Reasoning Path:\n2012 Masters Tournament -> freebase.valuenotation.is_reviewed -> Location(s) -> owl#inverseOf -> Events\n# Answer:\nLocation(s)", "# Reasoning Path:\n2012 Masters Tournament -> common.topic.notable_types -> Competition -> freebase.type_hints.included_types -> Event\n# Answer:\nCompetition", "# Reasoning Path:\n2012 Masters Tournament -> freebase.valuenotation.is_reviewed -> End date -> rdf-schema#range -> Date/Time\n# Answer:\nEnd date"], "ground_truth": ["Augusta National Golf Course"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1255", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.treatments -> Radiation therapy\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.treatments -> Cervical conization\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> common.topic.webpage -> m.04tnn4f\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> people.cause_of_death.people -> Yvette Wilson\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.person.gender -> Female -> medicine.risk_factor.diseases -> Ovarian cancer\n# Answer:\nFemale", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> common.topic.webpage -> m.09wbnt0\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.treatments -> Erlotinib\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.person.gender -> Female -> base.thoroughbredracing.thoroughbred_racehorse_sex.horses_of_this_sex -> Apology\n# Answer:\nFemale", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> common.topic.webpage -> m.09ybml5\n# Answer:\nCervical cancer"], "ground_truth": ["Cervical cancer", "Cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1256", "prediction": ["# Reasoning Path:\nTerrell Owens -> american_football.football_player.receiving -> m.09rltd7 -> american_football.player_receiving_statistics.team -> Buffalo Bills\n# Answer:\nBuffalo Bills", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.receiving -> m.09gr3sg -> american_football.player_receiving_statistics.team -> Buffalo Bills\n# Answer:\nBuffalo Bills", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.rushing -> m.07t77sv -> american_football.player_rushing_statistics.team -> Buffalo Bills\n# Answer:\nBuffalo Bills", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.receiving -> m.07ybzft -> american_football.player_receiving_statistics.team -> Buffalo Bills\n# Answer:\nBuffalo Bills", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.receiving -> m.09rltd7 -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.receiving -> m.09gr3sg -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.rushing -> m.07t77sv -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.rushing -> m.07mmd6t -> american_football.player_rushing_statistics.team -> Buffalo Bills\n# Answer:\nBuffalo Bills", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.rushing -> m.0791bhh -> american_football.player_rushing_statistics.season -> 2008 NFL season\n# Answer:\n2008 NFL season", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.receiving -> m.07ybzft -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season"], "ground_truth": ["Texas Revolution", "Buffalo Bills", "Cincinnati Bengals"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.5, "ans_recall": 0.3333333333333333, "path_f1": 0.15873015873015875, "path_precision": 0.5, "path_recall": 0.09433962264150944, "path_ans_f1": 0.4, "path_ans_precision": 0.5, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1257", "prediction": ["# Reasoning Path:\nDenmark -> location.location.containedby -> South Carolina -> location.location.containedby -> United States, with Territories\n# Answer:\nSouth Carolina", "# Reasoning Path:\nDenmark -> location.location.containedby -> South Carolina -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nSouth Carolina", "# Reasoning Path:\nDenmark -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nDenmark -> location.location.containedby -> South Carolina -> location.location.containedby -> United States of America\n# Answer:\nSouth Carolina", "# Reasoning Path:\nDenmark -> location.location.containedby -> Bamberg County -> location.location.containedby -> South Carolina\n# Answer:\nBamberg County", "# Reasoning Path:\nDenmark -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> South Carolina\n# Answer:\nUnited States of America", "# Reasoning Path:\nDenmark -> location.citytown.postal_codes -> 29042 -> location.location.containedby -> Bamberg County\n# Answer:\n29042", "# Reasoning Path:\nDenmark -> location.location.containedby -> South Carolina -> location.location.containedby -> Contiguous United States\n# Answer:\nSouth Carolina", "# Reasoning Path:\nDenmark -> location.location.containedby -> South Carolina -> location.administrative_division.capital -> m.0jvw4r3\n# Answer:\nSouth Carolina", "# Reasoning Path:\nDenmark -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> South Carolina\n# Answer:\nUnited States of America"], "ground_truth": ["Bamberg County", "United States of America", "South Carolina"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1259", "prediction": ["# Reasoning Path:\nErnest Hemingway -> influence.influence_node.influenced -> Andre Dubus -> influence.influence_node.influenced_by -> Richard Yates\n# Answer:\nAndre Dubus", "# Reasoning Path:\nErnest Hemingway -> influence.influence_node.influenced -> Andre Dubus -> people.person.gender -> Male\n# Answer:\nAndre Dubus", "# Reasoning Path:\nErnest Hemingway -> influence.influence_node.influenced -> Ann Beattie -> influence.influence_node.influenced_by -> Richard Yates\n# Answer:\nAnn Beattie", "# Reasoning Path:\nErnest Hemingway -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> heart attack\n# Answer:\nMale", "# Reasoning Path:\nErnest Hemingway -> influence.influence_node.influenced -> Andre Dubus -> influence.influence_node.influenced_by -> John Cheever\n# Answer:\nAndre Dubus", "# Reasoning Path:\nErnest Hemingway -> influence.influence_node.influenced -> Andre Dubus -> people.person.profession -> Teacher\n# Answer:\nAndre Dubus", "# Reasoning Path:\nErnest Hemingway -> common.topic.webpage -> m.09wc371 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nErnest Hemingway -> influence.influence_node.influenced -> Ann Beattie -> influence.influence_node.influenced -> Tao Lin\n# Answer:\nAnn Beattie", "# Reasoning Path:\nErnest Hemingway -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Cardiovascular disease\n# Answer:\nMale", "# Reasoning Path:\nErnest Hemingway -> influence.influence_node.influenced -> Andre Dubus -> influence.influence_node.influenced_by -> Anton Chekhov\n# Answer:\nAndre Dubus"], "ground_truth": ["Oeuvres Romanesques Vol. 1", "Der alte Mann und das Meer, und andere Meisterwerke", "The Letters of Ernest Hemingway: Volume 1, 1907-1922", "Las Nieves del Kilimanjaro", "Winner Take Nothing", "The president vanquishes", "The old man and the sea. (Lernmaterialien)", "The Old Man and the Sea (MacMillan Literature Series, Signature Edition)", "Bullfighting", "The Capital of the World", "Hemingway on Hunting (On)", "For Whom the Bell Tolls", "The old man and the sea.", "Across the River and Into the Trees", "Death in the Afternoon", "Major Works of Ernest Hemingway", "Wada'an Lilseelah", "Death in the afternoon", "The tradesman's return", "Men at war", "Storie della guerra de Spagna", "True at first light", "Hemingway and the mechanism of fame", "The Sun Also Rises (Archive of Literary Documents)", "The Spanish War", "Hemingway on war", "Los Asesinos", "Genio after Josie", "Sun Also Rises", "The Garden of Eden", "The Old Man and the Sea (Old Man & the Sea Illus Gift Ed C)", "To Have and Have Not (To Have & Have Not Srs)", "g.11b6s8f7gb", "Notes on dangerous game", "The denunciation", "In Our Time", "Three stories", "Death in the afternoon.", "Hayat Francis Macumber", "Collected poems", "The Cambridge Edition of the Letters of Ernest Hemingway", "Green hills of Africa.", "The Revolutionist", "Valentine", "On Paris", "Neues vom Festland. Stories", "A folyo n at, a fa k koze", "Across The River And Into The Trees", "The wild years", "Ernest Hemingway: The Collected Stories", "The Old Man and the Sea. (Lernmaterialien)", "50000 dollars", "Men Without Women (Arrow Classic)", "The garden of Eden", "The Fifth Column and the First Forty-Nine Stories", "In our time", "In Our Time (In Our Time Hre)", "Winner Take Nothing (Scribner Classic)", "Sun Also Rises (Library Reprint Editions)", "Defense of dirty words", "The Old Man and the Sea", "88 Poems", "The fifth column and four stories of the Spanish Civil War", "To Have and Have Not", "On Writing", "Across the river and into the trees.", "The sights of Whitehead Street", "The garden of Eden.", "Die sch\u00f6nsten Geschichten Afrikas", "Death in the Afternoon (Death in the Afternoon Hre)", "The Complete Short Stories of Ernest Hemingway: The Finca Vig\u00eda Edition", "La quinta columna", "To have and have not.", "The Old Man and the Sea (A Scribner Classic)", "The friend of Spain", "Winner take nothing.", "Old Man and the Sea (Special Student)", "Marlin off Cuba", "To Have and Have Not (To Have & Have Not Hre)", "The malady of power", "The sun also rises.", "The Torrents of Spring", "A Farewell To Arms", "Klokkene ringer for deg", "ACROSS THE RIVER AND INTO THE TREES", "Dear Papa, dear Hotch", "By-line, Ernest Hemingway", "Shootism versus sport", "Sobranie sochineni\u012d v chetyrekh tomakh", "The Christmas gift", "Islands in the Stream", "Complete poems", "Zhan di zhong sheng", "The Killers", "Sailfish off Mombasa", "A Simple Enquiry", "Fable", "Winner take nothing", "Big Two-Hearted River", "Across the river and into the trees", "The dangerous summer", "The Snows of Kilimanjaro", "The Old Man and The Sea (Annual Review of the Institute for Information Studies)", "Complete Poems", "A Farewell to Arms", "Nieves del Kilimanjaro, Las", "The faithful bull", "Ernest Hemingway in high school", "El Buen Leon", "The Short Happy Life of Francis Macomber", "g.1ym_l5zt2", "Vier Stories aus dem spanischen B\u00fcrgerkrieg", "Old Man and the Sea", "Obras Completas:Por qui\u00e9n doblan las campanas, El viejo y el mar", "A divine gesture", "Soldier's Home", "Relatos Ineditos", "Lao ren yu hai", "The Undefeated", "Die Stories", "Hills Like White Elephants", "Men without women", "The Old Man and the Sea (Old Man & the Sea Tr)", "By-Line", "The End of Something", "Bullfighting, sport & industry", "The soul of Spain with Mc. Almon and Bird the publishers", "The Sun Also Rises", "Night before battle", "The snows of Kilimanjaro", "Proshchai\u0306 oruzhie!", "The enduring Hemingway", "In Another Country", "Ernest Hemingway's Apprenticeship", "The short stories of Ernest Hemingway", "a.d. in Africa", "The TORRENTS OF SPRING", "Million dollar fright", "Old Man and the Sea (New Windmill)", "Monologue to the maestro", "The Butterfly and the Tank...fiction", "Un corresponsal llamado Hemingway", "Voor wien de klok luidt", "Best Of Bad Hemingway: Vol 1", "quarantanove racconti", "Der al\u1e6der un der yam", "Men Without Women", "There she breaches!", "To have and have not", "En ligne", "Cat in the Rain", "The colected poems of Ernest Hemingway", "A Moveable Feast", "Ernest Hemingway Selected Letters 1917\u20131961", "El Toro Fiel / the Faithful Bull", "The Gambler, the Nun, and the Radio", "Dateline: Toronto", "For Whom the Bell Tolls (Audio Library Classics)", "Three Stories and Ten Poems", "In Harry's Bar in Venice and More", "The snows of Kilimanjaro and other stories", "Green Hills of Africa", "Ernest Hemingway: Cub Reporter", "Reportagen 1920 - 1924", "Sun Also Rises (Sun Also Rises Tr)", "Farewell to Arms (A Scribner Classic)", "Hemingway on Fishing", "Conversations with Ernest Hemingway", "The only thing that counts", "E.H, apprenti reporter", "Bi xatire\u0302 s\u0131\u0302lehan", "A Farewell to Arms (Scribner Classics)", "The essential Hemingway", "The great blue river", "Across the River and into the Trees (Arrow Classic)", "Give us a prescription, Doctor", "For whom the bell tolls", "The fifth column, and four stories of the Spanish Civil War.", "True at First Light", "A Paris letter", "Up in Michigan", "The circus", "A moveable feast", "Across the River and into the Trees", "A Moveable Feast (Scribner Classic)", "The Dangerous Summer", "The old man and the sea =", "Hemingway", "Marlin off the Morro", "The Spanish earth", "Green hills of Africa", "A Clean, Well-Lighted Place", "a.d. southern style", "Che Ti Dice La Patria?", "Old Man and the Sea/ (Cassette)", "At have og ikke have", "A Farewell to Arms (Farewell to Arms Tr)", "The Fifth Column and Four Stories of the Spanish Civil War", "Ernest Hemingway, selected letters, 1917-1961", "For Whom the Bell Tolls (Vintage Classics)", "The Old Man and the Sea (Vintage Classics)", "A Day's Wait", "A farewell to arms", "Wings always over Africa", "On the blue water", "Hemingway on hunting", "The sun also rises", "The Old Man And The Sea", "Green Hills of Africa (Hudson River editions)", "For Whom the Bell Tolls (War Promo)", "The old man and the sea", "True At First Light", "The good lion", "Under Kilimanjaro", "Green Hills of Africa (Vintage Classics)", "The Sun Also Rises (A Scribner classic)", "A Farewell to Arms (Vintage Classics)", "He who gets slap happy", "By-Line Ernest Hemingway", "Fifth Column and Four Stories of the Spanish Civil War (Fifth Column & 4 Stories Hre)", "The collected poems of Ernest Hemingway", "The Old Man and the Sea (York Notes)", "On being shot again", "Die Wahrheit im Morgenlicht. Eine afrikanische Safari.", "Indian Camp", "L'Etrange contr\u00e9e", "The Battler", "The torrents of spring", "The Nick Adams Stories", "Le chaud et le froid", "Now I Lay Me", "After the Storm", "The Hemingway reader", "Green Hills of Africa (Scribner Classic)", "A Moveable Feast (Moveable Feast Srs)", "Three novels: The sun also rises; A farewell to arms; The old man and the sea", "De vye zanmi"], "ans_acc": 0.004219409282700422, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.008403361344537815, "path_ans_precision": 1.0, "path_ans_recall": 0.004219409282700422}
{"id": "WebQTest-126", "prediction": ["# Reasoning Path:\nJana Gana Mana -> music.composition.composer -> Rabindranath Tagore -> people.person.profession -> Visual Artist\n# Answer:\nRabindranath Tagore", "# Reasoning Path:\nJana Gana Mana -> music.composition.composer -> Rabindranath Tagore -> people.person.profession -> Composer\n# Answer:\nRabindranath Tagore", "# Reasoning Path:\nJana Gana Mana -> music.composition.lyricist -> Rabindranath Tagore -> people.person.profession -> Visual Artist\n# Answer:\nRabindranath Tagore", "# Reasoning Path:\nJana Gana Mana -> music.composition.recordings -> Jana Gana Mana (Indian National Song) -> music.recording.artist -> Leoncie\n# Answer:\nJana Gana Mana (Indian National Song)", "# Reasoning Path:\nJana Gana Mana -> music.composition.composer -> Rabindranath Tagore -> film.film_story_contributor.film_story_credits -> Uphaar\n# Answer:\nRabindranath Tagore", "# Reasoning Path:\nJana Gana Mana -> music.composition.composer -> Ram Singh Thakur -> common.topic.article -> m.02vx725\n# Answer:\nRam Singh Thakur", "# Reasoning Path:\nJana Gana Mana -> music.composition.composer -> Rabindranath Tagore -> people.person.quotations -> Clouds come floating into my life, no longer to carry rain or usher storm, but to add color to my sunset sky.\n# Answer:\nRabindranath Tagore", "# Reasoning Path:\nJana Gana Mana -> music.composition.composer -> Rabindranath Tagore -> film.film_story_contributor.film_story_credits -> An Obsolete Altar\n# Answer:\nRabindranath Tagore", "# Reasoning Path:\nJana Gana Mana -> music.composition.lyricist -> Rabindranath Tagore -> film.film_story_contributor.film_story_credits -> Uphaar\n# Answer:\nRabindranath Tagore", "# Reasoning Path:\nJana Gana Mana -> music.composition.composer -> Rabindranath Tagore -> people.person.profession -> Artist\n# Answer:\nRabindranath Tagore"], "ground_truth": ["Ram Singh Thakur", "Rabindranath Tagore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1260", "prediction": ["# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.article -> m.0642vqz\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> sports.sports_championship_event.season -> 2000 NFL season\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> common.topic.notable_types -> Super bowl\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> film.film_subject.films -> The Band That Wouldn't Die -> film.film.music -> Marcello Zavros\n# Answer:\nThe Band That Wouldn't Die", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> 2001 AFC Championship Game -> sports.sports_championship_event.runner_up -> Oakland Raiders\n# Answer:\n2001 AFC Championship Game", "# Reasoning Path:\nBaltimore Ravens -> film.film_subject.films -> The Band That Wouldn't Die -> film.film.subjects -> Baltimore Colts\n# Answer:\nThe Band That Wouldn't Die", "# Reasoning Path:\nBaltimore Ravens -> american_football.football_team.away_games -> Baltimore Ravens at Cincinnati Bengals, 2009-11-08 -> american_football.football_game.season -> 2009 NFL season\n# Answer:\nBaltimore Ravens at Cincinnati Bengals, 2009-11-08", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> 2001 AFC Championship Game -> sports.sports_championship_event.season -> 2000 NFL season\n# Answer:\n2001 AFC Championship Game", "# Reasoning Path:\nBaltimore Ravens -> american_football.football_team.away_games -> Baltimore Ravens at Cincinnati Bengals, 2008-11-30 -> american_football.football_game.season -> 2008 NFL season\n# Answer:\nBaltimore Ravens at Cincinnati Bengals, 2008-11-30"], "ground_truth": ["Super Bowl XXXV"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1261", "prediction": ["# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Flanders -> location.location.containedby -> Belgium\n# Answer:\nFlanders", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Flanders -> location.location.events -> 2009 Tour of Flanders\n# Answer:\nFlanders", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Passendale -> location.location.geolocation -> m.02_nfnm\n# Answer:\nPassendale", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Flanders -> location.location.events -> 2013 Flanders International Film Festival Ghent\n# Answer:\nFlanders", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Flanders -> location.location.events -> 2014 E3 Harelbeke\n# Answer:\nFlanders", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Flanders -> location.location.contains -> Zele\n# Answer:\nFlanders", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Passendale -> location.location.events -> First Battle of Passchendaele\n# Answer:\nPassendale", "# Reasoning Path:\nBattle of Passchendaele -> military.military_conflict.commanders -> m.04kxm8s -> military.military_command.military_commander -> Herbert Plumer, 1st Viscount Plumer\n# Answer:\nHerbert Plumer, 1st Viscount Plumer", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Flanders -> location.location.contains -> Wezemaal\n# Answer:\nFlanders", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Passendale -> common.topic.image -> Tyne Cot cemetery in Passchendoale\n# Answer:\nPassendale"], "ground_truth": ["Flanders", "Passendale"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1262", "prediction": ["# Reasoning Path:\nOhio -> location.location.containedby -> Midwestern United States -> location.location.containedby -> United States of America\n# Answer:\nMidwestern United States", "# Reasoning Path:\nOhio -> location.location.containedby -> Midwestern United States -> location.location.contains -> East North Central States\n# Answer:\nMidwestern United States", "# Reasoning Path:\nOhio -> location.location.containedby -> Midwestern United States -> book.book_subject.works -> Caught in the Middle: America's Heartland in the Age of Globalism\n# Answer:\nMidwestern United States", "# Reasoning Path:\nOhio -> location.location.containedby -> United States, with Territories -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nOhio -> location.location.containedby -> Midwestern United States -> location.location.contains -> Avenue of the Saints\n# Answer:\nMidwestern United States", "# Reasoning Path:\nOhio -> location.location.containedby -> United States, with Territories -> base.aareas.schema.earth.sovereign_domain.sovereign_state -> United States of America\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.legislative_sessions -> 111th United States Congress\n# Answer:\n111th United States Congress", "# Reasoning Path:\nOhio -> location.location.containedby -> Midwestern United States -> location.location.contains -> Council Grove\n# Answer:\nMidwestern United States", "# Reasoning Path:\nOhio -> location.location.containedby -> Contiguous United States -> location.location.containedby -> United States of America\n# Answer:\nContiguous United States", "# Reasoning Path:\nOhio -> location.location.containedby -> United States, with Territories -> location.country.first_level_divisions -> United States of America\n# Answer:\nUnited States, with Territories"], "ground_truth": ["Midwestern United States", "East North Central States"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1263", "prediction": ["# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzrlj -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010p4jhc -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.match -> 2014 Real Madrid CF vs. CA Osasuna football match\n# Answer:\n2014 Real Madrid CF vs. CA Osasuna football match", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w9gdcb -> soccer.football_player_stats.team -> Sporting-soc Desportiva De\n# Answer:\nSporting-soc Desportiva De", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzrlj -> soccer.football_goal.match -> 2014 Real Madrid CF vs. CA Osasuna football match\n# Answer:\n2014 Real Madrid CF vs. CA Osasuna football match", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010p4jhc -> soccer.football_goal.match -> 2014 UEFA Champions League Final\n# Answer:\n2014 UEFA Champions League Final", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w8w78v -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nCristiano Ronaldo -> award.award_winner.awards_won -> m.010nx4bl -> award.award_honor.award -> UEFA Champions League Top Goalscorer\n# Answer:\nUEFA Champions League Top Goalscorer", "# Reasoning Path:\nCristiano Ronaldo -> award.award_winner.awards_won -> m.010rnpty -> award.award_honor.award -> Pichichi Trophy\n# Answer:\nPichichi Trophy"], "ground_truth": ["Real Madrid C.F.", "Portugal national football team"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.2, "path_precision": 0.3, "path_recall": 0.15, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-1264", "prediction": ["# Reasoning Path:\nMichigan -> location.location.time_zones -> Eastern Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Eastern Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Eastern Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> UTC\u221205:00 -> common.topic.notable_types -> Time Zone\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nMichigan -> government.governmental_jurisdiction.governing_officials -> m.010f1h5h -> government.government_position_held.basic_title -> Attorney general\n# Answer:\nAttorney general", "# Reasoning Path:\nMichigan -> government.governmental_jurisdiction.governing_officials -> m.010f1h5h -> government.government_position_held.office_position_or_title -> Michigan Attorney General\n# Answer:\nMichigan Attorney General"], "ground_truth": ["Central Time Zone", "Eastern Time Zone", "UTC\u221205:00"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1265", "prediction": ["# Reasoning Path:\nJohn Tyler -> people.person.profession -> Politician -> common.topic.subjects -> Nathalie Kosciusko-Morizet\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Tyler -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Representative\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Tyler -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Senator\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Tyler -> people.person.religion -> Episcopal Church -> religion.religious_organization.leaders -> m.0w714g8\n# Answer:\nEpiscopal Church", "# Reasoning Path:\nJohn Tyler -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> Ambassador\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Tyler -> people.person.profession -> Politician -> common.topic.subjects -> Akinwunmi Ambode\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Tyler -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdn_\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Tyler -> people.person.profession -> Lawyer -> common.topic.subject_of -> Reyes Brown Reilley\n# Answer:\nLawyer", "# Reasoning Path:\nJohn Tyler -> people.person.religion -> Episcopal Church -> religion.religious_organization.leaders -> m.011m0m4g\n# Answer:\nEpiscopal Church", "# Reasoning Path:\nJohn Tyler -> people.person.religion -> Episcopal Church -> religion.religious_organization.member_organizations -> Episcopal Diocese of Olympia\n# Answer:\nEpiscopal Church"], "ground_truth": ["US President"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1266", "prediction": ["# Reasoning Path:\nSergio Romo -> people.person.profession -> Baseball player -> common.topic.notable_types -> Profession\n# Answer:\nBaseball player", "# Reasoning Path:\nSergio Romo -> sports.pro_athlete.teams -> m.04xhkd1 -> sports.sports_team_roster.position -> Pitcher\n# Answer:\nPitcher", "# Reasoning Path:\nSergio Romo -> people.person.profession -> Baseball player -> base.ontologies.ontology_instance.equivalent_instances -> m.0gy7hfc\n# Answer:\nBaseball player", "# Reasoning Path:\nSergio Romo -> people.person.profession -> Baseball player -> book.book_subject.works -> Highpockets\n# Answer:\nBaseball player", "# Reasoning Path:\nSergio Romo -> people.person.profession -> Baseball player -> base.ontologies.ontology_instance.equivalent_instances -> m.0gy7jbg\n# Answer:\nBaseball player", "# Reasoning Path:\nSergio Romo -> sports.pro_athlete.teams -> m.04xhkd1 -> sports.sports_team_roster.team -> San Francisco Giants\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nSergio Romo -> people.person.profession -> Baseball player -> book.book_subject.works -> The Glory of Their Times: The Story of the Early Days of Baseball Told by the Men Who Played It\n# Answer:\nBaseball player", "# Reasoning Path:\nSergio Romo -> people.person.profession -> Baseball player -> book.book_subject.works -> We Would Have Played for Nothing: Baseball Stars of the 1950s and 1960s Talk About the Game They Loved\n# Answer:\nBaseball player", "# Reasoning Path:\nSergio Romo -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nSergio Romo -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Alabama\n# Answer:\nUnited States of America"], "ground_truth": ["Baseball player"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1267", "prediction": ["# Reasoning Path:\nJamaica -> location.country.currency_used -> Jamaican dollar -> common.topic.notable_types -> Currency\n# Answer:\nJamaican dollar", "# Reasoning Path:\nJamaica -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f6_m\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nJamaica -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f6_n\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Keith -> meteorology.tropical_cyclone.affected_areas -> Yucat\u00e1n Peninsula\n# Answer:\nTropical Storm Keith", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Keith -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\nTropical Storm Keith", "# Reasoning Path:\nJamaica -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6fszp\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Keith -> meteorology.tropical_cyclone.tropical_cyclone_season -> 1988 Atlantic hurricane season\n# Answer:\nTropical Storm Keith", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Keith -> meteorology.tropical_cyclone.affected_areas -> Belize\n# Answer:\nTropical Storm Keith", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\n1812 Louisiana hurricane"], "ground_truth": ["Jamaican dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1268", "prediction": ["# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.location.containedby -> Eurasia\n# Answer:\nMaldives", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.location.containedby -> Asia\n# Answer:\nMaldives", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.statistical_region.agriculture_as_percent_of_gdp -> g.12tb6flp5\n# Answer:\nMaldives", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_burial -> Pleasure Point, Santa Cruz, California -> common.topic.image -> Rancho2\n# Answer:\nPleasure Point, Santa Cruz, California", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.location.containedby -> Indian subcontinent\n# Answer:\nMaldives", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.statistical_region.gni_in_ppp_dollars -> g.11b60vv5wn\n# Answer:\nMaldives", "# Reasoning Path:\nJay Moriarty -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.statistical_region.agriculture_as_percent_of_gdp -> g.1hhc37phc\n# Answer:\nMaldives", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.statistical_region.agriculture_as_percent_of_gdp -> g.1hhc3__tq\n# Answer:\nMaldives", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.statistical_region.gni_in_ppp_dollars -> g.1245__b0q\n# Answer:\nMaldives"], "ground_truth": ["Maldives"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1269", "prediction": ["# Reasoning Path:\nBoston Tea Party -> time.event.locations -> Boston Harbor -> location.location.containedby -> Massachusetts\n# Answer:\nBoston Harbor", "# Reasoning Path:\nBoston Tea Party -> common.topic.notable_for -> g.12574v___\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nBoston Tea Party -> time.event.locations -> Boston Harbor -> location.location.events -> Capture of USS Chesapeake\n# Answer:\nBoston Harbor", "# Reasoning Path:\nBoston Tea Party -> time.event.locations -> Boston Harbor -> geography.body_of_water.islands -> Bumpkin Island\n# Answer:\nBoston Harbor", "# Reasoning Path:\nBoston Tea Party -> time.event.locations -> Boston Harbor -> geography.body_of_water.islands -> Button Island\n# Answer:\nBoston Harbor", "# Reasoning Path:\nBoston Tea Party -> time.event.locations -> Boston Harbor -> geography.body_of_water.islands -> Calf Island\n# Answer:\nBoston Harbor", "# Reasoning Path:\nBoston Tea Party -> common.topic.webpage -> m.09y2fn0 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nBoston Tea Party -> common.topic.webpage -> m.09y2fn0 -> common.webpage.resource -> What I learned about the '70s from 'Swingtown'\n# Answer:\nWhat I learned about the '70s from 'Swingtown'"], "ground_truth": ["Boston Harbor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-127", "prediction": ["# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.notable_people_with_this_condition -> Diem Brown\n# Answer:\nOvarian cancer", "# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.parent_disease -> Cancer\n# Answer:\nOvarian cancer", "# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.parent_disease -> Female urologic disease\n# Answer:\nOvarian cancer", "# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.survival_rates -> m.04nvdb_\n# Answer:\nOvarian cancer", "# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.parent_disease -> Endocrine disease\n# Answer:\nOvarian cancer", "# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.survival_rates -> m.04nvdc7\n# Answer:\nOvarian cancer", "# Reasoning Path:\nGilda Radner -> music.artist.album -> Live From New York -> common.topic.notable_for -> g.12571zhk0\n# Answer:\nLive From New York", "# Reasoning Path:\nGilda Radner -> film.actor.film -> m.02vbgr3 -> film.performance.film -> Gilda Live\n# Answer:\nGilda Live", "# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.survival_rates -> m.04nvdc_\n# Answer:\nOvarian cancer", "# Reasoning Path:\nGilda Radner -> music.artist.album -> Live From New York -> music.album.album_content_type -> Live Album\n# Answer:\nLive From New York"], "ground_truth": ["Ovarian cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1270", "prediction": ["# Reasoning Path:\nLindsay Lohan -> base.popstra.celebrity.substance_abuse -> m.064wlqm -> base.popstra.substance_abuse.substance -> Cocaine\n# Answer:\nCocaine", "# Reasoning Path:\nLindsay Lohan -> base.popstra.celebrity.substance_abuse -> m.063fx3q -> base.popstra.substance_abuse.substance -> Alcoholic beverage\n# Answer:\nAlcoholic beverage", "# Reasoning Path:\nLindsay Lohan -> celebrities.celebrity.substance_abuse_problems -> m.04hykz6 -> celebrities.substance_abuse_problem.substance -> Alcoholic beverage\n# Answer:\nAlcoholic beverage", "# Reasoning Path:\nLindsay Lohan -> base.popstra.celebrity.friendship -> m.063gnw4 -> base.popstra.friendship.participant -> Britney Spears\n# Answer:\nBritney Spears", "# Reasoning Path:\nLindsay Lohan -> base.popstra.celebrity.friendship -> m.063hns8 -> base.popstra.friendship.participant -> Ashlee Simpson\n# Answer:\nAshlee Simpson", "# Reasoning Path:\nLindsay Lohan -> base.popstra.celebrity.friendship -> m.064wk92 -> base.popstra.friendship.participant -> Jonathan Bennett\n# Answer:\nJonathan Bennett"], "ground_truth": ["Cocaine", "Alcoholic beverage"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6, "path_precision": 0.5, "path_recall": 0.75, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1271", "prediction": ["# Reasoning Path:\nCarolina Panthers -> sports.sports_team.championships -> 2004 NFC Championship Game -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n2004 NFC Championship Game", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.championships -> 2004 NFC Championship Game -> sports.sports_championship_event.championship -> The NFC Championship Game\n# Answer:\n2004 NFC Championship Game", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.away_games -> Carolina Panthers at Arizona Cardinals, 2009-11-01 -> american_football.football_game.home_team -> Arizona Cardinals\n# Answer:\nCarolina Panthers at Arizona Cardinals, 2009-11-01", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.away_games -> Carolina Panthers at Atlanta Falcons, 2009-09-20 -> american_football.football_game.season -> 2009 NFL season\n# Answer:\nCarolina Panthers at Atlanta Falcons, 2009-09-20", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.away_games -> Carolina Panthers at Atlanta Falcons, 2008-11-23 -> american_football.football_game.season -> 2008 NFL season\n# Answer:\nCarolina Panthers at Atlanta Falcons, 2008-11-23", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.season_record -> m.075fhm9 -> sports.sports_team_season_record.season -> 2008 NFL season\n# Answer:\n2008 NFL season", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.season_record -> m.075fhmk -> sports.sports_team_season_record.season -> 2007 NFL season\n# Answer:\n2007 NFL season", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.season_record -> m.075fhmt -> sports.sports_team_season_record.season -> 2006 NFL season\n# Answer:\n2006 NFL season", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.away_games -> Carolina Panthers at Arizona Cardinals, 2009-11-01 -> american_football.football_game.receiving -> m.07z2llv\n# Answer:\nCarolina Panthers at Arizona Cardinals, 2009-11-01", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.away_games -> Carolina Panthers at Atlanta Falcons, 2008-11-23 -> american_football.football_game.rushing -> m.07948zg\n# Answer:\nCarolina Panthers at Atlanta Falcons, 2008-11-23"], "ground_truth": ["Super Bowl XXXVIII"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1272", "prediction": ["# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Myanmar -> location.country.official_language -> Burmese Language\n# Answer:\nMyanmar", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Myanmar -> location.location.events -> 2009 Kokang incident\n# Answer:\nMyanmar", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Myanmar -> location.location.events -> Battle of Bilin River\n# Answer:\nMyanmar", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Ayeyarwady Region -> location.administrative_division.country -> Myanmar\n# Answer:\nAyeyarwady Region", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Myanmar -> location.location.events -> Battle of Meiktila and Mandalay\n# Answer:\nMyanmar", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Myanmar -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_mygc\n# Answer:\nMyanmar", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Bangladesh -> location.location.events -> 2005 Bangladesh bombings\n# Answer:\nBangladesh", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Ayeyarwady Region -> common.topic.notable_for -> g.1257cw94m\n# Answer:\nAyeyarwady Region", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.category -> Category 4 Severe Tropical Cyclone (BOM) -> meteorology.tropical_cyclone_category.Beaufort_scale -> Beaufort force 12\n# Answer:\nCategory 4 Severe Tropical Cyclone (BOM)", "# Reasoning Path:\nCyclone Nargis -> common.topic.article -> m.0479wmw\n# Answer:\ncommon.topic.article"], "ground_truth": ["2008-04-27"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1274", "prediction": ["# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0gy -> government.government_position_held.office_holder -> Charles Allen Culberson\n# Answer:\nCharles Allen Culberson", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0k1 -> government.government_position_held.office_holder -> Price Daniel\n# Answer:\nPrice Daniel", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0j9 -> government.government_position_held.office_holder -> Miriam A. Ferguson\n# Answer:\nMiriam A. Ferguson", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0gy -> government.government_position_held.office_position_or_title -> Governor of Texas\n# Answer:\nGovernor of Texas", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0k1 -> government.government_position_held.office_position_or_title -> Governor of Texas\n# Answer:\nGovernor of Texas", "# Reasoning Path:\nTexas -> government.political_district.representatives -> m.04j5vd_ -> government.government_position_held.office_holder -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0k1 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0j9 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nTexas -> government.political_district.representatives -> m.052p616 -> government.government_position_held.office_holder -> William Neff Patman\n# Answer:\nWilliam Neff Patman", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0j9 -> government.government_position_held.office_position_or_title -> Governor of Texas\n# Answer:\nGovernor of Texas"], "ground_truth": ["Pendleton Murrah", "Richard Coke", "Joseph D. Sayers", "Ann Richards", "Allan Shivers", "George Tyler Wood", "Sam Houston", "James W. Throckmorton", "John Ireland", "Oran Milo Roberts", "Fletcher Stockdale", "J. W. Henderson", "Lawrence Sullivan Ross", "James Allred", "Peter Hansborough Bell", "Miriam A. Ferguson", "Bill Clements", "James E. Ferguson", "Mark White", "Preston Smith", "Francis Lubbock", "Charles Allen Culberson", "Coke R. Stevenson", "Elisha M. Pease", "Dolph Briscoe", "Pat Morris Neff", "Edmund J. Davis", "James Pinckney Henderson", "John Connally", "Beauford H. Jester", "Price Daniel", "W. Lee O'Daniel", "George W. Bush", "Jim Hogg", "Edward Clark", "Rick Perry", "Andrew Jackson Hamilton", "William P. Hobby", "Thomas Mitchell Campbell", "Oscar Branch Colquitt", "S. W. T. Lanham", "Richard B. Hubbard", "Hardin Richard Runnels", "Ross S. Sterling", "Dan Moody"], "ans_acc": 0.06666666666666667, "ans_hit": 1, "ans_f1": 0.1090909090909091, "ans_precission": 0.3, "ans_recall": 0.06666666666666667, "path_f1": 0.08, "path_precision": 0.3, "path_recall": 0.046153846153846156, "path_ans_f1": 0.1090909090909091, "path_ans_precision": 0.3, "path_ans_recall": 0.06666666666666667}
{"id": "WebQTest-1275", "prediction": ["# Reasoning Path:\nHelen Flanagan -> people.person.profession -> Actor -> people.profession.specializations -> Voice Actor\n# Answer:\nActor", "# Reasoning Path:\nHelen Flanagan -> people.person.profession -> Actor -> type.property.expected_type -> TV Actor\n# Answer:\nActor", "# Reasoning Path:\nHelen Flanagan -> people.person.profession -> Actor -> people.profession.specializations -> Audio Dramatist\n# Answer:\nActor", "# Reasoning Path:\nHelen Flanagan -> people.person.profession -> Actor -> music.special_music_video_performance_type.special_music_video_performances -> m.010wvs8m\n# Answer:\nActor", "# Reasoning Path:\nHelen Flanagan -> people.person.profession -> Actor -> people.profession.specializations -> Camgirl\n# Answer:\nActor", "# Reasoning Path:\nHelen Flanagan -> people.person.profession -> Actor -> music.special_music_video_performance_type.special_music_video_performances -> m.011qs93_\n# Answer:\nActor", "# Reasoning Path:\nHelen Flanagan -> common.topic.notable_types -> TV Actor -> base.descriptive_names.names.descriptive_name -> m.010h537s\n# Answer:\nTV Actor", "# Reasoning Path:\nHelen Flanagan -> people.person.profession -> Actor -> music.special_music_video_performance_type.special_music_video_performances -> m.0gw6dxs\n# Answer:\nActor", "# Reasoning Path:\nHelen Flanagan -> common.topic.notable_types -> TV Actor -> freebase.type_profile.published -> Published\n# Answer:\nTV Actor", "# Reasoning Path:\nHelen Flanagan -> common.topic.notable_types -> TV Actor -> base.descriptive_names.names.descriptive_name -> m.010h538b\n# Answer:\nTV Actor"], "ground_truth": ["Actor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1276", "prediction": ["# Reasoning Path:\nOlympic Games -> common.topic.notable_types -> Website Category -> common.topic.article -> m.02ht13p\n# Answer:\nWebsite Category", "# Reasoning Path:\nOlympic Games -> common.topic.notable_types -> Website Category -> freebase.type_profile.kind -> Classification\n# Answer:\nWebsite Category", "# Reasoning Path:\nOlympic Games -> common.topic.notable_types -> Website Category -> type.type.properties -> Websites of this genre\n# Answer:\nWebsite Category", "# Reasoning Path:\nOlympic Games -> time.recurring_event.current_frequency -> Quadrennial -> common.topic.notable_types -> Unit of frequency\n# Answer:\nQuadrennial", "# Reasoning Path:\nOlympic Games -> time.recurring_event.current_frequency -> Quadrennial -> common.topic.notable_for -> g.125d3_wnj\n# Answer:\nQuadrennial", "# Reasoning Path:\nOlympic Games -> book.book_subject.works -> Berlin Games -> book.written_work.subjects -> World War II\n# Answer:\nBerlin Games", "# Reasoning Path:\nOlympic Games -> time.recurring_event.current_frequency -> Quadrennial -> freebase.unit_profile.dimension -> Time\n# Answer:\nQuadrennial", "# Reasoning Path:\nOlympic Games -> book.book_subject.works -> All That Glitters Is Not Gold: The Olympic Game -> book.written_work.author -> William O. Johnson Jr.\n# Answer:\nAll That Glitters Is Not Gold: The Olympic Game", "# Reasoning Path:\nOlympic Games -> book.book_subject.works -> Olympic dreams -> book.book.genre -> Children's literature\n# Answer:\nOlympic dreams", "# Reasoning Path:\nOlympic Games -> book.book_subject.works -> Berlin Games -> common.topic.notable_types -> Book\n# Answer:\nBerlin Games"], "ground_truth": ["Quadrennial"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1277", "prediction": ["# Reasoning Path:\nTennessee River -> location.location.containedby -> United States of America -> location.location.partially_containedby -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nTennessee River -> location.location.partially_containedby -> Tennessee -> location.location.partially_contains -> Appalachian Mountains\n# Answer:\nTennessee", "# Reasoning Path:\nTennessee River -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nTennessee River -> location.location.partially_containedby -> Tennessee -> base.aareas.schema.administrative_area.administrative_children -> Humphreys County\n# Answer:\nTennessee", "# Reasoning Path:\nTennessee River -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Tennessee\n# Answer:\nUnited States of America", "# Reasoning Path:\nTennessee River -> location.location.partially_containedby -> Tennessee -> location.location.partially_contains -> Big South Fork of the Cumberland River\n# Answer:\nTennessee", "# Reasoning Path:\nTennessee River -> location.location.partially_containedby -> Tennessee -> organization.organization_scope.organizations_with_this_scope -> US Notary Public Association\n# Answer:\nTennessee", "# Reasoning Path:\nTennessee River -> location.location.partially_containedby -> Tennessee -> location.location.partially_contains -> Cumberland River\n# Answer:\nTennessee", "# Reasoning Path:\nTennessee River -> location.location.partially_containedby -> Kentucky -> location.location.adjoin_s -> m.02thc30\n# Answer:\nKentucky", "# Reasoning Path:\nTennessee River -> location.location.partially_containedby -> Tennessee -> organization.organization_scope.organizations_with_this_scope -> B'nai B'rith District Grand Lodge 7\n# Answer:\nTennessee"], "ground_truth": ["Alabama", "Tennessee", "Mississippi", "Kentucky"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5833333333333334, "ans_precission": 0.7, "ans_recall": 0.5, "path_f1": 0.5833333333333334, "path_precision": 0.7, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-1278", "prediction": ["# Reasoning Path:\nVenezuela -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nVenezuela -> location.location.events -> 2013 Venezuela Transaereo 5074 Britten-Norman Islander crash -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n2013 Venezuela Transaereo 5074 Britten-Norman Islander crash", "# Reasoning Path:\nVenezuela -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71v1t_3\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nVenezuela -> location.location.events -> 2007 RCTV protests -> common.topic.notable_for -> g.125773yxl\n# Answer:\n2007 RCTV protests", "# Reasoning Path:\nVenezuela -> location.location.events -> Battle of La Victoria -> common.topic.notable_types -> Military Conflict\n# Answer:\nBattle of La Victoria", "# Reasoning Path:\nVenezuela -> location.location.events -> 2013 Venezuela Transaereo 5074 Britten-Norman Islander crash -> freebase.valuenotation.is_reviewed -> Start date\n# Answer:\n2013 Venezuela Transaereo 5074 Britten-Norman Islander crash", "# Reasoning Path:\nVenezuela -> location.location.events -> 2007 RCTV protests -> base.newsevents.news_reported_event.news_report_s -> m.07wx5cm\n# Answer:\n2007 RCTV protests", "# Reasoning Path:\nVenezuela -> location.location.events -> Battle of La Victoria -> time.event.included_in_event -> Venezuelan War of Independence\n# Answer:\nBattle of La Victoria", "# Reasoning Path:\nVenezuela -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fdz2\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nVenezuela -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Argentina\n# Answer:\nSpanish Language"], "ground_truth": ["Los Maniceros massacre", "2013 Venezuela Transaereo 5074 Britten-Norman Islander crash", "2002 Venezuelan coup d'\u00e9tat attempt", "Dutch\u2013Venezuelan crisis of 1908", "g.122dshgz", "Venezuelan War of Independence", "2014 Caribbean Earthquake", "12th G-15 summit", "Machurucuto Incident", "Battle of Santa In\u00e9s", "Dutch\u2013Venezuela War", "2014\u201315 Venezuelan protests", "Battle of La Victoria", "Venezuelan crisis of 1902\u201303", "Bombardment of Fort San Carlos", "2014 Festival Internacional de Cine Infantil y Juvenil", "2007 RCTV protests", "2014 NACAM Rally Championship", "2011 ALBA Games"], "ans_acc": 0.21052631578947367, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.6, "ans_recall": 0.15789473684210525, "path_f1": 0.25, "path_precision": 0.6, "path_recall": 0.15789473684210525, "path_ans_f1": 0.3116883116883116, "path_ans_precision": 0.6, "path_ans_recall": 0.21052631578947367}
{"id": "WebQTest-1279", "prediction": ["# Reasoning Path:\nCorey Taylor -> music.group_member.instruments_played -> Bass guitar -> music.instrument.variation -> Kalindula\n# Answer:\nBass guitar", "# Reasoning Path:\nCorey Taylor -> music.group_member.instruments_played -> Bass guitar -> common.topic.webpage -> m.09wnh17\n# Answer:\nBass guitar", "# Reasoning Path:\nCorey Taylor -> music.group_member.instruments_played -> Bass guitar -> common.topic.subject_of -> Reverb\n# Answer:\nBass guitar", "# Reasoning Path:\nCorey Taylor -> music.group_member.instruments_played -> Drums -> common.topic.notable_for -> g.125brs154\n# Answer:\nDrums", "# Reasoning Path:\nCorey Taylor -> music.artist.track -> X-M@S -> common.topic.notable_types -> Musical Recording\n# Answer:\nX-M@S", "# Reasoning Path:\nCorey Taylor -> music.group_member.instruments_played -> Guitar -> music.instrument.variation -> Bass guitar\n# Answer:\nGuitar", "# Reasoning Path:\nCorey Taylor -> music.group_member.instruments_played -> Drums -> music.instrument.variation -> Ashiko\n# Answer:\nDrums", "# Reasoning Path:\nCorey Taylor -> music.artist.track -> X-M@S -> common.topic.notable_for -> g.126sr927j\n# Answer:\nX-M@S", "# Reasoning Path:\nCorey Taylor -> music.artist.track -> Death to All But Metal (Radio Edit) -> music.recording.artist -> Steel Panther\n# Answer:\nDeath to All But Metal (Radio Edit)", "# Reasoning Path:\nCorey Taylor -> music.group_member.instruments_played -> Guitar -> music.instrument.variation -> Viola caipira\n# Answer:\nGuitar"], "ground_truth": ["Bass guitar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-128", "prediction": ["# Reasoning Path:\nJuni Cortez -> film.film_character.portrayed_in_films -> m.0g51m5_ -> film.performance.actor -> Daryl Sabara\n# Answer:\nDaryl Sabara", "# Reasoning Path:\nJuni Cortez -> film.film_character.portrayed_in_films -> m.02s9tvb -> film.performance.actor -> Daryl Sabara\n# Answer:\nDaryl Sabara", "# Reasoning Path:\nJuni Cortez -> film.film_character.portrayed_in_films -> m.02s9trw -> film.performance.actor -> Daryl Sabara\n# Answer:\nDaryl Sabara", "# Reasoning Path:\nJuni Cortez -> common.topic.notable_for -> g.1254x65b_\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nJuni Cortez -> film.film_character.portrayed_in_films -> m.0g51m5_ -> film.performance.film -> Spy Kids: All the Time in the World\n# Answer:\nSpy Kids: All the Time in the World", "# Reasoning Path:\nJuni Cortez -> common.topic.article -> m.02ql78\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nJuni Cortez -> film.film_character.portrayed_in_films -> m.02s9tvb -> film.performance.film -> Spy Kids 2: The Island of Lost Dreams\n# Answer:\nSpy Kids 2: The Island of Lost Dreams", "# Reasoning Path:\nJuni Cortez -> film.film_character.portrayed_in_films -> m.02s9trw -> film.performance.film -> Spy Kids\n# Answer:\nSpy Kids"], "ground_truth": ["Daryl Sabara"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.375, "path_recall": 0.75, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1280", "prediction": ["# Reasoning Path:\nFranklin D. Roosevelt -> government.us_president.vice_president -> John Nance Garner -> people.person.place_of_birth -> Detroit\n# Answer:\nJohn Nance Garner", "# Reasoning Path:\nFranklin D. Roosevelt -> government.us_president.vice_president -> John Nance Garner -> government.politician.party -> m.03gjgf8\n# Answer:\nJohn Nance Garner", "# Reasoning Path:\nFranklin D. Roosevelt -> government.us_president.vice_president -> John Nance Garner -> people.deceased_person.place_of_death -> Uvalde\n# Answer:\nJohn Nance Garner", "# Reasoning Path:\nFranklin D. Roosevelt -> people.person.children -> Franklin D. Roosevelt, Jr. -> people.person.parents -> Eleanor Roosevelt\n# Answer:\nFranklin D. Roosevelt, Jr.", "# Reasoning Path:\nFranklin D. Roosevelt -> people.person.children -> Franklin D. Roosevelt, Jr. -> freebase.valuenotation.has_no_value -> Children\n# Answer:\nFranklin D. Roosevelt, Jr.", "# Reasoning Path:\nFranklin D. Roosevelt -> people.person.children -> Franklin D. Roosevelt, Jr. -> common.topic.notable_types -> Deceased Person\n# Answer:\nFranklin D. Roosevelt, Jr.", "# Reasoning Path:\nFranklin D. Roosevelt -> people.person.children -> Anna Roosevelt Halsted -> people.person.parents -> Eleanor Roosevelt\n# Answer:\nAnna Roosevelt Halsted", "# Reasoning Path:\nFranklin D. Roosevelt -> government.us_president.vice_president -> Harry S. Truman -> organization.organization_founder.organizations_founded -> Central Intelligence Agency (CIA)\n# Answer:\nHarry S. Truman", "# Reasoning Path:\nFranklin D. Roosevelt -> government.us_president.vice_president -> Henry A. Wallace -> people.person.nationality -> United States of America\n# Answer:\nHenry A. Wallace", "# Reasoning Path:\nFranklin D. Roosevelt -> people.person.children -> John Aspinwall Roosevelt -> people.person.children -> Sara Delano Roosevelt\n# Answer:\nJohn Aspinwall Roosevelt"], "ground_truth": ["Harry S. Truman"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1281", "prediction": ["# Reasoning Path:\nKate Gosselin -> people.person.place_of_birth -> Philadelphia -> base.biblioness.bibs_location.state -> Pennsylvania\n# Answer:\nPhiladelphia", "# Reasoning Path:\nKate Gosselin -> people.person.place_of_birth -> Philadelphia -> location.location.containedby -> United States of America\n# Answer:\nPhiladelphia", "# Reasoning Path:\nKate Gosselin -> people.person.children -> Madelyn Kate Gosselin -> people.person.place_of_birth -> Penn State Milton S. Hershey Medical Center\n# Answer:\nMadelyn Kate Gosselin", "# Reasoning Path:\nKate Gosselin -> people.person.place_of_birth -> Philadelphia -> location.location.containedby -> Pennsylvania\n# Answer:\nPhiladelphia", "# Reasoning Path:\nKate Gosselin -> common.topic.webpage -> m.046s2cx -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nKate Gosselin -> people.person.children -> Madelyn Kate Gosselin -> medicine.notable_person_with_medical_condition.condition -> Preterm birth\n# Answer:\nMadelyn Kate Gosselin", "# Reasoning Path:\nKate Gosselin -> people.person.children -> Aaden Jonathan Gosselin -> people.person.place_of_birth -> Penn State Milton S. Hershey Medical Center\n# Answer:\nAaden Jonathan Gosselin", "# Reasoning Path:\nKate Gosselin -> people.person.children -> Alexis Faith Gosselin -> people.person.place_of_birth -> Penn State Milton S. Hershey Medical Center\n# Answer:\nAlexis Faith Gosselin", "# Reasoning Path:\nKate Gosselin -> common.topic.webpage -> m.046s2cx -> common.webpage.resource -> The Gosselin 10\n# Answer:\nThe Gosselin 10", "# Reasoning Path:\nKate Gosselin -> common.topic.webpage -> m.09wcrf4 -> common.webpage.resource -> 'Jon & Kate Plus 8' star attracts hundreds of fans to California book signing\n# Answer:\n'Jon & Kate Plus 8' star attracts hundreds of fans to California book signing"], "ground_truth": ["Philadelphia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1282", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> South Korea\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> American Samoa\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Northern Mariana Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guam\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Mexico\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Marshall Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Germany\n# Answer:\nFederal republic"], "ground_truth": ["Federal republic", "Presidential system", "Constitutional republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1284", "prediction": ["# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> travel.tourist_attraction.near_travel_destination -> Blois\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> common.topic.notable_for -> g.1256sch6w\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Mus\u00e9e d'Orsay -> base.schemastaging.organization_extra.contact_webpages -> m.010ggx62\n# Answer:\nMus\u00e9e d'Orsay", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> architecture.structure.architect -> Domenico da Cortona\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Mus\u00e9e d'Orsay -> common.topic.image -> Mus\u00c3\u00a9e d'Orsay Clock\n# Answer:\nMus\u00e9e d'Orsay", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Disneyland Park -> location.location.containedby -> France\n# Answer:\nDisneyland Park", "# Reasoning Path:\nParis -> common.topic.webpage -> m.02k8dbw -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Disneyland Park -> common.topic.notable_types -> Amusement Park\n# Answer:\nDisneyland Park", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> architecture.structure.architect -> Pierre Nepveu\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> common.topic.webpage -> m.02k8dbw -> common.webpage.resource -> m.0bkwkw7\n# Answer:\ncommon.webpage.resource"], "ground_truth": ["Mus\u00e9e Maillol", "Centre Georges Pompidou", "Jardin du Luxembourg", "Notre Dame de Paris", "Grande Arche", "Mus\u00e9e d'Orsay", "Bois de Boulogne", "The Louvre", "Les Invalides", "Folies Berg\u00e8re", "Arc de Triomphe", "Parc Ast\u00e9rix", "La Maison Rouge", "Champs-\u00c9lys\u00e9es", "Galerie Claude Bernard", "Ch\u00e2teau de Chambord", "Disneyland Paris", "Mus\u00e9e de l'Orangerie", "Petit Palais", "Place de la Concorde", "Gare d'Orsay", "\u00cele de la Cit\u00e9", "Mus\u00e9e des Arts et M\u00e9tiers", "Eiffel Tower", "Mus\u00e9e du quai Branly", "Panth\u00e9on", "Sacr\u00e9-C\u0153ur, Paris", "Verdon Gorge", "Galerie nationale du Jeu de Paume", "Caf\u00e9 Volpini", "Disneyland Park"], "ans_acc": 0.0967741935483871, "ans_hit": 1, "ans_f1": 0.1726618705035971, "ans_precission": 0.8, "ans_recall": 0.0967741935483871, "path_f1": 0.1726618705035971, "path_precision": 0.8, "path_recall": 0.0967741935483871, "path_ans_f1": 0.1726618705035971, "path_ans_precision": 0.8, "path_ans_recall": 0.0967741935483871}
{"id": "WebQTest-1285", "prediction": ["# Reasoning Path:\nLassen Peak -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nLassen Peak -> location.location.containedby -> Shasta County -> location.location.containedby -> California\n# Answer:\nShasta County", "# Reasoning Path:\nLassen Peak -> common.topic.notable_for -> g.125cb11p0\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nLassen Peak -> location.location.containedby -> United States of America -> location.location.containedby -> Americas\n# Answer:\nUnited States of America", "# Reasoning Path:\nLassen Peak -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> California\n# Answer:\nUnited States of America", "# Reasoning Path:\nLassen Peak -> common.topic.image -> Lassen-Peak-Large -> common.image.appears_in_topic_gallery -> Upstate California\n# Answer:\nLassen-Peak-Large", "# Reasoning Path:\nLassen Peak -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nLassen Peak -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> California\n# Answer:\nUnited States of America", "# Reasoning Path:\nLassen Peak -> location.location.containedby -> Shasta County -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nShasta County", "# Reasoning Path:\nLassen Peak -> location.location.containedby -> California -> location.location.containedby -> West Coast of the United States\n# Answer:\nCalifornia"], "ground_truth": ["United States of America", "California", "Cascade Range", "Shasta County", "North America", "Lassen Volcanic National Park"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6153846153846154, "ans_precission": 0.8, "ans_recall": 0.5, "path_f1": 0.6153846153846154, "path_precision": 0.8, "path_recall": 0.5, "path_ans_f1": 0.6428571428571429, "path_ans_precision": 0.9, "path_ans_recall": 0.5}
{"id": "WebQTest-1287", "prediction": ["# Reasoning Path:\nOhio -> government.governmental_jurisdiction.official_symbols -> m.04st75g -> location.location_symbol_relationship.symbol -> Tomato juice\n# Answer:\nTomato juice", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.official_symbols -> m.04stkcq -> location.location_symbol_relationship.symbol -> Walleye\n# Answer:\nWalleye", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.official_symbols -> m.04st75g -> location.location_symbol_relationship.Kind_of_symbol -> State beverage\n# Answer:\nState beverage", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.official_symbols -> m.04st87j -> location.location_symbol_relationship.symbol -> Cardinal\n# Answer:\nCardinal", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.official_symbols -> m.04st87j -> location.location_symbol_relationship.Kind_of_symbol -> State bird\n# Answer:\nState bird", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> Tennessee\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.tropical_cyclone_season -> 1982 Atlantic hurricane season\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.legislative_sessions -> 111th United States Congress\n# Answer:\n111th United States Congress", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.office_holder -> George Voinovich\n# Answer:\nGeorge Voinovich", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator"], "ground_truth": ["With God, all things are possible"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1289", "prediction": ["# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Classical music -> music.genre.subgenre -> Baroque music\n# Answer:\nClassical music", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Opera -> common.topic.notable_types -> Composition type\n# Answer:\nOpera", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Classical music -> book.book_subject.works -> W.A. Mozart\n# Answer:\nClassical music", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Classical music -> music.genre.subgenre -> Chamber music\n# Answer:\nClassical music", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Classical music -> broadcast.genre.content -> 1.FM Otto's  Baroque music\n# Answer:\nClassical music", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Classical music -> music.genre.subgenre -> Symphony\n# Answer:\nClassical music", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Art song -> music.compositional_form.superforms -> Song\n# Answer:\nArt song", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Classical music -> broadcast.genre.content -> 1.FM Otto's Classical channel\n# Answer:\nClassical music", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Opera -> broadcast.genre.content -> 1.FM Otto's Opera House\n# Answer:\nOpera", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Art song -> music.compositional_form.subforms -> Lied\n# Answer:\nArt song"], "ground_truth": ["Opera", "Classical music", "Art song", "Chamber music", "Ballet"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 1.0, "ans_recall": 0.6, "path_f1": 0.7499999999999999, "path_precision": 1.0, "path_recall": 0.6, "path_ans_f1": 0.888888888888889, "path_ans_precision": 1.0, "path_ans_recall": 0.8}
{"id": "WebQTest-129", "prediction": ["# Reasoning Path:\nLucille Ball -> people.person.profession -> Television producer -> common.topic.subjects -> Michael Palance\n# Answer:\nTelevision producer", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Comedian -> people.profession.specializations -> Stand-up comedian\n# Answer:\nComedian", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Television producer -> common.topic.notable_types -> Profession\n# Answer:\nTelevision producer", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Television producer -> common.topic.subjects -> Billy Sorrentino\n# Answer:\nTelevision producer", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Television producer -> common.topic.subject_of -> Michael Palance\n# Answer:\nTelevision producer", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Comedian -> common.topic.notable_types -> Profession\n# Answer:\nComedian", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Actor -> owl#inverseOf -> Film performances\n# Answer:\nActor", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Television producer -> common.topic.subject_of -> Billy Sorrentino\n# Answer:\nTelevision producer", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Actor -> freebase.equivalent_topic.equivalent_type -> Film actor\n# Answer:\nActor", "# Reasoning Path:\nLucille Ball -> film.actor.film -> m.01345zx6 -> film.performance.film -> The Magic Carpet\n# Answer:\nThe Magic Carpet"], "ground_truth": ["Model", "Comedian", "Singer", "Actor", "Television producer"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7200000000000001, "ans_precission": 0.9, "ans_recall": 0.6, "path_f1": 0.7200000000000001, "path_precision": 0.9, "path_recall": 0.6, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 1.0, "path_ans_recall": 0.6}
{"id": "WebQTest-1291", "prediction": ["# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Rosemary Kennedy -> common.topic.notable_types -> Family member\n# Answer:\nRosemary Kennedy", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Ted Kennedy -> people.person.children -> Patrick J. Kennedy\n# Answer:\nTed Kennedy", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Rosemary Kennedy -> common.topic.article -> m.01kyp0\n# Answer:\nRosemary Kennedy", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Rosemary Kennedy -> people.person.sibling_s -> m.03lfqsd\n# Answer:\nRosemary Kennedy", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Ted Kennedy -> people.person.children -> Edward M. Kennedy, Jr.\n# Answer:\nTed Kennedy", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Eunice Kennedy Shriver -> people.person.children -> Timothy Shriver\n# Answer:\nEunice Kennedy Shriver", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Rosemary Kennedy -> people.person.sibling_s -> m.0703_fl\n# Answer:\nRosemary Kennedy", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> book.author.works_written -> I'm for Roosevelt -> common.topic.notable_types -> Book\n# Answer:\nI'm for Roosevelt", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Ted Kennedy -> people.person.children -> Kara Kennedy\n# Answer:\nTed Kennedy", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Rosemary Kennedy -> people.person.sibling_s -> m.075qm56\n# Answer:\nRosemary Kennedy"], "ground_truth": ["Kathleen Cavendish", "Ted Kennedy", "Robert F. Kennedy", "John F. Kennedy", "Rosemary Kennedy", "Patricia Kennedy Lawford", "Eunice Kennedy Shriver", "Jean Kennedy Smith", "Joseph P. Kennedy, Jr."], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.48648648648648646, "ans_precission": 0.9, "ans_recall": 0.3333333333333333, "path_f1": 0.48648648648648646, "path_precision": 0.9, "path_recall": 0.3333333333333333, "path_ans_f1": 0.48648648648648646, "path_ans_precision": 0.9, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1292", "prediction": ["# Reasoning Path:\nBarcelona -> location.location.nearby_airports -> Barcelona\u2013El Prat Airport -> aviation.airport.serves -> El Prat de Llobregat\n# Answer:\nBarcelona\u2013El Prat Airport", "# Reasoning Path:\nBarcelona -> location.location.nearby_airports -> Barcelona\u2013El Prat Airport -> aviation.airport.hub_for -> Air Nostrum\n# Answer:\nBarcelona\u2013El Prat Airport", "# Reasoning Path:\nBarcelona -> location.location.nearby_airports -> Barcelona\u2013El Prat Airport -> aviation.airport.hub_for -> Vueling\n# Answer:\nBarcelona\u2013El Prat Airport", "# Reasoning Path:\nBarcelona -> location.location.nearby_airports -> Barcelona\u2013El Prat Airport -> location.location.geolocation -> m.02_h61n\n# Answer:\nBarcelona\u2013El Prat Airport", "# Reasoning Path:\nBarcelona -> location.statistical_region.population -> g.11b7w16s2h\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nBarcelona -> location.location.nearby_airports -> Barcelona\u2013El Prat Airport -> aviation.airport.hub_for -> Clickair\n# Answer:\nBarcelona\u2013El Prat Airport", "# Reasoning Path:\nBarcelona -> location.location.nearby_airports -> Barcelona\u2013El Prat Airport -> aviation.airport.serves -> Adventures by Disney - Italy: Tuscany\n# Answer:\nBarcelona\u2013El Prat Airport", "# Reasoning Path:\nBarcelona -> location.statistical_region.population -> g.11bc860_dg\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nBarcelona -> common.topic.webpage -> m.051szjw -> common.webpage.resource -> Visitor Information\n# Answer:\nVisitor Information", "# Reasoning Path:\nBarcelona -> location.statistical_region.population -> g.11bcdlbdfj\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["Barcelona\u2013El Prat Airport"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1293", "prediction": ["# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> Indo-European languages -> language.language_family.sub_families -> Germanic languages\n# Answer:\nIndo-European languages", "# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> Indo-European languages -> common.topic.notable_types -> Language Family\n# Answer:\nIndo-European languages", "# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> Anglo-Frisian languages -> language.language_family.languages -> Frisian languages\n# Answer:\nAnglo-Frisian languages", "# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> Anglo-Frisian languages -> common.topic.article -> m.082j1_\n# Answer:\nAnglo-Frisian languages", "# Reasoning Path:\nEnglish Language -> education.school_category.schools_of_this_kind -> Piers Midwinter -> people.person.quotations -> Learn with a NEST. Please be my GUEST. I will give you my BEST,\n# Answer:\nPiers Midwinter", "# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> Anglo-Frisian languages -> common.topic.notable_types -> Language Family\n# Answer:\nAnglo-Frisian languages", "# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> West Germanic languages -> common.topic.image -> Europe germanic languages\n# Answer:\nWest Germanic languages", "# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> West Germanic languages -> common.topic.notable_types -> Language Family\n# Answer:\nWest Germanic languages", "# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> Anglo-Frisian languages -> language.language_family.languages -> Forth and Bargy dialect\n# Answer:\nAnglo-Frisian languages", "# Reasoning Path:\nEnglish Language -> education.school_category.schools_of_this_kind -> Piers Midwinter -> common.topic.notable_types -> Organization founder\n# Answer:\nPiers Midwinter"], "ground_truth": ["Indo-European languages", "West Germanic languages", "Anglo-Frisian languages"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1294", "prediction": ["# Reasoning Path:\nWorld War II -> time.event.locations -> Europe -> base.locations.continents.countries_within -> Poland\n# Answer:\nEurope", "# Reasoning Path:\nWorld War II -> time.event.locations -> Europe -> location.location.time_zones -> Eastern European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nWorld War II -> time.event.locations -> Europe -> base.locations.continents.countries_within -> United Kingdom\n# Answer:\nEurope", "# Reasoning Path:\nWorld War II -> time.event.locations -> Europe -> location.location.partially_contains -> Soviet Union\n# Answer:\nEurope", "# Reasoning Path:\nWorld War II -> time.event.locations -> Southeast Asia -> location.location.contains -> Philippines\n# Answer:\nSoutheast Asia", "# Reasoning Path:\nWorld War II -> base.skosbase.vocabulary_equivalent_topic.narrower_concept -> World War, 1939-1945--Hostages -> base.skosbase.skos_concept.broader_topic -> Hostage\n# Answer:\nWorld War, 1939-1945--Hostages", "# Reasoning Path:\nWorld War II -> time.event.locations -> Europe -> base.locations.continents.countries_within -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nWorld War II -> time.event.locations -> Southeast Asia -> location.location.contains -> Thailand\n# Answer:\nSoutheast Asia", "# Reasoning Path:\nWorld War II -> time.event.locations -> Southeast Asia -> location.location.geolocation -> m.0bg8gdv\n# Answer:\nSoutheast Asia", "# Reasoning Path:\nWorld War II -> time.event.locations -> Africa -> location.location.events -> World War I\n# Answer:\nAfrica"], "ground_truth": ["Battle of Falmouth"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1295", "prediction": ["# Reasoning Path:\nMatt Damon -> film.actor.film -> g.11b6v52kzy\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nMatt Damon -> film.actor.film -> m.0114bqjl -> film.performance.film -> Judge Not: In Defense of Dogma\n# Answer:\nJudge Not: In Defense of Dogma", "# Reasoning Path:\nMatt Damon -> award.award_nominee.award_nominations -> m.0b3x6dp -> award.award_nomination.nominated_for -> The Talented Mr. Ripley\n# Answer:\nThe Talented Mr. Ripley", "# Reasoning Path:\nMatt Damon -> award.award_nominee.award_nominations -> m.04dn0w4 -> award.award_nomination.nominated_for -> Saving Private Ryan\n# Answer:\nSaving Private Ryan", "# Reasoning Path:\nMatt Damon -> film.actor.film -> m.011kxb16 -> film.performance.film -> Extreme Realities\n# Answer:\nExtreme Realities", "# Reasoning Path:\nMatt Damon -> award.award_nominee.award_nominations -> m.0b3x6dp -> award.award_nomination.award -> MTV Movie Award for Best Villain\n# Answer:\nMTV Movie Award for Best Villain", "# Reasoning Path:\nMatt Damon -> award.award_nominee.award_nominations -> m.021yf48 -> award.award_nomination.nominated_for -> Good Will Hunting\n# Answer:\nGood Will Hunting", "# Reasoning Path:\nMatt Damon -> film.actor.film -> m.0114bqjl -> film.performance.special_performance_type -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nMatt Damon -> award.award_nominee.award_nominations -> m.04dn0w4 -> award.award_nomination.nominated_for -> Good Will Hunting\n# Answer:\nGood Will Hunting", "# Reasoning Path:\nMatt Damon -> award.award_nominee.award_nominations -> m.04dn0w4 -> award.award_nomination.award -> London Film Critics' Circle Award for Actor of the Year\n# Answer:\nLondon Film Critics' Circle Award for Actor of the Year"], "ground_truth": ["The Good Old Boys", "Courage Under Fire", "Saving Private Ryan", "Field of Dreams", "Extreme Realities", "School Ties", "The Rainmaker", "Behind the Candelabra", "Margaret", "The Bourne Ultimatum", "The Majestic", "The Monuments Men", "Dogma", "Judge Not: In Defense of Dogma", "Invictus", "Unauthorized: The Harvey Weinstein Project", "Howard Zinn: You Can\u00b4t Be Neutral on a Moving Train", "Rising Son", "Glory Daze", "Finding Forrester", "The Bourne Identity", "Good Will Hunting", "Ocean's Twelve", "Mystic Pizza", "Ocean's Thirteen", "Happy Feet Two", "Green Zone", "We Bought a Zoo", "Hereafter", "The People Speak", "True Grit", "Rounders 2", "The Bourne Supremacy", "The Brothers Grimm", "Confessions of a Dangerous Mind", "Rounders", "EuroTrip", "Push, Nevada", "Behind the Screens", "Contagion", "Youth Without Youth", "Interstellar", "The Good Shepherd", "Chasing Amy", "Gerry", "The Great Wall", "Jay and Silent Bob Strike Back", "The Third Wheel", "Inside Job", "The Legend of Bagger Vance", "The Informant!", "The Departed", "Geronimo: An American Legend", "Oh, What a Lovely Tea Party", "Syriana", "Promised Land", "Che: Part Two", "Stuck on You", "Ocean's Eleven", "Elysium", "The Adjustment Bureau", "Magnificent Desolation: Walking On The Moon 3D", "The Talented Mr. Ripley", "The Good Mother", "Spirit: Stallion of the Cimarron", "The Martian", "Jersey Girl", "Titan A.E.", "The Zero Theorem", "All the Pretty Horses"], "ans_acc": 0.08571428571428572, "ans_hit": 1, "ans_f1": 0.15, "ans_precission": 0.6, "ans_recall": 0.08571428571428572, "path_f1": 0.058823529411764705, "path_precision": 0.4, "path_recall": 0.031746031746031744, "path_ans_f1": 0.15, "path_ans_precision": 0.6, "path_ans_recall": 0.08571428571428572}
{"id": "WebQTest-1296", "prediction": ["# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> base.descriptive_names.names.descriptive_name -> m.0105y97v\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Basketball players\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> base.descriptive_names.names.descriptive_name -> m.01069wj6\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> base.descriptive_names.names.descriptive_name -> m.01069wkk\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> fictional_universe.character_occupation.characters_with_this_occupation -> Derek Roman\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> fictional_universe.character_occupation.characters_with_this_occupation -> Cam Calloway\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Actor -> people.profession.specializations -> Voice Actor\n# Answer:\nActor", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> fictional_universe.character_occupation.characters_with_this_occupation -> Jimmy Brooks\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.player_statistics -> m.04q9rcg -> basketball.basketball_player_stats.season -> 2003\u201304 NBA season\n# Answer:\n2003\u201304 NBA season", "# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.player_statistics -> m.04qb_fd -> basketball.basketball_player_stats.team -> Chicago Bulls\n# Answer:\nChicago Bulls"], "ground_truth": ["Actor", "Basketball player"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1297", "prediction": ["# Reasoning Path:\nCanada -> location.location.containedby -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nCanada -> location.location.containedby -> North America -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nNorth America", "# Reasoning Path:\nCanada -> base.locations.countries.continent -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nCanada -> location.location.containedby -> North America -> location.location.contains_major_portion_of -> United States of America\n# Answer:\nNorth America", "# Reasoning Path:\nCanada -> location.location.containedby -> DVD Region 1 -> location.location.contains -> United States of America\n# Answer:\nDVD Region 1", "# Reasoning Path:\nCanada -> base.locations.countries.continent -> North America -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nNorth America", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.12cp_k6ss\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nCanada -> location.location.containedby -> Americas -> location.location.time_zones -> Atlantic Time Zone\n# Answer:\nAmericas", "# Reasoning Path:\nCanada -> location.location.containedby -> Americas -> location.location.contains -> United States of America\n# Answer:\nAmericas", "# Reasoning Path:\nCanada -> base.locations.countries.continent -> North America -> location.location.contains_major_portion_of -> United States of America\n# Answer:\nNorth America"], "ground_truth": ["Americas", "North America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1298", "prediction": ["# Reasoning Path:\nGeorge Wilson -> people.person.profession -> American football player -> common.topic.notable_types -> Profession\n# Answer:\nAmerican football player", "# Reasoning Path:\nGeorge Wilson -> common.topic.notable_types -> American football player -> common.topic.notable_types -> Profession\n# Answer:\nAmerican football player", "# Reasoning Path:\nGeorge Wilson -> people.person.profession -> American football player -> type.type.domain -> American football\n# Answer:\nAmerican football player", "# Reasoning Path:\nGeorge Wilson -> people.person.profession -> American football player -> people.profession.specialization_of -> Football player\n# Answer:\nAmerican football player", "# Reasoning Path:\nGeorge Wilson -> common.topic.notable_types -> American football player -> type.type.domain -> American football\n# Answer:\nAmerican football player", "# Reasoning Path:\nGeorge Wilson -> american_football.football_player.position_s -> Safety -> common.topic.notable_types -> Sports position\n# Answer:\nSafety", "# Reasoning Path:\nGeorge Wilson -> common.topic.notable_types -> American football player -> people.profession.specialization_of -> Football player\n# Answer:\nAmerican football player", "# Reasoning Path:\nGeorge Wilson -> american_football.football_player.position_s -> Safety -> common.topic.article -> m.05b3tz\n# Answer:\nSafety", "# Reasoning Path:\nGeorge Wilson -> american_football.football_player.position_s -> Strong safety -> sports.sports_position.sport -> American football\n# Answer:\nStrong safety", "# Reasoning Path:\nGeorge Wilson -> american_football.football_player.position_s -> Safety -> organization.role.governors -> m.0_gm5jn\n# Answer:\nSafety"], "ground_truth": ["American football player"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Patrick Bouvier Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nPatrick Bouvier Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of death\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Patrick Bouvier Kennedy -> people.deceased_person.place_of_death -> Boston Children's Hospital\n# Answer:\nPatrick Bouvier Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Patrick Bouvier Kennedy -> freebase.valuenotation.has_no_value -> Spouse (or domestic partner)\n# Answer:\nPatrick Bouvier Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Yad Kennedy -> common.topic.notable_types -> Structure\n# Answer:\nYad Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.nationality -> United States of America\n# Answer:\nArabella Kennedy"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1300", "prediction": ["# Reasoning Path:\nFulham F.C. -> sports.sports_team.location -> Fulham -> location.location.containedby -> London\n# Answer:\nFulham", "# Reasoning Path:\nFulham F.C. -> sports.sports_team.location -> Fulham -> location.location.containedby -> England\n# Answer:\nFulham", "# Reasoning Path:\nFulham F.C. -> sports.sports_team.location -> Fulham -> location.location.contains -> Aragon House\n# Answer:\nFulham", "# Reasoning Path:\nFulham F.C. -> sports.sports_team.location -> Fulham -> common.topic.image -> Putney Bridge\n# Answer:\nFulham", "# Reasoning Path:\nFulham F.C. -> sports.sports_team.location -> Fulham -> location.location.contains -> Duke of Cumberland, Fulham\n# Answer:\nFulham", "# Reasoning Path:\nFulham F.C. -> sports.sports_team.location -> Fulham -> location.location.contains -> Stamford Bridge\n# Answer:\nFulham", "# Reasoning Path:\nFulham F.C. -> sports.sports_team.location -> Fulham -> common.topic.image -> Sheraton Skyline Hotel at London Heathrow is located in Greater London\n# Answer:\nFulham", "# Reasoning Path:\nFulham F.C. -> sports.professional_sports_team.owner_s -> Shahid Khan -> people.person.gender -> Male\n# Answer:\nShahid Khan", "# Reasoning Path:\nFulham F.C. -> soccer.football_team.player_statistics -> m.0w8_1dt -> soccer.football_player_stats.player -> Adam Watts\n# Answer:\nAdam Watts", "# Reasoning Path:\nFulham F.C. -> soccer.football_team.player_statistics -> m.0w8_sxc -> soccer.football_player_stats.player -> Sean Davis\n# Answer:\nSean Davis"], "ground_truth": ["Fulham"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1301", "prediction": ["# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Sutter's Fort -> symbols.namesake.named_after -> John Sutter\n# Answer:\nSutter's Fort", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> B Street Theatre -> architecture.building.building_function -> Theatre\n# Answer:\nB Street Theatre", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Sutter's Fort -> common.topic.article -> m.0962k\n# Answer:\nSutter's Fort", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> B Street Theatre -> location.location.geolocation -> m.0131pvp5\n# Answer:\nB Street Theatre", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> California State Capitol Museum -> common.topic.notable_types -> Museum\n# Answer:\nCalifornia State Capitol Museum", "# Reasoning Path:\nSacramento -> sports.sports_team_location.teams -> Sacramento State Hornets men's basketball -> sports.sports_team.sport -> Basketball\n# Answer:\nSacramento State Hornets men's basketball", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> B Street Theatre -> freebase.valuenotation.has_value -> Architect\n# Answer:\nB Street Theatre", "# Reasoning Path:\nSacramento -> location.location.events -> 2010 Tour of California -> time.event.locations -> Agoura Hills\n# Answer:\n2010 Tour of California", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> California State Capitol Museum -> common.topic.notable_for -> g.1255tttbm\n# Answer:\nCalifornia State Capitol Museum", "# Reasoning Path:\nSacramento -> sports.sports_team_location.teams -> Sacramento State Hornets men's basketball -> sports.sports_team.arena_stadium -> Colberg Court\n# Answer:\nSacramento State Hornets men's basketball"], "ground_truth": ["California Automobile Museum", "Sacramento Zoo", "Crocker Art Museum", "Raging Waters Sacramento", "Folsom Lake", "California State Railroad Museum", "B Street Theatre", "Sacramento History Museum", "California State Indian Museum", "California State Capitol Museum", "Sutter's Fort"], "ans_acc": 0.2727272727272727, "ans_hit": 1, "ans_f1": 0.3925233644859813, "ans_precission": 0.7, "ans_recall": 0.2727272727272727, "path_f1": 0.3925233644859813, "path_precision": 0.7, "path_recall": 0.2727272727272727, "path_ans_f1": 0.3925233644859813, "path_ans_precision": 0.7, "path_ans_recall": 0.2727272727272727}
{"id": "WebQTest-1303", "prediction": ["# Reasoning Path:\nJosh Hutchersonm -> award.award_nominee.award_nominations -> m.0_vw5gg -> award.award_nomination.nominated_for -> The Hunger Games: Catching Fire\n# Answer:\nThe Hunger Games: Catching Fire", "# Reasoning Path:\nJosh Hutchersonm -> award.award_nominee.award_nominations -> m.0z83x3f -> award.award_nomination.nominated_for -> The Hunger Games\n# Answer:\nThe Hunger Games", "# Reasoning Path:\nJosh Hutchersonm -> film.actor.film -> m.0gy8k_1 -> film.performance.film -> Party Wagon\n# Answer:\nParty Wagon", "# Reasoning Path:\nJosh Hutchersonm -> award.award_nominee.award_nominations -> m.0_vw5gg -> award.award_nomination.award_nominee -> Jennifer Lawrence\n# Answer:\nJennifer Lawrence", "# Reasoning Path:\nJosh Hutchersonm -> award.award_nominee.award_nominations -> m.0_vw5gg -> freebase.valuenotation.is_reviewed -> Ceremony\n# Answer:\nCeremony", "# Reasoning Path:\nJosh Hutchersonm -> award.award_nominee.award_nominations -> m.010wr2c8 -> award.award_nomination.nominated_for -> The Hunger Games: Catching Fire\n# Answer:\nThe Hunger Games: Catching Fire", "# Reasoning Path:\nJosh Hutchersonm -> film.actor.film -> g.11b6fyj7ls\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nJosh Hutchersonm -> award.award_nominee.award_nominations -> m.0z83x3f -> award.award_nomination.ceremony -> 2012 Teen Choice Awards\n# Answer:\n2012 Teen Choice Awards", "# Reasoning Path:\nJosh Hutchersonm -> film.actor.film -> g.11byn4zhzw\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nJosh Hutchersonm -> award.award_nominee.award_nominations -> m.0z83x3f -> award.award_nomination.award_nominee -> Jennifer Lawrence\n# Answer:\nJennifer Lawrence"], "ground_truth": ["Zathura", "Journey 2: The Mysterious Island", "Miracle Dogs", "Escobar: Paradise Lost", "Party Wagon", "Epic", "Winged Creatures", "The Hunger Games: Mockingjay, Part 2", "RV", "The Hunger Games", "The Long Home", "Journey to the Center of the Earth", "The Third Rule", "Red Dawn", "Kicking & Screaming", "Bridge to Terabithia", "House Blend", "7 Days in Havana", "Wilder Days", "The Hunger Games: Mockingjay, Part 1", "The Polar Express", "One Last Ride", "Little Manhattan", "The Forger", "Detention", "The Kids Are All Right", "Cirque du Freak: The Vampire's Assistant", "Motocross Kids", "American Splendor", "Firehouse Dog", "The Hunger Games: Catching Fire", "In Dubious Battle"], "ans_acc": 0.09375, "ans_hit": 1, "ans_f1": 0.1518987341772152, "ans_precission": 0.4, "ans_recall": 0.09375, "path_f1": 0.11940298507462686, "path_precision": 0.4, "path_recall": 0.07017543859649122, "path_ans_f1": 0.1518987341772152, "path_ans_precision": 0.4, "path_ans_recall": 0.09375}
{"id": "WebQTest-1304", "prediction": ["# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> organization.organization.founders -> Hermann G\u00f6ring\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> base.schemastaging.context_name.pronunciation -> g.125_pt37m\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.place_founded -> Nazi Germany\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.founders -> Heinrich Himmler\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> organization.organization.founders -> Anton Drexler\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> book.book_subject.works -> The Hidden Life of Otto Frank\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> organization.organization.founders -> Dietrich Eckart\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> book.book_subject.works -> Anne Frank and the Children of the Holocaust\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> book.book_subject.works -> Waffen SS\n# Answer:\nWaffen-SS"], "ground_truth": ["Gestapo", "1st SS Panzer Division Leibstandarte SS Adolf Hitler", "Sturmabteilung", "Nazi Party", "Hitler Youth", "Schutzstaffel", "Waffen-SS", "Wehrmacht"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.391304347826087, "ans_precission": 0.9, "ans_recall": 0.25, "path_f1": 0.391304347826087, "path_precision": 0.9, "path_recall": 0.25, "path_ans_f1": 0.391304347826087, "path_ans_precision": 0.9, "path_ans_recall": 0.25}
{"id": "WebQTest-1305", "prediction": ["# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.0909kmj -> award.award_nomination.award -> Golden Globe Award for Best Actress \u2013 Motion Picture \u2013 Musical or Comedy\n# Answer:\nGolden Globe Award for Best Actress \u2013 Motion Picture \u2013 Musical or Comedy", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.0909kq0 -> award.award_nomination.award -> Golden Globe Award for Best Actress \u2013 Motion Picture \u2013 Musical or Comedy\n# Answer:\nGolden Globe Award for Best Actress \u2013 Motion Picture \u2013 Musical or Comedy", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.0909kmj -> award.award_nomination.nominated_for -> Bus Stop\n# Answer:\nBus Stop", "# Reasoning Path:\nMarilyn Monroe -> award.award_winner.awards_won -> m.07ynmx5 -> award.award_honor.award -> Golden Globe Award for Best Actress \u2013 Motion Picture \u2013 Musical or Comedy\n# Answer:\nGolden Globe Award for Best Actress \u2013 Motion Picture \u2013 Musical or Comedy", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.07zn09y -> award.award_nomination.award -> BAFTA Award for Best Foreign Actress\n# Answer:\nBAFTA Award for Best Foreign Actress", "# Reasoning Path:\nMarilyn Monroe -> award.award_winner.awards_won -> m.0m20gt1 -> award.award_honor.award -> Golden Globe Henrietta Award for World Film Favorites\n# Answer:\nGolden Globe Henrietta Award for World Film Favorites", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.0909kq0 -> award.award_nomination.nominated_for -> Some Like It Hot\n# Answer:\nSome Like It Hot", "# Reasoning Path:\nMarilyn Monroe -> award.award_winner.awards_won -> m.04yplbb -> award.award_honor.award -> Playboy Playmate\n# Answer:\nPlayboy Playmate", "# Reasoning Path:\nMarilyn Monroe -> award.award_winner.awards_won -> m.07ynmx5 -> award.award_honor.ceremony -> 17th Golden Globe Awards\n# Answer:\n17th Golden Globe Awards", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.0909kq0 -> award.award_nomination.ceremony -> 17th Golden Globe Awards\n# Answer:\n17th Golden Globe Awards"], "ground_truth": ["David di Donatello Golden Plate Award", "Golden Globe Henrietta Award for World Film Favorites", "Golden Globe Award for Best Actress \u2013 Motion Picture \u2013 Musical or Comedy", "Playboy Playmate"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.5, "ans_recall": 0.75, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.6, "path_ans_precision": 0.5, "path_ans_recall": 0.75}
{"id": "WebQTest-1306", "prediction": ["# Reasoning Path:\nIran -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Iraq\n# Answer:\nParliamentary system", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal_per_capita -> g.11b60rrlgr\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal -> g.11bbwyx0z8\n# Answer:\nlocation.statistical_region.gdp_nominal", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6fdrh\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6fls_\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal -> g.11bbwz0bn1\n# Answer:\nlocation.statistical_region.gdp_nominal", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal -> g.11bbwz7d2q\n# Answer:\nlocation.statistical_region.gdp_nominal"], "ground_truth": ["Presidential system", "Unitary state", "Theocracy", "Parliamentary system", "Islamic republic"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.16666666666666666, "ans_precission": 0.14285714285714285, "ans_recall": 0.2, "path_f1": 0.16666666666666666, "path_precision": 0.14285714285714285, "path_recall": 0.2, "path_ans_f1": 0.16666666666666666, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 0.2}
{"id": "WebQTest-1307", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nHuntingdon College", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> University of Alabama School of Law\n# Answer:\nUniversity of Alabama School of Law", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138 -> education.education.institution -> University of Alabama\n# Answer:\nUniversity of Alabama", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.place_of_birth -> Los Angeles\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_types -> Book Edition\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_for -> g.125920htw\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Matar Un Ruisenor -> book.book_edition.book -> To Kill a Mockingbird\n# Answer:\nMatar Un Ruisenor", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Matar Un Ruisenor -> book.publication.contents -> m.0j1v7r4\n# Answer:\nMatar Un Ruisenor"], "ground_truth": ["Monroe County High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1308", "prediction": ["# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0n1m7cd -> education.education.institution -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.02wmyw7 -> education.education.institution -> Exeter College, Oxford\n# Answer:\nExeter College, Oxford", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0w48bvz -> education.education.institution -> King Edward's School, Birmingham\n# Answer:\nKing Edward's School, Birmingham", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0n1m7cd -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.02wmyw7 -> education.education.degree -> First Class Honours\n# Answer:\nFirst Class Honours", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.employment_history -> m.02ht_8b -> business.employment_tenure.company -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0n1m7cd -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.employment_history -> m.02ht_80 -> business.employment_tenure.company -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nJ. R. R. Tolkien -> book.author.works_written -> A Middle English Reader and Vocabulary -> book.written_work.author -> Kenneth Sisam\n# Answer:\nA Middle English Reader and Vocabulary", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0w48bvz -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date"], "ground_truth": ["University of Oxford", "Exeter College, Oxford"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1309", "prediction": ["# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Eyewitness: Reports from an Art World in Crisis\n# Answer:\nModern art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nModern art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.art_forms -> Painting -> base.descriptive_names.names.descriptive_name -> m.0109p390\n# Answer:\nPainting", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Der Blaue Reiter -> common.topic.notable_for -> g.12596yl82\n# Answer:\nDer Blaue Reiter", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Origins, Imitation, Conventions\n# Answer:\nModern art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Abstract art -> visual_art.art_period_movement.associated_artists -> Adja Yunkers\n# Answer:\nAbstract art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Abstract art -> common.topic.image -> Black Square\n# Answer:\nAbstract art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.art_forms -> Painting -> base.descriptive_names.names.descriptive_name -> m.0109p3lg\n# Answer:\nPainting", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.art_forms -> Painting -> interests.collection_category.collectors -> m.09vb4f2\n# Answer:\nPainting", "# Reasoning Path:\nWassily Kandinsky -> book.author.school_or_movement -> Modernism -> visual_art.art_period_movement.associated_artworks -> American Gothic\n# Answer:\nModernism"], "ground_truth": ["Painting"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-131", "prediction": ["# Reasoning Path:\nBarack Obama -> influence.influence_node.influenced_by -> Reinhold Niebuhr -> influence.influence_node.influenced -> Martin Luther King, Jr.\n# Answer:\nReinhold Niebuhr", "# Reasoning Path:\nBarack Obama -> influence.influence_node.influenced_by -> Nipsey Russell -> influence.influence_node.influenced -> Michael Jackson\n# Answer:\nNipsey Russell", "# Reasoning Path:\nBarack Obama -> influence.influence_node.influenced_by -> Reinhold Niebuhr -> common.topic.notable_types -> Author\n# Answer:\nReinhold Niebuhr", "# Reasoning Path:\nBarack Obama -> influence.influence_node.influenced_by -> Reinhold Niebuhr -> people.person.gender -> Male\n# Answer:\nReinhold Niebuhr", "# Reasoning Path:\nBarack Obama -> influence.influence_node.influenced_by -> Nipsey Russell -> people.deceased_person.place_of_death -> New York City\n# Answer:\nNipsey Russell", "# Reasoning Path:\nBarack Obama -> influence.influence_node.influenced_by -> Nipsey Russell -> people.person.ethnicity -> African American\n# Answer:\nNipsey Russell", "# Reasoning Path:\nBarack Obama -> influence.influence_node.influenced_by -> Saul Alinsky -> common.topic.notable_types -> Author\n# Answer:\nSaul Alinsky", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> Yes We Can: A Biography of President Barack Obama -> book.written_work.subjects -> United States of America\n# Answer:\nYes We Can: A Biography of President Barack Obama", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.subjects -> Guerrilla Decontextualization\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Bound Man -> book.written_work.subjects -> United States of America\n# Answer:\nA Bound Man"], "ground_truth": ["Nipsey Russell", "Reinhold Niebuhr", "Saul Alinsky"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1310", "prediction": ["# Reasoning Path:\nFootball League Cup -> sports.sports_championship.events -> 2012 Football League Cup Final -> sports.sports_championship_event.champion -> Liverpool F.C.\n# Answer:\n2012 Football League Cup Final", "# Reasoning Path:\nFootball League Cup -> sports.sports_championship.events -> 2013 Football League Cup Final -> sports.sports_championship_event.champion -> Swansea City A.F.C.\n# Answer:\n2013 Football League Cup Final", "# Reasoning Path:\nFootball League Cup -> sports.sports_championship.events -> 2012 Football League Cup Final -> freebase.valuenotation.is_reviewed -> Championship\n# Answer:\n2012 Football League Cup Final", "# Reasoning Path:\nFootball League Cup -> common.topic.notable_types -> Football Competition -> type.type.domain -> Soccer\n# Answer:\nFootball Competition", "# Reasoning Path:\nFootball League Cup -> sports.sports_championship.events -> 2012 Football League Cup Final -> time.event.locations -> Wembley Stadium\n# Answer:\n2012 Football League Cup Final", "# Reasoning Path:\nFootball League Cup -> sports.sports_championship.events -> 2013 Football League Cup Final -> freebase.valuenotation.is_reviewed -> Championship\n# Answer:\n2013 Football League Cup Final", "# Reasoning Path:\nFootball League Cup -> common.topic.notable_types -> Football Competition -> freebase.type_profile.kind -> Definition\n# Answer:\nFootball Competition", "# Reasoning Path:\nFootball League Cup -> sports.sports_championship.events -> 2013 Football League Cup Final -> freebase.valuenotation.has_no_value -> Comment\n# Answer:\n2013 Football League Cup Final", "# Reasoning Path:\nFootball League Cup -> sports.sports_championship.events -> 1964 Football League Cup Final -> sports.sports_championship_event.season -> 1963\u201364 Football League Cup\n# Answer:\n1964 Football League Cup Final", "# Reasoning Path:\nFootball League Cup -> common.topic.notable_types -> Football Competition -> common.topic.article -> m.021ypb2\n# Answer:\nFootball Competition"], "ground_truth": ["Liverpool F.C."], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.13333333333333333, "path_precision": 0.1, "path_recall": 0.2, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1311", "prediction": ["# Reasoning Path:\nAntoni Gaud\u00ed -> influence.influence_node.influenced_by -> William Morris -> influence.influence_node.influenced_by -> John Ruskin\n# Answer:\nWilliam Morris", "# Reasoning Path:\nAntoni Gaud\u00ed -> people.person.profession -> Visual Artist -> fictional_universe.character_occupation.characters_with_this_occupation -> Eug\u00e8ne Delacroix\n# Answer:\nVisual Artist", "# Reasoning Path:\nAntoni Gaud\u00ed -> architecture.architect.structures_designed -> Sagrada Fam\u00edlia -> architecture.structure.architect -> Carles Buigas\n# Answer:\nSagrada Fam\u00edlia", "# Reasoning Path:\nAntoni Gaud\u00ed -> influence.influence_node.influenced_by -> Eug\u00e8ne Viollet-le-Duc -> influence.influence_node.influenced -> William Burges\n# Answer:\nEug\u00e8ne Viollet-le-Duc", "# Reasoning Path:\nAntoni Gaud\u00ed -> influence.influence_node.influenced_by -> William Morris -> base.kwebbase.kwtopic.connections_from -> william morris a pal of alphonse mucha\n# Answer:\nWilliam Morris", "# Reasoning Path:\nAntoni Gaud\u00ed -> influence.influence_node.influenced_by -> Eug\u00e8ne Viollet-le-Duc -> influence.influence_node.influenced -> Benjamin Bucknall\n# Answer:\nEug\u00e8ne Viollet-le-Duc", "# Reasoning Path:\nAntoni Gaud\u00ed -> architecture.architect.structures_designed -> Sagrada Fam\u00edlia -> architecture.structure.architect -> Dom\u00e8nec Sugra\u00f1es i Gras\n# Answer:\nSagrada Fam\u00edlia", "# Reasoning Path:\nAntoni Gaud\u00ed -> architecture.architect.structures_designed -> Sagrada Fam\u00edlia -> people.place_of_interment.interred_here -> Miquel Capllonch Rotger\n# Answer:\nSagrada Fam\u00edlia", "# Reasoning Path:\nAntoni Gaud\u00ed -> influence.influence_node.influenced_by -> William Morris -> base.kwebbase.kwtopic.has_sentences -> After marrying, Morris commissioned an architect friend to build him a home at Bexleyheath in Kent which became known as the Red House  because it was built of red bricks with no stucco finish.\n# Answer:\nWilliam Morris", "# Reasoning Path:\nAntoni Gaud\u00ed -> influence.influence_node.influenced_by -> Eug\u00e8ne Viollet-le-Duc -> influence.influence_node.influenced -> Hector Guimard\n# Answer:\nEug\u00e8ne Viollet-le-Duc"], "ground_truth": ["Eug\u00e8ne Viollet-le-Duc", "William Morris"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1312", "prediction": ["# Reasoning Path:\nDanish Language -> language.human_language.countries_spoken_in -> Greenland -> location.administrative_division.country -> Denmark\n# Answer:\nGreenland", "# Reasoning Path:\nDanish Language -> language.human_language.countries_spoken_in -> Denmark -> location.location.containedby -> Northern Europe\n# Answer:\nDenmark", "# Reasoning Path:\nDanish Language -> language.human_language.main_country -> Denmark -> location.location.containedby -> Northern Europe\n# Answer:\nDenmark", "# Reasoning Path:\nDanish Language -> language.human_language.countries_spoken_in -> Denmark -> location.country.languages_spoken -> Faroese\n# Answer:\nDenmark", "# Reasoning Path:\nDanish Language -> language.human_language.main_country -> Denmark -> location.country.languages_spoken -> German Language\n# Answer:\nDenmark", "# Reasoning Path:\nDanish Language -> media_common.netflix_genre.titles -> R -> media_common.netflix_title.netflix_genres -> Thriller\n# Answer:\nR", "# Reasoning Path:\nDanish Language -> language.human_language.countries_spoken_in -> Denmark -> location.location.containedby -> Europe\n# Answer:\nDenmark", "# Reasoning Path:\nDanish Language -> language.human_language.main_country -> Denmark -> location.location.containedby -> Europe\n# Answer:\nDenmark", "# Reasoning Path:\nDanish Language -> language.human_language.countries_spoken_in -> Denmark -> location.location.containedby -> Scandinavia\n# Answer:\nDenmark", "# Reasoning Path:\nDanish Language -> language.human_language.main_country -> Denmark -> location.location.containedby -> Scandinavia\n# Answer:\nDenmark"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1313", "prediction": ["# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.children -> Persephone -> fictional_universe.fictional_character.parents -> Zeus\n# Answer:\nPersephone", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.children -> Persephone -> common.topic.article -> m.08nkrwy\n# Answer:\nPersephone", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.children -> Persephone -> film.film_character.portrayed_in_films -> m.08nknvb\n# Answer:\nPersephone", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.siblings -> m.0j85m5t -> fictional_universe.sibling_relationship_of_fictional_characters.siblings -> Zeus\n# Answer:\nZeus", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.children -> Persephone -> film.film_character.portrayed_in_films -> m.0bvrrhh\n# Answer:\nPersephone", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.siblings -> m.0gwhv5j -> fictional_universe.sibling_relationship_of_fictional_characters.siblings -> Zeus\n# Answer:\nZeus", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.gender -> Female -> medicine.risk_factor.diseases -> Acne\n# Answer:\nFemale", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.children -> Persephone -> film.film_character.portrayed_in_films -> m.0cs1kzn\n# Answer:\nPersephone", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.gender -> Female -> medicine.risk_factor.diseases -> Acrodermatitis chronica atrophicans\n# Answer:\nFemale", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.gender -> Female -> base.gender.gender_identity.people -> m.02_98hp\n# Answer:\nFemale"], "ground_truth": ["Zeus"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1314", "prediction": ["# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel Ritz Paris -> common.topic.image -> The Ritz garden caf\u00e9 by the Swiss artist, Pierre-Georges Jeanniot (1848-1934)\n# Answer:\nH\u00f4tel Ritz Paris", "# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel Ritz Paris -> common.topic.image -> Hotel Ritz Paris\n# Answer:\nH\u00f4tel Ritz Paris", "# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel Ritz Paris -> architecture.structure.architect -> Bernard Gaucherel\n# Answer:\nH\u00f4tel Ritz Paris", "# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel de Crillon -> common.topic.webpage -> m.0g5c9ry\n# Answer:\nH\u00f4tel de Crillon", "# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel Ritz Paris -> architecture.structure.architect -> Charles Mew\u00e8s\n# Answer:\nH\u00f4tel Ritz Paris", "# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel de Crillon -> common.topic.image -> H\u00c3\u00b4tel de Crillon 25 08 2007\n# Answer:\nH\u00f4tel de Crillon", "# Reasoning Path:\nParis -> common.topic.webpage -> m.02k8dbw -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel Ritz Paris -> architecture.structure.architect -> Jules Hardouin-Mansart\n# Answer:\nH\u00f4tel Ritz Paris", "# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel de Crillon -> common.topic.notable_for -> g.1259xm9y4\n# Answer:\nH\u00f4tel de Crillon", "# Reasoning Path:\nParis -> common.topic.webpage -> m.02k8dbw -> common.webpage.resource -> m.0bkwkw7\n# Answer:\ncommon.webpage.resource"], "ground_truth": ["H\u00f4tel de Crillon", "H\u00f4tel Ritz Paris"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1316", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> common.topic.image -> WilliamFranklin\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Francis Folger Franklin -> people.person.parents -> Deborah Read\n# Answer:\nFrancis Folger Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> people.person.gender -> Male\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Sarah Franklin Bache -> people.person.children -> Benjamin Franklin Bache\n# Answer:\nSarah Franklin Bache", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> American literature\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Sarah Franklin Bache -> people.person.spouse_s -> m.0j4k6gy\n# Answer:\nSarah Franklin Bache", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Francis Folger Franklin -> people.deceased_person.place_of_burial -> Christ Church Burial Ground\n# Answer:\nFrancis Folger Franklin", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Francis Folger Franklin -> common.topic.notable_for -> g.125by3nfc\n# Answer:\nFrancis Folger Franklin", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> :Library and information science\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze"], "ground_truth": ["Deborah Read"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1317", "prediction": ["# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> location.location.containedby -> Iberian Peninsula\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.country.second_level_divisions -> Zaragoza -> location.location.containedby -> Aragon\n# Answer:\nZaragoza", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> periodicals.newspaper_circulation_area.newspapers -> P\u00fablico\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> location.location.containedby -> Community of Madrid\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.country.second_level_divisions -> Zaragoza -> base.aareas.schema.administrative_area.administrative_parent -> Aragon\n# Answer:\nZaragoza", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> periodicals.newspaper_circulation_area.newspapers -> ADN\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.country.second_level_divisions -> Asturias -> base.aareas.schema.administrative_area.administrative_area_type -> Spanish province\n# Answer:\nAsturias", "# Reasoning Path:\nSpain -> location.country.administrative_divisions -> Andalusia -> location.location.contains -> Seville\n# Answer:\nAndalusia", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> periodicals.newspaper_circulation_area.newspapers -> Diario AS\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.country.second_level_divisions -> Zaragoza -> sports.sports_team_location.teams -> Basket Zaragoza 2002\n# Answer:\nZaragoza"], "ground_truth": ["Catalonia"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1319", "prediction": ["# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> baseball.baseball_player.batting_stats -> m.06sbxkk -> baseball.batting_statistics.team -> Detroit Tigers\n# Answer:\nDetroit Tigers", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> baseball.baseball_player.batting_stats -> m.06sbxlb -> baseball.batting_statistics.team -> Detroit Tigers\n# Answer:\nDetroit Tigers", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> sports.pro_athlete.teams -> m.0z9w091 -> sports.sports_team_roster.team -> Detroit Tigers\n# Answer:\nDetroit Tigers", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> baseball.baseball_player.batting_stats -> m.06sbxkk -> baseball.batting_statistics.season -> 2004 Major League Baseball season\n# Answer:\n2004 Major League Baseball season", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> baseball.baseball_player.batting_stats -> m.06sbx7d -> baseball.batting_statistics.team -> Texas Rangers\n# Answer:\nTexas Rangers", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> baseball.baseball_player.batting_stats -> m.06sbxlb -> baseball.batting_statistics.season -> 2005 Major League Baseball season\n# Answer:\n2005 Major League Baseball season", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> sports.pro_athlete.teams -> m.0ywy2gy -> sports.sports_team_roster.team -> Texas Rangers\n# Answer:\nTexas Rangers", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> sports.pro_athlete.teams -> m.0z9w091 -> sports.sports_team_roster.position -> Catcher\n# Answer:\nCatcher", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> sports.pro_athlete.teams -> m.0z9w064 -> sports.sports_team_roster.team -> Miami Marlins\n# Answer:\nMiami Marlins", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale"], "ground_truth": ["Miami Marlins", "Detroit Tigers", "Texas Rangers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.3870967741935483, "path_precision": 0.6, "path_recall": 0.2857142857142857, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-132", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Schuyler Frances Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSchuyler Frances Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.spouse -> Tracy Pollan\n# Answer:\nTracy Pollan", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.profession -> Actor\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.sibling_s -> m.0j217jw\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> common.topic.notable_types -> Person\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.nationality -> United States of America\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.location_of_ceremony -> Arlington\n# Answer:\nArlington", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.sibling_s -> m.0tjswcf\n# Answer:\nSam Michael Fox"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1320", "prediction": ["# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.arena_stadium -> Miller Park -> base.playball.baseball_stadium.naming_rights -> m.05lm6s0\n# Answer:\nMiller Park", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.arena_stadium -> Miller Park -> sports.sports_facility.home_venue_for -> m.0wz1z23\n# Answer:\nMiller Park", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.arena_stadium -> Miller Park -> architecture.structure.architecture_firm -> Eppstein Uhen Architects\n# Answer:\nMiller Park", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.arena_stadium -> Miller Park -> architecture.structure.architecture_firm -> NBBJ\n# Answer:\nMiller Park", "# Reasoning Path:\nMilwaukee Brewers -> base.schemastaging.organization_extra.contact_webpages -> m.010dwkgw -> internet.localized_uri.location -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.arena_stadium -> Miller Park -> architecture.structure.architecture_firm -> HKS, Inc.\n# Answer:\nMiller Park", "# Reasoning Path:\nMilwaukee Brewers -> base.schemastaging.organization_extra.contact_webpages -> m.010dwkgw -> internet.localized_uri.language -> English\n# Answer:\nEnglish", "# Reasoning Path:\nMilwaukee Brewers -> baseball.baseball_team.team_stats -> m.05n61_w -> baseball.baseball_team_stats.season -> 1977 Major League Baseball Season\n# Answer:\n1977 Major League Baseball Season", "# Reasoning Path:\nMilwaukee Brewers -> baseball.baseball_team.team_stats -> m.05n61hh -> baseball.baseball_team_stats.season -> 1970 Major League Baseball Season\n# Answer:\n1970 Major League Baseball Season", "# Reasoning Path:\nMilwaukee Brewers -> baseball.baseball_team.team_stats -> m.05n61lk -> baseball.baseball_team_stats.season -> 1971 Major League Baseball Season\n# Answer:\n1971 Major League Baseball Season"], "ground_truth": ["Miller Park"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1321", "prediction": ["# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> film.film.release_date_s -> m.04mvn_0\n# Answer:\nGallipoli", "# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> film.film.release_date_s -> m.04mvnyd\n# Answer:\nGallipoli", "# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> location.location.events -> Battle for No.3 Post\n# Answer:\nGallipoli", "# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> film.film.release_date_s -> m.04mvnyq\n# Answer:\nGallipoli", "# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> location.location.contains -> ANZAC Cove\n# Answer:\nGallipoli", "# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> location.location.events -> Battle of Chunuk Bair\n# Answer:\nGallipoli", "# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> location.location.events -> Battle of Gallipoli\n# Answer:\nGallipoli", "# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> location.location.contains -> Cape Helles\n# Answer:\nGallipoli", "# Reasoning Path:\nGallipoli Campaign -> military.military_conflict.military_personnel_involved -> Alexander Burton -> military.military_person.participated_in_conflicts -> World War I\n# Answer:\nAlexander Burton", "# Reasoning Path:\nGallipoli Campaign -> military.military_conflict.force_strengths -> m.04kc3p1 -> military.force_strength.combatant -> Ottoman Empire\n# Answer:\nOttoman Empire"], "ground_truth": ["Gallipoli"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1322", "prediction": ["# Reasoning Path:\nTom Hardy -> film.actor.film -> g.11b77pmp46\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nTom Hardy -> film.actor.film -> m.010s_rgp -> film.performance.film -> Legend\n# Answer:\nLegend", "# Reasoning Path:\nTom Hardy -> film.actor.film -> m.010s_szd -> film.performance.film -> Legend\n# Answer:\nLegend", "# Reasoning Path:\nTom Hardy -> freebase.valuenotation.has_no_value -> Siblings -> rdf-schema#domain -> Person\n# Answer:\nSiblings", "# Reasoning Path:\nTom Hardy -> award.award_nominee.award_nominations -> m.0dlskzm -> award.award_nomination.nominated_for -> Inception\n# Answer:\nInception", "# Reasoning Path:\nTom Hardy -> freebase.valuenotation.has_no_value -> Siblings -> type.property.master_property -> Sibling\n# Answer:\nSiblings", "# Reasoning Path:\nTom Hardy -> film.actor.film -> m.010s_rgp -> film.performance.character -> Ronald Kray\n# Answer:\nRonald Kray", "# Reasoning Path:\nTom Hardy -> film.actor.film -> m.010s_szd -> film.performance.character -> Reginald Kray\n# Answer:\nReginald Kray", "# Reasoning Path:\nTom Hardy -> freebase.valuenotation.has_no_value -> Siblings -> type.property.schema -> Person\n# Answer:\nSiblings", "# Reasoning Path:\nTom Hardy -> award.award_nominee.award_nominations -> m.0hjbfqm -> award.award_nomination.nominated_for -> Warrior\n# Answer:\nWarrior"], "ground_truth": ["Marie Antoinette", "Gideon's Daughter", "Minotaur", "London Road", "Warrior", "The Revenant", "Scenes of a Sexual Nature", "Splinter Cell", "The Reckoning", "Everest", "Dot the I", "Bronson", "Legend", "Colditz", "Deserter", "Locke", "Layer Cake", "Black Hawk Down", "W\u0394Z", "Inception", "The Virgin Queen", "The Inheritance", "Lawless", "Tinker Tailor Soldier Spy", "Perfect", "The Dark Knight Rises", "Star Trek Nemesis", "Child 44", "Lethal Dose", "The Drop", "Flood", "Sucker Punch", "The Outsider", "Mad Max: Fury Road", "This Means War", "EMR", "Sweeney Todd", "Sergeant Slaughter, My Big Brother", "RocknRolla", "Thick as Thieves"], "ans_acc": 0.075, "ans_hit": 1, "ans_f1": 0.1263157894736842, "ans_precission": 0.4, "ans_recall": 0.075, "path_f1": 0.11940298507462686, "path_precision": 0.4, "path_recall": 0.07017543859649122, "path_ans_f1": 0.1263157894736842, "path_ans_precision": 0.4, "path_ans_recall": 0.075}
{"id": "WebQTest-1323", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> sports.sports_facility.teams -> Atlanta Falcons\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> United States of America\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola -> location.location.containedby -> Fulton County\n# Answer:\nWorld of Coca-Cola", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola -> common.topic.notable_types -> Museum\n# Answer:\nWorld of Coca-Cola", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> common.topic.notable_types -> Zoo\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.containedby -> United States of America\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> travel.transport_terminus.travel_destinations_served -> m.052zwlt\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Atlanta Regional Airport -> aviation.airport.serves -> Peachtree City\n# Answer:\nAtlanta Regional Airport", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola -> location.location.containedby -> 30313\n# Answer:\nWorld of Coca-Cola"], "ground_truth": ["Turner Field", "Center for Puppetry Arts", "Atlanta Symphony Orchestra", "Atlanta Marriott Marquis", "Hyatt Regency Atlanta", "Six Flags White Water", "Arbor Place Mall", "Centennial Olympic Park", "Atlanta Cyclorama & Civil War Museum", "Jimmy Carter Library and Museum", "The Tabernacle", "Variety Playhouse", "World of Coca-Cola", "Four Seasons Hotel Atlanta", "Georgia State Capitol", "Woodruff Arts Center", "Atlanta History Center", "Atlanta Ballet", "Cobb Energy Performing Arts Centre", "Masquerade", "Underground Atlanta", "Margaret Mitchell House & Museum", "Fernbank Museum of Natural History", "Fernbank Science Center", "Fox Theatre", "Peachtree Road Race", "CNN Center", "Atlanta Jewish Film Festival", "Martin Luther King, Jr. National Historic Site", "Georgia Aquarium", "Six Flags Over Georgia", "Omni Coliseum", "Georgia Dome", "Philips Arena", "Zoo Atlanta", "Georgia World Congress Center"], "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0.1509433962264151, "ans_precission": 0.8, "ans_recall": 0.08333333333333333, "path_f1": 0.1509433962264151, "path_precision": 0.8, "path_recall": 0.08333333333333333, "path_ans_f1": 0.19512195121951217, "path_ans_precision": 0.8, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-1324", "prediction": ["# Reasoning Path:\nJohn Forbes Nash, Jr. -> people.person.education -> m.02kq1bj -> education.education.institution -> Princeton University\n# Answer:\nPrinceton University", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> people.person.education -> m.03kwg72 -> education.education.institution -> Carnegie Mellon College of Engineering\n# Answer:\nCarnegie Mellon College of Engineering", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> people.person.education -> m.02kq1bj -> education.education.degree -> Doctorate\n# Answer:\nDoctorate", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> people.person.education -> m.0n0rj5w -> education.education.institution -> Bluefield High School\n# Answer:\nBluefield High School", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> education.academic.advisors -> Albert W. Tucker -> education.academic.advisors -> Solomon Lefschetz\n# Answer:\nAlbert W. Tucker", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> people.person.education -> m.03kwg72 -> education.education.major_field_of_study -> Mathematics\n# Answer:\nMathematics", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> people.person.education -> m.03kwg72 -> education.education.degree -> Master of Science\n# Answer:\nMaster of Science", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> book.author.works_written -> Non-cooperative Games -> education.dissertation.institution -> Princeton University\n# Answer:\nNon-cooperative Games", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> book.author.works_written -> Non-cooperative Games -> education.dissertation.degree -> Doctorate\n# Answer:\nNon-cooperative Games", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> education.academic.advisors -> Albert W. Tucker -> people.person.education -> m.02kq0xy\n# Answer:\nAlbert W. Tucker"], "ground_truth": ["Princeton University", "Carnegie Mellon College of Engineering"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.3, "path_recall": 0.6, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1325", "prediction": ["# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Botswana\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> Maltese Language -> language.human_language.region -> Europe\n# Answer:\nMaltese Language", "# Reasoning Path:\nMalta -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.statistical_region.deposit_interest_rate -> g.1hhc37h6m\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nMalta -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Botswana\n# Answer:\nEnglish Language"], "ground_truth": ["English Language", "Maltese Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1326", "prediction": ["# Reasoning Path:\nThomas Edison -> base.argumentmaps.innovator.original_ideas -> Phonograph -> common.topic.article -> m.063mq\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Phonograph -> common.topic.article -> m.063mq\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> common.image.size -> m.02ctc4t\n# Answer:\ncommon.image.size", "# Reasoning Path:\nThomas Edison -> base.argumentmaps.innovator.original_ideas -> Phonograph -> base.argumentmaps.original_idea.innovator -> Eldridge R. Johnson\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> base.argumentmaps.innovator.original_ideas -> Phonograph -> law.invention.inventor -> Eldridge R. Johnson\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Carbon microphone -> common.topic.article -> m.0d9fl1\n# Answer:\nCarbon microphone", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Phonograph -> base.argumentmaps.original_idea.innovator -> Eldridge R. Johnson\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> base.argumentmaps.innovator.original_ideas -> Phonograph -> law.invention.inventor -> Emile Berliner\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Phonograph -> law.invention.inventor -> Eldridge R. Johnson\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> common.image.size -> m.01xjmzn\n# Answer:\ncommon.image.size"], "ground_truth": ["Movie camera", "Mimeograph", "Incandescent light bulb", "Carbon microphone", "Phonograph", "Electric Power Distribution", "Quadruplex telegraph", "Phonograph cylinder"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.38095238095238093, "ans_precission": 0.8, "ans_recall": 0.25, "path_f1": 0.3076923076923077, "path_precision": 0.4, "path_recall": 0.25, "path_ans_f1": 0.38095238095238093, "path_ans_precision": 0.8, "path_ans_recall": 0.25}
{"id": "WebQTest-1327", "prediction": ["# Reasoning Path:\nShakira -> people.person.place_of_birth -> Barranquilla -> location.location.containedby -> Colombia\n# Answer:\nBarranquilla", "# Reasoning Path:\nShakira -> people.person.nationality -> Colombia -> location.country.languages_spoken -> Spanish Language\n# Answer:\nColombia", "# Reasoning Path:\nShakira -> people.person.nationality -> Colombia -> common.topic.notable_types -> Country\n# Answer:\nColombia", "# Reasoning Path:\nShakira -> people.person.nationality -> Colombia -> location.country.official_language -> Spanish Language\n# Answer:\nColombia", "# Reasoning Path:\nShakira -> tv.tv_actor.guest_roles -> m.0v3dwp1 -> tv.tv_guest_role.special_performance_type -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nShakira -> tv.tv_actor.guest_roles -> m.09nb6_1 -> tv.tv_guest_role.episodes_appeared_in -> and then there were 9...\n# Answer:\nand then there were 9...", "# Reasoning Path:\nShakira -> tv.tv_actor.guest_roles -> m.0v3dwp1 -> tv.tv_guest_role.episodes_appeared_in -> Dude Looks Like Shakira\n# Answer:\nDude Looks Like Shakira", "# Reasoning Path:\nShakira -> tv.tv_actor.guest_roles -> m.09nb6_6 -> tv.tv_guest_role.episodes_appeared_in -> Episode 904A\n# Answer:\nEpisode 904A"], "ground_truth": ["Colombia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1328", "prediction": ["# Reasoning Path:\nRutherford B. Hayes -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Senator\n# Answer:\nPolitician", "# Reasoning Path:\nRutherford B. Hayes -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Representative\n# Answer:\nPolitician", "# Reasoning Path:\nRutherford B. Hayes -> common.topic.notable_types -> US President -> type.type.expected_by -> US Presidents\n# Answer:\nUS President", "# Reasoning Path:\nRutherford B. Hayes -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> Ambassador\n# Answer:\nPolitician", "# Reasoning Path:\nRutherford B. Hayes -> common.topic.notable_types -> US President -> type.type.expected_by -> President\n# Answer:\nUS President", "# Reasoning Path:\nRutherford B. Hayes -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdn_\n# Answer:\nPolitician", "# Reasoning Path:\nRutherford B. Hayes -> people.person.profession -> Politician -> people.profession.specializations -> Tyrant\n# Answer:\nPolitician", "# Reasoning Path:\nRutherford B. Hayes -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdq3\n# Answer:\nPolitician", "# Reasoning Path:\nRutherford B. Hayes -> common.topic.notable_types -> US President -> type.type.expected_by -> Owner\n# Answer:\nUS President", "# Reasoning Path:\nRutherford B. Hayes -> common.topic.notable_types -> US President -> type.type.properties -> Vice president\n# Answer:\nUS President"], "ground_truth": ["US President"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1329", "prediction": ["# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.02vc8t6 -> film.performance.actor -> Ian Holm\n# Answer:\nIan Holm", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> film.film.starring -> m.02vc8t6 -> film.performance.actor -> Ian Holm\n# Answer:\nIan Holm", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.0glw4lg -> film.performance.actor -> Ian Holm\n# Answer:\nIan Holm", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.0gwn5hf -> film.performance.actor -> Ian Holm\n# Answer:\nIan Holm", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.02vc8t6 -> film.performance.film -> The Lord of the Rings: The Fellowship of the Ring\n# Answer:\nfilm.performance.film", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> film.film.starring -> m.02vc6p4 -> film.performance.actor -> Dominic Monaghan\n# Answer:\nDominic Monaghan", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> film.film.starring -> m.02vc8t6 -> film.performance.character -> Old Bilbo\n# Answer:\nfilm.performance.character", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> film.film.starring -> m.02vc8d8 -> film.performance.actor -> Billy Boyd\n# Answer:\nBilly Boyd", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.0glw4lg -> film.performance.film -> The Hobbit: An Unexpected Journey\n# Answer:\nThe Hobbit: An Unexpected Journey", "# Reasoning Path:\nOld Bilbo -> fictional_universe.fictional_character.powers_or_abilities -> Invisibility -> book.book_subject.works -> Memoirs of an Invisible Man\n# Answer:\nInvisibility"], "ground_truth": ["Ian Holm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-133", "prediction": ["# Reasoning Path:\nDolly Parton -> music.artist.origin -> Sevierville -> location.location.containedby -> Tennessee\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> music.artist.origin -> Sevierville -> location.location.people_born_here -> Willadeene Parton\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> music.artist.origin -> Sevierville -> location.location.people_born_here -> Stella Parton\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> people.person.place_of_birth -> Sevierville -> location.location.containedby -> Tennessee\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> music.artist.origin -> Sevierville -> location.location.people_born_here -> Avie Lee Owens\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> music.artist.origin -> Sevierville -> location.location.containedby -> Sevier County\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> people.person.place_of_birth -> Sevierville -> location.location.people_born_here -> Willadeene Parton\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> people.person.place_of_birth -> Sevierville -> location.location.containedby -> Sevier County\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> people.person.place_of_birth -> Sevierville -> location.location.people_born_here -> Stella Parton\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> film.actor.film -> m.02vc16s -> film.performance.film -> Unlikely Angel\n# Answer:\nUnlikely Angel"], "ground_truth": ["Sevierville"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1330", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.administrative_divisions -> Illinois -> location.location.containedby -> United States, with Territories\n# Answer:\nIllinois", "# Reasoning Path:\nUnited States of America -> location.country.administrative_divisions -> Pennsylvania -> location.location.containedby -> Contiguous United States\n# Answer:\nPennsylvania", "# Reasoning Path:\nUnited States of America -> location.country.administrative_divisions -> Illinois -> base.locations.states_and_provences.cities_within -> Belleville\n# Answer:\nIllinois", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Illinois -> location.location.containedby -> United States, with Territories\n# Answer:\nIllinois", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_children -> Illinois -> base.locations.states_and_provences.cities_within -> Belleville\n# Answer:\nIllinois", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Illinois -> base.locations.states_and_provences.cities_within -> Belleville\n# Answer:\nIllinois", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_children -> Pennsylvania -> location.location.containedby -> Contiguous United States\n# Answer:\nPennsylvania", "# Reasoning Path:\nUnited States of America -> location.country.administrative_divisions -> Illinois -> location.location.containedby -> Contiguous United States\n# Answer:\nIllinois", "# Reasoning Path:\nUnited States of America -> location.country.administrative_divisions -> Pennsylvania -> freebase.valuenotation.is_reviewed -> Minimum wage\n# Answer:\nPennsylvania", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Pennsylvania -> location.location.containedby -> Contiguous United States\n# Answer:\nPennsylvania"], "ground_truth": ["New York"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1331", "prediction": ["# Reasoning Path:\nArizona -> government.political_district.representatives -> m.09w0zkh -> government.government_position_held.office_holder -> Barry Goldwater\n# Answer:\nBarry Goldwater", "# Reasoning Path:\nArizona -> government.political_district.representatives -> m.09w11bv -> government.government_position_held.office_holder -> Barry Goldwater\n# Answer:\nBarry Goldwater", "# Reasoning Path:\nArizona -> government.political_district.representatives -> m.09w0_n4 -> government.government_position_held.office_holder -> Marcus A. Smith\n# Answer:\nMarcus A. Smith", "# Reasoning Path:\nArizona -> government.political_district.representatives -> m.09w0zkh -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nUnited States Senate", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y42 -> government.government_position_held.office_holder -> Ernest McFarland\n# Answer:\nErnest McFarland", "# Reasoning Path:\nArizona -> government.political_district.representatives -> m.09w11bv -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nUnited States Senate", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.0108gfng -> government.government_position_held.office_holder -> Keith Brown\n# Answer:\nKeith Brown", "# Reasoning Path:\nArizona -> government.political_district.representatives -> m.09w0_n4 -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.010dylrt -> government.government_position_held.office_holder -> Terry Goddard\n# Answer:\nTerry Goddard", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y42 -> government.government_position_held.office_position_or_title -> Governor of Arizona\n# Answer:\nGovernor of Arizona"], "ground_truth": ["Barry Goldwater", "Jon Kyl", "Ralph H. Cameron", "Paul Fannin", "John McCain", "Jeff Flake", "Marcus A. Smith", "Henry F. Ashurst", "Carl Hayden", "Dennis DeConcini", "Ernest McFarland"], "ans_acc": 0.2727272727272727, "ans_hit": 1, "ans_f1": 0.3243243243243243, "ans_precission": 0.4, "ans_recall": 0.2727272727272727, "path_f1": 0.3333333333333333, "path_precision": 0.4, "path_recall": 0.2857142857142857, "path_ans_f1": 0.3243243243243243, "path_ans_precision": 0.4, "path_ans_recall": 0.2727272727272727}
{"id": "WebQTest-1332", "prediction": ["# Reasoning Path:\nVietnam War -> book.written_work.subjects -> United States of America -> military.military_combatant.force_deployments -> m.02h782s\n# Answer:\nUnited States of America", "# Reasoning Path:\nVietnam War -> time.event.included_in_event -> 1960s -> time.event.includes_event -> 1960\n# Answer:\n1960s", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> United States of America -> military.military_combatant.force_deployments -> m.02h7m00\n# Answer:\nUnited States of America", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> United States of America -> military.military_combatant.casualties -> m.043wptp\n# Answer:\nUnited States of America", "# Reasoning Path:\nVietnam War -> time.event.included_in_event -> 1970s -> time.event.includes_event -> United States Bicentennial\n# Answer:\n1970s", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> United States of America -> military.military_combatant.casualties -> m.04kb4m4\n# Answer:\nUnited States of America", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> Asia -> location.location.events -> World War II\n# Answer:\nAsia", "# Reasoning Path:\nVietnam War -> time.event.included_in_event -> 20th century -> time.event.includes_event -> 1970s\n# Answer:\n20th century", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> Southeast Asia -> location.location.events -> 2014 ABL season\n# Answer:\nSoutheast Asia", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> Asia -> base.locations.continents.countries_within -> Cambodia\n# Answer:\nAsia"], "ground_truth": ["1975-04-30"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1333", "prediction": ["# Reasoning Path:\nTennessee River -> geography.river.mouth -> Ohio River -> geography.river.mouth -> Cairo\n# Answer:\nOhio River", "# Reasoning Path:\nTennessee River -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nTennessee River -> geography.river.mouth -> Ohio River -> location.location.partially_containedby -> Kentucky\n# Answer:\nOhio River", "# Reasoning Path:\nTennessee River -> geography.river.mouth -> Ohio River -> geography.river.mouth -> Mississippi River\n# Answer:\nOhio River", "# Reasoning Path:\nTennessee River -> location.location.containedby -> United States of America -> location.location.partially_containedby -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nTennessee River -> geography.river.mouth -> Ohio River -> location.location.partially_containedby -> Ohio\n# Answer:\nOhio River", "# Reasoning Path:\nTennessee River -> geography.river.mouth -> Ohio River -> geography.body_of_water.bridges -> Aetnaville Bridge\n# Answer:\nOhio River", "# Reasoning Path:\nTennessee River -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Tennessee\n# Answer:\nUnited States of America", "# Reasoning Path:\nTennessee River -> location.location.containedby -> North America -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nNorth America", "# Reasoning Path:\nTennessee River -> geography.river.mouth -> Ohio River -> location.location.partially_containedby -> Illinois\n# Answer:\nOhio River"], "ground_truth": ["Ohio River"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1334", "prediction": ["# Reasoning Path:\nApollo 13 -> film.film.story_by -> Jim Lovell -> award.award_nominee.award_nominations -> m.0ycq_cj\n# Answer:\nJim Lovell", "# Reasoning Path:\nApollo 13 -> film.film.story_by -> Jim Lovell -> freebase.valuenotation.has_value -> Parents\n# Answer:\nJim Lovell", "# Reasoning Path:\nTom Hanks -> award.award_nominee.award_nominations -> m.010b2qn2 -> award.award_nomination.nominated_for -> Captain Phillips\n# Answer:\nCaptain Phillips", "# Reasoning Path:\nTom Hanks -> film.actor.film -> g.11b70lbbvb\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nApollo 13 -> film.film.story_by -> Jim Lovell -> people.person.profession -> Writer\n# Answer:\nJim Lovell", "# Reasoning Path:\nTom Hanks -> award.award_nominee.award_nominations -> m.010b2qn2 -> award.award_nomination.ceremony -> 3rd AACTA International Awards\n# Answer:\n3rd AACTA International Awards", "# Reasoning Path:\nApollo 13 -> film.film.story_by -> Jeffrey Kluger -> book.author.works_written -> Journey beyond Sele\u0304ne\u0304\n# Answer:\nJeffrey Kluger", "# Reasoning Path:\nTom Hanks -> award.award_nominee.award_nominations -> m.01140ymz -> award.award_nomination.nominated_for -> The Sixties\n# Answer:\nThe Sixties", "# Reasoning Path:\nTom Hanks -> award.award_winner.awards_won -> m.03mlq_x -> award.award_honor.honored_for -> Philadelphia\n# Answer:\nPhiladelphia", "# Reasoning Path:\nApollo 13 -> film.film.starring -> m.02tbgwr -> film.performance.actor -> Kathleen Quinlan\n# Answer:\nKathleen Quinlan"], "ground_truth": ["Jim Lovell"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1335", "prediction": ["# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> India\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Kenya\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.languages_spoken -> \u00c9w\u00e9 Language -> language.human_language.writing_system -> Latin alphabet\n# Answer:\n\u00c9w\u00e9 Language", "# Reasoning Path:\nGhana -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60ywwvy\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nGhana -> location.country.languages_spoken -> \u00c9w\u00e9 Language -> language.human_language.language_family -> Niger\u2013Congo languages\n# Answer:\n\u00c9w\u00e9 Language", "# Reasoning Path:\nGhana -> location.country.languages_spoken -> \u00c9w\u00e9 Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\n\u00c9w\u00e9 Language"], "ground_truth": ["Akan Language", "Kasem Language", "Dangme Language", "\u00c9w\u00e9 Language", "Asante dialect", "Dagaare language", "Fula language", "Nzema Language", "English Language", "Gonja Language", "Dagbani Language", "Ga Language"], "ans_acc": 0.16666666666666666, "ans_hit": 1, "ans_f1": 0.28125, "ans_precission": 0.9, "ans_recall": 0.16666666666666666, "path_f1": 0.23529411764705882, "path_precision": 0.4, "path_recall": 0.16666666666666666, "path_ans_f1": 0.28125, "path_ans_precision": 0.9, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTest-1336", "prediction": ["# Reasoning Path:\nUtah State Capitol -> location.location.containedby -> Salt Lake City -> location.location.containedby -> Utah\n# Answer:\nSalt Lake City", "# Reasoning Path:\nUtah State Capitol -> location.location.geolocation -> m.0cnlcc_\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nUtah -> base.aareas.schema.administrative_area.capital -> Salt Lake City -> location.statistical_region.population -> g.11b66h2b_k\n# Answer:\nSalt Lake City", "# Reasoning Path:\nUtah State Capitol -> location.location.containedby -> Capitol Hill -> location.location.containedby -> Utah\n# Answer:\nCapitol Hill", "# Reasoning Path:\nUtah State Capitol -> common.topic.image -> Utah State Capitol in 2002 -> common.image.appears_in_topic_gallery -> Utah State Legislature\n# Answer:\nUtah State Capitol in 2002", "# Reasoning Path:\nUtah -> base.aareas.schema.administrative_area.capital -> Salt Lake City -> location.statistical_region.population -> g.11b7tm7k1_\n# Answer:\nSalt Lake City", "# Reasoning Path:\nUtah -> base.aareas.schema.administrative_area.capital -> Salt Lake City -> tv.tv_location.tv_shows_filmed_here -> The Stand\n# Answer:\nSalt Lake City", "# Reasoning Path:\nUtah State Capitol -> location.location.containedby -> Salt Lake City -> location.statistical_region.population -> g.11b66h2b_k\n# Answer:\nSalt Lake City", "# Reasoning Path:\nUtah -> base.aareas.schema.administrative_area.capital -> Salt Lake City -> location.statistical_region.population -> g.11btt563rr\n# Answer:\nSalt Lake City", "# Reasoning Path:\nUtah -> base.aareas.schema.administrative_area.capital -> Salt Lake City -> location.citytown.postal_codes -> 84101\n# Answer:\nSalt Lake City"], "ground_truth": ["Salt Lake City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1337", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove -> common.topic.article -> m.029mn3\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_trwk\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> Glass\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> medicine.medical_treatment.used_to_treat -> Presbyopia\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_ty__\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.image -> A bifocal corrective eyeglasses lens\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> Waking up early\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_wj2d\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.instrument.family -> Crystallophone\n# Answer:\nGlass harmonica"], "ground_truth": ["Bifocals", "Lightning rod", "Glass harmonica", "Franklin stove"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.8, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.8, "path_ans_recall": 0.75}
{"id": "WebQTest-1339", "prediction": ["# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0ng9xr6 -> base.schemastaging.athlete_salary.team -> Houston Rockets\n# Answer:\nHouston Rockets", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0j2mvjf -> base.schemastaging.athlete_salary.team -> New York Knicks\n# Answer:\nNew York Knicks", "# Reasoning Path:\nJeremy Lin -> sports.pro_athlete.teams -> m.0k6s01p -> sports.sports_team_roster.team -> New York Knicks\n# Answer:\nNew York Knicks", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0ng9xr6 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0_qrbd1 -> base.schemastaging.athlete_salary.team -> Houston Rockets\n# Answer:\nHouston Rockets", "# Reasoning Path:\nJeremy Lin -> sports.pro_athlete.teams -> m.01145k7g -> sports.sports_team_roster.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0j2mvjf -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nJeremy Lin -> sports.pro_athlete.teams -> m.0k6s01p -> sports.sports_team_roster.position -> Point guard\n# Answer:\nPoint guard", "# Reasoning Path:\nJeremy Lin -> sports.pro_athlete.teams -> m.0j2dfqc -> sports.sports_team_roster.team -> Houston Rockets\n# Answer:\nHouston Rockets", "# Reasoning Path:\nJeremy Lin -> people.person.education -> m.0dw8bzt -> education.education.degree -> Bachelor's degree\n# Answer:\nBachelor's degree"], "ground_truth": ["Houston Rockets"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-134", "prediction": ["# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Evil Under the Sun -> film.film.genre -> Thriller\n# Answer:\nEvil Under the Sun", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Evil Under the Sun -> book.written_work.subjects -> England\n# Answer:\nEvil Under the Sun", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Evil Under the Sun -> film.film.genre -> Crime Fiction\n# Answer:\nEvil Under the Sun", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Evil Under the Sun -> film.film.genre -> Crime Thriller\n# Answer:\nEvil Under the Sun", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Evil Under the Sun -> film.film.language -> English Language\n# Answer:\nEvil Under the Sun", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Agatha Christie's Miss Marple: 4:50 from Paddington -> film.film.genre -> Mystery\n# Answer:\nAgatha Christie's Miss Marple: 4:50 from Paddington", "# Reasoning Path:\nAgatha Christie -> book.book_subject.works -> The Getaway Guide to Agatha Christie's England -> book.written_work.subjects -> England\n# Answer:\nThe Getaway Guide to Agatha Christie's England", "# Reasoning Path:\nAgatha Christie -> common.topic.webpage -> m.03lzz8x -> common.webpage.resource -> m.0bl41lc\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> A Caribbean Mystery -> film.film.genre -> Mystery\n# Answer:\nA Caribbean Mystery", "# Reasoning Path:\nAgatha Christie -> book.book_subject.works -> Agatha Christie and the Eleven Missing Days -> book.book.editions -> Agatha Christie and the eleven missing days\n# Answer:\nAgatha Christie and the Eleven Missing Days"], "ground_truth": ["Poet", "Novelist", "Writer", "Playwright", "Screenwriter"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1340", "prediction": ["# Reasoning Path:\nSteve Nash -> people.person.nationality -> United Kingdom -> location.country.languages_spoken -> English Language\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nSteve Nash -> people.person.place_of_birth -> Johannesburg -> base.biblioness.bibs_location.country -> South Africa\n# Answer:\nJohannesburg", "# Reasoning Path:\nSteve Nash -> people.person.nationality -> United Kingdom -> location.country.official_language -> English Language\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nSteve Nash -> people.person.nationality -> Canada -> location.country.languages_spoken -> English Language\n# Answer:\nCanada", "# Reasoning Path:\nSteve Nash -> people.person.nationality -> United Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nSteve Nash -> people.person.nationality -> Canada -> common.topic.notable_types -> Country\n# Answer:\nCanada", "# Reasoning Path:\nSteve Nash -> people.person.place_of_birth -> Johannesburg -> location.location.contains -> 120 End Street\n# Answer:\nJohannesburg", "# Reasoning Path:\nSteve Nash -> people.person.nationality -> Canada -> location.country.official_language -> English Language\n# Answer:\nCanada", "# Reasoning Path:\nSteve Nash -> tv.tv_actor.guest_roles -> m.0g976rf -> tv.tv_guest_role.episodes_appeared_in -> A Short Story and a Tall Tale\n# Answer:\nA Short Story and a Tall Tale", "# Reasoning Path:\nSteve Nash -> people.person.place_of_birth -> Johannesburg -> location.location.contains -> 2055\n# Answer:\nJohannesburg"], "ground_truth": ["United Kingdom", "Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1341", "prediction": ["# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.010f42tf -> government.government_position_held.office_holder -> Jim Cawley\n# Answer:\nJim Cawley", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.011crdxd -> government.government_position_held.office_holder -> Rosemary Brown\n# Answer:\nRosemary Brown", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.048157l -> government.government_position_held.office_holder -> Catherine Baker Knoll\n# Answer:\nCatherine Baker Knoll", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.title -> Director, Office of Health Care Reform\n# Answer:\nDirector, Office of Health Care Reform", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.010f42tf -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nBasic title", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.person -> Rosemarie B. Greco\n# Answer:\nRosemarie B. Greco", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.010f42tf -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nFrom", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.011crdxd -> freebase.valuenotation.has_no_value -> Appointed By (if Position is Appointed)\n# Answer:\nAppointed By (if Position is Appointed)", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.048157l -> government.government_position_held.office_position_or_title -> Lieutenant Governor of Pennsylvania\n# Answer:\nLieutenant Governor of Pennsylvania", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.010f42tf -> freebase.valuenotation.is_reviewed -> Jurisdiction of office\n# Answer:\nJurisdiction of office"], "ground_truth": ["Richard Brodhead", "James Buchanan", "James J. Davis", "Francis J. Myers", "Harris Wofford", "Richard Schweiker", "William Wilkins", "Joseph F. Guffey", "Abner Lacock", "Matthew Quay", "Bob Casey, Jr.", "Joseph S. Clark, Jr.", "James Ross", "Pat Toomey", "Robert Morris", "Daniel Sturgeon", "Boies Penrose", "William Findlay", "Rick Santorum", "Hugh Scott", "William Bigler", "Joseph R. Grundy", "Edward Martin", "Walter Lowrie", "Samuel McKean", "Simon Cameron", "George W. Pepper", "Arlen Specter", "Isaac D. Barnard", "James H. Duff", "Samuel Maclay", "Albert Gallatin", "George M. Dallas", "Peter Muhlenberg", "William E. Crow", "William Bingham", "George T. Oliver", "William Marks", "George Logan", "Philander C. Knox", "William Scott Vare", "David A. Reed", "Andrew Gregg", "Michael Leib", "Jonathan Roberts", "H. John Heinz III", "William Maclay"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1342", "prediction": ["# Reasoning Path:\nFrance -> location.country.official_language -> French -> language.human_language.language_family -> Indo-European languages\n# Answer:\nFrench", "# Reasoning Path:\nFrance -> location.country.official_language -> French -> language.human_language.language_family -> Romance languages\n# Answer:\nFrench", "# Reasoning Path:\nFrance -> location.country.official_language -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nFrance -> location.country.languages_spoken -> Yeniche Language -> language.human_language.main_country -> Germany\n# Answer:\nYeniche Language", "# Reasoning Path:\nFrance -> location.country.languages_spoken -> French -> language.human_language.language_family -> Romance languages\n# Answer:\nFrench", "# Reasoning Path:\nFrance -> location.country.languages_spoken -> Yeniche Language -> language.human_language.region -> Europe\n# Answer:\nYeniche Language", "# Reasoning Path:\nFrance -> location.country.languages_spoken -> Occitan language -> language.human_language.countries_spoken_in -> Monaco\n# Answer:\nOccitan language", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.11b60tv_69\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nFrance -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> Switzerland\n# Answer:\nFrench", "# Reasoning Path:\nFrance -> location.country.languages_spoken -> Yeniche Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nYeniche Language"], "ground_truth": ["French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1343", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.02wp75f -> education.education.degree -> PhD\n# Answer:\nPhD", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0n_xlq0 -> education.education.degree -> Secondary education\n# Answer:\nSecondary education", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.04hddst -> education.education.degree -> Bachelor of Arts\n# Answer:\nBachelor of Arts", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.02wp75f -> education.education.institution -> Boston University\n# Answer:\nBoston University", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0n_xlq0 -> freebase.valuenotation.has_no_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.04hddst -> education.education.major_field_of_study -> Sociology\n# Answer:\nSociology", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0n_xlq0 -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nAlpha Man: The Brotherhood of MLK", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0n_xlq0 -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nMinor", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> common.topic.article -> m.05nnh0\n# Answer:\nDexter Avenue Baptist Church"], "ground_truth": ["PhD", "Bachelor of Arts", "Secondary education", "Bachelor of Divinity"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.4285714285714285, "ans_precission": 0.3, "ans_recall": 0.75, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4285714285714285, "path_ans_precision": 0.3, "path_ans_recall": 0.75}
{"id": "WebQTest-1344", "prediction": ["# Reasoning Path:\nSt. Louis -> location.location.containedby -> Missouri -> location.location.containedby -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nSt. Louis -> location.location.containedby -> Missouri -> base.aareas.schema.administrative_area.administrative_children -> Franklin County\n# Answer:\nMissouri", "# Reasoning Path:\nSt. Louis -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nSt. Louis -> location.location.containedby -> Missouri -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nSt. Louis -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nSt. Louis -> location.location.containedby -> Area code 314 -> location.location.containedby -> Missouri\n# Answer:\nArea code 314", "# Reasoning Path:\nSt. Louis -> location.place_with_neighborhoods.neighborhoods -> Academy -> location.location.containedby -> St. Louis  MO-IL, Metropolitan Statistical Area\n# Answer:\nAcademy", "# Reasoning Path:\nSt. Louis -> location.place_with_neighborhoods.neighborhoods -> Baden -> location.location.containedby -> St. Louis  MO-IL, Metropolitan Statistical Area\n# Answer:\nBaden", "# Reasoning Path:\nSt. Louis -> location.location.containedby -> Area code 314 -> common.topic.notable_for -> g.125bvnkp4\n# Answer:\nArea code 314", "# Reasoning Path:\nSt. Louis -> location.place_with_neighborhoods.neighborhoods -> Benton Park -> location.location.containedby -> St. Louis  MO-IL, Metropolitan Statistical Area\n# Answer:\nBenton Park"], "ground_truth": ["Missouri"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1348", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.drafted_athlete.drafted -> m.04vb3pp -> sports.sports_league_draft_pick.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.drafted_athlete.drafted -> m.04vb3pp -> sports.sports_league_draft_pick.draft -> 1998 NFL draft\n# Answer:\n1998 NFL draft", "# Reasoning Path:\nPeyton Manning -> sports.drafted_athlete.drafted -> m.04vb3pp -> sports.sports_league_draft_pick.school -> University of Tennessee\n# Answer:\nUniversity of Tennessee", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nTeam", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nFrom", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback"], "ground_truth": ["New Orleans Saints", "Minnesota Vikings", "Ole Miss Rebels football", "Houston Oilers"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1350", "prediction": ["# Reasoning Path:\nJesse Owens -> olympics.olympic_athlete.medals_won -> m.04hdrqm -> olympics.olympic_medal_honor.event -> Athletics at the 1936 Summer Olympics \u2013 Men's long jump\n# Answer:\nAthletics at the 1936 Summer Olympics \u2013 Men's long jump", "# Reasoning Path:\nJesse Owens -> olympics.olympic_athlete.medals_won -> m.04hdrr2 -> olympics.olympic_medal_honor.event -> Athletics at the 1936 Summer Olympics \u2013 Men's 200 metres\n# Answer:\nAthletics at the 1936 Summer Olympics \u2013 Men's 200 metres", "# Reasoning Path:\nJesse Owens -> olympics.olympic_athlete.medals_won -> m.04hdrrk -> olympics.olympic_medal_honor.event -> Athletics at the 1936 Summer Olympics \u2013 Men's 4 \u00d7 100 metres relay\n# Answer:\nAthletics at the 1936 Summer Olympics \u2013 Men's 4 \u00d7 100 metres relay", "# Reasoning Path:\nJesse Owens -> olympics.olympic_athlete.medals_won -> m.04hdrqm -> olympics.olympic_medal_honor.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJesse Owens -> people.person.places_lived -> m.03pfl8z -> people.place_lived.location -> Cleveland\n# Answer:\nCleveland", "# Reasoning Path:\nJesse Owens -> olympics.olympic_athlete.medals_won -> m.04hdrr2 -> olympics.olympic_medal_honor.medal -> Gold medal\n# Answer:\nGold medal", "# Reasoning Path:\nJesse Owens -> film.person_or_entity_appearing_in_film.films -> m.046qw5v -> film.personal_film_appearance.type_of_appearance -> Subject of film\n# Answer:\nSubject of film", "# Reasoning Path:\nJesse Owens -> people.person.places_lived -> m.0wk2xk3 -> people.place_lived.location -> Oakville, Alabama\n# Answer:\nOakville, Alabama", "# Reasoning Path:\nJesse Owens -> olympics.olympic_athlete.medals_won -> m.04hdrrk -> olympics.olympic_medal_honor.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJesse Owens -> olympics.olympic_athlete.medals_won -> m.04hdrrk -> olympics.olympic_medal_honor.medalist -> Foy Draper\n# Answer:\nFoy Draper"], "ground_truth": ["Associated Press Male Athlete of the Year"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1351", "prediction": ["# Reasoning Path:\nPeru -> location.country.currency_used -> Peruvian nuevo sol -> common.topic.notable_types -> Currency\n# Answer:\nPeruvian nuevo sol", "# Reasoning Path:\nPeru -> location.country.currency_used -> Peruvian nuevo sol -> common.topic.notable_for -> g.125cdv3sb\n# Answer:\nPeruvian nuevo sol", "# Reasoning Path:\nPeru -> location.country.currency_used -> Peruvian nuevo sol -> common.topic.article -> m.0b4243\n# Answer:\nPeruvian nuevo sol", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.11b71n53cl\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nPeru -> location.statistical_region.energy_use_per_capita -> g.1245_8rt3\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.1hhc37ctc\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.1hhc38qlv\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nPeru -> location.statistical_region.energy_use_per_capita -> g.1245_9gm0\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nPeru -> location.statistical_region.energy_use_per_capita -> g.1245__jr1\n# Answer:\nlocation.statistical_region.energy_use_per_capita"], "ground_truth": ["Peruvian nuevo sol"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-1353", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Tyrone\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> base.aareas.schema.administrative_area.administrative_children -> East Midlands\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Scotland -> base.aareas.schema.administrative_area.administrative_children -> Aberdeen\n# Answer:\nScotland", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nChild pornography", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> base.aareas.schema.administrative_area.administrative_children -> East of England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> location.country.first_level_divisions -> East Midlands\n# Answer:\nEngland"], "ground_truth": ["England", "Northern Ireland", "Scotland", "Wales"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.6, "path_precision": 0.5, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-1354", "prediction": ["# Reasoning Path:\nJudy Garland -> people.person.parents -> Francis Avent Gumm -> people.person.spouse_s -> m.0w7r8xb\n# Answer:\nFrancis Avent Gumm", "# Reasoning Path:\nJudy Garland -> people.person.parents -> Francis Avent Gumm -> people.person.children -> Mary Jane Gumm\n# Answer:\nFrancis Avent Gumm", "# Reasoning Path:\nJudy Garland -> people.person.parents -> Francis Avent Gumm -> people.person.place_of_birth -> Tennessee\n# Answer:\nFrancis Avent Gumm", "# Reasoning Path:\nJudy Garland -> people.person.parents -> Francis Avent Gumm -> people.person.children -> Dorothy Virginia Gumm\n# Answer:\nFrancis Avent Gumm", "# Reasoning Path:\nJudy Garland -> people.person.parents -> Ethel Marion Milne -> people.person.spouse_s -> m.0w7r8xb\n# Answer:\nEthel Marion Milne", "# Reasoning Path:\nJudy Garland -> people.person.parents -> Ethel Marion Milne -> people.person.children -> Mary Jane Gumm\n# Answer:\nEthel Marion Milne", "# Reasoning Path:\nJudy Garland -> people.person.parents -> Ethel Marion Milne -> common.topic.notable_types -> Deceased Person\n# Answer:\nEthel Marion Milne", "# Reasoning Path:\nJudy Garland -> people.person.parents -> Ethel Marion Milne -> people.person.children -> Dorothy Virginia Gumm\n# Answer:\nEthel Marion Milne", "# Reasoning Path:\nJudy Garland -> film.actor.film -> m.012zh74p -> film.performance.film -> Bubbles\n# Answer:\nBubbles", "# Reasoning Path:\nJudy Garland -> film.actor.film -> m.01z0l40 -> film.performance.film -> Love Finds Andy Hardy\n# Answer:\nLove Finds Andy Hardy"], "ground_truth": ["Francis Avent Gumm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1355", "prediction": ["# Reasoning Path:\nRobert F. Kennedy -> people.person.education -> m.04yst0l -> education.education.institution -> University of Virginia\n# Answer:\nUniversity of Virginia", "# Reasoning Path:\nRobert F. Kennedy -> people.person.education -> m.04hddhr -> education.education.institution -> Bates College\n# Answer:\nBates College", "# Reasoning Path:\nRobert F. Kennedy -> people.person.education -> m.02wp24k -> education.education.institution -> Harvard University\n# Answer:\nHarvard University", "# Reasoning Path:\nRobert F. Kennedy -> people.family_member.family -> Kennedy family -> people.family.members -> Anthony Shriver\n# Answer:\nKennedy family", "# Reasoning Path:\nRobert F. Kennedy -> people.family_member.family -> Kennedy family -> common.topic.notable_for -> g.12573hgz4\n# Answer:\nKennedy family", "# Reasoning Path:\nRobert F. Kennedy -> people.family_member.family -> Kennedy family -> people.family.members -> Bobby Shriver\n# Answer:\nKennedy family", "# Reasoning Path:\nRobert F. Kennedy -> people.family_member.family -> Kennedy family -> common.topic.image -> Kennedy Arms\n# Answer:\nKennedy family", "# Reasoning Path:\nRobert F. Kennedy -> people.family_member.family -> Kennedy family -> people.family.members -> Caroline Kennedy\n# Answer:\nKennedy family", "# Reasoning Path:\nRobert F. Kennedy -> film.film_character.portrayed_in_films -> m.03jr73t -> film.performance.film -> Thirteen Days\n# Answer:\nThirteen Days", "# Reasoning Path:\nRobert F. Kennedy -> people.family_member.family -> Kennedy family -> common.topic.image -> Kennedy bros\n# Answer:\nKennedy family"], "ground_truth": ["Harvard University", "University of Virginia", "Milton Academy", "Bates College", "University of Virginia School of Law", "Harvard College"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.37499999999999994, "ans_precission": 0.3, "ans_recall": 0.5, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.37499999999999994, "path_ans_precision": 0.3, "path_ans_recall": 0.5}
{"id": "WebQTest-1356", "prediction": ["# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.cause_of_death -> Barbiturate overdose -> common.topic.article -> m.0bwj2gk\n# Answer:\nBarbiturate overdose", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.cause_of_death -> Barbiturate overdose -> people.cause_of_death.people -> Stephen Ward\n# Answer:\nBarbiturate overdose", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.cause_of_death -> Barbiturate overdose -> common.topic.notable_for -> g.125cvd366\n# Answer:\nBarbiturate overdose", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.cause_of_death -> Barbiturate overdose -> people.cause_of_death.people -> Chester Morris\n# Answer:\nBarbiturate overdose", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.07zn09y -> award.award_nomination.ceremony -> 9th British Academy Film Awards\n# Answer:\n9th British Academy Film Awards", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.cause_of_death -> Barbiturate overdose -> people.cause_of_death.people -> Cindy James\n# Answer:\nBarbiturate overdose", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.07zn0h0 -> award.award_nomination.award -> BAFTA Award for Best Foreign Actress\n# Answer:\nBAFTA Award for Best Foreign Actress", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.07zn09y -> award.award_nomination.award -> BAFTA Award for Best Foreign Actress\n# Answer:\nBAFTA Award for Best Foreign Actress", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.0909kmj -> award.award_nomination.award -> Golden Globe Award for Best Actress \u2013 Motion Picture \u2013 Musical or Comedy\n# Answer:\nGolden Globe Award for Best Actress \u2013 Motion Picture \u2013 Musical or Comedy", "# Reasoning Path:\nMarilyn Monroe -> common.topic.webpage -> m.03l7lnt -> common.webpage.resource -> m.0bk9tgl\n# Answer:\ncommon.webpage.resource"], "ground_truth": ["Barbiturate overdose"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1357", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.location.containedby -> Asia\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> Fukushima Prefecture\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> Japan\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> common.topic.image -> Okuma town office\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 8 -> common.topic.notable_for -> g.1259_1vs2\n# Answer:\nFukushima I \u2013 8", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Fukushima Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> T\u014dhoku region\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 8 -> base.infrastructure.nuclear_power_plant.reactor_type -> Advanced boiling water reactor\n# Answer:\nFukushima I \u2013 8", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_for -> g.125btmfy0\n# Answer:\nFukushima I \u2013 1"], "ground_truth": ["Okuma", "Japan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1358", "prediction": ["# Reasoning Path:\nLos Angeles -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nPacific Time Zone", "# Reasoning Path:\nLos Angeles -> base.popstra.location.vacationers -> m.065pdpt -> base.popstra.vacation_choice.vacationer -> Mark Wahlberg\n# Answer:\nMark Wahlberg", "# Reasoning Path:\nLos Angeles -> base.popstra.location.vacationers -> m.064c8fd -> base.popstra.vacation_choice.vacationer -> David Schwimmer\n# Answer:\nDavid Schwimmer", "# Reasoning Path:\nLos Angeles -> base.popstra.location.vacationers -> m.064sn_4 -> base.popstra.vacation_choice.vacationer -> Hilary Duff\n# Answer:\nHilary Duff", "# Reasoning Path:\nLos Angeles -> periodicals.newspaper_circulation_area.newspapers -> Urdu Times -> book.newspaper.circulation_areas -> Washington, D.C.\n# Answer:\nUrdu Times", "# Reasoning Path:\nLos Angeles -> periodicals.newspaper_circulation_area.newspapers -> Urdu Times -> common.topic.notable_for -> g.1259fp6h1\n# Answer:\nUrdu Times", "# Reasoning Path:\nLos Angeles -> periodicals.newspaper_circulation_area.newspapers -> Urdu Times -> book.periodical.first_issue_date -> m.0j0fcm4\n# Answer:\nUrdu Times", "# Reasoning Path:\nLos Angeles -> periodicals.newspaper_circulation_area.newspapers -> The Tidings -> book.periodical.subjects -> Catholicism\n# Answer:\nThe Tidings", "# Reasoning Path:\nLos Angeles -> periodicals.newspaper_circulation_area.newspapers -> Urdu Times -> book.newspaper.circulation_areas -> Atlanta\n# Answer:\nUrdu Times", "# Reasoning Path:\nLos Angeles -> periodicals.newspaper_circulation_area.newspapers -> The Tidings -> common.topic.notable_types -> Newspaper\n# Answer:\nThe Tidings"], "ground_truth": ["Pacific Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1359", "prediction": ["# Reasoning Path:\nNicolas Cage -> people.person.children -> Kal-El Coppola Cage -> people.person.parents -> Alice Kim\n# Answer:\nKal-El Coppola Cage", "# Reasoning Path:\nNicolas Cage -> people.person.children -> Kal-El Coppola Cage -> people.person.place_of_birth -> New York City\n# Answer:\nKal-El Coppola Cage", "# Reasoning Path:\nNicolas Cage -> people.person.children -> Weston Coppola Cage -> people.person.children -> Lucian Augustus Coppola Cage\n# Answer:\nWeston Coppola Cage", "# Reasoning Path:\nNicolas Cage -> people.person.children -> Kal-El Coppola Cage -> people.person.sibling_s -> m.0pcd733\n# Answer:\nKal-El Coppola Cage", "# Reasoning Path:\nNicolas Cage -> celebrities.celebrity.sexual_orientation -> m.05n7hz_ -> celebrities.sexual_orientation_phase.sexual_orientation -> Heterosexuality\n# Answer:\nHeterosexuality", "# Reasoning Path:\nNicolas Cage -> base.saturdaynightlive.snl_host.episodes_hosted -> SNL - 18.1 -> common.topic.notable_for -> g.1259fc15z\n# Answer:\nSNL - 18.1", "# Reasoning Path:\nNicolas Cage -> people.person.children -> Weston Coppola Cage -> people.person.profession -> Singer\n# Answer:\nWeston Coppola Cage", "# Reasoning Path:\nNicolas Cage -> base.saturdaynightlive.snl_host.episodes_hosted -> SNL - 18.1 -> base.saturdaynightlive.snl_episode.musical_guest -> Bobby Brown\n# Answer:\nSNL - 18.1", "# Reasoning Path:\nNicolas Cage -> people.person.children -> Weston Coppola Cage -> film.actor.film -> m.011dvr25\n# Answer:\nWeston Coppola Cage", "# Reasoning Path:\nNicolas Cage -> base.saturdaynightlive.snl_host.episodes_hosted -> SNL - 18.1 -> base.saturdaynightlive.snl_episode.musical_guest_performance -> m.04p50g3\n# Answer:\nSNL - 18.1"], "ground_truth": ["Kal-El Coppola Cage", "Weston Coppola Cage"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1361", "prediction": ["# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_0s31\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.11b60v4rn6\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_38p7\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_569j\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.12tb6hbx9\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.1hhc378k5\n# Answer:\nlocation.statistical_region.cpi_inflation_rate"], "ground_truth": ["Mexican peso"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1362", "prediction": ["# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0hz6vzk -> government.government_position_held.appointed_by -> Rick Scott\n# Answer:\nRick Scott", "# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0hz6vzk -> government.government_position_held.office_holder -> Kurt S. Browning\n# Answer:\nKurt S. Browning", "# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.04vb558 -> government.government_position_held.office_holder -> Charlie Crist\n# Answer:\nCharlie Crist", "# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0115c969 -> government.government_position_held.office_holder -> Jan Fortune\n# Answer:\nJan Fortune", "# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0hz6vzk -> government.government_position_held.basic_title -> Secretary of state\n# Answer:\nSecretary of state", "# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0115c969 -> government.government_position_held.basic_title -> State Representative\n# Answer:\nState Representative", "# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0115c969 -> freebase.valuenotation.has_no_value -> Appointed By (if Position is Appointed)\n# Answer:\nAppointed By (if Position is Appointed)", "# Reasoning Path:\nFlorida -> location.location.partiallycontains -> m.0wjpmn5 -> location.partial_containment_relationship.partially_contains -> Blackwater River\n# Answer:\nBlackwater River", "# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.04vb558 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nFlorida -> base.aareas.schema.administrative_area.administrative_children -> Baker County -> common.topic.notable_types -> US County\n# Answer:\nBaker County"], "ground_truth": ["Rick Scott"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1363", "prediction": ["# Reasoning Path:\nJessica Simpson -> people.person.spouse_s -> m.010gz8v_ -> people.marriage.spouse -> Eric Johnson\n# Answer:\nEric Johnson", "# Reasoning Path:\nJessica Simpson -> celebrities.celebrity.sexual_relationships -> m.010gz96f -> celebrities.romantic_relationship.celebrity -> Eric Johnson\n# Answer:\nEric Johnson", "# Reasoning Path:\nJessica Simpson -> people.person.spouse_s -> m.0113nnph -> people.marriage.spouse -> Eric Johnson\n# Answer:\nEric Johnson", "# Reasoning Path:\nJessica Simpson -> people.person.spouse_s -> m.07sl20z -> people.marriage.spouse -> Nick Lachey\n# Answer:\nNick Lachey", "# Reasoning Path:\nJessica Simpson -> people.person.spouse_s -> m.010gz8v_ -> freebase.valuenotation.has_no_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nJessica Simpson -> celebrities.celebrity.sexual_relationships -> m.010gz96f -> freebase.valuenotation.has_no_value -> End date\n# Answer:\nEnd date", "# Reasoning Path:\nJessica Simpson -> celebrities.celebrity.sexual_relationships -> m.010gz93y -> celebrities.romantic_relationship.celebrity -> Tony Romo\n# Answer:\nTony Romo", "# Reasoning Path:\nJessica Simpson -> people.person.spouse_s -> m.010gz8v_ -> people.marriage.type_of_union -> Domestic partnership\n# Answer:\nDomestic partnership", "# Reasoning Path:\nJessica Simpson -> celebrities.celebrity.sexual_relationships -> m.010gz96f -> celebrities.romantic_relationship.relationship_type -> Engaged to\n# Answer:\nEngaged to", "# Reasoning Path:\nJessica Simpson -> celebrities.celebrity.sexual_relationships -> m.010gz95y -> celebrities.romantic_relationship.celebrity -> Eric Johnson\n# Answer:\nEric Johnson"], "ground_truth": ["Nick Lachey", "Eric Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.47619047619047616, "path_precision": 0.5, "path_recall": 0.45454545454545453, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1364", "prediction": ["# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.main_country -> United Kingdom\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish English -> base.schemastaging.context_name.pronunciation -> g.125_nyqwz\n# Answer:\nScottish English", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scots Language -> common.topic.notable_types -> Human Language\n# Answer:\nScots Language", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.countries_spoken_in -> Kingdom of Great Britain\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scots Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nScots Language", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish English -> common.topic.image -> King David Book of Psalms from the reign of James VI\n# Answer:\nScottish English", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scots Language -> language.human_language.language_family -> Anglo-Frisian languages\n# Answer:\nScots Language", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish English -> common.topic.image -> Outwith\n# Answer:\nScottish English"], "ground_truth": ["English Language"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1365", "prediction": ["# Reasoning Path:\nJohn Dalton -> people.person.profession -> Chemist -> base.descriptive_names.names.descriptive_name -> m.01066g6n\n# Answer:\nChemist", "# Reasoning Path:\nJohn Dalton -> people.person.profession -> Chemist -> common.topic.article -> m.01pxq\n# Answer:\nChemist", "# Reasoning Path:\nJohn Dalton -> people.person.profession -> Chemist -> base.schemastaging.context_name.pronunciation -> g.125_m09_2\n# Answer:\nChemist", "# Reasoning Path:\nJohn Dalton -> people.person.profession -> Chemist -> base.descriptive_names.names.descriptive_name -> m.0106_ymb\n# Answer:\nChemist", "# Reasoning Path:\nJohn Dalton -> people.person.profession -> Chemist -> base.descriptive_names.names.descriptive_name -> m.010f2qwv\n# Answer:\nChemist", "# Reasoning Path:\nJohn Dalton -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Werner Heisenberg\n# Answer:\nPhysicist", "# Reasoning Path:\nJohn Dalton -> base.kwebbase.kwtopic.has_sentences -> After 1794, Dalton read the results of all his research to the Philosophical Society. -> base.kwebbase.kwsentence.dates -> m.0c1622q\n# Answer:\nAfter 1794, Dalton read the results of all his research to the Philosophical Society.", "# Reasoning Path:\nJohn Dalton -> base.kwebbase.kwtopic.connections_to -> abraham darby benefactor of john dalton -> base.kwebbase.kwconnection.relation -> benefactor of\n# Answer:\nabraham darby benefactor of john dalton", "# Reasoning Path:\nJohn Dalton -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Daniel Faraday\n# Answer:\nPhysicist", "# Reasoning Path:\nJohn Dalton -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.0101ddrk\n# Answer:\nPhysicist"], "ground_truth": ["Physicist", "Chemist"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1366", "prediction": ["# Reasoning Path:\nZac Brown Band -> music.musical_group.member -> m.011jzqwn -> music.group_membership.member -> Matt Mangano\n# Answer:\nMatt Mangano", "# Reasoning Path:\nZac Brown Band -> music.musical_group.member -> m.04d3bqx -> music.group_membership.member -> Zac Brown\n# Answer:\nZac Brown", "# Reasoning Path:\nZac Brown Band -> music.musical_group.member -> m.0h3f8pf -> music.group_membership.member -> Coy Bowles\n# Answer:\nCoy Bowles", "# Reasoning Path:\nZac Brown Band -> music.musical_group.member -> m.011jzqwn -> freebase.valuenotation.has_no_value -> Period (end)\n# Answer:\nPeriod (end)", "# Reasoning Path:\nZac Brown Band -> common.topic.webpage -> m.0460qbn -> common.webpage.resource -> m.0bjwcfr\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nZac Brown Band -> music.musical_group.member -> m.011jzqwn -> music.group_membership.role -> Bass guitar\n# Answer:\nBass guitar", "# Reasoning Path:\nZac Brown Band -> common.topic.webpage -> m.0460qbn -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nZac Brown Band -> common.topic.webpage -> m.0d5lkcw -> common.webpage.in_index -> Blissful Master Index\n# Answer:\nBlissful Master Index", "# Reasoning Path:\nZac Brown Band -> music.musical_group.member -> m.04d3bqx -> music.group_membership.role -> Lead Vocals\n# Answer:\nLead Vocals", "# Reasoning Path:\nZac Brown Band -> music.musical_group.member -> m.0h3f8pf -> music.group_membership.role -> Guitar\n# Answer:\nGuitar"], "ground_truth": ["Island Song", "Toes", "Valentines", "Free / Into the Mystic", "Mango Tree", "Bad Moon Rising", "I Shall Be Released (live)", "Smoke Rise", "Make This Day", "Let It Rain", "Intro", "It's Not Okay", "Bar", "The Night They Drove Old Dixie Down", "One Day", "Knee Deep (Feat. Jimmy Buffett)", "Forever and Ever, Amen", "Keep Me in Mind", "Violin Intro to Free", "Human", "Junkyard", "Tomorrow Never Comes", "All Alright", "Oh My Sweet Carolina (live)", "Highway 20 Ride", "Whatever It Is", "Free", "Day for the Dead", "The Night They Drove Old Dixie Down (live)", "Alabama Jubilee", "Bittersweet", "Sic 'em on the Chicken", "Overnight (Feat. Trombone Shorty)", "These Days", "Settle Me Down", "I Lost It (live)", "Lance's Song", "Tomorrow Never Comes (acoustic version)", "Trying to Drive", "A Different Kind of Fine", "Different Kind of Fine", "Uncaged", "It's Not OK", "Whiskey's Gone", "Jolene", "Keep Me In Mind", "Let It Go (Live In Atlanta)", "Martin", "Goodbye In Her Eyes", "Harmony", "Nothing", "I Play the Road", "Where the Boat Leaves From", "I Shall Be Released", "Castaway", "I'll Be Your Man (Song For a Daughter)", "Where the Boat Leaves From / One Love", "Not OK", "No Hurry", "Chicken Fried", "Beautiful Drug", "Black Water", "Natural Disaster", "Day That I Die (Feat. Amos Lee)", "Dress Blues", "On This Train", "As She's Walking Away", "Homegrown", "Martin (Live In Atlanta)", "Cold Hearted", "Remedy", "Chicken Fried (Full Version)", "Last But Not Least", "The Devil Went Down to Georgia", "Heather", "America the Beautiful", "Quiet Your Mind", "DJ", "Let It Go", "Jump Right In", "Blackbird", "Tax Man Shoes", "The Wind", "Loving You Easy", "Sic 'em on a Chicken", "Colder Weather", "We're Gonna Make This Day", "Trouble", "Every Little Bit", "Can't You See", "Day That I Die", "Wildfire", "The Muse", "Sweet Annie", "Who Knows", "Better Day", "Mary", "Young and Wild", "Every Little Bit (live)", "Heavy Is the Head", "Curse Me"], "ans_acc": 0.009900990099009901, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.018018018018018018, "path_ans_precision": 0.1, "path_ans_recall": 0.009900990099009901}
{"id": "WebQTest-1367", "prediction": ["# Reasoning Path:\nGlastonbury -> location.location.containedby -> United Kingdom -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nGlastonbury -> location.location.containedby -> Mendip -> location.location.containedby -> England\n# Answer:\nMendip", "# Reasoning Path:\nGlastonbury -> location.location.containedby -> United Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nGlastonbury -> location.location.containedby -> United Kingdom -> location.country.first_level_divisions -> England\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nGlastonbury -> location.location.containedby -> Mendip -> location.location.containedby -> South West England\n# Answer:\nMendip", "# Reasoning Path:\nGlastonbury -> travel.travel_destination.tourist_attractions -> The Tribunal, Glastonbury -> location.location.containedby -> United Kingdom\n# Answer:\nThe Tribunal, Glastonbury", "# Reasoning Path:\nGlastonbury -> location.location.containedby -> Mendip -> location.location.containedby -> Somerset\n# Answer:\nMendip", "# Reasoning Path:\nGlastonbury -> location.location.containedby -> Mendip -> base.aareas.schema.administrative_area.administrative_children -> Mells\n# Answer:\nMendip", "# Reasoning Path:\nGlastonbury -> location.location.containedby -> Mendip -> location.location.contains -> Ashwick\n# Answer:\nMendip", "# Reasoning Path:\nGlastonbury -> location.location.containedby -> Mendip -> base.aareas.schema.administrative_area.administrative_children -> West Bradley\n# Answer:\nMendip"], "ground_truth": ["United Kingdom", "Mendip"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1368", "prediction": ["# Reasoning Path:\nAmerican Federation of Labor -> organization.organization.founders -> Samuel Gompers -> people.person.profession -> Labor Union Leader\n# Answer:\nSamuel Gompers", "# Reasoning Path:\nAmerican Federation of Labor -> common.topic.article -> m.01z4fw\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nAmerican Federation of Labor -> organization.organization.founders -> Samuel Gompers -> organization.organization_founder.organizations_founded -> Federation of Organized Trades and Labor Unions\n# Answer:\nSamuel Gompers", "# Reasoning Path:\nAmerican Federation of Labor -> organization.organization.founders -> Samuel Gompers -> organization.organization_founder.organizations_founded -> Cigar Makers' International Union\n# Answer:\nSamuel Gompers", "# Reasoning Path:\nAmerican Federation of Labor -> organization.organization.founders -> Samuel Gompers -> book.author.works_written -> America's fight for the preservation of democracy\n# Answer:\nSamuel Gompers", "# Reasoning Path:\nAmerican Federation of Labor -> organization.organization.founders -> Samuel Gompers -> book.author.works_written -> 70 Years of Life and Labor\n# Answer:\nSamuel Gompers", "# Reasoning Path:\nAmerican Federation of Labor -> common.topic.image -> AFL-label -> common.image.size -> m.04sn0nx\n# Answer:\nAFL-label", "# Reasoning Path:\nAmerican Federation of Labor -> organization.organization.founders -> Samuel Gompers -> book.author.works_written -> Editorial in the American Federationist\n# Answer:\nSamuel Gompers"], "ground_truth": ["Samuel Gompers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-1369", "prediction": ["# Reasoning Path:\nDenver -> travel.travel_destination.tourist_attractions -> Chamberlin Observatory -> architecture.structure.architectural_style -> Romanesque architecture\n# Answer:\nChamberlin Observatory", "# Reasoning Path:\nDenver -> travel.travel_destination.tourist_attractions -> Chamberlin Observatory -> location.location.geolocation -> m.02_dk4s\n# Answer:\nChamberlin Observatory", "# Reasoning Path:\nDenver -> location.statistical_region.population -> g.11b66hmvpg\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nDenver -> travel.travel_destination.tourist_attractions -> Westin Westminster -> common.topic.image -> The Westin Westminster\n# Answer:\nWestin Westminster", "# Reasoning Path:\nDenver -> travel.travel_destination.tourist_attractions -> Westin Westminster -> location.location.geolocation -> m.0wmnr3b\n# Answer:\nWestin Westminster", "# Reasoning Path:\nDenver -> travel.travel_destination.tourist_attractions -> Denver Mint -> common.topic.image -> The Denver Mint\n# Answer:\nDenver Mint", "# Reasoning Path:\nDenver -> location.statistical_region.population -> g.11b7tnnw1n\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nDenver -> travel.travel_destination.tourist_attractions -> Denver Mint -> common.topic.notable_types -> Tourist attraction\n# Answer:\nDenver Mint", "# Reasoning Path:\nDenver -> location.statistical_region.population -> g.11bcdlfyf4\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nDenver -> common.topic.webpage -> m.03lwrj6 -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website"], "ground_truth": ["Westin Westminster", "Frederic C. Hamilton Building", "Denver Mint", "Forney Transportation Museum", "Black American West Museum & Heritage Center", "Chamberlin Observatory", "North Building", "Molly Brown House", "Elitch Gardens", "Colorado State Capitol", "Children's Museum of Denver", "Festivus Film Festival", "Denver Museum of Nature and Science", "Museum of Contemporary Art Denver", "Denver Botanic Gardens", "Denver Firefighters Museum"], "ans_acc": 0.1875, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 0.6, "ans_recall": 0.1875, "path_f1": 0.2857142857142857, "path_precision": 0.6, "path_recall": 0.1875, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.6, "path_ans_recall": 0.1875}
{"id": "WebQTest-1370", "prediction": ["# Reasoning Path:\nThomas Hobbes -> people.person.places_lived -> m.0jvv0vn -> people.place_lived.location -> Paris\n# Answer:\nParis", "# Reasoning Path:\nThomas Hobbes -> people.person.place_of_birth -> Westport, Wiltshire -> common.topic.notable_types -> Location\n# Answer:\nWestport, Wiltshire", "# Reasoning Path:\nThomas Hobbes -> people.deceased_person.place_of_death -> Derbyshire -> location.administrative_division.country -> United Kingdom\n# Answer:\nDerbyshire", "# Reasoning Path:\nThomas Hobbes -> people.person.place_of_birth -> Westport, Wiltshire -> common.topic.notable_for -> g.1256l2xh5\n# Answer:\nWestport, Wiltshire", "# Reasoning Path:\nThomas Hobbes -> people.deceased_person.place_of_death -> Derbyshire -> location.administrative_division.second_level_division_of -> England\n# Answer:\nDerbyshire", "# Reasoning Path:\nThomas Hobbes -> people.deceased_person.place_of_death -> Derbyshire -> base.aareas.schema.administrative_area.administrative_parent -> East Midlands\n# Answer:\nDerbyshire"], "ground_truth": ["Paris"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 0.16666666666666666, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.16666666666666666, "path_recall": 0.25, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-1373", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_death -> Dallas -> travel.travel_destination.tourist_attractions -> Dealey Plaza\n# Answer:\nDallas", "# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_death -> Dallas -> location.location.containedby -> United States of America\n# Answer:\nDallas", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr. -> people.deceased_person.place_of_death -> Atlantic Ocean\n# Answer:\nJohn F. Kennedy Jr.", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> White people -> medicine.risk_factor.diseases -> Breast cancer\n# Answer:\nWhite people", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> time.event.locations -> Dealey Plaza\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> common.topic.notable_types -> Event\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr. -> base.popstra.celebrity.friendship -> m.0645k94\n# Answer:\nJohn F. Kennedy Jr.", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School -> common.topic.notable_for -> g.125583hnb\n# Answer:\nBloomington Kennedy High School", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nIrish American", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> time.event.locations -> Dallas\n# Answer:\nAssassination of John F. Kennedy"], "ground_truth": ["Dallas"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1374", "prediction": ["# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Japan\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Turkey\n# Answer:\nParliamentary republic", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Albania\n# Answer:\nParliamentary republic", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37cxr\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Austria\n# Answer:\nParliamentary republic"], "ground_truth": ["Unitary state", "Parliamentary republic", "Constitutional republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1376", "prediction": ["# Reasoning Path:\nMontreal -> location.location.containedby -> Qu\u00e9bec -> location.location.containedby -> Canada\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nMontreal -> location.location.containedby -> Qu\u00e9bec -> base.aareas.schema.administrative_area.administrative_parent -> Canada\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nMontreal -> location.location.containedby -> Canada -> location.location.containedby -> North America\n# Answer:\nCanada", "# Reasoning Path:\nMontreal -> base.biblioness.bibs_location.state -> Qu\u00e9bec -> location.location.containedby -> Canada\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nMontreal -> location.location.containedby -> Qu\u00e9bec -> location.administrative_division.country -> Canada\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nMontreal -> location.location.containedby -> Canada -> base.aareas.schema.administrative_area.administrative_children -> Qu\u00e9bec\n# Answer:\nCanada", "# Reasoning Path:\nMontreal -> location.location.containedby -> Urban agglomeration of Montreal -> location.location.containedby -> Qu\u00e9bec\n# Answer:\nUrban agglomeration of Montreal", "# Reasoning Path:\nMontreal -> base.biblioness.bibs_location.state -> Qu\u00e9bec -> location.administrative_division.country -> Canada\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nMontreal -> location.location.containedby -> Canada -> base.aareas.schema.administrative_area.administrative_children -> Saskatchewan\n# Answer:\nCanada", "# Reasoning Path:\nMontreal -> location.location.containedby -> Urban agglomeration of Montreal -> location.location.containedby -> Canada\n# Answer:\nUrban agglomeration of Montreal"], "ground_truth": ["Qu\u00e9bec"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1378", "prediction": ["# Reasoning Path:\nKellan Lutz -> film.actor.film -> g.11b7qz5rxs\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nKellan Lutz -> film.actor.film -> g.11b7xkpw1f\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nKellan Lutz -> film.actor.film -> g.11bws79p_d\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nKellan Lutz -> common.topic.webpage -> m.0bvwg64 -> common.webpage.resource -> Kellan Lutz Pictures\n# Answer:\nKellan Lutz Pictures", "# Reasoning Path:\nKellan Lutz -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nKellan Lutz -> common.topic.webpage -> m.05cl_2n -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nKellan Lutz -> people.person.gender -> Male -> biology.hybrid_parent_gender.hybrids -> m.0zm9_69\n# Answer:\nMale", "# Reasoning Path:\nKellan Lutz -> common.topic.webpage -> m.09wb24h -> common.webpage.resource -> 'Twilight' stars, Dakota Fanning talk 'New Moon'\n# Answer:\n'Twilight' stars, Dakota Fanning talk 'New Moon'", "# Reasoning Path:\nKellan Lutz -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nKellan Lutz -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nMale"], "ground_truth": ["Rick Leland"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1379", "prediction": ["# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Virtuoso -> people.profession.specialization_of -> Musician\n# Answer:\nVirtuoso", "# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Virtuoso -> common.topic.article -> m.01cmdw\n# Answer:\nVirtuoso", "# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Film Score Composer -> people.profession.specialization_of -> Composer\n# Answer:\nFilm Score Composer", "# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Virtuoso -> people.profession.people_with_this_profession -> Arcadi Volodos\n# Answer:\nVirtuoso", "# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Composer -> fictional_universe.character_occupation.characters_with_this_occupation -> Fr\u00e9d\u00e9ric Chopin\n# Answer:\nComposer", "# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Composer -> type.type.expected_by -> composer\n# Answer:\nComposer", "# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Virtuoso -> people.profession.people_with_this_profession -> Lance Dossor\n# Answer:\nVirtuoso", "# Reasoning Path:\nLife of Franz Liszt -> base.kwebbase.kwtopic.connections_to -> achille-claude debussy played for franz liszt -> base.kwebbase.kwconnection.subject -> Claude Debussy\n# Answer:\nachille-claude debussy played for franz liszt", "# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Virtuoso -> people.profession.people_with_this_profession -> Marc-Andr\u00e9 Hamelin\n# Answer:\nVirtuoso", "# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Composer -> type.type.expected_by -> Music\n# Answer:\nComposer"], "ground_truth": ["Priest", "Virtuoso", "Pianist", "Composer", "Film Score Composer"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7200000000000001, "ans_precission": 0.9, "ans_recall": 0.6, "path_f1": 0.48, "path_precision": 0.6, "path_recall": 0.4, "path_ans_f1": 0.7200000000000001, "path_ans_precision": 0.9, "path_ans_recall": 0.6}
{"id": "WebQTest-138", "prediction": ["# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> language.human_language.countries_spoken_in -> Andorra\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> language.human_language.countries_spoken_in -> France\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Basque Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nBasque Language", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.region -> Europe\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.region -> Europe\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Basque Language -> language.human_language.main_country -> France\n# Answer:\nBasque Language"], "ground_truth": ["Galician Language", "Catalan language", "Spanish Language", "Occitan language", "Basque Language"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7200000000000001, "ans_precission": 0.9, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.7200000000000001, "path_ans_precision": 0.9, "path_ans_recall": 0.6}
{"id": "WebQTest-1380", "prediction": ["# Reasoning Path:\nNikola Tesla -> people.person.profession -> Scientist -> common.topic.subject_of -> Book\n# Answer:\nScientist", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Inventor -> people.profession.specialization_of -> Scientist\n# Answer:\nInventor", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Inventor -> common.topic.notable_for -> g.1258bjttw\n# Answer:\nInventor", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Inventor -> base.descriptive_names.names.descriptive_name -> m.0101hljz\n# Answer:\nInventor", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Inventor -> people.profession.specialization_of -> Technician\n# Answer:\nInventor", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Scientist -> tv.tv_subject.tv_programs -> The Secret Life of Scientists & Engineers\n# Answer:\nScientist", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Futurist -> common.topic.notable_types -> Profession\n# Answer:\nFuturist", "# Reasoning Path:\nNikola Tesla -> people.person.nationality -> Austria-Hungary -> base.datedlocationtest.dated_location_test.split_off -> m.0hp5m3v\n# Answer:\nAustria-Hungary", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Inventor -> base.descriptive_names.names.descriptive_name -> m.0105_nsh\n# Answer:\nInventor", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Scientist -> common.topic.subject_of -> Autobiography\n# Answer:\nScientist"], "ground_truth": ["Inventor", "Futurist", "Mechanical Engineer", "Electrical engineer", "Physicist", "Scientist"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6428571428571429, "ans_precission": 0.9, "ans_recall": 0.5, "path_f1": 0.6428571428571429, "path_precision": 0.9, "path_recall": 0.5, "path_ans_f1": 0.6428571428571429, "path_ans_precision": 0.9, "path_ans_recall": 0.5}
{"id": "WebQTest-1381", "prediction": ["# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08540 -> location.location.geolocation -> m.03dtqfp\n# Answer:\n08540", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08540-6449 -> location.location.containedby -> United States of America\n# Answer:\n08540-6449", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08540 -> location.location.contains -> Princeton University Stadium\n# Answer:\n08540", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08540 -> location.postal_code.country -> United States of America\n# Answer:\n08540", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08541 -> common.topic.notable_types -> Postal Code\n# Answer:\n08541", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08540 -> location.location.contains -> 08540-6449\n# Answer:\n08540", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08540-6449 -> common.topic.notable_types -> Postal Code\n# Answer:\n08540-6449", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08540 -> location.location.contains -> Colross\n# Answer:\n08540", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08540-6449 -> common.topic.notable_for -> g.1ypm_h1bc\n# Answer:\n08540-6449", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08541 -> common.topic.notable_for -> g.12551k_bx\n# Answer:\n08541"], "ground_truth": ["08542", "08544", "08541", "08540-6449", "08540", "08542-4511", "08543"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 1.0, "ans_recall": 0.42857142857142855, "path_f1": 0.36363636363636365, "path_precision": 0.5, "path_recall": 0.2857142857142857, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1382", "prediction": ["# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.sports -> Handball\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.sports -> Basketball\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Brazil\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Chile\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nArgentina -> location.location.partially_contains -> Paraguay River -> location.location.partially_containedby -> Paraguay\n# Answer:\nParaguay River", "# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.participating_countries -> Bolivia\n# Answer:\n1936 Summer Olympics"], "ground_truth": ["Paraguay", "Uruguay", "Bolivia", "Brazil", "Chile"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.13333333333333333, "ans_precission": 0.1, "ans_recall": 0.2, "path_f1": 0.05442176870748299, "path_precision": 0.4, "path_recall": 0.029197080291970802, "path_ans_f1": 0.5333333333333333, "path_ans_precision": 0.4, "path_ans_recall": 0.8}
{"id": "WebQTest-1383", "prediction": ["# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> language.human_language.language_family -> Germanic languages\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> language.human_language.region -> Europe\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> language.human_language.language_family -> North Germanic languages\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.official_language -> Icelandic Language -> language.human_language.language_family -> Germanic languages\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.official_language -> Icelandic Language -> language.human_language.region -> Europe\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.statistical_region.gni_in_ppp_dollars -> g.11b61jk1t5\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> common.topic.article -> m.01gmks\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.official_language -> Icelandic Language -> language.human_language.language_family -> North Germanic languages\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.statistical_region.gni_in_ppp_dollars -> g.1245_7d01\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars"], "ground_truth": ["Icelandic Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1384", "prediction": ["# Reasoning Path:\nIsrael -> location.statistical_region.religions -> m.0468s5y -> location.religion_percentage.religion -> Judaism\n# Answer:\nJudaism", "# Reasoning Path:\nIsrael -> symbols.namesake.named_after -> Jacob -> people.person.religion -> Judaism\n# Answer:\nJacob", "# Reasoning Path:\nIsrael -> media_common.quotation.author -> Aaron Friedman -> people.person.religion -> Judaism\n# Answer:\nAaron Friedman", "# Reasoning Path:\nIsrael -> symbols.namesake.named_after -> Jacob -> people.deceased_person.place_of_death -> Egypt\n# Answer:\nJacob", "# Reasoning Path:\nIsrael -> media_common.quotation.author -> Aaron Friedman -> people.person.nationality -> United States of America\n# Answer:\nAaron Friedman"], "ground_truth": ["Judaism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.6666666666666665, "path_precision": 0.6, "path_recall": 0.75, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1385", "prediction": ["# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.04xg_yc -> american_football.football_historical_coach_position.coach -> Dom Capers\n# Answer:\nDom Capers", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.0j81th4 -> american_football.football_historical_coach_position.coach -> John Fox\n# Answer:\nJohn Fox", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.04xg_yl -> american_football.football_historical_coach_position.coach -> George Seifert\n# Answer:\nGeorge Seifert", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.coaches -> m.0_3r22s -> sports.sports_team_coach_tenure.coach -> John Fox\n# Answer:\nJohn Fox", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.0j81th4 -> american_football.football_historical_coach_position.position -> Head coach\n# Answer:\nHead coach", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.coaches -> m.0_3r22s -> sports.sports_team_coach_tenure.position -> Head coach\n# Answer:\nHead coach", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> american_football.football_coach.coaching_history -> m.0hpyjm4\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> award.award_winner.awards_won -> m.0_qw472\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> american_football.football_coach.coaching_history -> m.0hpyjm_\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> american_football.football_coach.coaching_history -> m.0hpyjmf\n# Answer:\nRon Rivera"], "ground_truth": ["George Seifert", "John Fox", "Dom Capers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5333333333333333, "path_precision": 0.4, "path_recall": 0.8, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1387", "prediction": ["# Reasoning Path:\nToronto -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nToronto -> common.topic.webpage -> m.02nc8t6 -> common.webpage.resource -> City of Toronto Web site\n# Answer:\nCity of Toronto Web site", "# Reasoning Path:\nToronto -> common.topic.webpage -> m.059h8f0 -> common.webpage.resource -> Visitor Information\n# Answer:\nVisitor Information", "# Reasoning Path:\nToronto -> common.topic.webpage -> m.02nc8t6 -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nToronto -> travel.travel_destination.local_transportation -> GO Transit -> common.topic.notable_types -> Mass Transportation System\n# Answer:\nGO Transit", "# Reasoning Path:\nToronto -> travel.travel_destination.local_transportation -> Toronto rapid transit -> common.topic.article -> m.02skzz\n# Answer:\nToronto rapid transit", "# Reasoning Path:\nToronto -> travel.travel_destination.local_transportation -> Line 3 Scarborough -> metropolitan_transit.transit_line.service_type -> Rapid transit\n# Answer:\nLine 3 Scarborough", "# Reasoning Path:\nToronto -> common.topic.webpage -> m.059h8f0 -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nToronto -> common.topic.webpage -> m.09w_q2h -> common.webpage.resource -> David Cronenberg journeys to 'Cosmopolis'\n# Answer:\nDavid Cronenberg journeys to 'Cosmopolis'", "# Reasoning Path:\nToronto -> travel.travel_destination.local_transportation -> Toronto rapid transit -> metropolitan_transit.transit_system.transit_lines -> Line 4 Sheppard\n# Answer:\nToronto rapid transit"], "ground_truth": ["Eastern Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1388", "prediction": ["# Reasoning Path:\nToussaint Louverture -> people.person.nationality -> France -> location.location.partially_containedby -> Western Europe\n# Answer:\nFrance", "# Reasoning Path:\nToussaint Louverture -> people.person.nationality -> France -> location.location.containedby -> Western Europe\n# Answer:\nFrance", "# Reasoning Path:\nToussaint Louverture -> people.person.nationality -> France -> location.location.primarily_containedby -> Western Europe\n# Answer:\nFrance", "# Reasoning Path:\nToussaint Louverture -> people.person.nationality -> France -> location.location.partially_containedby -> Europe\n# Answer:\nFrance", "# Reasoning Path:\nToussaint Louverture -> people.person.children -> Saint-Jean Louverture -> people.person.gender -> Male\n# Answer:\nSaint-Jean Louverture", "# Reasoning Path:\nToussaint Louverture -> military.military_person.service -> m.0410mhn -> military.military_service.rank -> General officer\n# Answer:\nGeneral officer", "# Reasoning Path:\nToussaint Louverture -> people.person.nationality -> France -> location.location.containedby -> Europe\n# Answer:\nFrance", "# Reasoning Path:\nToussaint Louverture -> people.person.children -> Saint-Jean Louverture -> common.topic.notable_types -> Person\n# Answer:\nSaint-Jean Louverture", "# Reasoning Path:\nToussaint Louverture -> people.person.nationality -> France -> location.location.primarily_containedby -> Europe\n# Answer:\nFrance", "# Reasoning Path:\nToussaint Louverture -> people.person.children -> Saint-Jean Louverture -> common.topic.notable_for -> g.12564t_87\n# Answer:\nSaint-Jean Louverture"], "ground_truth": ["French Revolutionary Army", "Haitian Rebellion of 1891", "French Army"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1389", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Bard\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.notable_types -> Profession\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Poet\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> book.book_subject.works -> Authors Frequently Mentioned on the Web\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.article -> m.0knj9\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Author\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.image -> The Bard\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Author -> tv.tv_subject.tv_episodes -> Author David Kerr - The Ol' Turkey Hunter\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor"], "ground_truth": ["Author", "Poet", "Writer", "Bard"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.8571428571428571, "path_precision": 1.0, "path_recall": 0.75, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-139", "prediction": ["# Reasoning Path:\nNational Rifle Association -> common.topic.webpage -> m.03lsdwh -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nNational Rifle Association -> common.topic.notable_types -> Membership organization -> freebase.type_profile.strict_included_types -> Organization\n# Answer:\nMembership organization", "# Reasoning Path:\nNational Rifle Association -> common.topic.webpage -> m.03lsdwh -> common.webpage.resource -> m.0bkhqtx\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nNational Rifle Association -> common.topic.webpage -> m.042276w -> common.webpage.resource -> New York Times\n# Answer:\nNew York Times", "# Reasoning Path:\nNational Rifle Association -> common.topic.notable_types -> Membership organization -> type.type.expected_by -> Organization\n# Answer:\nMembership organization", "# Reasoning Path:\nNational Rifle Association -> common.topic.notable_types -> Membership organization -> freebase.type_hints.included_types -> Organization\n# Answer:\nMembership organization", "# Reasoning Path:\nNational Rifle Association -> organization.membership_organization.members -> m.04j6m6t -> organization.organization_membership.member -> Sarah Palin\n# Answer:\nSarah Palin", "# Reasoning Path:\nNational Rifle Association -> common.topic.webpage -> m.042276w -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nNational Rifle Association -> common.topic.notable_types -> Membership organization -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nMembership organization", "# Reasoning Path:\nNational Rifle Association -> organization.membership_organization.members -> m.03yt7ns -> organization.organization_membership.member -> Charlton Heston\n# Answer:\nCharlton Heston"], "ground_truth": ["Fairfax"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1390", "prediction": ["# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0hzbyqy -> government.government_position_held.office_holder -> Mark Shurtleff\n# Answer:\nMark Shurtleff", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0nbwncq -> government.government_position_held.office_holder -> John Swallow\n# Answer:\nJohn Swallow", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0zxk98b -> government.government_position_held.office_holder -> Sean Reyes\n# Answer:\nSean Reyes", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0hzbyqy -> government.government_position_held.basic_title -> Attorney general\n# Answer:\nAttorney general", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0hzbyqy -> government.government_position_held.office_position_or_title -> Utah Attorney General\n# Answer:\nUtah Attorney General", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0nbwncq -> government.government_position_held.office_position_or_title -> Utah Attorney General\n# Answer:\nUtah Attorney General", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0zxk98b -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nUtah -> travel.travel_destination.tour_operators -> Adventures by Disney -> travel.tour_operator.travel_destinations -> Abu Simbel temples\n# Answer:\nAdventures by Disney", "# Reasoning Path:\nUtah -> book.book_subject.works -> Blossoms of faith -> common.topic.notable_for -> g.125fjpyr7\n# Answer:\nBlossoms of faith", "# Reasoning Path:\nUtah -> book.book_subject.works -> Bound for Canaan -> book.written_work.author -> Margaret Blair Young\n# Answer:\nBound for Canaan"], "ground_truth": ["Sean Reyes", "John Swallow", "Mark Shurtleff"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1392", "prediction": ["# Reasoning Path:\nIndonesia -> location.country.languages_spoken -> Tobelo Language -> common.topic.notable_types -> Human Language\n# Answer:\nTobelo Language", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60ptk2z\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nIndonesia -> location.country.languages_spoken -> Bali Language -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nBali Language", "# Reasoning Path:\nIndonesia -> location.country.languages_spoken -> Indonesian Language -> language.human_language.countries_spoken_in -> Timor-Leste\n# Answer:\nIndonesian Language", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.12cp_j7n1\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nIndonesia -> location.country.languages_spoken -> Bali Language -> language.human_language.region -> Asia\n# Answer:\nBali Language", "# Reasoning Path:\nIndonesia -> location.country.languages_spoken -> Indonesian Language -> language.human_language.dialects -> Bali Language\n# Answer:\nIndonesian Language", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_4m6h\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nIndonesia -> location.country.languages_spoken -> Indonesian Language -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nIndonesian Language", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_67l9\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp"], "ground_truth": ["Batak Language", "Indonesian Language", "Sunda Language", "Madura Language", "Javanese Language", "Bali Language", "English Language", "Dutch Language", "Tobelo Language", "Malay Language"], "ans_acc": 0.3, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.6, "ans_recall": 0.3, "path_f1": 0.4, "path_precision": 0.6, "path_recall": 0.3, "path_ans_f1": 0.4, "path_ans_precision": 0.6, "path_ans_recall": 0.3}
{"id": "WebQTest-1394", "prediction": ["# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.04xg_yc -> american_football.football_historical_coach_position.coach -> Dom Capers\n# Answer:\nDom Capers", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> american_football.football_coach.coaching_history -> m.0hpyjm4\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> award.award_winner.awards_won -> m.0_qw472\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> american_football.football_coach.coaching_history -> m.0hpyjm_\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.0j81th4 -> american_football.football_historical_coach_position.coach -> John Fox\n# Answer:\nJohn Fox", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> american_football.football_coach.coaching_history -> m.0hpyjmf\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> people.person.education -> m.0g5vwbs\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.04xg_yl -> american_football.football_historical_coach_position.coach -> George Seifert\n# Answer:\nGeorge Seifert", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.0j81th4 -> american_football.football_historical_coach_position.position -> Head coach\n# Answer:\nHead coach", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> people.person.education -> m.0n0rkd4\n# Answer:\nRon Rivera"], "ground_truth": ["Ron Rivera"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1395", "prediction": ["# Reasoning Path:\nSouth Africa -> location.location.partially_contains -> Njesuthi -> location.location.partially_containedby -> Lesotho\n# Answer:\nNjesuthi", "# Reasoning Path:\nSouth Africa -> location.location.partially_contains -> Caledon River -> geography.river.basin_countries -> Lesotho\n# Answer:\nCaledon River", "# Reasoning Path:\nSouth Africa -> location.location.partially_contains -> Njesuthi -> freebase.valuenotation.has_value -> Prominence\n# Answer:\nNjesuthi", "# Reasoning Path:\nSouth Africa -> location.location.partially_contains -> Njesuthi -> common.topic.notable_types -> Mountain\n# Answer:\nNjesuthi", "# Reasoning Path:\nSouth Africa -> location.location.partially_contains -> Cleft Peak -> location.location.partially_containedby -> Lesotho\n# Answer:\nCleft Peak", "# Reasoning Path:\nSouth Africa -> location.location.partially_contains -> Caledon River -> geography.river.origin -> Mont-Aux-Sources\n# Answer:\nCaledon River", "# Reasoning Path:\nSouth Africa -> location.statistical_region.electricity_consumption_per_capita -> g.1245_4m5f\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nSouth Africa -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Botswana\n# Answer:\nEnglish Language", "# Reasoning Path:\nSouth Africa -> location.country.languages_spoken -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nSouth Africa -> location.country.languages_spoken -> Arabic Language -> common.topic.notable_types -> Human Language\n# Answer:\nArabic Language"], "ground_truth": ["Swaziland", "Mozambique", "Namibia", "Lesotho", "Botswana", "Zimbabwe"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.07692307692307693, "path_precision": 0.3, "path_recall": 0.04411764705882353, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.4, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1396", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.film -> Star Wars\n# Answer:\nStar Wars", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Uncredited\n# Answer:\nUncredited", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.film -> Return of the Jedi\n# Answer:\nReturn of the Jedi", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nThe Making of Star Wars"], "ground_truth": ["Sebastian Shaw", "James Earl Jones", "David Prowse"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.3157894736842105, "ans_precission": 0.3, "ans_recall": 0.3333333333333333, "path_f1": 0.28571428571428564, "path_precision": 0.3, "path_recall": 0.2727272727272727, "path_ans_f1": 0.3157894736842105, "path_ans_precision": 0.3, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1398", "prediction": ["# Reasoning Path:\nJoan Crawford -> award.award_nominee.award_nominations -> m.03mlww8 -> award.award_nomination.nominated_for -> Possessed\n# Answer:\nPossessed", "# Reasoning Path:\nJoan Crawford -> award.award_nominee.award_nominations -> m.03mlwz0 -> award.award_nomination.nominated_for -> Sudden Fear\n# Answer:\nSudden Fear", "# Reasoning Path:\nJoan Crawford -> award.award_nominee.award_nominations -> m.05bz0fg -> award.award_nomination.nominated_for -> Mildred Pierce\n# Answer:\nMildred Pierce", "# Reasoning Path:\nJoan Crawford -> award.award_nominee.award_nominations -> m.03mlww8 -> award.award_nomination.award -> Academy Award for Best Actress\n# Answer:\nAcademy Award for Best Actress", "# Reasoning Path:\nJoan Crawford -> award.award_nominee.award_nominations -> m.03mlwz0 -> award.award_nomination.ceremony -> 25th Academy Awards\n# Answer:\n25th Academy Awards", "# Reasoning Path:\nJoan Crawford -> award.award_nominee.award_nominations -> m.03mlwz0 -> award.award_nomination.award -> Academy Award for Best Actress\n# Answer:\nAcademy Award for Best Actress", "# Reasoning Path:\nJoan Crawford -> film.actor.film -> m.0115tf9q -> film.performance.character -> Della Chappell\n# Answer:\nDella Chappell", "# Reasoning Path:\nJoan Crawford -> tv.tv_actor.guest_roles -> m.0bv_0kc -> tv.tv_guest_role.episodes_appeared_in -> Strange Witness\n# Answer:\nStrange Witness", "# Reasoning Path:\nJoan Crawford -> award.award_nominee.award_nominations -> m.05bz0fg -> award.award_nomination.award -> Academy Award for Best Actress\n# Answer:\nAcademy Award for Best Actress", "# Reasoning Path:\nJoan Crawford -> film.actor.film -> m.0115tf9q -> film.performance.film -> Della\n# Answer:\nDella"], "ground_truth": ["The Shining Hour", "The Karate Killers", "Berserk!", "Sadie McKee", "This Woman Is Dangerous", "Hush\u2026 Hush, Sweet Charlotte", "The Story of Esther Costello", "Female on the Beach", "When Ladies Meet", "Flamingo Road", "Possessed", "The Law of the Range", "The Bride Wore Red", "The Ice Follies of 1939", "Untamed", "Dancing Lady", "Grand Hotel", "Torch Song", "Pretty Ladies", "Twelve Miles Out", "Sudden Fear", "Our Dancing Daughters", "The Unknown", "Mannequin", "The Taxi Dancer", "Sally, Irene and Mary", "Strait-Jacket", "Harriet Craig", "Today We Live", "Old Clothes", "Queen Bee", "Paid", "Spring Fever", "Rose-Marie", "Four Walls", "Chained", "Dream of Love", "Above Suspicion", "Susan and God", "Harry Langdon: The Forgotten Clown", "A Slave of Fashion", "Goodbye, My Fancy", "What Ever Happened to Baby Jane?", "Love on the Run", "The Circle", "The Last of Mrs. Cheyney", "Johnny Guitar", "West Point", "Winners of the Wilderness", "Daisy Kenyon", "Laughing Sinners", "A Woman's Face", "Montana Moon", "Tramp, Tramp, Tramp", "Autumn Leaves", "The Duke Steps Out", "The Damned Don't Cry!", "Our Blushing Brides", "The Boob", "The Caretakers", "It's a Great Feeling", "This Modern Age", "Paris", "Humoresque", "Letty Lynton", "Tide of Empire", "The Gorgeous Hussy", "They All Kissed the Bride", "Dance, Fools, Dance", "The Merry Widow", "Trog", "The Hollywood Revue of 1929", "Della", "Rain", "Our Modern Maidens", "Mildred Pierce", "No More Ladies", "I Saw What You Did", "The Best of Everything", "The Stolen Jools", "The Understanding Heart", "Great Day", "I Live My Life", "Reunion in France", "Strange Cargo", "The Women", "Forsaking All Others", "Across to Singapore"], "ans_acc": 0.045454545454545456, "ans_hit": 1, "ans_f1": 0.08333333333333334, "ans_precission": 0.5, "ans_recall": 0.045454545454545456, "path_f1": 0.06936416184971098, "path_precision": 0.3, "path_recall": 0.0392156862745098, "path_ans_f1": 0.08333333333333334, "path_ans_precision": 0.5, "path_ans_recall": 0.045454545454545456}
{"id": "WebQTest-1399", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Upper Midwest American English -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nUpper Midwest American English", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Northern Mariana Islands\n# Answer:\nSpanish Language", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_children -> Mississippi -> freebase.valuenotation.has_no_value -> Minimum wage\n# Answer:\nMississippi", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_children -> Nebraska -> location.location.containedby -> United States, with Territories\n# Answer:\nNebraska", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_children -> Missouri -> location.location.containedby -> United States, with Territories\n# Answer:\nMissouri", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nSpanish Language", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_children -> Mississippi -> location.location.containedby -> United States, with Territories\n# Answer:\nMississippi", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nSpanish Language", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Abenaki language -> language.human_language.main_country -> Canada\n# Answer:\nAbenaki language"], "ground_truth": ["Pharmaceutical Preparation", "Food Manufacturing", "Industrial Organic Chemicals, NEC", "Automotive industry"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.location.containedby -> Asia\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> Fukushima Prefecture\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_for -> g.125btmfy0\n# Answer:\nFukushima I \u2013 1", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> Japan\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> common.topic.image -> Okuma town office\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 8 -> common.topic.notable_for -> g.1259_1vs2\n# Answer:\nFukushima I \u2013 8", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Fukushima Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> T\u014dhoku region\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 8 -> base.infrastructure.nuclear_power_plant.reactor_type -> Advanced boiling water reactor\n# Answer:\nFukushima I \u2013 8"], "ground_truth": ["Okuma", "Japan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1402", "prediction": ["# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.0hz9xtw -> location.location_symbol_relationship.symbol -> Cardinal\n# Answer:\nCardinal", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.0hz9xtw -> location.location_symbol_relationship.Kind_of_symbol -> State bird\n# Answer:\nState bird", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0d65 -> location.location_symbol_relationship.symbol -> Brachiopod\n# Answer:\nBrachiopod", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0nqp -> location.location_symbol_relationship.symbol -> Liriodendron\n# Answer:\nLiriodendron", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0d65 -> location.location_symbol_relationship.Kind_of_symbol -> State fossil\n# Answer:\nState fossil", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0nqp -> location.location_symbol_relationship.Kind_of_symbol -> State tree\n# Answer:\nState tree", "# Reasoning Path:\nKentucky -> common.topic.webpage -> m.09wljkf -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nKentucky -> military.military_unit_place_of_origin.military_units -> 10th Kentucky Cavalry Regiment (Diamond's) -> common.topic.notable_types -> Military unit\n# Answer:\n10th Kentucky Cavalry Regiment (Diamond's)", "# Reasoning Path:\nKentucky -> military.military_unit_place_of_origin.military_units -> 10th Kentucky Cavalry Regiment (Diamond's) -> military.military_unit.armed_force -> Confederate States Army\n# Answer:\n10th Kentucky Cavalry Regiment (Diamond's)", "# Reasoning Path:\nKentucky -> common.topic.webpage -> m.09xmm58 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Cardinal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1403", "prediction": ["# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.04kc_7_ -> government.government_position_held.office_holder -> Jean Chr\u00e9tien\n# Answer:\nJean Chr\u00e9tien", "# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.04kk22r -> government.government_position_held.office_holder -> Brian Mulroney\n# Answer:\nBrian Mulroney", "# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.04kmh11 -> government.government_position_held.office_holder -> Alexander Mackenzie\n# Answer:\nAlexander Mackenzie", "# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.04kc_7_ -> government.government_position_held.jurisdiction_of_office -> Canada\n# Answer:\nCanada", "# Reasoning Path:\nPrime Minister of Canada -> base.schemastaging.context_name.pronunciation -> g.125_rtp7r\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.04kc_7_ -> government.government_position_held.governmental_body -> Parliament of Canada\n# Answer:\nParliament of Canada", "# Reasoning Path:\nPrime Minister of Canada -> base.firsts.achievement.firsts -> m.04wvp27 -> base.firsts.first_achievement.first -> Kim Campbell\n# Answer:\nKim Campbell", "# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.04kk22r -> government.government_position_held.basic_title -> Prime minister\n# Answer:\nPrime minister", "# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.04kk22r -> government.government_position_held.governmental_body -> Parliament of Canada\n# Answer:\nParliament of Canada", "# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.04kmh11 -> government.government_position_held.basic_title -> Prime minister\n# Answer:\nPrime minister"], "ground_truth": ["Jean Chr\u00e9tien", "Kim Campbell", "Alexander Mackenzie", "Brian Mulroney"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1405", "prediction": ["# Reasoning Path:\nSalt Lake City -> travel.travel_destination.tourist_attractions -> Red Butte Garden and Arboretum -> common.topic.notable_types -> Venue\n# Answer:\nRed Butte Garden and Arboretum", "# Reasoning Path:\nSalt Lake City -> travel.travel_destination.tourist_attractions -> Red Butte Garden and Arboretum -> location.location.geolocation -> m.0cnclfr\n# Answer:\nRed Butte Garden and Arboretum", "# Reasoning Path:\nSalt Lake City -> location.statistical_region.population -> g.11b66h2b_k\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSalt Lake City -> travel.travel_destination.tourist_attractions -> Hogle Zoo -> common.topic.notable_types -> Zoo\n# Answer:\nHogle Zoo", "# Reasoning Path:\nSalt Lake City -> travel.travel_destination.tourist_attractions -> Hogle Zoo -> location.location.geolocation -> m.0clz46_\n# Answer:\nHogle Zoo", "# Reasoning Path:\nSalt Lake City -> location.statistical_region.population -> g.11b7tm7k1_\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSalt Lake City -> location.statistical_region.population -> g.11btt563rr\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSalt Lake City -> tv.tv_location.tv_shows_filmed_here -> The Stand -> film.film.featured_film_locations -> Boulder\n# Answer:\nThe Stand", "# Reasoning Path:\nSalt Lake City -> travel.travel_destination.tourist_attractions -> Zion National Park -> location.location.contains -> Zion Wilderness\n# Answer:\nZion National Park", "# Reasoning Path:\nSalt Lake City -> travel.travel_destination.tourist_attractions -> Zion National Park -> location.location.events -> 2007 Red Rock Film Festival\n# Answer:\nZion National Park"], "ground_truth": ["Zion National Park", "Discovery Gateway", "Trolley Square", "Days of '47 Parade", "Red Butte Garden and Arboretum", "Church History Museum", "Bryce Canyon National Park", "Salt Lake Temple", "g.121xqqc4", "Antelope Island State Park", "Sugar House Park", "Hogle Zoo", "Salt Lake City Public Library", "Clark Planetarium", "Temple Square"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.3, "ans_precission": 0.6, "ans_recall": 0.2, "path_f1": 0.3, "path_precision": 0.6, "path_recall": 0.2, "path_ans_f1": 0.3, "path_ans_precision": 0.6, "path_ans_recall": 0.2}
{"id": "WebQTest-1408", "prediction": ["# Reasoning Path:\nUnited States Senate -> government.governmental_body.committees -> United States Senate Committee on Appropriations -> government.legislative_committee.members -> m.05nls2p\n# Answer:\nUnited States Senate Committee on Appropriations", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.committees -> United States Senate Committee on Commerce, Science, and Transportation -> government.legislative_committee.members -> m.02_wjxj\n# Answer:\nUnited States Senate Committee on Commerce, Science, and Transportation", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.committees -> United States Senate Committee on Appropriations -> government.legislative_committee.members -> m.02_wskm\n# Answer:\nUnited States Senate Committee on Appropriations", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.committees -> United States Senate Committee on Appropriations -> common.topic.notable_for -> g.125fs0hjq\n# Answer:\nUnited States Senate Committee on Appropriations", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.committees -> United States Senate Committee on Armed Services -> government.legislative_committee.members -> m.02_wjvp\n# Answer:\nUnited States Senate Committee on Armed Services", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> United States Senate majority leader -> government.government_office_or_title.office_holders -> m.052jk8c\n# Answer:\nUnited States Senate majority leader", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> United States Senator -> government.government_office_or_title.governmental_body_if_any -> United States Congress\n# Answer:\nUnited States Senator", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.committees -> United States Senate Committee on Appropriations -> government.legislative_committee.members -> m.02_wtnq\n# Answer:\nUnited States Senate Committee on Appropriations", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.committees -> United States Senate Committee on Appropriations -> common.topic.notable_types -> Legislative committee\n# Answer:\nUnited States Senate Committee on Appropriations", "# Reasoning Path:\nUnited States Senate -> common.topic.image -> 600px-Senate_Seal.svg.png -> common.image.appears_in_topic_gallery -> United States Senator\n# Answer:\n600px-Senate_Seal.svg.png"], "ground_truth": ["Sherrod Brown", "Rob Portman"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1409", "prediction": ["# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Wales\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Cornish Language -> language.human_language.countries_spoken_in -> England\n# Answer:\nCornish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> British Sign Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nBritish Sign Language", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nChild pornography", "# Reasoning Path:\nUnited Kingdom -> location.statistical_region.long_term_unemployment_rate -> g.12cp_jvpx\n# Answer:\nlocation.statistical_region.long_term_unemployment_rate", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> Australia\n# Answer:\nEnglish Language"], "ground_truth": ["English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-141", "prediction": ["# Reasoning Path:\nGimli -> fictional_universe.fictional_character.parents -> Gloin -> film.film_character.portrayed_in_films -> m.0gwn5ny\n# Answer:\nGloin", "# Reasoning Path:\nGimli -> fictional_universe.fictional_character.parents -> Gloin -> fictional_universe.fictional_character.organizations -> Thorin and Company\n# Answer:\nGloin", "# Reasoning Path:\nGimli -> fictional_universe.fictional_character.parents -> Gloin -> film.film_character.portrayed_in_films -> m.0y623dj\n# Answer:\nGloin", "# Reasoning Path:\nGimli -> film.film_character.portrayed_in_films -> m.03l6qbh -> film.performance.actor -> John Rhys-Davies\n# Answer:\nJohn Rhys-Davies", "# Reasoning Path:\nGimli -> fictional_universe.fictional_character.species -> Dwarf -> fictional_universe.character_species.characters_of_this_species -> Balin\n# Answer:\nDwarf", "# Reasoning Path:\nGimli -> fictional_universe.fictional_character.species -> Dwarf -> common.topic.article -> m.0h16z\n# Answer:\nDwarf", "# Reasoning Path:\nGimli -> fictional_universe.fictional_character.species -> Dwarf -> common.topic.image -> Cirth\n# Answer:\nDwarf", "# Reasoning Path:\nGimli -> film.film_character.portrayed_in_films -> m.03wlpyc -> film.performance.actor -> John Rhys-Davies\n# Answer:\nJohn Rhys-Davies", "# Reasoning Path:\nGimli -> fictional_universe.fictional_character.species -> Dwarf -> fictional_universe.character_species.characters_of_this_species -> D\u00e1in II Ironfoot\n# Answer:\nDwarf", "# Reasoning Path:\nGimli -> fictional_universe.fictional_character.species -> Dwarf -> fictional_universe.character_species.characters_of_this_species -> Fili\n# Answer:\nDwarf"], "ground_truth": ["Gloin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1410", "prediction": ["# Reasoning Path:\nSaint Nicholas -> book.book_subject.works -> St. Nicholas of Myra, Bari and Manhattan: Biography of a Legend -> book.book.editions -> Saint Nicholas of Myra, Bari, and Manhattan : biography of a legend\n# Answer:\nSt. Nicholas of Myra, Bari and Manhattan: Biography of a Legend", "# Reasoning Path:\nSaint Nicholas -> book.book_subject.works -> St. Nicholas of Myra, Bari and Manhattan: Biography of a Legend -> common.topic.notable_for -> g.1256xmt38\n# Answer:\nSt. Nicholas of Myra, Bari and Manhattan: Biography of a Legend", "# Reasoning Path:\nSaint Nicholas -> people.person.place_of_birth -> Patara, Lycia -> location.location.people_born_here -> Aedesius of Alexandria\n# Answer:\nPatara, Lycia", "# Reasoning Path:\nSaint Nicholas -> symbols.name_source.namesakes -> Agios Nikolaos -> location.location.geolocation -> m.0d84dct\n# Answer:\nAgios Nikolaos", "# Reasoning Path:\nSaint Nicholas -> people.person.place_of_birth -> Patara, Lycia -> travel.travel_destination.tourist_attractions -> Patara Beach\n# Answer:\nPatara, Lycia", "# Reasoning Path:\nSaint Nicholas -> symbols.name_source.namesakes -> Agios Nikolaos -> location.statistical_region.population -> m.011v56tc\n# Answer:\nAgios Nikolaos", "# Reasoning Path:\nSaint Nicholas -> symbols.name_source.namesakes -> Basilica di San Nicola -> common.topic.article -> m.0dw4h0\n# Answer:\nBasilica di San Nicola", "# Reasoning Path:\nSaint Nicholas -> people.person.place_of_birth -> Patara, Lycia -> common.topic.notable_for -> g.125g6zs00\n# Answer:\nPatara, Lycia", "# Reasoning Path:\nSaint Nicholas -> symbols.name_source.namesakes -> Agios Nikolaos -> location.administrative_division.third_level_division_of -> Greece\n# Answer:\nAgios Nikolaos", "# Reasoning Path:\nSaint Nicholas -> symbols.name_source.namesakes -> Basilica di San Nicola -> location.location.geolocation -> m.0wm95d9\n# Answer:\nBasilica di San Nicola"], "ground_truth": ["Bishop"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1414", "prediction": ["# Reasoning Path:\nPope John Paul II -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Old age\n# Answer:\nheart attack", "# Reasoning Path:\nPope John Paul II -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.parent_disease -> Cardiovascular disease\n# Answer:\nheart attack", "# Reasoning Path:\nPope John Paul II -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Male\n# Answer:\nheart attack", "# Reasoning Path:\nPope John Paul II -> people.deceased_person.cause_of_death -> heart attack -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nheart attack", "# Reasoning Path:\nPope John Paul II -> people.deceased_person.cause_of_death -> heart attack -> medicine.risk_factor.diseases -> Major depression\n# Answer:\nheart attack", "# Reasoning Path:\nPope John Paul II -> people.deceased_person.cause_of_death -> Parkinson's disease -> medicine.disease.treatments -> Tolcapone\n# Answer:\nParkinson's disease", "# Reasoning Path:\nPope John Paul II -> symbols.name_source.namesakes -> Blessed Pope John Paul II Catholic Secondary School -> education.educational_institution.school_type -> High school\n# Answer:\nBlessed Pope John Paul II Catholic Secondary School", "# Reasoning Path:\nPope John Paul II -> common.topic.webpage -> m.09xfw5c -> common.webpage.resource -> One pope to rule them all...\n# Answer:\nOne pope to rule them all...", "# Reasoning Path:\nPope John Paul II -> people.deceased_person.cause_of_death -> Parkinson's disease -> medicine.disease.treatments -> Amantadine\n# Answer:\nParkinson's disease", "# Reasoning Path:\nPope John Paul II -> symbols.name_source.namesakes -> John Paul Academy -> common.topic.image -> John Paul Academy in 2005\n# Answer:\nJohn Paul Academy"], "ground_truth": ["heart attack", "Parkinson's disease"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1416", "prediction": ["# Reasoning Path:\nFranklin Pierce -> people.person.profession -> Politician -> common.topic.subjects -> Nathalie Kosciusko-Morizet\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin Pierce -> people.person.profession -> Politician -> people.profession.specializations -> Tyrant\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin Pierce -> common.topic.notable_types -> US President -> type.type.expected_by -> President\n# Answer:\nUS President", "# Reasoning Path:\nFranklin Pierce -> people.person.profession -> Politician -> common.topic.subjects -> Akinwunmi Ambode\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin Pierce -> people.person.profession -> Politician -> people.profession.specializations -> AIDS Policy Specialist\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin Pierce -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdn_\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin Pierce -> common.topic.notable_types -> US President -> type.type.expected_by -> US Presidents\n# Answer:\nUS President", "# Reasoning Path:\nFranklin Pierce -> people.person.profession -> Politician -> people.profession.specializations -> Advocacy\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin Pierce -> common.topic.notable_types -> US President -> type.type.properties -> Vice president\n# Answer:\nUS President", "# Reasoning Path:\nFranklin Pierce -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdq3\n# Answer:\nPolitician"], "ground_truth": ["US President"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1417", "prediction": ["# Reasoning Path:\nBoston Bruins -> sports.sports_team.championships -> 2011 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n2011 Stanley Cup Finals", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.arena_stadium -> Boston Garden -> sports.sports_facility.teams -> Hartford Whalers\n# Answer:\nBoston Garden", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.arena_stadium -> TD Garden -> sports.sports_facility.teams -> Boston Celtics\n# Answer:\nTD Garden", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.championships -> 2011 Stanley Cup Finals -> time.event.locations -> Rogers Arena\n# Answer:\n2011 Stanley Cup Finals", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.championships -> 1929 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n1929 Stanley Cup Finals", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.championships -> 1939 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n1939 Stanley Cup Finals", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.arena_stadium -> Boston Garden -> location.location.events -> 1951 NBA All-Star Game\n# Answer:\nBoston Garden", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.arena_stadium -> Boston Garden -> location.location.geolocation -> m.0clmg5x\n# Answer:\nBoston Garden", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.championships -> 1929 Stanley Cup Finals -> common.topic.article -> m.02rnkk5\n# Answer:\n1929 Stanley Cup Finals", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.arena_stadium -> Boston Garden -> sports.sports_facility.teams -> Boston Braves\n# Answer:\nBoston Garden"], "ground_truth": ["2013 Stanley Cup Finals"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1418", "prediction": ["# Reasoning Path:\nMeredith Grey -> fictional_universe.fictional_character.married_to -> m.0gx44zk -> fictional_universe.marriage_of_fictional_characters.spouses -> Derek Shepherd\n# Answer:\nDerek Shepherd", "# Reasoning Path:\nMeredith Grey -> base.italiantv.adapted_tv_character.dubbing_performances -> m.09tms_9 -> base.italiantv.tv_dubbing_performance.actor -> Giuppy Izzo\n# Answer:\nGiuppy Izzo", "# Reasoning Path:\nMeredith Grey -> fictional_universe.fictional_character.employers -> m.0k53dtw -> fictional_universe.fictional_employment_tenure.title -> Surgeon\n# Answer:\nSurgeon", "# Reasoning Path:\nMeredith Grey -> base.italiantv.adapted_tv_character.dubbing_performances -> m.09tms_9 -> base.italiantv.tv_dubbing_performance.tv_program -> Italian adaptation of Grey's Anatomy\n# Answer:\nItalian adaptation of Grey's Anatomy", "# Reasoning Path:\nMeredith Grey -> fictional_universe.fictional_character.employers -> m.0k53dtw -> fictional_universe.fictional_employment_tenure.employer -> Seattle Grace Mercy West Hospital\n# Answer:\nSeattle Grace Mercy West Hospital"], "ground_truth": ["Chris Ivery"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1419", "prediction": ["# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced_by -> Domenico Ghirlandaio -> influence.influence_node.influenced_by -> Alesso Baldovinetti\n# Answer:\nDomenico Ghirlandaio", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced_by -> Domenico Ghirlandaio -> influence.influence_node.influenced_by -> Masaccio\n# Answer:\nDomenico Ghirlandaio", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced_by -> Melozzo da Forl\u00ec -> influence.influence_node.influenced_by -> Piero della Francesca\n# Answer:\nMelozzo da Forl\u00ec", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced_by -> Lorenzo de' Medici -> influence.influence_node.influenced -> Giovanni Pico della Mirandola\n# Answer:\nLorenzo de' Medici", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced_by -> Domenico Ghirlandaio -> influence.influence_node.influenced_by -> Andrea del Verrocchio\n# Answer:\nDomenico Ghirlandaio", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced_by -> Domenico Ghirlandaio -> visual_art.visual_artist.associated_periods_or_movements -> Italian Renaissance\n# Answer:\nDomenico Ghirlandaio", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced_by -> Domenico Ghirlandaio -> visual_art.visual_artist.artworks -> Adoration of the Magi\n# Answer:\nDomenico Ghirlandaio", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced_by -> Lorenzo de' Medici -> influence.influence_node.influenced -> Donatello\n# Answer:\nLorenzo de' Medici", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced_by -> Lorenzo de' Medici -> influence.influence_node.influenced -> Leonardo da Vinci\n# Answer:\nLorenzo de' Medici", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced_by -> Lorenzo de' Medici -> people.person.gender -> Male\n# Answer:\nLorenzo de' Medici"], "ground_truth": ["Domenico Ghirlandaio", "Donatello", "Girolamo Savonarola", "Giovanni Pico della Mirandola", "Melozzo da Forl\u00ec", "Lorenzo de' Medici"], "ans_acc": 0.8333333333333334, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 0.9090909090909091, "path_ans_precision": 1.0, "path_ans_recall": 0.8333333333333334}
{"id": "WebQTest-142", "prediction": ["# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Robert  E. Kahn -> law.inventor.inventions -> The Internet\n# Answer:\nRobert  E. Kahn", "# Reasoning Path:\nTransmission Control Protocol -> base.schemastaging.context_name.pronunciation -> g.125_qdtmg\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Robert  E. Kahn -> people.person.gender -> Male\n# Answer:\nRobert  E. Kahn", "# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Robert  E. Kahn -> law.inventor.inventions -> Internet Protocol\n# Answer:\nRobert  E. Kahn", "# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Robert  E. Kahn -> people.person.employment_history -> m.04_j1q_\n# Answer:\nRobert  E. Kahn", "# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Vint Cerf -> law.inventor.inventions -> The Internet\n# Answer:\nVint Cerf", "# Reasoning Path:\nTransmission Control Protocol -> base.argumentmaps.original_idea.innovator -> Robert  E. Kahn -> law.inventor.inventions -> The Internet\n# Answer:\nRobert  E. Kahn", "# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Robert  E. Kahn -> people.person.employment_history -> m.04_j1qm\n# Answer:\nRobert  E. Kahn", "# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Robert  E. Kahn -> people.person.employment_history -> m.04_j1sq\n# Answer:\nRobert  E. Kahn", "# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Vint Cerf -> education.academic.advisors -> Gerald Estrin\n# Answer:\nVint Cerf"], "ground_truth": ["Robert  E. Kahn", "Vint Cerf"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.6, "path_recall": 0.5, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1420", "prediction": ["# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agriculture -> business.industry.companies -> AMERICAN CRYSTAL SUGAR CO /MN/\n# Answer:\nAgriculture", "# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agriculture -> common.topic.article -> m.0hkr\n# Answer:\nAgriculture", "# Reasoning Path:\nMonsanto -> organization.organization.child -> m.09pdql9 -> organization.organization_relationship.child -> Mahyco Monsanto Biotech (I) Pvt. Ltd.\n# Answer:\nMahyco Monsanto Biotech (I) Pvt. Ltd.", "# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agriculture -> freebase.valuenotation.has_value -> NAICS 2007 code\n# Answer:\nAgriculture", "# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agriculture -> business.industry.companies -> AVEBE\n# Answer:\nAgriculture", "# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agrochemical -> business.industry.companies -> Aarti Industries\n# Answer:\nAgrochemical", "# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agrochemical -> base.nasagcmd.gcmd_keyword1.includes_keyword_s -> Pesticide\n# Answer:\nAgrochemical", "# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agriculture -> business.industry.companies -> Adler Seeds\n# Answer:\nAgriculture", "# Reasoning Path:\nMonsanto -> organization.organization.child -> m.09pftcw -> organization.organization_relationship.child -> Monsanto Ag Technologies LLC\n# Answer:\nMonsanto Ag Technologies LLC", "# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agrochemical -> business.industry.companies -> Agrium\n# Answer:\nAgrochemical"], "ground_truth": ["Agriculture", "Seed", "Agrochemical", "Chemical industry"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.6153846153846154, "ans_precission": 0.8, "ans_recall": 0.5, "path_f1": 0.6153846153846154, "path_precision": 0.8, "path_recall": 0.5, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.8, "path_ans_recall": 0.75}
{"id": "WebQTest-1421", "prediction": ["# Reasoning Path:\nCheers -> tv.tv_program.regular_cast -> m.02hsl63 -> tv.regular_tv_appearance.character -> Norm Peterson\n# Answer:\nNorm Peterson", "# Reasoning Path:\nCheers -> tv.tv_program.regular_cast -> m.0wyv_l7 -> tv.regular_tv_appearance.character -> Paul Krapence\n# Answer:\nPaul Krapence", "# Reasoning Path:\nWoody Harrelson -> tv.tv_actor.starring_roles -> m.0vm_d1v -> tv.regular_tv_appearance.series -> Game Change\n# Answer:\nGame Change", "# Reasoning Path:\nWoody Harrelson -> tv.tv_actor.starring_roles -> m.02t8y53 -> tv.regular_tv_appearance.character -> Woody Boyd\n# Answer:\nWoody Boyd", "# Reasoning Path:\nCheers -> tv.tv_program.regular_cast -> m.02hsl63 -> tv.regular_tv_appearance.actor -> George Wendt\n# Answer:\nGeorge Wendt", "# Reasoning Path:\nCheers -> tv.tv_program.regular_cast -> m.01y0qy_ -> tv.regular_tv_appearance.character -> Frasier Crane\n# Answer:\nFrasier Crane", "# Reasoning Path:\nWoody Harrelson -> tv.tv_actor.starring_roles -> m.0_0mgc5 -> tv.regular_tv_appearance.character -> Martin Hart\n# Answer:\nMartin Hart", "# Reasoning Path:\nWoody Harrelson -> film.actor.film -> g.11b6g7srhd\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nCheers -> tv.tv_program.regular_cast -> m.0wyv_l7 -> tv.regular_tv_appearance.actor -> Paul Lee Willson\n# Answer:\nPaul Lee Willson", "# Reasoning Path:\nCheers -> tv.tv_program.regular_cast -> m.0wyv_l7 -> tv.regular_tv_appearance.seasons -> Cheers - Season 11\n# Answer:\nCheers - Season 11"], "ground_truth": ["Woody Boyd"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1422", "prediction": ["# Reasoning Path:\nSweden -> location.country.official_language -> Swedish Language -> language.human_language.countries_spoken_in -> Finland\n# Answer:\nSwedish Language", "# Reasoning Path:\nSweden -> location.country.official_language -> Swedish Language -> language.human_language.region -> Europe\n# Answer:\nSwedish Language", "# Reasoning Path:\nSweden -> location.country.languages_spoken -> Yiddish Language -> language.human_language.countries_spoken_in -> Russia\n# Answer:\nYiddish Language", "# Reasoning Path:\nSweden -> location.country.official_language -> Swedish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSwedish Language", "# Reasoning Path:\nSweden -> location.country.languages_spoken -> Yiddish Language -> common.topic.notable_types -> Human Language\n# Answer:\nYiddish Language", "# Reasoning Path:\nSweden -> location.country.languages_spoken -> Swedish Language -> language.human_language.countries_spoken_in -> Finland\n# Answer:\nSwedish Language", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.country.languages_spoken -> Yiddish Language -> language.human_language.main_country -> United States of America\n# Answer:\nYiddish Language", "# Reasoning Path:\nSweden -> location.country.languages_spoken -> Swedish Language -> language.human_language.region -> Europe\n# Answer:\nSwedish Language", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ll\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars"], "ground_truth": ["Swedish Language", "Finnish Language", "Me\u00e4nkieli", "Yiddish Language", "Turoyo Language", "Romani language", "Swedish Sign Language"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.4210526315789473, "ans_precission": 0.8, "ans_recall": 0.2857142857142857, "path_f1": 0.3870967741935483, "path_precision": 0.6, "path_recall": 0.2857142857142857, "path_ans_f1": 0.4210526315789473, "path_ans_precision": 0.8, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-1423", "prediction": ["# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> India\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Kenya\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.languages_spoken -> \u00c9w\u00e9 Language -> language.human_language.writing_system -> Latin alphabet\n# Answer:\n\u00c9w\u00e9 Language", "# Reasoning Path:\nGhana -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60ywwvy\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nGhana -> location.country.languages_spoken -> \u00c9w\u00e9 Language -> language.human_language.language_family -> Niger\u2013Congo languages\n# Answer:\n\u00c9w\u00e9 Language", "# Reasoning Path:\nGhana -> location.country.languages_spoken -> \u00c9w\u00e9 Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\n\u00c9w\u00e9 Language"], "ground_truth": ["Akan Language", "Kasem Language", "Dangme Language", "\u00c9w\u00e9 Language", "Asante dialect", "Dagaare language", "Fula language", "Nzema Language", "English Language", "Gonja Language", "Dagbani Language", "Ga Language"], "ans_acc": 0.16666666666666666, "ans_hit": 1, "ans_f1": 0.28125, "ans_precission": 0.9, "ans_recall": 0.16666666666666666, "path_f1": 0.23529411764705882, "path_precision": 0.4, "path_recall": 0.16666666666666666, "path_ans_f1": 0.28125, "path_ans_precision": 0.9, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTest-1424", "prediction": ["# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Thailand\n# Answer:\nUnitary state", "# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> United States of America\n# Answer:\nPresidential system", "# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Japan\n# Answer:\nUnitary state", "# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Marshall Islands\n# Answer:\nUnitary state", "# Reasoning Path:\nPhilippines -> location.statistical_region.external_debt_stock -> g.11b71qs22v\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Indonesia\n# Answer:\nConstitutional republic", "# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Russia\n# Answer:\nConstitutional republic", "# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guam\n# Answer:\nPresidential system", "# Reasoning Path:\nPhilippines -> location.statistical_region.external_debt_stock -> g.1hhc37prj\n# Answer:\nlocation.statistical_region.external_debt_stock"], "ground_truth": ["Unitary state", "Presidential system", "Republic", "Constitutional republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1425", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.institution -> Harvard Law School\n# Answer:\nHarvard Law School", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkf4 -> education.education.institution -> Stanford University\n# Answer:\nStanford University", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.major_field_of_study -> Law\n# Answer:\nLaw", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0tk1pyb -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.0123vxrw -> education.educational_institution.students_graduates -> m.0123vxqw\n# Answer:\neducation.educational_institution.students_graduates", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkf4 -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0tk1pyb -> film.personal_film_appearance.film -> 8: The Mormon Proposition\n# Answer:\n8: The Mormon Proposition", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0_gvz4k -> film.personal_film_appearance.film -> Mitt\n# Answer:\nMitt", "# Reasoning Path:\nMitt Romney -> tv.tv_actor.guest_roles -> m.0bvvym_ -> tv.tv_guest_role.episodes_appeared_in -> The Comeback of President Bush\n# Answer:\nThe Comeback of President Bush", "# Reasoning Path:\nMitt Romney -> tv.tv_actor.guest_roles -> m.0bvw9my -> tv.tv_guest_role.episodes_appeared_in -> Mitt Romney, Ryan Sheckler, Jonas Brothers\n# Answer:\nMitt Romney, Ryan Sheckler, Jonas Brothers"], "ground_truth": ["Harvard University", "Harvard Business School", "Harvard Law School", "Brigham Young University", "Stanford University"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.26666666666666666, "ans_precission": 0.2, "ans_recall": 0.4, "path_f1": 0.25, "path_precision": 0.2, "path_recall": 0.3333333333333333, "path_ans_f1": 0.26666666666666666, "path_ans_precision": 0.2, "path_ans_recall": 0.4}
{"id": "WebQTest-1426", "prediction": ["# Reasoning Path:\nIbrahim Afellay -> people.person.place_of_birth -> Utrecht -> location.location.containedby -> Kingdom of the Netherlands\n# Answer:\nUtrecht", "# Reasoning Path:\nIbrahim Afellay -> people.person.place_of_birth -> Utrecht -> location.location.containedby -> Netherlands\n# Answer:\nUtrecht", "# Reasoning Path:\nIbrahim Afellay -> people.person.place_of_birth -> Utrecht -> location.statistical_region.population -> g.1jmcbdzl0\n# Answer:\nUtrecht", "# Reasoning Path:\nIbrahim Afellay -> people.person.place_of_birth -> Utrecht -> travel.travel_destination.tourist_attractions -> Centraal Museum\n# Answer:\nUtrecht", "# Reasoning Path:\nIbrahim Afellay -> soccer.football_player.statistics -> m.0w8x1wl -> soccer.football_player_stats.team -> PSV Eindhoven\n# Answer:\nPSV Eindhoven", "# Reasoning Path:\nIbrahim Afellay -> people.person.place_of_birth -> Utrecht -> location.statistical_region.population -> g.1jmcbfmb_\n# Answer:\nUtrecht", "# Reasoning Path:\nIbrahim Afellay -> people.person.profession -> Football player -> common.topic.article -> m.01lkv6\n# Answer:\nFootball player", "# Reasoning Path:\nIbrahim Afellay -> people.person.place_of_birth -> Utrecht -> location.statistical_region.population -> g.1jmcbgvqk\n# Answer:\nUtrecht", "# Reasoning Path:\nIbrahim Afellay -> people.person.place_of_birth -> Utrecht -> travel.travel_destination.tourist_attractions -> Singelloop Utrecht\n# Answer:\nUtrecht", "# Reasoning Path:\nIbrahim Afellay -> people.person.profession -> Football player -> type.type.expected_by -> On\n# Answer:\nOn"], "ground_truth": ["Utrecht"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1427", "prediction": ["# Reasoning Path:\nAndrew Jackson -> government.political_appointer.appointees -> m.0bws0wh -> government.government_position_held.office_holder -> Edward Livingston\n# Answer:\nEdward Livingston", "# Reasoning Path:\nAndrew Jackson -> government.political_appointer.appointees -> m.04kq1q9 -> government.government_position_held.office_holder -> Louis McLane\n# Answer:\nLouis McLane", "# Reasoning Path:\nAndrew Jackson -> government.us_president.vice_president -> Martin Van Buren -> government.us_president.vice_president -> Richard Mentor Johnson\n# Answer:\nMartin Van Buren", "# Reasoning Path:\nAndrew Jackson -> government.political_appointer.appointees -> m.0bws0wh -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nAndrew Jackson -> government.political_appointer.appointees -> m.04kq1q5 -> government.government_position_held.office_holder -> Samuel D. Ingham\n# Answer:\nSamuel D. Ingham", "# Reasoning Path:\nAndrew Jackson -> government.political_appointer.appointees -> m.04kq1q9 -> government.government_position_held.office_position_or_title -> United States Secretary of the Treasury\n# Answer:\nUnited States Secretary of the Treasury", "# Reasoning Path:\nAndrew Jackson -> government.political_appointer.appointees -> m.04kq1q9 -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nAndrew Jackson -> government.us_president.vice_president -> Martin Van Buren -> people.person.profession -> Politician\n# Answer:\nMartin Van Buren", "# Reasoning Path:\nAndrew Jackson -> government.us_president.vice_president -> Martin Van Buren -> government.politician.party -> m.03gjg5m\n# Answer:\nMartin Van Buren", "# Reasoning Path:\nAndrew Jackson -> people.deceased_person.place_of_death -> Nashville -> travel.travel_destination.tourist_attractions -> The Hermitage\n# Answer:\nNashville"], "ground_truth": ["James Alexander Hamilton", "Louis McLane", "Edward Livingston", "Martin Van Buren"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.5, "ans_recall": 0.75, "path_f1": 0.5454545454545454, "path_precision": 0.5, "path_recall": 0.6, "path_ans_f1": 0.6, "path_ans_precision": 0.5, "path_ans_recall": 0.75}
{"id": "WebQTest-1428", "prediction": ["# Reasoning Path:\nRon Stoppable -> fictional_universe.fictional_character.character_created_by -> Bob Schooley -> fictional_universe.fictional_character_creator.fictional_characters_created -> Kim Possible\n# Answer:\nBob Schooley", "# Reasoning Path:\nRon Stoppable -> fictional_universe.fictional_character.character_created_by -> Bob Schooley -> fictional_universe.fictional_character_creator.fictional_characters_created -> Yori\n# Answer:\nBob Schooley", "# Reasoning Path:\nRon Stoppable -> fictional_universe.fictional_character.character_created_by -> Mark McCorkle -> fictional_universe.fictional_character_creator.fictional_characters_created -> Shego\n# Answer:\nMark McCorkle", "# Reasoning Path:\nRon Stoppable -> film.film_character.portrayed_in_films -> m.0j_gwp -> film.performance.actor -> Will Friedle\n# Answer:\nWill Friedle", "# Reasoning Path:\nRon Stoppable -> fictional_universe.fictional_character.character_created_by -> Mark McCorkle -> fictional_universe.fictional_character_creator.fictional_characters_created -> Bobo\n# Answer:\nMark McCorkle", "# Reasoning Path:\nRon Stoppable -> common.topic.article -> m.09w3my\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nRon Stoppable -> fictional_universe.fictional_character.character_created_by -> Bob Schooley -> fictional_universe.fictional_character_creator.fictional_characters_created -> DNAmy\n# Answer:\nBob Schooley", "# Reasoning Path:\nRon Stoppable -> fictional_universe.fictional_character.character_created_by -> Bob Schooley -> common.topic.notable_types -> TV Writer\n# Answer:\nBob Schooley", "# Reasoning Path:\nRon Stoppable -> fictional_universe.fictional_character.character_created_by -> Mark McCorkle -> fictional_universe.fictional_character_creator.fictional_characters_created -> Kim Possible\n# Answer:\nMark McCorkle", "# Reasoning Path:\nRon Stoppable -> fictional_universe.fictional_character.character_created_by -> Bob Schooley -> tv.tv_writer.episodes_written -> Happy King Julien Day\n# Answer:\nBob Schooley"], "ground_truth": ["Will Friedle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.13333333333333333, "path_precision": 0.1, "path_recall": 0.2, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1429", "prediction": ["# Reasoning Path:\nNew Zealand -> location.country.currency_used -> New Zealand dollar -> finance.currency.countries_used -> Tokelau\n# Answer:\nNew Zealand dollar", "# Reasoning Path:\nNew Zealand -> location.country.currency_used -> New Zealand dollar -> finance.currency.countries_used -> Cook Islands\n# Answer:\nNew Zealand dollar", "# Reasoning Path:\nNew Zealand -> location.country.currency_used -> New Zealand dollar -> finance.currency.countries_used -> Niue\n# Answer:\nNew Zealand dollar", "# Reasoning Path:\nNew Zealand -> film.film_location.featured_in_films -> Yogi Bear -> film.film.country -> United States of America\n# Answer:\nYogi Bear", "# Reasoning Path:\nNew Zealand -> film.film_location.featured_in_films -> Yogi Bear -> media_common.netflix_title.netflix_genres -> Family\n# Answer:\nYogi Bear", "# Reasoning Path:\nNew Zealand -> olympics.olympic_participating_country.medals_won -> m.04gdvs5 -> olympics.olympic_medal_honor.medal -> Gold medal\n# Answer:\nGold medal", "# Reasoning Path:\nNew Zealand -> film.film_location.featured_in_films -> Black Sheep -> film.film.country -> Australia\n# Answer:\nBlack Sheep", "# Reasoning Path:\nNew Zealand -> film.film_location.featured_in_films -> Bridge to Terabithia -> film.film.country -> United States of America\n# Answer:\nBridge to Terabithia", "# Reasoning Path:\nNew Zealand -> olympics.olympic_participating_country.medals_won -> m.04gdvs5 -> olympics.olympic_medal_honor.olympics -> 2008 Summer Olympics\n# Answer:\n2008 Summer Olympics", "# Reasoning Path:\nNew Zealand -> film.film_location.featured_in_films -> Black Sheep -> common.topic.notable_types -> Film\n# Answer:\nBlack Sheep"], "ground_truth": ["New Zealand dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1432", "prediction": ["# Reasoning Path:\nWest Midlands -> base.aareas.schema.administrative_area.administrative_children -> Birmingham -> location.administrative_division.country -> United Kingdom\n# Answer:\nBirmingham", "# Reasoning Path:\nWest Midlands -> base.aareas.schema.administrative_area.administrative_children -> Worcestershire -> base.aareas.schema.administrative_area.administrative_children -> Redditch\n# Answer:\nWorcestershire", "# Reasoning Path:\nWest Midlands -> location.administrative_division.country -> England -> base.aareas.schema.administrative_area.administrative_children -> North East England\n# Answer:\nEngland", "# Reasoning Path:\nWest Midlands -> location.administrative_division.country -> England -> location.administrative_division.country -> United Kingdom\n# Answer:\nEngland", "# Reasoning Path:\nWest Midlands -> base.aareas.schema.administrative_area.administrative_children -> Birmingham -> book.book_subject.works -> The Rotters' Club\n# Answer:\nBirmingham", "# Reasoning Path:\nWest Midlands -> base.aareas.schema.administrative_area.administrative_children -> Worcestershire -> location.administrative_division.country -> United Kingdom\n# Answer:\nWorcestershire"], "ground_truth": ["Chadwick End", "Earlsdon", "Handsworth Wood", "Brandhall", "Erdington", "Barr Beacon", "West Heath, West Midlands", "Cradley Heath", "Metropolitan Borough of Solihull", "Tipton", "Gornal, West Midlands", "Hall Green", "Acocks Green", "Brades Village", "Bushbury", "Sparkbrook", "Hockley Heath", "Allesley Green", "Catherine-de-Barnes", "Lode Heath School and Specialist College", "Halesowen", "Shropshire", "Aston Ramsden", "Hodge Hill", "Springfield, Birmingham", "Ashted", "City College Coventry", "Quarry Bank", "Kings Norton", "Brandwood", "Middleton", "Edgbaston", "Longbridge", "Dorridge", "Solihull", "Selly Park", "Blossomfield", "Billesley, West Midlands", "Merridale", "Sutton Four Oaks", "Yardley Wood", "Blakenall Heath", "Harborne", "Park Village", "King Edward VI College, Stourbridge", "Wylde Green", "Chelmsley Wood", "Wall Heath", "City of Wolverhampton College", "Horseley Fields", "Chad Valley, Birmingham", "Cheylesmore", "Compton, Wolverhampton", "Olton", "Northfield, Birmingham", "Bearwood, West Midlands", "Dovecotes", "Bartley Green", "Chapelfields", "Rednal", "Rushall, West Midlands", "Aldridge", "Scotlands Estate", "University of Wolverhampton, Walsall Campus", "Fordhouses", "Worcestershire", "Tividale", "Brierley Hill", "Stowheath", "Birmingham City University", "Ashmore Park", "Alumwell Business and Enterprise College", "Nechells", "Rowley Regis", "Kingswinford", "Great Bridge, West Midlands", "Bordesley Green", "Birmingham", "National School of Blacksmithing, Holme Lacy campus", "Weoley Castle", "Sutton Trinity", "Lye", "Wolverhampton", "Brownhills", "Aston University, Birmingham", "Blakenhall", "Quinton, Birmingham", "Stoke-on-Trent", "Perry Barr", "Bournville", "Hockley", "Stone Cross, West Midlands", "Tudor Hill", "Selly Oak", "Clayhanger, West Midlands", "University of Wolverhampton, Compton Park Campus", "Great Barr", "Dunstall Hill", "Oakham, West Midlands", "Low Hill", "Oldbury", "Patton", "Jewellery Quarter", "Castle Bromwich", "Aston Business School", "Pelsall", "Kingshurst", "Shard End", "Sandwell College, Smethwick", "Netherton", "Sutton Vesey", "Meriden", "Shirley, West Midlands", "Sutton New Hall", "Boldmere", "Soho, West Midlands", "Brownhills West", "Fordbridge", "Bishopgate Green", "Hamstead, West Midlands", "Willenhall", "Kings Heath", "Moseley and Kings Heath", "Blackheath", "Barston", "Wednesbury", "Oscott", "City College, Birmingham", "Telford and Wrekin", "Ladywood", "Bournbrook", "Dudley College", "California, Birmingham", "Lyndon School, Solihull", "Turners Hill, West Midlands", "Knowle, West Midlands", "Handsworth, West Midlands", "Herefordshire", "Wednesfield", "Stourton", "Kingstanding", "Walsall College, Wisemore campus", "Tettenhall", "Pendeford", "Allesley", "The Sixth Form College, Solihull", "Whitmore Reans", "Redhill School, Stourbridge", "South Yardley", "Aston", "Balsall Common", "Bromford", "Wollaston", "Woodcross", "Sandwell College, Oldbury", "Amblecote", "Stockland Green", "New Frankley", "Stechford and Yardley North", "Bradley, West Midlands", "Sedgley", "Mary Arden's Farm", "Norton, Dudley", "Sandwell", "Gorsebrook", "Oxley, Wolverhampton", "University of Wolverhampton, City Campus", "Coseley", "Redditch", "Bloxwich", "Dudley", "Bilston", "Lozells and East Handsworth", "Walsall Wood", "West Midlands", "The Public, West Bromwich", "Woodsetton, Dudley", "Washwood Heath", "Stourbridge", "Tyburn, West Midlands", "Walsall College, Green Lane campus", "Metropolitan Borough of Walsall", "Sandwell College, West Bromwich", "Smith's Wood", "Staffordshire", "Wordsley", "Druids Heath", "Marston Green", "Coventry University", "Smethwick", "Saltley", "Bickenhill", "St Francis of Assisi Catholic Technology College", "Walmley", "Castlecroft", "Radford, Coventry", "Sheldon, West Midlands", "West Bromwich", "Brandwood End", "Pensnett", "Metropolitan Borough of Dudley", "Castle Vale", "Warwickshire", "Darlaston", "Streetly", "Moor Pool", "Hampton-in-Arden", "Walsall", "Coventry", "Berkswell"], "ans_acc": 0.01904761904761905, "ans_hit": 1, "ans_f1": 0.018779342723004695, "ans_precission": 0.6666666666666666, "ans_recall": 0.009523809523809525, "path_f1": 0.12, "path_precision": 1.0, "path_recall": 0.06382978723404255, "path_ans_f1": 0.03738317757009346, "path_ans_precision": 1.0, "path_ans_recall": 0.01904761904761905}
{"id": "WebQTest-1433", "prediction": ["# Reasoning Path:\nAsheville -> travel.travel_destination.tourist_attractions -> Carl Sandburg Home National Historic Site -> common.topic.image -> CarlSandburgNHS\n# Answer:\nCarl Sandburg Home National Historic Site", "# Reasoning Path:\nAsheville -> travel.travel_destination.tourist_attractions -> Blue Ridge Parkway -> travel.tourist_attraction.near_travel_destination -> Roanoke\n# Answer:\nBlue Ridge Parkway", "# Reasoning Path:\nAsheville -> travel.travel_destination.tourist_attractions -> Carl Sandburg Home National Historic Site -> protected_sites.protected_site.governing_body -> U.S. National Park Service\n# Answer:\nCarl Sandburg Home National Historic Site", "# Reasoning Path:\nAsheville -> location.statistical_region.population -> g.11b66ff3kh\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nAsheville -> travel.travel_destination.tourist_attractions -> Blue Ridge Parkway -> transportation.road.end2 -> m.0123xlpk\n# Answer:\nBlue Ridge Parkway", "# Reasoning Path:\nAsheville -> travel.travel_destination.tourist_attractions -> Blue Ridge Parkway -> common.topic.notable_types -> Tourist attraction\n# Answer:\nBlue Ridge Parkway", "# Reasoning Path:\nAsheville -> travel.travel_destination.tourist_attractions -> Thomas Wolfe House -> location.location.containedby -> North Carolina\n# Answer:\nThomas Wolfe House", "# Reasoning Path:\nAsheville -> location.statistical_region.population -> g.11bc85ny0m\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nAsheville -> travel.travel_destination.tourist_attractions -> Thomas Wolfe House -> base.usnris.nris_listing.significant_person -> Thomas Wolfe\n# Answer:\nThomas Wolfe House", "# Reasoning Path:\nAsheville -> location.statistical_region.population -> g.11x1cr6qs\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["Smith-McDowell House", "Biltmore Estate", "Folk Art Center", "Blue Ridge Parkway", "The Omni Grove Park Inn", "Asheville Art Museum", "Pisgah National Forest", "Bele Chere", "Carl Sandburg Home National Historic Site", "Thomas Wolfe House"], "ans_acc": 0.3, "ans_hit": 1, "ans_f1": 0.42, "ans_precission": 0.7, "ans_recall": 0.3, "path_f1": 0.42, "path_precision": 0.7, "path_recall": 0.3, "path_ans_f1": 0.42, "path_ans_precision": 0.7, "path_ans_recall": 0.3}
{"id": "WebQTest-1434", "prediction": ["# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Washington Senators -> sports.sports_team.previously_known_as -> Kansas City Blues\n# Answer:\nWashington Senators", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Texas Rangers -> baseball.baseball_team.current_manager -> Tim Bogar\n# Answer:\nTexas Rangers", "# Reasoning Path:\nAmerican League -> business.employer.employees -> g.11b88cjhcb\n# Answer:\nbusiness.employer.employees", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Washington Senators -> baseball.baseball_team.team_stats -> m.05n5_09\n# Answer:\nWashington Senators", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Washington Senators -> sports.defunct_sports_team.active -> m.05lkx23\n# Answer:\nWashington Senators", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Washington Senators -> baseball.baseball_team.team_stats -> m.05n5_28\n# Answer:\nWashington Senators", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Texas Rangers -> sports.professional_sports_team.draft_picks -> m.04vw_l5\n# Answer:\nTexas Rangers", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Washington Senators -> baseball.baseball_team.team_stats -> m.05n5_3c\n# Answer:\nWashington Senators", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Washington Senators -> sports.defunct_sports_team.active -> m.05ll108\n# Answer:\nWashington Senators", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Texas Rangers -> baseball.baseball_team.team_stats -> m.05lk3qd\n# Answer:\nTexas Rangers"], "ground_truth": ["Cleveland Indians", "Milwaukee Brewers", "Houston Astros", "Tampa Bay Rays", "Detroit Tigers", "Minnesota Twins", "Kansas City Royals", "Seattle Mariners", "Boston Red Sox", "Anaheim Angels", "Toronto Blue Jays", "Philadelphia Athletics", "California Angels", "Seattle Pilots", "Cleveland Naps", "New York Yankees", "Tampa Bay Devil Rays", "Baltimore Orioles", "Kansas City Athletics", "Texas Rangers", "Oakland Athletics", "Chicago White Sox", "New York Highlanders", "Los Angeles Angels of Anaheim", "Washington Senators", "St. Louis Browns", "Chicago White Stockings"], "ans_acc": 0.07407407407407407, "ans_hit": 1, "ans_f1": 0.13688212927756654, "ans_precission": 0.9, "ans_recall": 0.07407407407407407, "path_f1": 0.12413793103448276, "path_precision": 0.9, "path_recall": 0.06666666666666667, "path_ans_f1": 0.13688212927756654, "path_ans_precision": 0.9, "path_ans_recall": 0.07407407407407407}
{"id": "WebQTest-1435", "prediction": ["# Reasoning Path:\nKurt Warner -> sports.pro_athlete.teams -> m.045wt3q -> sports.sports_team_roster.team -> St. Louis Rams\n# Answer:\nSt. Louis Rams", "# Reasoning Path:\nKurt Warner -> sports.pro_athlete.teams -> m.0hprztb -> sports.sports_team_roster.team -> Arizona Cardinals\n# Answer:\nArizona Cardinals", "# Reasoning Path:\nKurt Warner -> sports.pro_athlete.teams -> m.05bd999 -> sports.sports_team_roster.team -> New York Giants\n# Answer:\nNew York Giants", "# Reasoning Path:\nKurt Warner -> sports.pro_athlete.teams -> m.045wt3q -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nKurt Warner -> american_football.football_player.rushing -> m.09tcw0f -> american_football.player_rushing_statistics.team -> Arizona Cardinals\n# Answer:\nArizona Cardinals", "# Reasoning Path:\nKurt Warner -> sports.pro_athlete.teams -> m.0hprztb -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nKurt Warner -> american_football.football_player.rushing -> m.0791hhh -> american_football.player_rushing_statistics.team -> Arizona Cardinals\n# Answer:\nArizona Cardinals", "# Reasoning Path:\nKurt Warner -> sports.pro_athlete.teams -> m.05bd999 -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nKurt Warner -> american_football.football_player.rushing -> m.09tcw0f -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nKurt Warner -> american_football.football_player.rushing -> m.07mmqb9 -> american_football.player_rushing_statistics.team -> Arizona Cardinals\n# Answer:\nArizona Cardinals"], "ground_truth": ["Iowa Barnstormers", "New York Giants", "Northern Iowa Panthers football", "Amsterdam Admirals", "Arizona Cardinals", "St. Louis Rams"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.6, "ans_recall": 0.5, "path_f1": 0.17391304347826086, "path_precision": 0.6, "path_recall": 0.1016949152542373, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.6, "path_ans_recall": 0.5}
{"id": "WebQTest-1436", "prediction": ["# Reasoning Path:\nDirk Nowitzki -> people.person.children -> Malaika Nowitzki -> common.topic.notable_for -> g.11b75n3xz8\n# Answer:\nMalaika Nowitzki", "# Reasoning Path:\nDirk Nowitzki -> people.person.spouse_s -> m.0r8dkzz -> people.marriage.spouse -> Jessica Olsson\n# Answer:\nJessica Olsson", "# Reasoning Path:\nDirk Nowitzki -> people.person.children -> Malaika Nowitzki -> common.topic.notable_types -> Person\n# Answer:\nMalaika Nowitzki", "# Reasoning Path:\nDirk Nowitzki -> people.person.spouse_s -> m.0r8dkzz -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nDirk Nowitzki -> people.person.spouse_s -> m.0r8dkzz -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nDirk Nowitzki -> award.award_winner.awards_won -> m.010f1rwg -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nWinning work", "# Reasoning Path:\nDirk Nowitzki -> award.award_winner.awards_won -> m.0_qrbz6 -> award.award_honor.award -> Bill Russell NBA Finals Most Valuable Player Award\n# Answer:\nBill Russell NBA Finals Most Valuable Player Award", "# Reasoning Path:\nDirk Nowitzki -> award.award_winner.awards_won -> m.010f1rwg -> award.award_honor.award -> Magic Johnson Award\n# Answer:\nMagic Johnson Award", "# Reasoning Path:\nDirk Nowitzki -> award.award_winner.awards_won -> m.0_qrln8 -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nWinning work", "# Reasoning Path:\nDirk Nowitzki -> award.award_winner.awards_won -> m.010f1rwg -> freebase.valuenotation.has_no_value -> Ceremony\n# Answer:\nCeremony"], "ground_truth": ["Jessica Olsson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1441", "prediction": ["# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Eug\u00e8ne Delacroix -> people.deceased_person.place_of_death -> Paris\n# Answer:\nEug\u00e8ne Delacroix", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Eug\u00e8ne Delacroix -> influence.influence_node.influenced -> \u00c9douard Manet\n# Answer:\nEug\u00e8ne Delacroix", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> \u00c9douard Manet -> influence.influence_node.influenced_by -> Peter Paul Rubens\n# Answer:\n\u00c9douard Manet", "# Reasoning Path:\nPablo Picasso -> people.person.places_lived -> m.0wfdzk8 -> people.place_lived.location -> Madrid\n# Answer:\nMadrid", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Eug\u00e8ne Delacroix -> book.author.school_or_movement -> Romanticism\n# Answer:\nEug\u00e8ne Delacroix", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> \u00c9douard Manet -> influence.influence_node.influenced -> Paul C\u00e9zanne\n# Answer:\n\u00c9douard Manet", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Eug\u00e8ne Delacroix -> influence.influence_node.influenced -> Camille Pissarro\n# Answer:\nEug\u00e8ne Delacroix", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> \u00c9douard Manet -> influence.influence_node.influenced_by -> Diego Vel\u00e1zquez\n# Answer:\n\u00c9douard Manet", "# Reasoning Path:\nPablo Picasso -> people.person.places_lived -> m.04yt_9c -> people.place_lived.location -> M\u00e1laga\n# Answer:\nM\u00e1laga", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Eug\u00e8ne Delacroix -> influence.influence_node.influenced -> Claude Monet\n# Answer:\nEug\u00e8ne Delacroix"], "ground_truth": ["France", "Madrid", "M\u00e1laga"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.30769230769230765, "ans_precission": 0.2, "ans_recall": 0.6666666666666666, "path_f1": 0.05714285714285714, "path_precision": 0.1, "path_recall": 0.04, "path_ans_f1": 0.30769230769230765, "path_ans_precision": 0.2, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1442", "prediction": ["# Reasoning Path:\nCroatia -> location.country.currency_used -> Croatian kuna -> common.topic.article -> m.02z8k3\n# Answer:\nCroatian kuna", "# Reasoning Path:\nCroatia -> common.topic.notable_for -> g.1256bxl73\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCroatia -> location.country.currency_used -> Croatian kuna -> common.topic.notable_types -> Currency\n# Answer:\nCroatian kuna", "# Reasoning Path:\nCroatia -> location.country.currency_used -> Croatian kuna -> common.topic.webpage -> m.04m1ls_\n# Answer:\nCroatian kuna", "# Reasoning Path:\nCroatia -> location.statistical_region.gdp_growth_rate -> g.11b60t8rf2\n# Answer:\nlocation.statistical_region.gdp_growth_rate", "# Reasoning Path:\nCroatia -> location.country.currency_used -> Croatian kuna -> common.topic.webpage -> m.04m1lsv\n# Answer:\nCroatian kuna", "# Reasoning Path:\nCroatia -> location.statistical_region.gdp_growth_rate -> g.1245_1jbw\n# Answer:\nlocation.statistical_region.gdp_growth_rate", "# Reasoning Path:\nCroatia -> location.country.currency_used -> Croatian kuna -> common.topic.webpage -> m.05nnkw1\n# Answer:\nCroatian kuna", "# Reasoning Path:\nCroatia -> location.statistical_region.gdp_growth_rate -> g.1245z0pzv\n# Answer:\nlocation.statistical_region.gdp_growth_rate"], "ground_truth": ["Croatian kuna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-1443", "prediction": ["# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> baseball.baseball_player.batting_stats -> m.06qyjc3 -> baseball.batting_statistics.team -> Seattle Mariners\n# Answer:\nSeattle Mariners", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> baseball.baseball_player.batting_stats -> m.06qyjdq -> baseball.batting_statistics.team -> Seattle Mariners\n# Answer:\nSeattle Mariners", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> baseball.baseball_player.batting_stats -> m.06qyjg3 -> baseball.batting_statistics.team -> Seattle Mariners\n# Answer:\nSeattle Mariners", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> sports.pro_athlete.teams -> m.0zs_gld -> sports.sports_team_roster.team -> Los Angeles Angels of Anaheim\n# Answer:\nLos Angeles Angels of Anaheim", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> baseball.baseball_player.batting_stats -> m.06qyjc3 -> baseball.batting_statistics.season -> 1996 Major League Baseball Season\n# Answer:\n1996 Major League Baseball Season", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> baseball.baseball_player.batting_stats -> m.06qyjdq -> baseball.batting_statistics.season -> 1997 Major League Baseball Season\n# Answer:\n1997 Major League Baseball Season", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> sports.pro_athlete.teams -> m.0zs_gld -> sports.sports_team_roster.position -> Outfielder\n# Answer:\nOutfielder", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> sports.pro_athlete.teams -> m.01118sn3 -> sports.sports_team_roster.team -> Kansas City Royals\n# Answer:\nKansas City Royals", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> sports.pro_athlete.teams -> m.0j2gtzk -> sports.sports_team_roster.team -> Philadelphia Phillies\n# Answer:\nPhiladelphia Phillies", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale"], "ground_truth": ["Kansas City Royals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.13333333333333333, "path_precision": 0.1, "path_recall": 0.2, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1445", "prediction": ["# Reasoning Path:\nMark Harmon -> award.award_nominee.award_nominations -> m.090f7v5 -> award.award_nomination.nominated_for -> Reasonable Doubts\n# Answer:\nReasonable Doubts", "# Reasoning Path:\nMark Harmon -> award.award_nominee.award_nominations -> m.0n4mxrn -> award.award_nomination.nominated_for -> NCIS\n# Answer:\nNCIS", "# Reasoning Path:\nMark Harmon -> award.award_nominee.award_nominations -> m.090f7v5 -> award.award_nomination.ceremony -> 50th Golden Globe Awards\n# Answer:\n50th Golden Globe Awards", "# Reasoning Path:\nMark Harmon -> award.award_nominee.award_nominations -> m.090f7v5 -> award.award_nomination.award -> Golden Globe Award for Best Performance by an Actor in a Television Series \u2013 Drama\n# Answer:\nGolden Globe Award for Best Performance by an Actor in a Television Series \u2013 Drama", "# Reasoning Path:\nMark Harmon -> award.award_nominee.award_nominations -> m.07t74mx -> award.award_nomination.nominated_for -> The West Wing\n# Answer:\nThe West Wing", "# Reasoning Path:\nMark Harmon -> tv.tv_actor.starring_roles -> m.0bmqqd2 -> tv.regular_tv_appearance.series -> Centennial\n# Answer:\nCentennial", "# Reasoning Path:\nMark Harmon -> award.award_nominee.award_nominations -> m.0n4mxrn -> award.award_nomination.ceremony -> 37th People's Choice Awards\n# Answer:\n37th People's Choice Awards", "# Reasoning Path:\nMark Harmon -> award.award_nominee.award_nominations -> m.0n4mxrn -> award.award_nomination.award -> People's Choice Award for Favorite TV Crime Fighter\n# Answer:\nPeople's Choice Award for Favorite TV Crime Fighter", "# Reasoning Path:\nMark Harmon -> tv.tv_actor.starring_roles -> m.02t8rt7 -> tv.regular_tv_appearance.series -> Flamingo Road\n# Answer:\nFlamingo Road", "# Reasoning Path:\nMark Harmon -> award.award_nominee.award_nominations -> m.07t74mx -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Guest Actor in a Drama Series\n# Answer:\nPrimetime Emmy Award for Outstanding Guest Actor in a Drama Series"], "ground_truth": ["Flamingo Road", "St. Elsewhere", "NCIS", "Eleanor and Franklin: The White House Years", "Centennial", "Sam", "Reasonable Doubts", "240-Robert", "From the Earth to the Moon", "Chicago Hope", "The Dream Merchants", "Charlie Grace"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.4, "ans_recall": 0.3333333333333333, "path_f1": 0.27586206896551724, "path_precision": 0.4, "path_recall": 0.21052631578947367, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.4, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1446", "prediction": ["# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> location.location.events -> 2005 SoCal Independent Film Festival\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> location.location.containedby -> Orange County\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> location.location.events -> 2014 SoCal Independent Film Festival\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> location.location.containedby -> Area code 657\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> location.location.containedby -> Area code 714\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> location.location.events -> 2006 SoCal Independent Film Festival\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> location.location.contains -> Huntington Beach Pier\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> location.location.contains -> Apollos University\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> common.topic.webpage -> m.03lc2g9 -> common.webpage.resource -> m.0bk4f79\n# Answer:\ncommon.webpage.resource"], "ground_truth": ["2009-12-28"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1447", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5rn -> film.performance.actor -> Jake Lloyd\n# Answer:\nJake Lloyd", "# Reasoning Path:\nStar Wars Episode I: The Phantom Menace -> film.film.starring -> m.02sg5rn -> film.performance.actor -> Jake Lloyd\n# Answer:\nJake Lloyd", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.010wvb1d -> film.performance.actor -> Abraham Benrubi\n# Answer:\nAbraham Benrubi", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5rn -> film.performance.film -> Star Wars Episode I: The Phantom Menace\n# Answer:\nfilm.performance.film", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0113l35j -> film.performance.actor -> Michael Fox\n# Answer:\nMichael Fox", "# Reasoning Path:\nStar Wars Episode I: The Phantom Menace -> film.film.starring -> m.02sg5rn -> film.performance.character -> Darth Vader\n# Answer:\nfilm.performance.character", "# Reasoning Path:\nStar Wars Episode I: The Phantom Menace -> film.film.starring -> m.0130_9qs -> film.performance.actor -> Peter Serafinowicz\n# Answer:\nPeter Serafinowicz", "# Reasoning Path:\nStar Wars Episode I: The Phantom Menace -> award.award_nominated_work.award_nominations -> m.0pllr38 -> award.award_nomination.award_nominee -> Jake Lloyd\n# Answer:\nJake Lloyd", "# Reasoning Path:\nStar Wars Episode I: The Phantom Menace -> film.film.starring -> m.025dlyw -> film.performance.actor -> Samuel L. Jackson\n# Answer:\nSamuel L. Jackson", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.010wvb1d -> film.performance.film -> Robot Chicken: Star Wars Episode III\n# Answer:\nRobot Chicken: Star Wars Episode III"], "ground_truth": ["Jake Lloyd"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1448", "prediction": ["# Reasoning Path:\nMichelle Pfeiffer -> people.person.place_of_birth -> Santa Ana -> location.location.containedby -> United States of America\n# Answer:\nSanta Ana", "# Reasoning Path:\nMichelle Pfeiffer -> people.person.places_lived -> m.04hdch1 -> people.place_lived.location -> Orange County\n# Answer:\nOrange County", "# Reasoning Path:\nMichelle Pfeiffer -> people.person.place_of_birth -> Santa Ana -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nSanta Ana", "# Reasoning Path:\nMichelle Pfeiffer -> people.person.place_of_birth -> Santa Ana -> location.location.containedby -> California\n# Answer:\nSanta Ana", "# Reasoning Path:\nMichelle Pfeiffer -> people.person.place_of_birth -> Santa Ana -> location.location.containedby -> Orange County\n# Answer:\nSanta Ana", "# Reasoning Path:\nMichelle Pfeiffer -> award.award_nominee.award_nominations -> m.021yhcs -> award.award_nomination.award -> Academy Award for Best Actress\n# Answer:\nAcademy Award for Best Actress", "# Reasoning Path:\nMichelle Pfeiffer -> award.award_nominee.award_nominations -> m.05brbfm -> award.award_nomination.award -> Academy Award for Best Actress\n# Answer:\nAcademy Award for Best Actress", "# Reasoning Path:\nMichelle Pfeiffer -> award.award_nominee.award_nominations -> m.021yhcs -> award.award_nomination.ceremony -> 62nd Academy Awards\n# Answer:\n62nd Academy Awards", "# Reasoning Path:\nMichelle Pfeiffer -> award.award_nominee.award_nominations -> m.05c7j74 -> award.award_nomination.nominated_for -> Dangerous Liaisons\n# Answer:\nDangerous Liaisons", "# Reasoning Path:\nMichelle Pfeiffer -> award.award_nominee.award_nominations -> m.05brbfm -> award.award_nomination.nominated_for -> Love Field\n# Answer:\nLove Field"], "ground_truth": ["Orange County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1449", "prediction": ["# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Writer -> people.profession.specializations -> Novelist\n# Answer:\nWriter", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Teacher -> common.topic.notable_types -> Profession\n# Answer:\nTeacher", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Writer -> people.profession.specializations -> Author\n# Answer:\nWriter", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Writer -> common.topic.article -> m.0cbdb\n# Answer:\nWriter", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Writer -> comic_strips.comic_strip_creator_role.creators -> m.0102ldp_\n# Answer:\nWriter", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Writer -> people.profession.specializations -> Bard\n# Answer:\nWriter", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Writer -> comic_strips.comic_strip_creator_role.creators -> m.02_6g4l\n# Answer:\nWriter", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Writer -> comic_strips.comic_strip_creator_role.creators -> m.02hrdmx\n# Answer:\nWriter", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Author -> tv.tv_subject.tv_episodes -> Author David Kerr - The Ol' Turkey Hunter\n# Answer:\nAuthor", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor"], "ground_truth": ["Journalist", "Author", "Writer", "Teacher"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.6153846153846154, "path_precision": 0.8, "path_recall": 0.5, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-145", "prediction": ["# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k05ps4 -> education.education.institution -> University of Zurich\n# Answer:\nUniversity of Zurich", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k03_k5 -> education.education.institution -> ETH Zurich\n# Answer:\nETH Zurich", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0j_z4g5 -> education.education.institution -> Luitpold Gymnasium\n# Answer:\nLuitpold Gymnasium", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k05ps4 -> freebase.valuenotation.has_value -> Minor\n# Answer:\nMinor", "# Reasoning Path:\nAlbert Einstein -> people.person.employment_history -> m.02kq3y4 -> business.employment_tenure.company -> University of Zurich\n# Answer:\nUniversity of Zurich", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k05ps4 -> freebase.valuenotation.is_reviewed -> Minor\n# Answer:\nMinor", "# Reasoning Path:\nAlbert Einstein -> people.person.employment_history -> m.02kq4ch -> business.employment_tenure.company -> Leiden University\n# Answer:\nLeiden University", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k05ps4 -> freebase.valuenotation.has_value -> Specialization\n# Answer:\nSpecialization", "# Reasoning Path:\nAlbert Einstein -> people.person.employment_history -> m.02kq3y4 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k03_k5 -> education.education.degree -> Bachelor of Science\n# Answer:\nBachelor of Science"], "ground_truth": ["University of Zurich", "ETH Zurich"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.3, "path_recall": 0.6, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1452", "prediction": ["# Reasoning Path:\nDwight Howard -> people.person.place_of_birth -> Atlanta -> base.biblioness.bibs_location.state -> Georgia\n# Answer:\nAtlanta", "# Reasoning Path:\nDwight Howard -> sports.pro_athlete.teams -> m.0w7fk1d -> sports.sports_team_roster.team -> Houston Rockets\n# Answer:\nHouston Rockets", "# Reasoning Path:\nDwight Howard -> people.person.place_of_birth -> Atlanta -> location.location.containedby -> Georgia\n# Answer:\nAtlanta", "# Reasoning Path:\nDwight Howard -> people.person.place_of_birth -> Atlanta -> location.hud_county_place.county -> Fulton County\n# Answer:\nAtlanta", "# Reasoning Path:\nDwight Howard -> people.person.place_of_birth -> Atlanta -> location.location.containedby -> United States of America\n# Answer:\nAtlanta", "# Reasoning Path:\nDwight Howard -> sports.pro_athlete.teams -> m.0w7fk1d -> sports.sports_team_roster.position -> Power forward\n# Answer:\nPower forward", "# Reasoning Path:\nDwight Howard -> sports.pro_athlete.teams -> m.0kq8zyx -> sports.sports_team_roster.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nDwight Howard -> tv.tv_actor.guest_roles -> m.0g97mjw -> tv.tv_guest_role.episodes_appeared_in -> A Short Story and a Tall Tale\n# Answer:\nA Short Story and a Tall Tale", "# Reasoning Path:\nDwight Howard -> people.person.place_of_birth -> Atlanta -> location.location.containedby -> Area code 404\n# Answer:\nAtlanta", "# Reasoning Path:\nDwight Howard -> sports.pro_athlete.teams -> m.0kq8z4b -> sports.sports_team_roster.team -> Orlando Magic\n# Answer:\nOrlando Magic"], "ground_truth": ["Houston Rockets"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1453", "prediction": ["# Reasoning Path:\nNick Lachey -> people.person.children -> Camden John Lachey -> people.person.parents -> Vanessa Lachey\n# Answer:\nCamden John Lachey", "# Reasoning Path:\nNick Lachey -> people.person.spouse_s -> m.07sl20z -> people.marriage.spouse -> Jessica Simpson\n# Answer:\nJessica Simpson", "# Reasoning Path:\nNick Lachey -> people.person.children -> Camden John Lachey -> common.topic.notable_for -> g.1259plkqz\n# Answer:\nCamden John Lachey", "# Reasoning Path:\nNick Lachey -> people.person.children -> Camden John Lachey -> common.topic.notable_types -> Person\n# Answer:\nCamden John Lachey", "# Reasoning Path:\nNick Lachey -> people.person.spouse_s -> m.0gyp51b -> people.marriage.spouse -> Vanessa Lachey\n# Answer:\nVanessa Lachey", "# Reasoning Path:\nNick Lachey -> people.person.spouse_s -> m.07sl20z -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nNick Lachey -> people.person.spouse_s -> m.07sl20z -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nNick Lachey -> people.person.spouse_s -> m.0gyp51b -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nNick Lachey -> music.artist.track_contributions -> m.01389mpb -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nNick Lachey -> people.person.spouse_s -> m.0gyp51b -> people.marriage.location_of_ceremony -> Necker Island\n# Answer:\nNecker Island"], "ground_truth": ["Vanessa Lachey"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1454", "prediction": ["# Reasoning Path:\nMadonna -> people.person.place_of_birth -> Bay City -> location.location.containedby -> United States of America\n# Answer:\nBay City", "# Reasoning Path:\nMadonna -> people.person.place_of_birth -> Bay City -> location.location.people_born_here -> Madonna Louise Ciccone\n# Answer:\nBay City", "# Reasoning Path:\nMadonna -> people.person.place_of_birth -> Bay City -> location.location.people_born_here -> Christopher Ciccone\n# Answer:\nBay City", "# Reasoning Path:\nMadonna -> music.composer.compositions -> Die Another Day -> music.composition.composer -> Mirwais Ahmadza\u00ef\n# Answer:\nDie Another Day", "# Reasoning Path:\nMadonna -> music.composer.compositions -> This Used to Be My Playground -> music.composition.composer -> Shep Pettibone\n# Answer:\nThis Used to Be My Playground", "# Reasoning Path:\nMadonna -> music.composer.compositions -> 4 Minutes -> music.composition.composer -> Timbaland\n# Answer:\n4 Minutes", "# Reasoning Path:\nMadonna -> music.composer.compositions -> This Used to Be My Playground -> award.award_nominated_work.award_nominations -> m.0b9wlfc\n# Answer:\nThis Used to Be My Playground", "# Reasoning Path:\nMadonna -> base.popstra.celebrity.dated -> m.064_jxy -> base.popstra.dated.participant -> Chris Rock\n# Answer:\nChris Rock", "# Reasoning Path:\nMadonna -> music.composer.compositions -> Die Another Day -> music.composition.recordings -> Die Another Day (radio edit)\n# Answer:\nDie Another Day", "# Reasoning Path:\nMadonna -> music.composer.compositions -> Die Another Day -> common.topic.notable_types -> Film\n# Answer:\nDie Another Day"], "ground_truth": ["Bay City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1455", "prediction": ["# Reasoning Path:\nBrazil -> location.country.capital -> Bras\u00edlia -> location.location.containedby -> Federal District\n# Answer:\nBras\u00edlia", "# Reasoning Path:\nBrazil -> location.country.capital -> Bras\u00edlia -> base.biblioness.bibs_location.city -> Federal District\n# Answer:\nBras\u00edlia", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc39sjp\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fm01\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc385hl\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp"], "ground_truth": ["Bras\u00edlia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1456", "prediction": ["# Reasoning Path:\nJohn Stockton -> people.person.education -> m.04hd6_9 -> education.education.institution -> Gonzaga University\n# Answer:\nGonzaga University", "# Reasoning Path:\nJohn Stockton -> people.person.education -> m.0n1l1ft -> education.education.institution -> Gonzaga Preparatory School\n# Answer:\nGonzaga Preparatory School", "# Reasoning Path:\nJohn Stockton -> award.hall_of_fame_inductee.hall_of_fame_inductions -> m.0bjck8b -> award.hall_of_fame_induction.category -> Basketball player\n# Answer:\nBasketball player", "# Reasoning Path:\nJohn Stockton -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nJohn Stockton -> award.hall_of_fame_inductee.hall_of_fame_inductions -> m.0bjck8b -> award.hall_of_fame_induction.hall_of_fame -> Naismith Memorial Basketball Hall of Fame\n# Answer:\nNaismith Memorial Basketball Hall of Fame", "# Reasoning Path:\nJohn Stockton -> freebase.valuenotation.is_reviewed -> Date of birth -> type.property.schema -> Person\n# Answer:\nDate of birth", "# Reasoning Path:\nJohn Stockton -> freebase.valuenotation.is_reviewed -> Gender -> type.property.schema -> Person\n# Answer:\nGender", "# Reasoning Path:\nJohn Stockton -> people.person.education -> m.0n1l1ft -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nJohn Stockton -> freebase.valuenotation.is_reviewed -> Date of birth -> type.property.expected_type -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nJohn Stockton -> freebase.valuenotation.is_reviewed -> Place of birth -> type.property.schema -> Person\n# Answer:\nPlace of birth"], "ground_truth": ["Gonzaga University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1457", "prediction": ["# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_0s31\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.11b60v4rn6\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_38p7\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_569j\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.12tb6hbx9\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.1hhc378k5\n# Answer:\nlocation.statistical_region.cpi_inflation_rate"], "ground_truth": ["Mexican peso"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1458", "prediction": ["# Reasoning Path:\nChrysler Group LLC -> organization.organization.parent -> m.01322yt1 -> organization.organization_relationship.parent -> Fiat Chrysler Automobiles\n# Answer:\nFiat Chrysler Automobiles", "# Reasoning Path:\nChrysler Group LLC -> organization.organization.parent -> m.0gb6_f8 -> organization.organization_relationship.parent -> Government of Canada\n# Answer:\nGovernment of Canada", "# Reasoning Path:\nChrysler Group LLC -> organization.organization.parent -> m.04kjv4n -> organization.organization_relationship.parent -> Cerberus Capital Management\n# Answer:\nCerberus Capital Management", "# Reasoning Path:\nChrysler Group LLC -> automotive.company.make_s -> Plymouth -> base.cars_refactor.make.model_s -> Plymouth Acclaim\n# Answer:\nPlymouth", "# Reasoning Path:\nChrysler Group LLC -> automotive.company.make_s -> Plymouth -> common.topic.notable_for -> g.125773lm0\n# Answer:\nPlymouth", "# Reasoning Path:\nChrysler Group LLC -> award.award_winner.awards_won -> m.04_j175 -> award.award_honor.award -> Motor Trend Car of the Year\n# Answer:\nMotor Trend Car of the Year", "# Reasoning Path:\nChrysler Group LLC -> automotive.company.make_s -> Plymouth -> base.cars_refactor.make.model_s -> Plymouth Backpack\n# Answer:\nPlymouth", "# Reasoning Path:\nChrysler Group LLC -> automotive.company.make_s -> Plymouth -> automotive.make.model_s -> Plymouth AAR 'Cuda\n# Answer:\nPlymouth", "# Reasoning Path:\nChrysler Group LLC -> automotive.company.make_s -> Chrysler -> automotive.make.model_s -> AMC Eagle\n# Answer:\nChrysler", "# Reasoning Path:\nChrysler Group LLC -> automotive.company.make_s -> Chrysler -> base.schemastaging.entity_hint.corresponds_to -> m.0y4l_9z\n# Answer:\nChrysler"], "ground_truth": ["Fiat Chrysler Automobiles", "Government of Canada", "Federal government of the United States", "Fiat S.p.A.", "Voluntary Employee Beneficiary Association"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.26666666666666666, "ans_precission": 0.2, "ans_recall": 0.4, "path_f1": 0.26666666666666666, "path_precision": 0.2, "path_recall": 0.4, "path_ans_f1": 0.26666666666666666, "path_ans_precision": 0.2, "path_ans_recall": 0.4}
{"id": "WebQTest-1459", "prediction": ["# Reasoning Path:\nMark Twain -> film.film_story_contributor.film_story_credits -> The Man That Corrupted Hadleyburg -> film.film.edited_by -> Jay Freund\n# Answer:\nThe Man That Corrupted Hadleyburg", "# Reasoning Path:\nMark Twain -> people.person.children -> Clara Clemens -> book.author.works_written -> My father\n# Answer:\nClara Clemens", "# Reasoning Path:\nMark Twain -> film.film_story_contributor.film_story_credits -> The Prince and the Pauper -> book.written_work.next_in_series -> Life on the Mississippi\n# Answer:\nThe Prince and the Pauper", "# Reasoning Path:\nMark Twain -> film.film_story_contributor.film_story_credits -> The Man That Corrupted Hadleyburg -> media_common.netflix_title.netflix_genres -> Drama\n# Answer:\nThe Man That Corrupted Hadleyburg", "# Reasoning Path:\nMark Twain -> film.film_story_contributor.film_story_credits -> A Connecticut Yankee in King Arthur's Court -> film.film.starring -> m.012zhwvg\n# Answer:\nA Connecticut Yankee in King Arthur's Court", "# Reasoning Path:\nMark Twain -> people.person.children -> Clara Clemens -> book.author.works_written -> My husband, Gabrilowitsch\n# Answer:\nClara Clemens", "# Reasoning Path:\nMark Twain -> people.person.children -> Clara Clemens -> people.person.children -> Nina  Gabrilowitsc\n# Answer:\nClara Clemens", "# Reasoning Path:\nMark Twain -> people.person.children -> Jean Clemens -> people.person.parents -> Olivia Langdon Clemens\n# Answer:\nJean Clemens", "# Reasoning Path:\nMark Twain -> film.film_story_contributor.film_story_credits -> The Man That Corrupted Hadleyburg -> media_common.netflix_title.netflix_genres -> Dramas Based on Classic Literature\n# Answer:\nThe Man That Corrupted Hadleyburg", "# Reasoning Path:\nMark Twain -> film.film_story_contributor.film_story_credits -> A Connecticut Yankee in King Arthur's Court -> film.film.genre -> Adventure Film\n# Answer:\nA Connecticut Yankee in King Arthur's Court"], "ground_truth": ["Tom Sawyer Detective (Austral Juvenil)", "Adventures of Tom Sawyer (Fiction)", "Tom Sawyer, Detective (Dover Evergreen Classics)", "Adventures of Huckleberry Finn (Tom Sawyer's comrade)", "The Innocents Abroad (1869) (The Oxford Mark Twain)", "A Tramp Abroad (Large Print Edition)", "The Prince and the Pauper (Webster's Chinese-Simplified Thesaurus Edition)", "Adventures of Tom Sawyer (Streamline Books)", "Roughing it", "The adventures of Huckleberry Finn", "The Adventures of Tom Sawyer and The Adventures of Huckleberry Finn (Signet Classics)", "Roughing It (Mark Twain Library)", "A Tramp Abroad (Konemann Classics)", "Roughing It (Works of Mark Twain, Volume One)", "Sketches New and Old", "Personal Recollections of Joan of Arc", "The Adventures of Tom Sawyer, (Classic Books on CDs) [UNABRIDGED] (Classic Books on CD)", "Adventures of Tom Sawyer GB", "How to tell a story, and other essays", "The Adventures of Tom Sawyer and The Adventures of Huckleberry Finn (Everyman's Library)", "Adventures of Tom Sawyer (Penguin Classics)", "Adventures of Tom Sawyer (Everyman's Classics S.)", "The Adventures of Huckleberry Finn CD set (Cambridge Literature)", "Tom Sawyer, detective", "The American Claimant", "Adventures of Huckleberry Finn (1885) (The Oxford Mark Twain)", "A Connecticut Yankee in King Arthur's Court, Set", "The Innocents Abroad (Classic Books on Cassettes Collection)", "A Connecticut Yankee in King Arthur's Court (Penguin Classics)", "Tom Sawyer Abroad (Watermill Classic)", "Adventures Of Tom Sawyer (Whole Story)", "Old Times on the Mississippi.", "The Mysterious Stranger", "Personal Recollections of Joan of Arc Volume 2", "Tom Sawyer - Detective", "A Connecticut Yankee in King Arthur's Court (Enriched Classics Series)", "Roughing It (Classics of the Old West)", "Personal Recollections of Joan of Arc (1896) (The Oxford Mark Twain)", "Adventures of Tom Sawyer (Children's Classics)", "A Connecticut Yankee in King Arthur's Court (Tor Classics)", "Tom Sawyer Abroad (The Works Of Mark Twain - 25 Volumes - Author's National Edition)", "Adventures of Huckleberry Finn With Reader's Guide (Amsco Literature Program Series Grade 7 12, R 120 ALP)", "The mysterious stranger", "Adventures of Huckleberry Finn/Tom Sawyer (Junior Classics)", "The adventures of Tom Sawyer ; The adventures of Huckleberry Finn ; The prince and the pauper", "Tom Sawyer Abroad (Penguin Classics)", "Adventures of Tom Sawyer (08454) (Deans Childrens Classics)", "A Tramp Abroad", "Personal Recollections of Joan of Arc Volume 1 (Large Print Edition)", "American Claimant (Writings of Mark Twain)", "The Mysterious Stranger (Large Print Edition)", "The American Claimant (1892) (The Oxford Mark Twain)", "The Adventures of Tom Sawyer Adventure Classic (Adventure Classics)", "Following the equator", "Old Times on the Mississippi", "The Prince and the Pauper (New Method Supplementary Readers)", "Tom Sawyer Abroad (Large Print Edition)", "Tom Sawyer Detective", "Christian Science (1907) (The Oxford Mark Twain)", "The prince and the pauper", "Old Times On The Mississippi", "The American Claimant (Large Print Edition)", "The Adventures of Tom Sawyer and The Adventures of Huckleberry Finn (Signet Classical Books)", "A Connecticut Yankee in King Arthur's Court", "The Adventures of Tom Sawyer and The Adventures of Huckleberry Finn (Everyman Paperbacks)", "The Adventures of Huckleberry Finn, Complete (Large Print)", "The adventures of Tom Sawyer ; The adventures of Huckleberry Finn ; The prince and the pauper ; Pudd'nhead Wilson ; Short stories ; A Connecticut Yankee at King Arthur's court", "Sketches, new and old", "Personal Recollections of Joan of Arc (Complete)", "The American Claimant (1896)", "Personal Recollections Of Joan Of Arc", "Roughing It (Signet Classics)", "The Prince and the Pauper (Webster's Italian Thesaurus Edition)", "The Adventures of Tom Sawyer", "The Adventures Of Tom Sawyer", "Christian Science (Large Print)", "Adventures of Tom Sawyer. Adventure stories typically. Prince and pauper. Stories", "The Prince and the Pauper", "Adventures of Tom Sawyer (Family Classics Library)", "Adventures of Tom Sawyer (Dramascripts)", "The American Claimant (Large Print)", "Tom Sawyer Abroad (Watermill Classics Library)", "Adventures of Tom Sawyer Promo (Action Packs)", "Adventures of Tom Sawyer (Progress English)", "Adventures of Tom Sawyer (Episodes from/Cdl 51205)", "The wit and wisdom of Mark Twain", "Christian Science (Large Print Edition)", "Following the Equator", "The Prince and the Pauper (Webster's Korean Thesaurus Edition)", "Adventures of Tom Sawyer (Longman Simplified English Series)", "Christian Science", "The adventures of Tom Sawyer", "American Claimant", "Old times on the Mississippi", "Adventures of Tom Sawyer (New Windmill)", "Tom Sawyer, Detective (Hesperus Classics)", "Wild Nights!", "Adventures of Tom Sawyer (Cassette Sac 967)", "The Adventures of Tom Sawyer, 1876 (IN RUSSIAN LANGUAGE) / (Las aventuras de Tom Sawyer / les Aventures de Tom Sawyer / Die Abenteuer des Tom Sawyer)", "Ignorance, Confidence, and Filthy Rich Friends: The Business Adventures of Mark Twain, Chronic Speculator and Entrepreneur", "The Adventures of Tom Sawyer (Puffin Books)", "The Prince and the Pauper (Dover Children's Thrift Classics)", "Tom Sawyer Abroad (1894) (The Oxford Mark Twain)", "The adventures of Tom Sawyer ; Tom Sawyer abroad", "Adventures of Huckleberry Finn (Illustrated Edition) (Dodo Press)", "Adventures of Tom Sawyer - Huckleberry Finn (Classic Compendium)", "Adventures of Tom Sawyer, The", "Sketches New And Old", "Tom Sawyer abroad", "The Prince And the Pauper", "Tom Sawyer, Detective (Tor Classics)", "Grant and Twain: The Story of a Friendship That Changed America", "Adventures of Huckleberry Finn", "A Connecticut Yankee in King Arthur's Court, 1889 (novel) (IN RUSSIAN LANGUAGE) / (Ein Yankee am Hofe des K\u00f6nig Artus / Janki iz Konnektikuta pri dvore korolja Artura)", "Sketches, New and Old (1875) (The Oxford Mark Twain)", "Adventures of Tom Sawyer and Huckleberry Finn", "Sketches New and Old (Complete)", "The Adventures of Tom Sawyer and the Adventures of Huckleberry Finn (Signet Classics)", "Adventures of Tom Sawyer (Webster's Korean Thesaurus Edition)", "Letters from the earth", "The Innocents Abroad, vol. 1: The Authorized Uniform Edition", "Personal Recollections of Joan of Arc Volume 2 (Large Print Edition)", "1601", "Tom Sawyer, Detective", "Roughing It (1872) (The Oxford Mark Twain)", "Who Is Mark Twain?", "Roughing it.", "Autobiography of Mark Twain", "Adventures of Tom Sawyer, The (Classic Collection)", "Adventures of Tom Sawyer (Ec04)", "Adventures of Tom Sawyer (Classic Library)", "Adventures of Tom Sawyer (Classics)", "The Adventures of Huckleberry Finn", "The Adventures of Huckleberry Finn (Signet Classics)", "Roughing It (Konemann Classics)", "The Prince and the Pauper (Webster's Chinese-Traditional Thesaurus Edition)", "Personal recollections of Joan of Arc", "Adventures of Tom Sawyer (Modern Library Classics (Sagebrush))", "The innocents abroad", "Personal Recollections of Joan of Arc, V2", "The Innocents Abroad (Signet Classics)", "Roughing It", "A Connecticut Yankee in King Arthur's Court (Classics Read By Celebrities Series)", "The Adventures of Tom Sawyer - Literary Touchstone Edition", "The adventures of Huckleberry Finn (Tom Sawyer's comrade)", "Personal Recollections of Joan of Arc Volume 1", "Life on the Mississippi", "The American claimant", "The Mysterious Stranger (Signet Classics)", "Adventures of Tom Sawyer (Oxford Progressive English Readers)", "Tom Sawyer Abroad", "Roughing It (Centre for Editions of American Authors)", "The Trouble Begins at 8: A Life of Mark Twain in the Wild, Wild West", "Adventures of Tom Sawyer (Deluxe Watermill Classics)", "Adventures of Tom Sawyer (Children's Illustrated Classics)", "Adventures of Tom Sawyer & Huck Finn Collector's Library Volume I (Collector's Library of Classics, Volume 1)", "Sketches New And Old (The Works Of Mark Twain - 25 Volumes - Author's National Edition)", "A Tramp Abroad (1880) (The Oxford Mark Twain)", "A Connecticut Yankee in King Arthur's Court Readalong", "Adventures of Tom Sawyer (Saddleback Classics)", "Adventures of Huckleberry Finn With Reader's Guide (Amsco Literature Program Series Grade 7-12)", "Letters from the Earth", "Tom Sawyer Abroad (Tor Classics)", "The Adventures of Tom Sawyer and The Adventures of Huckleberry Finn", "How to Tell a Story and Other Essays (1897) (The Oxford Mark Twain)", "Tom Sawyer Detective (Watermill Classic)", "The Adventures of Tom Sawyer With Audio CD (Hear It Read It)", "The Innocents Abroad", "The Adventures of Tom Sawyer (The Classic Collection)"], "ans_acc": 0.029585798816568046, "ans_hit": 1, "ans_f1": 0.043875685557586835, "ans_precission": 0.3, "ans_recall": 0.023668639053254437, "path_f1": 0.17647058823529413, "path_precision": 0.3, "path_recall": 0.125, "path_ans_f1": 0.05385996409335727, "path_ans_precision": 0.3, "path_ans_recall": 0.029585798816568046}
{"id": "WebQTest-146", "prediction": ["# Reasoning Path:\nJordan -> sports.sports_team_location.teams -> Al-Wehdat SC -> sports.sports_team.location -> Amman\n# Answer:\nAl-Wehdat SC", "# Reasoning Path:\nJordan -> royalty.kingdom.rulers -> Hussein of Jordan -> people.person.place_of_birth -> Amman\n# Answer:\nHussein of Jordan", "# Reasoning Path:\nJordan -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71xk1k6\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nJordan -> royalty.kingdom.rulers -> Hussein of Jordan -> royalty.monarch.royal_line -> Hashemites\n# Answer:\nHussein of Jordan", "# Reasoning Path:\nJordan -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6hc5l\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nJordan -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37x9d\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nJordan -> royalty.kingdom.rulers -> Abdullah I of Jordan -> government.politician.government_positions_held -> m.0k_ktrb\n# Answer:\nAbdullah I of Jordan", "# Reasoning Path:\nJordan -> royalty.kingdom.rulers -> Abdullah I of Jordan -> people.person.nationality -> Ottoman Empire\n# Answer:\nAbdullah I of Jordan", "# Reasoning Path:\nJordan -> royalty.kingdom.rulers -> Abdullah II of Jordan -> people.person.parents -> Hussein of Jordan\n# Answer:\nAbdullah II of Jordan", "# Reasoning Path:\nJordan -> royalty.kingdom.rulers -> Abdullah II of Jordan -> government.political_appointer.appointees -> m.0k27q6l\n# Answer:\nAbdullah II of Jordan"], "ground_truth": ["Jordan national football team", "Al-Wehdat SC"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.16666666666666669, "ans_precission": 0.1, "ans_recall": 0.5, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 0.1, "path_ans_recall": 0.5}
{"id": "WebQTest-1460", "prediction": ["# Reasoning Path:\nGregor Mendel -> people.person.profession -> Monk -> people.profession.people_with_this_profession -> Abhishiktananda\n# Answer:\nMonk", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Scientist -> common.topic.subject_of -> Book\n# Answer:\nScientist", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Monk -> common.topic.image -> StAnthony\n# Answer:\nMonk", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Botanist -> base.descriptive_names.names.descriptive_name -> m.0102h0md\n# Answer:\nBotanist", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Botanist -> common.topic.notable_for -> g.12568993_\n# Answer:\nBotanist", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Monk -> people.profession.people_with_this_profession -> Adrian Urmanov\n# Answer:\nMonk", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Monk -> base.descriptive_names.names.descriptive_name -> m.0100v_8n\n# Answer:\nMonk", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Botanist -> common.topic.notable_types -> Profession\n# Answer:\nBotanist", "# Reasoning Path:\nGregor Mendel -> common.topic.image -> Mendel -> astronomy.extraterrestrial_location.on_celestial_object -> Mars\n# Answer:\nMendel", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Monk -> common.topic.image -> Germany M\u00c3\u00bcnchen Monks\n# Answer:\nMonk"], "ground_truth": ["Botanist", "Scientist", "Monk"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1461", "prediction": ["# Reasoning Path:\nRay Allen -> people.person.spouse_s -> m.0cs25kk -> people.marriage.spouse -> Shannon Walker Williams\n# Answer:\nShannon Walker Williams", "# Reasoning Path:\nRay Allen -> people.person.spouse_s -> m.0cs25kk -> people.marriage.location_of_ceremony -> Martha's Vineyard\n# Answer:\nMartha's Vineyard", "# Reasoning Path:\nRay Allen -> people.person.spouse_s -> m.0cs25kk -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nRay Allen -> people.person.sibling_s -> m.0j2h3vr -> people.sibling_relationship.sibling -> Kristie Raye\n# Answer:\nKristie Raye", "# Reasoning Path:\nRay Allen -> people.person.sibling_s -> m.0ztqw9x -> people.sibling_relationship.sibling -> John Allen\n# Answer:\nJohn Allen", "# Reasoning Path:\nRay Allen -> people.person.sibling_s -> m.0ztqwb8 -> people.sibling_relationship.sibling -> Kim Allen\n# Answer:\nKim Allen", "# Reasoning Path:\nRay Allen -> sports.pro_athlete.teams -> m.05cf4bc -> sports.sports_team_roster.team -> Connecticut Huskies men's basketball\n# Answer:\nConnecticut Huskies men's basketball", "# Reasoning Path:\nRay Allen -> sports.pro_athlete.teams -> m.08s4cgk -> sports.sports_team_roster.position -> Shooting guard\n# Answer:\nShooting guard", "# Reasoning Path:\nRay Allen -> sports.pro_athlete.teams -> m.08s4cgk -> sports.sports_team_roster.team -> Miami Heat\n# Answer:\nMiami Heat", "# Reasoning Path:\nRay Allen -> sports.pro_athlete.teams -> m.0k2_4xw -> sports.sports_team_roster.position -> Shooting guard\n# Answer:\nShooting guard"], "ground_truth": ["Shannon Walker Williams"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.13333333333333333, "path_precision": 0.1, "path_recall": 0.2, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1462", "prediction": ["# Reasoning Path:\nBarack Obama -> government.us_president.vice_president -> Joe Biden -> people.person.profession -> Politician\n# Answer:\nJoe Biden", "# Reasoning Path:\nBarack Obama -> government.us_president.vice_president -> Joe Biden -> base.politicalconventions.vice_presidential_nominee.convention_nominated_at -> 2008 Democratic National Convention\n# Answer:\nJoe Biden", "# Reasoning Path:\nBarack Obama -> government.us_president.vice_president -> Joe Biden -> common.topic.subject_of -> President Barack Obama and the Message Beyond the Photograph\n# Answer:\nJoe Biden", "# Reasoning Path:\nBarack Obama -> government.us_president.vice_president -> Joe Biden -> people.person.profession -> Lawyer\n# Answer:\nJoe Biden", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> Yes We Can: A Biography of President Barack Obama -> book.written_work.subjects -> United States of America\n# Answer:\nYes We Can: A Biography of President Barack Obama", "# Reasoning Path:\nBarack Obama -> people.person.ethnicity -> English American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nEnglish American", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> Yes We Can: A Biography of President Barack Obama -> book.written_work.subjects -> History of the United States\n# Answer:\nYes We Can: A Biography of President Barack Obama", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.subjects -> Mitt Romney presidential campaign, 2012\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> Yes We Can: A Biography of President Barack Obama -> common.topic.notable_types -> Book\n# Answer:\nYes We Can: A Biography of President Barack Obama", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.author -> Aberjhani\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares"], "ground_truth": ["Joe Biden"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1463", "prediction": ["# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Wales\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Cornish Language -> language.human_language.countries_spoken_in -> England\n# Answer:\nCornish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> British Sign Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nBritish Sign Language", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nChild pornography", "# Reasoning Path:\nUnited Kingdom -> location.statistical_region.long_term_unemployment_rate -> g.12cp_jvpx\n# Answer:\nlocation.statistical_region.long_term_unemployment_rate", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> Australia\n# Answer:\nEnglish Language"], "ground_truth": ["English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1464", "prediction": ["# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Centre for the Arts -> architecture.structure.architect -> Eberhard Zeidler\n# Answer:\nToronto Centre for the Arts", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Centre for the Arts -> travel.tourist_attraction.near_travel_destination -> Newmarket\n# Answer:\nToronto Centre for the Arts", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Centre for the Arts -> common.topic.article -> m.05398q\n# Answer:\nToronto Centre for the Arts", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Islands -> geography.island.body_of_water -> Lake Ontario\n# Answer:\nToronto Islands", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Islands -> location.location.containedby -> Ontario\n# Answer:\nToronto Islands", "# Reasoning Path:\nToronto -> common.topic.webpage -> m.02nc8t6 -> common.webpage.resource -> City of Toronto Web site\n# Answer:\nCity of Toronto Web site", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Zoo -> common.topic.article -> m.02jlfj\n# Answer:\nToronto Zoo", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Zoo -> zoos.zoo.notable_animals -> m.0j0_p1n\n# Answer:\nToronto Zoo", "# Reasoning Path:\nToronto -> travel.travel_destination.local_transportation -> GO Transit -> common.topic.notable_types -> Mass Transportation System\n# Answer:\nGO Transit", "# Reasoning Path:\nToronto -> common.topic.webpage -> m.059h8f0 -> common.webpage.resource -> Visitor Information\n# Answer:\nVisitor Information"], "ground_truth": ["Queens Quay", "Kensington Market", "Chinatown, Toronto", "Gardiner Museum", "Bare Oaks Family Naturist Park", "Luminato", "Hockey Hall of Fame", "Textile Museum of Canada", "Air Canada Centre", "Toronto Islands", "High Park", "Royal Alexandra Theatre", "CN Tower", "Toronto Waterfront Marathon", "The Beaches", "Heartland Town Centre", "Edwards Gardens", "Danforth Avenue", "Planet in Focus", "Ontario Place", "Roy Thomson Hall", "McMichael Canadian Art Collection", "First Toronto Post Office", "Casa Loma", "Museum of Contemporary Canadian Art", "Hanlan's Point Beach", "Nathan Phillips Square", "Distillery District", "Toronto Eaton Centre", "Rogers Centre", "Princess of Wales Theatre", "Spadina House", "Toronto Centre for the Arts", "Queen Street West", "Cabbagetown, Toronto", "St. Lawrence Market", "Black Creek Pioneer Village", "Toronto Zoo", "Corktown Common", "Bata Shoe Museum", "BMO Field", "Bayview Village Shopping Centre", "Sugar Beach", "Ontario Science Centre", "Sony Centre for the Performing Arts", "Royal Ontario Museum", "Art Gallery of Ontario", "Canada's Wonderland", "Yorkville, Toronto"], "ans_acc": 0.061224489795918366, "ans_hit": 1, "ans_f1": 0.11260053619302948, "ans_precission": 0.7, "ans_recall": 0.061224489795918366, "path_f1": 0.11260053619302948, "path_precision": 0.7, "path_recall": 0.061224489795918366, "path_ans_f1": 0.11260053619302948, "path_ans_precision": 0.7, "path_ans_recall": 0.061224489795918366}
{"id": "WebQTest-1465", "prediction": ["# Reasoning Path:\nSouth Korea -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nSouth Korea -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nSouth Korea -> location.statistical_region.gdp_nominal_per_capita -> g.11b60sjxs7\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nSouth Korea -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Argentina\n# Answer:\nConstitutional republic", "# Reasoning Path:\nSouth Korea -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Angola\n# Answer:\nUnitary state", "# Reasoning Path:\nSouth Korea -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Chile\n# Answer:\nConstitutional republic", "# Reasoning Path:\nSouth Korea -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Chile\n# Answer:\nUnitary state", "# Reasoning Path:\nSouth Korea -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> United States of America\n# Answer:\nPresidential system", "# Reasoning Path:\nSouth Korea -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6g_c4\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nSouth Korea -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc37pmk\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita"], "ground_truth": ["Unitary state", "Presidential system", "Constitutional republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1467", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> common.topic.image -> WilliamFranklin\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Francis Folger Franklin -> people.person.parents -> Deborah Read\n# Answer:\nFrancis Folger Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> people.person.gender -> Male\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Sarah Franklin Bache -> people.person.children -> Benjamin Franklin Bache\n# Answer:\nSarah Franklin Bache", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Sarah Franklin Bache -> people.person.spouse_s -> m.0j4k6gy\n# Answer:\nSarah Franklin Bache", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> American literature\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Francis Folger Franklin -> people.deceased_person.place_of_burial -> Christ Church Burial Ground\n# Answer:\nFrancis Folger Franklin", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Francis Folger Franklin -> common.topic.notable_for -> g.125by3nfc\n# Answer:\nFrancis Folger Franklin", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> :Library and information science\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze"], "ground_truth": ["Deborah Read"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1468", "prediction": ["# Reasoning Path:\nSri Lanka -> location.country.capital -> Sri Jayawardenepura Kotte -> location.location.events -> 1987 grenade attack in the Sri Lankan Parliament\n# Answer:\nSri Jayawardenepura Kotte", "# Reasoning Path:\nSri Lanka -> location.country.capital -> Sri Jayawardenepura Kotte -> common.topic.notable_types -> City/Town/Village\n# Answer:\nSri Jayawardenepura Kotte", "# Reasoning Path:\nSri Lanka -> location.country.capital -> Sri Jayawardenepura Kotte -> common.topic.image -> The Parliament of Sri Lanka\n# Answer:\nSri Jayawardenepura Kotte", "# Reasoning Path:\nSri Lanka -> location.statistical_region.electricity_consumption_per_capita -> g.1245_0s28\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nSri Lanka -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vqmm4\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nSri Lanka -> location.statistical_region.electricity_consumption_per_capita -> g.1245_4m56\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nSri Lanka -> location.country.capital -> Colombo -> location.location.events -> Battle of Mulleriyawa\n# Answer:\nColombo", "# Reasoning Path:\nSri Lanka -> location.statistical_region.electricity_consumption_per_capita -> g.1245_67k2\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nSri Lanka -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gh7w\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nSri Lanka -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37hcc\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp"], "ground_truth": ["Sri Jayawardenepura Kotte", "Colombo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-147", "prediction": ["# Reasoning Path:\nTennessee -> government.governmental_jurisdiction.governing_officials -> m.04j5sk4 -> government.government_position_held.office_holder -> Andrew Johnson\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nTennessee -> government.governmental_jurisdiction.governing_officials -> m.04j5sk4 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nTennessee -> government.governmental_jurisdiction.governing_officials -> m.010178yn -> government.government_position_held.office_holder -> Andrew Farmer\n# Answer:\nAndrew Farmer", "# Reasoning Path:\nTennessee -> government.governmental_jurisdiction.governing_officials -> m.04j5sk4 -> government.government_position_held.office_position_or_title -> Governor of Tennessee\n# Answer:\nGovernor of Tennessee", "# Reasoning Path:\nTennessee -> government.governmental_jurisdiction.governing_officials -> m.01017wwj -> government.government_position_held.office_holder -> Timothy Hill\n# Answer:\nTimothy Hill", "# Reasoning Path:\nTennessee -> government.governmental_jurisdiction.governing_officials -> m.010178yn -> government.government_position_held.basic_title -> State Representative\n# Answer:\nState Representative", "# Reasoning Path:\nTennessee -> government.governmental_jurisdiction.governing_officials -> m.01017wwj -> government.government_position_held.office_position_or_title -> Tennessee State Representative\n# Answer:\nTennessee State Representative", "# Reasoning Path:\nTennessee -> book.book_subject.works -> White Bird -> book.book.editions -> White bird\n# Answer:\nWhite Bird", "# Reasoning Path:\nTennessee -> common.topic.webpage -> m.04lswkp -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nTennessee -> common.topic.webpage -> m.09wjqcp -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["William Haslam"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1470", "prediction": ["# Reasoning Path:\nPoland -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Second Polish Republic\n# Answer:\nParliamentary republic", "# Reasoning Path:\nPoland -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Soviet Union\n# Answer:\nParliamentary republic", "# Reasoning Path:\nPoland -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Austria\n# Answer:\nParliamentary republic", "# Reasoning Path:\nPoland -> location.statistical_region.size_of_armed_forces -> g.12cp_j3lx\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nPoland -> location.country.form_of_government -> Democracy -> organization.organization_sector.organizations_in_this_sector -> Council of Europe\n# Answer:\nDemocracy", "# Reasoning Path:\nPoland -> location.statistical_region.size_of_armed_forces -> g.1hhc385gk\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nPoland -> location.statistical_region.size_of_armed_forces -> g.1hhc39krm\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nPoland -> book.book_subject.works -> A Holocaust Controversy -> common.topic.notable_types -> Book\n# Answer:\nA Holocaust Controversy", "# Reasoning Path:\nPoland -> book.book_subject.works -> A Holocaust Controversy -> book.written_work.subjects -> The Holocaust\n# Answer:\nA Holocaust Controversy", "# Reasoning Path:\nPoland -> book.book_subject.works -> A concise history of Poland -> book.written_work.subjects -> History\n# Answer:\nA concise history of Poland"], "ground_truth": ["Democracy", "Parliamentary republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1471", "prediction": ["# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.010hbq1t -> government.government_position_held.office_holder -> John Carney\n# Answer:\nJohn Carney", "# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.010hbqbl -> government.government_position_held.office_holder -> Carl Danberg\n# Answer:\nCarl Danberg", "# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.010hbwj4 -> government.government_position_held.office_holder -> Harriet Smith Windsor\n# Answer:\nHarriet Smith Windsor", "# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.010hbq1t -> government.government_position_held.office_position_or_title -> Lieutenant Governor of Delaware\n# Answer:\nLieutenant Governor of Delaware", "# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.010hbq1t -> government.government_position_held.basic_title -> Lieutenant Governor\n# Answer:\nLieutenant Governor", "# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.010hbqbl -> government.government_position_held.basic_title -> Attorney general\n# Answer:\nAttorney general", "# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.010hbqbl -> government.government_position_held.office_position_or_title -> Attorney General of Delaware\n# Answer:\nAttorney General of Delaware", "# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.010hbwj4 -> government.government_position_held.appointed_by -> Ruth Ann Minner\n# Answer:\nRuth Ann Minner", "# Reasoning Path:\nDelaware -> location.location.people_born_here -> A.C. Golden -> common.topic.article -> m.0c0311g\n# Answer:\nA.C. Golden", "# Reasoning Path:\nDelaware -> military.military_unit_place_of_origin.military_units -> 1st Delaware Infantry Regiment -> military.military_unit.armed_force -> Union Army\n# Answer:\n1st Delaware Infantry Regiment"], "ground_truth": ["Ed Haskett", "Matt Stawicki", "Jeremy Conway", "James Tilton", "James B. Clark, Jr.", "Rex Carlton", "Hugh T. Broomall", "David McElwee", "Rebecca Lee Crumpler", "Joe Garcio", "Patrick Kearney", "William King", "Griffin Seward", "Chris Gutierrez", "Norman Hutchins", "Edward Groesbeck Voss", "Gimel \\\"Young Guru\\\" Keaton", "R. R. M. Carpenter", "A.C. Golden", "Eddie Paskey", "Andrew Cebulka", "Jim Wilson", "Jacqueline Jones", "William Grassie", "Bill Indursky", "Ann Marie Borghese", "Solomon Bayley", "Tom Peszek", "Fred Lonberg-Holm", "Hampton Del Ruth", "Steve Ressel", "Collins J. Seitz", "Tully Satre", "Nathaniel Harrington Bannister", "Outerbridge Horsey IV", "Francine Fournier", "John Sedwick", "Alfred I. du Pont", "Huck Betts", "Herbert Bennett Fenn", "Billy Ficca", "Reuben James", "Chris Dapkins", "Jeffrey W. Bullock", "Katharine Pyle"], "ans_acc": 0.022222222222222223, "ans_hit": 1, "ans_f1": 0.03636363636363636, "ans_precission": 0.1, "ans_recall": 0.022222222222222223, "path_f1": 0.03636363636363636, "path_precision": 0.1, "path_recall": 0.022222222222222223, "path_ans_f1": 0.03636363636363636, "path_ans_precision": 0.1, "path_ans_recall": 0.022222222222222223}
{"id": "WebQTest-1472", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Michigan\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> government.politician.government_positions_held -> m.04stpgl -> government.government_position_held.office_position_or_title -> Governor of Massachusetts\n# Answer:\nGovernor of Massachusetts", "# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> government.politician.government_positions_held -> m.04stpgl -> government.government_position_held.jurisdiction_of_office -> Massachusetts\n# Answer:\nMassachusetts", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0tk1pyb -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0tk1pyb -> film.personal_film_appearance.film -> 8: The Mormon Proposition\n# Answer:\n8: The Mormon Proposition", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0_gvz4k -> film.personal_film_appearance.film -> Mitt\n# Answer:\nMitt", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0_gvz4k -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0nhlrm9 -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0nhlrm9 -> film.personal_film_appearance.film -> The Religious Test\n# Answer:\nThe Religious Test"], "ground_truth": ["2003-01-02"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1473", "prediction": ["# Reasoning Path:\nCyprus -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Republic of Kosovo\n# Answer:\nEuro", "# Reasoning Path:\nCyprus -> location.country.currency_used -> Euro -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nEuro", "# Reasoning Path:\nCyprus -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Spain\n# Answer:\nEuro", "# Reasoning Path:\nCyprus -> location.country.currency_used -> Euro -> common.topic.notable_types -> Currency\n# Answer:\nEuro", "# Reasoning Path:\nCyprus -> location.statistical_region.gender_balance_members_of_parliament -> g.11b71y1l3w\n# Answer:\nlocation.statistical_region.gender_balance_members_of_parliament", "# Reasoning Path:\nCyprus -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nCyprus -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nCyprus -> location.statistical_region.gender_balance_members_of_parliament -> g.1hhc3_4dv\n# Answer:\nlocation.statistical_region.gender_balance_members_of_parliament", "# Reasoning Path:\nCyprus -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Mexico\n# Answer:\nPresidential system", "# Reasoning Path:\nCyprus -> location.statistical_region.gender_balance_members_of_parliament -> g.1hhc3f_q5\n# Answer:\nlocation.statistical_region.gender_balance_members_of_parliament"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1475", "prediction": ["# Reasoning Path:\nJohnny Depp -> music.group_member.instruments_played -> Guitar -> music.instrument.variation -> Bass guitar\n# Answer:\nGuitar", "# Reasoning Path:\nJohnny Depp -> music.group_member.instruments_played -> Guitar -> music.instrument.family -> Reverb\n# Answer:\nGuitar", "# Reasoning Path:\nJohnny Depp -> music.group_member.instruments_played -> Guitar -> common.topic.subject_of -> Reverb\n# Answer:\nGuitar", "# Reasoning Path:\nJohnny Depp -> music.group_member.instruments_played -> Guitar -> music.instrument.variation -> Electric guitar\n# Answer:\nGuitar", "# Reasoning Path:\nJohnny Depp -> music.group_member.instruments_played -> Slide guitar -> music.performance_role.guest_performances -> m.04flflz\n# Answer:\nSlide guitar", "# Reasoning Path:\nJohnny Depp -> music.group_member.instruments_played -> Slide guitar -> music.performance_role.guest_performances -> m.04flfm5\n# Answer:\nSlide guitar", "# Reasoning Path:\nJohnny Depp -> award.award_nominee.award_nominations -> m.03mlr5q -> award.award_nomination.nominated_for -> Pirates of the Caribbean: The Curse of the Black Pearl\n# Answer:\nPirates of the Caribbean: The Curse of the Black Pearl", "# Reasoning Path:\nJohnny Depp -> award.award_nominee.award_nominations -> m.03mlr72 -> award.award_nomination.ceremony -> 77th Academy Awards\n# Answer:\n77th Academy Awards", "# Reasoning Path:\nJohnny Depp -> award.award_nominee.award_nominations -> m.03mlr5q -> award.award_nomination.ceremony -> 76th Academy Awards\n# Answer:\n76th Academy Awards", "# Reasoning Path:\nJohnny Depp -> film.person_or_entity_appearing_in_film.films -> m.049ykr5 -> film.personal_film_appearance.type_of_appearance -> Narrator\n# Answer:\nNarrator"], "ground_truth": ["Guitar", "Slide guitar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1477", "prediction": ["# Reasoning Path:\nCarrie Underwood -> tv.tv_actor.guest_roles -> m.0bd32ns -> tv.tv_guest_role.episodes_appeared_in -> Hooked\n# Answer:\nHooked", "# Reasoning Path:\nCarrie Underwood -> tv.tv_actor.guest_roles -> m.0bvtx58 -> tv.tv_guest_role.episodes_appeared_in -> Winner Is Crowned\n# Answer:\nWinner Is Crowned", "# Reasoning Path:\nCarrie Underwood -> tv.tv_actor.guest_roles -> m.0bvtzgw -> tv.tv_guest_role.episodes_appeared_in -> Steve Carell, Blake Lively, Carrie Underwood\n# Answer:\nSteve Carell, Blake Lively, Carrie Underwood", "# Reasoning Path:\nCarrie Underwood -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nCarrie Underwood -> broadcast.artist.content -> 1Club.FM: Channel One -> broadcast.content.artist -> Aaliyah\n# Answer:\n1Club.FM: Channel One", "# Reasoning Path:\nCarrie Underwood -> broadcast.artist.content -> 1Club.FM: Channel One -> broadcast.content.location -> Chicago\n# Answer:\n1Club.FM: Channel One", "# Reasoning Path:\nCarrie Underwood -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nCarrie Underwood -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nCarrie Underwood -> broadcast.artist.content -> 1.FM Country -> broadcast.content.artist -> Alabama\n# Answer:\n1.FM Country", "# Reasoning Path:\nCarrie Underwood -> broadcast.artist.content -> 1.FM Country -> common.topic.article -> m.03hrwqw\n# Answer:\n1.FM Country"], "ground_truth": ["Hooked"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1478", "prediction": ["# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Making Our Democracy Work -> book.written_work.author -> Stephen Breyer\n# Answer:\nMaking Our Democracy Work", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.written_work.author -> L. A. Scot Powe\n# Answer:\nThe Warren Court and American Politics", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> people.appointed_role.appointment -> m.0hn35mm\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Making Our Democracy Work -> book.written_work.subjects -> United States Constitution\n# Answer:\nMaking Our Democracy Work", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Making Our Democracy Work -> common.topic.notable_for -> g.125728tkq\n# Answer:\nMaking Our Democracy Work", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.written_work.subjects -> Politics of the United States\n# Answer:\nThe Warren Court and American Politics", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Courtiers of the Marble Palace: The Rise and Influence of the Supreme Court Law Clerk -> book.written_work.author -> Todd C. Peppers\n# Answer:\nCourtiers of the Marble Palace: The Rise and Influence of the Supreme Court Law Clerk", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> law.judicial_title.judges -> m.046x3bs\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> law.court.inferior_courts -> Arkansas Supreme Court -> law.court.judges -> m.0wzgtvm\n# Answer:\nArkansas Supreme Court", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.written_work.author -> Lucas A. Powe\n# Answer:\nThe Warren Court and American Politics"], "ground_truth": ["Stephen Breyer", "Clarence Thomas", "Antonin Scalia", "Anthony Kennedy", "Ruth Bader Ginsburg", "Sonia Sotomayor", "Elena Kagan", "John Roberts", "Samuel Alito"], "ans_acc": 0.1111111111111111, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.10526315789473685, "path_ans_precision": 0.1, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-1479", "prediction": ["# Reasoning Path:\nDustin Johnson -> people.person.profession -> Golfer -> people.profession.specialization_of -> Athlete\n# Answer:\nGolfer", "# Reasoning Path:\nDustin Johnson -> people.person.profession -> Golfer -> common.topic.notable_types -> Profession\n# Answer:\nGolfer", "# Reasoning Path:\nDustin Johnson -> people.person.profession -> Golfer -> base.descriptive_names.names.descriptive_name -> m.0106cygr\n# Answer:\nGolfer", "# Reasoning Path:\nDustin Johnson -> people.person.profession -> Golfer -> base.descriptive_names.names.descriptive_name -> m.0106cyt1\n# Answer:\nGolfer", "# Reasoning Path:\nDustin Johnson -> common.topic.image -> Dustin Johnson 2008 US Open cropped -> common.image.size -> m.07bcqmr\n# Answer:\nDustin Johnson 2008 US Open cropped", "# Reasoning Path:\nDustin Johnson -> people.person.profession -> Golfer -> base.descriptive_names.names.descriptive_name -> m.0106db86\n# Answer:\nGolfer"], "ground_truth": ["Golfer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9090909090909091, "ans_precission": 0.8333333333333334, "ans_recall": 1.0, "path_f1": 0.9090909090909091, "path_precision": 0.8333333333333334, "path_recall": 1.0, "path_ans_f1": 0.9090909090909091, "path_ans_precision": 0.8333333333333334, "path_ans_recall": 1.0}
{"id": "WebQTest-1480", "prediction": ["# Reasoning Path:\nCher -> people.person.children -> Elijah Blue Allman -> people.person.parents -> Gregg Allman\n# Answer:\nElijah Blue Allman", "# Reasoning Path:\nCher -> people.person.children -> Elijah Blue Allman -> people.person.profession -> Singer\n# Answer:\nElijah Blue Allman", "# Reasoning Path:\nCher -> people.person.children -> Chaz Bono -> people.person.parents -> Sonny Bono\n# Answer:\nChaz Bono", "# Reasoning Path:\nCher -> people.person.children -> Elijah Blue Allman -> common.topic.image -> Elijah_Blue_Allman.jpg\n# Answer:\nElijah Blue Allman", "# Reasoning Path:\nCher -> people.person.children -> Chaz Bono -> award.award_nominee.award_nominations -> m.0r66c_b\n# Answer:\nChaz Bono", "# Reasoning Path:\nCher -> award.award_nominee.award_nominations -> m.0w4glrk -> award.award_nomination.award_nominee -> Sonny Bono\n# Answer:\nSonny Bono", "# Reasoning Path:\nCher -> people.person.spouse_s -> m.023b86m -> people.marriage.location_of_ceremony -> Tijuana\n# Answer:\nTijuana", "# Reasoning Path:\nCher -> people.person.children -> Chaz Bono -> people.person.nationality -> United States of America\n# Answer:\nChaz Bono", "# Reasoning Path:\nCher -> people.person.children -> Elijah Blue Allman -> people.person.profession -> Guitarist\n# Answer:\nElijah Blue Allman", "# Reasoning Path:\nCher -> people.person.spouse_s -> m.023b86m -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage"], "ground_truth": ["Sonny Bono", "Gregg Allman"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.16666666666666669, "ans_precission": 0.1, "ans_recall": 0.5, "path_f1": 0.23076923076923075, "path_precision": 0.3, "path_recall": 0.1875, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1481", "prediction": ["# Reasoning Path:\nUniversity of Oregon -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Oregon\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity of Oregon -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Oregon\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity of Oregon -> location.location.containedby -> Eugene -> location.location.containedby -> United States of America\n# Answer:\nEugene", "# Reasoning Path:\nUniversity of Oregon -> location.location.containedby -> United States of America -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity of Oregon -> location.location.containedby -> Oregon -> location.location.containedby -> United States of America\n# Answer:\nOregon", "# Reasoning Path:\nUniversity of Oregon -> education.university.number_of_undergraduates -> m.0h77tx1\n# Answer:\neducation.university.number_of_undergraduates", "# Reasoning Path:\nUniversity of Oregon -> location.location.containedby -> Eugene -> location.location.containedby -> Oregon\n# Answer:\nEugene", "# Reasoning Path:\nUniversity of Oregon -> location.location.containedby -> Eugene -> base.biblioness.bibs_location.state -> Oregon\n# Answer:\nEugene", "# Reasoning Path:\nUniversity of Oregon -> base.schemastaging.educational_institution_extra.libraries -> University of Oregon Library and Memorial Quadrangle -> location.location.containedby -> Eugene\n# Answer:\nUniversity of Oregon Library and Memorial Quadrangle", "# Reasoning Path:\nUniversity of Oregon -> education.university.number_of_undergraduates -> m.0k9p513\n# Answer:\neducation.university.number_of_undergraduates"], "ground_truth": ["Eugene"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1482", "prediction": ["# Reasoning Path:\nSpy Kids -> film.film.starring -> m.02s9tsc -> film.performance.actor -> Alexa Vega\n# Answer:\nAlexa Vega", "# Reasoning Path:\nSpy Kids -> film.film.starring -> m.02s9tsc -> film.performance.character -> Baby Carmen\n# Answer:\nBaby Carmen", "# Reasoning Path:\nSpy Kids -> film.film.starring -> m.01z0pft -> film.performance.actor -> Carla Gugino\n# Answer:\nCarla Gugino", "# Reasoning Path:\nSpy Kids -> film.film.starring -> m.01z0ph5 -> film.performance.actor -> Alan Cumming\n# Answer:\nAlan Cumming", "# Reasoning Path:\nSpy Kids -> common.topic.webpage -> http://elfman.filmmusic.com/spykids/\n# Answer:\nhttp://elfman.filmmusic.com/spykids/", "# Reasoning Path:\nSpy Kids -> film.film.sequel -> Spy Kids 2: The Island of Lost Dreams -> film.film.starring -> m.02s9tvb\n# Answer:\nSpy Kids 2: The Island of Lost Dreams", "# Reasoning Path:\nSpy Kids -> common.topic.webpage -> http://www.soundtrack.net/soundtracks/database/?id=2839\n# Answer:\nhttp://www.soundtrack.net/soundtracks/database/?id=2839", "# Reasoning Path:\nSpy Kids -> film.film.starring -> m.01z0pft -> film.performance.character -> Ingrid Cortez\n# Answer:\nIngrid Cortez", "# Reasoning Path:\nSpy Kids -> film.film.starring -> m.01z0ph5 -> film.performance.character -> Floop\n# Answer:\nFloop", "# Reasoning Path:\nSpy Kids -> film.film.sequel -> Spy Kids 2: The Island of Lost Dreams -> film.film.starring -> m.02s9tx3\n# Answer:\nSpy Kids 2: The Island of Lost Dreams"], "ground_truth": ["Addisyn Fair", "Alexa Vega"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.16666666666666669, "ans_precission": 0.1, "ans_recall": 0.5, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 0.1, "path_ans_recall": 0.5}
{"id": "WebQTest-1483", "prediction": ["# Reasoning Path:\nGermany -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Austria\n# Answer:\nFederal republic", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Czech Republic\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_38m_\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fdl8\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Austria\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc38c_4\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_67jr\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Poland\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_jlz4\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp"], "ground_truth": ["Federal republic", "Parliamentary republic", "Representative democracy", "Constitutional republic"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.4, "ans_recall": 0.5, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.4, "path_ans_recall": 0.5}
{"id": "WebQTest-1484", "prediction": ["# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Christopher Marlowe -> influence.influence_node.influenced_by -> Virgil\n# Answer:\nChristopher Marlowe", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Lucian -> influence.influence_node.influenced_by -> Menander\n# Answer:\nLucian", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Christopher Marlowe -> influence.influence_node.influenced_by -> Ovid\n# Answer:\nChristopher Marlowe", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Christopher Marlowe -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nChristopher Marlowe", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Lucian -> influence.influence_node.influenced_by -> Homer\n# Answer:\nLucian", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Christopher Marlowe -> influence.influence_node.influenced -> Anthony Burgess\n# Answer:\nChristopher Marlowe", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Lucian -> common.topic.notable_types -> Author\n# Answer:\nLucian", "# Reasoning Path:\nWilliam Shakespeare -> freebase.valuenotation.is_reviewed -> Art Form\n# Answer:\nArt Form", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Lucian -> influence.influence_node.influenced -> Baltasar Graci\u00e1n\n# Answer:\nLucian", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Thomas More -> people.deceased_person.place_of_death -> London\n# Answer:\nThomas More"], "ground_truth": ["Geoffrey Chaucer", "Thomas More", "Plautus", "Ovid", "John Pory", "Christopher Marlowe", "Seneca the Younger", "Thomas Kyd", "Edmund Spenser", "Michel de Montaigne", "Lucian", "Virgil", "Plutarch", "Terence"], "ans_acc": 0.35714285714285715, "ans_hit": 1, "ans_f1": 0.3461538461538461, "ans_precission": 0.9, "ans_recall": 0.21428571428571427, "path_f1": 0.3461538461538461, "path_precision": 0.9, "path_recall": 0.21428571428571427, "path_ans_f1": 0.5113636363636365, "path_ans_precision": 0.9, "path_ans_recall": 0.35714285714285715}
{"id": "WebQTest-1485", "prediction": ["# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Revaluation of the Turkish Lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nRevaluation of the Turkish Lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.065sxsl -> military.military_combatant_group.conflict -> Korean War\n# Answer:\nKorean War", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.066402y -> military.military_combatant_group.conflict -> Iraqi no-fly zones\n# Answer:\nIraqi no-fly zones"], "ground_truth": ["Turkish lira"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-1486", "prediction": ["# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> education.education.institution -> Mercy College\n# Answer:\nMercy College", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.04hc7zn -> education.education.institution -> Phillips Exeter Academy\n# Answer:\nPhillips Exeter Academy", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.02n93cn -> education.education.institution -> Harvard University\n# Answer:\nHarvard University", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nMinor", "# Reasoning Path:\nMark Zuckerberg -> influence.influence_node.influenced -> Austin Cameron -> influence.influence_node.influenced_by -> Tim Berners-Lee\n# Answer:\nAustin Cameron", "# Reasoning Path:\nMark Zuckerberg -> influence.influence_node.influenced -> Paul Scolnick -> people.person.education -> m.01314533\n# Answer:\nPaul Scolnick", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.02n93cn -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Specialization\n# Answer:\nSpecialization"], "ground_truth": ["Mercy College", "Harvard University", "Phillips Exeter Academy", "Ardsley High School"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.4285714285714285, "ans_precission": 0.3, "ans_recall": 0.75, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4285714285714285, "path_ans_precision": 0.3, "path_ans_recall": 0.75}
{"id": "WebQTest-1487", "prediction": ["# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.places_lived -> m.03plx6x -> people.place_lived.location -> Reading\n# Answer:\nReading", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.places_lived -> m.03pv20m -> people.place_lived.location -> Bucklebury\n# Answer:\nBucklebury", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.place_of_birth -> Royal Berkshire Hospital -> common.topic.webpage -> m.0fq430w\n# Answer:\nRoyal Berkshire Hospital", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.place_of_birth -> Royal Berkshire Hospital -> location.location.geolocation -> m.02_scs7\n# Answer:\nRoyal Berkshire Hospital", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.place_of_birth -> Royal Berkshire Hospital -> common.topic.notable_types -> Hospital\n# Answer:\nRoyal Berkshire Hospital", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.place_of_birth -> Royal Berkshire Hospital -> common.topic.webpage -> m.03l7bmh\n# Answer:\nRoyal Berkshire Hospital", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.education -> m.0nf_x05 -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.education -> m.0nf_x05 -> education.education.institution -> St Andrew's School, Pangbourne\n# Answer:\nSt Andrew's School, Pangbourne", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.education -> m.04q1brm -> education.education.institution -> Marlborough College\n# Answer:\nMarlborough College", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.education -> m.0nf_x05 -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date"], "ground_truth": ["Bucklebury", "Reading"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1488", "prediction": ["# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Surrealism -> influence.influence_node.influenced -> Aberjhani\n# Answer:\nSurrealism", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Surrealism -> business.product_theme.products -> Salvador Dali Registry\n# Answer:\nSurrealism", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Surrealism -> book.school_or_movement.associated_authors -> Aberjhani\n# Answer:\nSurrealism", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Eyewitness: Reports from an Art World in Crisis\n# Answer:\nModern art", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nModern art", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Surrealism -> book.school_or_movement.associated_authors -> August Strindberg\n# Answer:\nSurrealism", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Origins, Imitation, Conventions\n# Answer:\nModern art", "# Reasoning Path:\nMarc Chagall -> base.kwebbase.kwtopic.has_sentences -> After he and Bella were married, Chagall was surprised when the outbreak of World War I prevented him from leaving the country,  and he was conscripted into the army. -> base.kwebbase.kwsentence.previous_sentence -> He intended to make an appearance at the opening before continuing on to Vitebsk, where he would marry his sweetheart, Bella Rosenberg, and return with her to Paris.\n# Answer:\nAfter he and Bella were married, Chagall was surprised when the outbreak of World War I prevented him from leaving the country,  and he was conscripted into the army.", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Surrealism -> book.school_or_movement.associated_authors -> Benjamin Fondane\n# Answer:\nSurrealism", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Oscar Bluemner\n# Answer:\nModern art"], "ground_truth": ["Cubism", "Surrealism", "Expressionism", "Modern art", "Fauvism"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.5538461538461539, "ans_precission": 0.9, "ans_recall": 0.4, "path_f1": 0.5538461538461539, "path_precision": 0.9, "path_recall": 0.4, "path_ans_f1": 0.5538461538461539, "path_ans_precision": 0.9, "path_ans_recall": 0.4}
{"id": "WebQTest-1489", "prediction": ["# Reasoning Path:\nBilly Dee Williams -> film.actor.film -> m.0h2jz23 -> film.performance.character -> Lando Calrissian\n# Answer:\nLando Calrissian", "# Reasoning Path:\nBilly Dee Williams -> film.actor.film -> m.0h2jz23 -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nBilly Dee Williams -> film.actor.film -> m.0h2jz23 -> film.performance.film -> Robot Chicken: Star Wars Episode III\n# Answer:\nRobot Chicken: Star Wars Episode III", "# Reasoning Path:\nBilly Dee Williams -> film.actor.film -> m.011rh2vj -> film.performance.character -> Bartender (voice)\n# Answer:\nBartender (voice)", "# Reasoning Path:\nBilly Dee Williams -> film.actor.film -> m.010g_8m5 -> film.performance.film -> The Right of the People\n# Answer:\nThe Right of the People", "# Reasoning Path:\nBilly Dee Williams -> people.person.place_of_birth -> New York City -> common.topic.subject_of -> The Approaching 100th Anniversary of the Harlem Renaissance (part 2)\n# Answer:\nNew York City", "# Reasoning Path:\nBilly Dee Williams -> film.actor.film -> m.011rh2vj -> film.performance.film -> Oedipus\n# Answer:\nOedipus", "# Reasoning Path:\nBilly Dee Williams -> people.person.place_of_birth -> New York City -> base.militaryinfiction.location_in_fiction.contained_by -> New York\n# Answer:\nNew York City", "# Reasoning Path:\nBilly Dee Williams -> common.topic.webpage -> m.09433bs -> common.webpage.resource -> WILLIAMS TELLS\n# Answer:\nWILLIAMS TELLS", "# Reasoning Path:\nBilly Dee Williams -> common.topic.webpage -> m.0cq1wlg -> common.webpage.category -> Curated Topic\n# Answer:\nCurated Topic"], "ground_truth": ["Lando Calrissian"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-149", "prediction": ["# Reasoning Path:\nPortugal -> location.country.official_language -> Portuguese Language -> language.human_language.countries_spoken_in -> Mozambique\n# Answer:\nPortuguese Language", "# Reasoning Path:\nPortugal -> location.country.languages_spoken -> Portuguese Language -> language.human_language.countries_spoken_in -> Mozambique\n# Answer:\nPortuguese Language", "# Reasoning Path:\nPortugal -> location.country.official_language -> Portuguese Language -> language.human_language.region -> Europe\n# Answer:\nPortuguese Language", "# Reasoning Path:\nPortugal -> location.country.official_language -> Portuguese Language -> language.human_language.countries_spoken_in -> Angola\n# Answer:\nPortuguese Language", "# Reasoning Path:\nPortugal -> location.country.official_language -> Portuguese Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nPortuguese Language", "# Reasoning Path:\nPortugal -> location.country.languages_spoken -> Portuguese Language -> language.human_language.region -> Europe\n# Answer:\nPortuguese Language", "# Reasoning Path:\nPortugal -> location.country.currency_used -> Euro -> common.topic.notable_types -> Currency\n# Answer:\nEuro", "# Reasoning Path:\nPortugal -> location.country.languages_spoken -> Portuguese Language -> language.human_language.countries_spoken_in -> Angola\n# Answer:\nPortuguese Language", "# Reasoning Path:\nPortugal -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Spain\n# Answer:\nEuro", "# Reasoning Path:\nPortugal -> location.country.languages_spoken -> Portuguese Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nPortuguese Language"], "ground_truth": ["Portuguese Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1490", "prediction": ["# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> language.language_family.geographic_distribution -> Bolivia\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> common.topic.notable_types -> Human Language\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Guaran\u00ed language -> common.topic.notable_types -> Human Language\n# Answer:\nGuaran\u00ed language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> language.language_family.geographic_distribution -> Peru\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nItalian Language", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Paseana -> base.thoroughbredracing.thoroughbred_racehorse.color -> Bay\n# Answer:\nPaseana", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Italian Language -> common.topic.notable_types -> Human Language\n# Answer:\nItalian Language", "# Reasoning Path:\nArgentina -> location.location.partially_contains -> Alto San Juan -> location.location.partially_contained_by -> m.0wg8lvc\n# Answer:\nAlto San Juan", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Paseana -> biology.organism.organism_type -> Horse\n# Answer:\nPaseana", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nItalian Language"], "ground_truth": ["Guaran\u00ed language", "Spanish Language", "Yiddish Language", "Italian Language", "Quechuan languages"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6461538461538462, "ans_precission": 0.7, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.6461538461538462, "path_ans_precision": 0.7, "path_ans_recall": 0.6}
{"id": "WebQTest-1493", "prediction": ["# Reasoning Path:\nWilfred Owen -> people.person.education -> m.0n18h9_ -> education.education.institution -> University of Reading\n# Answer:\nUniversity of Reading", "# Reasoning Path:\nWilfred Owen -> people.person.education -> m.010hdpvg -> education.education.institution -> Wakeman School\n# Answer:\nWakeman School", "# Reasoning Path:\nWilfred Owen -> common.topic.notable_types -> Fictional Character -> freebase.type_profile.published -> Published\n# Answer:\nFictional Character", "# Reasoning Path:\nWilfred Owen -> people.person.education -> m.0n18h9_ -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nWilfred Owen -> people.person.education -> m.010hdpvg -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nWilfred Owen -> common.topic.notable_types -> Fictional Character -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nFictional Character", "# Reasoning Path:\nWilfred Owen -> people.person.education -> m.0n18h9_ -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nWilfred Owen -> common.topic.notable_types -> Fictional Character -> freebase.type_hints.included_types -> Topic\n# Answer:\nFictional Character", "# Reasoning Path:\nWilfred Owen -> people.person.education -> m.010hdpvg -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nWilfred Owen -> people.person.education -> m.0n18h9_ -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date"], "ground_truth": ["University of Reading", "Wakeman School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1494", "prediction": ["# Reasoning Path:\nKevin Jonas -> music.artist.contribution -> m.0cqz7_j -> music.recording_contribution.album -> Jonas L.A.\n# Answer:\nJonas L.A.", "# Reasoning Path:\nKevin Jonas -> music.artist.contribution -> m.0cqz7_2 -> music.recording_contribution.album -> JONAS\n# Answer:\nJONAS", "# Reasoning Path:\nKevin Jonas -> tv.tv_actor.starring_roles -> m.09j06y6 -> tv.regular_tv_appearance.series -> Jonas L.A.\n# Answer:\nJonas L.A.", "# Reasoning Path:\nKevin Jonas -> people.person.parents -> Paul Kevin Jonas, Sr. -> people.person.children -> Nick Jonas\n# Answer:\nPaul Kevin Jonas, Sr.", "# Reasoning Path:\nKevin Jonas -> people.person.parents -> Paul Kevin Jonas, Sr. -> people.person.children -> Frankie Jonas\n# Answer:\nPaul Kevin Jonas, Sr.", "# Reasoning Path:\nKevin Jonas -> people.person.parents -> Paul Kevin Jonas, Sr. -> people.person.nationality -> United States of America\n# Answer:\nPaul Kevin Jonas, Sr.", "# Reasoning Path:\nKevin Jonas -> people.person.parents -> Denise Miller-Jonas -> people.person.spouse_s -> m.0nbwt6z\n# Answer:\nDenise Miller-Jonas", "# Reasoning Path:\nKevin Jonas -> people.person.parents -> Paul Kevin Jonas, Sr. -> people.person.children -> Joe Jonas\n# Answer:\nPaul Kevin Jonas, Sr.", "# Reasoning Path:\nKevin Jonas -> people.person.parents -> Paul Kevin Jonas, Sr. -> freebase.valuenotation.has_value -> Parents\n# Answer:\nPaul Kevin Jonas, Sr.", "# Reasoning Path:\nKevin Jonas -> tv.tv_actor.starring_roles -> m.09j06y6 -> tv.regular_tv_appearance.character -> Kevin Lucas\n# Answer:\nKevin Lucas"], "ground_truth": ["Danielle Deleasa"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1495", "prediction": ["# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> Hartsfield\u2013Jackson Atlanta International Airport -> aviation.airport.serves -> Atlanta\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> Hartsfield\u2013Jackson Atlanta International Airport -> location.location.containedby -> United States of America\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> Hartsfield\u2013Jackson Atlanta International Airport -> aviation.airport.serves -> Bullpen Rib House\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.focus_cities -> Fort Lauderdale\u2013Hollywood International Airport -> aviation.airport.serves -> Fort Lauderdale\n# Answer:\nFort Lauderdale\u2013Hollywood International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> Hartsfield\u2013Jackson Atlanta International Airport -> aviation.airport.serves -> Courtyard Atlanta Airport North/Virginia Avenue\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> Hartsfield\u2013Jackson Atlanta International Airport -> aviation.airport.hub_for -> Air Atlanta\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> General Mitchell International Airport -> aviation.airport.focus_city_for -> Republic Airlines\n# Answer:\nGeneral Mitchell International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> Hartsfield\u2013Jackson Atlanta International Airport -> location.location.containedby -> Georgia\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> Baltimore\u2013Washington International Airport -> aviation.airport.hub_for -> AirTran JetConnect\n# Answer:\nBaltimore\u2013Washington International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> Baltimore\u2013Washington International Airport -> aviation.airport.serves -> Washington, D.C.\n# Answer:\nBaltimore\u2013Washington International Airport"], "ground_truth": ["Logan International Airport", "LaGuardia Airport", "Denver International Airport", "Philadelphia International Airport", "Raleigh\u2013Durham International Airport", "Baltimore\u2013Washington International Airport"], "ans_acc": 0.16666666666666666, "ans_hit": 1, "ans_f1": 0.1818181818181818, "ans_precission": 0.2, "ans_recall": 0.16666666666666666, "path_f1": 0.1818181818181818, "path_precision": 0.2, "path_recall": 0.16666666666666666, "path_ans_f1": 0.1818181818181818, "path_ans_precision": 0.2, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTest-1496", "prediction": ["# Reasoning Path:\nParis -> symbols.namesake.named_after -> Parisii -> common.topic.image -> ParisiiCoins\n# Answer:\nParisii", "# Reasoning Path:\nParis -> symbols.namesake.named_after -> Parisii -> common.topic.article -> m.0283_dp\n# Answer:\nParisii", "# Reasoning Path:\nParis -> symbols.namesake.named_after -> Parisii -> common.topic.image -> The Vellavi sited south of the Arverni\u2014 whose name is embodied in Auvergne\n# Answer:\nParisii", "# Reasoning Path:\nParis -> common.topic.webpage -> m.02k8dbw -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nParis -> common.topic.webpage -> m.02k8dbw -> common.webpage.resource -> m.0bkwkw7\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nParis -> common.topic.webpage -> m.051qt2p -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nParis -> base.popstra.business_location.customer -> m.064_5j4 -> base.popstra.hangout.customer -> Katie Holmes\n# Answer:\nKatie Holmes", "# Reasoning Path:\nParis -> common.topic.webpage -> m.051qt2p -> common.webpage.resource -> Visitor Information\n# Answer:\nVisitor Information", "# Reasoning Path:\nParis -> common.topic.webpage -> m.09w2fsh -> common.webpage.resource -> Breaking: Famke Janssen returning to 'Nip/Tuck'\n# Answer:\nBreaking: Famke Janssen returning to 'Nip/Tuck'", "# Reasoning Path:\nParis -> base.popstra.business_location.customer -> m.063y1sc -> base.popstra.hangout.customer -> Beyonc\u00e9 Knowles\n# Answer:\nBeyonc\u00e9 Knowles"], "ground_truth": ["Parisii"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1499", "prediction": ["# Reasoning Path:\nSandra Bullock -> base.popstra.celebrity.dated -> m.065pshj -> base.popstra.dated.participant -> Ryan Gosling\n# Answer:\nRyan Gosling", "# Reasoning Path:\nSandra Bullock -> base.popstra.celebrity.dated -> m.065p_f4 -> base.popstra.dated.participant -> Michael Mailer\n# Answer:\nMichael Mailer", "# Reasoning Path:\nSandra Bullock -> base.popstra.celebrity.dated -> m.065pps3 -> base.popstra.dated.participant -> Matthew McConaughey\n# Answer:\nMatthew McConaughey", "# Reasoning Path:\nSandra Bullock -> base.popstra.celebrity.infidelity_participant -> m.0650s2k -> base.popstra.infidelity.victim -> Janine Lindemulder\n# Answer:\nJanine Lindemulder", "# Reasoning Path:\nSandra Bullock -> base.popstra.celebrity.infidelity_participant -> m.0650s2k -> base.popstra.infidelity.perpetrator -> Jesse James\n# Answer:\nJesse James", "# Reasoning Path:\nSandra Bullock -> award.award_nominee.award_nominations -> m.010b2qz4 -> freebase.valuenotation.is_reviewed -> Year\n# Answer:\nYear", "# Reasoning Path:\nSandra Bullock -> award.award_nominee.award_nominations -> m.010wrd_8 -> award.award_nomination.nominated_for -> Gravity\n# Answer:\nGravity", "# Reasoning Path:\nSandra Bullock -> award.award_nominee.award_nominations -> m.010b2qz4 -> award.award_nomination.ceremony -> 3rd AACTA International Awards\n# Answer:\n3rd AACTA International Awards", "# Reasoning Path:\nSandra Bullock -> award.award_nominee.award_nominations -> m.08hhgmc -> award.award_nomination.award -> Razzie Award for Worst Actress\n# Answer:\nRazzie Award for Worst Actress", "# Reasoning Path:\nSandra Bullock -> award.award_nominee.award_nominations -> m.010wrd_8 -> award.award_nomination.award -> Teen Choice Award for Choice Movie Actress\n# Answer:\nTeen Choice Award for Choice Movie Actress"], "ground_truth": ["Jesse James"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-150", "prediction": ["# Reasoning Path:\nCuba -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nCuba -> location.country.languages_spoken -> Spanish Language -> language.human_language.region -> Europe\n# Answer:\nSpanish Language", "# Reasoning Path:\nCuba -> location.country.languages_spoken -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nCuba -> location.country.languages_spoken -> Haitian Creole -> language.human_language.countries_spoken_in -> Haiti\n# Answer:\nHaitian Creole", "# Reasoning Path:\nCuba -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Argentina\n# Answer:\nSpanish Language", "# Reasoning Path:\nCuba -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc38hmp\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nCuba -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Barbados\n# Answer:\nSpanish Language", "# Reasoning Path:\nCuba -> location.country.languages_spoken -> Lucumi Language -> base.rosetta.languoid.parent -> Edekiri Group\n# Answer:\nLucumi Language", "# Reasoning Path:\nCuba -> location.country.languages_spoken -> Haitian Creole -> language.human_language.main_country -> Puerto Rico\n# Answer:\nHaitian Creole", "# Reasoning Path:\nCuba -> book.book_subject.works -> A simple Habana melody -> book.written_work.subjects -> France\n# Answer:\nA simple Habana melody"], "ground_truth": ["Lucumi Language", "Haitian Creole", "Spanish Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.41379310344827586, "path_precision": 0.3, "path_recall": 0.6666666666666666, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1500", "prediction": ["# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Jefferson County -> location.location.containedby -> Washington\n# Answer:\nJefferson County", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Washington -> location.location.containedby -> West Coast of the United States\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> common.topic.notable_for -> g.1257hvh8r\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nOlympic National Park -> base.usnationalparks.us_national_park.state -> Washington -> location.location.containedby -> West Coast of the United States\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Jefferson County -> location.statistical_region.population -> g.11b66j25ww\n# Answer:\nJefferson County", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Washington -> location.location.containedby -> Contiguous United States\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> base.usnationalparks.us_national_park.state -> Washington -> location.location.containedby -> Contiguous United States\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Jefferson County -> location.location.adjoin_s -> m.03jq636\n# Answer:\nJefferson County", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Washington -> location.location.containedby -> Northwestern United States\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> base.usnationalparks.us_national_park.state -> Washington -> location.location.containedby -> Northwestern United States\n# Answer:\nWashington"], "ground_truth": ["Washington"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1501", "prediction": ["# Reasoning Path:\nUnited Nations -> organization.organization.headquarters -> m.04300z7 -> location.mailing_address.citytown -> New York City\n# Answer:\nNew York City", "# Reasoning Path:\nUnited Nations -> organization.organization.headquarters -> m.04300z7 -> common.topic.notable_for -> g.1254yl5sx\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nUnited Nations -> book.book_subject.works -> Report on 2011 International Year Part 7: Photography of The African Continuum -> book.written_work.subjects -> Photography\n# Answer:\nReport on 2011 International Year Part 7: Photography of The African Continuum", "# Reasoning Path:\nUnited Nations -> organization.organization.founders -> United States of America -> location.country.first_level_divisions -> New York\n# Answer:\nUnited States of America", "# Reasoning Path:\nUnited Nations -> organization.organization.headquarters -> m.04300z7 -> freebase.valuenotation.has_no_value -> Street Address 2\n# Answer:\nStreet Address 2", "# Reasoning Path:\nUnited Nations -> book.book_subject.works -> Report on 2011 International Year Part 7: Photography of The African Continuum -> base.reviews.review.mentions -> New York City\n# Answer:\nReport on 2011 International Year Part 7: Photography of The African Continuum", "# Reasoning Path:\nUnited Nations -> book.book_subject.works -> Black History Month Enhanced by International Year for People of African Descent -> book.written_work.subjects -> United Nations Secretariat\n# Answer:\nBlack History Month Enhanced by International Year for People of African Descent", "# Reasoning Path:\nUnited Nations -> organization.organization.founders -> Argentina -> location.country.languages_spoken -> Spanish Language\n# Answer:\nArgentina", "# Reasoning Path:\nUnited Nations -> organization.organization.founders -> United States of America -> location.country.first_level_divisions -> Florida\n# Answer:\nUnited States of America", "# Reasoning Path:\nUnited Nations -> organization.organization.headquarters -> m.04300z7 -> freebase.valuenotation.has_no_value -> Street Address 3\n# Answer:\nStreet Address 3"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1502", "prediction": ["# Reasoning Path:\nUniversity Yale -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> San Francisco\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity Yale -> location.location.containedby -> Connecticut -> location.location.containedby -> United States of America\n# Answer:\nConnecticut", "# Reasoning Path:\nUniversity Yale -> education.educational_institution.subsidiary_or_constituent_schools -> Berkeley College -> location.location.containedby -> New Haven\n# Answer:\nBerkeley College", "# Reasoning Path:\nUniversity Yale -> location.location.containedby -> New Haven -> location.location.containedby -> United States of America\n# Answer:\nNew Haven", "# Reasoning Path:\nUniversity Yale -> education.educational_institution.subsidiary_or_constituent_schools -> Berkeley College -> location.location.containedby -> United States of America\n# Answer:\nBerkeley College", "# Reasoning Path:\nUniversity Yale -> education.educational_institution.subsidiary_or_constituent_schools -> Branford College -> location.location.containedby -> United States of America\n# Answer:\nBranford College", "# Reasoning Path:\nUniversity Yale -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Connecticut\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity Yale -> education.educational_institution.subsidiary_or_constituent_schools -> Calhoun College -> location.location.containedby -> New Haven\n# Answer:\nCalhoun College", "# Reasoning Path:\nUniversity Yale -> location.location.containedby -> Connecticut -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nConnecticut", "# Reasoning Path:\nUniversity Yale -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> New York City\n# Answer:\nUnited States of America"], "ground_truth": ["New Haven"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1503", "prediction": ["# Reasoning Path:\nThomas Paine -> people.deceased_person.place_of_death -> New York City -> common.topic.subject_of -> The Approaching 100th Anniversary of the Harlem Renaissance (part 2)\n# Answer:\nNew York City", "# Reasoning Path:\nThomas Paine -> people.deceased_person.place_of_death -> New York City -> location.administrative_division.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nThomas Paine -> base.kwebbase.kwtopic.has_sentences -> After his death in New York City in 1809, only six mourners attended his funeral. -> base.kwebbase.kwsentence.dates -> m.0c12k2g\n# Answer:\nAfter his death in New York City in 1809, only six mourners attended his funeral.", "# Reasoning Path:\nThomas Paine -> base.kwebbase.kwtopic.has_sentences -> After his death in New York City in 1809, only six mourners attended his funeral. -> base.kwebbase.kwsentence.previous_sentence -> In 1806, he was refused the right to vote because he was not an American citizen.\n# Answer:\nAfter his death in New York City in 1809, only six mourners attended his funeral.", "# Reasoning Path:\nThomas Paine -> base.kwebbase.kwtopic.has_sentences -> After working as corset-maker for a number of years, Paine became an excise officer. -> base.kwebbase.kwsentence.next_sentence -> The 1760s were a decade of rising prices, falling wages, and sporadic food riots.\n# Answer:\nAfter working as corset-maker for a number of years, Paine became an excise officer.", "# Reasoning Path:\nThomas Paine -> base.kwebbase.kwtopic.has_sentences -> As well as analyzing the reasons for discontent in European society, it outlined a plan for a rudimentary welfare state, financed by progressive taxation on property. -> base.kwebbase.kwsentence.next_sentence -> The tract became a central text of the British radical tradition.\n# Answer:\nAs well as analyzing the reasons for discontent in European society, it outlined a plan for a rudimentary welfare state, financed by progressive taxation on property.", "# Reasoning Path:\nThomas Paine -> people.person.quotations -> A bad cause will never be supported by bad means and bad men. -> media_common.quotation.subjects -> Causes\n# Answer:\nA bad cause will never be supported by bad means and bad men.", "# Reasoning Path:\nThomas Paine -> people.person.quotations -> A long habit of not thinking a thing wrong, gives it a superficial appearance of being right, and raises at first a formidable outcry in defense of custom. But the tumult soon subsides. Time makes more converts than reason. -> media_common.quotation.subjects -> Thoughts and Thinking\n# Answer:\nA long habit of not thinking a thing wrong, gives it a superficial appearance of being right, and raises at first a formidable outcry in defense of custom. But the tumult soon subsides. Time makes more converts than reason.", "# Reasoning Path:\nThomas Paine -> base.kwebbase.kwtopic.has_sentences -> As well as analyzing the reasons for discontent in European society, it outlined a plan for a rudimentary welfare state, financed by progressive taxation on property. -> base.kwebbase.kwsentence.previous_sentence -> At least eight editions were published in one year and it was quickly reprinted in the U.S.  When Burke replied, Paine wrote \\\"The Rights of Man, Part the Second,\\\" published in 1792.\n# Answer:\nAs well as analyzing the reasons for discontent in European society, it outlined a plan for a rudimentary welfare state, financed by progressive taxation on property.", "# Reasoning Path:\nThomas Paine -> people.person.quotations -> A long habit of not thinking a thing wrong, gives it a superficial appearance of being right, and raises at first a formidable outcry in defense of custom. But the tumult soon subsides. Time makes more converts than reason. -> common.topic.notable_for -> g.125f13wcy\n# Answer:\nA long habit of not thinking a thing wrong, gives it a superficial appearance of being right, and raises at first a formidable outcry in defense of custom. But the tumult soon subsides. Time makes more converts than reason."], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1504", "prediction": ["# Reasoning Path:\nBarack Obama, Sr. -> people.person.place_of_birth -> Nyang'oma Kogelo -> location.location.containedby -> Siaya County\n# Answer:\nNyang'oma Kogelo", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.place_of_birth -> Nyang'oma Kogelo -> location.location.containedby -> Nyanza Province\n# Answer:\nNyang'oma Kogelo", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.place_of_birth -> Nyang'oma Kogelo -> location.location.geolocation -> m.04nvlsq\n# Answer:\nNyang'oma Kogelo", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.place_of_birth -> Nyang'oma Kogelo -> common.topic.article -> m.04f2ymv\n# Answer:\nNyang'oma Kogelo", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.religion -> Atheism -> base.activism.activism_issue.activists -> Karl Marx\n# Answer:\nAtheism", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.spouse_s -> m.0j4l4yg -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.religion -> Atheism -> base.activism.activism_issue.activists -> Vladimir Lenin\n# Answer:\nAtheism", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.religion -> Atheism -> book.book_subject.works -> Atheism: The Case Against God\n# Answer:\nAtheism", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.religion -> Islam -> religion.religion.is_part_of -> Abrahamic religions\n# Answer:\nIslam", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.spouse_s -> m.0j4l4yg -> people.marriage.spouse -> Kezia Obama\n# Answer:\nKezia Obama"], "ground_truth": ["Nyang'oma Kogelo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1506", "prediction": ["# Reasoning Path:\nNew England Patriots -> sports.sports_team.arena_stadium -> Gillette Stadium -> location.location.containedby -> Foxborough\n# Answer:\nGillette Stadium", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.arena_stadium -> Gillette Stadium -> sports.sports_facility.teams -> New England Revolution\n# Answer:\nGillette Stadium", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.arena_stadium -> Gillette Stadium -> soccer.football_pitch.matches -> 2014 Pre-World Cup Friendly Mexico vs Portugal football match\n# Answer:\nGillette Stadium", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.colors -> White -> base.roses.rose_color.roses_of_this_color -> Rosa laevigata\n# Answer:\nWhite", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.colors -> White -> base.schemastaging.visual_color_extra.srgb -> m.010q1h63\n# Answer:\nWhite", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.colors -> White -> base.petbreeds.dog_coat_color.dog_breeds -> Border Collie\n# Answer:\nWhite", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.fight_song -> I'm Shipping Up to Boston -> music.composition.composer -> Dropkick Murphys\n# Answer:\nI'm Shipping Up to Boston", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.fight_song -> I'm Shipping Up to Boston -> music.composition.language -> American English\n# Answer:\nI'm Shipping Up to Boston", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.colors -> White -> base.roses.rose_color.roses_of_this_color -> Rosa 'Ambiente'\n# Answer:\nWhite", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.colors -> Nautical Blue -> common.topic.notable_for -> g.125dysf2k\n# Answer:\nNautical Blue"], "ground_truth": ["Gillette Stadium"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1507", "prediction": ["# Reasoning Path:\nKate Chopin -> people.person.profession -> Novelist -> fictional_universe.character_occupation.characters_with_this_occupation -> Seth Mortensen\n# Answer:\nNovelist", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Novelist -> fictional_universe.character_occupation.characters_with_this_occupation -> Nathaniel Hawthorne\n# Answer:\nNovelist", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Businessperson -> people.profession.specializations -> Actuary\n# Answer:\nBusinessperson", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Novelist -> fictional_universe.character_occupation.characters_with_this_occupation -> Bruce Butler\n# Answer:\nNovelist", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Novelist -> common.topic.notable_types -> Profession\n# Answer:\nNovelist", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Author -> fictional_universe.character_occupation.characters_with_this_occupation -> Ted Cole\n# Answer:\nAuthor", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Novelist -> base.descriptive_names.names.descriptive_name -> m.01064zd_\n# Answer:\nNovelist", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Businessperson -> people.profession.specializations -> Agent\n# Answer:\nBusinessperson", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Businessperson -> people.profession.specializations -> Antiquarian\n# Answer:\nBusinessperson", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Businessperson -> common.topic.subjects -> Krishna Mali\n# Answer:\nBusinessperson"], "ground_truth": ["Writer", "Author", "Novelist", "Businessperson"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.8571428571428571, "path_precision": 1.0, "path_recall": 0.75, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-1508", "prediction": ["# Reasoning Path:\nBill Clinton -> people.person.education -> m.0h1khf8 -> education.education.institution -> Edmund A. Walsh School of Foreign Service\n# Answer:\nEdmund A. Walsh School of Foreign Service", "# Reasoning Path:\nBill Clinton -> people.person.education -> m.02hvncd -> education.education.institution -> University College, Oxford\n# Answer:\nUniversity College, Oxford", "# Reasoning Path:\nBill Clinton -> people.person.education -> m.0125cddf -> education.education.institution -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nBill Clinton -> people.person.education -> m.0h1khf8 -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nBill Clinton -> people.person.education -> m.02hvncd -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nBill Clinton -> film.person_or_entity_appearing_in_film.films -> m.010l29t4 -> film.personal_film_appearance.film -> All About Ann: Governor Richards of the Lone Star State\n# Answer:\nAll About Ann: Governor Richards of the Lone Star State", "# Reasoning Path:\nBill Clinton -> people.person.education -> m.02hvncd -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nBill Clinton -> award.award_nominee.award_nominations -> m.0k08gry -> award.award_nomination.ceremony -> 47th Annual Grammy Awards\n# Answer:\n47th Annual Grammy Awards", "# Reasoning Path:\nBill Clinton -> award.award_nominee.award_nominations -> m.0k08gry -> award.award_nomination.award -> Grammy Award for Best Spoken Word Album\n# Answer:\nGrammy Award for Best Spoken Word Album", "# Reasoning Path:\nBill Clinton -> award.award_nominee.award_nominations -> m.0xnp1nm -> award.award_nomination.nominated_for -> Back To Work: Why We Need Smart Government For A Strong Economy\n# Answer:\nBack To Work: Why We Need Smart Government For A Strong Economy"], "ground_truth": ["Yale Law School", "University College, Oxford", "University Yale", "Georgetown University", "Edmund A. Walsh School of Foreign Service", "University of Oxford"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.37499999999999994, "ans_precission": 0.3, "ans_recall": 0.5, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.37499999999999994, "path_ans_precision": 0.3, "path_ans_recall": 0.5}
{"id": "WebQTest-1510", "prediction": ["# Reasoning Path:\nStaffordshire Bull Terrier -> biology.animal_breed.breed_of -> Dog -> base.descriptive_names.names.descriptive_name -> m.010x329h\n# Answer:\nDog", "# Reasoning Path:\nStaffordshire Bull Terrier -> biology.animal_breed.breed_of -> Dog -> base.petsupplyandaccessory.pet_apparel_user.pet_apparel_company -> The Woofer\n# Answer:\nDog", "# Reasoning Path:\nStaffordshire Bull Terrier -> common.image.appears_in_topic_gallery -> Pit bull\n# Answer:\nPit bull", "# Reasoning Path:\nStaffordshire Bull Terrier -> biology.animal_breed.breed_of -> Dog -> base.descriptive_names.names.descriptive_name -> m.010x32bc\n# Answer:\nDog", "# Reasoning Path:\nStaffordshire Bull Terrier -> common.topic.notable_types -> Animal breed -> freebase.type_profile.strict_included_types -> Animal\n# Answer:\nAnimal breed", "# Reasoning Path:\nStaffordshire Bull Terrier -> biology.animal_breed.breed_of -> Dog -> base.pethealth.pet_with_medical_condition.diseases_and_other_conditions_of_this_pet -> Ehrlichiosis\n# Answer:\nDog", "# Reasoning Path:\nStaffordshire Bull Terrier -> common.topic.notable_types -> Animal breed -> type.type.expected_by -> Breed\n# Answer:\nAnimal breed", "# Reasoning Path:\nStaffordshire Bull Terrier -> biology.animal_breed.breed_of -> Dog -> base.pethealth.pet_with_medical_condition.diseases_and_other_conditions_of_this_pet -> Canine heartworm disease\n# Answer:\nDog", "# Reasoning Path:\nStaffordshire Bull Terrier -> common.topic.notable_types -> Animal breed -> common.topic.article -> m.05c6g93\n# Answer:\nAnimal breed", "# Reasoning Path:\nStaffordshire Bull Terrier -> common.topic.notable_types -> Animal breed -> freebase.type_profile.strict_included_types -> Abstract\n# Answer:\nAnimal breed"], "ground_truth": ["Dog"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1513", "prediction": ["# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Sutter's Fort -> symbols.namesake.named_after -> John Sutter\n# Answer:\nSutter's Fort", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> B Street Theatre -> architecture.building.building_function -> Theatre\n# Answer:\nB Street Theatre", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Sutter's Fort -> common.topic.article -> m.0962k\n# Answer:\nSutter's Fort", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> B Street Theatre -> location.location.geolocation -> m.0131pvp5\n# Answer:\nB Street Theatre", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> California State Capitol Museum -> common.topic.notable_types -> Museum\n# Answer:\nCalifornia State Capitol Museum", "# Reasoning Path:\nSacramento -> sports.sports_team_location.teams -> Sacramento State Hornets men's basketball -> sports.sports_team.arena_stadium -> Colberg Court\n# Answer:\nSacramento State Hornets men's basketball", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> B Street Theatre -> freebase.valuenotation.has_value -> Architect\n# Answer:\nB Street Theatre", "# Reasoning Path:\nSacramento -> location.location.events -> 2010 Tour of California -> time.event.locations -> Agoura Hills\n# Answer:\n2010 Tour of California", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> California State Capitol Museum -> common.topic.notable_for -> g.1255tttbm\n# Answer:\nCalifornia State Capitol Museum", "# Reasoning Path:\nSacramento -> location.location.events -> 2010 Tour of California -> award.competition.winner -> Michael Rogers\n# Answer:\n2010 Tour of California"], "ground_truth": ["California Automobile Museum", "Sacramento Zoo", "Crocker Art Museum", "Raging Waters Sacramento", "Folsom Lake", "California State Railroad Museum", "B Street Theatre", "Sacramento History Museum", "California State Indian Museum", "California State Capitol Museum", "Sutter's Fort"], "ans_acc": 0.2727272727272727, "ans_hit": 1, "ans_f1": 0.3925233644859813, "ans_precission": 0.7, "ans_recall": 0.2727272727272727, "path_f1": 0.3925233644859813, "path_precision": 0.7, "path_recall": 0.2727272727272727, "path_ans_f1": 0.3925233644859813, "path_ans_precision": 0.7, "path_ans_recall": 0.2727272727272727}
{"id": "WebQTest-1515", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.parents -> Phyllis Piper -> common.topic.notable_for -> g.125c8dhdm\n# Answer:\nPhyllis Piper", "# Reasoning Path:\nMichael J. Fox -> people.person.parents -> Phyllis Piper -> people.person.employment_history -> m.0jb95tm\n# Answer:\nPhyllis Piper", "# Reasoning Path:\nMichael J. Fox -> people.person.parents -> Phyllis Piper -> people.person.children -> Kelli Fox\n# Answer:\nPhyllis Piper", "# Reasoning Path:\nMichael J. Fox -> people.person.parents -> Phyllis Piper -> people.person.children -> Jacki Fox\n# Answer:\nPhyllis Piper", "# Reasoning Path:\nMichael J. Fox -> people.person.parents -> William Fox -> freebase.valuenotation.has_value -> Parents\n# Answer:\nWilliam Fox", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvttg -> award.award_nomination.nominated_for -> Spin City\n# Answer:\nSpin City", "# Reasoning Path:\nMichael J. Fox -> people.person.parents -> Phyllis Piper -> people.person.children -> Karen Fox\n# Answer:\nPhyllis Piper", "# Reasoning Path:\nMichael J. Fox -> people.person.parents -> William Fox -> people.person.children -> Jacki Fox\n# Answer:\nWilliam Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.parents -> William Fox -> people.person.nationality -> Canada\n# Answer:\nWilliam Fox", "# Reasoning Path:\nMichael J. Fox -> common.topic.webpage -> m.08wwmzk -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Phyllis Piper", "William Fox"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1516", "prediction": ["# Reasoning Path:\nSanjay Gupta -> people.person.education -> m.02wnv1q -> education.education.institution -> University of Michigan\n# Answer:\nUniversity of Michigan", "# Reasoning Path:\nSanjay Gupta -> people.person.education -> m.05b8sml -> education.education.institution -> University of Michigan Health System\n# Answer:\nUniversity of Michigan Health System", "# Reasoning Path:\nSanjay Gupta -> education.honorary_degree_recipient.honorary_degrees -> m.0xn83q1 -> education.honorary_degree.institution -> University of Michigan\n# Answer:\nUniversity of Michigan", "# Reasoning Path:\nSanjay Gupta -> people.person.education -> m.02wnv1q -> education.education.degree -> Bachelor's degree\n# Answer:\nBachelor's degree", "# Reasoning Path:\nSanjay Gupta -> people.person.education -> m.0n1j65x -> education.education.institution -> Novi High School\n# Answer:\nNovi High School", "# Reasoning Path:\nSanjay Gupta -> people.person.education -> m.05b8sml -> education.education.degree -> Doctor of Medicine\n# Answer:\nDoctor of Medicine", "# Reasoning Path:\nSanjay Gupta -> award.award_nominee.award_nominations -> m.010g1lf8 -> award.award_nomination.award_nominee -> Ashley Bateman\n# Answer:\nAshley Bateman", "# Reasoning Path:\nSanjay Gupta -> award.award_nominee.award_nominations -> m.010g1lf8 -> award.award_nomination.nominated_for -> Sanjay Gupta MD\n# Answer:\nSanjay Gupta MD", "# Reasoning Path:\nSanjay Gupta -> award.award_nominee.award_nominations -> m.010g1lf8 -> award.award_nomination.ceremony -> 41st Daytime Creative Arts Emmy Awards\n# Answer:\n41st Daytime Creative Arts Emmy Awards", "# Reasoning Path:\nSanjay Gupta -> award.award_nominee.award_nominations -> m.010g1lf8 -> award.award_nomination.award_nominee -> Ben Tinker\n# Answer:\nBen Tinker"], "ground_truth": ["University of Michigan", "University of Michigan Health System", "Novi High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1518", "prediction": ["# Reasoning Path:\nNorth American Free Trade Agreement -> organization.membership_organization.members -> m.03w60cf -> organization.organization_membership.member -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.membership_organization.members -> m.03w60c3 -> organization.organization_membership.member -> Mexico\n# Answer:\nMexico", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.membership_organization.members -> m.03w60c8 -> organization.organization_membership.member -> Canada\n# Answer:\nCanada", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.organization.founders -> George H. W. Bush -> people.person.nationality -> United States of America\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nNorth American Free Trade Agreement -> freebase.valuenotation.has_value -> Place founded -> rdf-schema#range -> Location\n# Answer:\nPlace founded", "# Reasoning Path:\nNorth American Free Trade Agreement -> freebase.valuenotation.has_value -> Place founded -> rdf-schema#domain -> Organization\n# Answer:\nPlace founded", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.organization.founders -> George H. W. Bush -> people.person.profession -> Politician\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nNorth American Free Trade Agreement -> freebase.valuenotation.has_value -> Place founded -> type.property.expected_type -> Location\n# Answer:\nPlace founded", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.organization.founders -> George H. W. Bush -> government.political_appointer.appointees -> m.048zk8w\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.organization.founders -> George H. W. Bush -> people.person.profession -> Businessperson\n# Answer:\nGeorge H. W. Bush"], "ground_truth": ["Canada", "United States of America", "Mexico"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1519", "prediction": ["# Reasoning Path:\nCayman Islands -> location.location.containedby -> United Kingdom, with Dependencies and Territories -> base.aareas.schema.administrative_area.administrative_children -> United Kingdom\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nCayman Islands -> location.location.containedby -> Americas -> location.location.containedby -> Western Hemisphere\n# Answer:\nAmericas", "# Reasoning Path:\nCayman Islands -> location.location.containedby -> United Kingdom, with Dependencies and Territories -> common.topic.notable_for -> g.12586lyg9\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nCayman Islands -> location.location.containedby -> Greater Antilles -> location.location.containedby -> Antilles\n# Answer:\nGreater Antilles", "# Reasoning Path:\nCayman Islands -> location.statistical_region.internet_users_percent_population -> g.11b60v9ztp\n# Answer:\nlocation.statistical_region.internet_users_percent_population", "# Reasoning Path:\nCayman Islands -> location.location.containedby -> United Kingdom, with Dependencies and Territories -> base.aareas.schema.administrative_area.administrative_children -> Akrotiri and Dhekelia\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nCayman Islands -> location.location.containedby -> United Kingdom, with Dependencies and Territories -> location.country.first_level_divisions -> Akrotiri and Dhekelia\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nCayman Islands -> location.administrative_division.first_level_division_of -> United Kingdom, with Dependencies and Territories -> base.aareas.schema.administrative_area.administrative_children -> United Kingdom\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nCayman Islands -> location.location.containedby -> United Kingdom, with Dependencies and Territories -> base.aareas.schema.administrative_area.administrative_children -> Anguilla\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nCayman Islands -> location.administrative_division.first_level_division_of -> United Kingdom, with Dependencies and Territories -> location.country.first_level_divisions -> Akrotiri and Dhekelia\n# Answer:\nUnited Kingdom, with Dependencies and Territories"], "ground_truth": ["Americas", "North America"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.16666666666666669, "ans_precission": 0.1, "ans_recall": 0.5, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 0.1, "path_ans_recall": 0.5}
{"id": "WebQTest-1520", "prediction": ["# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Portuguese Language -> language.human_language.dialects -> Brazilian Portuguese\n# Answer:\nPortuguese Language", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Brazilian Portuguese -> language.language_dialect.language -> Portuguese Language\n# Answer:\nBrazilian Portuguese", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc39sjp\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fm01\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc385hl\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp"], "ground_truth": ["Portuguese Language", "Brazilian Portuguese", "Italian Language"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.36363636363636365, "ans_precission": 0.25, "ans_recall": 0.6666666666666666, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 0.36363636363636365, "path_ans_precision": 0.25, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1522", "prediction": ["# Reasoning Path:\nKroy Biermann -> american_football.football_player.games -> m.07nvpcg -> american_football.player_game_statistics.team -> Atlanta Falcons\n# Answer:\nAtlanta Falcons", "# Reasoning Path:\nKroy Biermann -> base.schemastaging.athlete_extra.salary -> m.0j8kwh4 -> base.schemastaging.athlete_salary.team -> Atlanta Falcons\n# Answer:\nAtlanta Falcons", "# Reasoning Path:\nKroy Biermann -> american_football.football_player.games -> m.07sh4xh -> american_football.player_game_statistics.team -> Atlanta Falcons\n# Answer:\nAtlanta Falcons", "# Reasoning Path:\nKroy Biermann -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#range -> Person\n# Answer:\nChildren", "# Reasoning Path:\nKroy Biermann -> american_football.football_player.games -> m.07tds9c -> american_football.player_game_statistics.team -> Atlanta Falcons\n# Answer:\nAtlanta Falcons", "# Reasoning Path:\nKroy Biermann -> american_football.football_player.games -> m.07nvpcg -> american_football.player_game_statistics.season -> 2008 NFL season\n# Answer:\n2008 NFL season", "# Reasoning Path:\nKroy Biermann -> freebase.valuenotation.is_reviewed -> Children -> type.property.schema -> Person\n# Answer:\nChildren", "# Reasoning Path:\nKroy Biermann -> base.schemastaging.athlete_extra.salary -> m.0j34ls4 -> base.schemastaging.athlete_salary.team -> Atlanta Falcons\n# Answer:\nAtlanta Falcons", "# Reasoning Path:\nKroy Biermann -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#domain -> Person\n# Answer:\nChildren", "# Reasoning Path:\nKroy Biermann -> base.schemastaging.athlete_extra.salary -> m.0j8kwh4 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar"], "ground_truth": ["Atlanta Falcons"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.3225806451612903, "path_precision": 0.5, "path_recall": 0.23809523809523808, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1523", "prediction": ["# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> Super Bowl XLIII -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XLIII", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> Super Bowl XLIII -> sports.sports_championship_event.runner_up -> Arizona Cardinals\n# Answer:\nSuper Bowl XLIII", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> 1996 AFC Championship Game -> sports.sports_championship_event.championship -> AFC Championship Game\n# Answer:\n1996 AFC Championship Game", "# Reasoning Path:\nPittsburgh Steelers -> american_football.football_team.current_head_coach -> Mike Tomlin -> american_football.football_coach.coaching_history -> m.05cvv1h\n# Answer:\nMike Tomlin", "# Reasoning Path:\nPittsburgh Steelers -> common.topic.webpage -> m.0gw54r6 -> common.webpage.in_index -> Blissful Master Index\n# Answer:\nBlissful Master Index", "# Reasoning Path:\nPittsburgh Steelers -> american_football.football_team.current_head_coach -> Mike Tomlin -> sports.pro_athlete.teams -> m.05cvv_4\n# Answer:\nMike Tomlin", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> 1996 AFC Championship Game -> common.topic.article -> m.0hzps_m\n# Answer:\n1996 AFC Championship Game", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> 2006 AFC Championship Game -> sports.sports_championship_event.runner_up -> Denver Broncos\n# Answer:\n2006 AFC Championship Game", "# Reasoning Path:\nPittsburgh Steelers -> american_football.football_team.current_head_coach -> Mike Tomlin -> american_football.football_coach.coaching_history -> m.05cvsxx\n# Answer:\nMike Tomlin", "# Reasoning Path:\nPittsburgh Steelers -> common.topic.webpage -> m.0gw54r6 -> common.webpage.category -> Curated Topic\n# Answer:\nCurated Topic"], "ground_truth": ["Super Bowl XLIII"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1524", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.notable_figures -> Muhammad in Islam -> book.author.works_written -> Sayings of Mohammed\n# Answer:\nMuhammad in Islam", "# Reasoning Path:\nIslam -> religion.religion.founding_figures -> Muhammad in Islam -> book.author.works_written -> Sayings of Mohammed\n# Answer:\nMuhammad in Islam", "# Reasoning Path:\nIslam -> religion.religion.notable_figures -> Muhammad in Islam -> influence.influence_node.influenced -> Muhammad ibn Ya'qub al-Kulayni\n# Answer:\nMuhammad in Islam", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Hinduism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.notable_figures -> Abu Bakr -> people.person.parents -> Uthman Abu Quhafa\n# Answer:\nAbu Bakr", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Sikhism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.article -> m.055v4j\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.notable_figures -> Muhammad in Islam -> influence.influence_node.influenced -> Abd Allah ibn Abbas\n# Answer:\nMuhammad in Islam", "# Reasoning Path:\nIslam -> religion.religion.notable_figures -> Abu Bakr -> people.person.children -> Umm Kulthum bint Abi Bakr\n# Answer:\nAbu Bakr", "# Reasoning Path:\nIslam -> religion.religion.notable_figures -> Ali -> people.person.parents -> Abu Talib ibn Abd al-Muttalib\n# Answer:\nAli"], "ground_truth": ["Muhammad in Islam"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1525", "prediction": ["# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.06vz4t9 -> military.military_combatant_group.combatants -> Argentina\n# Answer:\nArgentina", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> France\n# Answer:\nFrance", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z973l -> military.military_combatant_group.combatants -> Iraq\n# Answer:\nIraq", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> royalty.kingdom.rulers -> Abdullah of Saudi Arabia\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> location.country.official_language -> Arabic Language\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.locations -> Arabian Peninsula -> location.location.contains -> Saudi Arabia\n# Answer:\nArabian Peninsula", "# Reasoning Path:\nGulf War -> time.event.includes_event -> 1991 uprisings in Iraq -> military.military_conflict.combatants -> m.04y_t_s\n# Answer:\n1991 uprisings in Iraq", "# Reasoning Path:\nGulf War -> time.event.includes_event -> 1991 uprisings in Iraq -> time.event.locations -> Iraq\n# Answer:\n1991 uprisings in Iraq"], "ground_truth": ["Argentina", "United States of America", "Saudi Arabia", "France", "Australia", "United Kingdom", "Iraq"], "ans_acc": 0.8571428571428571, "ans_hit": 1, "ans_f1": 0.8780487804878048, "ans_precission": 0.9, "ans_recall": 0.8571428571428571, "path_f1": 0.15306122448979592, "path_precision": 0.6, "path_recall": 0.08771929824561403, "path_ans_f1": 0.923076923076923, "path_ans_precision": 1.0, "path_ans_recall": 0.8571428571428571}
{"id": "WebQTest-1526", "prediction": ["# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> Italian Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> Italian Language -> language.human_language.main_country -> Italy\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Italy\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> Italian Language -> language.human_language.region -> Europe\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> Romansh language -> language.human_language.region -> Europe\n# Answer:\nRomansh language", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> German Language -> language.human_language.countries_spoken_in -> Liechtenstein\n# Answer:\nGerman Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> Italian Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d0tc3g\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita"], "ground_truth": ["German Language", "Romansh language", "French", "Italian Language"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-1527", "prediction": ["# Reasoning Path:\nBarack Obama -> government.politician.party -> m.03gjhww -> government.political_party_tenure.party -> Democratic Party\n# Answer:\nDemocratic Party", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.party -> Democratic Party\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> government.politician.party -> m.03gjhww -> freebase.valuenotation.is_reviewed -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.venues -> Pepsi Center\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> government.politician.party -> m.03gjhww -> freebase.valuenotation.is_reviewed -> Party\n# Answer:\nParty", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.speeches -> m.04j00hr\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.subjects -> 2012 Presidential debate\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Bound Man -> book.written_work.subjects -> United States of America\n# Answer:\nA Bound Man", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.venues -> Sports Authority Field at Mile High\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Writer's Journey to Selma, Alabama -> book.written_work.subjects -> Selma\n# Answer:\nA Writer's Journey to Selma, Alabama"], "ground_truth": ["Democratic Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.2, "path_recall": 0.3333333333333333, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1528", "prediction": ["# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.03jq21w -> film.performance.film -> The Butterfly Effect\n# Answer:\nThe Butterfly Effect", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.03jq2p4 -> film.performance.film -> 3:10 to Yuma\n# Answer:\n3:10 to Yuma", "# Reasoning Path:\nLogan Lerman -> common.topic.article -> m.04f1sc\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.04ls5pn -> film.performance.film -> My One and Only\n# Answer:\nMy One and Only", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.03jq21w -> film.performance.character -> Evan Treborn\n# Answer:\nEvan Treborn", "# Reasoning Path:\nLogan Lerman -> people.person.profession -> Musician -> freebase.equivalent_topic.equivalent_type -> Musical Artist\n# Answer:\nMusician", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.03jq2p4 -> film.performance.character -> William Evans\n# Answer:\nWilliam Evans", "# Reasoning Path:\nLogan Lerman -> people.person.profession -> Musician -> common.topic.notable_types -> Profession\n# Answer:\nMusician", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.04ls5pn -> film.performance.character -> George Devereaux\n# Answer:\nGeorge Devereaux", "# Reasoning Path:\nLogan Lerman -> people.person.profession -> Musician -> award.award_category.winners -> Lisa Heller\n# Answer:\nMusician"], "ground_truth": ["Gamer", "3:10 to Yuma", "The Perks of Being a Wallflower", "Percy Jackson & the Olympians: The Lightning Thief", "What Women Want", "The Three Musketeers", "Meet Bill", "Hoot", "My One and Only", "Percy Jackson: Sea of Monsters", "Noah", "Stuck in Love", "The Scribe", "Riding in Cars with Boys", "The Butterfly Effect", "The Flannerys", "The Patriot", "Fury", "The Only Living Boy in New York", "The Number 23", "A Painted House"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0.19354838709677416, "ans_precission": 0.3, "ans_recall": 0.14285714285714285, "path_f1": 0.13953488372093023, "path_precision": 0.3, "path_recall": 0.09090909090909091, "path_ans_f1": 0.19354838709677416, "path_ans_precision": 0.3, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-1529", "prediction": ["# Reasoning Path:\nConstantine the Great -> people.deceased_person.place_of_death -> Nicomedia -> common.topic.notable_for -> g.1255y5dbj\n# Answer:\nNicomedia", "# Reasoning Path:\nConstantine the Great -> people.deceased_person.place_of_death -> Nicomedia -> location.location.people_born_here -> Aaron ben Elijah\n# Answer:\nNicomedia", "# Reasoning Path:\nConstantine the Great -> people.deceased_person.place_of_death -> Nicomedia -> common.topic.notable_types -> City/Town/Village\n# Answer:\nNicomedia", "# Reasoning Path:\nConstantine the Great -> symbols.name_source.namesakes -> Ni\u0161 Constantine the Great Airport -> location.location.containedby -> Yugoslavia\n# Answer:\nNi\u0161 Constantine the Great Airport", "# Reasoning Path:\nConstantine the Great -> people.deceased_person.place_of_death -> Nicomedia -> location.location.people_born_here -> Arrian\n# Answer:\nNicomedia", "# Reasoning Path:\nConstantine the Great -> symbols.name_source.namesakes -> Ni\u0161 Constantine the Great Airport -> common.topic.image -> Thompsonfly-ini\n# Answer:\nNi\u0161 Constantine the Great Airport", "# Reasoning Path:\nConstantine the Great -> people.deceased_person.place_of_death -> Nicomedia -> location.location.people_born_here -> Donatus of Arezzo\n# Answer:\nNicomedia", "# Reasoning Path:\nConstantine the Great -> symbols.name_source.namesakes -> Arch of Constantine -> common.topic.article -> m.02p9gq\n# Answer:\nArch of Constantine", "# Reasoning Path:\nConstantine the Great -> people.person.sibling_s -> m.0w15spf -> people.sibling_relationship.sibling -> Eutropia\n# Answer:\nEutropia", "# Reasoning Path:\nConstantine the Great -> people.person.sibling_s -> m.0k6lgkh -> people.sibling_relationship.sibling -> Flavia Julia Constantia\n# Answer:\nFlavia Julia Constantia"], "ground_truth": ["Nicomedia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-153", "prediction": ["# Reasoning Path:\nMadrid -> base.arthist.helynevek.intezmeny -> Thyssen-Bornemisza Museum -> location.location.geolocation -> m.02_sq5m\n# Answer:\nThyssen-Bornemisza Museum", "# Reasoning Path:\nMadrid -> travel.travel_destination.tourist_attractions -> Thyssen-Bornemisza Museum -> location.location.geolocation -> m.02_sq5m\n# Answer:\nThyssen-Bornemisza Museum", "# Reasoning Path:\nMadrid -> location.statistical_region.population -> g.11b7vbxnhf\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nMadrid -> base.arthist.helynevek.intezmeny -> Thyssen-Bornemisza Museum -> visual_art.art_owner.artworks_owned -> m.01012z21\n# Answer:\nThyssen-Bornemisza Museum", "# Reasoning Path:\nMadrid -> base.arthist.helynevek.intezmeny -> Museo Nacional Del Prado -> exhibitions.exhibition_venue.exhibitions_at_this_venue -> m.04sg179\n# Answer:\nMuseo Nacional Del Prado", "# Reasoning Path:\nMadrid -> base.arthist.helynevek.intezmeny -> Collecci\u00f3n Fern\u00e1ndez Araoz -> common.topic.notable_types -> Art owner\n# Answer:\nCollecci\u00f3n Fern\u00e1ndez Araoz", "# Reasoning Path:\nMadrid -> location.statistical_region.population -> g.11btt59cjx\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nMadrid -> base.arthist.helynevek.intezmeny -> Thyssen-Bornemisza Museum -> visual_art.art_owner.artworks_owned -> m.046rz_t\n# Answer:\nThyssen-Bornemisza Museum", "# Reasoning Path:\nMadrid -> base.arthist.helynevek.intezmeny -> Museo Nacional Del Prado -> visual_art.art_owner.artworks_owned -> m.043zgzc\n# Answer:\nMuseo Nacional Del Prado", "# Reasoning Path:\nMadrid -> base.arthist.helynevek.intezmeny -> Collecci\u00f3n Fern\u00e1ndez Araoz -> common.topic.notable_for -> g.125c_8fc5\n# Answer:\nCollecci\u00f3n Fern\u00e1ndez Araoz"], "ground_truth": ["Paseo del Prado", "El Escorial", "Plaza de Cibeles", "Parque Warner Madrid", "Museo Nacional Centro de Arte Reina Sof\u00eda", "Palace of la Bolsa de Madrid", "Museum of L\u00e1zaro Galdiano", "Royal Palace of Madrid", "Gran V\u00eda", "Plaza Mayor, Madrid", "Madrid Arena", "Madrid Marathon", "Thyssen-Bornemisza Museum", "Festimad", "Puerta del Sol", "Sorolla Museum", "Buen Retiro Park", "Almudena Cathedral", "Museo Nacional Del Prado", "Summercase", "Temple of Debod", "La Vaguada"], "ans_acc": 0.09090909090909091, "ans_hit": 1, "ans_f1": 0.15789473684210525, "ans_precission": 0.6, "ans_recall": 0.09090909090909091, "path_f1": 0.15384615384615385, "path_precision": 0.5, "path_recall": 0.09090909090909091, "path_ans_f1": 0.15789473684210525, "path_ans_precision": 0.6, "path_ans_recall": 0.09090909090909091}
{"id": "WebQTest-1531", "prediction": ["# Reasoning Path:\nMali -> location.location.containedby -> West Africa -> location.location.containedby -> Africa\n# Answer:\nWest Africa", "# Reasoning Path:\nMali -> location.location.containedby -> West Africa -> book.book_subject.works -> Sundiata\n# Answer:\nWest Africa", "# Reasoning Path:\nMali -> location.location.containedby -> West Africa -> location.location.contains -> Togo\n# Answer:\nWest Africa", "# Reasoning Path:\nMali -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d0tc16\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nMali -> base.locations.countries.continent -> Africa -> base.locations.continents.countries_within -> Algeria\n# Answer:\nAfrica", "# Reasoning Path:\nMali -> base.locations.countries.continent -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica", "# Reasoning Path:\nMali -> location.location.containedby -> West Africa -> location.location.contains -> Benin\n# Answer:\nWest Africa", "# Reasoning Path:\nMali -> location.location.containedby -> Africa -> base.locations.continents.countries_within -> Algeria\n# Answer:\nAfrica", "# Reasoning Path:\nMali -> location.location.containedby -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica", "# Reasoning Path:\nMali -> base.locations.countries.continent -> Africa -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nAfrica"], "ground_truth": ["Africa", "West Africa"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1533", "prediction": ["# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.place_of_birth -> g.120mc0gk\n# Answer:\nfictional_universe.fictional_character.place_of_birth", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.places_lived -> Tatooine -> fictional_universe.fictional_setting.characters_that_have_lived_here -> Darth Vader\n# Answer:\nTatooine", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.places_lived -> Tatooine -> common.topic.notable_for -> g.1256pb32r\n# Answer:\nTatooine", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.places_lived -> Tatooine -> fictional_universe.fictional_setting.fictional_characters_born_here -> Darth Vader\n# Answer:\nTatooine", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.places_lived -> Tatooine -> fictional_universe.fictional_setting.characters_that_have_lived_here -> Beru Lars\n# Answer:\nTatooine", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.places_lived -> Tatooine -> fictional_universe.fictional_setting.characters_that_have_lived_here -> C-3PO\n# Answer:\nTatooine", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.powers_or_abilities -> Levitation -> common.topic.notable_for -> g.1255179v2\n# Answer:\nLevitation", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.powers_or_abilities -> Levitation -> fictional_universe.character_powers.characters_with_this_ability -> Darth Vader\n# Answer:\nLevitation", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.powers_or_abilities -> Levitation -> common.topic.article -> m.02ty99\n# Answer:\nLevitation", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.powers_or_abilities -> Levitation -> fictional_universe.character_powers.characters_with_this_ability -> Alley-Kat-Abra\n# Answer:\nLevitation"], "ground_truth": ["Polis Massa"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1534", "prediction": ["# Reasoning Path:\nOakland Raiders -> american_football.football_team.current_head_coach -> Tony Sparano -> sports.pro_athlete.sports_played_professionally -> m.0d6m4sl\n# Answer:\nTony Sparano", "# Reasoning Path:\nOakland Raiders -> american_football.football_team.current_head_coach -> Tony Sparano -> people.person.profession -> Coach\n# Answer:\nTony Sparano", "# Reasoning Path:\nOakland Raiders -> american_football.football_team.current_head_coach -> Tony Sparano -> people.person.profession -> American football player\n# Answer:\nTony Sparano", "# Reasoning Path:\nOakland Raiders -> american_football.football_team.historical_coaching_staff -> m.04ykh5s -> american_football.football_historical_coach_position.coach -> Lane Kiffin\n# Answer:\nLane Kiffin", "# Reasoning Path:\nOakland Raiders -> american_football.football_team.historical_coaching_staff -> m.04ykh_2 -> american_football.football_historical_coach_position.coach -> Mike Shanahan\n# Answer:\nMike Shanahan", "# Reasoning Path:\nOakland Raiders -> american_football.football_team.historical_coaching_staff -> m.04ykh5s -> american_football.football_historical_coach_position.position -> Head coach\n# Answer:\nHead coach", "# Reasoning Path:\nOakland Raiders -> american_football.football_team.historical_coaching_staff -> m.04ykh_2 -> american_football.football_historical_coach_position.position -> Head coach\n# Answer:\nHead coach", "# Reasoning Path:\nOakland Raiders -> american_football.football_team.historical_coaching_staff -> m.011xd62y -> american_football.football_historical_coach_position.coach -> Dennis Allen\n# Answer:\nDennis Allen", "# Reasoning Path:\nOakland Raiders -> american_football.football_team.historical_coaching_staff -> m.011xd62y -> american_football.football_historical_coach_position.position -> Head coach\n# Answer:\nHead coach", "# Reasoning Path:\nOakland Raiders -> american_football.football_team.home_games -> Atlanta Falcons at Oakland Raiders, 2008-11-02 -> american_football.football_game.rushing -> m.07932gb\n# Answer:\nAtlanta Falcons at Oakland Raiders, 2008-11-02"], "ground_truth": ["Tony Sparano"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1535", "prediction": ["# Reasoning Path:\nMicrosoft Corporation -> organization.organization.place_founded -> Albuquerque -> location.location.containedby -> United States of America\n# Answer:\nAlbuquerque", "# Reasoning Path:\nMicrosoft Corporation -> organization.organization.place_founded -> Albuquerque -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nAlbuquerque", "# Reasoning Path:\nMicrosoft Corporation -> organization.organization_sector.organizations_in_this_sector -> Exclaimer -> organization.organization.place_founded -> United Kingdom\n# Answer:\nExclaimer", "# Reasoning Path:\nMicrosoft Corporation -> business.business_operation.current_liabilities -> m.0906wt9 -> measurement_unit.dated_money_value.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nMicrosoft Corporation -> organization.organization_sector.organizations_in_this_sector -> Exclaimer -> business.business_operation.industry -> Software\n# Answer:\nExclaimer", "# Reasoning Path:\nMicrosoft Corporation -> organization.organization_sector.organizations_in_this_sector -> Exclaimer -> freebase.valuenotation.is_reviewed -> Image\n# Answer:\nExclaimer", "# Reasoning Path:\nMicrosoft Corporation -> organization.organization_sector.organizations_in_this_sector -> Exclaimer -> business.business_operation.industry -> Computer\n# Answer:\nExclaimer", "# Reasoning Path:\nMicrosoft Corporation -> organization.organization_sector.organizations_in_this_sector -> Exclaimer -> business.business_operation.industry -> Information technology\n# Answer:\nExclaimer"], "ground_truth": ["Redmond"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1536", "prediction": ["# Reasoning Path:\nJohn Steinbeck -> people.person.education -> m.0n1dbm0 -> education.education.institution -> Salinas High School\n# Answer:\nSalinas High School", "# Reasoning Path:\nJohn Steinbeck -> people.person.education -> m.0n1dbm0 -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nJohn Steinbeck -> people.person.education -> m.02wpt95 -> education.education.institution -> Stanford University\n# Answer:\nStanford University", "# Reasoning Path:\nJohn Steinbeck -> people.person.education -> m.02wpt95 -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nJohn Steinbeck -> award.award_nominee.award_nominations -> m.0b9zmns -> award.award_nomination.ceremony -> 1955 National Book Awards\n# Answer:\n1955 National Book Awards", "# Reasoning Path:\nJohn Steinbeck -> book.author.works_written -> A Russian Journal -> common.topic.notable_for -> g.1256s0x_d\n# Answer:\nA Russian Journal", "# Reasoning Path:\nJohn Steinbeck -> book.author.works_written -> A Russian Journal -> book.book.editions -> A Russian journal\n# Answer:\nA Russian Journal", "# Reasoning Path:\nJohn Steinbeck -> book.author.works_written -> A letter -> common.topic.notable_for -> g.125fb994m\n# Answer:\nA letter", "# Reasoning Path:\nJohn Steinbeck -> award.award_nominee.award_nominations -> m.0b9zmns -> award.award_nomination.nominated_for -> Sweet Thursday\n# Answer:\nSweet Thursday", "# Reasoning Path:\nJohn Steinbeck -> book.author.works_written -> A primer on the thirties -> common.topic.notable_for -> g.1254zp4_p\n# Answer:\nA primer on the thirties"], "ground_truth": ["Stanford University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1537", "prediction": ["# Reasoning Path:\nNiels Henrik David Bohr -> base.nobelprizes.nobel_prize_winner.nobel_honor -> m.064wn6r -> base.nobelprizes.nobel_honor.subject_area -> Quantum mechanics\n# Answer:\nQuantum mechanics", "# Reasoning Path:\nNiels Henrik David Bohr -> symbols.name_source.namesakes -> Niels Bohr Institute -> common.topic.notable_for -> g.1258dvpk0\n# Answer:\nNiels Bohr Institute", "# Reasoning Path:\nNiels Henrik David Bohr -> symbols.name_source.namesakes -> Niels Bohr Institute -> business.employer.employees -> m.04kr8tr\n# Answer:\nNiels Bohr Institute", "# Reasoning Path:\nNiels Henrik David Bohr -> symbols.name_source.namesakes -> 3948 Bohr -> astronomy.star_system_body.star_system -> Solar System\n# Answer:\n3948 Bohr", "# Reasoning Path:\nNiels Henrik David Bohr -> symbols.name_source.namesakes -> Niels Bohr Institute -> common.topic.article -> m.0dz_p\n# Answer:\nNiels Bohr Institute", "# Reasoning Path:\nNiels Henrik David Bohr -> symbols.name_source.namesakes -> 3948 Bohr -> astronomy.orbital_relationship.orbits -> Sun\n# Answer:\n3948 Bohr", "# Reasoning Path:\nNiels Henrik David Bohr -> symbols.name_source.namesakes -> Bohr -> common.topic.notable_types -> Extraterrestrial location\n# Answer:\nBohr", "# Reasoning Path:\nNiels Henrik David Bohr -> base.kwebbase.kwtopic.connections_from -> niels henrik david bohr activist with julius robert oppenheimer -> base.kwebbase.kwconnection.other -> J. Robert Oppenheimer\n# Answer:\nniels henrik david bohr activist with julius robert oppenheimer", "# Reasoning Path:\nNiels Henrik David Bohr -> symbols.name_source.namesakes -> 3948 Bohr -> common.topic.notable_for -> g.125c3yszl\n# Answer:\n3948 Bohr", "# Reasoning Path:\nNiels Henrik David Bohr -> symbols.name_source.namesakes -> Bohr -> common.topic.article -> m.0469fk\n# Answer:\nBohr"], "ground_truth": ["Quantum mechanics"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1538", "prediction": ["# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> common.topic.article -> m.03tgtz\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> film.film.production_companies -> Walt Disney Pictures\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> Holes -> book.written_work.next_in_series -> Stanley Yelnats' Survival Guide to Camp Green Lake\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> common.topic.article -> m.0dq62d\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> The Boy Who Lost His Face -> common.topic.notable_types -> Book\n# Answer:\nThe Boy Who Lost His Face", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> The Cardturner -> award.award_nominated_work.award_nominations -> m.0g2978l\n# Answer:\nThe Cardturner", "# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> film.film.starring -> m.02tb466\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> film.film.production_companies -> Phoenix Pictures\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> The Cardturner -> common.topic.article -> m.0fq08x4\n# Answer:\nThe Cardturner", "# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> film.film.production_companies -> The Walt Disney Company\n# Answer:\nHoles"], "ground_truth": ["Small Steps (Readers Circle)", "Wayside School Gets A Little Stranger", "Someday Angeline", "A Flying Birthday Cake?", "Holes", "Marvin Redpost", "Johnny's in the Basement", "Class President (A Stepping Stone Book(TM))", "Wayside School Gets a Little Stranger (rack) (Wayside School)", "Stanley Yelnats' Survival Guide to Camp Green Lake", "Why Pick on Me?", "Marvin Redpost.", "There's a Boy in the Girls' Bathroom", "A magic crystal?", "More Sideways Arithmetic from Wayside School", "Wayside School is falling down (Celebrate reading, Scott Foresman)", "Boy Who Lost His Face", "Hoyos/Holes", "Wayside School is Falling Down", "Monkey soup", "Holes (Readers Circle)", "There's a boy in the girls bathroom", "Alone in His Teacher's House", "Der Fluch des David Ballinger. ( Ab 11 J.).", "Sixth Grade Secrets (Apple Paperbacks)", "Holes. (Lernmaterialien)", "Why Pick on Me? (A Stepping Stone Book(TM))", "Wayside School Is Falling Down", "Super Fast, Out of Control! (A Stepping Stone Book(TM))", "Sideways Stories from Wayside School", "A Flying Birthday Cake? (A Stepping Stone Book(TM))", "Sideways stories from Wayside School", "Wayside School Gets a Little Stranger", "Wayside School is falling down", "Someday Angeline (Avon/Camelot Book)", "Hay Un Chico En El Bano De Las Chicas", "Class President", "Wayside School Boxed Set", "Il y a un gar\u00e7on dans les toilettes des filles", "Holes (with \\\"Connections\\\") HRW Library (HRW library)", "Stanley Yelnats Survival Guide to Camp Green Lake", "Kidnapped at Birth? (A Stepping Stone Book(TM))", "L\u00f6cher", "Holes (Listening Library)", "Holes (Cascades)", "Sideways Arithmetic from Wayside School", "Holes (Yearling Books)", "Holes (World Book Day 2001)", "Small steps", "Dogs Don't Tell Jokes", "Holes Activity Pack", "Kidnapped at Birth?", "Sixth grade secrets", "Wayside School Collection", "The Cardturner", "Sixth Grade Secrets", "Louis Sacher Collection", "More Sideways Arithmetic From Wayside School", "Wayside School gets a little stranger", "Sideways Arithmetic From Wayside School", "Pequenos Pasos/ Small Steps", "g.1218f5g0", "Small Steps", "Super Fast, Out of Control!", "Johnny's in the basement", "The boy who lost his face", "Sixth Grade Secrets (An Apple Paperback)", "The Boy Who Lost His Face"], "ans_acc": 0.10294117647058823, "ans_hit": 1, "ans_f1": 0.13698630136986303, "ans_precission": 1.0, "ans_recall": 0.07352941176470588, "path_f1": 0.22907488986784144, "path_precision": 0.4, "path_recall": 0.16049382716049382, "path_ans_f1": 0.18666666666666665, "path_ans_precision": 1.0, "path_ans_recall": 0.10294117647058823}
{"id": "WebQTest-1539", "prediction": ["# Reasoning Path:\nRyan Reynolds -> people.person.parents -> Tammy Reynolds -> people.person.children -> Jeff Reynolds\n# Answer:\nTammy Reynolds", "# Reasoning Path:\nRyan Reynolds -> people.person.parents -> Tammy Reynolds -> people.person.spouse_s -> m.0pbmp4r\n# Answer:\nTammy Reynolds", "# Reasoning Path:\nRyan Reynolds -> people.person.spouse_s -> m.0ygrgyh -> people.marriage.spouse -> Blake Lively\n# Answer:\nBlake Lively", "# Reasoning Path:\nRyan Reynolds -> people.person.parents -> Tammy Reynolds -> people.person.children -> Patrick Reynolds\n# Answer:\nTammy Reynolds", "# Reasoning Path:\nRyan Reynolds -> people.person.parents -> Jim Reynolds -> people.person.children -> Jeff Reynolds\n# Answer:\nJim Reynolds", "# Reasoning Path:\nRyan Reynolds -> people.person.spouse_s -> m.0ygrgyh -> freebase.valuenotation.has_no_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nRyan Reynolds -> people.person.spouse_s -> m.0ygrd3d -> people.marriage.spouse -> Scarlett Johansson\n# Answer:\nScarlett Johansson", "# Reasoning Path:\nRyan Reynolds -> people.person.parents -> Tammy Reynolds -> people.person.children -> Terry Reynolds\n# Answer:\nTammy Reynolds", "# Reasoning Path:\nRyan Reynolds -> people.person.parents -> Tammy Reynolds -> people.person.gender -> Female\n# Answer:\nTammy Reynolds", "# Reasoning Path:\nRyan Reynolds -> people.person.parents -> Jim Reynolds -> people.person.spouse_s -> m.0pbmp4r\n# Answer:\nJim Reynolds"], "ground_truth": ["Blake Lively"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-154", "prediction": ["# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.family -> Brass instrument\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> music.instrument.family -> Brass instrument\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> common.topic.notable_types -> Musical instrument\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.instrumentalists -> W. C. Handy\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> book.written_work.subjects -> Trumpeter\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.instrumentalists -> Bix Beiderbecke\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> common.topic.notable_types -> Book\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.instrumentalists -> James Morrison\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.featured_artist.recordings -> Boog It -> music.recording.artist -> Mills Brothers\n# Answer:\nBoog It", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> common.topic.notable_types -> Musical instrument\n# Answer:\nTrumpet"], "ground_truth": ["Trumpet", "Cornet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1540", "prediction": ["# Reasoning Path:\nNewt Gingrich -> people.person.spouse_s -> m.0j17836 -> people.marriage.spouse -> Marianne Ginther\n# Answer:\nMarianne Ginther", "# Reasoning Path:\nNewt Gingrich -> people.person.spouse_s -> m.0j17836 -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nNewt Gingrich -> people.person.children -> Jackie Gingrich Cushman -> people.person.parents -> Jackie Battley\n# Answer:\nJackie Gingrich Cushman", "# Reasoning Path:\nNewt Gingrich -> people.person.spouse_s -> m.0j17836 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nNewt Gingrich -> people.person.spouse_s -> m.0j1782r -> people.marriage.spouse -> Jackie Battley\n# Answer:\nJackie Battley", "# Reasoning Path:\nNewt Gingrich -> people.person.children -> Kathy Lubbers -> people.person.spouse_s -> m.03sc8bv\n# Answer:\nKathy Lubbers", "# Reasoning Path:\nNewt Gingrich -> people.person.spouse_s -> m.0hybx80 -> people.marriage.spouse -> Callista Gingrich\n# Answer:\nCallista Gingrich", "# Reasoning Path:\nNewt Gingrich -> people.person.spouse_s -> m.0hybx80 -> people.marriage.location_of_ceremony -> Alexandria\n# Answer:\nAlexandria", "# Reasoning Path:\nNewt Gingrich -> people.person.children -> Jackie Gingrich Cushman -> people.person.gender -> Female\n# Answer:\nJackie Gingrich Cushman", "# Reasoning Path:\nNewt Gingrich -> people.person.spouse_s -> m.0hybx80 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["Callista Gingrich"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1541", "prediction": ["# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> music.genre.parent_genre -> Art music\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> broadcast.genre.content -> Classica\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> broadcast.genre.content -> ORSRADIO.COM Classical\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> radio.radio_subject.programs_with_this_subject -> Adventures in Good Music\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Baroque music -> music.genre.parent_genre -> Classical music\n# Answer:\nBaroque music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Baroque music -> common.topic.image -> Baschenis - Musical Instruments\n# Answer:\nBaroque music", "# Reasoning Path:\nAntonio Vivaldi -> base.ontologies.ontology_instance.equivalent_instances -> m.07ndktx -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nOpenCyc", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> broadcast.genre.content -> 1.FM Otto's  Baroque music\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> radio.radio_subject.programs_with_this_subject -> Music 'Til Dawn\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Baroque music -> music.genre.parent_genre -> Classical music and folk music\n# Answer:\nBaroque music"], "ground_truth": ["Classical music", "Baroque music"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1542", "prediction": ["# Reasoning Path:\nJames Franco -> people.person.education -> m.0n1n3d5 -> education.education.institution -> Rhode Island School of Design\n# Answer:\nRhode Island School of Design", "# Reasoning Path:\nJames Franco -> people.person.education -> m.0gxbcv6 -> education.education.institution -> Brooklyn College\n# Answer:\nBrooklyn College", "# Reasoning Path:\nJames Franco -> people.person.education -> m.064zvgv -> education.education.institution -> University of California, Los Angeles\n# Answer:\nUniversity of California, Los Angeles", "# Reasoning Path:\nJames Franco -> people.person.education -> m.0gxbcv6 -> education.education.degree -> Master of Fine Arts\n# Answer:\nMaster of Fine Arts", "# Reasoning Path:\nJames Franco -> common.topic.webpage -> m.098dnd6 -> common.webpage.resource -> Where are the ''Freaks and Geeks'' now?\n# Answer:\nWhere are the ''Freaks and Geeks'' now?", "# Reasoning Path:\nJames Franco -> people.person.education -> m.0gxbcv6 -> education.education.major_field_of_study -> Fiction writing\n# Answer:\nFiction writing", "# Reasoning Path:\nJames Franco -> people.person.education -> m.064zvgv -> education.education.degree -> Bachelor of Arts\n# Answer:\nBachelor of Arts", "# Reasoning Path:\nJames Franco -> award.award_nominee.award_nominations -> m.0110_sg8 -> award.award_nomination.award_nominee -> Seth Rogen\n# Answer:\nSeth Rogen", "# Reasoning Path:\nJames Franco -> common.topic.webpage -> m.098dx2c -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nJames Franco -> people.person.education -> m.064zvgv -> education.education.specialization -> Creative writing\n# Answer:\nCreative writing"], "ground_truth": ["University of California, Los Angeles", "Columbia University School of the Arts", "University Yale", "Brooklyn College", "Palo Alto High School", "Rhode Island School of Design", "Warren Wilson College", "Tisch School of the Arts"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.33333333333333326, "ans_precission": 0.3, "ans_recall": 0.375, "path_f1": 0.33333333333333326, "path_precision": 0.3, "path_recall": 0.375, "path_ans_f1": 0.33333333333333326, "path_ans_precision": 0.3, "path_ans_recall": 0.375}
{"id": "WebQTest-1543", "prediction": ["# Reasoning Path:\nMark Twain -> people.person.profession -> Journalist -> people.profession.specialization_of -> Writer\n# Answer:\nJournalist", "# Reasoning Path:\nMark Twain -> people.person.profession -> Journalist -> common.topic.notable_types -> Profession\n# Answer:\nJournalist", "# Reasoning Path:\nMark Twain -> people.person.profession -> Humorist -> people.profession.specializations -> Raconteur\n# Answer:\nHumorist", "# Reasoning Path:\nMark Twain -> people.person.profession -> Journalist -> people.profession.corresponding_type -> Author\n# Answer:\nJournalist", "# Reasoning Path:\nMark Twain -> people.person.profession -> Humorist -> people.profession.specialization_of -> Writer\n# Answer:\nHumorist", "# Reasoning Path:\nMark Twain -> people.person.profession -> Humorist -> common.topic.notable_for -> g.1256kn4dk\n# Answer:\nHumorist", "# Reasoning Path:\nMark Twain -> people.person.profession -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor", "# Reasoning Path:\nMark Twain -> people.person.children -> Clara Clemens -> people.person.profession -> Singer\n# Answer:\nClara Clemens", "# Reasoning Path:\nMark Twain -> people.person.profession -> Author -> common.topic.subject_of -> The Passion-Driven Writer and the Digital-Age Literary Marketplace\n# Answer:\nAuthor", "# Reasoning Path:\nMark Twain -> film.film_story_contributor.film_story_credits -> A Connecticut Yankee -> film.film.written_by -> William Conselman\n# Answer:\nA Connecticut Yankee"], "ground_truth": ["Writer", "Teacher", "Novelist", "Author", "Humorist", "Journalist"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.6153846153846154, "ans_precission": 0.8, "ans_recall": 0.5, "path_f1": 0.42857142857142855, "path_precision": 0.6, "path_recall": 0.3333333333333333, "path_ans_f1": 0.7272727272727272, "path_ans_precision": 0.8, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1544", "prediction": ["# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx817 -> government.government_position_held.office_holder -> Harry S. Truman\n# Answer:\nHarry S. Truman", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03q2h_9 -> government.government_position_held.office_holder -> Franklin D. Roosevelt\n# Answer:\nFranklin D. Roosevelt", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx8cf -> government.government_position_held.office_holder -> Richard Nixon\n# Answer:\nRichard Nixon", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx817 -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03q2h_9 -> government.government_position_held.basic_title -> President\n# Answer:\nPresident", "# Reasoning Path:\nPresident of the United States -> freebase.equivalent_topic.equivalent_type -> US President -> freebase.type_profile.kind -> Title\n# Answer:\nUS President", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx8cf -> government.government_position_held.basic_title -> President\n# Answer:\nPresident", "# Reasoning Path:\nPresident of the United States -> freebase.equivalent_topic.equivalent_type -> US President -> type.type.properties -> Vice president\n# Answer:\nUS President", "# Reasoning Path:\nPresident of the United States -> base.firsts.achievement.firsts -> m.04wrwjv -> base.firsts.first_achievement.first -> Barack Obama\n# Answer:\nBarack Obama", "# Reasoning Path:\nPresident of the United States -> base.firsts.achievement.firsts -> m.04wrx4g -> base.firsts.first_achievement.category -> Person\n# Answer:\nPerson"], "ground_truth": ["John Tyler"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1546", "prediction": ["# Reasoning Path:\nRick Santorum -> people.person.education -> m.012gg3yy -> education.education.institution -> Joseph M. Katz Graduate School of Business\n# Answer:\nJoseph M. Katz Graduate School of Business", "# Reasoning Path:\nRick Santorum -> people.person.education -> m.02wn695 -> education.education.institution -> Pennsylvania State University\n# Answer:\nPennsylvania State University", "# Reasoning Path:\nRick Santorum -> people.person.education -> m.04hx35s -> education.education.institution -> University of Pittsburgh\n# Answer:\nUniversity of Pittsburgh", "# Reasoning Path:\nRick Santorum -> business.board_member.organization_board_memberships -> m.09tymds -> organization.organization_board_membership.organization -> Universal Health Services\n# Answer:\nUniversal Health Services", "# Reasoning Path:\nRick Santorum -> people.person.education -> m.02wn695 -> education.education.degree -> Bachelor of Arts\n# Answer:\nBachelor of Arts", "# Reasoning Path:\nRick Santorum -> government.politician.party -> m.03gjgj7 -> government.political_party_tenure.party -> Republican Party\n# Answer:\nRepublican Party", "# Reasoning Path:\nRick Santorum -> people.person.education -> m.02wn695 -> education.education.major_field_of_study -> Political Science\n# Answer:\nPolitical Science", "# Reasoning Path:\nRick Santorum -> people.person.education -> m.04hx35s -> education.education.major_field_of_study -> Business Administration\n# Answer:\nBusiness Administration"], "ground_truth": ["Carmel High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1547", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> book.author.works_written -> The George W. Bush foreign policy reader -> common.topic.notable_types -> Book\n# Answer:\nThe George W. Bush foreign policy reader", "# Reasoning Path:\nGeorge W. Bush -> book.author.works_written -> The George W. Bush foreign policy reader -> common.topic.notable_for -> g.125b58jd4\n# Answer:\nThe George W. Bush foreign policy reader", "# Reasoning Path:\nGeorge W. Bush -> book.author.works_written -> A Charge to Keep -> book.written_work.author -> Michael Herskowitz\n# Answer:\nA Charge to Keep", "# Reasoning Path:\nGeorge W. Bush -> book.author.works_written -> A Charge to Keep -> book.written_work.next_in_series -> Decision Points\n# Answer:\nA Charge to Keep", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointed_role -> Director of the Bureau of Counterterrorism\n# Answer:\nDirector of the Bureau of Counterterrorism", "# Reasoning Path:\nGeorge W. Bush -> book.author.works_written -> A Charge to Keep -> common.topic.article -> m.03cwz0w\n# Answer:\nA Charge to Keep", "# Reasoning Path:\nGeorge W. Bush -> book.author.works_written -> 41: A Portrait of My Father -> book.book.genre -> Biography\n# Answer:\n41: A Portrait of My Father", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pk -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointee -> Henry A. Crumpton\n# Answer:\nHenry A. Crumpton", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1 -> people.appointment.appointed_role -> United States Ambassador to Angola\n# Answer:\nUnited States Ambassador to Angola"], "ground_truth": ["In My Time: A Personal and Political Memoir"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1548", "prediction": ["# Reasoning Path:\nAntoine Lavoisier -> people.deceased_person.place_of_death -> Place de la Concorde -> location.location.contains -> Luxor Obelisk\n# Answer:\nPlace de la Concorde", "# Reasoning Path:\nAntoine Lavoisier -> people.deceased_person.place_of_death -> Place de la Concorde -> common.topic.article -> m.0kx9q\n# Answer:\nPlace de la Concorde", "# Reasoning Path:\nAntoine Lavoisier -> people.deceased_person.place_of_death -> Place de la Concorde -> base.schemastaging.context_name.pronunciation -> g.11b7zfzkkp\n# Answer:\nPlace de la Concorde", "# Reasoning Path:\nAntoine Lavoisier -> influence.influence_node.influenced_by -> Joseph Priestley -> people.deceased_person.place_of_death -> Pennsylvania\n# Answer:\nJoseph Priestley", "# Reasoning Path:\nAntoine Lavoisier -> freebase.valuenotation.is_reviewed -> Place of death -> type.property.schema -> Deceased Person\n# Answer:\nPlace of death", "# Reasoning Path:\nAntoine Lavoisier -> influence.influence_node.influenced_by -> Guillaume-Fran\u00e7ois Rouelle -> people.deceased_person.place_of_death -> Paris\n# Answer:\nGuillaume-Fran\u00e7ois Rouelle", "# Reasoning Path:\nAntoine Lavoisier -> freebase.valuenotation.is_reviewed -> Place of death -> rdf-schema#range -> Location\n# Answer:\nPlace of death", "# Reasoning Path:\nAntoine Lavoisier -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nAntoine Lavoisier -> influence.influence_node.influenced_by -> Joseph Priestley -> influence.influence_node.influenced_by -> John Amos Comenius\n# Answer:\nJoseph Priestley", "# Reasoning Path:\nAntoine Lavoisier -> influence.influence_node.influenced_by -> John Dalton -> people.deceased_person.place_of_death -> Manchester\n# Answer:\nJohn Dalton"], "ground_truth": ["Place de la Concorde"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-155", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> base.inaugurations.inauguration_speaker.inauguration -> George W. Bush 2001 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nGeorge W. Bush 2001 presidential inauguration", "# Reasoning Path:\nGeorge W. Bush -> base.inaugurations.inauguration_speaker.inauguration -> George W. Bush 2001 presidential inauguration -> common.topic.image -> GWBush1.jpg\n# Answer:\nGeorge W. Bush 2001 presidential inauguration", "# Reasoning Path:\nGeorge W. Bush -> base.inaugurations.inauguration_speaker.inauguration -> George W. Bush 2005 presidential inauguration -> time.event.instance_of_recurring_event -> United States presidential inauguration\n# Answer:\nGeorge W. Bush 2005 presidential inauguration", "# Reasoning Path:\nGeorge W. Bush -> base.inaugurations.inauguration_speaker.inauguration -> George W. Bush 2001 presidential inauguration -> time.event.instance_of_recurring_event -> United States presidential inauguration\n# Answer:\nGeorge W. Bush 2001 presidential inauguration", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2000 -> government.election_campaign.election -> United States presidential election, 2000\n# Answer:\nGeorge W. Bush presidential campaign, 2000", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2004 -> government.election_campaign.election -> United States presidential election, 2004\n# Answer:\nGeorge W. Bush presidential campaign, 2004", "# Reasoning Path:\nGeorge W. Bush -> base.inaugurations.inauguration_speaker.inauguration -> George W. Bush 2005 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nGeorge W. Bush 2005 presidential inauguration", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2000 -> government.election_campaign.party -> Republican Party\n# Answer:\nGeorge W. Bush presidential campaign, 2000", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2000 -> common.topic.notable_types -> Election campaign\n# Answer:\nGeorge W. Bush presidential campaign, 2000", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2004 -> common.topic.notable_for -> g.1257w3www\n# Answer:\nGeorge W. Bush presidential campaign, 2004"], "ground_truth": ["2004-11-02", "2000-11-07"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1550", "prediction": ["# Reasoning Path:\nGeorge VI -> people.person.spouse_s -> m.0j4k1c9 -> people.marriage.spouse -> Queen Elizabeth The Queen Mother\n# Answer:\nQueen Elizabeth The Queen Mother", "# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.person.children -> Prince Edward, Earl of Wessex\n# Answer:\nElizabeth II", "# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.person.spouse_s -> m.028zms8\n# Answer:\nElizabeth II", "# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.person.children -> Anne, Princess Royal\n# Answer:\nElizabeth II", "# Reasoning Path:\nGeorge VI -> people.person.children -> Princess Margaret, Countess of Snowdon -> people.person.children -> David Armstrong-Jones, Viscount Linley\n# Answer:\nPrincess Margaret, Countess of Snowdon", "# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.person.children -> Charles, Prince of Wales\n# Answer:\nElizabeth II", "# Reasoning Path:\nGeorge VI -> people.person.children -> Princess Margaret, Countess of Snowdon -> people.person.spouse_s -> m.02h4982\n# Answer:\nPrincess Margaret, Countess of Snowdon", "# Reasoning Path:\nGeorge VI -> people.person.spouse_s -> m.0j4k1c9 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nGeorge VI -> people.person.children -> Princess Margaret, Countess of Snowdon -> people.person.children -> Lady Sarah Chatto\n# Answer:\nPrincess Margaret, Countess of Snowdon", "# Reasoning Path:\nGeorge VI -> people.person.spouse_s -> m.0j4k1c9 -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage"], "ground_truth": ["Queen Elizabeth The Queen Mother"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.13333333333333333, "path_precision": 0.1, "path_recall": 0.2, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1551", "prediction": ["# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> sports.sports_championship_event.season -> 2013 NFL season\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> time.event.instance_of_recurring_event -> Super Bowl\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> freebase.valuenotation.is_reviewed -> Championship\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> 2006 NFC Championship Game -> common.topic.notable_for -> g.1z2spvm2w\n# Answer:\n2006 NFC Championship Game", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> 2006 NFC Championship Game -> time.event.locations -> CenturyLink Field\n# Answer:\n2006 NFC Championship Game", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.season_record -> m.075fsdt -> sports.sports_team_season_record.season -> 2005 NFL season\n# Answer:\n2005 NFL season", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.season_record -> m.075fsd1 -> sports.sports_team_season_record.season -> 2008 NFL season\n# Answer:\n2008 NFL season", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> 2006 NFC Championship Game -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n2006 NFC Championship Game", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> freebase.valuenotation.is_reviewed -> Champion\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> 2014 NFC Championship Game -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n2014 NFC Championship Game"], "ground_truth": ["Super Bowl XLVIII", "Super Bowl XL"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1553", "prediction": ["# Reasoning Path:\nSami Yusuf -> music.artist.origin -> Tehran -> base.biblioness.bibs_location.country -> Iran\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> people.person.place_of_birth -> Tehran -> base.biblioness.bibs_location.country -> Iran\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> people.person.places_lived -> m.03pn2p8 -> people.place_lived.location -> Tehran\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> music.artist.origin -> Tehran -> location.location.events -> 1974 Tehran International Short Film Festival\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> music.artist.origin -> Tehran -> sports.sports_team_location.teams -> Esteghlal Tehran Football Club\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> music.artist.origin -> Tehran -> location.location.events -> 1983 Asian Wrestling Championships\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> people.person.place_of_birth -> Tehran -> location.location.events -> 1974 Tehran International Short Film Festival\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> music.artist.origin -> Tehran -> location.location.events -> 1983 Fajr International Film Festival\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> music.artist.origin -> Tehran -> sports.sports_team_location.teams -> Paykan Tehran VC\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> people.person.place_of_birth -> Tehran -> sports.sports_team_location.teams -> Esteghlal Tehran Football Club\n# Answer:\nTehran"], "ground_truth": ["Tehran"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1554", "prediction": ["# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Portuguese Language -> language.human_language.dialects -> Brazilian Portuguese\n# Answer:\nPortuguese Language", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Brazilian Portuguese -> language.language_dialect.language -> Portuguese Language\n# Answer:\nBrazilian Portuguese", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc39sjp\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fm01\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc385hl\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp"], "ground_truth": ["Portuguese Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1555", "prediction": ["# Reasoning Path:\nParliament of Nepal -> government.governmental_body.jurisdiction -> Nepal -> location.country.languages_spoken -> Nepali Language\n# Answer:\nNepal", "# Reasoning Path:\nParliament of Nepal -> government.governmental_body.jurisdiction -> Nepal -> location.country.official_language -> Nepali Language\n# Answer:\nNepal", "# Reasoning Path:\nParliament of Nepal -> government.governmental_body.jurisdiction -> Nepal -> location.country.languages_spoken -> Bhojpuri Language\n# Answer:\nNepal", "# Reasoning Path:\nParliament of Nepal -> government.governmental_body.jurisdiction -> Nepal -> location.country.languages_spoken -> Awadhi Language\n# Answer:\nNepal", "# Reasoning Path:\nParliament of Nepal -> common.topic.image -> Nepal Coat of Arms -> common.image.appears_in_topic_gallery -> Emblem of Nepal\n# Answer:\nNepal Coat of Arms", "# Reasoning Path:\nParliament of Nepal -> government.governmental_body.jurisdiction -> Nepal -> location.statistical_region.electricity_consumption_per_capita -> g.1245_0stl\n# Answer:\nNepal", "# Reasoning Path:\nParliament of Nepal -> common.topic.notable_types -> Governmental Body -> type.type.expected_by -> Parliament\n# Answer:\nGovernmental Body", "# Reasoning Path:\nParliament of Nepal -> common.topic.image -> Nepal Coat of Arms -> common.image.appears_in_topic_gallery -> Janani Janmabhoomischa Swargadapi Gariyasi\n# Answer:\nNepal Coat of Arms", "# Reasoning Path:\nParliament of Nepal -> common.topic.image -> Nepal Coat of Arms -> common.image.size -> m.0bn01yk\n# Answer:\nNepal Coat of Arms", "# Reasoning Path:\nParliament of Nepal -> government.governmental_body.jurisdiction -> Nepal -> location.statistical_region.electricity_consumption_per_capita -> g.1245_4_mx\n# Answer:\nNepal"], "ground_truth": ["Parliament of Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1556", "prediction": ["# Reasoning Path:\nNew York Mets -> sports.sports_team.championships -> 1986 World Series -> time.event.locations -> Fenway Park\n# Answer:\n1986 World Series", "# Reasoning Path:\nNew York Mets -> sports.sports_team.championships -> 1986 World Series -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1986 World Series", "# Reasoning Path:\nNew York Mets -> sports.sports_team.championships -> 1986 World Series -> time.event.locations -> Shea Stadium\n# Answer:\n1986 World Series", "# Reasoning Path:\nNew York Mets -> sports.sports_team.championships -> 1969 World Series -> time.event.locations -> Shea Stadium\n# Answer:\n1969 World Series", "# Reasoning Path:\nNew York Mets -> sports.sports_team.championships -> 1969 World Series -> film.film_subject.films -> 1969 World Series Highlight Film\n# Answer:\n1969 World Series", "# Reasoning Path:\nNew York Mets -> sports.sports_team.championships -> 1969 World Series -> common.topic.notable_for -> g.12553vqj6\n# Answer:\n1969 World Series", "# Reasoning Path:\nNew York Mets -> sports.sports_team.championships -> 1969 World Series -> time.event.locations -> Memorial Stadium\n# Answer:\n1969 World Series", "# Reasoning Path:\nNew York Mets -> baseball.baseball_team.team_stats -> m.05n69_2 -> baseball.baseball_team_stats.season -> 1964 Major League Baseball Season\n# Answer:\n1964 Major League Baseball Season", "# Reasoning Path:\nNew York Mets -> sports.sports_team.arena_stadium -> Citi Field -> sports.sports_facility.home_venue_for -> m.0wz24lc\n# Answer:\nCiti Field", "# Reasoning Path:\nNew York Mets -> sports.sports_team.arena_stadium -> Citi Field -> common.topic.webpage -> m.0b48pjp\n# Answer:\nCiti Field"], "ground_truth": ["1986 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1557", "prediction": ["# Reasoning Path:\nFlorida Panthers -> sports.sports_team.arena_stadium -> BB&T Center -> sports.sports_facility.home_venue_for -> m.0wz2lgr\n# Answer:\nBB&T Center", "# Reasoning Path:\nFlorida Panthers -> sports.sports_team.arena_stadium -> Miami Arena -> sports.sports_facility.teams -> Miami Matadors\n# Answer:\nMiami Arena", "# Reasoning Path:\nFlorida Panthers -> sports.sports_team.arena_stadium -> BB&T Center -> location.location.geolocation -> m.02_vl7c\n# Answer:\nBB&T Center", "# Reasoning Path:\nFlorida Panthers -> sports.sports_team.arena_stadium -> Miami Arena -> location.location.containedby -> Miami\n# Answer:\nMiami Arena", "# Reasoning Path:\nFlorida Panthers -> sports.sports_team.arena_stadium -> Miami Arena -> common.topic.article -> m.039xm3\n# Answer:\nMiami Arena", "# Reasoning Path:\nFlorida Panthers -> sports.sports_team.arena_stadium -> Miami Arena -> location.location.containedby -> 33136\n# Answer:\nMiami Arena", "# Reasoning Path:\nFlorida Panthers -> ice_hockey.hockey_team.conference -> Eastern Conference -> ice_hockey.hockey_conference.teams -> Atlanta Thrashers\n# Answer:\nEastern Conference", "# Reasoning Path:\nFlorida Panthers -> ice_hockey.hockey_team.conference -> Eastern Conference -> ice_hockey.hockey_conference.league -> National Hockey League\n# Answer:\nEastern Conference", "# Reasoning Path:\nFlorida Panthers -> common.topic.notable_types -> Professional Sports Team -> freebase.type_hints.included_types -> Sports Team\n# Answer:\nProfessional Sports Team", "# Reasoning Path:\nFlorida Panthers -> common.topic.notable_types -> Professional Sports Team -> type.type.domain -> Sports\n# Answer:\nProfessional Sports Team"], "ground_truth": ["BB&T Center", "Miami Arena"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1558", "prediction": ["# Reasoning Path:\nJohn Cena -> people.person.parents -> Carol Cena -> people.person.spouse_s -> m.0kbm513\n# Answer:\nCarol Cena", "# Reasoning Path:\nJohn Cena -> people.person.parents -> Carol Cena -> people.person.children -> Dan Cena\n# Answer:\nCarol Cena", "# Reasoning Path:\nJohn Cena -> people.person.spouse_s -> m.07n79hl -> people.marriage.spouse -> Elizabeth Huberdeau\n# Answer:\nElizabeth Huberdeau", "# Reasoning Path:\nJohn Cena -> people.person.parents -> John Cena Sr. -> people.person.spouse_s -> m.0kbm513\n# Answer:\nJohn Cena Sr.", "# Reasoning Path:\nJohn Cena -> people.person.parents -> Carol Cena -> people.person.children -> Matt Cena\n# Answer:\nCarol Cena", "# Reasoning Path:\nJohn Cena -> people.person.parents -> Carol Cena -> common.topic.notable_for -> g.125hbdcdp\n# Answer:\nCarol Cena", "# Reasoning Path:\nJohn Cena -> people.person.spouse_s -> m.07n79hl -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nJohn Cena -> people.person.parents -> Carol Cena -> people.person.children -> Sean Cena\n# Answer:\nCarol Cena", "# Reasoning Path:\nJohn Cena -> people.person.spouse_s -> m.07n79hl -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nJohn Cena -> people.person.parents -> John Cena Sr. -> people.person.children -> Dan Cena\n# Answer:\nJohn Cena Sr."], "ground_truth": ["Elizabeth Huberdeau"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1559", "prediction": ["# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> Happy Hollow Park & Zoo -> location.location.geolocation -> m.0d4qkys\n# Answer:\nHappy Hollow Park & Zoo", "# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> Happy Hollow Park & Zoo -> common.topic.webpage -> m.03l4p6m\n# Answer:\nHappy Hollow Park & Zoo", "# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> Santana Row -> common.topic.article -> m.072p6d\n# Answer:\nSantana Row", "# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> Santana Row -> location.location.containedby -> United States of America\n# Answer:\nSantana Row", "# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> San Jose Museum of Art -> common.topic.image -> The San Jose Museum of Art\n# Answer:\nSan Jose Museum of Art", "# Reasoning Path:\nSan Jose -> location.location.containedby -> United States of America -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> Happy Hollow Park & Zoo -> common.topic.webpage -> m.0bd3sxn\n# Answer:\nHappy Hollow Park & Zoo", "# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> Happy Hollow Park & Zoo -> common.topic.webpage -> m.0cv4ts1\n# Answer:\nHappy Hollow Park & Zoo", "# Reasoning Path:\nSan Jose -> location.location.containedby -> United States of America -> sports.sports_team_location.teams -> Team RadioShack\n# Answer:\nUnited States of America", "# Reasoning Path:\nSan Jose -> location.place_with_neighborhoods.neighborhoods -> Almaden Valley -> common.topic.notable_for -> g.1258t6537\n# Answer:\nAlmaden Valley"], "ground_truth": ["Children's Discovery Museum of San Jose", "San Jose Municipal Rose Garden", "Kelley Park", "San Jose Museum of Quilts & Textiles", "Downtown San Jose", "San Jose Museum of Art", "Winchester Mystery House", "The Tech Museum of Innovation", "Santana Row", "Cinequest Film Festival", "Japantown", "Rosicrucian Egyptian Museum", "Alum Rock Park", "Rock \u2018n\u2019 Roll San Jose Half Marathon", "Happy Hollow Park & Zoo"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.3111111111111111, "ans_precission": 0.7, "ans_recall": 0.2, "path_f1": 0.3111111111111111, "path_precision": 0.7, "path_recall": 0.2, "path_ans_f1": 0.3111111111111111, "path_ans_precision": 0.7, "path_ans_recall": 0.2}
{"id": "WebQTest-156", "prediction": ["# Reasoning Path:\nCalifornia -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nPacific Time Zone", "# Reasoning Path:\nCalifornia -> location.location.events -> 2009 California Independent Film Festival -> common.topic.notable_types -> Film festival event\n# Answer:\n2009 California Independent Film Festival", "# Reasoning Path:\nCalifornia -> location.location.events -> 1987 Wine Country Film Festival -> time.event.locations -> Sonoma Valley\n# Answer:\n1987 Wine Country Film Festival", "# Reasoning Path:\nCalifornia -> film.film_location.featured_in_films -> The Cell -> film.film.country -> United States of America\n# Answer:\nThe Cell", "# Reasoning Path:\nCalifornia -> location.location.events -> 2009 California Independent Film Festival -> common.topic.notable_for -> g.1s0669t8c\n# Answer:\n2009 California Independent Film Festival", "# Reasoning Path:\nCalifornia -> film.film_location.featured_in_films -> Crazy in Alabama -> film.film.country -> United States of America\n# Answer:\nCrazy in Alabama", "# Reasoning Path:\nCalifornia -> film.film_location.featured_in_films -> The Cell -> common.topic.notable_types -> Film\n# Answer:\nThe Cell", "# Reasoning Path:\nCalifornia -> location.location.events -> 1987 Wine Country Film Festival -> time.event.instance_of_recurring_event -> Wine Country Film Festival\n# Answer:\n1987 Wine Country Film Festival", "# Reasoning Path:\nCalifornia -> location.location.events -> 1988 Wine Country Film Festival -> time.event.locations -> Sonoma Valley\n# Answer:\n1988 Wine Country Film Festival", "# Reasoning Path:\nCalifornia -> location.location.events -> 1987 Wine Country Film Festival -> freebase.valuenotation.has_value -> Official website\n# Answer:\n1987 Wine Country Film Festival"], "ground_truth": ["Pacific Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1560", "prediction": ["# Reasoning Path:\nNew York University -> organization.organization.founders -> Albert Gallatin -> organization.organization_founder.organizations_founded -> American Ethnological Society\n# Answer:\nAlbert Gallatin", "# Reasoning Path:\nNew York University -> organization.organization.founders -> Mordecai Manuel Noah -> book.author.works_written -> Correspondence and documents relative to the attempt to negotiate for the release of the American captives at Algiers\n# Answer:\nMordecai Manuel Noah", "# Reasoning Path:\nNew York University -> organization.organization.founders -> Albert Gallatin -> people.deceased_person.place_of_death -> Astoria\n# Answer:\nAlbert Gallatin", "# Reasoning Path:\nNew York University -> organization.organization.founders -> Mordecai Manuel Noah -> people.person.profession -> Diplomat\n# Answer:\nMordecai Manuel Noah", "# Reasoning Path:\nNew York University -> law.litigant.party_to_cases -> m.0dggt0t -> law.legal_case_party_relationship.case -> New York University v. Ariel Fund Ltd\n# Answer:\nNew York University v. Ariel Fund Ltd", "# Reasoning Path:\nNew York University -> organization.organization.founders -> Mordecai Manuel Noah -> book.author.works_written -> Discourse delivered at the consecration of the synagogue of [K.K. She\u02bberit Yisra\u02bbel] in the city of New-York\n# Answer:\nMordecai Manuel Noah", "# Reasoning Path:\nNew York University -> organization.organization.founders -> Mordecai Manuel Noah -> book.author.works_written -> Marion\n# Answer:\nMordecai Manuel Noah", "# Reasoning Path:\nNew York University -> organization.organization.founders -> Mordecai Manuel Noah -> people.person.profession -> Journalist\n# Answer:\nMordecai Manuel Noah", "# Reasoning Path:\nNew York University -> organization.organization.founders -> Albert Gallatin -> government.politician.government_positions_held -> m.04kq1pk\n# Answer:\nAlbert Gallatin", "# Reasoning Path:\nNew York University -> law.litigant.party_to_cases -> m.0dggt0t -> law.legal_case_party_relationship.role -> Plaintiff\n# Answer:\nPlaintiff"], "ground_truth": ["Mordecai Manuel Noah", "Albert Gallatin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1561", "prediction": ["# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> Cookie Jar Group -> common.topic.notable_types -> Production company\n# Answer:\nCookie Jar Group", "# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> Cookie Jar Group -> film.production_company.films -> Voulez-vous coucher avec God?\n# Answer:\nCookie Jar Group", "# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> Marc Brown -> tv.tv_program_creator.programs_created -> Postcards from Buster\n# Answer:\nMarc Brown", "# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> Cookie Jar Group -> common.topic.webpage -> m.04m3pcs\n# Answer:\nCookie Jar Group", "# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> Cookie Jar Group -> film.production_company.films -> Caillou's Holiday Movie\n# Answer:\nCookie Jar Group", "# Reasoning Path:\nArthur -> award.award_nominated_work.award_nominations -> m.010g23zh -> award.award_nomination.award_nominee -> Alan Cumming\n# Answer:\nAlan Cumming", "# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> WGBH -> tv.tv_program_creator.programs_created -> The New Yankee Workshop\n# Answer:\nWGBH", "# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> Cookie Jar Group -> film.production_company.films -> The Real Story of Happy Birthday to You\n# Answer:\nCookie Jar Group", "# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> Cookie Jar Group -> common.topic.webpage -> m.09ynwmc\n# Answer:\nCookie Jar Group", "# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> Marc Brown -> people.person.place_of_birth -> Erie\n# Answer:\nMarc Brown"], "ground_truth": ["Marc Brown", "WGBH", "Cookie Jar Group"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1562", "prediction": ["# Reasoning Path:\nEuropean Union -> organization.organization.geographic_scope -> Europe -> base.locations.continents.countries_within -> United Kingdom\n# Answer:\nEurope", "# Reasoning Path:\nEuropean Union -> organization.organization.geographic_scope -> Europe -> base.locations.continents.countries_within -> Sweden\n# Answer:\nEurope", "# Reasoning Path:\nEuropean Union -> organization.organization.geographic_scope -> Europe -> government.governmental_jurisdiction.governing_officials -> m.010pyz2k\n# Answer:\nEurope", "# Reasoning Path:\nEuropean Union -> organization.organization.geographic_scope -> Europe -> base.locations.continents.countries_within -> Austria\n# Answer:\nEurope", "# Reasoning Path:\nEuropean Union -> organization.organization.geographic_scope -> Europe -> organization.organization_scope.organizations_with_this_scope -> Council of Europe\n# Answer:\nEurope", "# Reasoning Path:\nEuropean Union -> location.statistical_region.broadband_penetration_rate -> g.1245_6ndt\n# Answer:\nlocation.statistical_region.broadband_penetration_rate", "# Reasoning Path:\nEuropean Union -> internet.website_owner.websites_owned -> http://europa.eu/index_en.htm -> common.topic.notable_types -> Website\n# Answer:\nhttp://europa.eu/index_en.htm", "# Reasoning Path:\nEuropean Union -> location.statistical_region.broadband_penetration_rate -> g.1245yvl64\n# Answer:\nlocation.statistical_region.broadband_penetration_rate", "# Reasoning Path:\nEuropean Union -> location.statistical_region.broadband_penetration_rate -> g.1245ywrjj\n# Answer:\nlocation.statistical_region.broadband_penetration_rate", "# Reasoning Path:\nEuropean Union -> internet.website_owner.websites_owned -> http://europa.eu/index_en.htm -> common.topic.notable_for -> g.1255697l2\n# Answer:\nhttp://europa.eu/index_en.htm"], "ground_truth": ["Denmark", "Austria", "Slovakia", "Germany", "Spain", "Italy", "Malta", "France", "Republic of Ireland", "Estonia", "Finland", "Hungary", "Czech Republic", "Greece", "United Kingdom", "Luxembourg", "Bulgaria", "Slovenia", "Lithuania", "Romania", "Sweden", "Netherlands", "Latvia", "Portugal", "Cyprus", "Poland", "Croatia", "Belgium"], "ans_acc": 0.10714285714285714, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.08450704225352113, "path_precision": 0.3, "path_recall": 0.04918032786885246, "path_ans_f1": 0.15789473684210525, "path_ans_precision": 0.3, "path_ans_recall": 0.10714285714285714}
{"id": "WebQTest-1563", "prediction": ["# Reasoning Path:\nNorth Africa -> location.location.contains -> Sudan -> location.location.containedby -> Africa\n# Answer:\nSudan", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Sudan -> location.location.containedby -> Arab world\n# Answer:\nSudan", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Sudan -> location.location.time_zones -> East Africa Time Zone\n# Answer:\nSudan", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Sudan -> location.location.partiallycontains -> m.0wg8_26\n# Answer:\nSudan", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Ptolemaic Kingdom -> location.location.containedby -> Southern Europe\n# Answer:\nPtolemaic Kingdom", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Al Sharqia Governorate -> location.location.containedby -> Egypt\n# Answer:\nAl Sharqia Governorate", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Ptolemaic Kingdom -> symbols.namesake.named_after -> Ptolemy I Soter\n# Answer:\nPtolemaic Kingdom", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Ptolemaic Kingdom -> location.location.containedby -> Near East\n# Answer:\nPtolemaic Kingdom", "# Reasoning Path:\nNorth Africa -> location.location.partiallycontains -> m.0ws8w38 -> location.partial_containment_relationship.partially_contains -> Roman Empire\n# Answer:\nRoman Empire", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Ptolemaic Kingdom -> common.topic.article -> m.02vkl1y\n# Answer:\nPtolemaic Kingdom"], "ground_truth": ["Sudan", "United Arab Republic", "Caliphate of C\u00f3rdoba", "Rashidun Caliphate", "Western Roman Empire", "Ptolemaic Kingdom", "Roman Republic"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.4210526315789473, "ans_precission": 0.8, "ans_recall": 0.2857142857142857, "path_f1": 0.4210526315789473, "path_precision": 0.8, "path_recall": 0.2857142857142857, "path_ans_f1": 0.4210526315789473, "path_ans_precision": 0.8, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-1564", "prediction": ["# Reasoning Path:\nBobby Darin -> people.deceased_person.cause_of_death -> Surgical complications -> people.cause_of_death.parent_cause_of_death -> Assassination by 'Accident'\n# Answer:\nSurgical complications", "# Reasoning Path:\nBobby Darin -> people.deceased_person.cause_of_death -> Surgical complications -> common.topic.notable_types -> Cause Of Death\n# Answer:\nSurgical complications", "# Reasoning Path:\nBobby Darin -> people.deceased_person.cause_of_death -> Surgical complications -> common.topic.notable_for -> g.12565x30z\n# Answer:\nSurgical complications", "# Reasoning Path:\nBobby Darin -> music.composer.compositions -> I'll Be There -> music.composition.recordings -> I\u2019ll Be There\n# Answer:\nI'll Be There", "# Reasoning Path:\nBobby Darin -> music.composer.compositions -> As Long as I'm Singing -> music.single.versions -> As Long as I'm Singing (Quickstep, 52mpm)\n# Answer:\nAs Long as I'm Singing", "# Reasoning Path:\nBobby Darin -> music.composer.compositions -> As Long as I'm Singing -> common.topic.notable_types -> Composition\n# Answer:\nAs Long as I'm Singing", "# Reasoning Path:\nBobby Darin -> music.composer.compositions -> Bill Bailey -> music.recording.releases -> The Bobby Darin Story\n# Answer:\nBill Bailey", "# Reasoning Path:\nBobby Darin -> music.composer.compositions -> I'll Be There -> music.composition.recordings -> I'l Be There\n# Answer:\nI'll Be There", "# Reasoning Path:\nBobby Darin -> music.composer.compositions -> As Long as I'm Singing -> common.topic.notable_types -> Musical Recording\n# Answer:\nAs Long as I'm Singing", "# Reasoning Path:\nBobby Darin -> music.composer.compositions -> Bill Bailey -> music.recording.releases -> Darin at the Copa\n# Answer:\nBill Bailey"], "ground_truth": ["Surgical complications"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1565", "prediction": ["# Reasoning Path:\nHouston Dynamo -> sports.sports_team.arena_stadium -> BBVA Compass Stadium -> location.location.containedby -> Houston\n# Answer:\nBBVA Compass Stadium", "# Reasoning Path:\nHouston Dynamo -> sports.sports_team.arena_stadium -> BBVA Compass Stadium -> location.location.containedby -> United States of America\n# Answer:\nBBVA Compass Stadium", "# Reasoning Path:\nHouston Dynamo -> sports.sports_team.arena_stadium -> BBVA Compass Stadium -> common.topic.notable_types -> Sports Facility\n# Answer:\nBBVA Compass Stadium", "# Reasoning Path:\nHouston Dynamo -> sports.sports_team.arena_stadium -> BBVA Compass Stadium -> location.location.containedby -> East Downtown Houston\n# Answer:\nBBVA Compass Stadium", "# Reasoning Path:\nHouston Dynamo -> sports.sports_team.roster -> m.0112s_x_ -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nHouston Dynamo -> soccer.football_team.player_statistics -> m.0w8__hc -> soccer.football_player_stats.player -> Julius James\n# Answer:\nJulius James", "# Reasoning Path:\nHouston Dynamo -> sports.sports_team.roster -> m.0112syxs -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nHouston Dynamo -> soccer.football_team.player_statistics -> m.0w8wfw5 -> soccer.football_player_stats.player -> Wade Barrett\n# Answer:\nWade Barrett", "# Reasoning Path:\nHouston Dynamo -> sports.sports_team.roster -> m.0112s_x_ -> sports.sports_team_roster.position -> Defender\n# Answer:\nDefender", "# Reasoning Path:\nHouston Dynamo -> sports.sports_team.roster -> m.0112szth -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["Houston"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1566", "prediction": ["# Reasoning Path:\nJohn Mayer -> music.guitarist.guitars_played -> Fender Stratocaster -> music.guitar.guitarists -> Eric Clapton\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Country rock -> music.genre.parent_genre -> Country\n# Answer:\nCountry rock", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Country rock -> music.genre.parent_genre -> Neotraditional country\n# Answer:\nCountry rock", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Country rock -> common.topic.article -> m.0mhf_\n# Answer:\nCountry rock", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Country rock -> music.genre.parent_genre -> Rock music\n# Answer:\nCountry rock", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Country rock -> music.genre.subgenre -> Southern rock\n# Answer:\nCountry rock", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Folk music -> broadcast.genre.content -> Acoustic Caf.\n# Answer:\nFolk music", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Soul music -> music.genre.subgenre -> Blue-eyed soul\n# Answer:\nSoul music", "# Reasoning Path:\nJohn Mayer -> common.topic.webpage -> http://www.discogs.com/artist/John+Mayer -> common.webpage.category -> Topic Webpage\n# Answer:\nhttp://www.discogs.com/artist/John+Mayer", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Soul music -> music.genre.parent_genre -> Blues\n# Answer:\nSoul music"], "ground_truth": ["Fender Stratocaster"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1567", "prediction": ["# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.treatments -> Pancreatectomy\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.risk_factors -> Old age\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Old age\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9g\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.treatments -> Pancreaticoduodenectomy\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.risk_factors -> Tobacco smoking\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nPancreatic cancer"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1568", "prediction": ["# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgmw -> education.education.institution -> Columbia University\n# Answer:\nColumbia University", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgn2 -> education.education.institution -> Harvard Law School\n# Answer:\nHarvard Law School", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> education.education.institution -> Occidental College\n# Answer:\nOccidental College", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgmw -> education.education.degree -> Bachelor of Arts\n# Answer:\nBachelor of Arts", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgn2 -> education.education.degree -> Juris Doctor\n# Answer:\nJuris Doctor", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> Yes We Can: A Biography of President Barack Obama -> book.written_work.subjects -> United States of America\n# Answer:\nYes We Can: A Biography of President Barack Obama", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Bound Man -> book.written_work.subjects -> United States of America\n# Answer:\nA Bound Man", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.subjects -> Guerrilla Decontextualization\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study"], "ground_truth": ["Columbia University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1569", "prediction": ["# Reasoning Path:\nJamaica -> location.country.currency_used -> Jamaican dollar -> common.topic.notable_types -> Currency\n# Answer:\nJamaican dollar", "# Reasoning Path:\nJamaica -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f6_m\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nJamaica -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f6_n\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Keith -> meteorology.tropical_cyclone.affected_areas -> Yucat\u00e1n Peninsula\n# Answer:\nTropical Storm Keith", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Keith -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\nTropical Storm Keith", "# Reasoning Path:\nJamaica -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6fszp\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Keith -> meteorology.tropical_cyclone.tropical_cyclone_season -> 1988 Atlantic hurricane season\n# Answer:\nTropical Storm Keith", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Keith -> meteorology.tropical_cyclone.affected_areas -> Belize\n# Answer:\nTropical Storm Keith", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Louisiana\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\n1812 Louisiana hurricane"], "ground_truth": ["Jamaican dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-157", "prediction": ["# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> Ventura County -> location.location.containedby -> United States of America\n# Answer:\nVentura County", "# Reasoning Path:\nMission San Buenaventura -> common.topic.article -> m.0gbsh\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> Ventura County -> location.location.containedby -> California\n# Answer:\nVentura County", "# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> 93001 -> location.location.containedby -> Ventura County\n# Answer:\n93001", "# Reasoning Path:\nMission San Buenaventura -> common.topic.notable_for -> g.1259zm5db\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> Ventura County -> location.location.containedby -> Greater Los Angeles Area\n# Answer:\nVentura County", "# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> Ventura County -> location.location.people_born_here -> William Wolfe Wileman\n# Answer:\nVentura County", "# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> Ventura County -> location.statistical_region.population -> g.11b66gyw72\n# Answer:\nVentura County", "# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> Ventura County -> location.location.people_born_here -> Alexander Agate\n# Answer:\nVentura County", "# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> 93001 -> location.location.geolocation -> m.03dnfyg\n# Answer:\n93001"], "ground_truth": ["Ventura County", "93001"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1570", "prediction": ["# Reasoning Path:\nArizona Cardinals -> sports.sports_team.arena_stadium -> University of Phoenix Stadium -> location.location.containedby -> Glendale\n# Answer:\nUniversity of Phoenix Stadium", "# Reasoning Path:\nArizona Cardinals -> sports.sports_team.location -> Arizona -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nArizona", "# Reasoning Path:\nArizona Cardinals -> sports.sports_team.arena_stadium -> University of Phoenix Stadium -> common.topic.image -> UofPStadiumLogo\n# Answer:\nUniversity of Phoenix Stadium", "# Reasoning Path:\nArizona Cardinals -> sports.sports_team.arena_stadium -> University of Phoenix Stadium -> location.location.events -> 2007 BCS National Championship Game\n# Answer:\nUniversity of Phoenix Stadium", "# Reasoning Path:\nArizona Cardinals -> sports.sports_team.location -> Arizona -> law.court_jurisdiction_area.courts -> United States Bankruptcy Court, District of Arizona\n# Answer:\nArizona", "# Reasoning Path:\nArizona Cardinals -> sports.sports_team.location -> Arizona -> organization.organization_scope.organizations_with_this_scope -> Brazilian Consulate General, Los Angeles\n# Answer:\nArizona", "# Reasoning Path:\nArizona Cardinals -> sports.sports_team.arena_stadium -> University of Phoenix Stadium -> location.location.events -> 2007 Fiesta Bowl\n# Answer:\nUniversity of Phoenix Stadium", "# Reasoning Path:\nArizona Cardinals -> sports.sports_team.arena_stadium -> University of Phoenix Stadium -> location.location.events -> 2008 Fiesta Bowl\n# Answer:\nUniversity of Phoenix Stadium", "# Reasoning Path:\nArizona Cardinals -> american_football.football_team.away_games -> Arizona Cardinals at Carolina Panthers, 2009-01-10 -> american_football.football_game.home_team -> Carolina Panthers\n# Answer:\nArizona Cardinals at Carolina Panthers, 2009-01-10", "# Reasoning Path:\nArizona Cardinals -> american_football.football_team.away_games -> Arizona Cardinals at Carolina Panthers, 2008-10-26 -> american_football.football_game.receiving -> m.07948d4\n# Answer:\nArizona Cardinals at Carolina Panthers, 2008-10-26"], "ground_truth": ["Arizona"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1571", "prediction": ["# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> Newton's law of universal gravitation -> common.topic.article -> m.01kffv\n# Answer:\nNewton's law of universal gravitation", "# Reasoning Path:\nIsaac Newton -> base.argumentmaps.innovator.original_ideas -> Newton's law of universal gravitation -> common.topic.article -> m.01kffv\n# Answer:\nNewton's law of universal gravitation", "# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> Newton's law of universal gravitation -> common.topic.image -> Isaacnewton\n# Answer:\nNewton's law of universal gravitation", "# Reasoning Path:\nIsaac Newton -> base.argumentmaps.innovator.original_ideas -> Newton's law of universal gravitation -> common.topic.image -> Isaacnewton\n# Answer:\nNewton's law of universal gravitation", "# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> Newton's law of universal gravitation -> common.topic.image -> WMAP image of the (extremely tiny) anisotropies in the cosmic background radiation\n# Answer:\nNewton's law of universal gravitation", "# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> 8000 Isaac Newton -> common.topic.notable_types -> Asteroid\n# Answer:\n8000 Isaac Newton", "# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> 8000 Isaac Newton -> common.topic.webpage -> m.0h_l9mq\n# Answer:\n8000 Isaac Newton", "# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> Isaac Newton Institute -> common.topic.article -> m.036_xn\n# Answer:\nIsaac Newton Institute", "# Reasoning Path:\nIsaac Newton -> base.argumentmaps.innovator.original_ideas -> Newton's law of universal gravitation -> common.topic.image -> WMAP image of the (extremely tiny) anisotropies in the cosmic background radiation\n# Answer:\nNewton's law of universal gravitation", "# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> Isaac Newton Institute -> common.topic.notable_types -> Organization\n# Answer:\nIsaac Newton Institute"], "ground_truth": ["Newton's law of universal gravitation"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1572", "prediction": ["# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Golden Gate Park -> travel.travel_destination.tourist_attractions -> M. H. de Young Memorial Museum\n# Answer:\nGolden Gate Park", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Golden Gate Park -> travel.travel_destination.tourist_attractions -> AIDS Memorial Grove\n# Answer:\nGolden Gate Park", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Golden Gate Park -> travel.travel_destination.tourist_attractions -> California Academy of Sciences\n# Answer:\nGolden Gate Park", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Yerba Buena Center for the Arts -> common.topic.article -> m.027yy0f\n# Answer:\nYerba Buena Center for the Arts", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Yerba Buena Center for the Arts -> architecture.structure.architect -> Fumihiko Maki\n# Answer:\nYerba Buena Center for the Arts", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> San Francisco Railway Museum -> common.topic.notable_for -> g.1255l10md\n# Answer:\nSan Francisco Railway Museum", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> book.written_work.subjects -> California\n# Answer:\n1906", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Golden Gate Park -> location.location.contains -> M. H. de Young Memorial Museum\n# Answer:\nGolden Gate Park", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> San Francisco Railway Museum -> location.location.containedby -> 94105\n# Answer:\nSan Francisco Railway Museum", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Golden Gate Park -> location.location.contains -> AIDS Memorial Grove\n# Answer:\nGolden Gate Park"], "ground_truth": ["San Francisco Ferry Building", "Travefy", "Union Square", "San Francisco Museum of Modern Art", "Golden Gate Park", "Presidio of San Francisco", "Ghirardelli Square", "St. Regis Museum Tower", "Golden Gate Bridge", "Twin Peaks", "Exploratorium", "Baker Beach", "Ripley's Believe It or Not! Museum", "Cartoon Art Museum", "Fisherman's Wharf", "Chinatown", "Yerba Buena Center for the Arts", "San Francisco Railway Museum", "Contemporary Jewish Museum", "Palace of the Legion of Honor", "Lombard Street", "Angel Island", "San Francisco City Hall", "San Francisco Fire Department Museum", "Mus\u00e9e M\u00e9canique", "Coit Tower", "Haas-Lilienthal House", "Japanese Tea Garden", "Alcatraz Island", "Asian Art Museum of San Francisco", "Camera Obscura", "Crissy Field", "Museum of the African Diaspora", "San Francisco cable car system", "Consulate General of Mexico, San Francisco"], "ans_acc": 0.08571428571428572, "ans_hit": 1, "ans_f1": 0.1565217391304348, "ans_precission": 0.9, "ans_recall": 0.08571428571428572, "path_f1": 0.1565217391304348, "path_precision": 0.9, "path_recall": 0.08571428571428572, "path_ans_f1": 0.1565217391304348, "path_ans_precision": 0.9, "path_ans_recall": 0.08571428571428572}
{"id": "WebQTest-1573", "prediction": ["# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Eug\u00e8ne Delacroix -> influence.influence_node.influenced_by -> Peter Paul Rubens\n# Answer:\nEug\u00e8ne Delacroix", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Eug\u00e8ne Delacroix -> influence.influence_node.influenced -> \u00c9douard Manet\n# Answer:\nEug\u00e8ne Delacroix", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Diego Vel\u00e1zquez -> influence.influence_node.influenced_by -> Peter Paul Rubens\n# Answer:\nDiego Vel\u00e1zquez", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Eug\u00e8ne Delacroix -> book.author.school_or_movement -> Romanticism\n# Answer:\nEug\u00e8ne Delacroix", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Sara Murphy -> influence.influence_node.influenced -> Robert Benchley\n# Answer:\nSara Murphy", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Eug\u00e8ne Delacroix -> influence.influence_node.influenced -> Camille Pissarro\n# Answer:\nEug\u00e8ne Delacroix", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Diego Vel\u00e1zquez -> influence.influence_node.influenced -> \u00c9douard Manet\n# Answer:\nDiego Vel\u00e1zquez", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Eug\u00e8ne Delacroix -> influence.influence_node.influenced -> Claude Monet\n# Answer:\nEug\u00e8ne Delacroix", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Diego Vel\u00e1zquez -> people.person.nationality -> Spain\n# Answer:\nDiego Vel\u00e1zquez", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Sara Murphy -> influence.influence_node.influenced -> Archibald MacLeish\n# Answer:\nSara Murphy"], "ground_truth": ["Francisco Goya", "Paul C\u00e9zanne", "Diego Vel\u00e1zquez", "Alfred Jarry", "Eug\u00e8ne Delacroix", "Vincent van Gogh", "Jean-Auguste-Dominique Ingres", "Henri Rousseau", "Sara Murphy", "Paul Gauguin", "\u00c9douard Manet", "Henri de Toulouse-Lautrec"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 1.0, "ans_recall": 0.25, "path_f1": 0.4, "path_precision": 1.0, "path_recall": 0.25, "path_ans_f1": 0.5, "path_ans_precision": 1.0, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1574", "prediction": ["# Reasoning Path:\nPennsylvania -> location.location.time_zones -> Eastern Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nPennsylvania -> location.location.time_zones -> UTC\u221205:00 -> common.topic.notable_types -> Time Zone\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.title -> Director, Office of Health Care Reform\n# Answer:\nDirector, Office of Health Care Reform", "# Reasoning Path:\nPennsylvania -> location.location.time_zones -> UTC\u221205:00 -> common.topic.notable_for -> g.125fnb_r1\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.person -> Rosemarie B. Greco\n# Answer:\nRosemarie B. Greco", "# Reasoning Path:\nPennsylvania -> location.location.time_zones -> UTC\u221205:00 -> common.topic.image -> Timezones2008 UTC-5\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nPennsylvania -> common.topic.webpage -> m.04lsvtk -> common.webpage.resource -> m.0blz_lk\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nPennsylvania -> common.topic.webpage -> m.09wcxfl -> common.webpage.resource -> Exclusive: Jon Gosselin tells EW 'I am not guilty of cheating on my wife'\n# Answer:\nExclusive: Jon Gosselin tells EW 'I am not guilty of cheating on my wife'", "# Reasoning Path:\nPennsylvania -> common.topic.webpage -> m.04lsvtk -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nPennsylvania -> common.topic.webpage -> m.09wcyd_ -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Eastern Time Zone", "UTC\u221205:00"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1575", "prediction": ["# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Chrome -> common.topic.subjects -> Google Apps for Work\n# Answer:\nGoogle Chrome", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> organization.organization_partnership.members -> Redcorus\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> common.topic.subject_of -> Google for Education\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> common.topic.subjects -> Google Apps for Work -> common.topic.subjects -> Gmail\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> common.topic.subjects -> Google Apps for Work -> organization.organization_partnership.members -> Redcorus\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> common.topic.subject_of -> Google Chrome\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Chrome -> common.topic.subjects -> Google for Work\n# Answer:\nGoogle Chrome", "# Reasoning Path:\nGoogle -> common.topic.subjects -> Google Apps for Work -> common.topic.subjects -> App Store\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> common.topic.subjects -> Google Cloud Platform -> common.topic.subjects -> Google Developers\n# Answer:\nGoogle Cloud Platform", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> common.topic.subject_of -> Google Docs, Sheets, and Slides\n# Answer:\nGoogle Apps for Work"], "ground_truth": ["Google Maps", "Chromecast", "Google Glass", "Nexus Q", "Google Classroom", "Nexus 10", "Nexus 7", "Google Drive", "Nexus S", "Google Earth", "Google Docs, Sheets, and Slides", "Apache Wave", "Google Buzz"], "ans_acc": 0.07692307692307693, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.08695652173913043, "path_ans_precision": 0.1, "path_ans_recall": 0.07692307692307693}
{"id": "WebQTest-1577", "prediction": ["# Reasoning Path:\nCliff Lee -> baseball.baseball_player.batting_stats -> m.06r7541 -> baseball.batting_statistics.team -> Cleveland Indians\n# Answer:\nCleveland Indians", "# Reasoning Path:\nCliff Lee -> sports.pro_athlete.teams -> m.0j2qj2k -> sports.sports_team_roster.team -> Cleveland Indians\n# Answer:\nCleveland Indians", "# Reasoning Path:\nCliff Lee -> baseball.baseball_player.batting_stats -> m.06r7541 -> baseball.batting_statistics.season -> 2008 Major League Baseball season\n# Answer:\n2008 Major League Baseball season", "# Reasoning Path:\nCliff Lee -> baseball.baseball_player.batting_stats -> m.06r74_5 -> baseball.batting_statistics.team -> Cleveland Indians\n# Answer:\nCleveland Indians", "# Reasoning Path:\nCliff Lee -> baseball.baseball_player.batting_stats -> m.06r74vn -> baseball.batting_statistics.team -> Cleveland Indians\n# Answer:\nCleveland Indians", "# Reasoning Path:\nCliff Lee -> sports.pro_athlete.teams -> m.0j2qj2k -> sports.sports_team_roster.position -> Pitcher\n# Answer:\nPitcher", "# Reasoning Path:\nCliff Lee -> sports.pro_athlete.teams -> m.0j2s9p9 -> sports.sports_team_roster.team -> Philadelphia Phillies\n# Answer:\nPhiladelphia Phillies", "# Reasoning Path:\nCliff Lee -> people.person.profession -> Baseball player -> common.topic.notable_types -> Profession\n# Answer:\nBaseball player", "# Reasoning Path:\nCliff Lee -> baseball.baseball_player.batting_stats -> m.06r74_5 -> baseball.batting_statistics.season -> 2005 Major League Baseball season\n# Answer:\n2005 Major League Baseball season", "# Reasoning Path:\nCliff Lee -> baseball.baseball_player.batting_stats -> m.06r74vn -> baseball.batting_statistics.season -> 2002 Major League Baseball Season\n# Answer:\n2002 Major League Baseball Season"], "ground_truth": ["Philadelphia Phillies"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1580", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> South Korea\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> American Samoa\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Northern Mariana Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guam\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Marshall Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Mexico\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Germany\n# Answer:\nFederal republic"], "ground_truth": ["Federal republic", "Presidential system", "Constitutional republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1582", "prediction": ["# Reasoning Path:\nHong Kong -> location.location.time_zones -> Hong Kong Time Zone -> time.time_zone.locations_in_this_time_zone -> Lantau Island\n# Answer:\nHong Kong Time Zone", "# Reasoning Path:\nHong Kong -> location.location.time_zones -> Hong Kong Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nHong Kong Time Zone", "# Reasoning Path:\nHong Kong -> location.location.time_zones -> Hong Kong Time Zone -> time.time_zone.locations_in_this_time_zone -> Lamma Island\n# Answer:\nHong Kong Time Zone", "# Reasoning Path:\nHong Kong -> location.statistical_region.gdp_nominal_per_capita -> g.11b60qhw67\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nHong Kong -> location.statistical_region.co2_emissions_per_capita -> g.1245_2gxn\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nHong Kong -> location.location.time_zones -> Hong Kong Time Zone -> time.time_zone.locations_in_this_time_zone -> Asia\n# Answer:\nHong Kong Time Zone", "# Reasoning Path:\nHong Kong -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6g4jl\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nHong Kong -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc37pmj\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nHong Kong -> location.statistical_region.co2_emissions_per_capita -> g.1245_3w31\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nHong Kong -> location.statistical_region.co2_emissions_per_capita -> g.1245__rsm\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita"], "ground_truth": ["Hong Kong Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1583", "prediction": ["# Reasoning Path:\nTaylor Lautner -> award.award_winner.awards_won -> m.0kb9_qw -> award.award_honor.honored_for -> Abduction\n# Answer:\nAbduction", "# Reasoning Path:\nTaylor Lautner -> award.award_winner.awards_won -> m.0kb9_qw -> award.award_honor.ceremony -> 2012 Teen Choice Awards\n# Answer:\n2012 Teen Choice Awards", "# Reasoning Path:\nTaylor Lautner -> award.award_nominee.award_nominations -> m.0z837pz -> award.award_nomination.nominated_for -> Abduction\n# Answer:\nAbduction", "# Reasoning Path:\nTaylor Lautner -> award.award_nominee.award_nominations -> m.0j2vjzw -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nTaylor Lautner -> award.award_winner.awards_won -> m.09rpwdk -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nWinning work", "# Reasoning Path:\nTaylor Lautner -> award.award_nominee.award_nominations -> m.0j2vjzw -> award.award_nomination.nominated_for -> Abduction\n# Answer:\nAbduction", "# Reasoning Path:\nTaylor Lautner -> award.award_winner.awards_won -> m.01086803 -> award.award_honor.award -> Scream Awards for Male Breakout Performance\n# Answer:\nScream Awards for Male Breakout Performance", "# Reasoning Path:\nTaylor Lautner -> award.award_nominee.award_nominations -> m.0z837pz -> award.award_nomination.award -> Teen Choice Award for Choice Movie Actor: Drama/Action Adventure\n# Answer:\nTeen Choice Award for Choice Movie Actor: Drama/Action Adventure", "# Reasoning Path:\nTaylor Lautner -> award.award_winner.awards_won -> m.01086803 -> award.award_honor.honored_for -> Twilight\n# Answer:\nTwilight", "# Reasoning Path:\nTaylor Lautner -> award.award_winner.awards_won -> m.09rpwdk -> award.award_honor.ceremony -> 36th People's Choice Awards\n# Answer:\n36th People's Choice Awards"], "ground_truth": ["The Twilight Saga: Breaking Dawn - Part 1", "Abduction", "Grown Ups 2", "Eclipse", "Twilight", "Incarceron", "The Nick and Jessica Variety Hour", "Run the Tide", "The Twilight Saga: Breaking Dawn - Part 2", "The Adventures of Sharkboy and Lavagirl", "He's a Bully, Charlie Brown", "Field of Dreams 2: Lockout", "The Twilight Saga: New Moon", "Cheaper by the Dozen 2", "Northern Lights", "The Ridiculous Six", "Valentine's Day", "Shadow Fury", "Tracers"], "ans_acc": 0.15789473684210525, "ans_hit": 1, "ans_f1": 0.23999999999999996, "ans_precission": 0.5, "ans_recall": 0.15789473684210525, "path_f1": 0.13157894736842105, "path_precision": 0.5, "path_recall": 0.07575757575757576, "path_ans_f1": 0.23999999999999996, "path_ans_precision": 0.5, "path_ans_recall": 0.15789473684210525}
{"id": "WebQTest-1584", "prediction": ["# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> medicine.disease.symptoms -> Weight loss\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> medicine.disease.symptoms -> Cutaneous manifestations of sarcoidosis\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> medicine.disease.symptoms -> Garland's triad\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> common.topic.subject_of -> Del Sorbo Antonio\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Complications from pneumonia -> common.topic.notable_for -> g.1yl5vldtz\n# Answer:\nComplications from pneumonia", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Complications from pneumonia -> people.cause_of_death.people -> Aram Haigaz\n# Answer:\nComplications from pneumonia", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Complications from pneumonia -> people.cause_of_death.parent_cause_of_death -> Pneumonia\n# Answer:\nComplications from pneumonia", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> common.topic.subject_of -> JAMA Dermatology\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Complications from pneumonia -> people.cause_of_death.people -> Blanca Vicu\u00f1a Ardohain\n# Answer:\nComplications from pneumonia", "# Reasoning Path:\nBernie Mac -> common.topic.image -> MrBernieMac -> common.image.size -> m.02cyp8t\n# Answer:\nMrBernieMac"], "ground_truth": ["Complications from pneumonia", "Sarcoidosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1585", "prediction": ["# Reasoning Path:\nSpike Spiegel -> base.schemastaging.tv_character_extra.regular_dubbing_performances -> m.0nh3wqf -> base.schemastaging.tv_star_dubbing_performance.actor -> Steven Blum\n# Answer:\nSteven Blum", "# Reasoning Path:\nSpike Spiegel -> film.film_character.portrayed_in_films -> m.03lg03s -> film.performance.actor -> K\u014dichi Yamadera\n# Answer:\nK\u014dichi Yamadera", "# Reasoning Path:\nSpike Spiegel -> base.schemastaging.tv_character_extra.regular_dubbing_performances -> m.0nh3wqf -> base.schemastaging.tv_star_dubbing_performance.language -> English Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nSpike Spiegel -> tv.tv_character.appeared_in_tv_program -> m.0gj1nnq -> tv.regular_tv_appearance.actor -> K\u014dichi Yamadera\n# Answer:\nK\u014dichi Yamadera", "# Reasoning Path:\nSpike Spiegel -> film.film_character.portrayed_in_films -> m.03lg03s -> film.performance.film -> Cowboy Bebop: The Movie\n# Answer:\nCowboy Bebop: The Movie", "# Reasoning Path:\nSpike Spiegel -> tv.tv_character.appeared_in_tv_program -> m.0gj1nnq -> tv.regular_tv_appearance.series -> Cowboy Bebop\n# Answer:\nCowboy Bebop", "# Reasoning Path:\nSpike Spiegel -> film.film_character.portrayed_in_films -> m.03lg034 -> film.performance.actor -> Steven Blum\n# Answer:\nSteven Blum", "# Reasoning Path:\nSpike Spiegel -> tv.tv_character.appeared_in_tv_program -> m.0gj1nnq -> tv.regular_tv_appearance.special_performance_type -> Voice acting in Japan\n# Answer:\nVoice acting in Japan", "# Reasoning Path:\nSpike Spiegel -> film.film_character.portrayed_in_films -> m.03lg034 -> film.performance.film -> Cowboy Bebop: The Movie\n# Answer:\nCowboy Bebop: The Movie"], "ground_truth": ["K\u014dichi Yamadera", "Steven Blum"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 0.4444444444444444, "ans_recall": 1.0, "path_f1": 0.6153846153846153, "path_precision": 0.4444444444444444, "path_recall": 1.0, "path_ans_f1": 0.6153846153846153, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 1.0}
{"id": "WebQTest-1586", "prediction": ["# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> Isabella of Portugal, Queen of Castile -> people.person.parents -> John, Constable of Portugal\n# Answer:\nIsabella of Portugal, Queen of Castile", "# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> Isabella of Portugal, Queen of Castile -> people.person.parents -> Isabel of Barcelos\n# Answer:\nIsabella of Portugal, Queen of Castile", "# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> Isabella of Portugal, Queen of Castile -> people.person.nationality -> Portugal\n# Answer:\nIsabella of Portugal, Queen of Castile", "# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> Isabella of Portugal, Queen of Castile -> people.person.children -> Alfonso, Prince of Asturias\n# Answer:\nIsabella of Portugal, Queen of Castile", "# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> John II of Castile -> people.person.parents -> Catherine of Lancaster\n# Answer:\nJohn II of Castile", "# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> John II of Castile -> people.person.parents -> Henry III of Castile\n# Answer:\nJohn II of Castile", "# Reasoning Path:\nIsabella I of Castile -> people.person.religion -> Catholicism -> religion.religion.holidays -> All Saints' Day\n# Answer:\nCatholicism", "# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> John II of Castile -> people.person.gender -> Male\n# Answer:\nJohn II of Castile", "# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> John II of Castile -> people.person.spouse_s -> m.0j4lydq\n# Answer:\nJohn II of Castile", "# Reasoning Path:\nIsabella I of Castile -> people.person.religion -> Catholicism -> religion.religion.holidays -> Corpus Christi\n# Answer:\nCatholicism"], "ground_truth": ["Isabella of Portugal, Queen of Castile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1587", "prediction": ["# Reasoning Path:\nChris Hemsworth -> people.person.children -> India Rose Hemsworth -> people.person.parents -> Elsa Pataky\n# Answer:\nIndia Rose Hemsworth", "# Reasoning Path:\nChris Hemsworth -> people.person.children -> India Rose Hemsworth -> people.person.gender -> Female\n# Answer:\nIndia Rose Hemsworth", "# Reasoning Path:\nChris Hemsworth -> people.person.children -> Tristan Hemsworth -> people.person.place_of_birth -> West Hollywood\n# Answer:\nTristan Hemsworth", "# Reasoning Path:\nChris Hemsworth -> people.person.spouse_s -> m.0g5bzqc -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nChris Hemsworth -> people.person.children -> India Rose Hemsworth -> common.topic.notable_types -> Person\n# Answer:\nIndia Rose Hemsworth", "# Reasoning Path:\nChris Hemsworth -> people.person.children -> Tristan Hemsworth -> people.person.gender -> Male\n# Answer:\nTristan Hemsworth", "# Reasoning Path:\nChris Hemsworth -> people.person.spouse_s -> m.0g5bzqc -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nChris Hemsworth -> people.person.children -> Tristan Hemsworth -> people.person.parents -> Elsa Pataky\n# Answer:\nTristan Hemsworth", "# Reasoning Path:\nChris Hemsworth -> people.person.children -> Sasha Hemsworth -> common.topic.notable_types -> Person\n# Answer:\nSasha Hemsworth", "# Reasoning Path:\nChris Hemsworth -> people.person.spouse_s -> m.0g5bzqc -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["Elsa Pataky"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.26666666666666666, "path_precision": 0.2, "path_recall": 0.4, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1589", "prediction": ["# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Winnipeg -> location.location.containedby -> Winnipeg Capital Region\n# Answer:\nWinnipeg", "# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Winnipeg -> location.location.nearby_airports -> Winnipeg James Armstrong Richardson International Airport\n# Answer:\nWinnipeg", "# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Winnipeg -> location.location.containedby -> Area code 204\n# Answer:\nWinnipeg", "# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Winnipeg -> location.location.containedby -> Area code 431\n# Answer:\nWinnipeg", "# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Rural Municipality of Headingley -> location.location.containedby -> Winnipeg Capital Region\n# Answer:\nRural Municipality of Headingley", "# Reasoning Path:\nMTS Iceplex -> sports.sports_facility.home_venue_for -> m.0wz2_y_ -> sports.team_venue_relationship.team -> Winnipeg Blues\n# Answer:\nWinnipeg Blues", "# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Winnipeg -> location.location.nearby_airports -> St. Andrews Airport\n# Answer:\nWinnipeg", "# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Winnipeg -> location.location.events -> Winnipeg General Strike\n# Answer:\nWinnipeg", "# Reasoning Path:\nMTS Iceplex -> common.topic.notable_types -> Venue -> common.topic.article -> m.0hn6c73\n# Answer:\nVenue", "# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Rural Municipality of Headingley -> location.location.containedby -> Manitoba\n# Answer:\nRural Municipality of Headingley"], "ground_truth": ["Rural Municipality of Headingley", "Winnipeg"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-159", "prediction": ["# Reasoning Path:\nHilo -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> common.topic.article -> m.02lctt\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nHilo -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nHilo -> location.location.geolocation -> m.0kbnpg\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nHilo -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> common.topic.notable_for -> g.125bn2ktb\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nHilo -> location.location.people_born_here -> Derek Shizuto -> common.topic.notable_for -> g.125b5498b\n# Answer:\nDerek Shizuto", "# Reasoning Path:\nHilo -> location.location.people_born_here -> Derek Shizuto -> people.person.gender -> Male\n# Answer:\nDerek Shizuto", "# Reasoning Path:\nHilo -> location.location.people_born_here -> Albert Spencer Wilcox -> common.topic.image -> Kauai-AlbertSpencerWilcox-beachhouse\n# Answer:\nAlbert Spencer Wilcox", "# Reasoning Path:\nHilo -> location.location.people_born_here -> Albert Spencer Wilcox -> people.person.parents -> Abner Wilcox\n# Answer:\nAlbert Spencer Wilcox", "# Reasoning Path:\nHilo -> location.location.people_born_here -> Bennett N\u0101m\u0101k\u0113h\u0101 -> people.person.parents -> Kinoiki Kekaulike\n# Answer:\nBennett N\u0101m\u0101k\u0113h\u0101", "# Reasoning Path:\nHilo -> location.location.people_born_here -> Bennett N\u0101m\u0101k\u0113h\u0101 -> people.deceased_person.place_of_burial -> Royal Mausoleum of Hawaii\n# Answer:\nBennett N\u0101m\u0101k\u0113h\u0101"], "ground_truth": ["Hawaii-Aleutian Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1590", "prediction": ["# Reasoning Path:\nIsrael -> location.statistical_region.religions -> m.0468s5y -> location.religion_percentage.religion -> Judaism\n# Answer:\nJudaism", "# Reasoning Path:\nIsrael -> symbols.namesake.named_after -> Jacob -> people.person.religion -> Judaism\n# Answer:\nJacob", "# Reasoning Path:\nIsrael -> media_common.quotation.author -> Aaron Friedman -> people.person.religion -> Judaism\n# Answer:\nAaron Friedman", "# Reasoning Path:\nIsrael -> symbols.namesake.named_after -> Jacob -> people.deceased_person.place_of_death -> Egypt\n# Answer:\nJacob", "# Reasoning Path:\nIsrael -> media_common.quotation.author -> Aaron Friedman -> people.person.nationality -> United States of America\n# Answer:\nAaron Friedman"], "ground_truth": ["Judaism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.6666666666666665, "path_precision": 0.6, "path_recall": 0.75, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1591", "prediction": ["# Reasoning Path:\nBarack Obama -> people.person.profession -> Law professor -> people.profession.specialization_of -> g.121bkpjb\n# Answer:\nLaw professor", "# Reasoning Path:\nBarack Obama -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Senator\n# Answer:\nPolitician", "# Reasoning Path:\nBarack Obama -> people.person.profession -> Law professor -> common.topic.notable_types -> Profession\n# Answer:\nLaw professor", "# Reasoning Path:\nBarack Obama -> people.person.profession -> Law professor -> common.topic.notable_for -> g.125g4w3jj\n# Answer:\nLaw professor", "# Reasoning Path:\nBarack Obama -> people.person.profession -> Author -> book.book_subject.works -> C-SPAN's LCV 2011 U.S. Cities Tour Wraps up Savannah Shoot, Now in Charleston\n# Answer:\nAuthor", "# Reasoning Path:\nBarack Obama -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> Ambassador\n# Answer:\nPolitician", "# Reasoning Path:\nBarack Obama -> people.person.profession -> Politician -> freebase.type_profile.kind -> Title\n# Answer:\nPolitician", "# Reasoning Path:\nBarack Obama -> government.politician.government_positions_held -> m.03h9hn4 -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nBarack Obama -> people.person.profession -> Politician -> type.type.expected_by -> Officeholder\n# Answer:\nPolitician", "# Reasoning Path:\nBarack Obama -> government.politician.government_positions_held -> m.03h96h6 -> government.government_position_held.office_position_or_title -> Illinois State Senator\n# Answer:\nIllinois State Senator"], "ground_truth": ["Writer", "Author", "Politician", "Lawyer", "Law professor"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6857142857142857, "ans_precission": 0.8, "ans_recall": 0.6, "path_f1": 0.6857142857142857, "path_precision": 0.8, "path_recall": 0.6, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 1.0, "path_ans_recall": 0.6}
{"id": "WebQTest-1592", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> Somebody to Love\n# Answer:\nSomebody to Love", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> My Worlds: The Collection -> music.album.genre -> Pop music\n# Answer:\nMy Worlds: The Collection", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> My Worlds: The Collection -> music.album.release_type -> Album\n# Answer:\nMy Worlds: The Collection", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.featured_artists -> Ludacris\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> My Worlds: The Collection -> music.album.album_content_type -> Compilation album\n# Answer:\nMy Worlds: The Collection", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> My Worlds: The Collection -> music.album.genre -> Contemporary R&B\n# Answer:\nMy Worlds: The Collection", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> music.composition.composer -> Ryan Toby\n# Answer:\nAll Bad", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> My Worlds: The Collection -> music.album.genre -> Dance-pop\n# Answer:\nMy Worlds: The Collection", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.artist -> Ludacris\n# Answer:\nAll Around the World"], "ground_truth": ["Confident", "Eenie Meenie", "Bigger", "Heartbreaker", "Hold Tight", "Thought Of You", "All Around The World", "Die in Your Arms", "Beautiful", "As Long as You Love Me", "Pray", "Home to Mama", "Lolly", "Recovery", "All That Matters", "Right Here", "Live My Life", "Change Me", "Somebody to Love", "Never Let You Go", "Beauty And A Beat", "Never Say Never", "Boyfriend", "Bad Day", "All Bad", "Roller Coaster", "#thatPower", "Baby", "Wait for a Minute", "PYD", "First Dance", "Turn to You (Mother's Day Dedication)"], "ans_acc": 0.09375, "ans_hit": 1, "ans_f1": 0.1518987341772152, "ans_precission": 0.4, "ans_recall": 0.09375, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.1518987341772152, "path_ans_precision": 0.4, "path_ans_recall": 0.09375}
{"id": "WebQTest-1593", "prediction": ["# Reasoning Path:\nRiver Thames -> geography.river.origin -> Thames Head -> location.location.containedby -> Gloucestershire\n# Answer:\nThames Head", "# Reasoning Path:\nRiver Thames -> geography.river.origin -> Thames Head -> common.topic.notable_for -> g.1258953bg\n# Answer:\nThames Head", "# Reasoning Path:\nRiver Thames -> geography.river.origin -> Thames Head -> common.topic.article -> m.02pj_ts\n# Answer:\nThames Head", "# Reasoning Path:\nRiver Thames -> geography.body_of_water.islands -> Two Tree Island -> geography.island.body_of_water -> Tideway\n# Answer:\nTwo Tree Island", "# Reasoning Path:\nRiver Thames -> location.location.containedby -> United Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nRiver Thames -> geography.body_of_water.islands -> Two Tree Island -> location.location.containedby -> United Kingdom\n# Answer:\nTwo Tree Island", "# Reasoning Path:\nRiver Thames -> location.location.containedby -> United Kingdom -> periodicals.newspaper_circulation_area.newspapers -> i\n# Answer:\ni", "# Reasoning Path:\nRiver Thames -> geography.body_of_water.islands -> Brentford Ait -> location.location.containedby -> United Kingdom\n# Answer:\nBrentford Ait", "# Reasoning Path:\nRiver Thames -> location.location.containedby -> United Kingdom -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nRiver Thames -> geography.body_of_water.islands -> Two Tree Island -> common.topic.image -> The scrape on Two Tree Island, seen from the hide.\n# Answer:\nTwo Tree Island"], "ground_truth": ["Thames Head"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1595", "prediction": ["# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83n09 -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z8jm72 -> award.award_nomination.nominated_for -> The Twilight Saga: New Moon\n# Answer:\nThe Twilight Saga: New Moon", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.award_nominee -> Kristen Stewart\n# Answer:\nKristen Stewart", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.ceremony -> 2012 Teen Choice Awards\n# Answer:\n2012 Teen Choice Awards", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83n09 -> award.award_nomination.award -> Teen Choice Award for Choice Movie Actor: Romance\n# Answer:\nTeen Choice Award for Choice Movie Actor: Romance", "# Reasoning Path:\nRobert Pattinson -> award.award_winner.awards_won -> m.0100q0tx -> award.award_honor.honored_for -> The Twilight Saga: Breaking Dawn - Part 2\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 2", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z8jm72 -> award.award_nomination.award -> Teen Choice Award for Choice Movie Actor: Fantasy\n# Answer:\nTeen Choice Award for Choice Movie Actor: Fantasy", "# Reasoning Path:\nRobert Pattinson -> award.award_winner.awards_won -> m.0b0b974 -> award.award_honor.honored_for -> Twilight\n# Answer:\nTwilight", "# Reasoning Path:\nRobert Pattinson -> people.person.sibling_s -> m.07sh9v2 -> people.sibling_relationship.sibling -> Lizzy Pattinson\n# Answer:\nLizzy Pattinson"], "ground_truth": ["Bel Ami", "The Twilight Saga: Breaking Dawn - Part 1", "Eclipse", "Vanity Fair", "Life", "The Summer House", "Twilight", "Remember Me", "Little Ashes", "Maps to the Stars", "The Childhood of a Leader", "How to Be", "Hold on to Me", "The Twilight Saga: Breaking Dawn - Part 2", "Dark Kingdom: The Dragon King", "Mission: Blacklist", "Queen of the Desert", "Cosmopolis", "The Rover", "The Haunted Airman", "The Twilight Saga: New Moon", "Unbound Captives", "Harry Potter and the Goblet of Fire", "Water for Elephants", "The Bad Mother's Handbook", "Harry Potter and the Order of the Phoenix", "Love & Distrust"], "ans_acc": 0.14814814814814814, "ans_hit": 1, "ans_f1": 0.22857142857142856, "ans_precission": 0.5, "ans_recall": 0.14814814814814814, "path_f1": 0.09803921568627451, "path_precision": 0.5, "path_recall": 0.05434782608695652, "path_ans_f1": 0.22857142857142856, "path_ans_precision": 0.5, "path_ans_recall": 0.14814814814814814}
{"id": "WebQTest-1596", "prediction": ["# Reasoning Path:\nPakistan -> location.country.languages_spoken -> Hindustani language -> language.human_language.writing_system -> Devanagari\n# Answer:\nHindustani language", "# Reasoning Path:\nPakistan -> location.country.languages_spoken -> Hindustani language -> language.human_language.writing_system -> Persian alphabet\n# Answer:\nHindustani language", "# Reasoning Path:\nPakistan -> location.country.languages_spoken -> Hindustani language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nHindustani language", "# Reasoning Path:\nPakistan -> location.country.languages_spoken -> Hindustani language -> common.topic.notable_types -> Human Language\n# Answer:\nHindustani language", "# Reasoning Path:\nPakistan -> location.statistical_region.consumer_price_index -> g.11b60w91bg\n# Answer:\nlocation.statistical_region.consumer_price_index", "# Reasoning Path:\nPakistan -> location.statistical_region.labor_participation_rate -> g.11b71mdmvg\n# Answer:\nlocation.statistical_region.labor_participation_rate", "# Reasoning Path:\nPakistan -> location.country.languages_spoken -> Hindustani language -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nHindustani language", "# Reasoning Path:\nPakistan -> location.statistical_region.consumer_price_index -> g.12tb6gb2b\n# Answer:\nlocation.statistical_region.consumer_price_index", "# Reasoning Path:\nPakistan -> location.country.languages_spoken -> Hindustani language -> language.human_language.language_family -> Indo-Aryan languages\n# Answer:\nHindustani language", "# Reasoning Path:\nPakistan -> location.statistical_region.consumer_price_index -> g.1hhc37ps6\n# Answer:\nlocation.statistical_region.consumer_price_index"], "ground_truth": ["English Language", "Urdu Language"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1597", "prediction": ["# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Sports equipment -> business.industry.child_industry -> Fabricated Rubber Products, NEC (wet suits)\n# Answer:\nSports equipment", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Sports equipment -> business.industry.parent_industry -> Other Miscellaneous Manufacturing\n# Answer:\nSports equipment", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Sports equipment -> freebase.equivalent_topic.equivalent_type -> Sports Equipment\n# Answer:\nSports equipment", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Footwear Manufacturing -> business.industry.child_industry -> Athletic Shoes & Apparel\n# Answer:\nFootwear Manufacturing", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Sports equipment -> business.industry.child_industry -> Ice hockey equipment\n# Answer:\nSports equipment", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Athletic Shoes & Apparel -> business.industry.companies -> Crocs\n# Answer:\nAthletic Shoes & Apparel", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Athletic Shoes & Apparel -> common.topic.notable_types -> Industry\n# Answer:\nAthletic Shoes & Apparel", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Sports equipment -> business.industry.child_industry -> Sporting and Athletic Goods\n# Answer:\nSports equipment", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Athletic Shoes & Apparel -> business.industry.companies -> Deckers Outdoor Corporation\n# Answer:\nAthletic Shoes & Apparel", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Footwear Manufacturing -> common.topic.notable_for -> g.1257xhlq7\n# Answer:\nFootwear Manufacturing"], "ground_truth": ["Athletic Shoes & Apparel", "Footwear Manufacturing", "Sports equipment", "Footwear", "Fashion accessory", "Clothing"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 1.0, "ans_recall": 0.6666666666666666, "path_f1": 0.8, "path_precision": 1.0, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1598", "prediction": ["# Reasoning Path:\nLyndon B. Johnson -> people.person.children -> Luci Baines Johnson -> people.person.children -> Rebekah Johnson Nugent\n# Answer:\nLuci Baines Johnson", "# Reasoning Path:\nLyndon B. Johnson -> people.person.spouse_s -> m.0j4k73z -> people.marriage.spouse -> Lady Bird Johnson\n# Answer:\nLady Bird Johnson", "# Reasoning Path:\nLyndon B. Johnson -> base.famouspets.pet_owner.pets_owned -> m.05hqsbs -> base.famouspets.pet_ownership.owner -> Lady Bird Johnson\n# Answer:\nLady Bird Johnson", "# Reasoning Path:\nLyndon B. Johnson -> people.person.children -> Luci Baines Johnson -> people.person.children -> Claudia Taylor Nugent\n# Answer:\nLuci Baines Johnson", "# Reasoning Path:\nLyndon B. Johnson -> people.person.children -> Luci Baines Johnson -> people.person.spouse_s -> m.05kg486\n# Answer:\nLuci Baines Johnson", "# Reasoning Path:\nLyndon B. Johnson -> people.person.children -> Lynda Bird Johnson Robb -> people.person.children -> Catherine Lewis Robb\n# Answer:\nLynda Bird Johnson Robb", "# Reasoning Path:\nLyndon B. Johnson -> people.person.children -> Lynda Bird Johnson Robb -> people.person.spouse_s -> m.0j4l1hb\n# Answer:\nLynda Bird Johnson Robb", "# Reasoning Path:\nLyndon B. Johnson -> people.person.spouse_s -> m.0j4k73z -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nLyndon B. Johnson -> people.person.children -> Luci Baines Johnson -> people.person.children -> Nicole Marie Nugent\n# Answer:\nLuci Baines Johnson", "# Reasoning Path:\nLyndon B. Johnson -> people.person.children -> Luci Baines Johnson -> organization.organization_member.member_of -> m.05kg44b\n# Answer:\nLuci Baines Johnson"], "ground_truth": ["Lady Bird Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.2105263157894737, "path_precision": 0.2, "path_recall": 0.2222222222222222, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1599", "prediction": ["# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_death -> Pyongyang -> location.location.contains -> Kumsusan Palace of the Sun\n# Answer:\nPyongyang", "# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_death -> Pyongyang -> location.location.contains -> Kim Il-sung Stadium\n# Answer:\nPyongyang", "# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_death -> Pyongyang -> common.topic.notable_types -> City/Town/Village\n# Answer:\nPyongyang", "# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_death -> Pyongyang -> location.location.events -> 1987 Pyongyang Film Festival\n# Answer:\nPyongyang", "# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_death -> Pyongyang -> location.location.contains -> Arch of Triumph\n# Answer:\nPyongyang", "# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_death -> Pyongyang -> location.location.events -> 1990 Pyongyang Film Festival\n# Answer:\nPyongyang", "# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_death -> Pyongyang -> location.location.events -> 1992 Pyongyang Film Festival\n# Answer:\nPyongyang", "# Reasoning Path:\nKim Jong-il -> people.person.education -> m.0114tdkm -> education.education.institution -> Mangyongdae Revolutionary School\n# Answer:\nMangyongdae Revolutionary School", "# Reasoning Path:\nKim Jong-il -> book.author.works_written -> Abuses of socialism are intolerable -> common.topic.notable_types -> Written Work\n# Answer:\nAbuses of socialism are intolerable", "# Reasoning Path:\nKim Jong-il -> book.author.works_written -> Abuses of socialism are intolerable -> common.topic.notable_for -> g.1yl5kr5fw\n# Answer:\nAbuses of socialism are intolerable"], "ground_truth": ["Pyongyang"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Tyrone\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> base.aareas.schema.administrative_area.administrative_children -> East Midlands\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Scotland -> location.country.first_level_divisions -> Aberdeen\n# Answer:\nScotland", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nChild pornography", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> base.aareas.schema.administrative_area.administrative_children -> East of England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> location.country.first_level_divisions -> East Midlands\n# Answer:\nEngland"], "ground_truth": ["England", "Northern Ireland", "Scotland", "Wales"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.6, "path_precision": 0.5, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-160", "prediction": ["# Reasoning Path:\nRudolf Virchow -> people.person.employment_history -> m.04hx7xj -> business.employment_tenure.company -> University of W\u00fcrzburg\n# Answer:\nUniversity of W\u00fcrzburg", "# Reasoning Path:\nRudolf Virchow -> people.person.employment_history -> m.04hx7xp -> business.employment_tenure.company -> Humboldt University of Berlin\n# Answer:\nHumboldt University of Berlin", "# Reasoning Path:\nRudolf Virchow -> base.kwebbase.kwtopic.connections_to -> carl weigert assistant to rudolf carl virchow -> base.kwebbase.kwconnection.sentence -> At the University of Berlin for a time he worked as personal assistant to the pathologist Virchow.\n# Answer:\ncarl weigert assistant to rudolf carl virchow", "# Reasoning Path:\nRudolf Virchow -> education.academic.advisees -> Edwin Klebs -> people.person.employment_history -> m.04hqvc_\n# Answer:\nEdwin Klebs", "# Reasoning Path:\nRudolf Virchow -> base.kwebbase.kwtopic.connections_to -> carl weigert assistant to rudolf carl virchow -> base.kwebbase.kwconnection.subject -> Karl Weigert\n# Answer:\ncarl weigert assistant to rudolf carl virchow", "# Reasoning Path:\nRudolf Virchow -> base.kwebbase.kwtopic.connections_to -> carl weigert assistant to rudolf carl virchow -> base.kwebbase.kwconnection.relation -> assistant to\n# Answer:\ncarl weigert assistant to rudolf carl virchow", "# Reasoning Path:\nRudolf Virchow -> base.kwebbase.kwtopic.connections_to -> ernst haeckel taught by rudolf carl virchow -> base.kwebbase.kwconnection.subject -> Ernst Haeckel\n# Answer:\nernst haeckel taught by rudolf carl virchow", "# Reasoning Path:\nRudolf Virchow -> education.academic.advisees -> Oscar Liebreich -> education.academic.advisors -> Carl Remigius Fresenius\n# Answer:\nOscar Liebreich", "# Reasoning Path:\nRudolf Virchow -> base.kwebbase.kwtopic.connections_to -> cesare lombroso read work of rudolf carl virchow -> base.kwebbase.kwconnection.relation -> read work of\n# Answer:\ncesare lombroso read work of rudolf carl virchow", "# Reasoning Path:\nRudolf Virchow -> education.academic.advisees -> Oscar Liebreich -> common.topic.notable_types -> Academic\n# Answer:\nOscar Liebreich"], "ground_truth": ["University of W\u00fcrzburg", "Humboldt University of Berlin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1603", "prediction": ["# Reasoning Path:\nTaoism -> religion.religion.texts -> Tao Te Ching -> book.written_work.subjects -> Religion\n# Answer:\nTao Te Ching", "# Reasoning Path:\nTaoism -> religion.religion.texts -> Tao Te Ching -> book.written_work.subjects -> Music\n# Answer:\nTao Te Ching", "# Reasoning Path:\nTaoism -> religion.religion.texts -> Tao Te Ching -> book.book.editions -> Tao te ching\n# Answer:\nTao Te Ching", "# Reasoning Path:\nTaoism -> religion.religion.texts -> Tao Te Ching -> book.written_work.subjects -> New Age\n# Answer:\nTao Te Ching", "# Reasoning Path:\nTaoism -> book.book_subject.works -> Tao Te Ching -> book.written_work.subjects -> Religion\n# Answer:\nTao Te Ching", "# Reasoning Path:\nTaoism -> religion.religion.texts -> Tao Te Ching -> media_common.quotation_source.quotations -> A violent wind does not last for a whole morning...\n# Answer:\nTao Te Ching", "# Reasoning Path:\nTaoism -> religion.religion.texts -> Zhuang Zhou -> book.author.works_written -> Zhuangzi\n# Answer:\nZhuang Zhou", "# Reasoning Path:\nTaoism -> religion.religion.texts -> Zhuang Zhou -> influence.influence_node.influenced_by -> Confucius\n# Answer:\nZhuang Zhou", "# Reasoning Path:\nTaoism -> religion.religion.texts -> Daozang -> common.topic.notable_types -> Religious Text\n# Answer:\nDaozang", "# Reasoning Path:\nTaoism -> religion.religion.texts -> Tao Te Ching -> book.book.editions -> Dao de jing\n# Answer:\nTao Te Ching"], "ground_truth": ["Tao Te Ching", "Zhuang Zhou", "I Ching", "Daozang"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.28571428571428575, "path_precision": 0.2, "path_recall": 0.5, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-1604", "prediction": ["# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Kingdom of Great Britain -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nKingdom of Great Britain", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> United States of America -> location.country.languages_spoken -> Spanish Language\n# Answer:\nUnited States of America", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Kingdom of Great Britain -> location.location.containedby -> British Isles\n# Answer:\nKingdom of Great Britain", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.location.containedby -> North America\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Kingdom of Great Britain -> location.country.form_of_government -> Parliamentary system\n# Answer:\nKingdom of Great Britain", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Kingdom of Great Britain -> location.country.capital -> London\n# Answer:\nKingdom of Great Britain", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> New Zealand -> location.country.languages_spoken -> Standard Chinese\n# Answer:\nNew Zealand", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Australia -> sports.sport_country.multi_event_tournaments_participated_in -> 2010 Commonwealth Games\n# Answer:\nAustralia", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Papua New Guinea -> location.country.languages_spoken -> Tok Pisin Language\n# Answer:\nPapua New Guinea"], "ground_truth": ["Kingdom of Great Britain", "Canada", "United States of America", "Australia", "Republic of Ireland", "New Zealand", "United Kingdom"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.6292134831460674, "ans_precission": 0.7, "ans_recall": 0.5714285714285714, "path_f1": 0.5, "path_precision": 0.6, "path_recall": 0.42857142857142855, "path_ans_f1": 0.6292134831460674, "path_ans_precision": 0.7, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-1607", "prediction": ["# Reasoning Path:\nFedEx Cup -> sports.sports_award_type.winners -> m.0n1v8cy -> sports.sports_award.season -> 2012 PGA Tour\n# Answer:\n2012 PGA Tour", "# Reasoning Path:\nFedEx Cup -> common.topic.article -> m.08q5wy\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nFedEx Cup -> sports.sports_award_type.winners -> m.0n1v8cy -> sports.sports_award.award_winner -> Brandt Snedeker\n# Answer:\nBrandt Snedeker", "# Reasoning Path:\nFedEx Cup -> common.topic.notable_for -> g.12559n8g_\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["Brandt Snedeker"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1608", "prediction": ["# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Modern Standard Arabic -> base.rosetta.languoid.parent -> Arabic Group\n# Answer:\nModern Standard Arabic", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Modern Standard Arabic -> language.human_language.language_family -> Afroasiatic languages\n# Answer:\nModern Standard Arabic", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.writing_system -> Arabic alphabet\n# Answer:\nArabic Language", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Siwi Language -> language.human_language.language_family -> Afroasiatic languages\n# Answer:\nSiwi Language", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3f_h6\n# Answer:\nlocation.statistical_region.size_of_armed_forces"], "ground_truth": ["Sa'idi Arabic", "Bedawi Arabic", "Egyptian Arabic", "Siwi Language"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.14285714285714288, "ans_precission": 0.1, "ans_recall": 0.25, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.14285714285714288, "path_ans_precision": 0.1, "path_ans_recall": 0.25}
{"id": "WebQTest-1609", "prediction": ["# Reasoning Path:\nALF -> tv.tv_program.program_creator -> Tom Patchett -> tv.tv_program_creator.programs_created -> Working Girl\n# Answer:\nTom Patchett", "# Reasoning Path:\nALF -> tv.tv_program.program_creator -> Tom Patchett -> tv.tv_program_creator.programs_created -> The Tony Randall Show\n# Answer:\nTom Patchett", "# Reasoning Path:\nALF -> tv.tv_program.program_creator -> Tom Patchett -> common.topic.notable_for -> g.1259mrnmt\n# Answer:\nTom Patchett", "# Reasoning Path:\nALF -> tv.tv_program.program_creator -> Tom Patchett -> tv.tv_program_creator.programs_created -> Buffalo Bill\n# Answer:\nTom Patchett", "# Reasoning Path:\nALF -> tv.tv_program.program_creator -> Tom Patchett -> tv.tv_producer.programs_produced -> m.0n5dfsq\n# Answer:\nTom Patchett", "# Reasoning Path:\nALF -> award.award_nominated_work.award_nominations -> m.0z8z081 -> award.award_nomination.award_nominee -> Paul Fusco\n# Answer:\nPaul Fusco", "# Reasoning Path:\nALF -> tv.tv_program.program_creator -> Paul Fusco -> people.person.profession -> Voice Actor\n# Answer:\nPaul Fusco", "# Reasoning Path:\nALF -> media_common.netflix_title.netflix_genres -> Television -> film.film_subject.films -> 15 Minutes\n# Answer:\nTelevision", "# Reasoning Path:\nALF -> media_common.netflix_title.netflix_genres -> TV Sitcoms\n# Answer:\nTV Sitcoms", "# Reasoning Path:\nALF -> tv.tv_program.program_creator -> Tom Patchett -> tv.tv_producer.programs_produced -> m.0n73swq\n# Answer:\nTom Patchett"], "ground_truth": ["Paul Fusco"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-161", "prediction": ["# Reasoning Path:\nGordon Brown -> people.person.education -> m.02wp5xk -> education.education.institution -> University of Edinburgh\n# Answer:\nUniversity of Edinburgh", "# Reasoning Path:\nGordon Brown -> people.person.education -> m.0k1qt_h -> education.education.institution -> Kirkcaldy High School\n# Answer:\nKirkcaldy High School", "# Reasoning Path:\nGordon Brown -> people.person.education -> m.0k1qvbm -> education.education.institution -> Kirkcaldy West Primary School\n# Answer:\nKirkcaldy West Primary School", "# Reasoning Path:\nGordon Brown -> people.person.profession -> Teacher -> base.lightweight.profession.professions_similar -> Librarian\n# Answer:\nTeacher", "# Reasoning Path:\nGordon Brown -> people.person.education -> m.02wp5xk -> education.education.degree -> Master of Arts\n# Answer:\nMaster of Arts", "# Reasoning Path:\nGordon Brown -> common.topic.image -> Gordon Brown Davos Jan 08 -> common.image.appears_in_topic_gallery -> United Kingdom general election, 2010\n# Answer:\nGordon Brown Davos Jan 08", "# Reasoning Path:\nGordon Brown -> people.person.education -> m.02wp5xk -> education.education.major_field_of_study -> History\n# Answer:\nHistory", "# Reasoning Path:\nGordon Brown -> common.topic.image -> Gordon Brown, the Chancellor of the Exchequer who abolished ACT and introduced the quarterly instalment r\u00e9gime in 1999 -> common.image.appears_in_topic_gallery -> United Kingdom corporation tax\n# Answer:\nGordon Brown, the Chancellor of the Exchequer who abolished ACT and introduced the quarterly instalment r\u00e9gime in 1999", "# Reasoning Path:\nGordon Brown -> common.topic.image -> Gordon Brown Davos Jan 08 -> common.image.appears_in_topic_gallery -> Brown ministry\n# Answer:\nGordon Brown Davos Jan 08", "# Reasoning Path:\nGordon Brown -> common.topic.image -> Gordon Brown Davos Jan 08 -> common.image.size -> m.04y43w2\n# Answer:\nGordon Brown Davos Jan 08"], "ground_truth": ["University of Edinburgh"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1610", "prediction": ["# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> common.topic.notable_types -> US Vice President\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> people.person.sibling_s -> m.05hlvsz\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> common.topic.notable_types -> Film character\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> people.person.sibling_s -> m.05px29t\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> people.person.quotations -> A strong defense is the surest way to peace. Strength makes detente attainable. Weakness invites war, as my generationmy generationknows from four very bitter experiences. Just as Americas will for peace is second to none, so will Americas strength be second to none. We cannot rely on the forbearance of others to protect this Nation. The power and diversity of the Armed Forces, active Guard and Reserve, the resolve of our fellow citizens, the flexibility in our command to navigate international waters that remain troubled are all essential to our security. -> media_common.quotation.subjects -> Uncategorised\n# Answer:\nA strong defense is the surest way to peace. Strength makes detente attainable. Weakness invites war, as my generationmy generationknows from four very bitter experiences. Just as Americas will for peace is second to none, so will Americas strength be second to none. We cannot rely on the forbearance of others to protect this Nation. The power and diversity of the Armed Forces, active Guard and Reserve, the resolve of our fellow citizens, the flexibility in our command to navigate international waters that remain troubled are all essential to our security.", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> people.person.sibling_s -> m.05px29z\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.political_appointer.appointees -> m.01xrx8h -> government.government_position_held.office_holder -> George H. W. Bush\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nGerald Ford -> government.political_appointer.appointees -> m.01z0qwq -> government.government_position_held.office_holder -> Dick Cheney\n# Answer:\nDick Cheney", "# Reasoning Path:\nGerald Ford -> people.person.quotations -> All of us who served in one war or another know very well that all wars are the glory and the agony of the young. -> common.topic.notable_types -> Quotation\n# Answer:\nAll of us who served in one war or another know very well that all wars are the glory and the agony of the young.", "# Reasoning Path:\nGerald Ford -> government.political_appointer.appointees -> m.04cnx_5 -> government.government_position_held.office_holder -> Donald Rumsfeld\n# Answer:\nDonald Rumsfeld"], "ground_truth": ["Nelson Rockefeller"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1611", "prediction": ["# Reasoning Path:\nArmenia -> location.country.currency_used -> Armenian dram -> finance.currency.countries_used -> Nagorno-Karabakh Republic\n# Answer:\nArmenian dram", "# Reasoning Path:\nArmenia -> location.country.currency_used -> Armenian dram -> common.topic.webpage -> m.04m1q3h\n# Answer:\nArmenian dram", "# Reasoning Path:\nArmenia -> location.country.currency_used -> Armenian dram -> common.topic.notable_for -> g.125gnq5zv\n# Answer:\nArmenian dram", "# Reasoning Path:\nArmenia -> location.statistical_region.energy_use_per_capita -> g.1245_391m\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nArmenia -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60znv7_\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nArmenia -> location.statistical_region.energy_use_per_capita -> g.1245_f6xm\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nArmenia -> location.statistical_region.energy_use_per_capita -> g.1245_jsph\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nArmenia -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fsr3\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nArmenia -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc37pgh\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp"], "ground_truth": ["Armenian dram"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-1612", "prediction": ["# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Drawing -> visual_art.art_subject.artwork_on_the_subject -> Ukrayina (Video art)\n# Answer:\nDrawing", "# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Drawing -> interests.hobby.people_with_this_hobby -> Mukil Elango\n# Answer:\nDrawing", "# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Drawing -> education.field_of_study.students_majoring -> m.0103397j\n# Answer:\nDrawing", "# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Painting -> business.product_theme.products -> Andy Warhol Registry\n# Answer:\nPainting", "# Reasoning Path:\nRaphael -> common.topic.article -> m.02mx4l\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Drawing -> education.field_of_study.students_majoring -> m.010bfdc7\n# Answer:\nDrawing", "# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Painting -> business.product_theme.products -> Banksy Registry\n# Answer:\nPainting", "# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Painting -> base.descriptive_names.names.descriptive_name -> m.0109p390\n# Answer:\nPainting", "# Reasoning Path:\nRaphael -> base.kwebbase.kwtopic.has_sentences -> Although neither fresco was ever finished, Leonardo's \\\"Battle of Anghiari\\\" and Michelangelo's \\\"Battle of Cascina\\\" had a great influence on the many students, including Raphael, who went to view them and make copies. -> base.kwebbase.kwsentence.previous_sentence -> His own artistic activity during the period 1505 - 1507 showed the influence of Leonardo and Michelangelo, who at that time were striving to outdo each other in the decoration of the Sala del Consiglio in the Palazzo dell Signoria.\n# Answer:\nAlthough neither fresco was ever finished, Leonardo's \\\"Battle of Anghiari\\\" and Michelangelo's \\\"Battle of Cascina\\\" had a great influence on the many students, including Raphael, who went to view them and make copies.", "# Reasoning Path:\nRaphael -> common.topic.article -> m.0c43s\n# Answer:\ncommon.topic.article"], "ground_truth": ["Painting", "Drawing"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1613", "prediction": ["# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Botswana\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> Maltese Language -> language.human_language.region -> Europe\n# Answer:\nMaltese Language", "# Reasoning Path:\nMalta -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.statistical_region.deposit_interest_rate -> g.1hhc37h6m\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nMalta -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Botswana\n# Answer:\nEnglish Language"], "ground_truth": ["English Language", "Maltese Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1614", "prediction": ["# Reasoning Path:\nLouis XIV of France -> people.person.religion -> Catholicism -> religion.religion.types_of_places_of_worship -> Church\n# Answer:\nCatholicism", "# Reasoning Path:\nLouis XIV of France -> people.person.religion -> Catholicism -> religion.religion.types_of_places_of_worship -> Basilica\n# Answer:\nCatholicism", "# Reasoning Path:\nLouis XIV of France -> people.person.religion -> Catholicism -> religion.religion.types_of_places_of_worship -> Cathedral\n# Answer:\nCatholicism", "# Reasoning Path:\nLouis XIV of France -> base.kwebbase.kwtopic.has_sentences -> A change of ministry in England in 1710 fortunately displaced the party in favour of continuing the war, and the new government, pledged to peace, signed the Treaty of Utrecht with France in April 1713. -> base.kwebbase.kwsentence.next_sentence -> Louis XIV died at Versailles two years later.\n# Answer:\nA change of ministry in England in 1710 fortunately displaced the party in favour of continuing the war, and the new government, pledged to peace, signed the Treaty of Utrecht with France in April 1713.", "# Reasoning Path:\nLouis XIV of France -> organization.organization_founder.organizations_founded -> Royal Academy of Dance -> education.educational_institution.students_graduates -> m.0h5hhg1\n# Answer:\nRoyal Academy of Dance", "# Reasoning Path:\nLouis XIV of France -> organization.organization_founder.organizations_founded -> Royal Academy of Dance -> common.topic.image -> Rad-logo\n# Answer:\nRoyal Academy of Dance", "# Reasoning Path:\nLouis XIV of France -> base.kwebbase.kwtopic.has_sentences -> Always keen on expansion, after the death of his father-in-law in 1665, Louis made claim to part of the Spanish Netherlands and launched the War of Dutch Devolution (1667-68). -> base.kwebbase.kwsentence.next_sentence -> This provoked an alliance between Britain, Holland and Sweden and ended with the  Treaty of Aix-La-Chapelle.\n# Answer:\nAlways keen on expansion, after the death of his father-in-law in 1665, Louis made claim to part of the Spanish Netherlands and launched the War of Dutch Devolution (1667-68).", "# Reasoning Path:\nLouis XIV of France -> organization.organization_founder.organizations_founded -> Royal Academy of Dance -> common.topic.webpage -> m.06wgxyp\n# Answer:\nRoyal Academy of Dance", "# Reasoning Path:\nLouis XIV of France -> base.kwebbase.kwtopic.has_sentences -> A change of ministry in England in 1710 fortunately displaced the party in favour of continuing the war, and the new government, pledged to peace, signed the Treaty of Utrecht with France in April 1713. -> base.kwebbase.kwsentence.previous_sentence -> France was now on its knees, almost bankrupt and deep in depression.\n# Answer:\nA change of ministry in England in 1710 fortunately displaced the party in favour of continuing the war, and the new government, pledged to peace, signed the Treaty of Utrecht with France in April 1713.", "# Reasoning Path:\nLouis XIV of France -> base.kwebbase.kwtopic.has_sentences -> As a result of these artists' endeavours, Louis's reign became culturally the most brilliant in French history. -> base.kwebbase.kwsentence.next_sentence -> In 1672, allied with his cousin Charles II of England in resentment at the commercial success of the Dutch, Louis' armies of about 120,000 men invaded Dutch territory, his unprovoked assault only halted when William of Orange (later William III of Britain) opened the sluices and inundated the country.\n# Answer:\nAs a result of these artists' endeavours, Louis's reign became culturally the most brilliant in French history."], "ground_truth": ["Catholicism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1615", "prediction": ["# Reasoning Path:\nMike Wright -> sports.pro_athlete.teams -> m.0j3qfq_ -> sports.sports_team_roster.team -> New England Patriots\n# Answer:\nNew England Patriots", "# Reasoning Path:\nMike Wright -> sports.pro_athlete.teams -> m.0j3qfq_ -> sports.sports_team_roster.position -> Defensive end\n# Answer:\nDefensive end", "# Reasoning Path:\nMike Wright -> sports.pro_athlete.teams -> m.0j3qfq_ -> freebase.valuenotation.has_value -> To\n# Answer:\nTo", "# Reasoning Path:\nMike Wright -> american_football.football_player.games -> m.07nvh58 -> american_football.player_game_statistics.team -> New England Patriots\n# Answer:\nNew England Patriots", "# Reasoning Path:\nMike Wright -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Kansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nMike Wright -> american_football.football_player.games -> m.07sgss2 -> american_football.player_game_statistics.team -> New England Patriots\n# Answer:\nNew England Patriots", "# Reasoning Path:\nMike Wright -> american_football.football_player.games -> m.07t7ftb -> american_football.player_game_statistics.team -> New England Patriots\n# Answer:\nNew England Patriots", "# Reasoning Path:\nMike Wright -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Nebraska\n# Answer:\nUnited States of America", "# Reasoning Path:\nMike Wright -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Nebraska\n# Answer:\nUnited States of America", "# Reasoning Path:\nMike Wright -> people.person.nationality -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Kansas\n# Answer:\nUnited States of America"], "ground_truth": ["New England Patriots"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.2857142857142857, "path_precision": 0.4, "path_recall": 0.2222222222222222, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1616", "prediction": ["# Reasoning Path:\nShannon Hoon -> people.deceased_person.cause_of_death -> Drug overdose -> people.cause_of_death.includes_causes_of_death -> Cocaine overdose\n# Answer:\nDrug overdose", "# Reasoning Path:\nShannon Hoon -> people.deceased_person.cause_of_death -> Drug overdose -> people.cause_of_death.includes_causes_of_death -> Heroin overdose\n# Answer:\nDrug overdose", "# Reasoning Path:\nShannon Hoon -> people.deceased_person.cause_of_death -> Drug overdose -> people.cause_of_death.parent_cause_of_death -> Assassination by 'Suicide'\n# Answer:\nDrug overdose", "# Reasoning Path:\nShannon Hoon -> people.deceased_person.cause_of_death -> Drug overdose -> fictional_universe.medical_condition_in_fiction.characters_with_this_condition -> Leo Taylor\n# Answer:\nDrug overdose", "# Reasoning Path:\nShannon Hoon -> people.deceased_person.cause_of_death -> Drug overdose -> people.cause_of_death.parent_cause_of_death -> Assassination by 'Accident'\n# Answer:\nDrug overdose", "# Reasoning Path:\nShannon Hoon -> common.topic.notable_types -> Composer -> people.profession.specialization_of -> Musician\n# Answer:\nComposer", "# Reasoning Path:\nShannon Hoon -> people.deceased_person.cause_of_death -> Drug overdose -> fictional_universe.medical_condition_in_fiction.characters_with_this_condition -> Natasha Andersen\n# Answer:\nDrug overdose", "# Reasoning Path:\nShannon Hoon -> common.topic.notable_types -> Composer -> type.type.expected_by -> Music\n# Answer:\nComposer", "# Reasoning Path:\nShannon Hoon -> music.artist.track_contributions -> m.012d6m_t -> music.track_contribution.track -> Lemonade\n# Answer:\nLemonade", "# Reasoning Path:\nShannon Hoon -> music.artist.track_contributions -> m.012d6m_t -> music.track_contribution.role -> Vocals\n# Answer:\nVocals"], "ground_truth": ["Drug overdose"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1617", "prediction": ["# Reasoning Path:\nNicolaus Copernicus -> base.argumentmaps.innovator.original_ideas -> Heliocentrism -> base.schemastaging.context_name.pronunciation -> g.125_rtnb2\n# Answer:\nHeliocentrism", "# Reasoning Path:\nNicolaus Copernicus -> symbols.name_source.namesakes -> Copernicus -> common.topic.notable_types -> Extraterrestrial location\n# Answer:\nCopernicus", "# Reasoning Path:\nNicolaus Copernicus -> base.argumentmaps.innovator.original_ideas -> Heliocentrism -> common.topic.image -> Heliocentric\n# Answer:\nHeliocentrism", "# Reasoning Path:\nNicolaus Copernicus -> symbols.name_source.namesakes -> Copernicus -> astronomy.extraterrestrial_location.on_celestial_object -> Moon\n# Answer:\nCopernicus", "# Reasoning Path:\nNicolaus Copernicus -> symbols.name_source.namesakes -> Wroc\u0142aw\u2013Copernicus Airport -> common.topic.notable_types -> Airport\n# Answer:\nWroc\u0142aw\u2013Copernicus Airport", "# Reasoning Path:\nNicolaus Copernicus -> symbols.name_source.namesakes -> Copernicus -> common.topic.article -> m.01z1rs\n# Answer:\nCopernicus", "# Reasoning Path:\nNicolaus Copernicus -> base.argumentmaps.innovator.original_ideas -> Copernican Revolution -> common.topic.article -> m.02p6y31\n# Answer:\nCopernican Revolution", "# Reasoning Path:\nNicolaus Copernicus -> base.argumentmaps.innovator.original_ideas -> Copernican heliocentrism -> base.argumentmaps.explanation.explained_thing -> Day\n# Answer:\nCopernican heliocentrism", "# Reasoning Path:\nNicolaus Copernicus -> symbols.name_source.namesakes -> Wroc\u0142aw\u2013Copernicus Airport -> location.location.containedby -> Poland\n# Answer:\nWroc\u0142aw\u2013Copernicus Airport", "# Reasoning Path:\nNicolaus Copernicus -> symbols.name_source.namesakes -> Wroc\u0142aw\u2013Copernicus Airport -> aviation.airport.focus_city_for -> Air Polonia\n# Answer:\nWroc\u0142aw\u2013Copernicus Airport"], "ground_truth": ["Copernican heliocentrism", "Copernican Revolution", "Heliocentrism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.2, "path_recall": 0.3333333333333333, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1618", "prediction": ["# Reasoning Path:\nManny Ramirez -> sports.pro_athlete.teams -> m.010p78y8 -> sports.sports_team_roster.team -> Chicago Cubs\n# Answer:\nChicago Cubs", "# Reasoning Path:\nManny Ramirez -> sports.pro_athlete.teams -> m.0w483f0 -> sports.sports_team_roster.team -> Chicago White Sox\n# Answer:\nChicago White Sox", "# Reasoning Path:\nManny Ramirez -> sports.pro_athlete.teams -> m.010p78y8 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nManny Ramirez -> sports.pro_athlete.teams -> m.04fcvk4 -> sports.sports_team_roster.team -> Cleveland Indians\n# Answer:\nCleveland Indians", "# Reasoning Path:\nManny Ramirez -> baseball.baseball_player.batting_stats -> m.06s7mwn -> baseball.batting_statistics.team -> Boston Red Sox\n# Answer:\nBoston Red Sox", "# Reasoning Path:\nManny Ramirez -> sports.pro_athlete.teams -> m.010p78y8 -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nManny Ramirez -> baseball.baseball_player.batting_stats -> m.06s7m7d -> baseball.batting_statistics.team -> Cleveland Indians\n# Answer:\nCleveland Indians", "# Reasoning Path:\nManny Ramirez -> baseball.baseball_player.batting_stats -> m.06s7m94 -> baseball.batting_statistics.team -> Cleveland Indians\n# Answer:\nCleveland Indians", "# Reasoning Path:\nManny Ramirez -> sports.pro_athlete.teams -> m.010p78y8 -> freebase.valuenotation.has_value -> Position\n# Answer:\nPosition", "# Reasoning Path:\nManny Ramirez -> baseball.baseball_player.batting_stats -> m.06s7mwn -> baseball.batting_statistics.season -> 2006 Major League Baseball season\n# Answer:\n2006 Major League Baseball season"], "ground_truth": ["Cleveland Indians", "Chicago Cubs", "Texas Rangers", "Los Angeles Dodgers", "EDA Rhinos", "Oakland Athletics", "Chicago White Sox", "Tampa Bay Rays", "Boston Red Sox"], "ans_acc": 0.4444444444444444, "ans_hit": 1, "ans_f1": 0.5106382978723405, "ans_precission": 0.6, "ans_recall": 0.4444444444444444, "path_f1": 0.3076923076923077, "path_precision": 0.6, "path_recall": 0.20689655172413793, "path_ans_f1": 0.5106382978723405, "path_ans_precision": 0.6, "path_ans_recall": 0.4444444444444444}
{"id": "WebQTest-1619", "prediction": ["# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> location.location.containedby -> United States, with Territories\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> location.location.partially_contains -> Wild River\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> Qu\u00e9bec -> base.aareas.schema.administrative_area.administrative_parent -> Canada\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> location.location.partially_contains -> Androscoggin River\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> Qu\u00e9bec -> base.aareas.schema.administrative_area.capital -> Quebec City\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nAppalachian Mountains -> location.location.containedby -> North America -> location.location.time_zones -> Saint Pierre and Miquelon\u00a0Time Zone\n# Answer:\nNorth America", "# Reasoning Path:\nAppalachian Mountains -> location.location.containedby -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> location.location.containedby -> United States of America\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> location.location.partially_contains -> Connecticut River\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> symbols.name_source.namesakes -> USS New Hampshire\n# Answer:\nNew Hampshire"], "ground_truth": ["North America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-162", "prediction": ["# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Albania\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Bulgaria\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n1896 Summer Olympics", "# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.sports -> Ice Hockey\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 1900 Summer Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n1900 Summer Olympics", "# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nGreece -> location.location.partially_contains -> Baba Mountain, Macedonia -> location.location.partially_containedby -> Republic of Macedonia\n# Answer:\nBaba Mountain, Macedonia", "# Reasoning Path:\nGreece -> location.location.partially_contains -> Ao\u00f6s -> geography.river.basin_countries -> Albania\n# Answer:\nAo\u00f6s", "# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> freebase.valuenotation.is_reviewed -> Location(s)\n# Answer:\n2014 Winter Olympics"], "ground_truth": ["Albania", "Turkey", "Bulgaria", "Republic of Macedonia"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.0689655172413793, "path_precision": 0.3, "path_recall": 0.03896103896103896, "path_ans_f1": 0.5217391304347827, "path_ans_precision": 0.4, "path_ans_recall": 0.75}
{"id": "WebQTest-1620", "prediction": ["# Reasoning Path:\nWoodrow Wilson -> book.written_work.subjects -> United States of America -> location.country.administrative_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nWoodrow Wilson -> book.written_work.subjects -> United States of America -> location.country.administrative_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nWoodrow Wilson -> book.written_work.subjects -> United States of America -> location.country.first_level_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nWoodrow Wilson -> book.written_work.subjects -> 20th century -> time.event.includes_event -> World War I\n# Answer:\n20th century", "# Reasoning Path:\nWoodrow Wilson -> event.public_speaker.speeches_or_presentations -> m.05rlb3g -> event.speech_or_presentation.speech_topic -> World War I\n# Answer:\nWorld War I", "# Reasoning Path:\nWoodrow Wilson -> book.written_work.subjects -> History of the United States\n# Answer:\nHistory of the United States", "# Reasoning Path:\nWoodrow Wilson -> book.written_work.subjects -> United States of America -> location.country.administrative_divisions -> New Jersey\n# Answer:\nUnited States of America", "# Reasoning Path:\nWoodrow Wilson -> common.topic.notable_for -> g.1255d7s1s\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nWoodrow Wilson -> event.public_speaker.speeches_or_presentations -> m.05rlb3g -> event.speech_or_presentation.presented_work -> Fourteen Points\n# Answer:\nFourteen Points", "# Reasoning Path:\nWoodrow Wilson -> book.written_work.subjects -> United States of America -> location.country.first_level_divisions -> Alabama\n# Answer:\nUnited States of America"], "ground_truth": ["1913-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1621", "prediction": ["# Reasoning Path:\nJerry Brown -> government.politician.election_campaigns -> Jerry Brown for Governor, 2010 -> government.election_campaign.party -> Democratic Party\n# Answer:\nJerry Brown for Governor, 2010", "# Reasoning Path:\nJerry Brown -> government.politician.election_campaigns -> Jerry Brown for Governor, 2010 -> common.topic.notable_for -> g.1255xf_wq\n# Answer:\nJerry Brown for Governor, 2010", "# Reasoning Path:\nJerry Brown -> government.politician.party -> m.040ptrq -> government.political_party_tenure.party -> Democratic Party\n# Answer:\nDemocratic Party", "# Reasoning Path:\nJerry Brown -> common.image.appears_in_topic_gallery -> California Attorney General election, 2006\n# Answer:\nCalifornia Attorney General election, 2006", "# Reasoning Path:\nJerry Brown -> government.politician.party -> m.0lr11c0 -> government.political_party_tenure.party -> California Democratic Party\n# Answer:\nCalifornia Democratic Party", "# Reasoning Path:\nJerry Brown -> common.image.appears_in_topic_gallery -> California gubernatorial election, 1974\n# Answer:\nCalifornia gubernatorial election, 1974", "# Reasoning Path:\nJerry Brown -> common.image.appears_in_topic_gallery -> California gubernatorial election, 1978\n# Answer:\nCalifornia gubernatorial election, 1978"], "ground_truth": ["Democratic Party", "California Democratic Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.2857142857142857, "ans_recall": 1.0, "path_f1": 0.6, "path_precision": 0.42857142857142855, "path_recall": 1.0, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-1622", "prediction": ["# Reasoning Path:\nHawaii -> location.location.containedby -> Hawaii County -> location.location.containedby -> Hawaiian Islands\n# Answer:\nHawaii County", "# Reasoning Path:\nHawaii -> location.location.containedby -> Pacific Ocean -> location.location.contains -> Baker Island\n# Answer:\nPacific Ocean", "# Reasoning Path:\nHawaii -> location.location.containedby -> Pacific Ocean -> location.location.partially_contains -> United States, with Territories\n# Answer:\nPacific Ocean", "# Reasoning Path:\nHawaii -> location.location.containedby -> Pacific Ocean -> film.film_location.featured_in_films -> Mutiny on the Bounty\n# Answer:\nPacific Ocean", "# Reasoning Path:\nHawaii -> location.location.containedby -> Pacific Ocean -> location.location.contains -> French Polynesia\n# Answer:\nPacific Ocean"], "ground_truth": ["Pacific Ocean"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1623", "prediction": ["# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Yiddish Language -> common.topic.notable_types -> Human Language\n# Answer:\nYiddish Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Yiddish Language -> language.human_language.main_country -> United States of America\n# Answer:\nYiddish Language", "# Reasoning Path:\nArgentina -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Guaran\u00ed language -> common.topic.notable_types -> Human Language\n# Answer:\nGuaran\u00ed language", "# Reasoning Path:\nArgentina -> location.country.official_language -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Bolivia\n# Answer:\nSpanish Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Chile\n# Answer:\nSpanish Language", "# Reasoning Path:\nArgentina -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> Bolivia\n# Answer:\nSpanish Language"], "ground_truth": ["Spanish Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1624", "prediction": ["# Reasoning Path:\nTunisia -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Libya\n# Answer:\nArabic Language", "# Reasoning Path:\nTunisia -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Mauritania\n# Answer:\nArabic Language", "# Reasoning Path:\nTunisia -> location.country.official_language -> Arabic Language -> language.human_language.countries_spoken_in -> Mauritania\n# Answer:\nArabic Language", "# Reasoning Path:\nTunisia -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Algeria\n# Answer:\nArabic Language", "# Reasoning Path:\nTunisia -> location.country.official_language -> Arabic Language -> language.human_language.countries_spoken_in -> Egypt\n# Answer:\nArabic Language", "# Reasoning Path:\nTunisia -> location.statistical_region.gdp_nominal_per_capita -> g.11b60srfh8\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nTunisia -> location.country.languages_spoken -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nTunisia -> location.country.official_language -> Arabic Language -> language.human_language.countries_spoken_in -> Algeria\n# Answer:\nArabic Language", "# Reasoning Path:\nTunisia -> location.country.languages_spoken -> French -> language.human_language.countries_spoken_in -> Algeria\n# Answer:\nFrench", "# Reasoning Path:\nTunisia -> location.country.languages_spoken -> French -> language.human_language.main_country -> France\n# Answer:\nFrench"], "ground_truth": ["Arabic Language", "French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1625", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> base.argumentmaps.innovator.original_ideas -> Viola organista -> common.topic.notable_for -> g.1254z0xjb\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> base.argumentmaps.innovator.original_ideas -> Viola organista -> music.performance_role.track_performances -> m.0112zmz9\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> base.argumentmaps.innovator.original_ideas -> Viola organista -> common.topic.notable_types -> Musical instrument\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Viola organista -> common.topic.notable_for -> g.1254z0xjb\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> base.argumentmaps.innovator.original_ideas -> Viola organista -> music.performance_role.track_performances -> m.0nk45qs\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> base.argumentmaps.innovator.original_ideas -> Viola organista -> music.performance_role.track_performances -> m.0rtvlvk\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Viola organista -> music.performance_role.track_performances -> m.0112zmz9\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Armored car -> common.topic.article -> m.0y8z\n# Answer:\nArmored car", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Armored car -> base.ontologies.ontology_instance.equivalent_instances -> m.07ndlr9\n# Answer:\nArmored car", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Viola organista -> music.performance_role.track_performances -> m.0nk45qs\n# Answer:\nViola organista"], "ground_truth": ["Viola organista", "Double hull", "Diving suit", "Parachute", "Triple Barrel Canon", "Ornithopter", "Anemometer", "Aerial Screw", "33-Barreled Organ", "Armored car"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 1.0, "ans_recall": 0.2, "path_f1": 0.3111111111111111, "path_precision": 0.7, "path_recall": 0.2, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 1.0, "path_ans_recall": 0.2}
{"id": "WebQTest-1626", "prediction": ["# Reasoning Path:\nSt Trinian's School -> common.topic.notable_for -> g.12596535g\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nSt Trinian's School -> common.topic.notable_types -> Fictional Universe -> type.type.expected_by -> Fictional universe\n# Answer:\nFictional Universe", "# Reasoning Path:\nSt Trinian's School -> common.topic.notable_types -> Fictional Universe -> type.type.properties -> Created by\n# Answer:\nFictional Universe", "# Reasoning Path:\nSt Trinian's School -> common.topic.notable_types -> Fictional Universe -> freebase.type_hints.included_types -> Topic\n# Answer:\nFictional Universe", "# Reasoning Path:\nSt Trinian's School -> fictional_universe.fictional_universe.created_by -> Ronald Searle -> people.person.profession -> Cartoonist\n# Answer:\nRonald Searle", "# Reasoning Path:\nSt Trinian's School -> common.topic.notable_types -> Fictional Universe -> type.type.expected_by -> Appears In These Fictional Universes\n# Answer:\nFictional Universe", "# Reasoning Path:\nSt Trinian's School -> common.topic.notable_types -> Fictional Universe -> type.type.expected_by -> Appears in Fictional Universe\n# Answer:\nFictional Universe", "# Reasoning Path:\nSt Trinian's School -> common.topic.notable_types -> Fictional Universe -> type.type.properties -> Calendar systems\n# Answer:\nFictional Universe", "# Reasoning Path:\nSt Trinian's School -> fictional_universe.fictional_universe.created_by -> Ronald Searle -> influence.influence_node.influenced -> Harry Harrison\n# Answer:\nRonald Searle", "# Reasoning Path:\nSt Trinian's School -> common.topic.notable_types -> Fictional Universe -> type.type.properties -> Characters\n# Answer:\nFictional Universe"], "ground_truth": ["Ronald Searle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1627", "prediction": ["# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.webpage -> m.03lfx0d\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.notable_types -> Currency\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.notable_for -> g.1258v8707\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.statistical_region.size_of_armed_forces -> g.12cp_jm8t\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.webpage -> m.04m10rm\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.statistical_region.gdp_nominal_per_capita -> g.11b60thpy3\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nMalaysia -> location.statistical_region.size_of_armed_forces -> g.1hhc37x7v\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nMalaysia -> location.statistical_region.size_of_armed_forces -> g.1hhc39wxf\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nMalaysia -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6gh1z\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nMalaysia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc38hmv\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita"], "ground_truth": ["Malaysian ringgit"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1628", "prediction": ["# Reasoning Path:\nMexico -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nMexico -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> United States of America\n# Answer:\nPresidential system", "# Reasoning Path:\nMexico -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> United States of America\n# Answer:\nFederal republic", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_0s31\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Guatemala\n# Answer:\nConstitutional republic", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.11b60v4rn6\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_38p7\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guatemala\n# Answer:\nPresidential system", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_569j\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.12tb6hbx9\n# Answer:\nlocation.statistical_region.cpi_inflation_rate"], "ground_truth": ["Federal republic", "Presidential system", "Constitutional republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1629", "prediction": ["# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.article -> m.04lt88\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.webpage -> m.04m2gwk\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.statistical_region.co2_emissions_per_capita -> g.1245_1qfx\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nDominican Republic -> location.statistical_region.internet_users_percent_population -> g.11b60vv5zn\n# Answer:\nlocation.statistical_region.internet_users_percent_population", "# Reasoning Path:\nDominican Republic -> location.statistical_region.co2_emissions_per_capita -> g.1245_22j7\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nDominican Republic -> location.statistical_region.co2_emissions_per_capita -> g.1245_4lf3\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nDominican Republic -> location.statistical_region.internet_users_percent_population -> g.1245_22jn\n# Answer:\nlocation.statistical_region.internet_users_percent_population", "# Reasoning Path:\nDominican Republic -> location.statistical_region.internet_users_percent_population -> g.1245_m66j\n# Answer:\nlocation.statistical_region.internet_users_percent_population"], "ground_truth": ["Dominican peso"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-163", "prediction": ["# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> time.event.instance_of_recurring_event -> Super Bowl\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> 2006 NFC Championship Game -> common.topic.notable_for -> g.1z2spvm2w\n# Answer:\n2006 NFC Championship Game", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.season_record -> m.075fsd1 -> sports.sports_team_season_record.season -> 2008 NFL season\n# Answer:\n2008 NFL season", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> 2006 NFC Championship Game -> time.event.locations -> CenturyLink Field\n# Answer:\n2006 NFC Championship Game", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> 2014 NFC Championship Game -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n2014 NFC Championship Game", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> 2014 NFC Championship Game -> sports.sports_championship_event.season -> 2013 NFL season\n# Answer:\n2014 NFC Championship Game", "# Reasoning Path:\nSeattle Seahawks -> american_football.football_team.away_games -> Seattle Seahawks at Arizona Cardinals, 2008-12-28 -> american_football.football_game.season -> 2008 NFL season\n# Answer:\nSeattle Seahawks at Arizona Cardinals, 2008-12-28", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> 2014 NFC Championship Game -> freebase.valuenotation.is_reviewed -> Location(s)\n# Answer:\n2014 NFC Championship Game", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.season_record -> m.075fsdk -> sports.sports_team_season_record.season -> 2006 NFL season\n# Answer:\n2006 NFL season"], "ground_truth": ["Super Bowl XLVIII"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1630", "prediction": ["# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> James Brown -> influence.influence_node.influenced -> Lenny Henry\n# Answer:\nJames Brown", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> Charlie Chaplin -> influence.influence_node.influenced -> Redd Foxx\n# Answer:\nCharlie Chaplin", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> James Brown -> influence.influence_node.influenced -> Nipsey Russell\n# Answer:\nJames Brown", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> James Brown -> music.artist.genre -> Funk\n# Answer:\nJames Brown", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> Charlie Chaplin -> people.person.gender -> Male\n# Answer:\nCharlie Chaplin", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> James Brown -> broadcast.artist.content -> 1.FM Euro 80's\n# Answer:\nJames Brown", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> James Brown -> music.artist.genre -> Hip hop music\n# Answer:\nJames Brown", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> James Brown -> music.artist.genre -> Jazz\n# Answer:\nJames Brown", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> James Brown -> broadcast.artist.content -> 1Club.FM: 80s (Pop)\n# Answer:\nJames Brown", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> Charlie Chaplin -> freebase.valuenotation.is_reviewed -> Children\n# Answer:\nCharlie Chaplin"], "ground_truth": ["Redd Foxx", "Nipsey Russell", "Charlie Chaplin", "Walt Disney", "James Brown"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 1.0, "ans_recall": 0.4, "path_f1": 0.5714285714285715, "path_precision": 1.0, "path_recall": 0.4, "path_ans_f1": 0.888888888888889, "path_ans_precision": 1.0, "path_ans_recall": 0.8}
{"id": "WebQTest-1631", "prediction": ["# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0gy -> government.government_position_held.office_holder -> Charles Allen Culberson\n# Answer:\nCharles Allen Culberson", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0k1 -> government.government_position_held.office_holder -> Price Daniel\n# Answer:\nPrice Daniel", "# Reasoning Path:\nTexas -> government.political_district.representatives -> m.0bfmmss -> government.government_position_held.office_holder -> Morris Sheppard\n# Answer:\nMorris Sheppard", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0gy -> government.government_position_held.office_position_or_title -> Governor of Texas\n# Answer:\nGovernor of Texas", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.0101mqt4 -> government.government_position_held.office_holder -> Susan King\n# Answer:\nSusan King", "# Reasoning Path:\nTexas -> government.political_district.representatives -> m.0bfmmq0 -> government.government_position_held.office_holder -> Richard Coke\n# Answer:\nRichard Coke", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0k1 -> government.government_position_held.office_position_or_title -> Governor of Texas\n# Answer:\nGovernor of Texas", "# Reasoning Path:\nTexas -> organization.organization_scope.organizations_with_this_scope -> B'nai B'rith District Grand Lodge 7 -> organization.organization.geographic_scope -> Tennessee\n# Answer:\nB'nai B'rith District Grand Lodge 7", "# Reasoning Path:\nTexas -> government.political_district.representatives -> m.0bfmmss -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0k1 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor"], "ground_truth": ["Earle Bradford Mayfield", "Richard Coke", "John Hemphill", "Ted Cruz", "Horace Chilton", "Sam Houston", "William A. Blakley", "Morgan C. Hamilton", "Samuel B. Maxey", "Tom Connally", "Lloyd Bentsen", "Joseph Weldon Bailey", "Bob Krueger", "John Cornyn", "Thomas Jefferson Rusk", "John Tower", "Roger Q. Mills", "Kay Bailey Hutchison", "James W. Flanagan", "Ralph Yarborough", "Rienzi Melville Johnston", "John Henninger Reagan", "Charles Allen Culberson", "James Pinckney Henderson", "Morris Sheppard", "Price Daniel", "W. Lee O'Daniel", "Phil Gramm", "Louis Wigfall", "Andrew Jackson Houston", "Matthias Ward", "Lyndon B. Johnson"], "ans_acc": 0.125, "ans_hit": 1, "ans_f1": 0.19047619047619047, "ans_precission": 0.4, "ans_recall": 0.125, "path_f1": 0.14035087719298245, "path_precision": 0.4, "path_recall": 0.0851063829787234, "path_ans_f1": 0.19047619047619047, "path_ans_precision": 0.4, "path_ans_recall": 0.125}
{"id": "WebQTest-1632", "prediction": ["# Reasoning Path:\nLondon -> travel.travel_destination.tourist_attractions -> Victoria and Albert Museum -> travel.tourist_attraction.near_travel_destination -> City of London\n# Answer:\nVictoria and Albert Museum", "# Reasoning Path:\nLondon -> travel.travel_destination.tourist_attractions -> Victoria and Albert Museum -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.0jx2dvh\n# Answer:\nVictoria and Albert Museum", "# Reasoning Path:\nLondon -> travel.travel_destination.tourist_attractions -> Victoria and Albert Museum -> common.topic.notable_types -> Structure\n# Answer:\nVictoria and Albert Museum", "# Reasoning Path:\nLondon -> travel.travel_destination.tourist_attractions -> London Charterhouse -> location.location.geolocation -> m.02_tkdf\n# Answer:\nLondon Charterhouse", "# Reasoning Path:\nLondon -> location.location.partially_contains -> River Thames -> location.location.containedby -> United Kingdom\n# Answer:\nRiver Thames", "# Reasoning Path:\nLondon -> travel.travel_destination.tourist_attractions -> National Maritime Museum -> travel.tourist_attraction.near_travel_destination -> Greenwich\n# Answer:\nNational Maritime Museum", "# Reasoning Path:\nLondon -> travel.travel_destination.tourist_attractions -> London Charterhouse -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.0djwn2k\n# Answer:\nLondon Charterhouse", "# Reasoning Path:\nLondon -> travel.travel_destination.tourist_attractions -> National Maritime Museum -> location.location.containedby -> Greenwich\n# Answer:\nNational Maritime Museum", "# Reasoning Path:\nLondon -> periodicals.newspaper_circulation_area.newspapers -> i -> book.periodical.frequency_or_issues_per_year -> m.0jw2n9_\n# Answer:\ni", "# Reasoning Path:\nLondon -> location.location.partially_contains -> River Thames -> location.location.partially_containedby -> Kent\n# Answer:\nRiver Thames"], "ground_truth": ["Sir John Soane's Museum", "The Clink", "Wallace Collection", "Buckingham Palace", "Waterlily House", "Tate Modern, London", "Olympia", "Newington, London", "British Museum", "Diana, Princess of Wales Memorial Fountain", "Royal Institution", "Holloway", "Chokushi-Mon", "Palace of Westminster", "Chinawhite", "London School of Economics and Political Science", "The Building Centre", "City University London", "Earls Court Exhibition Centre", "Westminster Abbey", "Victoria and Albert Museum", "Museum of London", "London Underground", "London Marathon", "Serpentine Galleries", "Liverpool Street station", "Travefy", "Royal Albert Hall, London", "Central London", "London Victoria station", "London Charterhouse", "National Police Memorial", "The Nash Conservatory", "Smithfield, London", "G-A-Y", "Design Museum", "Sainsbury Wing", "London Eye", "University College London", "Science Museum, London", "Tower Bridge", "Wimbledon", "Wellcome Collection", "Horniman Museum", "Polish Institute and Sikorski Museum", "Southgate, London", "Wasps RFC", "Regent's Park", "London Bridge", "Natural History Museum, London", "Tate Gallery, Britain", "National Portrait Gallery, London", "Leighton House Museum", "Strand, London", "Tramp", "Tower of London", "Sutcliffe Park", "Duke of York's Headquarters", "Henman Hill", "Museum of London Docklands", "Bank of England Museum", "Crystal Palace Park", "Hippodrome, London", "Whetstone, London", "Selfridges, Oxford Street", "Queen's House", "Euston railway station", "Imperial War Museum London", "National Maritime Museum", "Wallington, London", "London Palladium", "London Zoo", "London Paddington station", "Churchill War Rooms", "St. James's Park", "New London Architecture", "Big Ben", "V&A Museum of Childhood", "University of London", "Hyde Park", "Trafalgar Square", "Madame Tussauds London", "Jewish Museum London", "Apsley House", "Chessington World of Adventures", "British Museum Reading Room", "Barbican Centre", "William Wilkins's Building", "RAF Bomber Command Memorial", "Imperial College London", "St Paul's Cathedral"], "ans_acc": 0.03296703296703297, "ans_hit": 1, "ans_f1": 0.06296851574212893, "ans_precission": 0.7, "ans_recall": 0.03296703296703297, "path_f1": 0.08187134502923976, "path_precision": 0.7, "path_recall": 0.043478260869565216, "path_ans_f1": 0.06296851574212893, "path_ans_precision": 0.7, "path_ans_recall": 0.03296703296703297}
{"id": "WebQTest-1633", "prediction": ["# Reasoning Path:\nMona Lisa -> visual_art.artwork.period_or_movement -> The Renaissance -> visual_art.art_period_movement.associated_artworks -> A Young Man Being Introduced to the Seven Liberal Arts\n# Answer:\nThe Renaissance", "# Reasoning Path:\nMona Lisa -> visual_art.artwork.period_or_movement -> The Renaissance -> visual_art.art_period_movement.associated_artworks -> Altarpiece of Ober-St. Veit\n# Answer:\nThe Renaissance", "# Reasoning Path:\nMona Lisa -> visual_art.artwork.period_or_movement -> The Renaissance -> visual_art.art_period_movement.associated_artworks -> Angel\n# Answer:\nThe Renaissance", "# Reasoning Path:\nMona Lisa -> visual_art.artwork.period_or_movement -> The Renaissance -> base.ontologies.ontology_instance.equivalent_instances -> m.07nfqtv\n# Answer:\nThe Renaissance", "# Reasoning Path:\nMona Lisa -> visual_art.artwork.period_or_movement -> The Renaissance -> common.topic.webpage -> m.09xk6x8\n# Answer:\nThe Renaissance", "# Reasoning Path:\nMona Lisa -> visual_art.artwork.period_or_movement -> The Renaissance -> common.topic.webpage -> m.09y4hf0\n# Answer:\nThe Renaissance", "# Reasoning Path:\nMona Lisa -> visual_art.artwork.period_or_movement -> The Renaissance -> common.topic.webpage -> m.09ybgy7\n# Answer:\nThe Renaissance", "# Reasoning Path:\nMona Lisa -> exhibitions.exhibit.exhibitions_displayed_in -> Mona Lisa by Leonardo da Vinci -> exhibitions.exhibition.venues -> m.0kbt5rx\n# Answer:\nMona Lisa by Leonardo da Vinci", "# Reasoning Path:\nMona Lisa -> exhibitions.exhibit.exhibitions_displayed_in -> Mona Lisa by Leonardo da Vinci -> exhibitions.exhibition.exhibition_types -> Art exhibition\n# Answer:\nMona Lisa by Leonardo da Vinci", "# Reasoning Path:\nMona Lisa -> exhibitions.exhibit.exhibitions_displayed_in -> Mona Lisa by Leonardo da Vinci -> common.topic.webpage -> m.0kbthtd\n# Answer:\nMona Lisa by Leonardo da Vinci"], "ground_truth": ["Portrait"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1634", "prediction": ["# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> fictional_universe.fictional_character_creator.fictional_characters_created -> Glenn Quagmire\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> fictional_universe.fictional_character_creator.fictional_characters_created -> Stewie Griffin\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> fictional_universe.fictional_character_creator.fictional_characters_created -> Brian Griffin\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> people.person.profession -> Voice Actor\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> people.person.profession -> Actor\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.010gysx9\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.parents -> Thelma Griffin -> tv.tv_character.appeared_in_tv_episodes -> m.05st127\n# Answer:\nThelma Griffin", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.parents -> Thelma Griffin -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Family Guy Universe\n# Answer:\nThelma Griffin", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.parents -> Francis Griffin -> fictional_universe.fictional_character.children -> Frances Griffin\n# Answer:\nFrancis Griffin", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> people.person.profession -> Animator\n# Answer:\nSeth MacFarlane"], "ground_truth": ["Seth MacFarlane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1635", "prediction": ["# Reasoning Path:\nTony Romo -> base.popstra.celebrity.friendship -> m.065pl45 -> base.popstra.friendship.participant -> Carrie Underwood\n# Answer:\nCarrie Underwood", "# Reasoning Path:\nTony Romo -> american_football.football_player.passing -> m.09tclvf -> american_football.player_passing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nTony Romo -> american_football.football_player.passing -> m.0791bch -> american_football.player_passing_statistics.season -> 2008 NFL season\n# Answer:\n2008 NFL season", "# Reasoning Path:\nTony Romo -> american_football.football_player.games -> m.07nvmcd -> american_football.player_game_statistics.team -> Dallas Cowboys\n# Answer:\nDallas Cowboys", "# Reasoning Path:\nTony Romo -> base.popstra.celebrity.friendship -> m.063hcgf -> base.popstra.friendship.participant -> Carrie Underwood\n# Answer:\nCarrie Underwood", "# Reasoning Path:\nTony Romo -> american_football.football_player.passing -> m.09tclvf -> american_football.player_passing_statistics.team -> Dallas Cowboys\n# Answer:\nDallas Cowboys", "# Reasoning Path:\nTony Romo -> american_football.football_player.passing -> m.0791bch -> american_football.player_passing_statistics.team -> Dallas Cowboys\n# Answer:\nDallas Cowboys", "# Reasoning Path:\nTony Romo -> american_football.football_player.passing -> m.07mmkjm -> american_football.player_passing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nTony Romo -> base.popstra.celebrity.friendship -> m.065pg0l -> base.popstra.friendship.participant -> Donald Faison\n# Answer:\nDonald Faison", "# Reasoning Path:\nTony Romo -> american_football.football_player.passing -> m.07mmkjm -> american_football.player_passing_statistics.team -> Dallas Cowboys\n# Answer:\nDallas Cowboys"], "ground_truth": ["2002"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1637", "prediction": ["# Reasoning Path:\nCatholicism -> religion.religion.holidays -> All Saints' Day -> time.holiday.day_of_year -> November 1\n# Answer:\nAll Saints' Day", "# Reasoning Path:\nCatholicism -> religion.religion.holidays -> Euthymius the Great's Feast Day -> common.topic.notable_types -> Holiday\n# Answer:\nEuthymius the Great's Feast Day", "# Reasoning Path:\nCatholicism -> religion.religion.deities -> The Father -> common.topic.notable_types -> Deity\n# Answer:\nThe Father", "# Reasoning Path:\nCatholicism -> base.argumentmaps.thing_of_disputed_value.disparagement -> A Connecticut Yankee in King Arthur's Court -> film.film.subjects -> Time travel\n# Answer:\nA Connecticut Yankee in King Arthur's Court", "# Reasoning Path:\nCatholicism -> religion.religion.holidays -> Corpus Christi -> base.schemastaging.holiday_extra.observance_rule -> Corpus Christi observance rule (Austria, 2000 - now)\n# Answer:\nCorpus Christi", "# Reasoning Path:\nCatholicism -> religion.religion.holidays -> All Saints' Day -> common.topic.notable_for -> g.1255bx1xr\n# Answer:\nAll Saints' Day", "# Reasoning Path:\nCatholicism -> religion.religion.deities -> The Father -> common.topic.article -> m.077_qp0\n# Answer:\nThe Father", "# Reasoning Path:\nCatholicism -> religion.religion.holidays -> All Saints' Day -> base.schemastaging.holiday_extra.observance_rule -> All Saints' Day observance rule (Austria, 2000 - now)\n# Answer:\nAll Saints' Day", "# Reasoning Path:\nCatholicism -> religion.religion.holidays -> Euthymius the Great's Feast Day -> time.holiday.featured_in_religions -> Eastern Orthodox Church\n# Answer:\nEuthymius the Great's Feast Day", "# Reasoning Path:\nCatholicism -> religion.religion.deities -> God -> common.topic.notable_for -> g.125d2mw4b\n# Answer:\nGod"], "ground_truth": ["Feast of Assumption", "St Casimir's Day", "St. Anthony's Day", "Palm Sunday", "World Day of Peace", "Feast of Saints Peter and Paul", "Name day", "St. Stephen's Day", "Saint Patrick's Day", "Maundy Thursday", "All Saints' Day", "St Nicholas Day", "Solemnity of Mary, Mother of God", "Corpus Christi", "St Crispin's Day", "Feast of Our Lady of the Rosary", "Feast of Our Lady of Sorrows", "Feast of St. Margaret of the \u00c1rp\u00e1d House", "Fat Thursday", "Feast of Christ the King", "Our Lady of Guadalupe Day", "Euthymius the Great's Feast Day", "Olsok", "Saints Cyril and Methodius Day", "Nativity of Mary", "Feast of the Immaculate Conception", "Saint Nicholas' Eve", "Feast of St Francis of Assisi", "Feast of the Cross", "St. Spiridon Day", "The Feast of Our Lady of Mount Carmel", "Feast of Our Lady of Hungary", "Our Lady of Aparecida's day", "Father Damien Day", "St Joseph's Day"], "ans_acc": 0.08571428571428572, "ans_hit": 1, "ans_f1": 0.15, "ans_precission": 0.6, "ans_recall": 0.08571428571428572, "path_f1": 0.15, "path_precision": 0.6, "path_recall": 0.08571428571428572, "path_ans_f1": 0.15, "path_ans_precision": 0.6, "path_ans_recall": 0.08571428571428572}
{"id": "WebQTest-1638", "prediction": ["# Reasoning Path:\nBelgium -> location.country.languages_spoken -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> French -> language.human_language.language_family -> Indo-European languages\n# Answer:\nFrench", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> Dutch Language -> language.human_language.main_country -> Netherlands\n# Answer:\nDutch Language", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> French -> language.human_language.main_country -> France\n# Answer:\nFrench", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> West Flemish -> language.human_language.language_family -> West Germanic languages\n# Answer:\nWest Flemish", "# Reasoning Path:\nBelgium -> location.statistical_region.gni_in_ppp_dollars -> g.11b61jk1pw\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> Dutch Language -> language.human_language.dialects -> Flemish language\n# Answer:\nDutch Language", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> West Flemish -> language.human_language.countries_spoken_in -> France\n# Answer:\nWest Flemish", "# Reasoning Path:\nBelgium -> media_common.netflix_genre.titles -> 22nd of May -> film.film.language -> French\n# Answer:\n22nd of May", "# Reasoning Path:\nBelgium -> location.statistical_region.gni_in_ppp_dollars -> g.1245__rg8\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars"], "ground_truth": ["German Language", "Dutch Language", "French"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.5, "ans_recall": 0.6666666666666666, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.631578947368421, "path_ans_precision": 0.6, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1639", "prediction": ["# Reasoning Path:\nL. Frank Baum -> people.deceased_person.place_of_death -> Hollywood -> base.barbie.barbie_theme.dolls_with_this_theme -> Barbie as Dorothy in The Wizard of Oz\n# Answer:\nHollywood", "# Reasoning Path:\nL. Frank Baum -> people.deceased_person.place_of_death -> Hollywood -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nHollywood", "# Reasoning Path:\nL. Frank Baum -> people.deceased_person.place_of_death -> Hollywood -> location.neighborhood.neighborhood_of -> Los Angeles\n# Answer:\nHollywood", "# Reasoning Path:\nL. Frank Baum -> people.deceased_person.place_of_death -> Hollywood -> base.barbie.barbie_theme.dolls_with_this_theme -> Barbie as Glinda the Good Witch in The Wizard of Oz #14901\n# Answer:\nHollywood", "# Reasoning Path:\nL. Frank Baum -> film.film_story_contributor.film_story_credits -> Wizard of Oz -> theater.theater_character.plays_appears_in -> The Wiz\n# Answer:\nWizard of Oz", "# Reasoning Path:\nL. Frank Baum -> film.film_story_contributor.film_story_credits -> Ay\u015fecik ve Sihirli C\u00fcceler R\u00fcyalar \u00dclkesinde -> film.film.music -> Turgud \u00d6ren\n# Answer:\nAy\u015fecik ve Sihirli C\u00fcceler R\u00fcyalar \u00dclkesinde", "# Reasoning Path:\nL. Frank Baum -> film.film_story_contributor.film_story_credits -> Fantasy 3 -> film.film.story_by -> Hans Christian Andersen\n# Answer:\nFantasy 3", "# Reasoning Path:\nL. Frank Baum -> film.film_story_contributor.film_story_credits -> Wizard of Oz -> theater.theater_character.plays_appears_in -> Wicked\n# Answer:\nWizard of Oz", "# Reasoning Path:\nL. Frank Baum -> book.author.works_written -> L. Frank Baum's Juvenile Speaker -> book.written_work.original_language -> English Language\n# Answer:\nL. Frank Baum's Juvenile Speaker", "# Reasoning Path:\nL. Frank Baum -> film.film_story_contributor.film_story_credits -> Wizard of Oz -> fictional_universe.fictional_character.occupation -> Con Artist\n# Answer:\nWizard of Oz"], "ground_truth": ["Hollywood"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-164", "prediction": ["# Reasoning Path:\nStephen Hawking -> people.person.profession -> Science writer -> base.descriptive_names.names.descriptive_name -> m.01039sc4\n# Answer:\nScience writer", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Science writer -> common.topic.notable_types -> Profession\n# Answer:\nScience writer", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Science writer -> people.profession.specialization_of -> Writer\n# Answer:\nScience writer", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Astronomer -> people.profession.specialization_of -> Scientist\n# Answer:\nAstronomer", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Science writer -> base.descriptive_names.names.descriptive_name -> m.0107446b\n# Answer:\nScience writer", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.0101ddrk\n# Answer:\nPhysicist", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Science writer -> base.descriptive_names.names.descriptive_name -> m.01087k89\n# Answer:\nScience writer", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Physicist -> people.profession.part_of_professional_field -> Physics\n# Answer:\nPhysicist", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.01066smw\n# Answer:\nPhysicist", "# Reasoning Path:\nStephen Hawking -> music.artist.track_contributions -> m.0rvpv1n -> music.track_contribution.track -> A Glorious Dawn\n# Answer:\nA Glorious Dawn"], "ground_truth": ["Science writer", "Writer", "Astronomer", "Astrophysicist", "Author", "Cosmologist", "Professor", "Scientist", "Physicist", "Mathematician"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5538461538461539, "ans_precission": 0.9, "ans_recall": 0.4, "path_f1": 0.45000000000000007, "path_precision": 0.9, "path_recall": 0.3, "path_ans_f1": 0.6428571428571429, "path_ans_precision": 0.9, "path_ans_recall": 0.5}
{"id": "WebQTest-1640", "prediction": ["# Reasoning Path:\nTunis -> location.location.containedby -> Tunisia -> location.location.containedby -> Arab world\n# Answer:\nArab world", "# Reasoning Path:\nTunisia -> location.location.containedby -> Arab world -> location.location.contains -> Algeria\n# Answer:\nArab world", "# Reasoning Path:\nTunis -> location.location.containedby -> Tunisia -> location.location.containedby -> Africa\n# Answer:\nAfrica", "# Reasoning Path:\nTunis -> location.location.containedby -> Tunisia -> base.locations.countries.continent -> Africa\n# Answer:\nAfrica", "# Reasoning Path:\nTunisia -> location.location.containedby -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica", "# Reasoning Path:\nTunis -> location.location.containedby -> Tunisia -> location.location.containedby -> Greater Maghreb\n# Answer:\nGreater Maghreb", "# Reasoning Path:\nTunisia -> base.locations.countries.continent -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica", "# Reasoning Path:\nTunis -> location.location.containedby -> Tunis Governorate -> location.location.containedby -> Tunisia\n# Answer:\nTunis Governorate", "# Reasoning Path:\nTunis -> location.location.contains -> \u00c9cole Nationale d'Architecture et d'Urbanisme -> location.location.containedby -> Tunisia\n# Answer:\n\u00c9cole Nationale d'Architecture et d'Urbanisme", "# Reasoning Path:\nTunis -> location.location.containedby -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica"], "ground_truth": ["Africa", "Tunis Governorate", "Tunisia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.631578947368421, "ans_precission": 0.6, "ans_recall": 0.6666666666666666, "path_f1": 0.8, "path_precision": 1.0, "path_recall": 0.6666666666666666, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1641", "prediction": ["# Reasoning Path:\nNicolas Cage -> base.popstra.celebrity.dated -> m.063g718 -> base.popstra.dated.participant -> Brooke Shields\n# Answer:\nBrooke Shields", "# Reasoning Path:\nNicolas Cage -> base.popstra.celebrity.dated -> m.063jy50 -> base.popstra.dated.participant -> Paris Hilton\n# Answer:\nParis Hilton", "# Reasoning Path:\nNicolas Cage -> celebrities.celebrity.sexual_orientation -> m.05n7hz_ -> celebrities.sexual_orientation_phase.sexual_orientation -> Heterosexuality\n# Answer:\nHeterosexuality", "# Reasoning Path:\nNicolas Cage -> base.popstra.celebrity.dated -> m.065prv1 -> base.popstra.dated.participant -> Jenna Jameson\n# Answer:\nJenna Jameson", "# Reasoning Path:\nNicolas Cage -> base.saturdaynightlive.snl_host.episodes_hosted -> SNL - 18.1 -> common.topic.notable_for -> g.1259fc15z\n# Answer:\nSNL - 18.1", "# Reasoning Path:\nNicolas Cage -> base.saturdaynightlive.snl_host.episodes_hosted -> SNL - 18.1 -> base.saturdaynightlive.snl_episode.musical_guest -> Bobby Brown\n# Answer:\nSNL - 18.1", "# Reasoning Path:\nNicolas Cage -> base.saturdaynightlive.snl_host.episodes_hosted -> SNL - 18.1 -> base.saturdaynightlive.snl_episode.musical_guest_performance -> m.04p50g3\n# Answer:\nSNL - 18.1"], "ground_truth": ["Alice Kim"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1643", "prediction": ["# Reasoning Path:\nChild's Play -> film.film.starring -> m.03jssz4 -> film.performance.actor -> Dinah Manoff\n# Answer:\nDinah Manoff", "# Reasoning Path:\nChild's Play -> film.film.starring -> m.03jt26n -> film.performance.actor -> Alex Vincent\n# Answer:\nAlex Vincent", "# Reasoning Path:\nChild's Play -> film.film.starring -> m.09hxz1j -> film.performance.actor -> Jack Colvin\n# Answer:\nJack Colvin", "# Reasoning Path:\nChild's Play -> film.film.directed_by -> Tom Holland -> film.director.film -> We All Scream for Ice Cream\n# Answer:\nTom Holland", "# Reasoning Path:\nChild's Play -> film.film.distributors -> m.08cv9m4 -> film.film_film_distributor_relationship.distributor -> Rogue\n# Answer:\nRogue", "# Reasoning Path:\nChild's Play -> film.film.starring -> m.03jssz4 -> film.performance.character -> Maggie Peterson\n# Answer:\nMaggie Peterson", "# Reasoning Path:\nChild's Play -> film.film.starring -> m.09hxz1j -> film.performance.character -> Dr. Ardmore\n# Answer:\nDr. Ardmore", "# Reasoning Path:\nChild's Play -> film.film.starring -> m.03jt26n -> film.performance.character -> Andy Barclay\n# Answer:\nAndy Barclay", "# Reasoning Path:\nChild's Play -> film.film.directed_by -> Tom Holland -> film.director.film -> Fatal Beauty\n# Answer:\nTom Holland", "# Reasoning Path:\nChild's Play -> film.film.directed_by -> Tom Holland -> film.producer.films_executive_produced -> The Owl\n# Answer:\nTom Holland"], "ground_truth": ["Edan Gross", "John Franklin", "Brad Dourif"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1644", "prediction": ["# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> base.activism.activism_issue.activists -> Abby Morton Diaz\n# Answer:\nWomen's rights", "# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> base.activism.activism_issue.activists -> Abigail Bush\n# Answer:\nWomen's rights", "# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> organization.organization_sector.organizations_in_this_sector -> Abortion Law Reform Association of New Zealand\n# Answer:\nWomen's rights", "# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> base.activism.activism_issue.activists -> Alice Moore Hubbard\n# Answer:\nWomen's rights", "# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> book.book_subject.works -> The Selected Papers of Margaret Sanger, Volume 2\n# Answer:\nWomen's rights", "# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> organization.organization_sector.organizations_in_this_sector -> Abortion Rights\n# Answer:\nWomen's rights", "# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> book.book_subject.works -> The Margaret Sanger papers microfilm edition\n# Answer:\nWomen's rights", "# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> organization.organization_sector.organizations_in_this_sector -> Abortion Rights Coalition of Canada\n# Answer:\nWomen's rights", "# Reasoning Path:\nMary Wollstonecraft -> people.person.profession -> Philosopher -> fictional_universe.character_occupation.characters_with_this_occupation -> Bandaran adepts\n# Answer:\nPhilosopher", "# Reasoning Path:\nMary Wollstonecraft -> base.kwebbase.kwtopic.has_sentences -> At Johnson's home she met the radical philosophers Godwin and Paine, the Swiss artist Fuseli and the artist and poet Blake. -> base.kwebbase.kwsentence.previous_sentence -> She introduced the principles of education for women in her reviews and she singled out for praise writers who gave equal respect to women's intellect.\n# Answer:\nAt Johnson's home she met the radical philosophers Godwin and Paine, the Swiss artist Fuseli and the artist and poet Blake."], "ground_truth": ["Women's rights"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1646", "prediction": ["# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.league -> m.0crtdbg -> sports.sports_league_participation.league -> National League East\n# Answer:\nNational League East", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.championships -> 1980 World Series -> common.topic.image -> 1980 World Series Program\n# Answer:\n1980 World Series", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.league -> m.0crt4k0 -> sports.sports_league_participation.league -> Major League Baseball\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.championships -> 1980 World Series -> sports.sports_championship_event.championship -> World Series\n# Answer:\n1980 World Series", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.league -> m.0crt7bg -> sports.sports_league_participation.league -> National League\n# Answer:\nNational League", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.championships -> 2008 National League Championship Series -> common.topic.article -> m.04f7kvq\n# Answer:\n2008 National League Championship Series", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.championships -> 2008 National League Championship Series -> time.event.locations -> Citizens Bank Park\n# Answer:\n2008 National League Championship Series", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.championships -> 2008 National League Championship Series -> sports.sports_championship_event.runner_up -> Los Angeles Dodgers\n# Answer:\n2008 National League Championship Series", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.championships -> 2008 World Series -> base.newsevents.news_reported_event.news_report_s -> m.07w2gtg\n# Answer:\n2008 World Series", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.championships -> 2008 World Series -> sports.sports_championship_event.runner_up -> Tampa Bay Rays\n# Answer:\n2008 World Series"], "ground_truth": ["1883"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1648", "prediction": ["# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> language.human_language.language_family -> Germanic languages\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> language.human_language.region -> Europe\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> language.human_language.language_family -> North Germanic languages\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.official_language -> Icelandic Language -> language.human_language.language_family -> Germanic languages\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.official_language -> Icelandic Language -> language.human_language.region -> Europe\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.statistical_region.gni_in_ppp_dollars -> g.11b61jk1t5\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> common.topic.article -> m.01gmks\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.official_language -> Icelandic Language -> language.human_language.language_family -> North Germanic languages\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.statistical_region.gni_in_ppp_dollars -> g.1245_7d01\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars"], "ground_truth": ["Icelandic Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1649", "prediction": ["# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Visual Artist -> fictional_universe.character_occupation.characters_with_this_occupation -> Eug\u00e8ne Delacroix\n# Answer:\nVisual Artist", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Visual Artist -> common.topic.subject_of -> Michael Godard\n# Answer:\nVisual Artist", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Visual Artist -> base.descriptive_names.names.descriptive_name -> m.0106_1y2\n# Answer:\nVisual Artist", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> theater.theater_production_staff_role.people_who_have_had_this_role -> m.0111n9h6\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> common.topic.subject_of -> Dum-Doodles\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Television producer -> common.topic.subjects -> Michael Palance\n# Answer:\nTelevision producer", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Television producer -> base.lightweight.profession.specialization_of -> Producers and Directors\n# Answer:\nTelevision producer", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Visual Artist -> base.descriptive_names.names.descriptive_name -> m.0106_2f_\n# Answer:\nVisual Artist", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> theater.theater_production_staff_role.people_who_have_had_this_role -> m.0_grv_v\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> book.author.works_written -> ARE YOU MY MOTHER MINI PB -> common.topic.notable_for -> g.125fbm0pz\n# Answer:\nARE YOU MY MOTHER MINI PB"], "ground_truth": ["Theodor Seuss Geisel"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-165", "prediction": ["# Reasoning Path:\nDonna Noble -> tv.tv_character.appeared_in_tv_program -> m.010pxvzy -> tv.regular_tv_appearance.actor -> Catherine Tate\n# Answer:\nCatherine Tate", "# Reasoning Path:\nDonna Noble -> tv.tv_character.appeared_in_tv_program -> m.010pxvzy -> tv.regular_tv_appearance.series -> Doctor Who\n# Answer:\nDoctor Who", "# Reasoning Path:\nDonna Noble -> tv.tv_character.appeared_in_tv_program -> m.010pxvzy -> tv.regular_tv_appearance.seasons -> Doctor Who Series 4 (2008)\n# Answer:\nDoctor Who Series 4 (2008)", "# Reasoning Path:\nDonna Noble -> fictional_universe.fictional_character.occupation -> office temp-worker -> fictional_universe.character_occupation.characters_with_this_occupation -> Wendy Watson\n# Answer:\noffice temp-worker", "# Reasoning Path:\nDonna Noble -> fictional_universe.fictional_character.occupation -> office temp-worker -> common.topic.notable_for -> g.1259jvdvh\n# Answer:\noffice temp-worker", "# Reasoning Path:\nDonna Noble -> fictional_universe.fictional_character.occupation -> office temp-worker -> fictional_universe.fictional_job_title.fictional_characters_with_this_job -> m.046d77x\n# Answer:\noffice temp-worker", "# Reasoning Path:\nDonna Noble -> tv.tv_character.appeared_in_tv_episodes -> m.04gf26q -> tv.tv_guest_role.actor -> Catherine Tate\n# Answer:\nCatherine Tate", "# Reasoning Path:\nDonna Noble -> tv.tv_character.appeared_in_tv_episodes -> m.04gf26q -> tv.tv_guest_role.episodes_appeared_in -> The Runaway Bride\n# Answer:\nThe Runaway Bride", "# Reasoning Path:\nDonna Noble -> tv.tv_character.appeared_in_tv_episodes -> m.04gf29d -> tv.tv_guest_role.episodes_appeared_in -> Turn Left\n# Answer:\nTurn Left", "# Reasoning Path:\nDonna Noble -> fictional_universe.fictional_character.occupation -> office temp-worker -> fictional_universe.fictional_job_title.fictional_characters_with_this_job -> m.04gf2gb\n# Answer:\noffice temp-worker"], "ground_truth": ["Catherine Tate"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.14814814814814817, "path_precision": 0.2, "path_recall": 0.11764705882352941, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1650", "prediction": ["# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> Choice USA -> common.topic.notable_types -> Organization\n# Answer:\nChoice USA", "# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> Choice USA -> organization.organization.sectors -> United States pro-choice movement\n# Answer:\nChoice USA", "# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> New York Media, LLC -> organization.organization.headquarters -> m.0hn2t5l\n# Answer:\nNew York Media, LLC", "# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> National Women's Political Caucus -> common.topic.notable_for -> g.12591d_31\n# Answer:\nNational Women's Political Caucus", "# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> Choice USA -> organization.organization.sectors -> Women's rights\n# Answer:\nChoice USA", "# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> National Women's Political Caucus -> common.topic.article -> m.04wwk1\n# Answer:\nNational Women's Political Caucus", "# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> New York Media, LLC -> business.employer.employees -> m.0hn2sx7\n# Answer:\nNew York Media, LLC", "# Reasoning Path:\nGloria Steinem -> book.author.works_written -> Doing Sixty and Seventy -> common.topic.notable_types -> Book\n# Answer:\nDoing Sixty and Seventy", "# Reasoning Path:\nGloria Steinem -> book.author.works_written -> Feminist Family Values -> common.topic.notable_for -> g.1255tp2p4\n# Answer:\nFeminist Family Values", "# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> New York Media, LLC -> common.topic.webpage -> m.05st96g\n# Answer:\nNew York Media, LLC"], "ground_truth": ["Feminist Majority Foundation", "National Women's Political Caucus", "New York Media, LLC", "Women's Action Alliance", "Ms. Foundation for Women", "Choice USA", "Coalition of Labor Union Women"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5581395348837209, "ans_precission": 0.8, "ans_recall": 0.42857142857142855, "path_f1": 0.5581395348837209, "path_precision": 0.8, "path_recall": 0.42857142857142855, "path_ans_f1": 0.5581395348837209, "path_ans_precision": 0.8, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1652", "prediction": ["# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.main_country -> United Kingdom\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish English -> base.schemastaging.context_name.pronunciation -> g.125_nyqwz\n# Answer:\nScottish English", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scots Language -> common.topic.notable_types -> Human Language\n# Answer:\nScots Language", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.countries_spoken_in -> Kingdom of Great Britain\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish English -> common.topic.image -> King David Book of Psalms from the reign of James VI\n# Answer:\nScottish English", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scots Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nScots Language", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scots Language -> language.human_language.language_family -> Anglo-Frisian languages\n# Answer:\nScots Language", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish English -> common.topic.image -> Outwith\n# Answer:\nScottish English"], "ground_truth": ["English Language"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1653", "prediction": ["# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> education.education.institution -> Mercy College\n# Answer:\nMercy College", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.04hc7zn -> education.education.institution -> Phillips Exeter Academy\n# Answer:\nPhillips Exeter Academy", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.02n93cn -> education.education.institution -> Harvard University\n# Answer:\nHarvard University", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nMinor", "# Reasoning Path:\nMark Zuckerberg -> influence.influence_node.influenced -> Austin Cameron -> influence.influence_node.influenced_by -> Tim Berners-Lee\n# Answer:\nAustin Cameron", "# Reasoning Path:\nMark Zuckerberg -> influence.influence_node.influenced -> Paul Scolnick -> people.person.education -> m.01314533\n# Answer:\nPaul Scolnick", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.02n93cn -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Specialization\n# Answer:\nSpecialization"], "ground_truth": ["Phillips Exeter Academy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1654", "prediction": ["# Reasoning Path:\nBigos -> food.dish.cuisine -> Polish food -> dining.cuisine.dishes -> Kugelis\n# Answer:\nPolish food", "# Reasoning Path:\nBigos -> food.dish.cuisine -> Polish food -> dining.cuisine.dishes -> Borscht\n# Answer:\nPolish food", "# Reasoning Path:\nBigos -> food.dish.cuisine -> Polish food -> dining.cuisine.dishes -> Kishka\n# Answer:\nPolish food", "# Reasoning Path:\nBigos -> food.dish.cuisine -> Polish food -> common.topic.notable_types -> Cuisine\n# Answer:\nPolish food", "# Reasoning Path:\nBigos -> common.topic.notable_types -> Dish -> type.type.properties -> Type of dish\n# Answer:\nDish", "# Reasoning Path:\nBigos -> food.dish.cuisine -> Polish food -> common.topic.image -> A meat dish in Poland\n# Answer:\nPolish food", "# Reasoning Path:\nBigos -> food.dish.cuisine -> Polish food -> common.topic.image -> Polish pastries\n# Answer:\nPolish food", "# Reasoning Path:\nBigos -> common.topic.notable_types -> Dish -> type.type.properties -> Cuisine\n# Answer:\nDish", "# Reasoning Path:\nBigos -> food.dish.ingredients -> Cabbage -> food.food.nutrients -> m.07x7v_9\n# Answer:\nCabbage", "# Reasoning Path:\nBigos -> food.dish.ingredients -> Meat -> common.topic.article -> m.04scv\n# Answer:\nMeat"], "ground_truth": ["Stew"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1655", "prediction": ["# Reasoning Path:\nTina Turner -> people.person.children -> Ronnie Turner -> people.person.place_of_birth -> Los Angeles\n# Answer:\nRonnie Turner", "# Reasoning Path:\nTina Turner -> people.person.children -> Ronnie Turner -> people.person.parents -> Ike Turner\n# Answer:\nRonnie Turner", "# Reasoning Path:\nTina Turner -> people.person.children -> Raymond Craig Turner -> people.person.parents -> Ike Turner\n# Answer:\nRaymond Craig Turner", "# Reasoning Path:\nTina Turner -> people.person.children -> Ronnie Turner -> people.person.spouse_s -> m.0t60dj8\n# Answer:\nRonnie Turner", "# Reasoning Path:\nTina Turner -> people.person.children -> Raymond Craig Turner -> people.person.parents -> Raymond Hill\n# Answer:\nRaymond Craig Turner", "# Reasoning Path:\nTina Turner -> people.person.children -> Raymond Craig Turner -> people.person.nationality -> United States of America\n# Answer:\nRaymond Craig Turner", "# Reasoning Path:\nTina Turner -> people.person.children -> Raymond Craig Turner -> people.person.place_of_birth -> United States of America\n# Answer:\nRaymond Craig Turner", "# Reasoning Path:\nTina Turner -> award.award_nominee.award_nominations -> m.05bnxy1 -> award.award_nomination.nominated_for -> River: The Joni Letters\n# Answer:\nRiver: The Joni Letters", "# Reasoning Path:\nTina Turner -> award.award_nominee.award_nominations -> m.0_tlmwl -> award.award_nomination.nominated_for -> Mad Max Beyond Thunderdome\n# Answer:\nMad Max Beyond Thunderdome", "# Reasoning Path:\nTina Turner -> award.award_nominee.award_nominations -> m.05bnxy1 -> award.award_nomination.award_nominee -> Norah Jones\n# Answer:\nNorah Jones"], "ground_truth": ["Ronnie Turner", "Raymond Craig Turner"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1656", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> base.argumentmaps.innovator.original_ideas -> Viola organista -> common.topic.notable_for -> g.1254z0xjb\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> base.argumentmaps.innovator.original_ideas -> Viola organista -> common.topic.notable_types -> Musical instrument\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> base.argumentmaps.innovator.original_ideas -> Viola organista -> music.performance_role.track_performances -> m.0112zmz9\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Viola organista -> common.topic.notable_for -> g.1254z0xjb\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> base.argumentmaps.innovator.original_ideas -> Viola organista -> music.performance_role.track_performances -> m.0nk45qs\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> base.argumentmaps.innovator.original_ideas -> Viola organista -> music.performance_role.track_performances -> m.0rtvlvk\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Viola organista -> music.performance_role.track_performances -> m.0112zmz9\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Triple Barrel Canon -> common.topic.notable_for -> g.1yl5vgrll\n# Answer:\nTriple Barrel Canon", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi -> visual_art.artwork.art_subject -> Biblical Magi\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Triple Barrel Canon -> common.topic.notable_types -> Invention\n# Answer:\nTriple Barrel Canon"], "ground_truth": ["Viola organista", "Double hull", "Diving suit", "Parachute", "Triple Barrel Canon", "Ornithopter", "Anemometer", "Aerial Screw", "33-Barreled Organ", "Armored car"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.32727272727272727, "ans_precission": 0.9, "ans_recall": 0.2, "path_f1": 0.3111111111111111, "path_precision": 0.7, "path_recall": 0.2, "path_ans_f1": 0.32727272727272727, "path_ans_precision": 0.9, "path_ans_recall": 0.2}
{"id": "WebQTest-1657", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.season -> 1992\u201393 NBA season\n# Answer:\n1992\u201393 NBA season", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.04ynxq7 -> sports.sports_award.season -> 2000\u201301 NBA season\n# Answer:\n2000\u201301 NBA season", "# Reasoning Path:\nShaquille O'Neal -> sports.drafted_athlete.drafted -> m.04fw77r -> sports.sports_league_draft_pick.draft -> 1992 NBA draft\n# Answer:\n1992 NBA draft", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc58 -> sports.sports_award.season -> 1999\u20132000 NBA season\n# Answer:\n1999\u20132000 NBA season", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.04ynxq7 -> sports.sports_award.award -> NBA All-Star Game Most Valuable Player Award\n# Answer:\nNBA All-Star Game Most Valuable Player Award", "# Reasoning Path:\nShaquille O'Neal -> sports.drafted_athlete.drafted -> m.04fw77r -> sports.sports_league_draft_pick.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player -> common.topic.notable_for -> g.125ctmqvt\n# Answer:\nPlayer", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc58 -> sports.sports_award.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> Player -> music.recording.releases -> You cant STOP the REIGN\n# Answer:\nPlayer"], "ground_truth": ["1992 NBA draft"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1658", "prediction": ["# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Republic of Kosovo -> location.country.official_language -> Albanian language\n# Answer:\nRepublic of Kosovo", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Republic of Kosovo -> location.country.official_language -> Serbian language\n# Answer:\nRepublic of Kosovo", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Republic of Kosovo -> location.location.contains -> Pristina\n# Answer:\nRepublic of Kosovo", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Austria -> location.country.languages_spoken -> German Language\n# Answer:\nAustria", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Republic of Kosovo -> location.location.contains -> Erenik\n# Answer:\nRepublic of Kosovo", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Republic of Kosovo -> base.schemastaging.disputed_location_claimant.disputed_territories -> White Drin\n# Answer:\nRepublic of Kosovo", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Austria -> organization.organization_scope.organizations_with_this_scope -> Marcandi Ltd.\n# Answer:\nAustria", "# Reasoning Path:\nTurkish Language -> base.rosetta.languoid.local_name -> Turkish -> base.rosetta.local_name.locale -> United States of America\n# Answer:\nTurkish", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Austria -> location.country.languages_spoken -> Bosnian language\n# Answer:\nAustria", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Austria -> base.aareas.schema.administrative_area.administrative_children -> Vorarlberg\n# Answer:\nAustria"], "ground_truth": ["Turkey", "Austria", "Republic of Kosovo", "Cyprus", "Republic of Macedonia"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.5538461538461539, "ans_precission": 0.9, "ans_recall": 0.4, "path_f1": 0.5538461538461539, "path_precision": 0.9, "path_recall": 0.4, "path_ans_f1": 0.5538461538461539, "path_ans_precision": 0.9, "path_ans_recall": 0.4}
{"id": "WebQTest-1659", "prediction": ["# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94596 -> common.topic.notable_for -> g.1258sqjf5\n# Answer:\n94596", "# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94596 -> location.location.geolocation -> m.03dnkgn\n# Answer:\n94596", "# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94597 -> location.postal_code.country -> United States of America\n# Answer:\n94597", "# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94596 -> location.location.containedby -> Contra Costa County\n# Answer:\n94596", "# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94598 -> location.location.containedby -> Contra Costa County\n# Answer:\n94598", "# Reasoning Path:\nWalnut Creek -> location.statistical_region.population -> g.11b66b709v\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94597 -> common.topic.notable_types -> Postal Code\n# Answer:\n94597", "# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94597 -> common.topic.notable_for -> g.125bwr97g\n# Answer:\n94597", "# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94598 -> location.location.geolocation -> m.03dnkgy\n# Answer:\n94598", "# Reasoning Path:\nWalnut Creek -> location.statistical_region.population -> g.11bymm147x\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["94597", "94598", "94596", "94595"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.8, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.8, "path_ans_recall": 0.75}
{"id": "WebQTest-166", "prediction": ["# Reasoning Path:\nTheodore Lesieg -> people.person.education -> m.0n1kynd -> education.education.institution -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nTheodore Lesieg -> people.person.education -> m.04ytk85 -> education.education.institution -> Dartmouth College\n# Answer:\nDartmouth College", "# Reasoning Path:\nTheodore Lesieg -> people.person.education -> m.03p87mv -> education.education.institution -> Lincoln College, Oxford\n# Answer:\nLincoln College, Oxford", "# Reasoning Path:\nTheodore Lesieg -> book.author.works_written -> ARE YOU MY MOTHER MINI PB -> common.topic.notable_for -> g.125fbm0pz\n# Answer:\nARE YOU MY MOTHER MINI PB", "# Reasoning Path:\nTheodore Lesieg -> book.author.works_written -> ARE YOU MY MOTHER MINI PB -> common.topic.notable_types -> Book\n# Answer:\nARE YOU MY MOTHER MINI PB", "# Reasoning Path:\nTheodore Lesieg -> book.author.works_written -> Bartholomew and the Oobleck -> freebase.valuenotation.has_value -> Date written\n# Answer:\nBartholomew and the Oobleck", "# Reasoning Path:\nTheodore Lesieg -> music.artist.track -> A Quarter of Dawn -> music.recording.artist -> Thurl Ravenscroft\n# Answer:\nA Quarter of Dawn", "# Reasoning Path:\nTheodore Lesieg -> music.artist.track -> Be Kind to Your Small Person Friends -> music.recording.tracks -> Be Kind to Your Small Person Friends (song only) (Horton)\n# Answer:\nBe Kind to Your Small Person Friends", "# Reasoning Path:\nTheodore Lesieg -> book.author.works_written -> And to Think That I Saw It on Mulberry Street -> common.topic.article -> m.05_th_\n# Answer:\nAnd to Think That I Saw It on Mulberry Street", "# Reasoning Path:\nTheodore Lesieg -> music.artist.track -> A Quarter of Dawn -> music.recording.contributions -> m.0110d72d\n# Answer:\nA Quarter of Dawn"], "ground_truth": ["University of Oxford", "Dartmouth College", "Lincoln College, Oxford"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1661", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.film -> Star Wars\n# Answer:\nStar Wars", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Uncredited\n# Answer:\nUncredited", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nThe Making of Star Wars", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.film -> Return of the Jedi\n# Answer:\nReturn of the Jedi"], "ground_truth": ["James Earl Jones"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1662", "prediction": ["# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.location.containedby -> North America\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Papua New Guinea -> location.country.languages_spoken -> Tok Pisin Language\n# Answer:\nPapua New Guinea", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Papua New Guinea -> location.country.official_language -> Tok Pisin Language\n# Answer:\nPapua New Guinea", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.location.containedby -> Americas\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> education.school_category.schools_of_this_kind -> Piers Midwinter -> people.person.quotations -> Learn with a NEST. Please be my GUEST. I will give you my BEST,\n# Answer:\nPiers Midwinter", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> New Zealand -> location.country.languages_spoken -> Standard Chinese\n# Answer:\nNew Zealand", "# Reasoning Path:\nEnglish Language -> education.school_category.schools_of_this_kind -> Piers Midwinter -> common.topic.notable_types -> Organization founder\n# Answer:\nPiers Midwinter", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> New Zealand -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nNew Zealand", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Papua New Guinea -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nPapua New Guinea"], "ground_truth": ["Montserrat", "Mandatory Palestine", "Hong Kong", "Republic of Ireland", "Brunei", "South Africa", "South Yemen", "Cura\u00e7ao", "Sierra Leone", "Kenya", "Honduras", "Gazankulu", "Pakistan", "Cameroon", "Tuvalu", "Transkei", "Cyprus", "Isle of Man", "Saint Lucia", "Vatican City", "China", "Japan", "Vanuatu", "Sri Lanka", "Bonaire", "Rwanda", "Indonesia", "Samoa", "Papua New Guinea", "Fiji", "Territory of Papua and New Guinea", "Antigua and Barbuda", "Zimbabwe", "Swaziland", "England", "Ethiopia", "Uganda", "Nigeria", "Tanzania", "Turks and Caicos Islands", "Marshall Islands", "Belize", "Guam", "Gibraltar", "Australia", "Philippines", "United Kingdom", "Qatar", "Laos", "Malaysia", "Cook Islands", "New Zealand", "Saint Vincent and the Grenadines", "Kiribati", "Kingdom of Great Britain", "United States of America", "Bahamas", "Sudan", "India", "Lesotho", "Guyana", "Israel", "Puerto Rico", "Bangladesh", "Jersey", "Jordan", "Bermuda", "Malta", "Singapore", "Cayman Islands", "Canada", "Gambia", "Saint Kitts and Nevis", "Namibia", "Dominica", "Nauru", "Ghana", "Liberia", "Timor-Leste", "Zambia", "Barbados", "Tokelau", "State of Palestine", "Grenada", "Botswana", "Wales", "Territory of New Guinea"], "ans_acc": 0.034482758620689655, "ans_hit": 1, "ans_f1": 0.06611570247933884, "ans_precission": 0.8, "ans_recall": 0.034482758620689655, "path_f1": 0.06611570247933884, "path_precision": 0.8, "path_recall": 0.034482758620689655, "path_ans_f1": 0.06611570247933884, "path_ans_precision": 0.8, "path_ans_recall": 0.034482758620689655}
{"id": "WebQTest-1663", "prediction": ["# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.family -> Brass instrument\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.artist.genre -> Swing music -> music.genre.parent_genre -> Ragtime\n# Answer:\nSwing music", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> music.instrument.family -> Brass instrument\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> common.topic.notable_types -> Musical instrument\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.instrumentalists -> W. C. Handy\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.artist.genre -> Swing music -> music.genre.parent_genre -> Dixieland\n# Answer:\nSwing music", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> book.written_work.subjects -> Trumpeter\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.artist.genre -> Swing music -> common.topic.article -> m.015y_z\n# Answer:\nSwing music", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.instrumentalists -> Bix Beiderbecke\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> common.topic.notable_types -> Book\n# Answer:\nTrumpet"], "ground_truth": ["Traditional pop music", "Swing music", "Jazz", "Scat singing", "Dixieland"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.24, "ans_precission": 0.3, "ans_recall": 0.2, "path_f1": 0.24, "path_precision": 0.3, "path_recall": 0.2, "path_ans_f1": 0.34285714285714286, "path_ans_precision": 0.3, "path_ans_recall": 0.4}
{"id": "WebQTest-1664", "prediction": ["# Reasoning Path:\nGermany -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fdl8\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc38c_4\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1665", "prediction": ["# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> language.language_family.languages -> Aequian language\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> common.topic.image -> Iron Age Italy\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> common.topic.notable_for -> g.125d12rwn\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> base.rosetta.languoid.parent -> Latino-Faliscan Group -> base.rosetta.languoid.parent -> Italic Group\n# Answer:\nLatino-Faliscan Group", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> language.language_family.languages -> Angevin dialect\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> language.language_family.languages -> Augeron\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Indo-European languages -> language.language_family.sub_families -> Italic languages\n# Answer:\nIndo-European languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Indo-European languages -> common.topic.notable_types -> Language Family\n# Answer:\nIndo-European languages", "# Reasoning Path:\nLatin Language -> base.rosetta.languoid.parent -> Latino-Faliscan Group -> base.rosetta.languoid.languoid_class -> Group\n# Answer:\nLatino-Faliscan Group", "# Reasoning Path:\nLatin Language -> music.genre.albums -> Acustico -> music.album.artist -> La 5\u00aa Estaci\u00f3n\n# Answer:\nAcustico"], "ground_truth": ["Italic languages", "Indo-European languages"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1666", "prediction": ["# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Baths of Caracalla -> common.topic.image -> Thermae of Caracalla Panorama\n# Answer:\nBaths of Caracalla", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Baths of Caracalla -> common.topic.notable_for -> g.1255mqf0k\n# Answer:\nBaths of Caracalla", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Baths of Caracalla -> common.topic.image -> Caraca\n# Answer:\nBaths of Caracalla", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Piazza Navona -> common.topic.notable_for -> g.1256m3_q7\n# Answer:\nPiazza Navona", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Palazzo Spada -> architecture.structure.architect -> Francesco Borromini\n# Answer:\nPalazzo Spada", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Baths of Caracalla -> common.topic.image -> BathsOfCaracalla\n# Answer:\nBaths of Caracalla", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Palazzo Spada -> architecture.building.building_function -> Art Gallery\n# Answer:\nPalazzo Spada", "# Reasoning Path:\nRome -> base.popstra.location.vacationers -> m.063hfk9 -> base.popstra.vacation_choice.vacationer -> Eva Longoria\n# Answer:\nEva Longoria", "# Reasoning Path:\nRome -> base.popstra.location.vacationers -> m.0649tj_ -> base.popstra.vacation_choice.vacationer -> David Beckham\n# Answer:\nDavid Beckham", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Palazzo Spada -> architecture.building.building_function -> Palace\n# Answer:\nPalazzo Spada"], "ground_truth": ["Basilica di Santa Maria Maggiore", "Archbasilica of St. John Lateran", "Piazza Venezia", "Vatican City", "Palazzo Barberini", "Raphael Rooms", "Tabularium", "Bioparco di Roma", "Palazzo Spada", "Castel Sant'Angelo", "Policlinico Umberto I", "Pyramid of Cestius", "Palazzo Venezia", "St. Peter's Basilica", "Capitoline Hill", "Villa d'Este", "Via Veneto", "Domus Aurea", "Pantheon", "Rome Observatory", "Trevi Fountain", "Spanish Steps", "Galleria Borghese", "Piazza del Popolo", "Hadrian's Villa", "Fontana delle Naiadi", "Piazza Navona", "Vatican Museums", "Seven Pilgrim Churches of Rome", "Roman Forum", "Palazzo Chigi", "Circus Maximus", "Altare della Patria", "Appian Way", "Catacombs of Rome", "Sistine Chapel", "Apostolic Palace", "Santa Maria sopra Minerva", "Quirinal Palace", "Basilica of Saint Paul Outside the Walls", "Bocca della Verit\u00e0", "Baths of Caracalla", "Palazzo Farnese", "Colosseum", "Roma-Ostia Half Marathon", "San Nicola in Carcere"], "ans_acc": 0.06521739130434782, "ans_hit": 1, "ans_f1": 0.1206030150753769, "ans_precission": 0.8, "ans_recall": 0.06521739130434782, "path_f1": 0.1206030150753769, "path_precision": 0.8, "path_recall": 0.06521739130434782, "path_ans_f1": 0.1206030150753769, "path_ans_precision": 0.8, "path_ans_recall": 0.06521739130434782}
{"id": "WebQTest-1667", "prediction": ["# Reasoning Path:\nSolomon -> people.person.children -> Menelik I -> people.person.parents -> Queen of Sheba\n# Answer:\nMenelik I", "# Reasoning Path:\nSolomon -> people.person.children -> Rehoboam -> people.person.children -> Abijah of Judah\n# Answer:\nRehoboam", "# Reasoning Path:\nSolomon -> people.person.children -> Menelik I -> common.topic.article -> m.01qn7g\n# Answer:\nMenelik I", "# Reasoning Path:\nSolomon -> people.person.children -> Menelik I -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nMenelik I", "# Reasoning Path:\nSolomon -> people.person.children -> Menelik I -> freebase.valuenotation.has_value -> Date of death\n# Answer:\nMenelik I", "# Reasoning Path:\nSolomon -> people.person.children -> Rehoboam -> people.person.religion -> Judaism\n# Answer:\nRehoboam", "# Reasoning Path:\nSolomon -> royalty.noble_person.titles -> m.0cvmm_r -> royalty.noble_title_tenure.noble_title -> King of Israel\n# Answer:\nKing of Israel", "# Reasoning Path:\nSolomon -> people.person.children -> Menelik I -> freebase.valuenotation.has_value -> Place of death\n# Answer:\nMenelik I", "# Reasoning Path:\nSolomon -> people.person.children -> Rehoboam -> royalty.monarch.royal_line -> Davidic line\n# Answer:\nRehoboam", "# Reasoning Path:\nSolomon -> film.film_subject.films -> Solomon and Sheba -> film.film.subjects -> The Bible\n# Answer:\nSolomon and Sheba"], "ground_truth": ["Menelik I", "Rehoboam"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1668", "prediction": ["# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> medicine.risk_factor.diseases -> Ventricular septal defect\n# Answer:\nheart attack", "# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nheart attack", "# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> Verapamil\n# Answer:\nheart attack", "# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w0hpl\n# Answer:\nheart attack", "# Reasoning Path:\nAndy Irons -> sports.pro_athlete.sports_played_professionally -> m.0z6442d -> sports.pro_sports_played.sport -> Surfing\n# Answer:\nSurfing", "# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> ACE inhibitor\n# Answer:\nheart attack", "# Reasoning Path:\nAndy Irons -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> medicine.risk_factor.diseases -> Complex regional pain syndrome\n# Answer:\nheart attack", "# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> Acebutolol\n# Answer:\nheart attack", "# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w4lwz\n# Answer:\nheart attack"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1669", "prediction": ["# Reasoning Path:\nAuburn University -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nAuburn University -> location.location.containedby -> Alabama -> base.aareas.schema.administrative_area.administrative_children -> Lee County\n# Answer:\nAlabama", "# Reasoning Path:\nAuburn University -> location.location.containedby -> Auburn University Historic District -> location.location.containedby -> Alabama\n# Answer:\nAuburn University Historic District", "# Reasoning Path:\nAuburn University -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Florida\n# Answer:\nUnited States of America", "# Reasoning Path:\nAuburn University -> location.location.containedby -> Alabama -> location.location.containedby -> United States of America\n# Answer:\nAlabama", "# Reasoning Path:\nAuburn University -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Georgia\n# Answer:\nUnited States of America", "# Reasoning Path:\nAuburn University -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nAuburn University -> education.university.international_tuition -> m.0zsbgkr -> measurement_unit.dated_money_value.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nAuburn University -> location.location.containedby -> Alabama -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nAlabama", "# Reasoning Path:\nAuburn University -> location.location.containedby -> Auburn University Historic District -> location.location.containedby -> Auburn\n# Answer:\nAuburn University Historic District"], "ground_truth": ["Alabama", "United States of America", "Auburn University Historic District", "Auburn"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-167", "prediction": ["# Reasoning Path:\nQueensland -> symbols.namesake.named_after -> Queen Victoria\n# Answer:\nQueen Victoria", "# Reasoning Path:\nQueensland -> government.political_district.representatives -> m.04_87_t -> government.government_position_held.office_position_or_title -> Australian Senator\n# Answer:\nAustralian Senator", "# Reasoning Path:\nQueensland -> government.political_district.representatives -> m.04_87_t -> government.government_position_held.office_holder -> Len Harris\n# Answer:\nLen Harris", "# Reasoning Path:\nQueensland -> government.political_district.representatives -> m.04_8828 -> government.government_position_held.governmental_body -> Australian Senate\n# Answer:\nAustralian Senate", "# Reasoning Path:\nQueensland -> government.political_district.representatives -> m.04_fgk9 -> government.government_position_held.office_holder -> Brenda Gibbs\n# Answer:\nBrenda Gibbs", "# Reasoning Path:\nQueensland -> government.governmental_jurisdiction.governing_officials -> m.0104n88_ -> government.government_position_held.office_holder -> Francis Isidore Power\n# Answer:\nFrancis Isidore Power", "# Reasoning Path:\nQueensland -> government.governmental_jurisdiction.governing_officials -> m.0107h9yk -> freebase.valuenotation.is_reviewed -> To\n# Answer:\nTo", "# Reasoning Path:\nQueensland -> government.political_district.representatives -> m.04_8828 -> government.government_position_held.office_holder -> John Herron\n# Answer:\nJohn Herron", "# Reasoning Path:\nQueensland -> government.governmental_jurisdiction.governing_officials -> m.0104n88_ -> government.government_position_held.basic_title -> Member of Legislative Council\n# Answer:\nMember of Legislative Council", "# Reasoning Path:\nQueensland -> government.political_district.representatives -> m.04_8828 -> government.government_position_held.office_position_or_title -> Australian Senator\n# Answer:\nAustralian Senator"], "ground_truth": ["Queen Victoria"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1670", "prediction": ["# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sonia Ali -> people.person.children -> Anastasia N. Ali\n# Answer:\nSonia Ali", "# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sonia Ali -> common.topic.notable_for -> g.125980l9b\n# Answer:\nSonia Ali", "# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sonia Ali -> freebase.valuenotation.has_value -> Parents\n# Answer:\nSonia Ali", "# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sonia Ali -> people.person.children -> Kimberly Ali\n# Answer:\nSonia Ali", "# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sheriff Ali -> common.topic.notable_types -> Person\n# Answer:\nSheriff Ali", "# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sonia Ali -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nSonia Ali", "# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sheriff Ali -> people.person.children -> Anastasia N. Ali\n# Answer:\nSheriff Ali", "# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sheriff Ali -> people.person.spouse_s -> m.0r8wjcd\n# Answer:\nSheriff Ali", "# Reasoning Path:\nTatyana Ali -> common.topic.webpage -> m.03l4yjy -> common.webpage.resource -> m.0bjkgr1\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nTatyana Ali -> tv.tv_actor.guest_roles -> m.0bgnv4d -> tv.tv_guest_role.episodes_appeared_in -> Ep. #9256\n# Answer:\nEp. #9256"], "ground_truth": ["Sonia Ali", "Sheriff Ali"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1671", "prediction": ["# Reasoning Path:\nLeif Eriksson -> people.person.place_of_birth -> K\u00f6ping -> common.topic.notable_types -> Statistical region\n# Answer:\nK\u00f6ping", "# Reasoning Path:\nLeif Eriksson -> people.person.place_of_birth -> K\u00f6ping -> location.location.people_born_here -> Daniel Gunnarsson\n# Answer:\nK\u00f6ping", "# Reasoning Path:\nLeif Eriksson -> people.person.place_of_birth -> K\u00f6ping -> common.topic.notable_for -> g.1259m4hv9\n# Answer:\nK\u00f6ping", "# Reasoning Path:\nLeif Eriksson -> sports.pro_athlete.sports_played_professionally -> m.0d5r_4t -> sports.pro_sports_played.sport -> Football\n# Answer:\nFootball", "# Reasoning Path:\nLeif Eriksson -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nLeif Eriksson -> people.person.gender -> Male -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Males\n# Answer:\nMale", "# Reasoning Path:\nLeif Eriksson -> people.person.gender -> Male -> common.topic.notable_for -> g.1255p_jgc\n# Answer:\nMale", "# Reasoning Path:\nLeif Eriksson -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nLeif Eriksson -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Acute myeloid leukemia\n# Answer:\nMale"], "ground_truth": ["K\u00f6ping"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-1672", "prediction": ["# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.countries_spoken_in -> Tanzania\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.parent -> Ongamo-Maa Group\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> common.topic.article -> m.064klf\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.countries_spoken_in -> Kenya\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai -> common.image.size -> m.02bgrp_\n# Answer:\nMaasai", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai women and children -> common.image.size -> m.03qy0z9\n# Answer:\nMaasai women and children", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai-jump -> common.image.size -> m.02br_p7\n# Answer:\nMaasai-jump"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1673", "prediction": ["# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> organization.organization.founders -> Hermann G\u00f6ring\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> base.schemastaging.context_name.pronunciation -> g.125_pt37m\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAdolf Hitler -> government.politician.party -> m.0btmmq1 -> government.political_party_tenure.party -> German Workers' Party\n# Answer:\nGerman Workers' Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> organization.organization.founders -> Anton Drexler\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> book.book_subject.works -> The Hidden Life of Otto Frank\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Gestapo -> common.topic.notable_types -> Defunct Organization\n# Answer:\nGestapo", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> 1st SS Panzer Division Leibstandarte SS Adolf Hitler -> base.schemastaging.context_name.pronunciation -> m.01314fx8\n# Answer:\n1st SS Panzer Division Leibstandarte SS Adolf Hitler", "# Reasoning Path:\nAdolf Hitler -> government.politician.party -> m.075rkrk -> government.political_party_tenure.party -> Nazi Party\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> organization.organization.founders -> Dietrich Eckart\n# Answer:\nNazi Party"], "ground_truth": ["Nazi Party", "German Workers' Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1674", "prediction": ["# Reasoning Path:\nNorth Korea -> government.governmental_jurisdiction.governing_officials -> m.0c9_j0f -> government.government_position_held.office_holder -> Kim Il-sung\n# Answer:\nKim Il-sung", "# Reasoning Path:\nNorth Korea -> government.governmental_jurisdiction.governing_officials -> m.010r1v7b -> government.government_position_held.office_holder -> Kim Il-sung\n# Answer:\nKim Il-sung", "# Reasoning Path:\nNorth Korea -> government.governmental_jurisdiction.governing_officials -> m.0c9_j0f -> government.government_position_held.basic_title -> President\n# Answer:\nPresident", "# Reasoning Path:\nNorth Korea -> government.governmental_jurisdiction.governing_officials -> m.0c9_h__ -> government.government_position_held.office_holder -> Kim Jong-il\n# Answer:\nKim Jong-il", "# Reasoning Path:\nNorth Korea -> government.governmental_jurisdiction.governing_officials -> m.010r1v7b -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nNorth Korea -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f719\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nNorth Korea -> location.location.events -> 1969 EC-121 shootdown incident -> base.culturalevent.event.entity_involved -> United States of America\n# Answer:\n1969 EC-121 shootdown incident", "# Reasoning Path:\nNorth Korea -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6fdwt\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nNorth Korea -> government.governmental_jurisdiction.governing_officials -> m.0c9_h__ -> government.government_position_held.office_position_or_title -> Supreme Leader of North Korea\n# Answer:\nSupreme Leader of North Korea", "# Reasoning Path:\nNorth Korea -> location.location.events -> UN Offensive, 1950 -> base.culturalevent.event.entity_involved -> United States of America\n# Answer:\nUN Offensive, 1950"], "ground_truth": ["Kim Jong-un"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1675", "prediction": ["# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> location.location.partially_contains -> Wild River\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> Qu\u00e9bec -> base.aareas.schema.administrative_area.administrative_parent -> Canada\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> location.location.partially_contains -> Androscoggin River\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> symbols.name_source.namesakes -> USS New Hampshire\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> Qu\u00e9bec -> base.aareas.schema.administrative_area.capital -> Quebec City\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> location.location.partially_contains -> Connecticut River\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> government.political_district.representatives -> m.010d4j50\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Jersey -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Amy\n# Answer:\nNew Jersey", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> symbols.name_source.namesakes -> USS New Hampshire (1864)\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Jersey -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Danielle\n# Answer:\nNew Jersey"], "ground_truth": ["North Carolina", "Georgia", "Saint Pierre and Miquelon", "Ohio", "New Brunswick", "Qu\u00e9bec", "Alabama", "Maine", "Massachusetts", "South Carolina", "Kentucky", "Vermont", "West Virginia", "New England", "Prince Edward Island", "Pennsylvania", "Virginia", "Newfoundland and Labrador", "Nova Scotia", "New York", "New Jersey", "Connecticut", "New Hampshire", "Tennessee"], "ans_acc": 0.16666666666666666, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 1.0, "ans_recall": 0.125, "path_f1": 0.2222222222222222, "path_precision": 1.0, "path_recall": 0.125, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 1.0, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTest-1676", "prediction": ["# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Andorra\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Belgium\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60xs9d2\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> France\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc38hhk\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc3dvnk\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.1hhc39shg\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.1hhc3h715\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1677", "prediction": ["# Reasoning Path:\nTerry Waite -> people.person.place_of_birth -> Bollington -> location.location.containedby -> United Kingdom\n# Answer:\nBollington", "# Reasoning Path:\nTerry Waite -> people.person.place_of_birth -> Bollington -> location.location.containedby -> Macclesfield\n# Answer:\nBollington", "# Reasoning Path:\nTerry Waite -> people.person.place_of_birth -> Bollington -> common.topic.notable_types -> English civil parish\n# Answer:\nBollington", "# Reasoning Path:\nTerry Waite -> people.person.nationality -> United Kingdom -> base.aareas.schema.administrative_area.administrative_parent -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nTerry Waite -> people.person.nationality -> United Kingdom -> base.popstra.location.arrestee -> m.0ghc35h\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nTerry Waite -> people.person.nationality -> United Kingdom -> location.statistical_region.long_term_unemployment_rate -> g.12cp_jvpx\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nTerry Waite -> book.author.works_written -> Wysheid Uit Die Psalms -> common.topic.notable_types -> Book\n# Answer:\nWysheid Uit Die Psalms", "# Reasoning Path:\nTerry Waite -> book.author.works_written -> Wysheid Uit Die Psalms -> common.topic.notable_for -> g.125500sm_\n# Answer:\nWysheid Uit Die Psalms", "# Reasoning Path:\nTerry Waite -> book.author.works_written -> Footfalls In Memory - Reflections From Solitude -> common.topic.notable_types -> Book\n# Answer:\nFootfalls In Memory - Reflections From Solitude", "# Reasoning Path:\nTerry Waite -> people.person.nationality -> United Kingdom -> location.statistical_region.long_term_unemployment_rate -> g.1hhc390lx\n# Answer:\nUnited Kingdom"], "ground_truth": ["Bollington"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1678", "prediction": ["# Reasoning Path:\nBattle of San Juan Hill -> common.image.size -> m.02cgdw0\n# Answer:\ncommon.image.size", "# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> travel.travel_destination.tourist_attractions -> Turquino National Park\n# Answer:\nSantiago de Cuba", "# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> travel.travel_destination.tourist_attractions -> La Gran Piedra (The Big Rock)\n# Answer:\nSantiago de Cuba", "# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> location.statistical_region.population -> g.11b7tdz1nw\n# Answer:\nSantiago de Cuba", "# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> travel.travel_destination.tourist_attractions -> Baconao\n# Answer:\nSantiago de Cuba", "# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> location.location.events -> Moncada Barracks\n# Answer:\nSantiago de Cuba", "# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> location.statistical_region.population -> g.11bc87rwmn\n# Answer:\nSantiago de Cuba", "# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> location.location.events -> Siege of Santiago\n# Answer:\nSantiago de Cuba", "# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> location.statistical_region.population -> g.11btt6x2r4\n# Answer:\nSantiago de Cuba", "# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> location.location.events -> Attack on moncada barracks\n# Answer:\nSantiago de Cuba"], "ground_truth": ["Santiago de Cuba"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1679", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> South Korea\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> American Samoa\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Northern Mariana Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guam\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Marshall Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Mexico\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Germany\n# Answer:\nFederal republic"], "ground_truth": ["Federal republic", "Presidential system", "Constitutional republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-168", "prediction": ["# Reasoning Path:\nJames Madison -> book.author.works_written -> The Papers of James Madison, Secretary of State Series, Vol. 9 -> freebase.valuenotation.has_value -> Date written\n# Answer:\nThe Papers of James Madison, Secretary of State Series, Vol. 9", "# Reasoning Path:\nJames Madison -> book.author.works_written -> The Papers of James Madison, Secretary of State Series, Vol. 9 -> book.written_work.original_language -> English Language\n# Answer:\nThe Papers of James Madison, Secretary of State Series, Vol. 9", "# Reasoning Path:\nJames Madison -> book.author.works_written -> The Papers of James Madison, Secretary of State Series, Vol. 4 -> book.written_work.previous_in_series -> The Papers of James Madison, Secretary of State Series, Vol. 3\n# Answer:\nThe Papers of James Madison, Secretary of State Series, Vol. 4", "# Reasoning Path:\nJames Madison -> book.author.works_written -> The Papers of James Madison, Secretary of State Series, Vol. 9 -> book.written_work.part_of_series -> The Papers of James Madison: Secretary of State Series\n# Answer:\nThe Papers of James Madison, Secretary of State Series, Vol. 9", "# Reasoning Path:\nJames Madison -> book.author.works_written -> The Papers of James Madison, Secretary of State Series, Vol. 1 -> common.topic.notable_for -> g.1ydnq031x\n# Answer:\nThe Papers of James Madison, Secretary of State Series, Vol. 1", "# Reasoning Path:\nJames Madison -> book.author.works_written -> The Papers of James Madison, Secretary of State Series, Vol. 4 -> freebase.valuenotation.has_value -> Date written\n# Answer:\nThe Papers of James Madison, Secretary of State Series, Vol. 4", "# Reasoning Path:\nJames Madison -> organization.organization_founder.organizations_founded -> The United States Constitutional Convention -> time.event.locations -> Philadelphia\n# Answer:\nThe United States Constitutional Convention", "# Reasoning Path:\nJames Madison -> book.author.works_written -> The Papers of James Madison, Secretary of State Series, Vol. 4 -> common.topic.notable_for -> g.1ydnq7_n2\n# Answer:\nThe Papers of James Madison, Secretary of State Series, Vol. 4", "# Reasoning Path:\nJames Madison -> book.author.works_written -> The Papers of James Madison, Secretary of State Series, Vol. 1 -> book.written_work.next_in_series -> The Papers of James Madison, Secretary of State Series, Vol. 2\n# Answer:\nThe Papers of James Madison, Secretary of State Series, Vol. 1", "# Reasoning Path:\nJames Madison -> book.author.works_written -> The Papers of James Madison, Secretary of State Series, Vol. 1 -> freebase.valuenotation.has_no_value -> Previous in series\n# Answer:\nThe Papers of James Madison, Secretary of State Series, Vol. 1"], "ground_truth": ["The Papers of James Madison, Secretary of State Series, Vol. 6", "The Papers of James Madison, Presidential Series Vol. 3", "Federalist No. 41", "Federalist No. 48", "President Madison's inaugural speech", "The Papers of James Madison, Secretary of State Series, Vol. 7", "Federalist No. 45", "All impressments unlawful and inadmissible", "The forging of American federalism", "The Papers of James Madison, Secretary of State Series, Vol. 2", "The Papers of James Madison, Presidential Series Vol. 6", "The Federalist Papers", "The Papers of James Madison, Presidential Series Vol. 1", "Extract of a letter from the Secretary of State to Mr. Monroe, relative to impressments", "The Papers of James Madison, Secretary of State Series, Vol. 4", "The Papers of James Madison, Secretary of State Series, Vol. 3", "The complete Madison", "Federalist No. 47", "Federalist No. 52", "Federalist No. 19", "Federalist No. 39", "The Papers of James Madison Congressional Series, Vol. 12: 2 October 1789 - 20 January 1790", "Federalist No. 18", "Federalist No. 44", "Federalist No. 10", "Federalist No. 54", "Federalist No. 14", "Religious freedom", "The Papers of James Madison, Vol. 4", "Federalist No. 63", "The Papers of James Madison Retirement Series, Volume 1: 4 March 1817-31 January 1820", "Federalist No. 53", "James Madison: Writings", "Letters and other writings of James Madison", "The Papers of James Madison, Secretary of State Series, Vol. 8", "Jonathan Bull and Mary Bull", "The Papers of James Madison, Secretary of State Series, Vol. 1", "An address delivered before the Agricultural Society of Albemarle, on Tuesday, May 12, 1818", "Selections from the private correspondence of James Madison, from 1813 to 1836", "Federalist No. 38", "An examination of the British doctrine, which subjects to capture a neutral trade, not open in time of peace", "The Papers of James Madison, Secretary of State Series, Vol. 5", "The reply of Mr. Madison, in answer to Mr. Rose, in discussing the affair of the Chesapeake", "The Papers of James Madison, Secretary of State Series, Vol. 9", "Federalist No. 42", "Federalist No. 46", "Federalist No. 58", "An examination of the British doctrine", "Federalist No. 57", "Federalist No. 55", "Letters from the Secretary of State to Messrs. Monroe and Pinkney, on subjects committed to their joint negotiations", "Federalist No. 43", "Federalist No. 49", "The Papers of James Madison, Presidential Series Vol. 5", "A vocabulary of New Jersey Delaware", "Letters from the Secretary of State to Mr. Monroe, on the subject of impressments, &c", "The James Madison Papers, 1723 - 1836", "Federalist No. 20", "Federalist No. 62", "The mind of the founder", "The Papers of James Madison, Presidential Series Vol. 4", "Federalist No. 51", "Federalist No. 56", "Federalist No. 50", "James Madison, 1751-1836", "Mr. Madison's motion for commercial restrictions", "Papers of James Madison Volume 10: May 27, 1787-March 3, 1788", "The Papers of James Madison, Presidential Series Vol. 2", "The Papers of James Madison, Presidential Series Vol. 7", "Calendar of the correspondence of James Madison", "Federalist No. 37", "Equal religious liberty stated and defended", "Federalist No. 40"], "ans_acc": 0.0684931506849315, "ans_hit": 1, "ans_f1": 0.07860262008733623, "ans_precission": 0.9, "ans_recall": 0.0410958904109589, "path_f1": 0.07860262008733623, "path_precision": 0.9, "path_recall": 0.0410958904109589, "path_ans_f1": 0.1272984441301273, "path_ans_precision": 0.9, "path_ans_recall": 0.0684931506849315}
{"id": "WebQTest-1680", "prediction": ["# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1983 World Series -> sports.sports_championship_event.championship -> World Series\n# Answer:\n1983 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1983 World Series -> time.event.locations -> Memorial Stadium\n# Answer:\n1983 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1983 World Series -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1983 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1970 World Series -> time.event.locations -> Riverfront Stadium\n# Answer:\n1970 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1970 World Series -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1970 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1983 World Series -> time.event.locations -> Veterans Stadium\n# Answer:\n1983 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1970 World Series -> sports.sports_championship_event.runner_up -> Cincinnati Reds\n# Answer:\n1970 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.venue -> m.0wz1z2b -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1970 World Series -> time.event.locations -> Memorial Stadium\n# Answer:\n1970 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1966 World Series -> sports.sports_championship_event.season -> 1966 Major League Baseball Season\n# Answer:\n1966 World Series"], "ground_truth": ["1983 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1682", "prediction": ["# Reasoning Path:\nBen Stiller -> base.popstra.celebrity.dated -> m.065q5w_ -> base.popstra.dated.participant -> Janeane Garofalo\n# Answer:\nJaneane Garofalo", "# Reasoning Path:\nBen Stiller -> base.popstra.celebrity.dated -> m.063fz2s -> base.popstra.dated.participant -> Jeanne Tripplehorn\n# Answer:\nJeanne Tripplehorn", "# Reasoning Path:\nBen Stiller -> celebrities.celebrity.sexual_relationships -> m.04dq6cg -> celebrities.romantic_relationship.celebrity -> Jeanne Tripplehorn\n# Answer:\nJeanne Tripplehorn", "# Reasoning Path:\nBen Stiller -> base.popstra.celebrity.dated -> m.063fz0t -> base.popstra.dated.participant -> Amanda Peet\n# Answer:\nAmanda Peet", "# Reasoning Path:\nBen Stiller -> celebrities.celebrity.sexual_relationships -> m.04dq6cq -> celebrities.romantic_relationship.celebrity -> Janeane Garofalo\n# Answer:\nJaneane Garofalo", "# Reasoning Path:\nBen Stiller -> celebrities.celebrity.sexual_relationships -> m.04dq6cz -> celebrities.romantic_relationship.celebrity -> Amanda Peet\n# Answer:\nAmanda Peet", "# Reasoning Path:\nBen Stiller -> celebrities.celebrity.sexual_relationships -> m.04dq6cg -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nDated", "# Reasoning Path:\nBen Stiller -> celebrities.celebrity.sexual_relationships -> m.04dq6cq -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nDated", "# Reasoning Path:\nBen Stiller -> celebrities.celebrity.sexual_relationships -> m.04dq6cz -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nDated", "# Reasoning Path:\nBen Stiller -> film.actor.film -> m.010g_cgd -> film.performance.film -> Nantucket Film Festival's Comedy Roundtable\n# Answer:\nNantucket Film Festival's Comedy Roundtable"], "ground_truth": ["Jeanne Tripplehorn", "Christine Taylor", "Calista Flockhart", "Amanda Peet", "Rhea Durham", "Janeane Garofalo", "Claire Forlani"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.6, "ans_recall": 0.42857142857142855, "path_f1": 0.41379310344827586, "path_precision": 0.6, "path_recall": 0.3157894736842105, "path_ans_f1": 0.5, "path_ans_precision": 0.6, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1683", "prediction": ["# Reasoning Path:\nNorway -> location.country.official_language -> Nynorsk -> common.topic.image -> Norwegianmalforms\n# Answer:\nNynorsk", "# Reasoning Path:\nNorway -> location.country.official_language -> Norwegian Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nNorwegian Language", "# Reasoning Path:\nNorway -> location.statistical_region.net_migration -> g.1q5jhf535\n# Answer:\nlocation.statistical_region.net_migration", "# Reasoning Path:\nNorway -> location.country.official_language -> Norwegian Language -> common.topic.image -> Norwegianmalforms\n# Answer:\nNorwegian Language", "# Reasoning Path:\nNorway -> location.statistical_region.gdp_nominal_per_capita -> g.11b60vk8pf\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nNorway -> location.country.official_language -> Nynorsk -> language.human_language.writing_system -> Danish and Norwegian alphabet\n# Answer:\nNynorsk", "# Reasoning Path:\nNorway -> location.country.official_language -> Norwegian Language -> common.topic.notable_types -> Human Language\n# Answer:\nNorwegian Language", "# Reasoning Path:\nNorway -> location.statistical_region.net_migration -> g.1q5jsymtt\n# Answer:\nlocation.statistical_region.net_migration", "# Reasoning Path:\nNorway -> location.country.official_language -> Bokm\u00e5l -> common.topic.image -> Norwegianmalforms\n# Answer:\nBokm\u00e5l", "# Reasoning Path:\nNorway -> location.statistical_region.net_migration -> g.1q5k0_3l2\n# Answer:\nlocation.statistical_region.net_migration"], "ground_truth": ["Bokm\u00e5l", "Norwegian Language", "Nynorsk"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.41379310344827586, "path_precision": 0.3, "path_recall": 0.6666666666666666, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1684", "prediction": ["# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Lucian -> influence.influence_node.influenced_by -> Menander\n# Answer:\nLucian", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Lucian -> influence.influence_node.influenced_by -> Homer\n# Answer:\nLucian", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Christopher Marlowe -> influence.influence_node.influenced_by -> Virgil\n# Answer:\nChristopher Marlowe", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Lucian -> common.topic.notable_types -> Author\n# Answer:\nLucian", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Lucian -> influence.influence_node.influenced -> Baltasar Graci\u00e1n\n# Answer:\nLucian", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Christopher Marlowe -> influence.influence_node.influenced_by -> Ovid\n# Answer:\nChristopher Marlowe", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Christopher Marlowe -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nChristopher Marlowe", "# Reasoning Path:\nWilliam Shakespeare -> freebase.valuenotation.is_reviewed -> Art Form\n# Answer:\nArt Form", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Christopher Marlowe -> influence.influence_node.influenced -> Anthony Burgess\n# Answer:\nChristopher Marlowe", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Thomas More -> people.deceased_person.place_of_death -> London\n# Answer:\nThomas More"], "ground_truth": ["Geoffrey Chaucer", "Thomas More", "Plautus", "Ovid", "John Pory", "Christopher Marlowe", "Seneca the Younger", "Thomas Kyd", "Edmund Spenser", "Michel de Montaigne", "Lucian", "Virgil", "Plutarch", "Terence"], "ans_acc": 0.35714285714285715, "ans_hit": 1, "ans_f1": 0.3461538461538461, "ans_precission": 0.9, "ans_recall": 0.21428571428571427, "path_f1": 0.3461538461538461, "path_precision": 0.9, "path_recall": 0.21428571428571427, "path_ans_f1": 0.5113636363636365, "path_ans_precision": 0.9, "path_ans_recall": 0.35714285714285715}
{"id": "WebQTest-1685", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> education.education.institution -> Boston Latin School\n# Answer:\nBoston Latin School", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.is_reviewed -> Institution\n# Answer:\nInstitution", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> American literature\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.has_no_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nMinor", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.is_reviewed -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> :Library and information science\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.size -> m.02bc9fn\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze"], "ground_truth": ["Boston Latin School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1686", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y42 -> government.government_position_held.office_holder -> Ernest McFarland\n# Answer:\nErnest McFarland", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y46 -> government.government_position_held.office_holder -> Paul Fannin\n# Answer:\nPaul Fannin", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.0hz834l -> government.government_position_held.office_holder -> Jan Brewer\n# Answer:\nJan Brewer", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y42 -> government.government_position_held.office_position_or_title -> Governor of Arizona\n# Answer:\nGovernor of Arizona", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y46 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.0hz834l -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.0hz834l -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nBasic title", "# Reasoning Path:\nArizona -> location.statistical_region.religions -> m.04403h9 -> location.religion_percentage.religion -> Pentecostalism\n# Answer:\nPentecostalism", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.resource -> ATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!\n# Answer:\nATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!"], "ground_truth": ["Janet Napolitano", "Jan Brewer"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.16666666666666669, "ans_precission": 0.1, "ans_recall": 0.5, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 0.1, "path_ans_recall": 0.5}
{"id": "WebQTest-1687", "prediction": ["# Reasoning Path:\nSpike Lee -> film.director.film -> Clockers -> film.film.genre -> Thriller\n# Answer:\nClockers", "# Reasoning Path:\nSpike Lee -> film.director.film -> 25th Hour -> film.film.edited_by -> Barry Alexander Brown\n# Answer:\n25th Hour", "# Reasoning Path:\nSpike Lee -> film.producer.film -> Love & Basketball -> film.film.film_casting_director -> Aisha Coley\n# Answer:\nLove & Basketball", "# Reasoning Path:\nSpike Lee -> film.director.film -> Clockers -> film.film.rating -> R (USA)\n# Answer:\nClockers", "# Reasoning Path:\nSpike Lee -> film.producer.film -> Love & Basketball -> film.film.subjects -> Basketball\n# Answer:\nLove & Basketball", "# Reasoning Path:\nSpike Lee -> film.director.film -> 25th Hour -> film.film.rating -> R (USA)\n# Answer:\n25th Hour", "# Reasoning Path:\nSpike Lee -> film.director.film -> 4 Little Girls -> film.film.cinematography -> Ellen Kuras\n# Answer:\n4 Little Girls", "# Reasoning Path:\nSpike Lee -> film.producer.film -> Love & Basketball -> media_common.netflix_title.netflix_genres -> Basketball\n# Answer:\nLove & Basketball", "# Reasoning Path:\nSpike Lee -> film.director.film -> Clockers -> film.film.genre -> Coming of age\n# Answer:\nClockers", "# Reasoning Path:\nSpike Lee -> film.director.film -> 25th Hour -> film.film.language -> English Language\n# Answer:\n25th Hour"], "ground_truth": ["Love & Basketball", "Concert For New York City", "Bad 25", "She's Gotta Have It", "Sucker Free City", "Joe's Bed-Stuy Barbershop: We Cut Heads", "HIStory on Film, Volume II", "School Daze", "Crooklyn", "Do the Right Thing", "Inside Man", "Ten Minutes Older: The Trumpet", "New Jersey Drive", "Mo' Better Blues", "If God Is Willing and da Creek Don't Rise", "Miracle at St. Anna", "Chiraq", "When the Levees Broke: A Requiem in Four Acts", "The Original Kings of Comedy", "The Best Man", "Clockers", "25th Hour", "Lovers & Haters", "Jungle Fever", "A Huey P. Newton Story", "Kobe Doin' Work", "4 Little Girls", "Malcolm X", "Saint John of Las Vegas", "Lumi\u00e8re and Company", "Bamboozled", "She Hate Me", "Passing Strange", "He Got Game", "M.O.N.Y.", "And Ya Don't Stop: Hip Hop's Greatest Videos, Vol. 1", "Red Hook Summer", "Da Sweet Blood of Jesus", "Freak", "Oldboy", "Jim Brown: All-American", "Last Hustle in Brooklyn", "Summer of Sam", "Mike Tyson: Undisputed Truth", "Get on the Bus", "Girl 6", "All the Invisible Children"], "ans_acc": 0.0851063829787234, "ans_hit": 1, "ans_f1": 0.1568627450980392, "ans_precission": 1.0, "ans_recall": 0.0851063829787234, "path_f1": 0.07947019867549669, "path_precision": 0.6, "path_recall": 0.0425531914893617, "path_ans_f1": 0.1568627450980392, "path_ans_precision": 1.0, "path_ans_recall": 0.0851063829787234}
{"id": "WebQTest-1688", "prediction": ["# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> medicine.disease.symptoms -> Weight loss\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> medicine.disease.symptoms -> Cutaneous manifestations of sarcoidosis\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> medicine.disease.symptoms -> Garland's triad\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> common.topic.subject_of -> Del Sorbo Antonio\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> common.topic.subject_of -> JAMA Dermatology\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Complications from pneumonia -> people.cause_of_death.people -> Aram Haigaz\n# Answer:\nComplications from pneumonia", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Complications from pneumonia -> common.topic.notable_for -> g.1yl5vldtz\n# Answer:\nComplications from pneumonia", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> medicine.disease_cause.diseases -> Erythema nodosum\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> medicine.disease_cause.diseases -> Goitre\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Complications from pneumonia -> people.cause_of_death.parent_cause_of_death -> Pneumonia\n# Answer:\nComplications from pneumonia"], "ground_truth": ["Complications from pneumonia", "Sarcoidosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-169", "prediction": ["# Reasoning Path:\nBelgium -> location.country.languages_spoken -> German Language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nGerman Language", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> German Language -> language.human_language.region -> Europe\n# Answer:\nGerman Language", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> Dutch Language -> language.human_language.main_country -> Netherlands\n# Answer:\nDutch Language", "# Reasoning Path:\nBelgium -> location.statistical_region.gni_in_ppp_dollars -> g.11b61jk1pw\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> German Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nGerman Language", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> French -> language.human_language.language_family -> Indo-European languages\n# Answer:\nFrench", "# Reasoning Path:\nBelgium -> media_common.netflix_genre.titles -> 22nd of May -> film.film.language -> French\n# Answer:\n22nd of May", "# Reasoning Path:\nBelgium -> location.statistical_region.gni_in_ppp_dollars -> g.1245__rg8\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars"], "ground_truth": ["German Language", "Dutch Language", "French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.3157894736842105, "path_precision": 0.3, "path_recall": 0.3333333333333333, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1690", "prediction": ["# Reasoning Path:\nElectrolux -> organization.organization.headquarters -> m.03ll14c -> location.mailing_address.citytown -> Stockholm\n# Answer:\nStockholm", "# Reasoning Path:\nElectrolux -> organization.organization.headquarters -> m.03ll14c -> common.topic.notable_for -> g.1257hvmfs\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nElectrolux -> organization.organization.headquarters -> m.03ll14c -> freebase.valuenotation.has_no_value -> Street Address 2\n# Answer:\nStreet Address 2", "# Reasoning Path:\nElectrolux -> business.business_operation.net_profit -> m.04j7kvv -> measurement_unit.dated_money_value.currency -> Swedish krona\n# Answer:\nSwedish krona", "# Reasoning Path:\nElectrolux -> organization.organization.headquarters -> m.03ll14c -> freebase.valuenotation.has_no_value -> Street Address 3\n# Answer:\nStreet Address 3", "# Reasoning Path:\nElectrolux -> organization.organization.headquarters -> m.03ll14c -> freebase.valuenotation.has_no_value -> Street Address 4\n# Answer:\nStreet Address 4", "# Reasoning Path:\nElectrolux -> base.schemastaging.organization_extra.contact_webpages -> m.010pzf7c -> internet.localized_uri.location -> India\n# Answer:\nIndia", "# Reasoning Path:\nElectrolux -> base.schemastaging.organization_extra.contact_webpages -> m.010b0w6t -> internet.localized_uri.location -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nElectrolux -> base.schemastaging.organization_extra.contact_webpages -> m.010pzf7c -> internet.localized_uri.language -> English\n# Answer:\nEnglish", "# Reasoning Path:\nElectrolux -> base.schemastaging.organization_extra.contact_webpages -> m.010b0w9_ -> internet.localized_uri.location -> Canada\n# Answer:\nCanada"], "ground_truth": ["Stockholm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1691", "prediction": ["# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.statistical_region.population -> g.11b66fk05m\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.statistical_region.population -> g.11b7tcbqts\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nHouston -> location.place_with_neighborhoods.neighborhoods -> Manchester -> common.topic.notable_for -> g.1255tj187\n# Answer:\nManchester", "# Reasoning Path:\nHouston -> location.statistical_region.population -> g.11btt54h79\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nHouston -> location.place_with_neighborhoods.neighborhoods -> Manchester -> location.neighborhood.neighborhood_of -> Greater Houston\n# Answer:\nManchester", "# Reasoning Path:\nHouston -> location.place_with_neighborhoods.neighborhoods -> Addicks -> location.location.containedby -> Harris County\n# Answer:\nAddicks"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1694", "prediction": ["# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Hamnet Shakespeare -> people.person.parents -> Anne Hathaway\n# Answer:\nHamnet Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Judith Quiney -> people.person.parents -> Anne Hathaway\n# Answer:\nJudith Quiney", "# Reasoning Path:\nWilliam Shakespeare -> people.person.spouse_s -> m.02wtqtm -> people.marriage.spouse -> Anne Hathaway\n# Answer:\nAnne Hathaway", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Hamnet Shakespeare -> freebase.valuenotation.has_value -> Place of death\n# Answer:\nHamnet Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Susanna Hall -> people.person.children -> Elizabeth Barnard\n# Answer:\nSusanna Hall", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Hamnet Shakespeare -> people.person.sibling_s -> m.02_5694\n# Answer:\nHamnet Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.spouse_s -> m.02wtqtm -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Judith Quiney -> people.person.nationality -> United Kingdom\n# Answer:\nJudith Quiney", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Susanna Hall -> people.person.place_of_birth -> Stratford-upon-Avon\n# Answer:\nSusanna Hall", "# Reasoning Path:\nWilliam Shakespeare -> people.person.spouse_s -> m.02wtqtm -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage"], "ground_truth": ["Anne Hathaway"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1695", "prediction": ["# Reasoning Path:\nHarry S. Truman -> base.inaugurations.inauguration_speaker.inauguration -> Harry S. Truman 1945 presidential inauguration -> time.event.locations -> White House\n# Answer:\nHarry S. Truman 1945 presidential inauguration", "# Reasoning Path:\nHarry S. Truman -> base.inaugurations.inauguration_speaker.inauguration -> Harry S. Truman 1945 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nHarry S. Truman 1945 presidential inauguration", "# Reasoning Path:\nHarry S. Truman -> government.us_president.vice_president -> Alben W. Barkley -> people.person.nationality -> United States of America\n# Answer:\nAlben W. Barkley", "# Reasoning Path:\nHarry S. Truman -> base.inaugurations.inauguration_speaker.inauguration -> Harry S. Truman 1945 presidential inauguration -> common.topic.notable_types -> Event\n# Answer:\nHarry S. Truman 1945 presidential inauguration", "# Reasoning Path:\nHarry S. Truman -> base.inaugurations.inauguration_speaker.inauguration -> Harry S. Truman 1949 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nHarry S. Truman 1949 presidential inauguration", "# Reasoning Path:\nHarry S. Truman -> base.inaugurations.inauguration_speaker.inauguration -> Harry S. Truman 1945 presidential inauguration -> base.culturalevent.event.entity_involved -> Harlan F. Stone\n# Answer:\nHarry S. Truman 1945 presidential inauguration", "# Reasoning Path:\nHarry S. Truman -> book.author.works_written -> A new era in world affairs -> book.written_work.subjects -> Foreign relations\n# Answer:\nA new era in world affairs", "# Reasoning Path:\nHarry S. Truman -> base.inaugurations.inauguration_speaker.inauguration -> Harry S. Truman 1949 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nHarry S. Truman 1949 presidential inauguration", "# Reasoning Path:\nHarry S. Truman -> base.inaugurations.inauguration_speaker.inauguration -> Harry S. Truman 1949 presidential inauguration -> common.topic.notable_types -> Event\n# Answer:\nHarry S. Truman 1949 presidential inauguration", "# Reasoning Path:\nHarry S. Truman -> base.inaugurations.inauguration_speaker.inauguration -> Harry S. Truman 1949 presidential inauguration -> common.topic.notable_for -> g.1257xzm2t\n# Answer:\nHarry S. Truman 1949 presidential inauguration"], "ground_truth": ["1945-04-12"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1696", "prediction": ["# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Navigator -> people.profession.people_with_this_profession -> Abel Tasman\n# Answer:\nNavigator", "# Reasoning Path:\nFerdinand Magellan -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> g.11btv6zfvr\n# Answer:\nprotected_sites.listed_site.designation_as_natural_or_cultural_site", "# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Navigator -> people.profession.people_with_this_profession -> Ahmad ibn M\u0101jid\n# Answer:\nNavigator", "# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Navigator -> base.descriptive_names.names.descriptive_name -> m.0106708r\n# Answer:\nNavigator", "# Reasoning Path:\nFerdinand Magellan -> symbols.name_source.namesakes -> Magellan -> common.topic.article -> m.01m0j_\n# Answer:\nMagellan", "# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Navigator -> people.profession.people_with_this_profession -> Aksel Magdahl\n# Answer:\nNavigator", "# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Navigator -> people.profession.specializations -> Sailor\n# Answer:\nNavigator", "# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Sailor -> fictional_universe.character_occupation.characters_with_this_occupation -> Jack Aubrey\n# Answer:\nSailor", "# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Sailor -> base.schemastaging.context_name.pronunciation -> g.125_nnp67\n# Answer:\nSailor", "# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Explorer -> base.descriptive_names.names.descriptive_name -> m.0106bf9p\n# Answer:\nExplorer"], "ground_truth": ["Explorer", "Sailor", "Navigator"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1698", "prediction": ["# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Thailand -> location.country.capital -> Bangkok\n# Answer:\nThailand", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Thailand -> location.location.containedby -> Southeast Asia\n# Answer:\nThailand", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Thailand -> location.country.official_language -> Thai Language\n# Answer:\nThailand", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Thailand -> location.location.containedby -> Eurasia\n# Answer:\nThailand", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Thailand -> location.location.containedby -> Asia\n# Answer:\nThailand", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Myanmar -> location.statistical_region.consumer_price_index -> g.11b60wz9qs\n# Answer:\nMyanmar", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Myanmar -> location.statistical_region.consumer_price_index -> g.1hhc37x6b\n# Answer:\nMyanmar", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Myanmar -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_mygc\n# Answer:\nMyanmar", "# Reasoning Path:\nBurmese cat -> base.petbreeds.cat_breed.temperament -> Friendly -> common.topic.notable_types -> Literature Subject\n# Answer:\nFriendly", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Myanmar -> location.statistical_region.consumer_price_index -> g.1hhc3_4b3\n# Answer:\nMyanmar"], "ground_truth": ["Myanmar", "Thailand"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1699", "prediction": ["# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0hhzbby -> film.performance.actor -> Chris Pine\n# Answer:\nChris Pine", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8ffv -> film.performance.actor -> William Shatner\n# Answer:\nWilliam Shatner", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8g09 -> film.performance.actor -> William Shatner\n# Answer:\nWilliam Shatner", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0hhzbby -> film.performance.film -> Star Trek Into Darkness\n# Answer:\nStar Trek Into Darkness", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8ffv -> film.performance.film -> Star Trek III: The Search for Spock\n# Answer:\nStar Trek III: The Search for Spock", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8g09 -> film.performance.film -> Star Trek VI: The Undiscovered Country\n# Answer:\nStar Trek VI: The Undiscovered Country", "# Reasoning Path:\nJames T. Kirk -> common.topic.notable_types -> Film character -> freebase.type_profile.kind -> Annotation\n# Answer:\nFilm character", "# Reasoning Path:\nJames T. Kirk -> common.topic.notable_types -> Film character -> type.type.expected_by -> Character\n# Answer:\nFilm character", "# Reasoning Path:\nJames T. Kirk -> common.topic.webpage -> m.09wcz0z -> common.webpage.resource -> Wolverines! Chris Hemsworth nabs lead in 'Red Dawn'\n# Answer:\nWolverines! Chris Hemsworth nabs lead in 'Red Dawn'", "# Reasoning Path:\nJames T. Kirk -> common.topic.webpage -> m.09wcz0z -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Jimmy Bennett", "Chris Pine", "William Shatner"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.41379310344827586, "ans_precission": 0.3, "ans_recall": 0.6666666666666666, "path_f1": 0.24, "path_precision": 0.3, "path_recall": 0.2, "path_ans_f1": 0.41379310344827586, "path_ans_precision": 0.3, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-170", "prediction": ["# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.0glw4lg -> film.performance.actor -> Ian Holm\n# Answer:\nIan Holm", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.02vc8t6 -> film.performance.actor -> Ian Holm\n# Answer:\nIan Holm", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.0gwn5hf -> film.performance.actor -> Ian Holm\n# Answer:\nIan Holm", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.0glw4lg -> film.performance.film -> The Hobbit: An Unexpected Journey\n# Answer:\nThe Hobbit: An Unexpected Journey", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.02vc8t6 -> film.performance.film -> The Lord of the Rings: The Fellowship of the Ring\n# Answer:\nThe Lord of the Rings: The Fellowship of the Ring", "# Reasoning Path:\nOld Bilbo -> fictional_universe.fictional_character.powers_or_abilities -> Invisibility -> book.book_subject.works -> Memoirs of an Invisible Man\n# Answer:\nInvisibility", "# Reasoning Path:\nOld Bilbo -> book.book_character.appears_in_book -> The Hobbit -> book.book_edition.author_editor -> J. R. R. Tolkien\n# Answer:\nThe Hobbit", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.0gwn5hf -> film.performance.film -> The Hobbit: The Desolation of Smaug\n# Answer:\nThe Hobbit: The Desolation of Smaug", "# Reasoning Path:\nOld Bilbo -> fictional_universe.fictional_character.powers_or_abilities -> Invisibility -> fictional_universe.character_powers.characters_with_this_ability -> Ray\n# Answer:\nInvisibility", "# Reasoning Path:\nOld Bilbo -> fictional_universe.fictional_character.powers_or_abilities -> Invisibility -> film.film_subject.films -> The Unknown Purple\n# Answer:\nInvisibility"], "ground_truth": ["Norman Bird"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1700", "prediction": ["# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.containedby -> Washington\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.containedby -> Portland-Vancouver-Beaverton, OR-WA Metropolitan Statistical Area\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.contains -> Mount St. Helens National Volcanic Monument\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.containedby -> Contiguous United States\n# Answer:\nWashington", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.partially_contains -> White Salmon River\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> geography.mountain.mountain_range -> Cascade Range -> location.location.containedby -> North America\n# Answer:\nCascade Range", "# Reasoning Path:\nMount St. Helens -> common.image.size -> m.02bkwf9\n# Answer:\ncommon.image.size", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.contains -> Skamania\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.containedby -> Northwestern United States\n# Answer:\nWashington"], "ground_truth": ["Skamania County", "United States of America", "Washington", "Cascade Range", "North America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8470588235294118, "ans_precission": 0.9, "ans_recall": 0.8, "path_f1": 0.6857142857142857, "path_precision": 0.8, "path_recall": 0.6, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1701", "prediction": ["# Reasoning Path:\nSaint Paul -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nSaint Paul -> location.location.containedby -> Ramsey County -> location.location.containedby -> Minnesota\n# Answer:\nRamsey County", "# Reasoning Path:\nSaint Paul -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Minnesota\n# Answer:\nUnited States of America", "# Reasoning Path:\nSaint Paul -> location.location.containedby -> Area code 651 -> location.location.containedby -> Minnesota\n# Answer:\nArea code 651", "# Reasoning Path:\nSaint Paul -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nSaint Paul -> location.location.containedby -> Ramsey County -> location.location.containedby -> Minneapolis-St. Paul-Bloomington, MN-WI Metropolitan Statistical Area\n# Answer:\nRamsey County", "# Reasoning Path:\nSaint Paul -> location.citytown.postal_codes -> 55101 -> location.location.containedby -> United States of America\n# Answer:\n55101", "# Reasoning Path:\nSaint Paul -> location.statistical_region.population -> g.11b66dwnhr\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSaint Paul -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Alaska\n# Answer:\nUnited States of America", "# Reasoning Path:\nSaint Paul -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Minnesota\n# Answer:\nUnited States of America"], "ground_truth": ["Ramsey County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1702", "prediction": ["# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> common.topic.article -> m.02sx6l\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.spouse_s -> m.03mlj1z\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> common.topic.article -> m.0hn9rcf\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.place_of_birth -> Blackmore\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Edward VI of England -> people.person.religion -> Protestantism\n# Answer:\nEdward VI of England", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> people.person.nationality -> Kingdom of England\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> symbols.name_source.namesakes -> King Henry's Drive tram stop -> common.topic.image -> King Henry's Drive tramstop look south\n# Answer:\nKing Henry's Drive tram stop", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> royalty.noble_person.titles -> m.0hqncyh\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Edward VI of England -> people.deceased_person.place_of_burial -> Henry VII Chapel\n# Answer:\nEdward VI of England", "# Reasoning Path:\nHenry VIII of England -> symbols.name_source.namesakes -> King Henry's Drive tram stop -> common.topic.notable_for -> g.1257k6j6b\n# Answer:\nKing Henry's Drive tram stop"], "ground_truth": ["Edward VI of England", "Henry, Duke of Cornwall", "Henry FitzRoy, 1st Duke of Richmond and Somerset"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1705", "prediction": ["# Reasoning Path:\nFranz Ferdinand -> award.award_nominee.award_nominations -> m.0_l3qm4 -> award.award_nomination.award -> NME Award for Best Music Video\n# Answer:\nNME Award for Best Music Video", "# Reasoning Path:\nFranz Ferdinand -> award.award_nominee.award_nominations -> m.0_l3qm4 -> award.award_nomination.nominated_for -> Do You Want To\n# Answer:\nDo You Want To", "# Reasoning Path:\nFranz Ferdinand -> music.featured_artist.recordings -> The Dark of the Matinee -> music.recording.tracks -> The Dark of the Matin\u00e9e\n# Answer:\nThe Dark of the Matinee", "# Reasoning Path:\nFranz Ferdinand -> award.award_nominee.award_nominations -> m.0_l54ck -> freebase.valuenotation.is_reviewed -> Award\n# Answer:\nAward", "# Reasoning Path:\nFranz Ferdinand -> award.award_nominee.award_nominations -> m.0_l54ck -> award.award_nomination.award -> NME Award for Best Live Band\n# Answer:\nNME Award for Best Live Band", "# Reasoning Path:\nFranz Ferdinand -> music.artist.album -> 2004-11-15: Heineken Music Hall, Amsterdam, Netherlands -> music.album.album_content_type -> Live Album\n# Answer:\n2004-11-15: Heineken Music Hall, Amsterdam, Netherlands", "# Reasoning Path:\nFranz Ferdinand -> music.artist.album -> 2009-03-05: Glasgow Barrowlands, Scotland, UK -> music.album.album_content_type -> Live Album\n# Answer:\n2009-03-05: Glasgow Barrowlands, Scotland, UK", "# Reasoning Path:\nFranz Ferdinand -> award.award_nominee.award_nominations -> m.0_l6djw -> freebase.valuenotation.is_reviewed -> Award\n# Answer:\nAward", "# Reasoning Path:\nFranz Ferdinand -> award.award_nominee.award_nominations -> m.0_l6djw -> award.award_nomination.nominated_for -> Do You Want To\n# Answer:\nDo You Want To", "# Reasoning Path:\nFranz Ferdinand -> music.artist.album -> 2004-11-15: Heineken Music Hall, Amsterdam, Netherlands -> freebase.valuenotation.has_value -> Initial release date\n# Answer:\n2004-11-15: Heineken Music Hall, Amsterdam, Netherlands"], "ground_truth": ["Glasgow"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1706", "prediction": ["# Reasoning Path:\nBenedict Arnold -> base.honouriam.dishonoured_person.dishonor_bestowed -> Traitor\n# Answer:\nTraitor", "# Reasoning Path:\nBenedict Arnold -> common.topic.notable_types -> Military Commander -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nMilitary Commander", "# Reasoning Path:\nBenedict Arnold -> common.topic.notable_types -> Military Commander -> freebase.type_profile.strict_included_types -> Person\n# Answer:\nMilitary Commander", "# Reasoning Path:\nBenedict Arnold -> common.topic.notable_types -> Military Commander -> freebase.type_hints.included_types -> Topic\n# Answer:\nMilitary Commander", "# Reasoning Path:\nBenedict Arnold -> common.topic.notable_types -> Film character -> freebase.type_profile.published -> Published\n# Answer:\nFilm character", "# Reasoning Path:\nBenedict Arnold -> common.topic.notable_types -> Military Commander -> type.type.properties -> Military Commands\n# Answer:\nMilitary Commander", "# Reasoning Path:\nBenedict Arnold -> common.topic.notable_types -> Military Commander -> freebase.type_hints.included_types -> Person\n# Answer:\nMilitary Commander", "# Reasoning Path:\nBenedict Arnold -> common.topic.notable_types -> Film character -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nFilm character", "# Reasoning Path:\nBenedict Arnold -> military.military_commander.military_commands -> m.049yl9_ -> military.military_command.military_conflict -> Battle of Quebec\n# Answer:\nBattle of Quebec", "# Reasoning Path:\nBenedict Arnold -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Topic\n# Answer:\nFilm character"], "ground_truth": ["Traitor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1707", "prediction": ["# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.03bxf_b -> tv.regular_tv_appearance.actor -> Michael Rosenbaum\n# Answer:\nMichael Rosenbaum", "# Reasoning Path:\nSmallville -> award.award_nominated_work.award_nominations -> m.0n4pm1k -> award.award_nomination.award_nominee -> Michael Rosenbaum\n# Answer:\nMichael Rosenbaum", "# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.03bxf_b -> tv.regular_tv_appearance.seasons -> Smallville - Season 9\n# Answer:\nSmallville - Season 9", "# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.02t8f_n -> tv.regular_tv_appearance.actor -> Ian Somerhalder\n# Answer:\nIan Somerhalder", "# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.02t8q51 -> tv.regular_tv_appearance.actor -> John Glover\n# Answer:\nJohn Glover", "# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.03bxf_b -> tv.regular_tv_appearance.seasons -> Smallville - Season 1\n# Answer:\nSmallville - Season 1", "# Reasoning Path:\nSmallville -> award.award_nominated_work.award_nominations -> m.0n4pm1k -> award.award_nomination.award -> Satellite Award for Best Supporting Actor - Drama Series\n# Answer:\nSatellite Award for Best Supporting Actor - Drama Series", "# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.02t8f_n -> tv.regular_tv_appearance.seasons -> Smallville - Season 3\n# Answer:\nSmallville - Season 3", "# Reasoning Path:\nSmallville -> award.award_nominated_work.award_nominations -> m.099sq7_ -> award.award_nomination.award_nominee -> Miles Millar\n# Answer:\nMiles Millar", "# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.03bxf_b -> tv.regular_tv_appearance.seasons -> Smallville - Season 10\n# Answer:\nSmallville - Season 10"], "ground_truth": ["Michael Rosenbaum"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.23529411764705882, "path_precision": 0.2, "path_recall": 0.2857142857142857, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1708", "prediction": ["# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Royal blue -> common.topic.notable_types -> Color\n# Answer:\nRoyal blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Royal blue -> common.topic.notable_for -> g.1255wtr8_\n# Answer:\nRoyal blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Silver -> base.schemastaging.context_name.pronunciation -> g.125_rvx57\n# Answer:\nSilver", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Royal blue -> common.topic.article -> m.06kqt8\n# Answer:\nRoyal blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Navy Blue -> base.schemastaging.visual_color_extra.srgb -> m.010q_4pn\n# Answer:\nNavy Blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Silver -> common.topic.notable_types -> Color\n# Answer:\nSilver", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Silver -> common.topic.notable_for -> g.125g6j40s\n# Answer:\nSilver", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Navy Blue -> base.schemastaging.visual_color_extra.cmyk -> m.010q_4qh\n# Answer:\nNavy Blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Navy Blue -> common.topic.notable_types -> Color\n# Answer:\nNavy Blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.season_record -> m.075fj_6 -> sports.sports_team_season_record.season -> 1984 NFL season\n# Answer:\n1984 NFL season"], "ground_truth": ["White", "Silver", "Royal blue", "Blue", "Navy Blue"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.8470588235294118, "ans_precission": 0.9, "ans_recall": 0.8, "path_f1": 0.7200000000000001, "path_precision": 0.9, "path_recall": 0.6, "path_ans_f1": 0.8470588235294118, "path_ans_precision": 0.9, "path_ans_recall": 0.8}
{"id": "WebQTest-171", "prediction": ["# Reasoning Path:\nDestin -> location.location.nearby_airports -> Destin\u2013Fort Walton Beach Airport -> aviation.airport.serves -> Fort Walton Beach\n# Answer:\nDestin\u2013Fort Walton Beach Airport", "# Reasoning Path:\nDestin -> location.location.nearby_airports -> Destin\u2013Fort Walton Beach Airport -> common.topic.article -> m.01q6d5\n# Answer:\nDestin\u2013Fort Walton Beach Airport", "# Reasoning Path:\nDestin -> location.location.nearby_airports -> Destin\u2013Fort Walton Beach Airport -> location.location.containedby -> Eglin Air Force Base\n# Answer:\nDestin\u2013Fort Walton Beach Airport", "# Reasoning Path:\nDestin -> location.location.nearby_airports -> Destin\u2013Fort Walton Beach Airport -> aviation.airport.serves -> Valparaiso\n# Answer:\nDestin\u2013Fort Walton Beach Airport", "# Reasoning Path:\nDestin -> location.location.nearby_airports -> Destin Executive Airport -> aviation.airport.hub_for -> Southern Airways Express\n# Answer:\nDestin Executive Airport", "# Reasoning Path:\nDestin -> travel.travel_destination.tourist_attractions -> Big Kahuna's -> common.topic.article -> m.0dz54q\n# Answer:\nBig Kahuna's", "# Reasoning Path:\nDestin -> travel.travel_destination.tourist_attractions -> Big Kahuna's -> location.location.geolocation -> m.0wmx861\n# Answer:\nBig Kahuna's", "# Reasoning Path:\nDestin -> location.location.nearby_airports -> Destin Executive Airport -> location.location.containedby -> 32541\n# Answer:\nDestin Executive Airport", "# Reasoning Path:\nDestin -> location.location.contains -> Pompano Joe's -> common.topic.notable_types -> Venue\n# Answer:\nPompano Joe's", "# Reasoning Path:\nDestin -> travel.travel_destination.tourist_attractions -> Destin History & Fishing Museum -> common.topic.notable_types -> Museum\n# Answer:\nDestin History & Fishing Museum"], "ground_truth": ["Destin\u2013Fort Walton Beach Airport", "Destin Executive Airport"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1710", "prediction": ["# Reasoning Path:\nCzech Republic -> location.country.currency_used -> Czech koruna -> common.topic.notable_types -> Currency\n# Answer:\nCzech koruna", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> language.human_language.countries_spoken_in -> Czechoslovakia\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> language.human_language.countries_spoken_in -> Croatia\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> Beskids -> location.location.partially_containedby -> Ukraine\n# Answer:\nBeskids", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> media_common.netflix_genre.titles -> Adelheid\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> Beskids -> location.location.partially_contained_by -> m.0wg8sdh\n# Answer:\nBeskids", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> B\u00f3br -> geography.river.mouth -> Oder\n# Answer:\nB\u00f3br", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> media_common.netflix_genre.titles -> All My Loved Ones\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> Beskids -> location.location.partially_containedby -> Poland\n# Answer:\nBeskids", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> B\u00f3br -> location.location.partially_containedby -> Poland\n# Answer:\nB\u00f3br"], "ground_truth": ["Czech koruna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1711", "prediction": ["# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup Finals\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> time.event.instance_of_recurring_event -> Stanley Cup Finals\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2012 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n2012 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> ice_hockey.hockey_team.division -> Pacific Division -> common.topic.notable_types -> Ice hockey division\n# Answer:\nPacific Division", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2012 Stanley Cup Finals -> time.event.locations -> Staples Center\n# Answer:\n2012 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2012 Stanley Cup Finals -> common.topic.article -> m.0jt52q2\n# Answer:\n2012 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> ice_hockey.hockey_team.division -> Pacific Division -> common.topic.notable_for -> g.1256g4cnw\n# Answer:\nPacific Division", "# Reasoning Path:\nLos Angeles Kings -> award.award_nominee.award_nominations -> m.0z1ycgx -> award.award_nomination.award -> Outstanding Team ESPY Award\n# Answer:\nOutstanding Team ESPY Award"], "ground_truth": ["2014 Stanley Cup Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1712", "prediction": ["# Reasoning Path:\nSpencer Pratt -> people.person.education -> m.0n1cn_d -> education.education.institution -> University of Southern California\n# Answer:\nUniversity of Southern California", "# Reasoning Path:\nSpencer Pratt -> base.popstra.celebrity.dated -> m.065pyr3 -> base.popstra.dated.participant -> Heidi Montag\n# Answer:\nHeidi Montag", "# Reasoning Path:\nSpencer Pratt -> tv.tv_actor.guest_roles -> m.0bmrvc1 -> tv.tv_guest_role.episodes_appeared_in -> Benefits\n# Answer:\nBenefits", "# Reasoning Path:\nSpencer Pratt -> base.popstra.celebrity.dated -> m.065pxjn -> base.popstra.dated.participant -> Kelly Durbin\n# Answer:\nKelly Durbin", "# Reasoning Path:\nSpencer Pratt -> tv.tv_actor.guest_roles -> m.09nzmv0 -> tv.tv_guest_role.episodes_appeared_in -> Heidi and Spencer / Miss USA 2008\n# Answer:\nHeidi and Spencer / Miss USA 2008", "# Reasoning Path:\nSpencer Pratt -> tv.tv_actor.guest_roles -> m.09nzmv5 -> tv.tv_guest_role.episodes_appeared_in -> Whoopi Goldberg , Spencer Pratt, Nick Griffin\n# Answer:\nWhoopi Goldberg , Spencer Pratt, Nick Griffin"], "ground_truth": ["University of Southern California"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 0.16666666666666666, "ans_recall": 1.0, "path_f1": 0.2857142857142857, "path_precision": 0.16666666666666666, "path_recall": 1.0, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-1713", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Strabismus -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nStrabismus", "# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Smallpox -> medicine.disease.notable_people_with_this_condition -> George Washington\n# Answer:\nSmallpox", "# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Marfan syndrome -> medicine.disease.notable_people_with_this_condition -> Vincent Schiavelli\n# Answer:\nMarfan syndrome", "# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Marfan syndrome -> symbols.namesake.named_after -> Antoine Marfan\n# Answer:\nMarfan syndrome", "# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Smallpox -> medicine.disease.notable_people_with_this_condition -> Andrew Jackson\n# Answer:\nSmallpox", "# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Smallpox -> medicine.icd_9_cm_classification.includes_classifications -> Alastrim\n# Answer:\nSmallpox", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pgr_5 -> people.place_lived.location -> Kentucky\n# Answer:\nKentucky", "# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Smallpox -> medicine.disease.includes_diseases -> Alastrim\n# Answer:\nSmallpox", "# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Marfan syndrome -> medicine.disease.notable_people_with_this_condition -> Austin Carlile\n# Answer:\nMarfan syndrome", "# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Smallpox -> medicine.icd_9_cm_classification.includes_classifications -> Modified smallpox\n# Answer:\nSmallpox"], "ground_truth": ["Strabismus", "Marfan syndrome", "Smallpox"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1714", "prediction": ["# Reasoning Path:\nVancouver Canucks -> sports.sports_team.coaches -> m.0113n9pr -> sports.sports_team_coach_tenure.coach -> Doug Lidster\n# Answer:\nDoug Lidster", "# Reasoning Path:\nVancouver Canucks -> sports.sports_team.coaches -> m.0w1yrsm -> sports.sports_team_coach_tenure.coach -> John Tortorella\n# Answer:\nJohn Tortorella", "# Reasoning Path:\nVancouver Canucks -> sports.sports_team.coaches -> m.0113n9pr -> sports.sports_team_coach_tenure.position -> Assistant Coach\n# Answer:\nAssistant Coach", "# Reasoning Path:\nVancouver Canucks -> sports.sports_team.coaches -> m.010yzbv3 -> sports.sports_team_coach_tenure.coach -> Willie Desjardins\n# Answer:\nWillie Desjardins", "# Reasoning Path:\nVancouver Canucks -> ice_hockey.hockey_team.coach -> Willie Desjardins -> people.person.place_of_birth -> Climax\n# Answer:\nWillie Desjardins", "# Reasoning Path:\nVancouver Canucks -> sports.sports_team.coaches -> m.0w1yrsm -> sports.sports_team_coach_tenure.position -> Head coach\n# Answer:\nHead coach", "# Reasoning Path:\nVancouver Canucks -> ice_hockey.hockey_team.coach -> Willie Desjardins -> common.topic.notable_for -> g.1259nc0p2\n# Answer:\nWillie Desjardins", "# Reasoning Path:\nVancouver Canucks -> ice_hockey.hockey_team.coach -> Willie Desjardins -> people.person.gender -> Male\n# Answer:\nWillie Desjardins", "# Reasoning Path:\nVancouver Canucks -> sports.sports_team.coaches -> m.010yzbv3 -> sports.sports_team_coach_tenure.position -> Head coach\n# Answer:\nHead coach", "# Reasoning Path:\nVancouver Canucks -> sports.sports_team.sport -> Ice Hockey -> olympics.olympic_sport.olympic_games_contested -> 1920 Summer Olympics\n# Answer:\nIce Hockey"], "ground_truth": ["Alain Vigneault", "Willie Desjardins", "John Tortorella", "Doug Lidster"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.6666666666666665, "ans_precission": 0.6, "ans_recall": 0.75, "path_f1": 0.6, "path_precision": 0.5, "path_recall": 0.75, "path_ans_f1": 0.6666666666666665, "path_ans_precision": 0.6, "path_ans_recall": 0.75}
{"id": "WebQTest-1715", "prediction": ["# Reasoning Path:\nC. S. Lewis -> people.person.place_of_birth -> Belfast -> location.location.containedby -> Northern Ireland\n# Answer:\nBelfast", "# Reasoning Path:\nC. S. Lewis -> influence.influence_node.influenced_by -> William Morris -> influence.influence_node.influenced_by -> John Ruskin\n# Answer:\nWilliam Morris", "# Reasoning Path:\nC. S. Lewis -> influence.influence_node.influenced_by -> G. K. Chesterton -> people.person.place_of_birth -> Kensington\n# Answer:\nG. K. Chesterton", "# Reasoning Path:\nC. S. Lewis -> people.person.places_lived -> m.04hm5pn -> people.place_lived.location -> Belfast\n# Answer:\nBelfast", "# Reasoning Path:\nC. S. Lewis -> people.person.place_of_birth -> Belfast -> location.location.containedby -> United Kingdom\n# Answer:\nBelfast", "# Reasoning Path:\nC. S. Lewis -> people.person.place_of_birth -> Belfast -> travel.travel_destination.tourist_attractions -> Belfast Castle\n# Answer:\nBelfast", "# Reasoning Path:\nC. S. Lewis -> influence.influence_node.influenced_by -> William Morris -> people.person.profession -> Visual Artist\n# Answer:\nWilliam Morris", "# Reasoning Path:\nC. S. Lewis -> people.person.place_of_birth -> Belfast -> location.location.contains -> Waterfront Hall\n# Answer:\nBelfast", "# Reasoning Path:\nC. S. Lewis -> people.person.place_of_birth -> Belfast -> travel.travel_destination.tourist_attractions -> Belfast City Hall\n# Answer:\nBelfast", "# Reasoning Path:\nC. S. Lewis -> influence.influence_node.influenced_by -> G. K. Chesterton -> influence.influence_node.influenced_by -> William Cobbett\n# Answer:\nG. K. Chesterton"], "ground_truth": ["Belfast"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1716", "prediction": ["# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.nationality -> Dominican Republic -> location.country.administrative_divisions -> Azua Province\n# Answer:\nDominican Republic", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.place_of_birth -> Santo Domingo -> base.biblioness.bibs_location.country -> Dominican Republic\n# Answer:\nSanto Domingo", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.nationality -> Dominican Republic -> location.country.administrative_divisions -> Baoruco Province\n# Answer:\nDominican Republic", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.nationality -> Dominican Republic -> base.aareas.schema.administrative_area.administrative_children -> Azua Province\n# Answer:\nDominican Republic", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.place_of_birth -> Santo Domingo -> location.location.containedby -> Dominican Republic\n# Answer:\nSanto Domingo", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.nationality -> Dominican Republic -> location.country.administrative_divisions -> Barahona Province\n# Answer:\nDominican Republic", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.nationality -> Dominican Republic -> location.country.first_level_divisions -> Azua Province\n# Answer:\nDominican Republic", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.nationality -> Dominican Republic -> base.aareas.schema.administrative_area.administrative_children -> Baoruco Province\n# Answer:\nDominican Republic", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.place_of_birth -> Santo Domingo -> location.location.containedby -> Hispaniola\n# Answer:\nSanto Domingo", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.nationality -> Dominican Republic -> base.aareas.schema.administrative_area.administrative_children -> Barahona Province\n# Answer:\nDominican Republic"], "ground_truth": ["Dominican Republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1719", "prediction": ["# Reasoning Path:\nJane Wyman -> people.person.spouse_s -> m.0hmv78j -> people.marriage.spouse -> Fred Karger\n# Answer:\nFred Karger", "# Reasoning Path:\nJane Wyman -> people.person.spouse_s -> m.0hmv78v -> people.marriage.spouse -> Myron Futterman\n# Answer:\nMyron Futterman", "# Reasoning Path:\nJane Wyman -> people.person.spouse_s -> m.0hmv798 -> people.marriage.spouse -> Fred Karger\n# Answer:\nFred Karger", "# Reasoning Path:\nJane Wyman -> film.actor.film -> g.11b6s3lhg1\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nJane Wyman -> people.person.spouse_s -> m.0hmv78j -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nJane Wyman -> people.person.spouse_s -> m.0hmv78j -> people.marriage.location_of_ceremony -> Santa Barbara\n# Answer:\nSanta Barbara", "# Reasoning Path:\nJane Wyman -> film.actor.film -> g.11btym0bwy\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nJane Wyman -> people.person.spouse_s -> m.0hmv78v -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nJane Wyman -> film.actor.film -> g.11bv5s4kzr\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nJane Wyman -> people.person.spouse_s -> m.0hmv78v -> people.marriage.location_of_ceremony -> New Orleans\n# Answer:\nNew Orleans"], "ground_truth": ["Fred Karger"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-172", "prediction": ["# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> United States Senate majority leader -> government.government_office_or_title.office_holders -> m.052jk8c\n# Answer:\nUnited States Senate majority leader", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> United States Senator -> government.government_office_or_title.governmental_body_if_any -> United States Congress\n# Answer:\nUnited States Senator", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> United States Senate majority leader -> government.government_office_or_title.office_holders -> m.052jk8v\n# Answer:\nUnited States Senate majority leader", "# Reasoning Path:\nUnited States Senate -> common.topic.image -> 600px-Senate_Seal.svg.png -> common.image.appears_in_topic_gallery -> United States Senator\n# Answer:\n600px-Senate_Seal.svg.png", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> United States Senate majority leader -> government.government_office_or_title.office_holders -> m.05k9hxg\n# Answer:\nUnited States Senate majority leader", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> United States Senate majority leader -> common.topic.notable_types -> Government Office or Title\n# Answer:\nUnited States Senate majority leader", "# Reasoning Path:\nUnited States Senate -> common.topic.image -> 600px-Senate_Seal.svg.png -> common.image.size -> m.01x3k95\n# Answer:\n600px-Senate_Seal.svg.png", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> United States Senator -> fictional_universe.character_occupation.characters_with_this_occupation -> Senator Ross Garrison\n# Answer:\nUnited States Senator", "# Reasoning Path:\nUnited States Senate -> common.topic.webpage -> m.03ldgbc -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> United States Senator -> base.descriptive_names.names.descriptive_name -> m.0100zlls\n# Answer:\nUnited States Senator"], "ground_truth": ["Chief of staff", "Senate majority leader", "State Senator", "Senator", "President of the Senate", "President pro tempore"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.45161290322580644, "ans_precission": 0.7, "ans_recall": 0.3333333333333333, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0.47058823529411764, "path_ans_precision": 0.8, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1721", "prediction": ["# Reasoning Path:\nPeru -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nPeru -> location.country.languages_spoken -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nPeru -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nPeru -> location.country.official_language -> Quechuan languages -> language.language_family.geographic_distribution -> Ecuador\n# Answer:\nQuechuan languages", "# Reasoning Path:\nPeru -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Argentina\n# Answer:\nSpanish Language", "# Reasoning Path:\nPeru -> location.country.official_language -> Quechuan languages -> language.human_language.countries_spoken_in -> Argentina\n# Answer:\nQuechuan languages", "# Reasoning Path:\nPeru -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.11b71n53cl\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nPeru -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Bolivia\n# Answer:\nSpanish Language", "# Reasoning Path:\nPeru -> location.country.languages_spoken -> Puquina Language -> language.human_language.countries_spoken_in -> Bolivia\n# Answer:\nPuquina Language"], "ground_truth": ["Puquina Language", "Omagua dialect", "Waorani Language", "Aymara language", "Spanish Language", "Quechuan languages", "Ayacucho Quechua"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5806451612903225, "ans_precission": 0.9, "ans_recall": 0.42857142857142855, "path_f1": 0.5581395348837209, "path_precision": 0.8, "path_recall": 0.42857142857142855, "path_ans_f1": 0.5806451612903225, "path_ans_precision": 0.9, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1723", "prediction": ["# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.location.containedby -> North America\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.locations.countries.continent -> North America\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> New Zealand -> location.country.languages_spoken -> Standard Chinese\n# Answer:\nNew Zealand", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Zimbabwe -> base.locations.countries.continent -> Africa\n# Answer:\nZimbabwe", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.location.containedby -> Americas\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> New Zealand -> location.location.containedby -> Oceania\n# Answer:\nNew Zealand", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Zimbabwe -> location.location.containedby -> Africa\n# Answer:\nZimbabwe", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> New Zealand -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nNew Zealand", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> United States of America -> location.location.primarily_containedby -> North America\n# Answer:\nUnited States of America"], "ground_truth": ["Montserrat", "Mandatory Palestine", "Hong Kong", "Republic of Ireland", "Brunei", "South Africa", "South Yemen", "Cura\u00e7ao", "Sierra Leone", "Kenya", "Honduras", "Gazankulu", "Pakistan", "Cameroon", "Tuvalu", "Transkei", "Cyprus", "Isle of Man", "Saint Lucia", "Vatican City", "China", "Japan", "Vanuatu", "Sri Lanka", "Bonaire", "Rwanda", "Indonesia", "Samoa", "Papua New Guinea", "Fiji", "Territory of Papua and New Guinea", "Antigua and Barbuda", "Zimbabwe", "Swaziland", "England", "Ethiopia", "Uganda", "Nigeria", "Tanzania", "Turks and Caicos Islands", "Marshall Islands", "Belize", "Guam", "Gibraltar", "Australia", "Philippines", "United Kingdom", "Qatar", "Laos", "Malaysia", "Cook Islands", "New Zealand", "Saint Vincent and the Grenadines", "Kiribati", "Kingdom of Great Britain", "United States of America", "Bahamas", "Sudan", "India", "Lesotho", "Guyana", "Israel", "Puerto Rico", "Bangladesh", "Jersey", "Jordan", "Bermuda", "Malta", "Singapore", "Cayman Islands", "Canada", "Gambia", "Saint Kitts and Nevis", "Namibia", "Dominica", "Nauru", "Ghana", "Liberia", "Timor-Leste", "Zambia", "Barbados", "Tokelau", "State of Palestine", "Grenada", "Botswana", "Wales", "Territory of New Guinea"], "ans_acc": 0.04597701149425287, "ans_hit": 1, "ans_f1": 0.08791208791208792, "ans_precission": 1.0, "ans_recall": 0.04597701149425287, "path_f1": 0.08791208791208792, "path_precision": 1.0, "path_recall": 0.04597701149425287, "path_ans_f1": 0.08791208791208792, "path_ans_precision": 1.0, "path_ans_recall": 0.04597701149425287}
{"id": "WebQTest-1724", "prediction": ["# Reasoning Path:\nArsenal F.C. -> sports.sports_team.championships -> 1929\u201330 FA Cup -> time.event.next_in_series -> 1930\u201331 FA Cup\n# Answer:\n1929\u201330 FA Cup", "# Reasoning Path:\nArsenal F.C. -> sports.sports_team.championships -> 1929\u201330 FA Cup -> time.event.previous_in_series -> 1928\u201329 FA Cup\n# Answer:\n1929\u201330 FA Cup", "# Reasoning Path:\nArsenal F.C. -> sports.sports_team.championships -> 1935\u201336 FA Cup -> common.topic.notable_for -> g.1259xm0g8\n# Answer:\n1935\u201336 FA Cup", "# Reasoning Path:\nArsenal F.C. -> soccer.football_team.player_statistics -> m.0w8_167 -> soccer.football_player_stats.player -> Nacer Barazite\n# Answer:\nNacer Barazite", "# Reasoning Path:\nArsenal F.C. -> sports.sports_team.championships -> 1935\u201336 FA Cup -> time.event.previous_in_series -> 1934\u201335 FA Cup\n# Answer:\n1935\u201336 FA Cup", "# Reasoning Path:\nArsenal F.C. -> sports.sports_team.championships -> 1949\u201350 FA Cup -> sports.sports_league_season.championship -> 1950 FA Cup Final\n# Answer:\n1949\u201350 FA Cup", "# Reasoning Path:\nArsenal F.C. -> sports.sports_team.championships -> 1949\u201350 FA Cup -> common.topic.notable_types -> Event\n# Answer:\n1949\u201350 FA Cup", "# Reasoning Path:\nArsenal F.C. -> soccer.football_team.player_statistics -> m.0w8_81l -> soccer.football_player_stats.player -> Pedro Roberto Silva Botelho\n# Answer:\nPedro Roberto Silva Botelho", "# Reasoning Path:\nArsenal F.C. -> soccer.football_team.matches -> 1971 FA Cup Final -> soccer.football_match.goals -> m.0bfprp4\n# Answer:\n1971 FA Cup Final", "# Reasoning Path:\nArsenal F.C. -> soccer.football_team.matches -> 1971 FA Cup Final -> common.topic.article -> m.027_52k\n# Answer:\n1971 FA Cup Final"], "ground_truth": ["2001\u201302 FA Cup", "2002\u201303 FA Cup", "2013\u201314 FA Cup", "1978\u201379 FA Cup", "1997\u201398 FA Cup", "1970\u201371 FA Cup", "2004\u201305 FA Cup", "1992\u201393 FA Cup"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1725", "prediction": ["# Reasoning Path:\nPhoenix -> location.location.time_zones -> Mountain Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nPhoenix -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nPhoenix -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nPhoenix -> education.school_mascot.school -> Ypsilanti High School\n# Answer:\nYpsilanti High School", "# Reasoning Path:\nPhoenix -> location.citytown.postal_codes -> 85001 -> common.topic.notable_types -> Postal Code\n# Answer:\n85001", "# Reasoning Path:\nPhoenix -> location.citytown.postal_codes -> 85053 -> common.topic.notable_for -> g.1258hqrj1\n# Answer:\n85053", "# Reasoning Path:\nPhoenix -> location.citytown.postal_codes -> 85053 -> location.postal_code.country -> United States of America\n# Answer:\n85053", "# Reasoning Path:\nPhoenix -> location.citytown.postal_codes -> 85002 -> location.postal_code.country -> United States of America\n# Answer:\n85002", "# Reasoning Path:\nPhoenix -> education.school_mascot.school -> Aquinas University -> education.educational_institution.colors -> Black\n# Answer:\nAquinas University", "# Reasoning Path:\nPhoenix -> location.citytown.postal_codes -> 85001 -> location.postal_code.country -> United States of America\n# Answer:\n85001"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1726", "prediction": ["# Reasoning Path:\nSouth Portland -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nSouth Portland -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Maine\n# Answer:\nUnited States of America", "# Reasoning Path:\nSouth Portland -> location.location.containedby -> Maine -> location.location.containedby -> United States, with Territories\n# Answer:\nMaine", "# Reasoning Path:\nSouth Portland -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Maine\n# Answer:\nUnited States of America", "# Reasoning Path:\nSouth Portland -> location.location.containedby -> Cumberland County -> location.location.containedby -> Maine\n# Answer:\nCumberland County", "# Reasoning Path:\nSouth Portland -> location.hud_county_place.county -> Cumberland County -> location.location.containedby -> Maine\n# Answer:\nCumberland County", "# Reasoning Path:\nSouth Portland -> location.location.containedby -> Maine -> location.location.containedby -> United States of America\n# Answer:\nMaine", "# Reasoning Path:\nSouth Portland -> location.statistical_region.population -> g.11b66h2c5f\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSouth Portland -> location.location.containedby -> Maine -> location.location.containedby -> New England\n# Answer:\nMaine", "# Reasoning Path:\nSouth Portland -> location.location.containedby -> Maine -> base.aareas.schema.administrative_area.capital -> Augusta\n# Answer:\nMaine"], "ground_truth": ["Maine", "Cumberland County", "United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1727", "prediction": ["# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Gabon -> common.topic.notable_types -> Country\n# Answer:\nGabon", "# Reasoning Path:\nFrench -> language.human_language.main_country -> France -> location.country.first_level_divisions -> Martinique\n# Answer:\nFrance", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Gabon -> location.country.form_of_government -> Republic\n# Answer:\nGabon", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Rwanda -> location.country.official_language -> English Language\n# Answer:\nRwanda", "# Reasoning Path:\nFrench -> language.human_language.main_country -> France -> organization.organization_founder.organizations_founded -> World Trade Organization\n# Answer:\nFrance", "# Reasoning Path:\nFrench -> language.human_language.countries_spoken_in -> Rwanda -> location.country.official_language -> English Language\n# Answer:\nRwanda", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Gabon -> location.country.form_of_government -> Presidential system\n# Answer:\nGabon", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Rwanda -> common.topic.notable_types -> Country\n# Answer:\nRwanda", "# Reasoning Path:\nFrench -> language.human_language.main_country -> France -> base.locations.countries.planet -> Earth\n# Answer:\nFrance", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Rwanda -> location.country.form_of_government -> Parliamentary system\n# Answer:\nRwanda"], "ground_truth": ["Seychelles", "Jersey", "Comoros", "Monaco", "Djibouti", "Central African Republic", "France", "Haiti", "Mali", "Guinea", "Equatorial Guinea", "Niger", "Martinique", "Burkina Faso", "Benin", "Switzerland", "Ivory Coast", "Congo", "Senegal", "Luxembourg", "Vanuatu", "Rwanda", "Gabon", "Chad", "Madagascar", "Cameroon", "Burundi", "Belgium"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0.19354838709677416, "ans_precission": 1.0, "ans_recall": 0.10714285714285714, "path_f1": 0.05263157894736841, "path_precision": 0.1, "path_recall": 0.03571428571428571, "path_ans_f1": 0.25, "path_ans_precision": 1.0, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-1728", "prediction": ["# Reasoning Path:\nPtolemy -> people.person.profession -> Mathematician -> fictional_universe.character_occupation.characters_with_this_occupation -> You Bastard\n# Answer:\nMathematician", "# Reasoning Path:\nPtolemy -> people.person.profession -> Mathematician -> fictional_universe.character_occupation.characters_with_this_occupation -> Beremiz Samir\n# Answer:\nMathematician", "# Reasoning Path:\nPtolemy -> people.person.profession -> Mathematician -> base.descriptive_names.names.descriptive_name -> m.01066gkh\n# Answer:\nMathematician", "# Reasoning Path:\nPtolemy -> people.person.profession -> Philosopher -> fictional_universe.character_occupation.characters_with_this_occupation -> Vroomfondle\n# Answer:\nPhilosopher", "# Reasoning Path:\nPtolemy -> people.person.profession -> Mathematician -> fictional_universe.character_occupation.characters_with_this_occupation -> Dyhianna Selei\n# Answer:\nMathematician", "# Reasoning Path:\nPtolemy -> people.person.profession -> Mathematician -> common.topic.image -> GodfreyKneller-IsaacNewton-1689\n# Answer:\nMathematician", "# Reasoning Path:\nPtolemy -> people.person.profession -> Philosopher -> common.topic.notable_for -> g.125h79w9z\n# Answer:\nPhilosopher", "# Reasoning Path:\nPtolemy -> people.person.profession -> Mathematician -> base.descriptive_names.names.descriptive_name -> m.010f3g_r\n# Answer:\nMathematician", "# Reasoning Path:\nPtolemy -> people.person.profession -> Philosopher -> fictional_universe.character_occupation.characters_with_this_occupation -> Bandaran adepts\n# Answer:\nPhilosopher", "# Reasoning Path:\nPtolemy -> people.person.profession -> Philosopher -> base.descriptive_names.names.descriptive_name -> m.0105dm0j\n# Answer:\nPhilosopher"], "ground_truth": ["Astronomer", "Philosopher", "Geographer", "Astrologer", "Mathematician"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 1.0, "ans_recall": 0.4, "path_f1": 0.5714285714285715, "path_precision": 1.0, "path_recall": 0.4, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 1.0, "path_ans_recall": 0.4}
{"id": "WebQTest-1729", "prediction": ["# Reasoning Path:\nHenry VIII of England -> people.person.spouse_s -> m.0j4kx1_ -> people.marriage.spouse -> Anne Boleyn\n# Answer:\nAnne Boleyn", "# Reasoning Path:\nHenry VIII of England -> people.person.spouse_s -> m.0j4kx3p -> people.marriage.spouse -> Catherine of Aragon\n# Answer:\nCatherine of Aragon", "# Reasoning Path:\nHenry VIII of England -> people.person.spouse_s -> m.0j4kx1_ -> people.marriage.location_of_ceremony -> Westminster Abbey\n# Answer:\nWestminster Abbey", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Elizabeth I of England -> people.person.parents -> Anne Boleyn\n# Answer:\nElizabeth I of England", "# Reasoning Path:\nHenry VIII of England -> people.person.spouse_s -> m.0j4kx1_ -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nHenry VIII of England -> people.person.spouse_s -> m.03l46gn -> people.marriage.spouse -> Catherine Parr\n# Answer:\nCatherine Parr", "# Reasoning Path:\nHenry VIII of England -> symbols.name_source.namesakes -> Henry FitzAlan, 19th Earl of Arundel -> people.person.spouse_s -> m.07vyqz1\n# Answer:\nHenry FitzAlan, 19th Earl of Arundel", "# Reasoning Path:\nHenry VIII of England -> people.person.spouse_s -> m.0j4kx3p -> people.marriage.location_of_ceremony -> Palace of Placentia\n# Answer:\nPalace of Placentia", "# Reasoning Path:\nHenry VIII of England -> people.person.spouse_s -> m.0j4kx3p -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> people.person.parents -> Catherine of Aragon\n# Answer:\nHenry, Duke of Cornwall"], "ground_truth": ["Catherine of Aragon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.26666666666666666, "path_precision": 0.2, "path_recall": 0.4, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-173", "prediction": ["# Reasoning Path:\nBaltic states -> location.location.contains -> Estonia -> location.location.containedby -> Eurasia\n# Answer:\nEstonia", "# Reasoning Path:\nBaltic states -> common.topic.image -> The three Baltic states: Estonia, Latvia, and Lithuania -> common.image.appears_in_topic_gallery -> Baltic Air Policing\n# Answer:\nThe three Baltic states: Estonia, Latvia, and Lithuania", "# Reasoning Path:\nBaltic states -> location.location.contains -> Estonia -> location.location.containedby -> Europe\n# Answer:\nEstonia", "# Reasoning Path:\nBaltic states -> location.location.contains -> Estonia -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.1hhc3cztm\n# Answer:\nEstonia", "# Reasoning Path:\nBaltic states -> location.location.contains -> Lithuania -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc388jk\n# Answer:\nLithuania", "# Reasoning Path:\nBaltic states -> location.location.contains -> Estonia -> location.statistical_region.high_tech_as_percent_of_manufactured_exports -> g.1hhc39sn_\n# Answer:\nEstonia", "# Reasoning Path:\nBaltic states -> location.location.contains -> Latvia -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nLatvia", "# Reasoning Path:\nBaltic states -> location.location.contains -> Estonia -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.1hhc3dz61\n# Answer:\nEstonia", "# Reasoning Path:\nBaltic states -> location.location.contains -> Lithuania -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc3bf3d\n# Answer:\nLithuania", "# Reasoning Path:\nBaltic states -> location.location.contains -> Lithuania -> location.location.partiallycontains -> m.0wg8w49\n# Answer:\nLithuania"], "ground_truth": ["Latvia", "Lithuania", "Estonia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1730", "prediction": ["# Reasoning Path:\nParan\u00e1 River -> location.location.containedby -> South America -> location.location.containedby -> Americas\n# Answer:\nSouth America", "# Reasoning Path:\nParan\u00e1 River -> location.location.containedby -> South America -> location.location.containedby -> DVD Region 4\n# Answer:\nSouth America", "# Reasoning Path:\nParan\u00e1 River -> location.location.containedby -> South America -> location.location.containedby -> Western Hemisphere\n# Answer:\nSouth America", "# Reasoning Path:\nParan\u00e1 River -> geography.river.basin_countries -> Paraguay -> location.location.containedby -> South America\n# Answer:\nParaguay", "# Reasoning Path:\nParan\u00e1 River -> location.location.containedby -> South America -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Maricoxi\n# Answer:\nSouth America", "# Reasoning Path:\nParan\u00e1 River -> location.location.containedby -> South America -> book.book_subject.works -> A Brazilian Alphabet for the Younger Reader\n# Answer:\nSouth America", "# Reasoning Path:\nParan\u00e1 River -> geography.river.basin_countries -> Paraguay -> location.location.containedby -> Americas\n# Answer:\nParaguay", "# Reasoning Path:\nParan\u00e1 River -> location.location.containedby -> South America -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Mono Grande\n# Answer:\nSouth America", "# Reasoning Path:\nParan\u00e1 River -> geography.river.basin_countries -> Argentina -> location.location.containedby -> South America\n# Answer:\nArgentina", "# Reasoning Path:\nParan\u00e1 River -> location.location.containedby -> South America -> book.book_subject.works -> A History of Venice\n# Answer:\nSouth America"], "ground_truth": ["South America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1731", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> people.person.children -> Christopher Nixon Cox\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> film.person_or_entity_appearing_in_film.films -> m.0vpghfz\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> business.board_member.organization_board_memberships -> m.05nm0rl\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.children -> Alexander Richard Eisenhower\n# Answer:\nJulie Nixon Eisenhower", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.children -> Jennie Eisenhower\n# Answer:\nJulie Nixon Eisenhower", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.employment_history -> m.0k0dcyp\n# Answer:\nJulie Nixon Eisenhower", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> business.board_member.organization_board_memberships -> m.05nm0rr\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.children -> Melanie Catherine Eisenhower\n# Answer:\nJulie Nixon Eisenhower", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.gender -> Female\n# Answer:\nJulie Nixon Eisenhower"], "ground_truth": ["Richard Nixon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1732", "prediction": ["# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0n0tzlj -> education.education.institution -> Humboldt University of Berlin\n# Answer:\nHumboldt University of Berlin", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmxyk -> education.education.institution -> University of W\u00fcrzburg\n# Answer:\nUniversity of W\u00fcrzburg", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmy28 -> education.education.institution -> University of Bonn\n# Answer:\nUniversity of Bonn", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0n0tzlj -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nTheodor Schwann -> common.topic.image -> Theodore Schwann -> common.image.size -> m.0klz7g\n# Answer:\nTheodore Schwann", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmxyk -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmy28 -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nTheodor Schwann -> base.kwebbase.kwtopic.connections_from -> theodor schwann anticipated by lorenz oken -> base.kwebbase.kwconnection.relation -> anticipated by\n# Answer:\ntheodor schwann anticipated by lorenz oken", "# Reasoning Path:\nTheodor Schwann -> base.kwebbase.kwtopic.connections_from -> theodor schwann anticipated by lorenz oken -> base.kwebbase.kwconnection.sentence -> Oken foreshadowed some of his work.\n# Answer:\ntheodor schwann anticipated by lorenz oken", "# Reasoning Path:\nTheodor Schwann -> base.kwebbase.kwtopic.connections_from -> theodor schwann influenced rudolf carl virchow -> base.kwebbase.kwconnection.sentence -> Influenced Virchow.\n# Answer:\ntheodor schwann influenced rudolf carl virchow"], "ground_truth": ["University of Bonn", "University of W\u00fcrzburg", "Humboldt University of Berlin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1733", "prediction": ["# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> William Morris -> influence.influence_node.influenced_by -> John Ruskin\n# Answer:\nWilliam Morris", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Ebenezer Howard -> influence.influence_node.influenced_by -> Walt Whitman\n# Answer:\nEbenezer Howard", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Henry David Thoreau -> influence.influence_node.influenced_by -> Thomas Carlyle\n# Answer:\nHenry David Thoreau", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Ebenezer Howard -> influence.influence_node.influenced_by -> Edward Bellamy\n# Answer:\nEbenezer Howard", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> William Morris -> base.kwebbase.kwtopic.connections_from -> william morris a pal of alphonse mucha\n# Answer:\nWilliam Morris", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Ebenezer Howard -> common.topic.notable_types -> Author\n# Answer:\nEbenezer Howard", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> William Morris -> base.kwebbase.kwtopic.has_sentences -> After marrying, Morris commissioned an architect friend to build him a home at Bexleyheath in Kent which became known as the Red House  because it was built of red bricks with no stucco finish.\n# Answer:\nWilliam Morris", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Ebenezer Howard -> influence.influence_node.influenced_by -> Ralph Waldo Emerson\n# Answer:\nEbenezer Howard", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Ebenezer Howard -> symbols.name_source.namesakes -> Howard Centre\n# Answer:\nEbenezer Howard", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Henry David Thoreau -> influence.influence_node.influenced_by -> Charles Darwin\n# Answer:\nHenry David Thoreau"], "ground_truth": ["Elizabeth Gordon", "Bruce Price", "William Morris", "Georges I. Gurdjieff and Thomas De Hartmann", "Louis Sullivan", "Ebenezer Howard", "Henry David Thoreau"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 1.0, "ans_recall": 0.42857142857142855, "path_f1": 0.6, "path_precision": 1.0, "path_recall": 0.42857142857142855, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1734", "prediction": ["# Reasoning Path:\nHouston Rockets -> sports.sports_team.championships -> 1994 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n1994 NBA Finals", "# Reasoning Path:\nHouston Rockets -> sports.sports_team.championships -> 1994 NBA Finals -> sports.sports_championship_event.runner_up -> New York Knicks\n# Answer:\n1994 NBA Finals", "# Reasoning Path:\nHouston Rockets -> sports.sports_team.championships -> 1995 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n1995 NBA Finals", "# Reasoning Path:\nHouston Rockets -> sports.sports_team.roster -> m.010b9n9r -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nHouston Rockets -> sports.sports_team.championships -> 1995 NBA Finals -> sports.sports_championship_event.runner_up -> Orlando Magic\n# Answer:\n1995 NBA Finals", "# Reasoning Path:\nHouston Rockets -> base.sportbase.sport_sport_club.involves_in -> Basketball -> sports.sport.positions -> Small forward\n# Answer:\nBasketball", "# Reasoning Path:\nHouston Rockets -> base.sportbase.sport_sport_club.involves_in -> Basketball -> base.sportbase.sport.played_by_clubs -> Dallas Mavericks\n# Answer:\nBasketball", "# Reasoning Path:\nHouston Rockets -> base.sportbase.sport_sport_club.involves_in -> Basketball -> organization.organization_sector.organizations_in_this_sector -> National Basketball Association\n# Answer:\nBasketball", "# Reasoning Path:\nHouston Rockets -> sports.sports_team.roster -> m.010b9n9r -> sports.sports_team_roster.position -> Power forward\n# Answer:\nPower forward", "# Reasoning Path:\nHouston Rockets -> sports.sports_team.roster -> m.0110wl2p -> sports.sports_team_roster.position -> Power forward\n# Answer:\nPower forward"], "ground_truth": ["1994 NBA Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1735", "prediction": ["# Reasoning Path:\nRussia -> location.country.administrative_divisions -> Central Federal District -> base.aareas.schema.administrative_area.administrative_children -> Moscow\n# Answer:\nCentral Federal District", "# Reasoning Path:\nRussia -> location.country.administrative_divisions -> Adygea -> location.location.containedby -> North Caucasus\n# Answer:\nAdygea", "# Reasoning Path:\nRussia -> base.aareas.schema.administrative_area.administrative_children -> Central Federal District -> base.aareas.schema.administrative_area.administrative_children -> Moscow\n# Answer:\nCentral Federal District", "# Reasoning Path:\nRussia -> location.country.administrative_divisions -> Altai Krai -> location.location.containedby -> Siberian Federal District\n# Answer:\nAltai Krai", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.11b60vv5th\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nRussia -> location.country.administrative_divisions -> Adygea -> location.location.events -> Russo-Circassian War\n# Answer:\nAdygea", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.12tb6gh0b\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nRussia -> location.country.administrative_divisions -> Altai Krai -> base.aareas.schema.administrative_area.administrative_area_type -> Russian krai\n# Answer:\nAltai Krai", "# Reasoning Path:\nRussia -> base.aareas.schema.administrative_area.administrative_children -> Northwestern Federal District -> base.aareas.schema.administrative_area.administrative_children -> Saint Petersburg\n# Answer:\nNorthwestern Federal District", "# Reasoning Path:\nRussia -> base.aareas.schema.administrative_area.administrative_children -> Far Eastern Federal District -> base.aareas.schema.administrative_area.administrative_area_type -> Russian federal district\n# Answer:\nFar Eastern Federal District"], "ground_truth": ["Central Federal District"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1736", "prediction": ["# Reasoning Path:\nHoly Roman Empire -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nHoly Roman Empire -> location.location.containedby -> Europe -> location.location.partially_contains -> Russia\n# Answer:\nEurope", "# Reasoning Path:\nHoly Roman Empire -> book.book_subject.works -> The Emperor Charles V -> book.written_work.subjects -> Germany\n# Answer:\nThe Emperor Charles V", "# Reasoning Path:\nHoly Roman Empire -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Austria\n# Answer:\nEurope", "# Reasoning Path:\nHoly Roman Empire -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nHoly Roman Empire -> location.location.containedby -> Europe -> location.location.contains_major_portion_of -> Spain\n# Answer:\nEurope", "# Reasoning Path:\nHoly Roman Empire -> book.book_subject.works -> The Emperor Charles V -> book.written_work.subjects -> History\n# Answer:\nThe Emperor Charles V", "# Reasoning Path:\nHoly Roman Empire -> location.location.containedby -> Europe -> location.location.partially_contains -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nHoly Roman Empire -> location.location.containedby -> Europe -> location.location.partially_contains -> France\n# Answer:\nEurope", "# Reasoning Path:\nHoly Roman Empire -> location.location.containedby -> Europe -> location.location.contains_major_portion_of -> France\n# Answer:\nEurope"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1737", "prediction": ["# Reasoning Path:\nLiam Hemsworth -> award.award_nominee.award_nominations -> m.0z83xb2 -> award.award_nomination.nominated_for -> The Hunger Games\n# Answer:\nThe Hunger Games", "# Reasoning Path:\nLiam Hemsworth -> award.award_nominee.award_nominations -> m.0z8jj89 -> award.award_nomination.nominated_for -> The Last Song\n# Answer:\nThe Last Song", "# Reasoning Path:\nLiam Hemsworth -> award.award_winner.awards_won -> m.0z83x7v -> award.award_honor.honored_for -> The Hunger Games\n# Answer:\nThe Hunger Games", "# Reasoning Path:\nLiam Hemsworth -> award.award_nominee.award_nominations -> m.0z83xb2 -> award.award_nomination.award -> Teen Choice Award for Choice Movie Scene Stealer: Male\n# Answer:\nTeen Choice Award for Choice Movie Scene Stealer: Male", "# Reasoning Path:\nLiam Hemsworth -> award.award_nominee.award_nominations -> m.010wr2b8 -> award.award_nomination.nominated_for -> The Hunger Games: Catching Fire\n# Answer:\nThe Hunger Games: Catching Fire", "# Reasoning Path:\nLiam Hemsworth -> award.award_winner.awards_won -> m.0kbb5_j -> award.award_honor.honored_for -> The Hunger Games\n# Answer:\nThe Hunger Games", "# Reasoning Path:\nLiam Hemsworth -> award.award_nominee.award_nominations -> m.0z8jj89 -> award.award_nomination.ceremony -> 2010 Teen Choice Awards\n# Answer:\n2010 Teen Choice Awards", "# Reasoning Path:\nLiam Hemsworth -> award.award_winner.awards_won -> m.0pkpn95 -> award.award_honor.honored_for -> The Hunger Games\n# Answer:\nThe Hunger Games", "# Reasoning Path:\nLiam Hemsworth -> award.award_winner.awards_won -> m.0z83x7v -> award.award_honor.ceremony -> 2012 Teen Choice Awards\n# Answer:\n2012 Teen Choice Awards", "# Reasoning Path:\nLiam Hemsworth -> award.award_nominee.award_nominations -> m.0z8jj89 -> award.award_nomination.award -> Teen Choice Award for Choice Movie: Chemistry\n# Answer:\nTeen Choice Award for Choice Movie: Chemistry"], "ground_truth": ["Timeless", "The Hunger Games", "Empire State", "Triangle", "Arabian Nights", "By Way of Helena", "Knowing", "The Expendables 2", "The Last Song", "The Hunger Games: Mockingjay, Part 1", "Aurora Rising", "Draft:Independence Day 2", "Cut Bank", "Paranoia", "The Dressmaker", "Love and Honor", "The Hunger Games: Catching Fire", "The Hunger Games: Mockingjay, Part 2"], "ans_acc": 0.16666666666666666, "ans_hit": 1, "ans_f1": 0.2608695652173913, "ans_precission": 0.6, "ans_recall": 0.16666666666666666, "path_f1": 0.3, "path_precision": 0.6, "path_recall": 0.2, "path_ans_f1": 0.2608695652173913, "path_ans_precision": 0.6, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTest-1738", "prediction": ["# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.regular_cast -> m.010g2jfs -> tv.regular_tv_appearance.actor -> Amelia Heinle\n# Answer:\nAmelia Heinle", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.regular_cast -> m.010g2x3j -> tv.regular_tv_appearance.actor -> Daniel Polo\n# Answer:\nDaniel Polo", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.regular_cast -> m.010g6jxk -> tv.regular_tv_appearance.actor -> Steve Burton\n# Answer:\nSteve Burton", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.regular_cast -> m.010g2jfs -> tv.regular_tv_appearance.character -> Victoria Newman\n# Answer:\nVictoria Newman", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.recurring_writers -> m.0bmlttr -> tv.tv_program_writer_relationship.writer -> Jim Houghton\n# Answer:\nJim Houghton", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.regular_cast -> m.010g6jxk -> tv.regular_tv_appearance.character -> Dylan McAvoy\n# Answer:\nDylan McAvoy", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.program_creator -> William J. Bell -> tv.tv_program_creator.programs_created -> Another World\n# Answer:\nWilliam J. Bell", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.program_creator -> William J. Bell -> people.person.profession -> Television producer\n# Answer:\nWilliam J. Bell", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.program_creator -> William J. Bell -> award.award_nominee.award_nominations -> m.0n3h18m\n# Answer:\nWilliam J. Bell", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.program_creator -> Lee Phillip Bell -> tv.tv_program_creator.programs_created -> The Bold and the Beautiful\n# Answer:\nLee Phillip Bell"], "ground_truth": ["Joshua Morrow"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1739", "prediction": ["# Reasoning Path:\nSouth Park -> tv.tv_program.program_creator -> Trey Parker -> fictional_universe.fictional_character_creator.fictional_characters_created -> Eric Cartman\n# Answer:\nTrey Parker", "# Reasoning Path:\nSouth Park -> tv.tv_program.program_creator -> Trey Parker -> music.producer.releases_produced -> Chocolate Salty Balls\n# Answer:\nTrey Parker", "# Reasoning Path:\nSouth Park -> tv.tv_program.program_creator -> Matt Stone -> fictional_universe.fictional_character_creator.fictional_characters_created -> Eric Cartman\n# Answer:\nMatt Stone", "# Reasoning Path:\nSouth Park -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> South Park Universe -> fictional_universe.fictional_universe.works_set_here -> South Park: Bigger, Longer & Uncut\n# Answer:\nSouth Park Universe", "# Reasoning Path:\nSouth Park -> tv.tv_program.program_creator -> Trey Parker -> fictional_universe.fictional_character_creator.fictional_characters_created -> Bebe Stevens\n# Answer:\nTrey Parker", "# Reasoning Path:\nSouth Park -> tv.tv_program.program_creator -> Trey Parker -> fictional_universe.fictional_character_creator.fictional_characters_created -> Butters Stotch\n# Answer:\nTrey Parker", "# Reasoning Path:\nSouth Park -> tv.tv_program.program_creator -> Trey Parker -> film.producer.film -> The Spirit of Christmas\n# Answer:\nTrey Parker", "# Reasoning Path:\nSouth Park -> tv.tv_program.languages -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nSouth Park -> tv.tv_program.program_creator -> Matt Stone -> fictional_universe.fictional_character_creator.fictional_characters_created -> Bebe Stevens\n# Answer:\nMatt Stone", "# Reasoning Path:\nSouth Park -> tv.tv_program.program_creator -> Trey Parker -> film.producer.film -> Cannibal! The Musical\n# Answer:\nTrey Parker"], "ground_truth": ["Trey Parker"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-174", "prediction": ["# Reasoning Path:\nColin Kaepernick -> people.person.profession -> American football player -> common.topic.notable_types -> Profession\n# Answer:\nAmerican football player", "# Reasoning Path:\nColin Kaepernick -> common.topic.notable_types -> American football player -> common.topic.notable_types -> Profession\n# Answer:\nAmerican football player", "# Reasoning Path:\nColin Kaepernick -> people.person.profession -> American football player -> people.profession.specialization_of -> Athlete\n# Answer:\nAmerican football player", "# Reasoning Path:\nColin Kaepernick -> people.person.profession -> American football player -> fictional_universe.character_occupation.characters_with_this_occupation -> A. A. Ron Balakay\n# Answer:\nAmerican football player", "# Reasoning Path:\nColin Kaepernick -> people.person.parents -> Heidi Russo -> common.topic.notable_types -> Person\n# Answer:\nHeidi Russo", "# Reasoning Path:\nColin Kaepernick -> people.person.profession -> American football player -> people.profession.specialization_of -> Football player\n# Answer:\nAmerican football player", "# Reasoning Path:\nColin Kaepernick -> common.topic.notable_types -> American football player -> people.profession.specialization_of -> Athlete\n# Answer:\nAmerican football player", "# Reasoning Path:\nColin Kaepernick -> people.person.profession -> American football player -> fictional_universe.character_occupation.characters_with_this_occupation -> D'Glester Hardunkichud\n# Answer:\nAmerican football player", "# Reasoning Path:\nColin Kaepernick -> common.topic.notable_types -> American football player -> fictional_universe.character_occupation.characters_with_this_occupation -> A. A. Ron Balakay\n# Answer:\nAmerican football player", "# Reasoning Path:\nColin Kaepernick -> people.person.parents -> Heidi Russo -> common.topic.notable_for -> g.12blst_xm\n# Answer:\nHeidi Russo"], "ground_truth": ["San Francisco 49ers"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1740", "prediction": ["# Reasoning Path:\nAngelina Jolie -> people.person.children -> Knox Leon Jolie-Pitt -> people.person.parents -> Brad Pitt\n# Answer:\nKnox Leon Jolie-Pitt", "# Reasoning Path:\nAngelina Jolie -> people.person.children -> Maddox Chivan Jolie-Pitt -> people.person.parents -> Brad Pitt\n# Answer:\nMaddox Chivan Jolie-Pitt", "# Reasoning Path:\nAngelina Jolie -> people.person.children -> Pax Thien Jolie-Pitt -> people.person.parents -> Brad Pitt\n# Answer:\nPax Thien Jolie-Pitt", "# Reasoning Path:\nAngelina Jolie -> people.person.children -> Knox Leon Jolie-Pitt -> common.topic.notable_for -> g.12556r571\n# Answer:\nKnox Leon Jolie-Pitt", "# Reasoning Path:\nAngelina Jolie -> people.person.children -> Knox Leon Jolie-Pitt -> people.person.nationality -> United States of America\n# Answer:\nKnox Leon Jolie-Pitt", "# Reasoning Path:\nAngelina Jolie -> book.author.book_editions_published -> Notes from My Travels -> common.topic.notable_types -> Book\n# Answer:\nNotes from My Travels", "# Reasoning Path:\nAngelina Jolie -> people.person.children -> Maddox Chivan Jolie-Pitt -> base.cinemainspector.person_sign.star_sign -> Leo\n# Answer:\nMaddox Chivan Jolie-Pitt", "# Reasoning Path:\nAngelina Jolie -> award.award_nominee.award_nominations -> m.010wr37v -> award.award_nomination.nominated_for -> Maleficent\n# Answer:\nMaleficent", "# Reasoning Path:\nAngelina Jolie -> people.person.children -> Maddox Chivan Jolie-Pitt -> people.person.gender -> Male\n# Answer:\nMaddox Chivan Jolie-Pitt", "# Reasoning Path:\nAngelina Jolie -> people.person.children -> Pax Thien Jolie-Pitt -> people.person.profession -> Actor\n# Answer:\nPax Thien Jolie-Pitt"], "ground_truth": ["James Haven"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1741", "prediction": ["# Reasoning Path:\nBank of America Corporation -> organization.organization.headquarters -> m.03lfgp9 -> location.mailing_address.citytown -> Charlotte\n# Answer:\nCharlotte", "# Reasoning Path:\nBank of America Corporation -> organization.organization.headquarters -> m.03lfgp9 -> location.mailing_address.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nBank of America Corporation -> organization.organization.headquarters -> m.03lfgp9 -> location.mailing_address.state_province_region -> North Carolina\n# Answer:\nNorth Carolina", "# Reasoning Path:\nBank of America Corporation -> common.topic.webpage -> m.01xwxgw -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nBank of America Corporation -> common.topic.webpage -> m.0422j0r -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nBank of America Corporation -> organization.organization.locations -> Bank of America, Westfield West Covina -> common.topic.notable_for -> g.1254z5q8d\n# Answer:\nBank of America, Westfield West Covina", "# Reasoning Path:\nBank of America Corporation -> common.topic.webpage -> m.01xwxgw -> common.webpage.resource -> Bank of America\n# Answer:\nBank of America", "# Reasoning Path:\nBank of America Corporation -> organization.organization.locations -> Bank of America, Bangor Mall -> common.topic.notable_types -> Organization Location\n# Answer:\nBank of America, Bangor Mall", "# Reasoning Path:\nBank of America Corporation -> organization.organization.locations -> Bank of America, Westfield West Covina -> business.business_location.in_shopping_center -> Plaza West Covina\n# Answer:\nBank of America, Westfield West Covina", "# Reasoning Path:\nBank of America Corporation -> organization.organization.locations -> Bank of America, Burlington Mall -> business.business_location.in_shopping_center -> Burlington Mall\n# Answer:\nBank of America, Burlington Mall"], "ground_truth": ["Charlotte"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1742", "prediction": ["# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> United Kingdom\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Australia\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.11b71r82sc\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Canada\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_4lv6\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_6fj1\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita"], "ground_truth": ["Parliamentary system", "Constitutional monarchy", "Unitary state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1744", "prediction": ["# Reasoning Path:\nJosh Hamilton -> base.schemastaging.athlete_extra.salary -> m.0r66q69 -> base.schemastaging.athlete_salary.team -> Los Angeles Angels of Anaheim\n# Answer:\nLos Angeles Angels of Anaheim", "# Reasoning Path:\nJosh Hamilton -> base.schemastaging.athlete_extra.salary -> m.0r66q69 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nJosh Hamilton -> base.schemastaging.athlete_extra.salary -> m.01064_wv -> base.schemastaging.athlete_salary.team -> Los Angeles Angels of Anaheim\n# Answer:\nLos Angeles Angels of Anaheim", "# Reasoning Path:\nJosh Hamilton -> sports.pro_athlete.teams -> m.0r66mqx -> sports.sports_team_roster.team -> Cincinnati Reds\n# Answer:\nCincinnati Reds", "# Reasoning Path:\nJosh Hamilton -> base.schemastaging.athlete_extra.salary -> m.0j2ql15 -> base.schemastaging.athlete_salary.team -> Texas Rangers\n# Answer:\nTexas Rangers", "# Reasoning Path:\nJosh Hamilton -> baseball.baseball_player.batting_stats -> m.06qnhgk -> baseball.batting_statistics.team -> Texas Rangers\n# Answer:\nTexas Rangers", "# Reasoning Path:\nJosh Hamilton -> sports.pro_athlete.teams -> m.0r66mqx -> sports.sports_team_roster.position -> Outfielder\n# Answer:\nOutfielder", "# Reasoning Path:\nJosh Hamilton -> baseball.baseball_player.batting_stats -> m.06qnhf5 -> baseball.batting_statistics.team -> Cincinnati Reds\n# Answer:\nCincinnati Reds", "# Reasoning Path:\nJosh Hamilton -> base.schemastaging.athlete_extra.salary -> m.01064_wv -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nJosh Hamilton -> sports.pro_athlete.teams -> m.0nbmmjd -> sports.sports_team_roster.team -> Texas Rangers\n# Answer:\nTexas Rangers"], "ground_truth": ["Los Angeles Angels of Anaheim"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1746", "prediction": ["# Reasoning Path:\nBurundi -> location.location.containedby -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica", "# Reasoning Path:\nBurundi -> location.location.containedby -> Africa -> base.locations.continents.countries_within -> Zambia\n# Answer:\nAfrica", "# Reasoning Path:\nBurundi -> location.location.containedby -> Africa -> location.location.time_zones -> Central Africa Time Zone\n# Answer:\nAfrica", "# Reasoning Path:\nBurundi -> location.location.containedby -> Africa -> base.locations.continents.countries_within -> Angola\n# Answer:\nAfrica", "# Reasoning Path:\nBurundi -> base.locations.countries.continent -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica", "# Reasoning Path:\nBurundi -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b71nzld4\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nBurundi -> location.location.containedby -> Africa -> base.locations.continents.countries_within -> Cameroon\n# Answer:\nAfrica", "# Reasoning Path:\nBurundi -> base.locations.countries.continent -> Africa -> base.locations.continents.countries_within -> Zambia\n# Answer:\nAfrica", "# Reasoning Path:\nBurundi -> base.locations.countries.continent -> Africa -> location.location.time_zones -> Central Africa Time Zone\n# Answer:\nAfrica", "# Reasoning Path:\nBurundi -> base.locations.countries.continent -> Africa -> base.locations.continents.countries_within -> Angola\n# Answer:\nAfrica"], "ground_truth": ["Africa"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1748", "prediction": ["# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Niger -> location.country.languages_spoken -> Hausa Language\n# Answer:\nNiger", "# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Niger -> location.location.partially_contains -> Sahel\n# Answer:\nNiger", "# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Niger -> base.locations.countries.continent -> Africa\n# Answer:\nNiger", "# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Niger -> location.country.languages_spoken -> French\n# Answer:\nNiger", "# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Niger -> location.location.partially_contains -> Yobe River\n# Answer:\nNiger", "# Reasoning Path:\nNiger River -> geography.river.basin_countries -> Niger -> location.location.partially_contains -> Sahel\n# Answer:\nNiger", "# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Niger -> location.country.languages_spoken -> Fula language\n# Answer:\nNiger", "# Reasoning Path:\nNiger River -> geography.river.basin_countries -> Niger -> location.location.partially_contains -> M\u00e9krou River\n# Answer:\nNiger", "# Reasoning Path:\nNiger River -> geography.river.basin_countries -> Nigeria -> location.location.adjoin_s -> m.02xj5pm\n# Answer:\nNigeria", "# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Niger -> location.location.partially_contains -> M\u00e9krou River\n# Answer:\nNiger"], "ground_truth": ["Nigeria", "Niger", "Mali"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 1.0, "ans_recall": 0.6666666666666666, "path_f1": 0.3157894736842105, "path_precision": 0.3, "path_recall": 0.3333333333333333, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-175", "prediction": ["# Reasoning Path:\nJoe Flacco -> sports.drafted_athlete.drafted -> m.05bj8xr -> sports.sports_league_draft_pick.school -> University of Delaware\n# Answer:\nUniversity of Delaware", "# Reasoning Path:\nJoe Flacco -> sports.drafted_athlete.drafted -> m.05bj8xr -> sports.sports_league_draft_pick.team -> Baltimore Ravens\n# Answer:\nBaltimore Ravens", "# Reasoning Path:\nJoe Flacco -> sports.drafted_athlete.drafted -> m.05bj8xr -> sports.sports_league_draft_pick.draft -> 2008 NFL draft\n# Answer:\n2008 NFL draft", "# Reasoning Path:\nJoe Flacco -> people.person.places_lived -> m.0wkkybd -> people.place_lived.location -> Audubon\n# Answer:\nAudubon", "# Reasoning Path:\nJoe Flacco -> base.schemastaging.athlete_extra.salary -> m.0j2jm2m -> base.schemastaging.athlete_salary.team -> Baltimore Ravens\n# Answer:\nBaltimore Ravens", "# Reasoning Path:\nJoe Flacco -> base.schemastaging.athlete_extra.salary -> m.0j2jm2m -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nJoe Flacco -> base.schemastaging.athlete_extra.salary -> m.0j2kdfx -> base.schemastaging.athlete_salary.team -> Baltimore Ravens\n# Answer:\nBaltimore Ravens", "# Reasoning Path:\nJoe Flacco -> base.schemastaging.athlete_extra.salary -> m.0j2jm2w -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nJoe Flacco -> base.schemastaging.athlete_extra.salary -> m.0j2jm2w -> base.schemastaging.athlete_salary.team -> Baltimore Ravens\n# Answer:\nBaltimore Ravens", "# Reasoning Path:\nJoe Flacco -> base.schemastaging.athlete_extra.salary -> m.0j2kdfx -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar"], "ground_truth": ["University of Delaware"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1750", "prediction": ["# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> location.location.people_born_here -> Alex Burkart\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> people.person.places_lived -> m.03pt7rq -> people.place_lived.location -> Janesville\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> location.location.people_born_here -> Aurelia Harwood\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> location.statistical_region.population -> g.11b66f8swj\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> location.location.people_born_here -> Bob Strampe\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> location.location.contains -> The Back Bar\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> location.statistical_region.population -> g.11bc88b5vp\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> location.statistical_region.population -> g.11x1cgp16\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> location.location.contains -> Academy of Cosmetology\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality"], "ground_truth": ["Janesville"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1751", "prediction": ["# Reasoning Path:\nAcadia University -> location.location.containedby -> Wolfville -> location.location.containedby -> Kings County\n# Answer:\nWolfville", "# Reasoning Path:\nAcadia University -> location.location.containedby -> Nova Scotia -> location.location.containedby -> Canada\n# Answer:\nNova Scotia", "# Reasoning Path:\nAcadia University -> location.location.containedby -> Wolfville -> location.location.containedby -> Canada\n# Answer:\nWolfville", "# Reasoning Path:\nAcadia University -> location.location.containedby -> Wolfville -> film.film_location.featured_in_films -> Amelia\n# Answer:\nWolfville", "# Reasoning Path:\nAcadia University -> location.location.containedby -> Nova Scotia -> base.aareas.schema.administrative_area.administrative_parent -> Canada\n# Answer:\nNova Scotia", "# Reasoning Path:\nAcadia University -> location.location.containedby -> Canada -> location.location.containedby -> North America\n# Answer:\nCanada", "# Reasoning Path:\nAcadia University -> education.educational_institution.phone_number -> m.05sq0y3\n# Answer:\neducation.educational_institution.phone_number", "# Reasoning Path:\nAcadia University -> location.location.containedby -> Wolfville -> location.location.people_born_here -> William Hall\n# Answer:\nWolfville", "# Reasoning Path:\nAcadia University -> location.location.containedby -> Canada -> base.aareas.schema.administrative_area.administrative_children -> Nova Scotia\n# Answer:\nCanada", "# Reasoning Path:\nAcadia University -> education.educational_institution.phone_number -> m.05sq0xy\n# Answer:\neducation.educational_institution.phone_number"], "ground_truth": ["Wolfville"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1752", "prediction": ["# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Novelist -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nNovelist", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Novelist -> people.profession.specialization_of -> Writer\n# Answer:\nNovelist", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Novelist -> common.topic.notable_types -> Profession\n# Answer:\nNovelist", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Poet -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nPoet", "# Reasoning Path:\nF. Scott Fitzgerald -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Male\n# Answer:\nheart attack", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Writer -> people.profession.specializations -> Screenwriter\n# Answer:\nWriter", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Poet -> common.topic.notable_types -> Profession\n# Answer:\nPoet", "# Reasoning Path:\nF. Scott Fitzgerald -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.parent_disease -> Cardiovascular disease\n# Answer:\nheart attack", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Poet -> people.profession.specialization_of -> Writer\n# Answer:\nPoet", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Writer -> people.profession.specializations -> Novelist\n# Answer:\nWriter"], "ground_truth": ["Novelist", "Poet", "Writer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1753", "prediction": ["# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdxf42 -> base.schemastaging.phone_sandbox.service_location -> Switzerland\n# Answer:\nSwitzerland", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdx6cj -> base.schemastaging.phone_sandbox.service_location -> France\n# Answer:\nFrance", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdxcyk -> base.schemastaging.phone_sandbox.service_location -> Belgium\n# Answer:\nBelgium", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdxf42 -> base.schemastaging.phone_sandbox.caller_category -> Consumer Service\n# Answer:\nConsumer Service", "# Reasoning Path:\nTomtom Nv -> common.topic.notable_for -> g.125dysf1w\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdxf42 -> base.schemastaging.phone_sandbox.contact_category -> Technical Support\n# Answer:\nTechnical Support", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdx6cj -> base.schemastaging.phone_sandbox.contact_category -> Technical Support\n# Answer:\nTechnical Support", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdx6cj -> base.schemastaging.phone_sandbox.caller_category -> Existing Customers\n# Answer:\nExisting Customers", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdxcyk -> base.schemastaging.phone_sandbox.service_language -> French\n# Answer:\nFrench", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdxcyk -> base.schemastaging.phone_sandbox.caller_category -> Consumer Service\n# Answer:\nConsumer Service"], "ground_truth": ["Canada", "United States of America", "France", "Australia", "Switzerland", "United Kingdom", "Belgium"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.3529411764705882, "ans_precission": 0.3, "ans_recall": 0.42857142857142855, "path_f1": 0.28571428571428564, "path_precision": 0.3, "path_recall": 0.2727272727272727, "path_ans_f1": 0.3529411764705882, "path_ans_precision": 0.3, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1754", "prediction": ["# Reasoning Path:\nIran -> location.country.official_language -> Persian Language -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.official_language -> Persian Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Persian Language -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.official_language -> Persian Language -> common.topic.notable_types -> Human Language\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Persian Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal_per_capita -> g.11b60rrlgr\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Persian Language -> common.topic.notable_types -> Human Language\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nArabic Language", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6fdrh\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Arabic Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nArabic Language"], "ground_truth": ["Persian Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1755", "prediction": ["# Reasoning Path:\nSecond Battle of Ypres -> time.event.locations -> Ypres -> location.location.contains -> Bellewaerde\n# Answer:\nYpres", "# Reasoning Path:\nSecond Battle of Ypres -> time.event.locations -> Ypres -> location.location.geolocation -> m.0239wqc\n# Answer:\nYpres", "# Reasoning Path:\nSecond Battle of Ypres -> time.event.locations -> Ypres -> location.location.people_born_here -> Adolph, Landgrave of Hesse-Philippsthal-Barchfeld\n# Answer:\nYpres", "# Reasoning Path:\nSecond Battle of Ypres -> time.event.locations -> Ypres -> location.location.people_born_here -> Albert Dev\u00e8ze\n# Answer:\nYpres", "# Reasoning Path:\nSecond Battle of Ypres -> time.event.locations -> Ypres -> location.location.people_born_here -> Andreas Hyperius\n# Answer:\nYpres", "# Reasoning Path:\nSecond Battle of Ypres -> military.military_conflict.military_personnel_involved -> Oswald Mosley -> people.deceased_person.place_of_death -> Orsay\n# Answer:\nOswald Mosley", "# Reasoning Path:\nSecond Battle of Ypres -> military.military_conflict.commanders -> m.04m1gm_ -> military.military_command.military_commander -> Herbert Plumer, 1st Viscount Plumer\n# Answer:\nHerbert Plumer, 1st Viscount Plumer", "# Reasoning Path:\nSecond Battle of Ypres -> military.military_conflict.commanders -> m.08n9zdj -> military.military_command.military_combatant -> United Kingdom\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nSecond Battle of Ypres -> military.military_conflict.commanders -> m.08n9zdj -> military.military_command.military_commander -> Horace Smith-Dorrien\n# Answer:\nHorace Smith-Dorrien", "# Reasoning Path:\nSecond Battle of Ypres -> military.military_conflict.military_personnel_involved -> Oswald Mosley -> government.politician.government_positions_held -> m.04yv2x2\n# Answer:\nOswald Mosley"], "ground_truth": ["Ypres"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1756", "prediction": ["# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.region -> Europe\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Basque Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nBasque Language", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Basque Language -> language.human_language.main_country -> France\n# Answer:\nBasque Language", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.region -> Europe\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Andorra\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> language.human_language.countries_spoken_in -> Andorra\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Gibraltar\n# Answer:\nSpanish Language"], "ground_truth": ["Spanish Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1757", "prediction": ["# Reasoning Path:\nMichael Crabtree -> common.topic.notable_types -> American football player -> common.topic.notable_types -> Profession\n# Answer:\nAmerican football player", "# Reasoning Path:\nMichael Crabtree -> common.topic.notable_types -> American football player -> type.type.domain -> American football\n# Answer:\nAmerican football player", "# Reasoning Path:\nMichael Crabtree -> common.topic.notable_types -> American football player -> people.profession.specialization_of -> Football player\n# Answer:\nAmerican football player", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.receiving -> m.07ychkl -> american_football.player_receiving_statistics.team -> San Francisco 49ers\n# Answer:\nSan Francisco 49ers", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.receiving -> m.07z2fqm -> american_football.player_receiving_statistics.team -> San Francisco 49ers\n# Answer:\nSan Francisco 49ers", "# Reasoning Path:\nMichael Crabtree -> common.topic.notable_types -> American football player -> people.profession.specialization_of -> Athlete\n# Answer:\nAmerican football player", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.receiving -> m.07ychkl -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.receiving -> m.088yq56 -> american_football.player_receiving_statistics.team -> San Francisco 49ers\n# Answer:\nSan Francisco 49ers", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.games -> m.07ych9z -> american_football.player_game_statistics.team -> San Francisco 49ers\n# Answer:\nSan Francisco 49ers", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.receiving -> m.07z2fqm -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season"], "ground_truth": ["American football player", "Wide receiver", "Athlete"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.4, "ans_recall": 0.3333333333333333, "path_f1": 0.3636363636363636, "path_precision": 0.4, "path_recall": 0.3333333333333333, "path_ans_f1": 0.5, "path_ans_precision": 0.4, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1758", "prediction": ["# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Botswana -> location.location.adjoin_s -> m.03x68v0\n# Answer:\nBotswana", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Botswana -> location.country.languages_spoken -> English Language\n# Answer:\nBotswana", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Botswana -> location.location.adjoin_s -> m.0441kl4\n# Answer:\nBotswana", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Botswana -> location.country.administrative_divisions -> Southern District\n# Answer:\nBotswana", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Transkei -> location.location.containedby -> South Africa\n# Answer:\nTranskei", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Botswana -> location.location.adjoin_s -> m.0441kll\n# Answer:\nBotswana", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Botswana -> location.country.administrative_divisions -> Central District\n# Answer:\nBotswana", "# Reasoning Path:\nAfrikaans Language -> language.human_language.language_family -> West Germanic languages -> common.topic.image -> Europe germanic languages\n# Answer:\nWest Germanic languages", "# Reasoning Path:\nAfrikaans Language -> language.human_language.language_family -> West Germanic languages -> common.topic.article -> m.0140r1\n# Answer:\nWest Germanic languages", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Transkei -> location.location.geolocation -> m.0wmfc6f\n# Answer:\nTranskei"], "ground_truth": ["Malawi", "Transkei", "Zambia", "Namibia", "Gazankulu", "South Africa", "Botswana"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.4210526315789473, "ans_precission": 0.8, "ans_recall": 0.2857142857142857, "path_f1": 0.4210526315789473, "path_precision": 0.8, "path_recall": 0.2857142857142857, "path_ans_f1": 0.5581395348837209, "path_ans_precision": 0.8, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1759", "prediction": ["# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> organization.organization.geographic_scope -> Alabama\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> CRHOME -> organization.organization.geographic_scope -> Alabama\n# Answer:\nCRHOME", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> organization.organization.geographic_scope -> Arkansas\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> organization.organization.geographic_scope -> California\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> B'nai B'rith District Grand Lodge 7 -> organization.organization.geographic_scope -> Alabama\n# Answer:\nB'nai B'rith District Grand Lodge 7", "# Reasoning Path:\nFlorida -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> common.topic.subjects -> Automation\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> CRHOME -> organization.organization.geographic_scope -> Arizona\n# Answer:\nCRHOME", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> CRHOME -> organization.organization.place_founded -> Los Angeles\n# Answer:\nCRHOME", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> organization.organization.sectors -> Automation\n# Answer:\nEPIC Systems, Inc"], "ground_truth": ["Alabama", "Georgia"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615385, "path_precision": 0.3, "path_recall": 0.10344827586206896, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.4, "path_ans_recall": 0.5}
{"id": "WebQTest-176", "prediction": ["# Reasoning Path:\nEmma Stone -> people.person.parents -> Jeff Stone -> people.person.parents -> Conrad Ostberg Stone\n# Answer:\nJeff Stone", "# Reasoning Path:\nEmma Stone -> people.person.parents -> Jeff Stone -> common.topic.notable_for -> g.1258vxmmk\n# Answer:\nJeff Stone", "# Reasoning Path:\nEmma Stone -> people.person.parents -> Jeff Stone -> people.person.spouse_s -> m.0j4jwqj\n# Answer:\nJeff Stone", "# Reasoning Path:\nEmma Stone -> people.person.parents -> Krista Stone -> people.person.children -> Spencer Stone\n# Answer:\nKrista Stone", "# Reasoning Path:\nEmma Stone -> people.person.parents -> Krista Stone -> freebase.valuenotation.has_value -> Parents\n# Answer:\nKrista Stone", "# Reasoning Path:\nEmma Stone -> people.person.parents -> Krista Stone -> people.person.profession -> Homemaking\n# Answer:\nKrista Stone", "# Reasoning Path:\nEmma Stone -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nEmma Stone -> people.person.parents -> Krista Stone -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nKrista Stone", "# Reasoning Path:\nEmma Stone -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nEmma Stone -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth"], "ground_truth": ["Jeff Stone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1760", "prediction": ["# Reasoning Path:\nMossad Agent Ziva David -> tv.tv_character.appeared_in_tv_program -> m.043zbc6 -> tv.regular_tv_appearance.actor -> Cote de Pablo\n# Answer:\nCote de Pablo", "# Reasoning Path:\nMossad Agent Ziva David -> tv.tv_character.appeared_in_tv_program -> m.043zbc6 -> tv.regular_tv_appearance.seasons -> NCIS - Season 10\n# Answer:\nNCIS - Season 10", "# Reasoning Path:\nMossad Agent Ziva David -> tv.tv_character.appeared_in_tv_program -> m.043zbc6 -> tv.regular_tv_appearance.seasons -> NCIS - Season 11\n# Answer:\nNCIS - Season 11", "# Reasoning Path:\nMossad Agent Ziva David -> tv.tv_character.appeared_in_tv_program -> m.043zbc6 -> tv.regular_tv_appearance.seasons -> NCIS - Season 3\n# Answer:\nNCIS - Season 3", "# Reasoning Path:\nMossad Agent Ziva David -> tv.tv_character.appeared_in_tv_episodes -> m.03ly823 -> tv.tv_guest_role.actor -> Cote de Pablo\n# Answer:\nCote de Pablo", "# Reasoning Path:\nMossad Agent Ziva David -> tv.tv_character.appeared_in_tv_episodes -> m.09nj77s -> tv.tv_guest_role.episodes_appeared_in -> Kill Ari - Part 2\n# Answer:\nKill Ari - Part 2", "# Reasoning Path:\nMossad Agent Ziva David -> fictional_universe.fictional_character.character_created_by -> Donald P. Bellisario -> tv.tv_program_creator.programs_created -> Tequila and Bonetti\n# Answer:\nDonald P. Bellisario", "# Reasoning Path:\nMossad Agent Ziva David -> tv.tv_character.appeared_in_tv_episodes -> m.03ly823 -> tv.tv_guest_role.episodes_appeared_in -> Kill Ari - Part 1\n# Answer:\nKill Ari - Part 1", "# Reasoning Path:\nMossad Agent Ziva David -> fictional_universe.fictional_character.character_created_by -> Donald P. Bellisario -> tv.tv_program_creator.programs_created -> Airwolf\n# Answer:\nDonald P. Bellisario", "# Reasoning Path:\nMossad Agent Ziva David -> fictional_universe.fictional_character.character_created_by -> Donald P. Bellisario -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nDonald P. Bellisario"], "ground_truth": ["Cote de Pablo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.26666666666666666, "path_precision": 0.2, "path_recall": 0.4, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1761", "prediction": ["# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.spouse_s -> m.03mlj1z\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> common.topic.article -> m.02sx6l\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.place_of_birth -> Blackmore\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> common.topic.article -> m.0hn9rcf\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> symbols.name_source.namesakes -> King Henry's Drive tram stop -> common.topic.image -> King Henry's Drive tramstop look south\n# Answer:\nKing Henry's Drive tram stop", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> people.person.nationality -> Kingdom of England\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Edward VI of England -> people.person.religion -> Protestantism\n# Answer:\nEdward VI of England", "# Reasoning Path:\nHenry VIII of England -> symbols.name_source.namesakes -> King Henry's Drive tram stop -> common.topic.notable_for -> g.1257k6j6b\n# Answer:\nKing Henry's Drive tram stop", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> royalty.noble_person.titles -> m.0hqncyh\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Edward VI of England -> people.deceased_person.place_of_burial -> Henry VII Chapel\n# Answer:\nEdward VI of England"], "ground_truth": ["Edward VI of England"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1763", "prediction": ["# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_coaches -> m.0514q1f -> baseball.current_coaching_tenure.baseball_coach -> Dave Duncan\n# Answer:\nDave Duncan", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_coaches -> m.0514q0p -> baseball.current_coaching_tenure.baseball_coach -> Jos\u00e9 Oquendo\n# Answer:\nJos\u00e9 Oquendo", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_manager -> Mike Matheny -> people.person.gender -> Male\n# Answer:\nMike Matheny", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_coaches -> m.0514q0y -> baseball.current_coaching_tenure.baseball_coach -> Joe Pettini\n# Answer:\nJoe Pettini", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_manager -> Mike Matheny -> baseball.baseball_player.batting_stats -> m.06rgj_g\n# Answer:\nMike Matheny", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_coaches -> m.0514q1f -> baseball.current_coaching_tenure.coaching_position -> Pitching Coach\n# Answer:\nPitching Coach", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_manager -> Mike Matheny -> baseball.baseball_player.batting_stats -> m.06rgjmg\n# Answer:\nMike Matheny", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_coaches -> m.0514q0p -> baseball.current_coaching_tenure.coaching_position -> Third Base Coach\n# Answer:\nThird Base Coach", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_manager -> Mike Matheny -> baseball.baseball_player.batting_stats -> m.06rgjnw\n# Answer:\nMike Matheny", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_coaches -> m.0514q0y -> baseball.current_coaching_tenure.coaching_position -> Bench Coach\n# Answer:\nBench Coach"], "ground_truth": ["Hal McRae", "Dave Duncan", "Jos\u00e9 Oquendo", "Joe Pettini", "Dave McKay", "Marty Mason"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.37499999999999994, "ans_precission": 0.3, "ans_recall": 0.5, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.37499999999999994, "path_ans_precision": 0.3, "path_ans_recall": 0.5}
{"id": "WebQTest-1764", "prediction": ["# Reasoning Path:\nDawood Ibrahim -> people.person.place_of_birth -> Ratnagiri district -> location.location.containedby -> Konkan\n# Answer:\nRatnagiri district", "# Reasoning Path:\nDawood Ibrahim -> people.person.place_of_birth -> Ratnagiri district -> common.topic.notable_for -> g.1256k4jqk\n# Answer:\nRatnagiri district", "# Reasoning Path:\nDawood Ibrahim -> people.person.place_of_birth -> Ratnagiri district -> location.location.contains -> Ratnagiri\n# Answer:\nRatnagiri district", "# Reasoning Path:\nDawood Ibrahim -> people.person.place_of_birth -> Ratnagiri district -> location.location.containedby -> Maharashtra\n# Answer:\nRatnagiri district", "# Reasoning Path:\nDawood Ibrahim -> base.crime.convicted_criminal.convictions -> m.05h1g7c -> base.crime.criminal_conviction.guilty_of -> Illegal drug trade\n# Answer:\nIllegal drug trade", "# Reasoning Path:\nDawood Ibrahim -> people.person.nationality -> India -> location.location.containedby -> Asia\n# Answer:\nIndia", "# Reasoning Path:\nDawood Ibrahim -> people.person.place_of_birth -> Ratnagiri district -> location.location.contains -> Alore\n# Answer:\nRatnagiri district", "# Reasoning Path:\nDawood Ibrahim -> people.person.nationality -> India -> location.country.administrative_divisions -> Andaman and Nicobar Islands\n# Answer:\nIndia", "# Reasoning Path:\nDawood Ibrahim -> people.person.place_of_birth -> Ratnagiri district -> location.location.contains -> Argaon\n# Answer:\nRatnagiri district", "# Reasoning Path:\nDawood Ibrahim -> people.person.nationality -> India -> location.country.official_language -> English Language\n# Answer:\nIndia"], "ground_truth": ["Maharashtra"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1765", "prediction": ["# Reasoning Path:\nTurkey -> location.country.administrative_divisions -> Adana Province -> location.location.containedby -> Mediterranean Region, Turkey\n# Answer:\nAdana Province", "# Reasoning Path:\nTurkey -> base.locations.countries.continent -> Eurasia -> base.locations.continents.countries_within -> France\n# Answer:\nEurasia", "# Reasoning Path:\nTurkey -> base.locations.countries.continent -> Eurasia -> common.topic.notable_types -> Location\n# Answer:\nEurasia", "# Reasoning Path:\nTurkey -> base.locations.countries.continent -> Eurasia -> base.locations.continents.countries_within -> Azerbaijan\n# Answer:\nEurasia", "# Reasoning Path:\nTurkey -> location.country.administrative_divisions -> Adana Province -> common.topic.notable_types -> Administrative Division\n# Answer:\nAdana Province", "# Reasoning Path:\nTurkey -> location.country.administrative_divisions -> Ad\u0131yaman Province -> location.location.containedby -> Southeastern Anatolia Region\n# Answer:\nAd\u0131yaman Province", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.065sxsl -> military.military_combatant_group.conflict -> Korean War\n# Answer:\nKorean War", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.066402y -> military.military_combatant_group.conflict -> Iraqi no-fly zones\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nTurkey -> location.country.administrative_divisions -> Afyonkarahisar Province -> location.location.containedby -> Aegean Region\n# Answer:\nAfyonkarahisar Province", "# Reasoning Path:\nTurkey -> location.country.administrative_divisions -> Afyonkarahisar Province -> common.topic.notable_types -> Administrative Division\n# Answer:\nAfyonkarahisar Province"], "ground_truth": ["Eurasia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1766", "prediction": ["# Reasoning Path:\nRussia -> location.country.languages_spoken -> Yiddish Language -> language.human_language.countries_spoken_in -> Sweden\n# Answer:\nYiddish Language", "# Reasoning Path:\nRussian -> people.ethnicity.languages_spoken -> Russian Language -> language.human_language.countries_spoken_in -> Russia\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Yiddish Language -> common.topic.notable_types -> Human Language\n# Answer:\nYiddish Language", "# Reasoning Path:\nRussian -> people.ethnicity.languages_spoken -> Russian Language -> common.topic.notable_types -> Human Language\n# Answer:\nRussian Language", "# Reasoning Path:\nRussian -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> L -> fictional_universe.fictional_character.gender -> Male\n# Answer:\nL", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Yiddish Language -> language.human_language.main_country -> United States of America\n# Answer:\nYiddish Language", "# Reasoning Path:\nRussian -> people.ethnicity.languages_spoken -> Russian Language -> language.human_language.countries_spoken_in -> Ukraine\n# Answer:\nRussian Language", "# Reasoning Path:\nRussian -> people.ethnicity.languages_spoken -> Russian Language -> language.human_language.region -> Eurasia\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.countries_spoken_in -> Ukraine\n# Answer:\nUkrainian Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> language.human_language.countries_spoken_in -> Ukraine\n# Answer:\nRussian Language"], "ground_truth": ["Altai language", "Abaza Language", "Russian Language", "Komi language", "Yakut Language", "Tuvin Language", "Yiddish Language", "Nogai Language", "Tabassaran Language", "Dargwa Language", "Aghul language", "Azerbaijani language", "Lak Language", "Bashkir Language", "Ukrainian Language", "Kalmyk-Oirat Language", "Rutul language", "Avar Language", "Khakas Language", "Erzya Language", "Adyghe Language", "Tsakhur Language", "Buryat language", "Osetin Language", "Crimean Turkish Language", "Lezgi Language", "Kumyk Language", "Udmurt Language", "Kabardian Language", "Moksha Language", "Chechen Language", "Karachay-Balkar Language", "Tatar Language", "Mari language", "Ingush Language"], "ans_acc": 0.08571428571428572, "ans_hit": 1, "ans_f1": 0.1565217391304348, "ans_precission": 0.9, "ans_recall": 0.08571428571428572, "path_f1": 0.10069930069930069, "path_precision": 0.9, "path_recall": 0.05333333333333334, "path_ans_f1": 0.1565217391304348, "path_ans_precision": 0.9, "path_ans_recall": 0.08571428571428572}
{"id": "WebQTest-1767", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_burial -> Martin Luther King, Jr. National Historic Site -> travel.tourist_attraction.near_travel_destination -> Atlanta\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_burial -> Martin Luther King, Jr. National Historic Site -> people.place_of_interment.interred_here -> Mojola Agbebi\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> travel.tourist_attraction.near_travel_destination -> Atlanta\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_burial -> Martin Luther King, Jr. National Historic Site -> location.location.geolocation -> m.0wmyhzk\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_burial -> Martin Luther King, Jr. National Historic Site -> people.place_of_interment.interred_here -> Ann Nixon Cooper\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_burial -> Martin Luther King, Jr. National Historic Site -> people.place_of_interment.interred_here -> Benjamin F. Ward\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> people.place_of_interment.interred_here -> Ann Nixon Cooper\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> location.location.geolocation -> m.0wmyhzk\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nAlpha Man: The Brotherhood of MLK", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> common.topic.article -> m.05nnh0\n# Answer:\nDexter Avenue Baptist Church"], "ground_truth": ["Martin Luther King, Jr. National Historic Site"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1768", "prediction": ["# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> language.language_family.geographic_distribution -> Bolivia\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> common.topic.notable_types -> Human Language\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Guaran\u00ed language -> common.topic.notable_types -> Human Language\n# Answer:\nGuaran\u00ed language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> language.language_family.geographic_distribution -> Peru\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nItalian Language", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Paseana -> base.thoroughbredracing.thoroughbred_racehorse.color -> Bay\n# Answer:\nPaseana", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Italian Language -> common.topic.notable_types -> Human Language\n# Answer:\nItalian Language", "# Reasoning Path:\nArgentina -> location.location.partially_contains -> Alto San Juan -> location.location.partially_contained_by -> m.0wg8lvc\n# Answer:\nAlto San Juan", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Paseana -> biology.organism.organism_type -> Horse\n# Answer:\nPaseana", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nItalian Language"], "ground_truth": ["Spanish Language"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1769", "prediction": ["# Reasoning Path:\nWales -> base.biblioness.bibs_location.country -> United Kingdom -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nWales -> location.location.containedby -> United Kingdom, with Dependencies and Territories -> base.aareas.schema.administrative_area.administrative_children -> United Kingdom\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nWales -> location.location.containedby -> United Kingdom -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nWales -> location.location.containedby -> United Kingdom, with Dependencies and Territories -> location.country.first_level_divisions -> Anguilla\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nWales -> location.location.containedby -> United Kingdom -> location.country.first_level_divisions -> England\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nWales -> base.biblioness.bibs_location.country -> United Kingdom -> location.country.first_level_divisions -> England\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nWales -> location.location.containedby -> United Kingdom, with Dependencies and Territories -> location.country.first_level_divisions -> Bermuda\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nWales -> location.location.containedby -> United Kingdom, with Dependencies and Territories -> location.location.partially_containedby -> Cyprus\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nWales -> base.biblioness.bibs_location.country -> United Kingdom -> location.location.containedby -> Europe\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nWales -> location.location.containedby -> United Kingdom, with Dependencies and Territories -> base.aareas.schema.administrative_area.administrative_children -> Anguilla\n# Answer:\nUnited Kingdom, with Dependencies and Territories"], "ground_truth": ["United Kingdom", "United Kingdom, with Dependencies and Territories"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-177", "prediction": ["# Reasoning Path:\nTheodore Roosevelt -> people.deceased_person.place_of_burial -> Youngs Memorial Cemetery -> common.topic.notable_types -> Cemetery\n# Answer:\nYoungs Memorial Cemetery", "# Reasoning Path:\nTheodore Roosevelt -> people.person.place_of_birth -> Manhattan -> base.aareas.schema.administrative_area.administrative_parent -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nTheodore Roosevelt -> people.deceased_person.place_of_burial -> Youngs Memorial Cemetery -> people.place_of_interment.interred_here -> Quentin Roosevelt II\n# Answer:\nYoungs Memorial Cemetery", "# Reasoning Path:\nTheodore Roosevelt -> people.deceased_person.place_of_burial -> Youngs Memorial Cemetery -> common.topic.article -> m.05230r2\n# Answer:\nYoungs Memorial Cemetery", "# Reasoning Path:\nTheodore Roosevelt -> people.deceased_person.place_of_burial -> Youngs Memorial Cemetery -> people.place_of_interment.interred_here -> Archibald Bulloch Roosevelt Jr.\n# Answer:\nYoungs Memorial Cemetery", "# Reasoning Path:\nTheodore Roosevelt -> people.person.place_of_birth -> Manhattan -> location.administrative_division.country -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nTheodore Roosevelt -> people.deceased_person.place_of_burial -> Youngs Memorial Cemetery -> people.place_of_interment.interred_here -> Archibald Roosevelt\n# Answer:\nYoungs Memorial Cemetery", "# Reasoning Path:\nTheodore Roosevelt -> people.person.place_of_birth -> Manhattan -> location.location.containedby -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nTheodore Roosevelt -> symbols.name_source.namesakes -> Roosevelt -> location.location.containedby -> Washington\n# Answer:\nRoosevelt", "# Reasoning Path:\nTheodore Roosevelt -> symbols.name_source.namesakes -> Roosevelt -> common.topic.article -> m.02gt_g\n# Answer:\nRoosevelt"], "ground_truth": ["Youngs Memorial Cemetery"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1770", "prediction": ["# Reasoning Path:\nRafael Nadal -> people.person.nationality -> Spain -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\nSpain", "# Reasoning Path:\nRafael Nadal -> people.person.nationality -> Spain -> common.topic.notable_types -> Country\n# Answer:\nSpain", "# Reasoning Path:\nRafael Nadal -> people.person.nationality -> Spain -> sports.sport_country.athletic_performances -> m.0fp9_m_\n# Answer:\nSpain", "# Reasoning Path:\nRafael Nadal -> tennis.tennis_player.matches_won -> m.010h70y7 -> tennis.tennis_match.match_format -> Men's singles\n# Answer:\nMen's singles", "# Reasoning Path:\nRafael Nadal -> award.award_nominee.award_nominations -> m.010_zj6l -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work", "# Reasoning Path:\nRafael Nadal -> tennis.tennis_player.matches_won -> m.010h70y7 -> tennis.tennis_match.event -> Miami Open\n# Answer:\nMiami Open", "# Reasoning Path:\nRafael Nadal -> tennis.tennis_player.matches_won -> m.010hdsln -> tennis.tennis_match.loser -> Roberto Bautista Agut\n# Answer:\nRoberto Bautista Agut", "# Reasoning Path:\nRafael Nadal -> tennis.tennis_player.matches_won -> m.010h70y7 -> tennis.tennis_match.loser -> Milos Raonic\n# Answer:\nMilos Raonic", "# Reasoning Path:\nRafael Nadal -> tennis.tennis_player.matches_won -> m.010hvfy9 -> tennis.tennis_match.loser -> Kei Nishikori\n# Answer:\nKei Nishikori", "# Reasoning Path:\nRafael Nadal -> award.award_nominee.award_nominations -> m.010_zj6l -> award.award_nomination.award -> Best Male Tennis Player ESPY Award\n# Answer:\nBest Male Tennis Player ESPY Award"], "ground_truth": ["Spain"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1771", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.beliefs -> \u1e6c\u016bb\u0101 -> common.topic.notable_for -> g.1q6hmhsk5\n# Answer:\n\u1e6c\u016bb\u0101", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> common.topic.notable_for -> g.1q69mrtxz\n# Answer:\nEntering Heaven alive", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> \u1e6c\u016bb\u0101 -> common.topic.article -> m.0hr6vbt\n# Answer:\n\u1e6c\u016bb\u0101", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Islamic holy books -> common.topic.notable_for -> g.125621qyv\n# Answer:\nIslamic holy books", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.beliefs -> End time\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> religion.belief.belief_of -> Zoroastrianism\n# Answer:\nEntering Heaven alive", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> common.topic.article -> m.02wvcg8\n# Answer:\nEntering Heaven alive", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Islamic holy books -> common.topic.notable_types -> Belief\n# Answer:\nIslamic holy books", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Sikhism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.article -> m.055v4j\n# Answer:\nRamdev Pir"], "ground_truth": ["Qiyamah", "Mahdi", "Prophets in Islam", "Monotheism", "\u1e6c\u016bb\u0101", "Islamic view of angels", "Entering Heaven alive", "Tawhid", "Masih ad-Dajjal", "Islamic holy books", "God in Islam", "Predestination in Islam", "Sharia"], "ans_acc": 0.23076923076923078, "ans_hit": 1, "ans_f1": 0.34710743801652894, "ans_precission": 0.7, "ans_recall": 0.23076923076923078, "path_f1": 0.34710743801652894, "path_precision": 0.7, "path_recall": 0.23076923076923078, "path_ans_f1": 0.34710743801652894, "path_ans_precision": 0.7, "path_ans_recall": 0.23076923076923078}
{"id": "WebQTest-1772", "prediction": ["# Reasoning Path:\nSpain -> government.governmental_jurisdiction.governing_officials -> m.0hnsn5h -> government.government_position_held.office_holder -> Mariano Rajoy\n# Answer:\nMariano Rajoy", "# Reasoning Path:\nSpain -> government.governmental_jurisdiction.governing_officials -> m.010wrj1t -> government.government_position_held.office_holder -> Mariano Rajoy\n# Answer:\nMariano Rajoy", "# Reasoning Path:\nSpain -> government.governmental_jurisdiction.governing_officials -> m.0cnvd2z -> government.government_position_held.office_holder -> Jos\u00e9 Luis Rodr\u00edguez Zapatero\n# Answer:\nJos\u00e9 Luis Rodr\u00edguez Zapatero", "# Reasoning Path:\nSpain -> government.governmental_jurisdiction.governing_officials -> m.0hnsn5h -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nSpain -> government.governmental_jurisdiction.governing_officials -> m.0hnsn5h -> government.government_position_held.basic_title -> Prime minister\n# Answer:\nPrime minister", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> government.governmental_jurisdiction.governing_officials -> m.010wrj1t -> government.government_position_held.basic_title -> Deputy prime minister\n# Answer:\nDeputy prime minister", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60xs9d2\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nSpain -> government.governmental_jurisdiction.governing_officials -> m.0cnvd2z -> government.government_position_held.basic_title -> Prime minister\n# Answer:\nPrime minister", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc38hhk\n# Answer:\nlocation.statistical_region.deposit_interest_rate"], "ground_truth": ["Jos\u00e9 Luis Rodr\u00edguez Zapatero", "Mariano Rajoy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1773", "prediction": ["# Reasoning Path:\nLeonidas I -> people.person.children -> Pleistarchus -> people.person.parents -> Gorgo, Queen of Sparta\n# Answer:\nPleistarchus", "# Reasoning Path:\nLeonidas I -> people.person.children -> Pleistarchus -> people.person.place_of_birth -> Sparta\n# Answer:\nPleistarchus", "# Reasoning Path:\nLeonidas I -> people.person.children -> Pleistarchus -> freebase.valuenotation.has_value -> Cause of death\n# Answer:\nPleistarchus", "# Reasoning Path:\nLeonidas I -> people.person.children -> Pleistarchus -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nPleistarchus", "# Reasoning Path:\nLeonidas I -> common.topic.webpage -> m.09xtfr4 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nLeonidas I -> common.topic.webpage -> m.09xv7nj -> common.webpage.resource -> Reviewing the Reviews: '300'\n# Answer:\nReviewing the Reviews: '300'", "# Reasoning Path:\nLeonidas I -> people.person.children -> Pleistarchus -> freebase.valuenotation.has_value -> Place of death\n# Answer:\nPleistarchus", "# Reasoning Path:\nLeonidas I -> common.topic.webpage -> m.09xtfr4 -> common.webpage.resource -> Are you eager to G(r)eek out and see '300'?\n# Answer:\nAre you eager to G(r)eek out and see '300'?", "# Reasoning Path:\nLeonidas I -> common.topic.webpage -> m.09xv7nj -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nLeonidas I -> common.topic.webpage -> m.09yc9vz -> common.webpage.resource -> '300' sequel: Zack Snyder says he's up for it...but are you?\n# Answer:\n'300' sequel: Zack Snyder says he's up for it...but are you?"], "ground_truth": ["Pleistarchus"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1774", "prediction": ["# Reasoning Path:\nJudi Dench -> people.person.spouse_s -> m.0hyl98h -> people.marriage.spouse -> Michael Williams\n# Answer:\nMichael Williams", "# Reasoning Path:\nJudi Dench -> people.person.spouse_s -> m.0hyl98h -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nJudi Dench -> people.person.spouse_s -> m.0hyl98h -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nJudi Dench -> people.person.profession -> Voice Actor -> people.profession.specialization_of -> Actor\n# Answer:\nVoice Actor", "# Reasoning Path:\nJudi Dench -> film.actor.film -> m.0113rwfg -> film.performance.character -> Evelyn Greenslade\n# Answer:\nEvelyn Greenslade", "# Reasoning Path:\nJudi Dench -> people.person.profession -> Voice Actor -> common.topic.notable_types -> Profession\n# Answer:\nVoice Actor", "# Reasoning Path:\nJudi Dench -> film.actor.film -> m.0113rwfg -> film.performance.film -> The Second Best Exotic Marigold Hotel\n# Answer:\nThe Second Best Exotic Marigold Hotel", "# Reasoning Path:\nJudi Dench -> people.person.profession -> Voice Actor -> film.film_job.films_with_this_crew_job -> m.0y8734m\n# Answer:\nVoice Actor", "# Reasoning Path:\nJudi Dench -> people.person.profession -> Actor -> people.profession.specializations -> Voice Actor\n# Answer:\nActor", "# Reasoning Path:\nJudi Dench -> film.actor.film -> m.012zj5xp -> film.performance.film -> Tulip Fever\n# Answer:\nTulip Fever"], "ground_truth": ["Michael Williams"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1775", "prediction": ["# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> language.human_language.countries_spoken_in -> Andorra\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> language.human_language.countries_spoken_in -> France\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Basque Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nBasque Language", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.region -> Europe\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.region -> Europe\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Basque Language -> language.human_language.main_country -> France\n# Answer:\nBasque Language"], "ground_truth": ["Galician Language", "Catalan language", "Spanish Language", "Occitan language", "Basque Language"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7200000000000001, "ans_precission": 0.9, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.7200000000000001, "path_ans_precision": 0.9, "path_ans_recall": 0.6}
{"id": "WebQTest-1776", "prediction": ["# Reasoning Path:\nSahara -> location.location.containedby -> North Africa -> location.location.containedby -> Africa\n# Answer:\nNorth Africa", "# Reasoning Path:\nSahara -> book.written_work.previous_in_series -> Hemingway Adventure\n# Answer:\nHemingway Adventure", "# Reasoning Path:\nSahara -> location.location.containedby -> North Africa -> location.location.contains -> Al Sharqia Governorate\n# Answer:\nNorth Africa", "# Reasoning Path:\nSahara -> location.location.containedby -> North Africa -> location.location.partiallycontains -> m.0wg920z\n# Answer:\nNorth Africa", "# Reasoning Path:\nSahara -> geography.geographical_feature.category -> Desert -> geography.geographical_feature_category.features -> Atacama Desert\n# Answer:\nDesert", "# Reasoning Path:\nSahara -> location.location.containedby -> North Africa -> location.location.contains -> Alexandria Governorate\n# Answer:\nNorth Africa", "# Reasoning Path:\nSahara -> geography.geographical_feature.category -> Desert -> fictional_universe.type_of_fictional_setting.settings -> Sandia\n# Answer:\nDesert", "# Reasoning Path:\nSahara -> location.location.containedby -> North Africa -> location.location.contains -> Ancient Greece\n# Answer:\nNorth Africa", "# Reasoning Path:\nSahara -> location.location.containedby -> North Africa -> location.location.partiallycontains -> m.0wg9213\n# Answer:\nNorth Africa", "# Reasoning Path:\nSahara -> geography.geographical_feature.category -> Desert -> geography.geographical_feature_category.features -> Devil's Garden\n# Answer:\nDesert"], "ground_truth": ["North Africa"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1777", "prediction": ["# Reasoning Path:\nRihanna -> people.person.parents -> Monica Braithwaite -> freebase.valuenotation.has_value -> Parents\n# Answer:\nMonica Braithwaite", "# Reasoning Path:\nRihanna -> people.person.parents -> Monica Braithwaite -> people.person.nationality -> Barbados\n# Answer:\nMonica Braithwaite", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> people.person.parents -> Monica Braithwaite -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nMonica Braithwaite", "# Reasoning Path:\nRihanna -> people.person.parents -> Monica Braithwaite -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nMonica Braithwaite", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> people.person.parents -> Ronald Fenty -> celebrities.celebrity.substance_abuse_problems -> m.0k8hg6n\n# Answer:\nRonald Fenty", "# Reasoning Path:\nRihanna -> people.person.parents -> Monica Braithwaite -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nMonica Braithwaite", "# Reasoning Path:\nRihanna -> people.person.parents -> Ronald Fenty -> people.person.children -> Kandy Fenty\n# Answer:\nRonald Fenty", "# Reasoning Path:\nRihanna -> people.person.parents -> Ronald Fenty -> common.topic.notable_for -> g.125dlnqzk\n# Answer:\nRonald Fenty"], "ground_truth": ["Monica Braithwaite"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1779", "prediction": ["# Reasoning Path:\nTheodore Roosevelt -> base.inaugurations.inauguration_speaker.inauguration -> Theodore Roosevelt 1901 presidential inauguration -> time.event.locations -> Theodore Roosevelt Inaugural National Historic Site\n# Answer:\nTheodore Roosevelt 1901 presidential inauguration", "# Reasoning Path:\nTheodore Roosevelt -> base.inaugurations.inauguration_speaker.inauguration -> Theodore Roosevelt 1905 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nTheodore Roosevelt 1905 presidential inauguration", "# Reasoning Path:\nTheodore Roosevelt -> people.person.employment_history -> m.0w7nv3t -> business.employment_tenure.title -> President\n# Answer:\nPresident", "# Reasoning Path:\nTheodore Roosevelt -> base.inaugurations.inauguration_speaker.inauguration -> Theodore Roosevelt 1901 presidential inauguration -> time.event.locations -> Buffalo\n# Answer:\nTheodore Roosevelt 1901 presidential inauguration", "# Reasoning Path:\nTheodore Roosevelt -> people.person.employment_history -> m.0w7nv3t -> business.employment_tenure.company -> Federal government of the United States\n# Answer:\nFederal government of the United States", "# Reasoning Path:\nTheodore Roosevelt -> government.us_president.vice_president -> Charles W. Fairbanks -> people.person.place_of_birth -> Unionville Center\n# Answer:\nCharles W. Fairbanks", "# Reasoning Path:\nTheodore Roosevelt -> government.us_president.vice_president -> Charles W. Fairbanks -> people.person.gender -> Male\n# Answer:\nCharles W. Fairbanks", "# Reasoning Path:\nTheodore Roosevelt -> base.inaugurations.inauguration_speaker.inauguration -> Theodore Roosevelt 1905 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nTheodore Roosevelt 1905 presidential inauguration", "# Reasoning Path:\nTheodore Roosevelt -> base.inaugurations.inauguration_speaker.inauguration -> Theodore Roosevelt 1905 presidential inauguration -> common.topic.notable_types -> Event\n# Answer:\nTheodore Roosevelt 1905 presidential inauguration", "# Reasoning Path:\nTheodore Roosevelt -> government.us_president.vice_president -> Charles W. Fairbanks -> people.person.education -> m.040gk97\n# Answer:\nCharles W. Fairbanks"], "ground_truth": ["1901-09-14"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-178", "prediction": ["# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.05rln07 -> event.speech_or_presentation.speaker_s -> Franklin D. Roosevelt\n# Answer:\nFranklin D. Roosevelt", "# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.05rpn5h -> event.speech_or_presentation.speaker_s -> Harry S. Truman\n# Answer:\nHarry S. Truman", "# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.05rln07 -> event.speech_or_presentation.presented_work -> Arsenal of Democracy\n# Answer:\nArsenal of Democracy", "# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.010nmtv6 -> event.speech_or_presentation.speaker_s -> Y\u014dhei K\u014dno\n# Answer:\nY\u014dhei K\u014dno", "# Reasoning Path:\nWorld War II -> base.skosbase.vocabulary_equivalent_topic.narrower_concept -> World War, 1939-1945--Hostages -> base.skosbase.skos_concept.broader_topic -> Hostage\n# Answer:\nWorld War, 1939-1945--Hostages", "# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.05rpn5h -> event.speech_or_presentation.presented_work -> Harry S. Truman's 1949 inaugural address\n# Answer:\nHarry S. Truman's 1949 inaugural address", "# Reasoning Path:\nWorld War II -> military.military_conflict.commanders -> m.04jx14r -> military.military_command.military_commander -> Harry S. Truman\n# Answer:\nHarry S. Truman", "# Reasoning Path:\nWorld War II -> military.military_conflict.commanders -> m.04jx14r -> military.military_command.military_combatant -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nWorld War II -> military.military_conflict.commanders -> m.010_09yn -> military.military_command.military_commander -> Hirohito\n# Answer:\nHirohito", "# Reasoning Path:\nWorld War II -> military.military_conflict.commanders -> m.02h77t6 -> military.military_command.military_commander -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson"], "ground_truth": ["Harry S. Truman", "Franklin D. Roosevelt"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1780", "prediction": ["# Reasoning Path:\nMichael Oher -> base.schemastaging.athlete_extra.salary -> m.0j2l9cn -> base.schemastaging.athlete_salary.team -> Baltimore Ravens\n# Answer:\nBaltimore Ravens", "# Reasoning Path:\nMichael Oher -> base.schemastaging.athlete_extra.salary -> m.0j2l9cx -> base.schemastaging.athlete_salary.team -> Baltimore Ravens\n# Answer:\nBaltimore Ravens", "# Reasoning Path:\nMichael Oher -> base.schemastaging.athlete_extra.salary -> m.0j2l9d4 -> base.schemastaging.athlete_salary.team -> Baltimore Ravens\n# Answer:\nBaltimore Ravens", "# Reasoning Path:\nMichael Oher -> american_football.football_player.receiving -> m.07sgvjq -> american_football.player_receiving_statistics.team -> Baltimore Ravens\n# Answer:\nBaltimore Ravens", "# Reasoning Path:\nMichael Oher -> base.schemastaging.athlete_extra.salary -> m.0j2l9cn -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nMichael Oher -> american_football.football_player.receiving -> m.09rlxpq -> american_football.player_receiving_statistics.team -> Baltimore Ravens\n# Answer:\nBaltimore Ravens", "# Reasoning Path:\nMichael Oher -> base.schemastaging.athlete_extra.salary -> m.0j2l9cx -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nMichael Oher -> base.schemastaging.athlete_extra.salary -> m.0j2l9d4 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nMichael Oher -> american_football.football_player.receiving -> m.07sgvjq -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nMichael Oher -> american_football.football_player.games -> m.07sgv1_ -> american_football.player_game_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season"], "ground_truth": ["Baltimore Ravens"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.2040816326530612, "path_precision": 0.5, "path_recall": 0.1282051282051282, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1781", "prediction": ["# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Arthur Schopenhauer -> influence.influence_node.influenced_by -> John Locke\n# Answer:\nArthur Schopenhauer", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Arthur Schopenhauer -> influence.influence_node.influenced -> Sigmund Freud\n# Answer:\nArthur Schopenhauer", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Arthur Schopenhauer -> influence.influence_node.influenced_by -> Buddhism\n# Answer:\nArthur Schopenhauer", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Baruch Spinoza -> influence.influence_node.influenced -> Sigmund Freud\n# Answer:\nBaruch Spinoza", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Arthur Schopenhauer -> influence.influence_node.influenced_by -> David Hume\n# Answer:\nArthur Schopenhauer", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Arthur Schopenhauer -> people.person.religion -> Atheism\n# Answer:\nArthur Schopenhauer", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Baruch Spinoza -> influence.influence_node.influenced -> Ludwig Wittgenstein\n# Answer:\nBaruch Spinoza", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Isaac Newton -> influence.influence_node.influenced -> Alfred North Whitehead\n# Answer:\nIsaac Newton", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Arthur Schopenhauer -> influence.influence_node.influenced -> Carl Jung\n# Answer:\nArthur Schopenhauer", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Arthur Schopenhauer -> people.person.religion -> Buddhism\n# Answer:\nArthur Schopenhauer"], "ground_truth": ["Isaac Newton", "Paul Val\u00e9ry", "Ernst Mach", "James Clerk Maxwell", "Thomas Young", "Bernhard Riemann", "Baruch Spinoza", "Karl Pearson", "Fyodor Dostoyevsky", "Hermann Minkowski", "Moritz Schlick", "Hendrik Lorentz", "George Bernard Shaw", "Henry George", "Arthur Schopenhauer", "Riazuddin", "David Hume", "Mahatma Gandhi"], "ans_acc": 0.2222222222222222, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 1.0, "ans_recall": 0.16666666666666666, "path_f1": 0.2857142857142857, "path_precision": 1.0, "path_recall": 0.16666666666666666, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 1.0, "path_ans_recall": 0.2222222222222222}
{"id": "WebQTest-1782", "prediction": ["# Reasoning Path:\nEgypt -> location.statistical_region.places_exported_to -> m.048prww -> location.imports_and_exports.exported_to -> Sudan\n# Answer:\nSudan", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Sudan\n# Answer:\nArabic Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.main_country -> Saudi Arabia\n# Answer:\nArabic Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.countries_spoken_in -> Sudan\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.location.events -> 1964 Arab League summit -> common.topic.notable_types -> Event\n# Answer:\n1964 Arab League summit", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Saudi Arabia\n# Answer:\nArabic Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.main_country -> Sudan\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.writing_system -> Arabic alphabet\n# Answer:\nArabic Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.region -> Africa\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.location.events -> 1976 Arab League summit -> common.topic.notable_types -> Event\n# Answer:\n1976 Arab League summit"], "ground_truth": ["Sudan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.24, "path_precision": 0.3, "path_recall": 0.2, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1784", "prediction": ["# Reasoning Path:\nAfrican Union -> organization.organization.headquarters -> m.0pcb32w -> location.mailing_address.citytown -> Addis Ababa\n# Answer:\nAddis Ababa", "# Reasoning Path:\nAfrican Union -> organization.organization.headquarters -> m.0pcb32w -> common.topic.notable_types -> Location\n# Answer:\nLocation", "# Reasoning Path:\nAfrican Union -> base.schemastaging.context_name.pronunciation -> g.125_l8y1s\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nAfrican Union -> organization.organization.headquarters -> m.0pcb32w -> location.mailing_address.postal_code -> 378\n# Answer:\n378", "# Reasoning Path:\nAfrican Union -> organization.membership_organization.members -> m.010nx8yt -> organization.organization_membership.member -> Guinea\n# Answer:\nGuinea", "# Reasoning Path:\nAfrican Union -> organization.membership_organization.members -> m.010p154h -> organization.organization_membership.member -> Niger\n# Answer:\nNiger", "# Reasoning Path:\nAfrican Union -> organization.membership_organization.members -> m.049yh0_ -> organization.organization_membership.member -> Tanzania\n# Answer:\nTanzania"], "ground_truth": ["Addis Ababa"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-1785", "prediction": ["# Reasoning Path:\nKim Richards -> film.actor.film -> m.0131hj6c -> film.performance.film -> Nanny and the Professor\n# Answer:\nNanny and the Professor", "# Reasoning Path:\nKim Richards -> film.actor.film -> m.02vblh0 -> film.performance.film -> Devil Dog: The Hound of Hell\n# Answer:\nDevil Dog: The Hound of Hell", "# Reasoning Path:\nKim Richards -> tv.tv_actor.starring_roles -> m.0bngb3y -> tv.regular_tv_appearance.series -> Hello, Larry\n# Answer:\nHello, Larry", "# Reasoning Path:\nKim Richards -> film.actor.film -> m.03jrr4j -> film.performance.film -> No Deposit, No Return\n# Answer:\nNo Deposit, No Return", "# Reasoning Path:\nKim Richards -> tv.tv_actor.starring_roles -> m.0bngq3l -> tv.regular_tv_appearance.series -> James at 15\n# Answer:\nJames at 15", "# Reasoning Path:\nKim Richards -> tv.tv_actor.starring_roles -> m.0bngyp9 -> tv.regular_tv_appearance.series -> Nanny and the Professor\n# Answer:\nNanny and the Professor", "# Reasoning Path:\nKim Richards -> people.person.parents -> Kathleen Richards -> people.person.spouse_s -> m.0j4js1t\n# Answer:\nKathleen Richards", "# Reasoning Path:\nKim Richards -> film.actor.film -> m.02vblh0 -> film.performance.character -> Bonnie Barry\n# Answer:\nBonnie Barry", "# Reasoning Path:\nKim Richards -> tv.tv_actor.starring_roles -> m.0bngb3y -> tv.regular_tv_appearance.character -> Ruthie Alder\n# Answer:\nRuthie Alder", "# Reasoning Path:\nKim Richards -> people.person.parents -> Kathleen Richards -> people.person.spouse_s -> m.0kv32c8\n# Answer:\nKathleen Richards"], "ground_truth": ["Tuff Turf", "The Car", "Return from Witch Mountain", "The Whiz Kid and the Mystery at Riverton", "Race to Witch Mountain", "Nanny and the Professor", "Black Snake Moan", "Meatballs Part II", "Devil Dog: The Hound of Hell", "Escape to Witch Mountain", "Raid on Entebbe", "No Deposit, No Return"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.3076923076923077, "ans_precission": 0.4, "ans_recall": 0.25, "path_f1": 0.34782608695652173, "path_precision": 0.4, "path_recall": 0.3076923076923077, "path_ans_f1": 0.3076923076923077, "path_ans_precision": 0.4, "path_ans_recall": 0.25}
{"id": "WebQTest-1788", "prediction": ["# Reasoning Path:\nHenri Rousseau -> common.topic.article -> m.02rdfj\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> The Body in Pieces\n# Answer:\nModern art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nModern art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Eyewitness: Reports from an Art World in Crisis\n# Answer:\nModern art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Origins, Imitation, Conventions\n# Answer:\nModern art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> visual_art.art_period_movement.associated_artworks -> Elemental: The Power of Illuminated Love\n# Answer:\nModern art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Na\u00efve art -> visual_art.art_period_movement.associated_artists -> Adolf Dietrich\n# Answer:\nNa\u00efve art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Na\u00efve art -> base.schemastaging.context_name.pronunciation -> g.125_s1w5k\n# Answer:\nNa\u00efve art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> visual_art.art_period_movement.associated_artworks -> Flayed Image\n# Answer:\nModern art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Post-Impressionism -> visual_art.art_period_movement.associated_artists -> Albert Lebourg\n# Answer:\nPost-Impressionism"], "ground_truth": ["Modern art", "Primitivism", "Na\u00efve art", "Post-Impressionism"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.8181818181818182, "path_precision": 0.9, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-1789", "prediction": ["# Reasoning Path:\nJohn Edwards -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Senator\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Edwards -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> Ambassador\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Edwards -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> Diplomat\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Edwards -> people.person.religion -> United Methodist Church -> religion.religion.founding_figures -> John Wesley\n# Answer:\nUnited Methodist Church", "# Reasoning Path:\nJohn Edwards -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdn_\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Edwards -> people.person.profession -> Lawyer -> common.topic.subject_of -> Lawyer SEO Services\n# Answer:\nLawyer", "# Reasoning Path:\nJohn Edwards -> people.person.profession -> Politician -> people.profession.specializations -> AIDS Policy Specialist\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Edwards -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdq3\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Edwards -> tv.tv_actor.guest_roles -> m.0bv_ry6 -> tv.tv_guest_role.episodes_appeared_in -> Hillary Clinton, Patrick Murphy, John Edwards, Barack Obama\n# Answer:\nHillary Clinton, Patrick Murphy, John Edwards, Barack Obama", "# Reasoning Path:\nJohn Edwards -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101h_l2\n# Answer:\nPolitician"], "ground_truth": ["Political corruption"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-179", "prediction": ["# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Neo-impressionism -> visual_art.art_period_movement.associated_artists -> Th\u00e9o van Rysselberghe\n# Answer:\nNeo-impressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Neo-impressionism -> visual_art.art_period_movement.associated_artists -> Albert Dubois-Pillet\n# Answer:\nNeo-impressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Neo-impressionism -> visual_art.art_period_movement.associated_artworks -> Sunday\n# Answer:\nNeo-impressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Fauvism -> visual_art.art_period_movement.associated_artworks -> Bathers by a River\n# Answer:\nFauvism", "# Reasoning Path:\nHenri Matisse -> book.book_subject.works -> Matisse -> common.topic.notable_for -> g.125dhx5l1\n# Answer:\nMatisse", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Neo-impressionism -> visual_art.art_period_movement.associated_artists -> Alfred William Finch\n# Answer:\nNeo-impressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Neo-impressionism -> common.topic.image -> Georges Seurat - Un dimanche apr\u00c3\u00a8s-midi \u00c3\u00a0 l'\u00c3\u008ele de la Grande Jatte v2\n# Answer:\nNeo-impressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Fauvism -> common.topic.notable_for -> g.1257w3_9g\n# Answer:\nFauvism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Eyewitness: Reports from an Art World in Crisis\n# Answer:\nModern art", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nModern art"], "ground_truth": ["Impressionism", "Neo-impressionism", "Modernism", "Modern art", "Fauvism"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.8470588235294118, "ans_precission": 0.9, "ans_recall": 0.8, "path_f1": 0.7200000000000001, "path_precision": 0.9, "path_recall": 0.6, "path_ans_f1": 0.8470588235294118, "path_ans_precision": 0.9, "path_ans_recall": 0.8}
{"id": "WebQTest-1790", "prediction": ["# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> film.film_location.featured_in_films -> A View to a Kill\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> film.film_location.featured_in_films -> Arena\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> location.place_with_neighborhoods.neighborhoods -> Adams Point\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> film.film_location.featured_in_films -> Baraka\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> government.governmental_jurisdiction.governing_officials -> m.04cpv8p\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> location.place_with_neighborhoods.neighborhoods -> Bella Vista\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> people.person.sibling_s -> m.0yqb6x_ -> people.sibling_relationship.sibling -> Lee Edward Newton\n# Answer:\nLee Edward Newton", "# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> location.place_with_neighborhoods.neighborhoods -> Brookfield Village, Oakland, California\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> government.governmental_jurisdiction.governing_officials -> m.05n26hx\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> people.person.sibling_s -> m.0yqb7hh -> people.sibling_relationship.sibling -> Walter Newton\n# Answer:\nWalter Newton"], "ground_truth": ["Oakland"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1791", "prediction": ["# Reasoning Path:\nPrinting press -> law.invention.inventor -> Johannes Gutenberg -> people.person.place_of_birth -> Mainz\n# Answer:\nJohannes Gutenberg", "# Reasoning Path:\nPrinting press -> law.invention.inventor -> Johannes Gutenberg -> base.argumentmaps.innovator.original_ideas -> Movable type\n# Answer:\nJohannes Gutenberg", "# Reasoning Path:\nPrinting press -> common.topic.article -> m.05sy2\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nPrinting press -> base.ontologies.ontology_instance.equivalent_instances -> m.09dvt_4 -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nOpenCyc"], "ground_truth": ["1440"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1792", "prediction": ["# Reasoning Path:\nMidlothian -> location.location.containedby -> Scotland -> location.location.containedby -> United Kingdom\n# Answer:\nScotland", "# Reasoning Path:\nMidlothian -> location.location.geolocation -> m.02_h136\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nMidlothian -> location.location.containedby -> United Kingdom -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nMidlothian -> location.location.containedby -> United Kingdom -> location.location.containedby -> Europe\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nMidlothian -> location.location.containedby -> Scotland -> location.administrative_division.country -> United Kingdom\n# Answer:\nScotland", "# Reasoning Path:\nMidlothian -> location.location.containedby -> United Kingdom -> location.location.containedby -> Western Europe\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nMidlothian -> location.location.containedby -> United Kingdom -> base.aareas.schema.administrative_area.pertinent_type -> UK constituent country\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nMidlothian -> location.location.containedby -> United Kingdom, with Dependencies and Territories -> base.aareas.schema.administrative_area.administrative_children -> United Kingdom\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nMidlothian -> location.location.contains -> Bilston -> location.location.containedby -> United Kingdom\n# Answer:\nBilston", "# Reasoning Path:\nMidlothian -> location.location.containedby -> United Kingdom -> location.country.second_level_divisions -> Aberdeen\n# Answer:\nUnited Kingdom"], "ground_truth": ["United Kingdom", "Scotland", "United Kingdom, with Dependencies and Territories"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1793", "prediction": ["# Reasoning Path:\nMarta -> sports.pro_athlete.teams -> m.04gd4np -> sports.sports_team_roster.team -> Ume\u00e5 IK\n# Answer:\nUme\u00e5 IK", "# Reasoning Path:\nMarta -> sports.pro_athlete.teams -> m.0z3vl6r -> sports.sports_team_roster.team -> Los Angeles Sol\n# Answer:\nLos Angeles Sol", "# Reasoning Path:\nMarta -> sports.pro_athlete.teams -> m.0z3vp21 -> sports.sports_team_roster.team -> Tyres\u00f6 FF\n# Answer:\nTyres\u00f6 FF", "# Reasoning Path:\nMarta -> sports.pro_athlete.teams -> m.04gd4np -> sports.sports_team_roster.position -> Forward\n# Answer:\nForward", "# Reasoning Path:\nMarta -> people.person.places_lived -> m.0wkn5gz -> people.place_lived.location -> Dois Riachos\n# Answer:\nDois Riachos", "# Reasoning Path:\nMarta -> sports.pro_athlete.teams -> m.0z3vl6r -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nMarta -> sports.pro_athlete.teams -> m.0z3vp21 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nMarta -> people.person.places_lived -> m.03pr_w0 -> people.place_lived.location -> Alagoas\n# Answer:\nAlagoas", "# Reasoning Path:\nMarta -> sports.pro_athlete.teams -> m.0z3vp21 -> sports.sports_team_roster.position -> Forward\n# Answer:\nForward", "# Reasoning Path:\nMarta -> people.person.places_lived -> m.0kdb3nq -> people.place_lived.location -> Tyres\u00f6 Municipality\n# Answer:\nTyres\u00f6 Municipality"], "ground_truth": ["Brazil women's national football team", "Tyres\u00f6 FF"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.16666666666666669, "ans_precission": 0.1, "ans_recall": 0.5, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 0.1, "path_ans_recall": 0.5}
{"id": "WebQTest-1794", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.place_of_birth -> Detroit -> base.biblioness.bibs_location.state -> Michigan\n# Answer:\nDetroit", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.parents -> Anna Amelia Romney\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.parents -> Gaskell Romney\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.place_of_birth -> Colonia Dubl\u00e1n\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> Lenore Romney -> people.person.parents -> Harold A. Lafount\n# Answer:\nLenore Romney", "# Reasoning Path:\nMitt Romney -> people.person.place_of_birth -> Detroit -> location.location.containedby -> Wayne County\n# Answer:\nDetroit", "# Reasoning Path:\nMitt Romney -> people.person.place_of_birth -> Detroit -> location.location.containedby -> Area code 313\n# Answer:\nDetroit", "# Reasoning Path:\nMitt Romney -> people.person.parents -> Lenore Romney -> people.person.place_of_birth -> Logan\n# Answer:\nLenore Romney", "# Reasoning Path:\nMitt Romney -> people.person.place_of_birth -> Detroit -> location.location.containedby -> Michigan\n# Answer:\nDetroit", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0_gvz4k -> film.personal_film_appearance.film -> Mitt\n# Answer:\nMitt"], "ground_truth": ["Detroit"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1795", "prediction": ["# Reasoning Path:\nJohn Terry -> people.person.children -> Georgie John Terry -> people.person.parents -> Toni Terry\n# Answer:\nGeorgie John Terry", "# Reasoning Path:\nJohn Terry -> people.person.children -> Georgie John Terry -> common.topic.notable_types -> Person\n# Answer:\nGeorgie John Terry", "# Reasoning Path:\nJohn Terry -> people.person.children -> Georgie John Terry -> people.person.sibling_s -> m.0k01v5f\n# Answer:\nGeorgie John Terry", "# Reasoning Path:\nJohn Terry -> people.person.children -> Summer Rose Terry -> people.person.parents -> Toni Terry\n# Answer:\nSummer Rose Terry", "# Reasoning Path:\nJohn Terry -> people.person.children -> Summer Rose Terry -> common.topic.notable_types -> Person\n# Answer:\nSummer Rose Terry", "# Reasoning Path:\nJohn Terry -> common.topic.image -> John-Terry2 -> common.image.size -> m.0292hlm\n# Answer:\nJohn-Terry2", "# Reasoning Path:\nJohn Terry -> people.person.children -> Summer Rose Terry -> common.topic.notable_for -> g.125fnd4rg\n# Answer:\nSummer Rose Terry", "# Reasoning Path:\nJohn Terry -> tv.tv_actor.guest_roles -> m.0ghp80k -> tv.tv_guest_role.episodes_appeared_in -> Cry God for Harry\n# Answer:\nCry God for Harry", "# Reasoning Path:\nJohn Terry -> common.topic.image -> JohnTerry -> common.image.size -> m.04r7rwn\n# Answer:\nJohnTerry", "# Reasoning Path:\nJohn Terry -> tv.tv_actor.guest_roles -> m.09nxf_l -> tv.tv_guest_role.episodes_appeared_in -> Episode 1\n# Answer:\nEpisode 1"], "ground_truth": ["Georgie John Terry", "Summer Rose Terry"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1796", "prediction": ["# Reasoning Path:\nMichael Jackson -> music.artist.origin -> Gary -> location.location.containedby -> United States of America\n# Answer:\nGary", "# Reasoning Path:\nMichael Jackson -> people.person.place_of_birth -> Gary -> location.location.containedby -> United States of America\n# Answer:\nGary", "# Reasoning Path:\nMichael Jackson -> film.person_or_entity_appearing_in_film.films -> m.010wz0tg -> film.personal_film_appearance.film -> Live at Wembley July 16, 1988\n# Answer:\nLive at Wembley July 16, 1988", "# Reasoning Path:\nMichael Jackson -> film.person_or_entity_appearing_in_film.films -> m.010wz0tg -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nMichael Jackson -> film.person_or_entity_appearing_in_film.films -> m.08mvytl -> film.personal_film_appearance.film -> Michael Jackson's This Is It\n# Answer:\nMichael Jackson's This Is It", "# Reasoning Path:\nMichael Jackson -> film.person_or_entity_appearing_in_film.films -> m.011qz88l -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nMichael Jackson -> film.person_or_entity_appearing_in_film.films -> m.011qz88l -> film.personal_film_appearance.film -> Michael: The Last Photo Shoots\n# Answer:\nMichael: The Last Photo Shoots", "# Reasoning Path:\nMichael Jackson -> film.person_or_entity_appearing_in_film.films -> m.08mvytl -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself"], "ground_truth": ["Gary"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1797", "prediction": ["# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> military.military_combatant.military_commanders -> m.048z_8v\n# Answer:\nConfederate States of America", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> military.military_combatant.military_commanders -> m.049y2m5\n# Answer:\nConfederate States of America", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> book.book_subject.works -> An Honorable Defeat: The Last Days of the Confederate Government\n# Answer:\nConfederate States of America", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> John C. Pemberton -> people.person.education -> m.04hw096\n# Answer:\nJohn C. Pemberton", "# Reasoning Path:\nSiege of Vicksburg -> military.military_conflict.military_personnel_involved -> William F. Draper -> military.military_person.participated_in_conflicts -> Siege of Petersburg\n# Answer:\nWilliam F. Draper", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> military.military_combatant.military_commanders -> m.049y2qw\n# Answer:\nConfederate States of America", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> location.country.capital -> Richmond\n# Answer:\nConfederate States of America", "# Reasoning Path:\nSiege of Vicksburg -> military.military_conflict.military_personnel_involved -> William F. Draper -> common.topic.notable_for -> g.12594n0fl\n# Answer:\nWilliam F. Draper", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> book.book_subject.works -> Apostles of Disunion: Southern Secession Commissioners and the Causes of the Civil War\n# Answer:\nConfederate States of America", "# Reasoning Path:\nSiege of Vicksburg -> military.military_conflict.military_personnel_involved -> William F. Draper -> military.military_person.participated_in_conflicts -> American Civil War\n# Answer:\nWilliam F. Draper"], "ground_truth": ["John C. Pemberton", "Ulysses S. Grant", "United States of America", "Union", "Confederate States of America"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.509090909090909, "ans_precission": 0.7, "ans_recall": 0.4, "path_f1": 0.509090909090909, "path_precision": 0.7, "path_recall": 0.4, "path_ans_f1": 0.6461538461538462, "path_ans_precision": 0.7, "path_ans_recall": 0.6}
{"id": "WebQTest-1799", "prediction": ["# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Playwright -> base.descriptive_names.names.descriptive_name -> m.01066t9b\n# Answer:\nPlaywright", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Playwright -> people.profession.specialization_of -> Writer\n# Answer:\nPlaywright", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Novelist -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nNovelist", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Playwright -> education.field_of_study.students_majoring -> m.012nj6qx\n# Answer:\nPlaywright", "# Reasoning Path:\nNikolai Gogol -> book.author.book_editions_published -> Dead Souls -> book.book.genre -> Fiction\n# Answer:\nDead Souls", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Playwright -> base.descriptive_names.names.descriptive_name -> m.01066tcy\n# Answer:\nPlaywright", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Novelist -> people.profession.specialization_of -> Writer\n# Answer:\nNovelist", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Playwright -> base.descriptive_names.names.descriptive_name -> m.01066tpc\n# Answer:\nPlaywright", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Novelist -> common.topic.notable_types -> Profession\n# Answer:\nNovelist", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Writer -> people.profession.specializations -> Playwright\n# Answer:\nWriter"], "ground_truth": ["Writer", "Playwright", "Novelist"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-180", "prediction": ["# Reasoning Path:\nMissouri River -> geography.river.mouth -> Mississippi River -> geography.river.mouth -> Gulf of Mexico\n# Answer:\nMississippi River", "# Reasoning Path:\nMissouri River -> geography.river.mouth -> Mississippi River -> location.location.containedby -> United States of America\n# Answer:\nMississippi River", "# Reasoning Path:\nMissouri River -> geography.river.mouth -> Mississippi River -> location.location.partially_containedby -> Mississippi\n# Answer:\nMississippi River", "# Reasoning Path:\nMissouri River -> geography.river.mouth -> Mississippi River -> location.location.containedby -> North America\n# Answer:\nMississippi River", "# Reasoning Path:\nMissouri River -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nMissouri River -> common.topic.notable_types -> River -> type.type.domain -> Physical Geography\n# Answer:\nRiver", "# Reasoning Path:\nMissouri River -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Mississippi\n# Answer:\nUnited States of America", "# Reasoning Path:\nMissouri River -> common.topic.notable_types -> River -> type.type.properties -> Mouth\n# Answer:\nRiver", "# Reasoning Path:\nMissouri River -> geography.river.mouth -> Mississippi River -> location.location.partially_containedby -> Tennessee\n# Answer:\nMississippi River", "# Reasoning Path:\nMissouri River -> common.topic.notable_types -> River -> freebase.type_profile.published -> Published\n# Answer:\nRiver"], "ground_truth": ["Mississippi River"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1800", "prediction": ["# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> sports.sports_league.seasons -> 2012 Major League Baseball season\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> sports.sports_league.seasons -> 1974 Major League Baseball Season\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> organization.organization.child -> m.0ck2ds6\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> business.employer.employees -> m.010vsyz7\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> organization.organization.child -> m.0ck2dsc\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> common.topic.webpage -> m.09w8snm -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> organization.organization.child -> m.0zs6st6\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> business.employer.employees -> m.0cmxjcn\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> common.topic.image -> Crowd outside the 1903 World Series -> common.image.size -> m.02bpf1c\n# Answer:\nCrowd outside the 1903 World Series", "# Reasoning Path:\nWorld Series -> common.topic.webpage -> m.09w8snm -> common.webpage.resource -> Ratings: Obama's ad, World Series clincher top the week\n# Answer:\nRatings: Obama's ad, World Series clincher top the week"], "ground_truth": ["1903 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1801", "prediction": ["# Reasoning Path:\nSimon Birch -> film.film_character.portrayed_in_films -> m.0k39m5 -> film.performance.actor -> Ian Michael Smith\n# Answer:\nIan Michael Smith", "# Reasoning Path:\nSimon Birch -> award.award_nominated_work.award_nominations -> m.0n4xs9w -> award.award_nomination.award_nominee -> Ian Michael Smith\n# Answer:\nIan Michael Smith", "# Reasoning Path:\nSimon Birch -> award.award_nominated_work.award_nominations -> m.0n4xs9w -> award.award_nomination.award -> Critics' Choice Movie Award for Best Young Performer\n# Answer:\nCritics' Choice Movie Award for Best Young Performer", "# Reasoning Path:\nSimon Birch -> award.award_winning_work.awards_won -> m.09tr1z6 -> award.award_honor.award_winner -> Ian Michael Smith\n# Answer:\nIan Michael Smith", "# Reasoning Path:\nSimon Birch -> award.award_winning_work.awards_won -> m.09tr1z6 -> award.award_honor.ceremony -> 4th Critics' Choice Awards\n# Answer:\n4th Critics' Choice Awards"], "ground_truth": ["Ian Michael Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1802", "prediction": ["# Reasoning Path:\nUnited States of America -> location.location.time_zones -> Hawaii-Aleutian Time Zone\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nUnited States of America -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nPacific Time Zone", "# Reasoning Path:\nUnited States of America -> location.location.time_zones -> Samoa Time Zone -> time.time_zone.locations_in_this_time_zone -> American Samoa\n# Answer:\nSamoa Time Zone", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.location.time_zones -> Samoa Time Zone -> time.time_zone.locations_in_this_time_zone -> Palmyra Atoll\n# Answer:\nSamoa Time Zone", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> periodicals.newspaper_circulation_area.newspapers -> China Daily -> book.newspaper.circulation_areas -> Washington, D.C.\n# Answer:\nChina Daily", "# Reasoning Path:\nUnited States of America -> periodicals.newspaper_circulation_area.newspapers -> American Free Press -> book.periodical.language -> English Language\n# Answer:\nAmerican Free Press", "# Reasoning Path:\nUnited States of America -> periodicals.newspaper_circulation_area.newspapers -> China Daily -> book.newspaper.circulation_areas -> New York City\n# Answer:\nChina Daily", "# Reasoning Path:\nUnited States of America -> periodicals.newspaper_circulation_area.newspapers -> American Free Press -> common.topic.notable_types -> Newspaper\n# Answer:\nAmerican Free Press"], "ground_truth": ["Atlantic Time Zone", "Mountain Time Zone", "Samoa Time Zone", "Central Time Zone", "Pacific Time Zone", "Chamorro Time Zone", "Eastern Time Zone", "Alaska Time Zone", "Hawaii-Aleutian Time Zone"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.4, "ans_recall": 0.3333333333333333, "path_f1": 0.3636363636363636, "path_precision": 0.4, "path_recall": 0.3333333333333333, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.4, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1803", "prediction": ["# Reasoning Path:\nGeneral Motors -> business.business_operation.industry -> Motor Vehicles and Passenger Car Bodies -> business.industry.companies -> Oakland Motor Car Company\n# Answer:\nMotor Vehicles and Passenger Car Bodies", "# Reasoning Path:\nGeneral Motors -> common.topic.subjects -> GMC -> common.topic.subjects -> GMC Yukon\n# Answer:\nGMC", "# Reasoning Path:\nGeneral Motors -> business.business_operation.industry -> Motor Vehicles and Passenger Car Bodies -> business.industry.companies -> Cadillac\n# Answer:\nMotor Vehicles and Passenger Car Bodies", "# Reasoning Path:\nGeneral Motors -> business.business_operation.industry -> Motor Vehicles and Passenger Car Bodies -> common.topic.notable_types -> Industry\n# Answer:\nMotor Vehicles and Passenger Car Bodies", "# Reasoning Path:\nGeneral Motors -> automotive.company.make_s -> Hummer -> automotive.make.parent_company -> AM General\n# Answer:\nHummer", "# Reasoning Path:\nGeneral Motors -> business.business_operation.industry -> Motor Vehicles and Passenger Car Bodies -> business.industry.child_industry -> Motor Vehicles and Passenger Car Bodies (automobiles)\n# Answer:\nMotor Vehicles and Passenger Car Bodies", "# Reasoning Path:\nGeneral Motors -> common.topic.subjects -> GMC -> common.topic.subjects -> GMC Acadia\n# Answer:\nGMC", "# Reasoning Path:\nGeneral Motors -> business.business_operation.industry -> Motor Vehicles and Passenger Car Bodies -> business.industry.companies -> American Motors\n# Answer:\nMotor Vehicles and Passenger Car Bodies", "# Reasoning Path:\nGeneral Motors -> business.business_operation.industry -> Automotive industry -> common.topic.subjects -> Towing\n# Answer:\nAutomotive industry", "# Reasoning Path:\nGeneral Motors -> common.topic.subjects -> GMC -> common.topic.subjects -> GMC Canyon\n# Answer:\nGMC"], "ground_truth": ["Motor Vehicles and Passenger Car Bodies", "Automotive industry"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1804", "prediction": ["# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Wales\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Cornish Language -> language.human_language.countries_spoken_in -> England\n# Answer:\nCornish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> British Sign Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nBritish Sign Language", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nChild pornography", "# Reasoning Path:\nUnited Kingdom -> location.statistical_region.long_term_unemployment_rate -> g.12cp_jvpx\n# Answer:\nlocation.statistical_region.long_term_unemployment_rate", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> Australia\n# Answer:\nEnglish Language"], "ground_truth": ["Esperanto Language", "Cornish Language", "Irish", "Welsh Language", "English Language", "Scots Language", "Scottish Gaelic", "British Sign Language"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.5106382978723405, "ans_precission": 0.8, "ans_recall": 0.375, "path_f1": 0.22222222222222224, "path_precision": 0.2, "path_recall": 0.25, "path_ans_f1": 0.5106382978723405, "path_ans_precision": 0.8, "path_ans_recall": 0.375}
{"id": "WebQTest-1805", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Costa Rica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.region -> Americas\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Louisiana\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Mississippi\n# Answer:\n1812 Louisiana hurricane"], "ground_truth": ["Jamaican Creole English Language", "Jamaican English"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1806", "prediction": ["# Reasoning Path:\nLeBron James -> sports.pro_athlete.sports_played_professionally -> m.0c3p_f6 -> sports.pro_sports_played.sport -> Basketball\n# Answer:\nBasketball", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.position_s -> Power forward -> sports.sports_position.sport -> Basketball\n# Answer:\nPower forward", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.position_s -> Point forward -> sports.sports_position.players -> g.11byb3s7cc\n# Answer:\nPoint forward", "# Reasoning Path:\nLeBron James -> sports.pro_athlete.sports_played_professionally -> m.0c3p_f6 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.position_s -> Small forward -> common.topic.image -> John Salmons\n# Answer:\nSmall forward", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.position_s -> Point forward -> sports.sports_position.players -> m.010fwg1m\n# Answer:\nPoint forward", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.position_s -> Point forward -> common.topic.image -> Basketball half-court\n# Answer:\nPoint forward", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.position_s -> Point forward -> sports.sports_position.players -> m.010fyfng\n# Answer:\nPoint forward", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.position_s -> Point forward -> basketball.basketball_position.players -> Anton Ponkrashov\n# Answer:\nPoint forward", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.position_s -> Power forward -> common.topic.notable_for -> g.125509zsm\n# Answer:\nPower forward"], "ground_truth": ["Shooting guard", "Small forward", "Forward", "Power forward", "Point forward"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.8000000000000002, "ans_precission": 0.8, "ans_recall": 0.8, "path_f1": 0.6153846153846154, "path_precision": 0.8, "path_recall": 0.5, "path_ans_f1": 0.8000000000000002, "path_ans_precision": 0.8, "path_ans_recall": 0.8}
{"id": "WebQTest-1807", "prediction": ["# Reasoning Path:\nWayne Gretzky -> sports.pro_athlete.teams -> m.07vkdw8 -> sports.sports_team_roster.team -> Edmonton Oilers\n# Answer:\nEdmonton Oilers", "# Reasoning Path:\nWayne Gretzky -> sports.pro_athlete.teams -> m.07vkdw8 -> sports.sports_team_roster.position -> Centerman\n# Answer:\nCenterman", "# Reasoning Path:\nWayne Gretzky -> sports.pro_athlete.teams -> m.0125clwk -> sports.sports_team_roster.position -> Centerman\n# Answer:\nCenterman", "# Reasoning Path:\nWayne Gretzky -> people.person.place_of_birth -> Brantford -> travel.travel_destination.tourist_attractions -> Wayne Gretzky Sports Centre\n# Answer:\nBrantford", "# Reasoning Path:\nWayne Gretzky -> people.person.place_of_birth -> Brantford -> government.governmental_jurisdiction.government_bodies -> Brantford City Council\n# Answer:\nBrantford", "# Reasoning Path:\nWayne Gretzky -> people.person.place_of_birth -> Brantford -> sports.sports_team_location.teams -> Brantford Smoke\n# Answer:\nBrantford", "# Reasoning Path:\nWayne Gretzky -> people.person.place_of_birth -> Brantford -> travel.travel_destination.tourist_attractions -> Arnold Anderson Stadium at Cockshutt Park\n# Answer:\nBrantford", "# Reasoning Path:\nWayne Gretzky -> people.person.place_of_birth -> Brantford -> travel.travel_destination.tourist_attractions -> Bell Telephone Memorial\n# Answer:\nBrantford", "# Reasoning Path:\nWayne Gretzky -> people.person.profession -> Athlete -> people.profession.specializations -> Alpine skier\n# Answer:\nAthlete", "# Reasoning Path:\nWayne Gretzky -> people.person.profession -> Athlete -> common.topic.notable_types -> Profession\n# Answer:\nAthlete"], "ground_truth": ["Edmonton Oilers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1808", "prediction": ["# Reasoning Path:\nPeru -> government.governmental_jurisdiction.governing_officials -> m.011c1dkp -> government.government_position_held.appointed_by -> Ollanta Humala\n# Answer:\nOllanta Humala", "# Reasoning Path:\nPeru -> government.governmental_jurisdiction.governing_officials -> m.0yp_d4w -> government.government_position_held.appointed_by -> Ollanta Humala\n# Answer:\nOllanta Humala", "# Reasoning Path:\nPeru -> government.governmental_jurisdiction.governing_officials -> m.011c1dkp -> government.government_position_held.basic_title -> Prime minister\n# Answer:\nPrime minister", "# Reasoning Path:\nPeru -> government.governmental_jurisdiction.governing_officials -> m.0yp_d4w -> government.government_position_held.office_holder -> Juan Jim\u00e9nez Mayor\n# Answer:\nJuan Jim\u00e9nez Mayor", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.11b71n53cl\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nPeru -> location.statistical_region.energy_use_per_capita -> g.1245_8rt3\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nPeru -> government.governmental_jurisdiction.governing_officials -> m.0yp_d4w -> government.government_position_held.office_position_or_title -> Prime Minister of Peru\n# Answer:\nPrime Minister of Peru", "# Reasoning Path:\nPeru -> government.governmental_jurisdiction.governing_officials -> m.010gf_3c -> government.government_position_held.office_holder -> Alberto Fujimori\n# Answer:\nAlberto Fujimori", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.1hhc37ctc\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.1hhc38qlv\n# Answer:\nlocation.statistical_region.external_debt_stock"], "ground_truth": ["Ollanta Humala"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.23529411764705882, "path_precision": 0.2, "path_recall": 0.2857142857142857, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1809", "prediction": ["# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Gibson SG -> music.guitar.guitarists -> The Edge\n# Answer:\nGibson SG", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Gibson SG -> business.product_line.producer_s -> m.0p62thw\n# Answer:\nGibson SG", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Gibson SG -> common.topic.image -> SG LPbody3\n# Answer:\nGibson SG", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Gibson Les Paul -> music.guitar.brand -> Gibson Guitar Corporation\n# Answer:\nGibson Les Paul", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Gibson SG -> music.guitar.guitarists -> Angus Young\n# Answer:\nGibson SG", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Fender Stratocaster -> music.guitar.guitarists -> Adrian Smith\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Fender Stratocaster -> common.topic.image -> Fender strat\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Gibson SG -> music.guitar.guitarists -> Eric Clapton\n# Answer:\nGibson SG", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Fender Stratocaster -> symbols.namesake.named_after -> Leo Fender\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Fender Stratocaster -> music.guitar.guitarists -> Aishwarya Uniyal\n# Answer:\nFender Stratocaster"], "ground_truth": ["Gibson Les Paul", "Fender Stratocaster", "Gibson SG"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-181", "prediction": ["# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yg\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nThailand -> location.statistical_region.gdp_growth_rate -> g.11b60tqlwz\n# Answer:\nlocation.statistical_region.gdp_growth_rate", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6gh2z\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.1hhc37pvk\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nThailand -> location.statistical_region.gdp_growth_rate -> g.1245_1qhj\n# Answer:\nlocation.statistical_region.gdp_growth_rate", "# Reasoning Path:\nThailand -> location.statistical_region.gdp_growth_rate -> g.1245_2hrc\n# Answer:\nlocation.statistical_region.gdp_growth_rate"], "ground_truth": ["Thai baht"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1810", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Raphael and his age\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Leonardo da Vinci and the Art of Sculpture: Inspiration and Invention\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> people.profession.people_with_this_profession -> Sandro Botticelli\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi -> visual_art.artwork.locations -> m.0pcqbm3\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> influence.influence_node.influenced_by -> Melozzo da Forl\u00ec\n# Answer:\nAntonio da Correggio", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi -> visual_art.artwork.art_subject -> Biblical Magi\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Annunciation -> common.topic.notable_types -> Artwork\n# Answer:\nAnnunciation", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Bernardino Luini -> visual_art.visual_artist.art_forms -> Painting\n# Answer:\nBernardino Luini", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> common.topic.subject_of -> BRS Custom Painting\n# Answer:\nPainting"], "ground_truth": ["Painting", "Sculpture", "Drawing"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1811", "prediction": ["# Reasoning Path:\nNew Jersey -> government.political_district.representatives -> m.0y7nq48 -> government.government_position_held.office_holder -> Cory Booker\n# Answer:\nCory Booker", "# Reasoning Path:\nNew Jersey -> government.political_district.representatives -> m.05kcsw0 -> government.government_position_held.office_holder -> Bob Menendez\n# Answer:\nBob Menendez", "# Reasoning Path:\nNew Jersey -> government.political_district.representatives -> m.0y7nq48 -> government.government_position_held.basic_title -> Senator\n# Answer:\nSenator", "# Reasoning Path:\nNew Jersey -> government.political_district.representatives -> m.05kcjc9 -> government.government_position_held.office_holder -> Frank Lautenberg\n# Answer:\nFrank Lautenberg", "# Reasoning Path:\nNew Jersey -> government.governmental_jurisdiction.governing_officials -> m.010f1q8l -> government.government_position_held.office_holder -> Kim Guadagno\n# Answer:\nKim Guadagno", "# Reasoning Path:\nNew Jersey -> government.governmental_jurisdiction.governing_officials -> m.025m17c -> government.government_position_held.office_holder -> Jon Corzine\n# Answer:\nJon Corzine", "# Reasoning Path:\nNew Jersey -> government.governmental_jurisdiction.governing_officials -> m.04j5sjd -> government.government_position_held.office_holder -> Woodrow Wilson\n# Answer:\nWoodrow Wilson", "# Reasoning Path:\nNew Jersey -> government.political_district.representatives -> m.05kcsw0 -> government.government_position_held.legislative_sessions -> 111th United States Congress\n# Answer:\n111th United States Congress", "# Reasoning Path:\nNew Jersey -> government.political_district.representatives -> m.05kcsw0 -> freebase.valuenotation.is_reviewed -> Appointed By (if Position is Appointed)\n# Answer:\nAppointed By (if Position is Appointed)", "# Reasoning Path:\nNew Jersey -> government.political_district.representatives -> m.05kcsw0 -> government.government_position_held.legislative_sessions -> 109th United States Congress\n# Answer:\n109th United States Congress"], "ground_truth": ["Cory Booker", "Bob Menendez"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1812", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> base.aareas.schema.administrative_area.administrative_parent -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.location.contains -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.country.administrative_divisions -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> common.topic.notable_types -> Country\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.064_ltw -> base.popstra.infidelity.participant -> Kanye West\n# Answer:\nKanye West", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> m.010_ydxv -> music.track_contribution.track -> If I Never See Your Face Again\n# Answer:\nIf I Never See Your Face Again"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1813", "prediction": ["# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010r25mp -> soccer.football_goal.point_awarded_to -> Argentina national football team\n# Answer:\nArgentina national football team", "# Reasoning Path:\nLionel Messi -> base.schemastaging.athlete_extra.salary -> m.0qtkxlg -> base.schemastaging.athlete_salary.team -> FC Barcelona\n# Answer:\nFC Barcelona", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010r25mp -> soccer.football_goal.match -> 2014 Argentina vs Slovenia friendly match\n# Answer:\n2014 Argentina vs Slovenia friendly match", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010h62gm -> soccer.football_goal.point_awarded_to -> FC Barcelona\n# Answer:\nFC Barcelona", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010gkw62 -> soccer.football_goal.point_awarded_to -> FC Barcelona\n# Answer:\nFC Barcelona", "# Reasoning Path:\nLionel Messi -> base.schemastaging.athlete_extra.salary -> m.0110zlj9 -> base.schemastaging.athlete_salary.team -> FC Barcelona\n# Answer:\nFC Barcelona", "# Reasoning Path:\nLionel Messi -> base.schemastaging.athlete_extra.salary -> m.0qtkxlg -> base.schemastaging.athlete_salary.currency -> Euro\n# Answer:\nEuro", "# Reasoning Path:\nLionel Messi -> soccer.football_player.matches_played -> m.0c0ltpz -> soccer.football_player_match_participation.team -> Argentina national football team\n# Answer:\nArgentina national football team", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010gkw62 -> soccer.football_goal.match -> 2014 Barcelona F.C. vs Real Madrid C.F. football match\n# Answer:\n2014 Barcelona F.C. vs Real Madrid C.F. football match", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010h62gm -> soccer.football_goal.match -> 2014 Barcelona vs Athletic Bilbao\n# Answer:\n2014 Barcelona vs Athletic Bilbao"], "ground_truth": ["FC Barcelona", "Argentina national football team"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.41379310344827586, "path_precision": 0.6, "path_recall": 0.3157894736842105, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1815", "prediction": ["# Reasoning Path:\nLil Wayne -> people.person.education -> m.0h2_zgr -> education.education.institution -> Lafayette Elementary School\n# Answer:\nLafayette Elementary School", "# Reasoning Path:\nLil Wayne -> people.person.education -> m.0h2_zpr -> education.education.institution -> University of Phoenix\n# Answer:\nUniversity of Phoenix", "# Reasoning Path:\nLil Wayne -> people.person.education -> m.0h2_zgy -> education.education.institution -> Eleanor McMain Secondary School\n# Answer:\nEleanor McMain Secondary School", "# Reasoning Path:\nLil Wayne -> people.person.education -> m.0h2_zpr -> education.education.major_field_of_study -> Psychology\n# Answer:\nPsychology", "# Reasoning Path:\nLil Wayne -> music.artist.contribution -> m.0rg70rk -> music.recording_contribution.album -> BedRock\n# Answer:\nBedRock", "# Reasoning Path:\nLil Wayne -> music.featured_artist.albums -> 4 My Town (Play Ball) -> music.composition.composer -> Baby\n# Answer:\n4 My Town (Play Ball)", "# Reasoning Path:\nLil Wayne -> music.featured_artist.albums -> 4 My Town (Play Ball) -> music.album.featured_artists -> Drake\n# Answer:\n4 My Town (Play Ball)", "# Reasoning Path:\nLil Wayne -> music.featured_artist.albums -> 4 My Town (Play Ball) -> music.composition.composer -> Drake\n# Answer:\n4 My Town (Play Ball)", "# Reasoning Path:\nLil Wayne -> music.featured_artist.albums -> 9 Piece -> music.album.release_type -> Single\n# Answer:\n9 Piece", "# Reasoning Path:\nLil Wayne -> music.featured_artist.albums -> 4 My Town (Play Ball) -> common.topic.notable_types -> Composition\n# Answer:\n4 My Town (Play Ball)"], "ground_truth": ["Mcmain Magnet Secondary School", "University of Houston", "Eleanor McMain Secondary School", "Lafayette Elementary School", "University of Phoenix"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.3, "ans_recall": 0.6, "path_f1": 0.4, "path_precision": 0.3, "path_recall": 0.6, "path_ans_f1": 0.4, "path_ans_precision": 0.3, "path_ans_recall": 0.6}
{"id": "WebQTest-1817", "prediction": ["# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0n1m7cd -> education.education.institution -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.02wmyw7 -> education.education.institution -> Exeter College, Oxford\n# Answer:\nExeter College, Oxford", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0w48bvz -> education.education.institution -> King Edward's School, Birmingham\n# Answer:\nKing Edward's School, Birmingham", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0n1m7cd -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.02wmyw7 -> education.education.degree -> First Class Honours\n# Answer:\nFirst Class Honours", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.employment_history -> m.02ht_8b -> business.employment_tenure.company -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0n1m7cd -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.employment_history -> m.02ht_80 -> business.employment_tenure.company -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nJ. R. R. Tolkien -> book.author.works_written -> A Middle English Reader and Vocabulary -> book.written_work.author -> Kenneth Sisam\n# Answer:\nA Middle English Reader and Vocabulary", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0w48bvz -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date"], "ground_truth": ["University of Oxford", "Exeter College, Oxford", "St. Philip's School", "King Edward's School, Birmingham"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.5, "ans_recall": 0.75, "path_f1": 0.625, "path_precision": 0.5, "path_recall": 0.8333333333333334, "path_ans_f1": 0.6, "path_ans_precision": 0.5, "path_ans_recall": 0.75}
{"id": "WebQTest-1818", "prediction": ["# Reasoning Path:\nSaint Louis University -> location.location.containedby -> Missouri -> location.location.containedby -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nSaint Louis University -> location.location.containedby -> Missouri -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nSaint Louis University -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nSaint Louis University -> location.location.containedby -> St. Louis -> location.location.containedby -> Missouri\n# Answer:\nSt. Louis", "# Reasoning Path:\nSaint Louis University -> location.location.containedby -> Missouri -> location.location.containedby -> United States, with Territories\n# Answer:\nMissouri", "# Reasoning Path:\nSaint Louis University -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nSaint Louis University -> education.educational_institution.subsidiary_or_constituent_schools -> Saint Louis University School of Public Health -> organization.organization.headquarters -> m.0wytzz_\n# Answer:\nSaint Louis University School of Public Health", "# Reasoning Path:\nSaint Louis University -> common.topic.article -> m.02tzb3\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nSaint Louis University -> location.location.containedby -> Missouri -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nSaint Louis University -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Missouri\n# Answer:\nUnited States of America"], "ground_truth": ["Missouri"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1819", "prediction": ["# Reasoning Path:\nCanada -> location.statistical_region.religions -> m.05spzvl -> location.religion_percentage.religion -> Protestantism\n# Answer:\nProtestantism", "# Reasoning Path:\nCanada -> location.statistical_region.religions -> m.05spzv4 -> location.religion_percentage.religion -> Catholicism\n# Answer:\nCatholicism", "# Reasoning Path:\nCanada -> location.statistical_region.religions -> m.05spzvc -> location.religion_percentage.religion -> Islam\n# Answer:\nIslam", "# Reasoning Path:\nCanada -> periodicals.newspaper_circulation_area.newspapers -> The Catholic Register -> book.periodical.subjects -> Catholicism\n# Answer:\nThe Catholic Register", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.12cp_k6ss\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nCanada -> periodicals.newspaper_circulation_area.newspapers -> Anglican Journal -> book.periodical.subjects -> Anglicanism\n# Answer:\nAnglican Journal", "# Reasoning Path:\nCanada -> periodicals.newspaper_circulation_area.newspapers -> Anglican Planet -> book.periodical.subjects -> Anglicanism\n# Answer:\nAnglican Planet", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.1hhc38hnd\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nCanada -> periodicals.newspaper_circulation_area.newspapers -> The Catholic Register -> common.topic.notable_types -> Newspaper\n# Answer:\nThe Catholic Register", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.1hhc3d2ty\n# Answer:\nlocation.statistical_region.part_time_employment_percent"], "ground_truth": ["Islam", "Protestantism", "Catholicism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.5333333333333333, "path_precision": 0.4, "path_recall": 0.8, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-182", "prediction": ["# Reasoning Path:\nKhlo\u00e9 Kardashian -> people.person.spouse_s -> m.07y7d6c -> people.marriage.spouse -> Lamar Odom\n# Answer:\nLamar Odom", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> people.person.spouse_s -> m.07y7d6c -> people.marriage.location_of_ceremony -> Beverly Hills\n# Answer:\nBeverly Hills", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> people.person.parents -> Robert Kardashian -> people.person.children -> Rob Kardashian\n# Answer:\nRobert Kardashian", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> people.person.spouse_s -> m.07y7d6c -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> people.person.parents -> Robert Kardashian -> people.person.gender -> Male\n# Answer:\nRobert Kardashian", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> tv.tv_program_guest.appeared_on -> m.0j7zbw4 -> tv.tv_guest_personal_appearance.episode -> Bill Paxton, The Kardashians, The Ting Tings\n# Answer:\nBill Paxton, The Kardashians, The Ting Tings", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> people.person.parents -> Robert Kardashian -> symbols.name_source.namesakes -> Rob Kardashian\n# Answer:\nRobert Kardashian", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> people.person.parents -> Kris Jenner -> people.person.children -> Kim Kardashian\n# Answer:\nKris Jenner", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> tv.tv_program_guest.appeared_on -> m.0j7zbw4 -> tv.tv_guest_personal_appearance.appearance_type -> Celebrity guest\n# Answer:\nCelebrity guest", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> people.person.parents -> Robert Kardashian -> people.person.children -> Kim Kardashian\n# Answer:\nRobert Kardashian"], "ground_truth": ["Lamar Odom"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1821", "prediction": ["# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.country.form_of_government -> Unitary state\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.location.containedby -> Africa\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.country.official_language -> French\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.country.form_of_government -> Presidential system\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.country.form_of_government -> Republic\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> base.rosetta.languoid.local_name -> Spanish -> base.rosetta.local_name.locale -> Equatorial Guinea\n# Answer:\nSpanish", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Venezuela -> location.location.containedby -> South America\n# Answer:\nVenezuela", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.country.official_language -> Portuguese Language\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Venezuela -> meteorology.cyclone_affected_area.cyclones -> Hurricane Anna\n# Answer:\nVenezuela", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Andorra -> location.country.form_of_government -> Parliamentary system\n# Answer:\nAndorra"], "ground_truth": ["Spain", "Uruguay", "Nicaragua", "Colombia", "Costa Rica", "Bolivia", "Equatorial Guinea", "Ecuador", "Panama", "Argentina", "El Salvador", "Venezuela", "Dominican Republic", "Honduras", "Cuba", "Guatemala", "Chile", "Paraguay", "Peru", "Puerto Rico"], "ans_acc": 0.1, "ans_hit": 1, "ans_f1": 0.1777777777777778, "ans_precission": 0.8, "ans_recall": 0.1, "path_f1": 0.1777777777777778, "path_precision": 0.8, "path_recall": 0.1, "path_ans_f1": 0.18000000000000002, "path_ans_precision": 0.9, "path_ans_recall": 0.1}
{"id": "WebQTest-1822", "prediction": ["# Reasoning Path:\nPaul Ryan -> people.person.parents -> Paul Murray Ryan -> people.person.nationality -> United States of America\n# Answer:\nPaul Murray Ryan", "# Reasoning Path:\nPaul Ryan -> people.person.parents -> Paul Murray Ryan -> people.deceased_person.cause_of_death -> heart attack\n# Answer:\nPaul Murray Ryan", "# Reasoning Path:\nPaul Ryan -> people.person.parents -> Elizabeth A. Ryan -> people.person.gender -> Female\n# Answer:\nElizabeth A. Ryan", "# Reasoning Path:\nPaul Ryan -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPaul Ryan -> people.person.parents -> Elizabeth A. Ryan -> common.topic.notable_for -> g.126tl4m_x\n# Answer:\nElizabeth A. Ryan", "# Reasoning Path:\nPaul Ryan -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nPaul Ryan -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPaul Ryan -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nPaul Ryan -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPaul Ryan -> freebase.valuenotation.is_reviewed -> Gender -> type.property.schema -> Person\n# Answer:\nGender"], "ground_truth": ["Paul Murray Ryan", "Elizabeth A. Ryan", "Samuel Ryan", "Janna Ryan", "Elizabeth Ryan", "Charles Ryan"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.4, "ans_recall": 0.5, "path_f1": 0.2857142857142857, "path_precision": 0.4, "path_recall": 0.2222222222222222, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.4, "path_ans_recall": 0.5}
{"id": "WebQTest-1823", "prediction": ["# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Iran\n# Answer:\nParliamentary system", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary system -> common.topic.notable_types -> Form of Government\n# Answer:\nParliamentary system", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> United States of America\n# Answer:\nFederal republic", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Iraqi Kurdistan\n# Answer:\nParliamentary system", "# Reasoning Path:\nIraq -> location.statistical_region.labor_participation_rate -> g.11b71vj1zg\n# Answer:\nlocation.statistical_region.labor_participation_rate", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Federal republic -> common.topic.notable_types -> Form of Government\n# Answer:\nFederal republic", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Jordan\n# Answer:\nParliamentary system", "# Reasoning Path:\nIraq -> book.book_subject.works -> A reed shaken by the wind -> book.written_work.subjects -> Middle East\n# Answer:\nA reed shaken by the wind", "# Reasoning Path:\nIraq -> location.statistical_region.labor_participation_rate -> g.12cp_j7p6\n# Answer:\nlocation.statistical_region.labor_participation_rate", "# Reasoning Path:\nIraq -> location.statistical_region.labor_participation_rate -> g.1hhc38qnx\n# Answer:\nlocation.statistical_region.labor_participation_rate"], "ground_truth": ["Parliamentary system", "Parliamentary republic", "Republic", "Federal republic"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.6666666666666665, "ans_precission": 0.6, "ans_recall": 0.75, "path_f1": 0.5454545454545454, "path_precision": 0.6, "path_recall": 0.5, "path_ans_f1": 0.6666666666666665, "path_ans_precision": 0.6, "path_ans_recall": 0.75}
{"id": "WebQTest-1824", "prediction": ["# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Andorra\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Belgium\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60xs9d2\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> France\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc38hhk\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc3dvnk\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.1hhc39shg\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.1hhc3h715\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1825", "prediction": ["# Reasoning Path:\nLincoln Steffens -> book.author.works_written -> The Shame of the Cities -> book.book.editions -> The shame of the cities\n# Answer:\nThe Shame of the Cities", "# Reasoning Path:\nLincoln Steffens -> book.author.works_written -> The autobiography of Lincoln Steffens -> common.topic.notable_types -> Book\n# Answer:\nThe autobiography of Lincoln Steffens", "# Reasoning Path:\nLincoln Steffens -> common.topic.notable_for -> g.1255gcgx2\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nLincoln Steffens -> book.author.works_written -> The autobiography of Lincoln Steffens -> common.topic.notable_for -> g.1255m88ck\n# Answer:\nThe autobiography of Lincoln Steffens", "# Reasoning Path:\nLincoln Steffens -> book.author.works_written -> The Shame of the Cities -> book.written_work.original_language -> English Language\n# Answer:\nThe Shame of the Cities", "# Reasoning Path:\nLincoln Steffens -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Kidney cancer\n# Answer:\nMale", "# Reasoning Path:\nLincoln Steffens -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nLincoln Steffens -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nMale", "# Reasoning Path:\nLincoln Steffens -> book.author.works_written -> Boy on horseback -> common.topic.notable_for -> g.1257g06_v\n# Answer:\nBoy on horseback", "# Reasoning Path:\nLincoln Steffens -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale"], "ground_truth": ["The struggle for self-government", "Die Geschichte meines Lebens", "Moses in red", "The Least Of These", "Boy on horseback", "Upbuilders", "The autobiography of Lincoln Steffens", "Lincoln Steffens speaking", "The Shame of the Cities", "Into Mexico and out", "The Old Jim Horse", "The world of Lincoln Steffens", "John Reed"], "ans_acc": 0.23076923076923078, "ans_hit": 1, "ans_f1": 0.3157894736842105, "ans_precission": 0.5, "ans_recall": 0.23076923076923078, "path_f1": 0.3157894736842105, "path_precision": 0.5, "path_recall": 0.23076923076923078, "path_ans_f1": 0.3157894736842105, "path_ans_precision": 0.5, "path_ans_recall": 0.23076923076923078}
{"id": "WebQTest-1826", "prediction": ["# Reasoning Path:\nAnders Celsius -> people.person.education -> m.02wp1_z -> education.education.institution -> Uppsala University\n# Answer:\nUppsala University", "# Reasoning Path:\nAnders Celsius -> symbols.name_source.namesakes -> Celsius -> common.topic.article -> m.04v_v4\n# Answer:\nCelsius", "# Reasoning Path:\nAnders Celsius -> symbols.name_source.namesakes -> Celsius -> common.topic.notable_types -> Extraterrestrial location\n# Answer:\nCelsius", "# Reasoning Path:\nAnders Celsius -> base.kwebbase.kwtopic.has_sentences -> Celsius read his famous paper on his thermometer: \\\"Observations  Concerning the Two Constant Degrees on a Thermometer\\\"  to the Swedish  Academy of Sciences in 1742, and it was long known as the \\\"Swedish  thermometer\\\", only around 1800 becoming known as the Celsius thermometer. -> base.kwebbase.kwsentence.previous_sentence -> The Celsius temperature scale, also called the Centigrade temperature scale because of the 100-degree interval between the defined points, is now based on 0\u00b0 for the freezing point of water and 100\u00b0 for the boiling point of water.\n# Answer:\nCelsius read his famous paper on his thermometer: \\\"Observations  Concerning the Two Constant Degrees on a Thermometer\\\"  to the Swedish  Academy of Sciences in 1742, and it was long known as the \\\"Swedish  thermometer\\\", only around 1800 becoming known as the Celsius thermometer.", "# Reasoning Path:\nAnders Celsius -> symbols.name_source.namesakes -> Degree Celsius -> common.topic.article -> m.0216d\n# Answer:\nDegree Celsius", "# Reasoning Path:\nAnders Celsius -> base.kwebbase.kwtopic.has_sentences -> He then travelled to Italy and then to Paris, where in 1735 he met the mathematician and  astronomer Maupertuis, who was preparing an expedition to measure  an arc of meridian in the far north, with the intention of verifying the  Newtonian theory that the Earth is a sphere flattened at the poles. -> base.kwebbase.kwsentence.previous_sentence -> He was the first to realize from these observations that the aurora borealis involved magnetic forces.\n# Answer:\nHe then travelled to Italy and then to Paris, where in 1735 he met the mathematician and  astronomer Maupertuis, who was preparing an expedition to measure  an arc of meridian in the far north, with the intention of verifying the  Newtonian theory that the Earth is a sphere flattened at the poles.", "# Reasoning Path:\nAnders Celsius -> symbols.name_source.namesakes -> Degree Celsius -> common.topic.notable_for -> g.125702_q5\n# Answer:\nDegree Celsius", "# Reasoning Path:\nAnders Celsius -> symbols.name_source.namesakes -> Degree Celsius -> freebase.unit_profile.dimension -> Temperature\n# Answer:\nDegree Celsius", "# Reasoning Path:\nAnders Celsius -> base.kwebbase.kwtopic.has_sentences -> He then travelled to Italy and then to Paris, where in 1735 he met the mathematician and  astronomer Maupertuis, who was preparing an expedition to measure  an arc of meridian in the far north, with the intention of verifying the  Newtonian theory that the Earth is a sphere flattened at the poles. -> base.kwebbase.kwsentence.contains_connections -> anders celsius invited on expedition by pierre-louis moreau de maupertuis\n# Answer:\nHe then travelled to Italy and then to Paris, where in 1735 he met the mathematician and  astronomer Maupertuis, who was preparing an expedition to measure  an arc of meridian in the far north, with the intention of verifying the  Newtonian theory that the Earth is a sphere flattened at the poles.", "# Reasoning Path:\nAnders Celsius -> base.kwebbase.kwtopic.has_sentences -> Celsius read his famous paper on his thermometer: \\\"Observations  Concerning the Two Constant Degrees on a Thermometer\\\"  to the Swedish  Academy of Sciences in 1742, and it was long known as the \\\"Swedish  thermometer\\\", only around 1800 becoming known as the Celsius thermometer. -> base.kwebbase.kwsentence.dates -> m.0c17h2q\n# Answer:\nCelsius read his famous paper on his thermometer: \\\"Observations  Concerning the Two Constant Degrees on a Thermometer\\\"  to the Swedish  Academy of Sciences in 1742, and it was long known as the \\\"Swedish  thermometer\\\", only around 1800 becoming known as the Celsius thermometer."], "ground_truth": ["Uppsala University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1827", "prediction": ["# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Solomon R. Guggenheim Museum -> travel.tourist_attraction.near_travel_destination -> New York\n# Answer:\nSolomon R. Guggenheim Museum", "# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Solomon R. Guggenheim Museum -> architecture.structure.architectural_style -> Modern architecture\n# Answer:\nSolomon R. Guggenheim Museum", "# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Museum of Modern Art -> travel.tourist_attraction.near_travel_destination -> New York\n# Answer:\nMuseum of Modern Art", "# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Solomon R. Guggenheim Museum -> architecture.building.building_function -> Museum\n# Answer:\nSolomon R. Guggenheim Museum", "# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Frick Collection -> travel.tourist_attraction.near_travel_destination -> New York\n# Answer:\nFrick Collection", "# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Museum of Modern Art -> architecture.building.building_function -> Museum\n# Answer:\nMuseum of Modern Art", "# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Museum of Modern Art -> location.location.containedby -> United States of America\n# Answer:\nMuseum of Modern Art", "# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Frick Collection -> symbols.namesake.named_after -> Henry Clay Frick\n# Answer:\nFrick Collection", "# Reasoning Path:\nNew York City -> visual_art.art_subject.artwork_on_the_subject -> End of 14th Street Crosstown Line -> visual_art.artwork.art_subject -> Labor unrest\n# Answer:\nEnd of 14th Street Crosstown Line", "# Reasoning Path:\nNew York City -> book.book_subject.works -> The Treatment -> book.written_work.subjects -> School Teacher\n# Answer:\nThe Treatment"], "ground_truth": ["Imagination Playground at Burling Slip", "Times Square", "Travefy", "American Museum of Natural History", "Tesla Science Center at Wardenclyffe", "Gavin Brown's Enterprise", "Morgan Library & Museum", "Broadway Theatre", "Statue of Liberty", "Headless Horseman Hayrides", "Peking", "A.I.R. Gallery", "International Center of Photography", "National Academy Museum and School", "Museum of Modern Art", "UAE Healthy Kidney 10K", "Andrew Carnegie Mansion", "Statue of Liberty National Monument", "High Line", "American Folk Art Museum", "Central Park", "Chrysler Building", "Wave Hill", "Museum of Mathematics", "Museum of Sex", "Crocheron Park", "The Cloisters", "Theodore Roosevelt Birthplace National Historic Site", "Chelsea Art Museum", "Little Italy", "Museum of Arts and Design", "Museum of Modern Art Department of Film", "Frick Collection", "Chinatown", "New York Public Library for the Performing Arts", "Louis Armstrong House", "Brooklyn Bridge", "Henry Clay Frick House", "Battery Park", "Flatiron Building", "Metropolitan Museum of Art", "United Nations Headquarters", "Central Park Zoo", "FusionArts Museum", "Japan Society of New York", "New York City Half Marathon", "Felix M. Warburg House", "Brooklyn Botanic Garden", "Empire State Building", "Darien Lake", "New York Aquarium", "George Gustav Heye Center", "New York International Fringe Festival", "Grand Central Terminal", "Madison Square Garden", "Freedomland U.S.A.", "Franklin D. Roosevelt Presidential Library and Museum", "St. Patrick's Cathedral", "Rockefeller Center", "Solomon R. Guggenheim Museum", "New York Mini 10K", "Staten Island Ferry"], "ans_acc": 0.04838709677419355, "ans_hit": 1, "ans_f1": 0.09125475285171103, "ans_precission": 0.8, "ans_recall": 0.04838709677419355, "path_f1": 0.11764705882352941, "path_precision": 0.8, "path_recall": 0.06349206349206349, "path_ans_f1": 0.09125475285171103, "path_ans_precision": 0.8, "path_ans_recall": 0.04838709677419355}
{"id": "WebQTest-1828", "prediction": ["# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> French Polynesia\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> French Polynesia\n# Answer:\nFrench franc", "# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> Andorra\n# Answer:\nFrench franc", "# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> New Caledonia\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.11b60tv_69\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> Wallis and Futuna\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Spain\n# Answer:\nEuro", "# Reasoning Path:\nFrance -> location.country.currency_used -> Euro -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nEuro", "# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> Monaco\n# Answer:\nFrench franc", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6gn9g\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita"], "ground_truth": ["CFP franc", "Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1829", "prediction": ["# Reasoning Path:\nMassachusetts -> location.statistical_region.religions -> m.04404v_ -> location.religion_percentage.religion -> Jehovah's Witnesses\n# Answer:\nJehovah's Witnesses", "# Reasoning Path:\nMassachusetts -> location.statistical_region.religions -> m.04405bz -> location.religion_percentage.religion -> Christianity\n# Answer:\nChristianity", "# Reasoning Path:\nMassachusetts -> location.statistical_region.religions -> m.04404p3 -> location.religion_percentage.religion -> Judaism\n# Answer:\nJudaism", "# Reasoning Path:\nMassachusetts -> location.administrative_division.capital -> m.0jvw4t4 -> location.administrative_division_capital_relationship.capital -> Boston\n# Answer:\nBoston", "# Reasoning Path:\nMassachusetts -> symbols.name_source.namesakes -> USS Massachusetts (BB-59) -> location.location.geolocation -> m.06_4sm9\n# Answer:\nUSS Massachusetts (BB-59)", "# Reasoning Path:\nMassachusetts -> symbols.name_source.namesakes -> USS Massachusetts (BB-59) -> common.topic.notable_for -> g.12564b93k\n# Answer:\nUSS Massachusetts (BB-59)", "# Reasoning Path:\nMassachusetts -> symbols.name_source.namesakes -> 4547 Massachusetts -> astronomy.orbital_relationship.orbits -> Sun\n# Answer:\n4547 Massachusetts", "# Reasoning Path:\nMassachusetts -> symbols.name_source.namesakes -> USS Massachusetts -> common.topic.notable_types -> Location\n# Answer:\nUSS Massachusetts", "# Reasoning Path:\nMassachusetts -> symbols.name_source.namesakes -> 4547 Massachusetts -> astronomy.celestial_object.category -> Asteroid\n# Answer:\n4547 Massachusetts", "# Reasoning Path:\nMassachusetts -> symbols.name_source.namesakes -> 4547 Massachusetts -> astronomy.astronomical_discovery.discoverer -> Kazuro Watanabe\n# Answer:\n4547 Massachusetts"], "ground_truth": ["Baptists", "Methodism", "Lutheranism", "Christianity", "Judaism", "Catholicism", "Buddhism", "Protestantism", "Presbyterianism", "United Church of Christ", "Pentecostalism", "Episcopal Church", "Churches of Christ", "Jehovah's Witnesses"], "ans_acc": 0.21428571428571427, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.3, "ans_recall": 0.21428571428571427, "path_f1": 0.25, "path_precision": 0.3, "path_recall": 0.21428571428571427, "path_ans_f1": 0.25, "path_ans_precision": 0.3, "path_ans_recall": 0.21428571428571427}
{"id": "WebQTest-1830", "prediction": ["# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Belize\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Achawa language -> language.human_language.language_family -> Macro-Arawakan languages\n# Answer:\nAchawa language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Achawa language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nAchawa language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Chile\n# Answer:\nSpanish Language"], "ground_truth": ["Guahibo language", "Cumeral Language", "Siona Language", "Cubeo Language", "Cuiba language", "Desano Language", "Andoque Language", "Nukak language", "Ember\u00e1, Northern Language", "Cof\u00e1n Language", "Uwa language", "Awa-Cuaiquer Language", "Minica Huitoto", "Cams\u00e1 Language", "Totoro Language", "Tunebo, Central Language", "Malayo Language", "Nonuya language", "Piapoco Language", "Cabiyar\u00ed Language", "Romani, Vlax Language", "Quechua, Napo Lowland Language", "Carijona Language", "Bora Language", "Chipiajes Language", "Piratapuyo Language", "Yucuna Language", "Guanano Language", "Siriano Language", "Baudo language", "Anserma Language", "Wayuu Language", "Ocaina Language", "Ponares Language", "Macagu\u00e1n Language", "Barasana Language", "Arhuaco Language", "Yukpa Language", "S\u00e1liba Language", "Inga, Jungle Language", "Achawa language", "Koreguaje Language", "Spanish Language", "Tinigua language", "Puinave Language", "Pijao Language", "Runa Language", "Guambiano Language", "Tama Language", "Macaguaje Language", "Piaroa Language", "Murui Huitoto language", "Cagua Language", "Curripaco Language", "Palenquero Language", "Kogi Language", "Ticuna language", "Tunebo, Western Language", "Macuna Language", "Andaqui Language", "Tuyuca language", "Muinane Language", "Carabayo Language", "Providencia Sign Language", "Nheengatu language", "Natagaimas Language", "Playero language", "Coxima Language", "P\u00e1ez language", "Tucano Language", "Coyaima Language", "Colombian Sign Language", "Cocama language", "Hupd\u00eb Language", "Inga Language", "Islander Creole English", "Kuna, Border Language", "Waimaj\u00e3 Language", "Tomedes Language", "Bar\u00ed Language", "Tanimuca-Retuar\u00e3 Language", "Tunebo, Barro Negro Language", "Guayabero Language", "Omejes Language", "Catio language", "Tunebo, Angosturas Language"], "ans_acc": 0.023255813953488372, "ans_hit": 1, "ans_f1": 0.04545454545454545, "ans_precission": 1.0, "ans_recall": 0.023255813953488372, "path_f1": 0.04552845528455284, "path_precision": 0.7, "path_recall": 0.023529411764705882, "path_ans_f1": 0.04545454545454545, "path_ans_precision": 1.0, "path_ans_recall": 0.023255813953488372}
{"id": "WebQTest-1833", "prediction": ["# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc39sjp\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fm01\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc385hl\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp"], "ground_truth": ["Federal republic", "Presidential system", "Constitutional republic"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1835", "prediction": ["# Reasoning Path:\nGregor Mendel -> people.deceased_person.place_of_death -> Brno -> location.location.containedby -> Czech Republic\n# Answer:\nBrno", "# Reasoning Path:\nGregor Mendel -> people.person.place_of_birth -> Hyn\u010dice -> location.location.containedby -> Czech Republic\n# Answer:\nHyn\u010dice", "# Reasoning Path:\nGregor Mendel -> people.person.nationality -> Austria-Hungary -> base.datedlocationtest.dated_location_test.split_off -> m.0hp5m3v\n# Answer:\nAustria-Hungary", "# Reasoning Path:\nGregor Mendel -> people.deceased_person.place_of_death -> Brno -> location.location.contains -> AZ Tower\n# Answer:\nBrno", "# Reasoning Path:\nGregor Mendel -> people.person.place_of_birth -> Hyn\u010dice -> common.topic.article -> m.01h10g\n# Answer:\nHyn\u010dice", "# Reasoning Path:\nGregor Mendel -> people.deceased_person.place_of_death -> Brno -> location.location.containedby -> Brno-City District\n# Answer:\nBrno", "# Reasoning Path:\nGregor Mendel -> people.deceased_person.place_of_death -> Brno -> location.location.events -> 1993 Women's European Volleyball Championship\n# Answer:\nBrno", "# Reasoning Path:\nGregor Mendel -> people.person.place_of_birth -> Hyn\u010dice -> location.location.geolocation -> m.0wmmvdg\n# Answer:\nHyn\u010dice", "# Reasoning Path:\nGregor Mendel -> people.deceased_person.place_of_death -> Brno -> location.location.contains -> Brno Conservatory\n# Answer:\nBrno", "# Reasoning Path:\nGregor Mendel -> people.deceased_person.place_of_death -> Brno -> location.location.containedby -> South Moravian Region\n# Answer:\nBrno"], "ground_truth": ["Brno", "Austrian Silesia"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.6, "ans_recall": 0.5, "path_f1": 0.5454545454545454, "path_precision": 0.6, "path_recall": 0.5, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.6, "path_ans_recall": 0.5}
{"id": "WebQTest-1836", "prediction": ["# Reasoning Path:\nLaura Ingalls Wilder -> people.person.places_lived -> m.0_ghjwl -> people.place_lived.location -> Kansas\n# Answer:\nKansas", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.places_lived -> m.0_ghjyl -> people.place_lived.location -> De Smet\n# Answer:\nDe Smet", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.places_lived -> m.0_gghr5 -> people.place_lived.location -> Burr Oak\n# Answer:\nBurr Oak", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.places_lived -> m.0_ghjwl -> freebase.valuenotation.has_value -> End date\n# Answer:\nEnd date", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> Hard Times on the Prairie -> book.written_work.subjects -> United States of America\n# Answer:\nHard Times on the Prairie", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.places_lived -> m.0_ghjwl -> freebase.valuenotation.has_value -> Start date\n# Answer:\nStart date", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.places_lived -> m.0_ghjyl -> freebase.valuenotation.has_value -> End date\n# Answer:\nEnd date", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.sibling_s -> m.0_gghg0 -> people.sibling_relationship.sibling -> g.12148r4f\n# Answer:\npeople.sibling_relationship.sibling", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.places_lived -> m.0_ghjyl -> freebase.valuenotation.has_value -> Start date\n# Answer:\nStart date", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> 1998 Laura Ingalls Wilder Country Engagement Book -> common.topic.notable_types -> Book\n# Answer:\n1998 Laura Ingalls Wilder Country Engagement Book"], "ground_truth": ["De Smet", "Kansas", "Mansfield", "Pepin", "Burr Oak", "Walnut Grove"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.37499999999999994, "ans_precission": 0.3, "ans_recall": 0.5, "path_f1": 0.2727272727272727, "path_precision": 0.3, "path_recall": 0.25, "path_ans_f1": 0.37499999999999994, "path_ans_precision": 0.3, "path_ans_recall": 0.5}
{"id": "WebQTest-1838", "prediction": ["# Reasoning Path:\nGermany -> sports.sports_team_location.teams -> Alba Berlin -> sports.sports_team.location -> Berlin\n# Answer:\nAlba Berlin", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> sports.sports_team_location.teams -> Germany women's national under-18 ice hockey team -> common.topic.notable_types -> Sports Team\n# Answer:\nGermany women's national under-18 ice hockey team", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_38m_\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nGermany -> sports.sports_team_location.teams -> Germany national handball team -> sports.tournament_team.tournaments_competed_in -> m.09t9_3v\n# Answer:\nGermany national handball team", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fdl8\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> sports.sports_team_location.teams -> Germany national handball team -> common.topic.notable_types -> Sports Team\n# Answer:\nGermany national handball team", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc38c_4\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_67jr\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_jlz4\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp"], "ground_truth": ["Alba Berlin", "Bayer Giants Leverkusen", "Germany men's national volleyball team", "Germany women's national basketball team", "EWE Baskets Oldenburg", "Germany women's national rugby union team", "Germany national handball team", "Germany men's national field hockey team", "Germany women's national football team", "Germany women's national ice hockey team", "Germany national badminton team", "German National Ice Hockey Team", "Brose Baskets", "FC Bayern Munich", "BG G\u00f6ttingen", "Germany national American football team", "Germany national basketball team", "Germany national korfball team", "Germany national beach handball team", "Germany Fed Cup team", "Germany national rugby league team", "Skyliners Frankfurt", "Germany Davis Cup team", "K\u00f6ln 99ers", "Telekom Baskets Bonn", "Germany national baseball team", "Germany national cricket team", "Germany women's national field hockey team", "Germany national football team", "Germany men's national water polo team", "Germany women's national handball team", "Germany women's national volleyball team", "Germany men's national inline hockey team", "Germany national speedway team", "Germany U20 women's volleyball team", "Germany men's national pitch and putt team", "Germany women's national beach handball team", "Germany women's national under-18 ice hockey team", "Germany national beach soccer team", "QTSV Quackenbr\u00fcck", "Germany national rugby union team"], "ans_acc": 0.07317073170731707, "ans_hit": 1, "ans_f1": 0.12371134020618556, "ans_precission": 0.4, "ans_recall": 0.07317073170731707, "path_f1": 0.12121212121212122, "path_precision": 0.4, "path_recall": 0.07142857142857142, "path_ans_f1": 0.12371134020618556, "path_ans_precision": 0.4, "path_ans_recall": 0.07317073170731707}
{"id": "WebQTest-1839", "prediction": ["# Reasoning Path:\nOklahoma -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nOklahoma -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nOklahoma -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nOklahoma -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nOklahoma -> government.governmental_jurisdiction.governing_officials -> m.0100vzfb -> government.government_position_held.office_holder -> John R. Bennett\n# Answer:\nJohn R. Bennett", "# Reasoning Path:\nOklahoma -> book.book_subject.works -> Winning the Dust Bowl -> book.written_work.subjects -> 20th century\n# Answer:\nWinning the Dust Bowl", "# Reasoning Path:\nOklahoma -> government.governmental_jurisdiction.governing_officials -> m.010f33zy -> government.government_position_held.basic_title -> Lieutenant Governor\n# Answer:\nLieutenant Governor", "# Reasoning Path:\nOklahoma -> book.book_subject.works -> Winning the Dust Bowl -> common.topic.notable_types -> Book\n# Answer:\nWinning the Dust Bowl", "# Reasoning Path:\nOklahoma -> government.governmental_jurisdiction.governing_officials -> m.0100vzfb -> freebase.valuenotation.has_no_value -> Appointed By (if Position is Appointed)\n# Answer:\nAppointed By (if Position is Appointed)", "# Reasoning Path:\nOklahoma -> government.governmental_jurisdiction.governing_officials -> m.010f33zy -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-184", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Michigan\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> people.person.places_lived -> m.03pvr9m -> people.place_lived.location -> Massachusetts\n# Answer:\nMassachusetts", "# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Michigan\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> people.person.places_lived -> m.0123x8q9 -> people.place_lived.location -> Park City\n# Answer:\nPark City", "# Reasoning Path:\nMitt Romney -> people.person.places_lived -> m.03pqrn3 -> people.place_lived.location -> Bloomfield Hills\n# Answer:\nBloomfield Hills", "# Reasoning Path:\nMitt Romney -> people.person.place_of_birth -> Detroit -> base.biblioness.bibs_location.state -> Michigan\n# Answer:\nDetroit", "# Reasoning Path:\nMitt Romney -> people.person.place_of_birth -> Detroit -> location.location.containedby -> United States of America\n# Answer:\nDetroit", "# Reasoning Path:\nMitt Romney -> people.person.place_of_birth -> Detroit -> location.location.containedby -> Michigan\n# Answer:\nDetroit"], "ground_truth": ["Massachusetts"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1840", "prediction": ["# Reasoning Path:\nNorway -> location.country.official_language -> Norwegian Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nNorwegian Language", "# Reasoning Path:\nNorway -> location.country.official_language -> Nynorsk -> common.topic.image -> Norwegianmalforms\n# Answer:\nNynorsk", "# Reasoning Path:\nNorway -> location.country.official_language -> Norwegian Language -> common.topic.image -> Norwegianmalforms\n# Answer:\nNorwegian Language", "# Reasoning Path:\nNorway -> location.country.official_language -> Norwegian Language -> common.topic.notable_types -> Human Language\n# Answer:\nNorwegian Language", "# Reasoning Path:\nNorway -> location.country.languages_spoken -> Saami, North Language -> language.human_language.countries_spoken_in -> Finland\n# Answer:\nSaami, North Language", "# Reasoning Path:\nNorway -> location.statistical_region.net_migration -> g.1q5jhf535\n# Answer:\nlocation.statistical_region.net_migration", "# Reasoning Path:\nNorway -> location.country.languages_spoken -> Saami, North Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nSaami, North Language", "# Reasoning Path:\nNorway -> location.country.official_language -> Bokm\u00e5l -> common.topic.image -> Norwegianmalforms\n# Answer:\nBokm\u00e5l", "# Reasoning Path:\nNorway -> location.country.languages_spoken -> Saami, North Language -> base.rosetta.languoid.local_name -> Saami, North\n# Answer:\nSaami, North Language", "# Reasoning Path:\nNorway -> location.statistical_region.net_migration -> g.1q5jsymtt\n# Answer:\nlocation.statistical_region.net_migration"], "ground_truth": ["Bokm\u00e5l", "Nynorsk", "Saami, Lule Language", "Norwegian Language", "Saami, North Language", "Finnish, Kven Language", "Saami, South Language"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.8, "ans_recall": 0.5714285714285714, "path_f1": 0.4615384615384615, "path_precision": 0.5, "path_recall": 0.42857142857142855, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.8, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-1842", "prediction": ["# Reasoning Path:\nSingapore -> location.country.currency_used -> Singapore dollar -> common.topic.article -> m.02f32r\n# Answer:\nSingapore dollar", "# Reasoning Path:\nSingapore -> location.statistical_region.deposit_interest_rate -> g.11b71rkqjy\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSingapore -> location.country.currency_used -> Singapore dollar -> common.topic.webpage -> m.04m19t9\n# Answer:\nSingapore dollar", "# Reasoning Path:\nSingapore -> location.statistical_region.deposit_interest_rate -> g.12cp_j7mk\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSingapore -> location.statistical_region.deposit_interest_rate -> g.12tb6f6tz\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSingapore -> location.country.currency_used -> Singapore dollar -> common.topic.webpage -> m.04m19tg\n# Answer:\nSingapore dollar", "# Reasoning Path:\nSingapore -> olympics.olympic_participating_country.athletes -> m.0k8qc9m -> olympics.olympic_athlete_affiliation.sport -> Sailing\n# Answer:\nSailing", "# Reasoning Path:\nSingapore -> olympics.olympic_participating_country.athletes -> m.0k8qcb1 -> olympics.olympic_athlete_affiliation.sport -> Sailing\n# Answer:\nSailing", "# Reasoning Path:\nSingapore -> olympics.olympic_participating_country.athletes -> m.0k8qc9m -> olympics.olympic_athlete_affiliation.olympics -> The London 2012 Summer Olympics\n# Answer:\nThe London 2012 Summer Olympics", "# Reasoning Path:\nSingapore -> olympics.olympic_participating_country.athletes -> m.0k8qcbk -> olympics.olympic_athlete_affiliation.olympics -> The London 2012 Summer Olympics\n# Answer:\nThe London 2012 Summer Olympics"], "ground_truth": ["Singapore dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1843", "prediction": ["# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> Blackfoot Language -> language.human_language.main_country -> Canada\n# Answer:\nBlackfoot Language", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> New Zealand\n# Answer:\nEnglish Language", "# Reasoning Path:\nBlackfoot Confederacy -> common.topic.notable_for -> g.125bpgfw7\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> English Language -> language.human_language.dialects -> Scottish English\n# Answer:\nEnglish Language", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> English Language -> education.school_category.schools_of_this_kind -> Piers Midwinter\n# Answer:\nEnglish Language", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> Blackfoot Language -> common.topic.image -> Traditional territory in green, reserves and reservations in orange.\n# Answer:\nBlackfoot Language", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> English Language -> language.human_language.dialects -> African American Vernacular English\n# Answer:\nEnglish Language", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> Blackfoot Language -> common.topic.notable_for -> g.12585j6s4\n# Answer:\nBlackfoot Language"], "ground_truth": ["English Language", "Blackfoot Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1844", "prediction": ["# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.produced_by -> Richard B. Lewis -> tv.tv_producer.programs_produced -> m.0gj0gz_\n# Answer:\nRichard B. Lewis", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.produced_by -> Richard B. Lewis -> people.person.gender -> Male\n# Answer:\nRichard B. Lewis", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.starring -> m.0k2m57 -> film.performance.actor -> Alan Rickman\n# Answer:\nAlan Rickman", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.produced_by -> Richard B. Lewis -> tv.tv_producer.programs_produced -> m.0z03fvz\n# Answer:\nRichard B. Lewis", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.starring -> m.0k2m57 -> film.performance.character -> The Sheriff of Nottingham\n# Answer:\nThe Sheriff of Nottingham", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.produced_by -> Richard B. Lewis -> award.award_nominee.award_nominations -> m.0sx46f3\n# Answer:\nRichard B. Lewis", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.produced_by -> Kevin Costner -> people.person.children -> Annie Costner\n# Answer:\nKevin Costner", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.costume_design_by -> John Bloomfield -> film.film_costumer_designer.costume_design_for_film -> A Good Woman\n# Answer:\nJohn Bloomfield", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.costume_design_by -> John Bloomfield -> people.person.place_of_birth -> Neath Port Talbot\n# Answer:\nJohn Bloomfield", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.produced_by -> Richard B. Lewis -> award.award_nominee.award_nominations -> m.0z0337x\n# Answer:\nRichard B. Lewis"], "ground_truth": ["Kevin Costner"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1845", "prediction": ["# Reasoning Path:\nTexas -> location.location.time_zones -> Mountain Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nTexas -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nTexas -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nTexas -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nTexas -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nTexas -> government.political_district.representatives -> m.04j5vd_ -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nTexas -> location.location.time_zones -> Central Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nTexas -> government.political_district.representatives -> m.04j5vd_ -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nUnited States Senate", "# Reasoning Path:\nTexas -> government.political_district.representatives -> m.052p616 -> government.government_position_held.office_holder -> William Neff Patman\n# Answer:\nWilliam Neff Patman", "# Reasoning Path:\nTexas -> government.political_district.representatives -> m.04j5vd_ -> government.government_position_held.office_holder -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson"], "ground_truth": ["Central Time Zone", "Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1847", "prediction": ["# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> location.location.partially_contains -> Turkey\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> base.locations.continents.countries_within -> Iraq\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> location.location.partially_contains -> Alazani\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> location.location.partiallycontains -> m.010r1xzh\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> location.location.partially_contains -> Aragvi River\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> base.locations.continents.countries_within -> Syria\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> people.ethnicity.people -> Abdullah Cevdet -> people.person.nationality -> Turkey\n# Answer:\nAbdullah Cevdet", "# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> location.location.partiallycontains -> m.0113v79q\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> base.locations.continents.countries_within -> Iran\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> location.location.partiallycontains -> m.0w_08b5\n# Answer:\nAsia"], "ground_truth": ["Asia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1848", "prediction": ["# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.family -> Brass instrument\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> music.instrument.family -> Brass instrument\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> common.topic.notable_types -> Musical instrument\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.instrumentalists -> W. C. Handy\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> book.written_work.subjects -> Trumpeter\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.instrumentalists -> Bix Beiderbecke\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> common.topic.notable_types -> Book\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.instrumentalists -> James Morrison\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.featured_artist.recordings -> Boog It -> music.recording.artist -> Mills Brothers\n# Answer:\nBoog It", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> common.topic.notable_types -> Musical instrument\n# Answer:\nTrumpet"], "ground_truth": ["Trumpet", "Cornet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1849", "prediction": ["# Reasoning Path:\nAlyson Stoner -> film.actor.film -> m.0y9pvhm -> film.performance.character -> Back Up Dancer\n# Answer:\nBack Up Dancer", "# Reasoning Path:\nCamp Rock -> film.film.starring -> m.0h5rcmk -> film.performance.character -> Brown Cesario\n# Answer:\nBrown Cesario", "# Reasoning Path:\nAlyson Stoner -> film.actor.film -> m.0y9pvhm -> film.performance.film -> A Cinderella Story: Once Upon a Song\n# Answer:\nA Cinderella Story: Once Upon a Song", "# Reasoning Path:\nAlyson Stoner -> film.actor.film -> m.010r560c -> film.performance.character -> Camille Gage\n# Answer:\nCamille Gage", "# Reasoning Path:\nCamp Rock -> film.film.cinematography -> Dean Cundey -> film.actor.film -> m.0y54l69\n# Answer:\nDean Cundey", "# Reasoning Path:\nAlyson Stoner -> film.actor.film -> m.011shk2t -> film.performance.character -> Strawberry\n# Answer:\nStrawberry", "# Reasoning Path:\nCamp Rock -> film.film.starring -> m.03jqmzg -> film.performance.character -> Caitlyn Geller\n# Answer:\nCaitlyn Geller", "# Reasoning Path:\nCamp Rock -> film.film.starring -> m.0h5rcmk -> film.performance.actor -> Daniel Fathers\n# Answer:\nDaniel Fathers", "# Reasoning Path:\nCamp Rock -> film.film.starring -> m.04dc588 -> film.performance.character -> g.122tc1ml\n# Answer:\nfilm.performance.character", "# Reasoning Path:\nAlyson Stoner -> film.actor.film -> m.010r560c -> film.performance.film -> Step Up: All In\n# Answer:\nStep Up: All In"], "ground_truth": ["Caitlyn Geller"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-185", "prediction": ["# Reasoning Path:\nFrance -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nFrance -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nFrance -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Martinique\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.11b60tv_69\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nFrance -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Andorra\n# Answer:\nUnitary state", "# Reasoning Path:\nFrance -> location.statistical_region.gni_in_ppp_dollars -> g.11b60wv0lq\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nFrance -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Angola\n# Answer:\nUnitary state", "# Reasoning Path:\nFrance -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Martinique\n# Answer:\nConstitutional republic", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6gn9g\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nFrance -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Algeria\n# Answer:\nSemi-presidential system"], "ground_truth": ["Unitary state", "Constitutional republic", "Semi-presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1850", "prediction": ["# Reasoning Path:\nMadonna -> people.person.religion -> Jewish Renewal\n# Answer:\nJewish Renewal", "# Reasoning Path:\nMadonna -> people.person.religion -> Catholicism -> media_common.quotation_subject.quotations_about_this_subject -> Catholicism is not a soothing religion. It's a painful religion. We're all gluttons for punishment.\n# Answer:\nCatholicism", "# Reasoning Path:\nMadonna -> base.popstra.celebrity.dated -> m.064_jxy -> base.popstra.dated.participant -> Chris Rock\n# Answer:\nChris Rock", "# Reasoning Path:\nMadonna -> broadcast.artist.content -> .977 The '80s Channel -> common.topic.image -> x.jpg\n# Answer:\n.977 The '80s Channel", "# Reasoning Path:\nMadonna -> base.popstra.celebrity.dated -> m.064f3ln -> base.popstra.dated.participant -> Dennis Rodman\n# Answer:\nDennis Rodman", "# Reasoning Path:\nMadonna -> base.popstra.celebrity.dated -> m.064t3bj -> base.popstra.dated.participant -> Alex Rodriguez\n# Answer:\nAlex Rodriguez", "# Reasoning Path:\nMadonna -> broadcast.artist.content -> .977 The '80s Channel -> broadcast.content.producer -> 977 Music\n# Answer:\n.977 The '80s Channel", "# Reasoning Path:\nMadonna -> broadcast.artist.content -> 1.FM Absolute  90's -> common.topic.notable_types -> Broadcast Content\n# Answer:\n1.FM Absolute  90's", "# Reasoning Path:\nMadonna -> broadcast.artist.content -> .977 The Hits Channel -> broadcast.content.artist -> Usher\n# Answer:\n.977 The Hits Channel", "# Reasoning Path:\nMadonna -> broadcast.artist.content -> .977 The Hits Channel -> common.topic.image -> x.jpg\n# Answer:\n.977 The Hits Channel"], "ground_truth": ["Jewish Renewal", "Catholicism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1851", "prediction": ["# Reasoning Path:\nEgypt -> location.location.partially_containedby -> North Africa -> location.location.partially_contains -> Arab world\n# Answer:\nNorth Africa", "# Reasoning Path:\nEgypt -> location.location.partially_containedby -> North Africa -> location.location.contains -> Suez Governorate\n# Answer:\nNorth Africa", "# Reasoning Path:\nEgypt -> location.location.partially_containedby -> Western Asia -> location.location.events -> Middle East Theatre of World War II\n# Answer:\nWestern Asia", "# Reasoning Path:\nEgypt -> location.location.partially_containedby -> North Africa -> location.location.partially_contains -> Middle East\n# Answer:\nNorth Africa", "# Reasoning Path:\nEgypt -> location.location.partially_containedby -> North Africa -> location.location.contains -> Sahara\n# Answer:\nNorth Africa", "# Reasoning Path:\nEgypt -> location.location.partially_containedby -> Western Asia -> location.location.partially_contains -> Arab world\n# Answer:\nWestern Asia", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Israel\n# Answer:\nArabic Language", "# Reasoning Path:\nEgypt -> location.location.partially_containedby -> Western Asia -> location.location.containedby -> Asia\n# Answer:\nWestern Asia", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.writing_system -> Arabic alphabet\n# Answer:\nArabic Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.countries_spoken_in -> Sudan\n# Answer:\nArabic, Sudanese Spoken Language"], "ground_truth": ["Israel", "Sudan", "Libya"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.15384615384615383, "ans_precission": 0.1, "ans_recall": 0.3333333333333333, "path_f1": 0.07692307692307693, "path_precision": 0.2, "path_recall": 0.047619047619047616, "path_ans_f1": 0.30769230769230765, "path_ans_precision": 0.2, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1853", "prediction": ["# Reasoning Path:\nRichard Hammond -> people.person.profession -> Voice Actor -> fictional_universe.character_occupation.characters_with_this_occupation -> Jenny Larson\n# Answer:\nVoice Actor", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Presenter -> fictional_universe.character_occupation.characters_with_this_occupation -> Harmony Kendall\n# Answer:\nPresenter", "# Reasoning Path:\nRichard Hammond -> tv.tv_personality.tv_regular_appearances -> g.11bv36x_sm\n# Answer:\ntv.tv_personality.tv_regular_appearances", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Presenter -> fictional_universe.character_occupation.characters_with_this_occupation -> Jackie Burkhart\n# Answer:\nPresenter", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Presenter -> base.descriptive_names.names.descriptive_name -> m.010543ty\n# Answer:\nPresenter", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Presenter -> fictional_universe.character_occupation.characters_with_this_occupation -> Liz Lemon\n# Answer:\nPresenter", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Voice Actor -> common.topic.notable_types -> Profession\n# Answer:\nVoice Actor", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Presenter -> base.descriptive_names.names.descriptive_name -> m.01064h0m\n# Answer:\nPresenter", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Voice Actor -> film.film_job.films_with_this_crew_job -> m.0y8734m\n# Answer:\nVoice Actor", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Presenter -> people.profession.specializations -> Auctioneer\n# Answer:\nPresenter"], "ground_truth": ["Voice Actor", "Writer", "Presenter", "Radio personality", "Screenwriter", "Author", "Stunt Performer", "Journalist", "Television producer"], "ans_acc": 0.2222222222222222, "ans_hit": 1, "ans_f1": 0.3564356435643564, "ans_precission": 0.9, "ans_recall": 0.2222222222222222, "path_f1": 0.3564356435643564, "path_precision": 0.9, "path_recall": 0.2222222222222222, "path_ans_f1": 0.3564356435643564, "path_ans_precision": 0.9, "path_ans_recall": 0.2222222222222222}
{"id": "WebQTest-1854", "prediction": ["# Reasoning Path:\nLois Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntr0s -> tv.regular_tv_appearance.actor -> Alex Borstein\n# Answer:\nAlex Borstein", "# Reasoning Path:\nLois Griffin -> film.film_character.portrayed_in_films -> m.0k3ssk -> film.performance.actor -> Alex Borstein\n# Answer:\nAlex Borstein", "# Reasoning Path:\nLois Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntr0s -> tv.regular_tv_appearance.series -> Family Guy\n# Answer:\nFamily Guy", "# Reasoning Path:\nLois Griffin -> tv.tv_character.appeared_in_tv_episodes -> g.11byb56_yy\n# Answer:\ntv.tv_character.appeared_in_tv_episodes", "# Reasoning Path:\nLois Griffin -> film.film_character.portrayed_in_films -> m.0h28yx8 -> film.performance.actor -> Alex Borstein\n# Answer:\nAlex Borstein", "# Reasoning Path:\nLois Griffin -> film.film_character.portrayed_in_films -> m.0k3ssk -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nLois Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntr0s -> tv.regular_tv_appearance.seasons -> Family Guy - Season 1\n# Answer:\nFamily Guy - Season 1", "# Reasoning Path:\nLois Griffin -> film.film_character.portrayed_in_films -> m.010x33y4 -> film.performance.actor -> Alex Borstein\n# Answer:\nAlex Borstein", "# Reasoning Path:\nLois Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntr0s -> tv.regular_tv_appearance.seasons -> Family Guy - Season 10\n# Answer:\nFamily Guy - Season 10", "# Reasoning Path:\nLois Griffin -> film.film_character.portrayed_in_films -> m.0h28yx8 -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice"], "ground_truth": ["Alex Borstein"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1855", "prediction": ["# Reasoning Path:\nParis -> location.location.nearby_airports -> Paris Orly Airport -> travel.transport_terminus.travel_destinations_served -> m.051qtw8\n# Answer:\nParis Orly Airport", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Charles de Gaulle Airport -> aviation.airport.hub_for -> Air France\n# Answer:\nCharles de Gaulle Airport", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Paris Orly Airport -> aviation.airport.hub_for -> Air France\n# Answer:\nParis Orly Airport", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Paris Orly Airport -> film.film_location.featured_in_films -> La Jet\u00e9e\n# Answer:\nParis Orly Airport", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Charles de Gaulle Airport -> aviation.airport.hub_for -> Vueling\n# Answer:\nCharles de Gaulle Airport", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Charles de Gaulle Airport -> travel.transport_terminus.travel_destinations_served -> m.051qtw8\n# Answer:\nCharles de Gaulle Airport", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Paris Orly Airport -> aviation.airport.hub_for -> Vueling\n# Answer:\nParis Orly Airport", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Charles de Gaulle Airport -> aviation.airport.serves -> Tour Triangle\n# Answer:\nCharles de Gaulle Airport", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Beauvais\u2013Till\u00e9 Airport -> common.topic.notable_types -> Transport terminus\n# Answer:\nBeauvais\u2013Till\u00e9 Airport", "# Reasoning Path:\nParis -> common.topic.webpage -> m.02k8dbw -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage"], "ground_truth": ["Charles de Gaulle Airport", "Paris Orly Airport"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1857", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.album -> g.121_5zdm\n# Answer:\nmusic.artist.album", "# Reasoning Path:\nRihanna -> music.artist.album -> Loud Tour Live at the O2 -> music.album.genre -> Pop rock\n# Answer:\nLoud Tour Live at the O2", "# Reasoning Path:\nRihanna -> music.artist.album -> Loud Tour Live at the O2 -> common.topic.article -> m.0nfvxvd\n# Answer:\nLoud Tour Live at the O2", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> music.artist.album -> Unapologetic -> music.album.release_type -> Album\n# Answer:\nUnapologetic", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.064_ltw -> base.popstra.infidelity.participant -> Kanye West\n# Answer:\nKanye West", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> music.artist.album -> Loud Tour Live at the O2 -> music.album.genre -> Contemporary R&B\n# Answer:\nLoud Tour Live at the O2", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> m.010_ydxv -> music.track_contribution.track -> If I Never See Your Face Again\n# Answer:\nIf I Never See Your Face Again", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.063y0bl -> base.popstra.infidelity.victim -> Beyonc\u00e9 Knowles\n# Answer:\nBeyonc\u00e9 Knowles"], "ground_truth": ["Jump (Club Cheval Rap Remix)", "Diamonds (remix)", "Cockiness (Love It) Remix", "Talk That Talk", "Devil in a Blue Dress", "g.121xjjw_", "Towards the Sun", "Unapologetic", "g.121n8wcw", "Rockstar 101: The Remixes", "Birthday Cake", "Diamonds", "Loud Tour Live at the O2", "g.121_5zdm"], "ans_acc": 0.21428571428571427, "ans_hit": 1, "ans_f1": 0.21052631578947364, "ans_precission": 0.4, "ans_recall": 0.14285714285714285, "path_f1": 0.3157894736842105, "path_precision": 0.5, "path_recall": 0.23076923076923078, "path_ans_f1": 0.3, "path_ans_precision": 0.5, "path_ans_recall": 0.21428571428571427}
{"id": "WebQTest-1858", "prediction": ["# Reasoning Path:\nDave Ramsey -> people.person.places_lived -> m.03phzfq -> people.place_lived.location -> Nashville\n# Answer:\nNashville", "# Reasoning Path:\nDave Ramsey -> people.person.place_of_birth -> Antioch -> location.location.people_born_here -> Casey Atwood\n# Answer:\nAntioch", "# Reasoning Path:\nDave Ramsey -> people.person.place_of_birth -> Antioch -> common.topic.article -> m.03gf9f\n# Answer:\nAntioch", "# Reasoning Path:\nDave Ramsey -> people.person.place_of_birth -> Antioch -> location.location.people_born_here -> Jim Turner\n# Answer:\nAntioch", "# Reasoning Path:\nDave Ramsey -> people.person.place_of_birth -> Antioch -> location.citytown.postal_codes -> 37011\n# Answer:\nAntioch", "# Reasoning Path:\nDave Ramsey -> people.person.place_of_birth -> Antioch -> location.location.people_born_here -> Keenan Reynolds\n# Answer:\nAntioch", "# Reasoning Path:\nDave Ramsey -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nDave Ramsey -> people.person.place_of_birth -> Antioch -> location.citytown.postal_codes -> 37013\n# Answer:\nAntioch", "# Reasoning Path:\nDave Ramsey -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nDave Ramsey -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nMale"], "ground_truth": ["Nashville"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-186", "prediction": ["# Reasoning Path:\nDenmark -> royalty.kingdom.rulers -> Margrethe II of Denmark -> royalty.monarch.royal_line -> House of Gl\u00fccksburg\n# Answer:\nMargrethe II of Denmark", "# Reasoning Path:\nDenmark -> royalty.kingdom.rulers -> Margrethe II of Denmark -> people.family_member.family -> House of Gl\u00fccksburg\n# Answer:\nMargrethe II of Denmark", "# Reasoning Path:\nDenmark -> royalty.kingdom.rulers -> Christian III of Denmark -> common.topic.notable_types -> Monarch\n# Answer:\nChristian III of Denmark", "# Reasoning Path:\nDenmark -> government.governmental_jurisdiction.governing_officials -> m.0_g55l8 -> government.government_position_held.appointed_by -> Helle Thorning-Schmidt\n# Answer:\nHelle Thorning-Schmidt", "# Reasoning Path:\nDenmark -> location.statistical_region.gender_balance_members_of_parliament -> g.11b71wx6lz\n# Answer:\nlocation.statistical_region.gender_balance_members_of_parliament", "# Reasoning Path:\nDenmark -> royalty.kingdom.rulers -> Christian III of Denmark -> people.person.parents -> Frederick I of Denmark\n# Answer:\nChristian III of Denmark", "# Reasoning Path:\nDenmark -> royalty.kingdom.rulers -> Christian II of Denmark -> people.person.parents -> John, King of Denmark\n# Answer:\nChristian II of Denmark", "# Reasoning Path:\nDenmark -> government.governmental_jurisdiction.governing_officials -> m.0yq0t10 -> government.government_position_held.appointed_by -> Prime Minister of Denmark\n# Answer:\nPrime Minister of Denmark", "# Reasoning Path:\nDenmark -> government.governmental_jurisdiction.governing_officials -> m.0_g55l8 -> government.government_position_held.basic_title -> Minister\n# Answer:\nMinister", "# Reasoning Path:\nDenmark -> royalty.kingdom.rulers -> Christian II of Denmark -> people.person.place_of_birth -> Nyborg\n# Answer:\nChristian II of Denmark"], "ground_truth": ["Margrethe Vestager", "Helle Thorning-Schmidt", "Margrethe II of Denmark"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.41379310344827586, "ans_precission": 0.3, "ans_recall": 0.6666666666666666, "path_f1": 0.34285714285714286, "path_precision": 0.3, "path_recall": 0.4, "path_ans_f1": 0.41379310344827586, "path_ans_precision": 0.3, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1860", "prediction": ["# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Ganymede -> astronomy.star_system_body.star_system -> Solar System\n# Answer:\nGanymede", "# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Io -> common.topic.notable_types -> Moon\n# Answer:\nIo", "# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Io -> astronomy.celestial_object.locations -> Volcanology of Io\n# Answer:\nIo", "# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Ganymede -> common.topic.notable_types -> Moon\n# Answer:\nGanymede", "# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Ganymede -> common.topic.image -> Ganymede g1 true 2\n# Answer:\nGanymede", "# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Callisto -> common.topic.image -> Callisto from space.  In \u201cThe Callistan Menace\u201d Callisto has an atmosphere and native life forms\n# Answer:\nCallisto", "# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Io -> astronomy.celestial_object.locations -> Ah Peku Patera\n# Answer:\nIo", "# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Io -> astronomy.celestial_object.locations -> Amaterasu Patera\n# Answer:\nIo", "# Reasoning Path:\nGalileo Galilei -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#range -> Person\n# Answer:\nChildren", "# Reasoning Path:\nGalileo Galilei -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality"], "ground_truth": ["Europa", "Ganymede", "Io", "Callisto"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-1861", "prediction": ["# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Unitary state -> common.topic.notable_types -> Form of Government\n# Answer:\nUnitary state", "# Reasoning Path:\nSaudi Arabia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60xj9xc\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Brunei\n# Answer:\nUnitary state", "# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Absolute monarchy -> government.form_of_government.countries -> United Arab Emirates\n# Answer:\nAbsolute monarchy", "# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Absolute monarchy -> common.topic.notable_types -> Form of Government\n# Answer:\nAbsolute monarchy", "# Reasoning Path:\nSaudi Arabia -> location.statistical_region.net_migration -> g.1q5jfl07n\n# Answer:\nlocation.statistical_region.net_migration", "# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Chechen Republic of Ichkeria\n# Answer:\nUnitary state", "# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Islamic state -> government.form_of_government.countries -> Brunei\n# Answer:\nIslamic state", "# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Islamic state -> common.topic.notable_types -> Form of Government\n# Answer:\nIslamic state"], "ground_truth": ["Unitary state", "Islamic state", "Absolute monarchy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1864", "prediction": ["# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> tv.tv_character.appeared_in_tv_program -> m.09l2mf7 -> tv.regular_tv_appearance.actor -> Michael Weatherly\n# Answer:\nMichael Weatherly", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> tv.tv_character.appeared_in_tv_program -> m.03jrblb -> tv.regular_tv_appearance.actor -> Michael Weatherly\n# Answer:\nMichael Weatherly", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> common.topic.notable_types -> TV Character -> type.type.properties -> Appeared In TV Episode Segments\n# Answer:\nTV Character", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> common.topic.notable_types -> TV Character -> type.type.expected_by -> Character\n# Answer:\nTV Character", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> common.topic.notable_types -> TV Character -> freebase.type_profile.strict_included_types -> Fictional Character\n# Answer:\nTV Character", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> fictional_universe.fictional_character.occupation -> Special Agent -> common.topic.article -> m.02lml8\n# Answer:\nSpecial Agent", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> tv.tv_character.appeared_in_tv_program -> m.03jrblb -> tv.regular_tv_appearance.seasons -> NCIS - Season 9\n# Answer:\nNCIS - Season 9", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> common.topic.notable_types -> TV Character -> type.type.properties -> Appeared In TV Episodes\n# Answer:\nTV Character", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> fictional_universe.fictional_character.occupation -> Special Agent -> fictional_universe.character_occupation.characters_with_this_occupation -> Agent Pleakley\n# Answer:\nSpecial Agent", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> fictional_universe.fictional_character.occupation -> Special Agent -> people.profession.specialization_of -> Law enforcement officer\n# Answer:\nSpecial Agent"], "ground_truth": ["Michael Weatherly"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1865", "prediction": ["# Reasoning Path:\nJesus Christ -> people.person.profession -> Prophet -> people.profession.specialization_of -> Religious Leader\n# Answer:\nProphet", "# Reasoning Path:\nJesus Christ -> people.person.profession -> Carpentry -> fictional_universe.character_occupation.characters_with_this_occupation -> Joseph\n# Answer:\nCarpentry", "# Reasoning Path:\nJesus Christ -> common.topic.notable_for -> g.12551f1xr\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nJesus Christ -> people.person.profession -> Prophet -> people.profession.specialization_of -> Spiritual leader\n# Answer:\nProphet", "# Reasoning Path:\nJesus Christ -> people.person.profession -> Prophet -> base.descriptive_names.names.descriptive_name -> m.01064z57\n# Answer:\nProphet", "# Reasoning Path:\nJesus Christ -> people.person.profession -> Carpentry -> fictional_universe.character_occupation.characters_with_this_occupation -> Jesus - The Christ\n# Answer:\nCarpentry", "# Reasoning Path:\nJesus Christ -> people.person.profession -> Carpentry -> common.topic.notable_types -> Profession\n# Answer:\nCarpentry", "# Reasoning Path:\nJesus Christ -> people.person.profession -> Prophet -> people.profession.people_with_this_profession -> Zephaniah\n# Answer:\nProphet", "# Reasoning Path:\nJesus Christ -> people.person.profession -> Preacher -> common.topic.image -> Pro\n# Answer:\nPreacher", "# Reasoning Path:\nJesus Christ -> book.book_subject.works -> Jesus -> common.topic.notable_types -> Book\n# Answer:\nJesus"], "ground_truth": ["Carpentry", "Prophet", "Preacher"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1866", "prediction": ["# Reasoning Path:\nNew York City -> base.aareas.schema.administrative_area.administrative_parent -> New York -> location.location.containedby -> United States of America\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> base.aareas.schema.administrative_area.administrative_parent -> New York -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> New York\n# Answer:\nUnited States of America", "# Reasoning Path:\nNew York City -> base.aareas.schema.administrative_area.administrative_parent -> New York -> government.governmental_jurisdiction.government_bodies -> United States Bankruptcy Court for the Southern District of New York\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nNew York City -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> New York\n# Answer:\nUnited States of America", "# Reasoning Path:\nNew York City -> location.location.containedby -> New York -> location.location.containedby -> United States of America\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> base.aareas.schema.administrative_area.administrative_children -> Manhattan -> location.place_with_neighborhoods.neighborhoods -> Washington Heights\n# Answer:\nManhattan", "# Reasoning Path:\nNew York City -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> California\n# Answer:\nUnited States of America", "# Reasoning Path:\nNew York City -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> New York\n# Answer:\nUnited States of America"], "ground_truth": ["New York"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1868", "prediction": ["# Reasoning Path:\nKnox Overstreet -> film.film_character.portrayed_in_films -> m.0jx21d -> film.performance.actor -> Josh Charles\n# Answer:\nJosh Charles", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_for -> g.125fszykz\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nKnox Overstreet -> film.film_character.portrayed_in_films -> m.0jx21d -> film.performance.film -> Dead Poets Society\n# Answer:\nDead Poets Society", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_types -> Film character -> type.type.expected_by -> Character\n# Answer:\nFilm character", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_types -> Film character -> type.type.expected_by -> Character using Wham-O product\n# Answer:\nFilm character", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Fictional Character\n# Answer:\nFilm character", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_types -> Film character -> type.type.expected_by -> Killed by\n# Answer:\nFilm character", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_types -> Film character -> freebase.type_profile.kind -> Annotation\n# Answer:\nFilm character", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Topic\n# Answer:\nFilm character", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_types -> Film character -> freebase.type_profile.kind -> Portrayal\n# Answer:\nFilm character"], "ground_truth": ["Josh Charles"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-187", "prediction": ["# Reasoning Path:\nStephen Hawking -> people.person.education -> m.02kq1zs -> education.education.major_field_of_study -> Physics\n# Answer:\nPhysics", "# Reasoning Path:\nStephen Hawking -> people.person.education -> m.02kq1zs -> education.education.degree -> Bachelor of Arts\n# Answer:\nBachelor of Arts", "# Reasoning Path:\nStephen Hawking -> people.person.education -> m.03p8dsf -> education.education.institution -> Trinity Hall, Cambridge\n# Answer:\nTrinity Hall, Cambridge", "# Reasoning Path:\nStephen Hawking -> people.person.education -> m.05svw_r -> education.education.institution -> St Albans School\n# Answer:\nSt Albans School", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Astronomer -> people.profession.specialization_of -> Scientist\n# Answer:\nAstronomer", "# Reasoning Path:\nStephen Hawking -> people.person.education -> m.03p8dsf -> education.education.degree -> Doctorate\n# Answer:\nDoctorate", "# Reasoning Path:\nStephen Hawking -> people.person.education -> m.05svw_r -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nStephen Hawking -> music.artist.track_contributions -> m.0rvpv1n -> music.track_contribution.track -> A Glorious Dawn\n# Answer:\nA Glorious Dawn", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Astronomer -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Astronomers\n# Answer:\nAstronomer", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor"], "ground_truth": ["Physics"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.125, "path_precision": 0.1, "path_recall": 0.16666666666666666, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1870", "prediction": ["# Reasoning Path:\nDavid Carradine -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> California\n# Answer:\nUnited States of America", "# Reasoning Path:\nDavid Carradine -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nDavid Carradine -> film.person_or_entity_appearing_in_film.films -> m.0_xwg9m -> film.personal_film_appearance.film -> Jonas in the Jungle\n# Answer:\nJonas in the Jungle", "# Reasoning Path:\nDavid Carradine -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> New York City\n# Answer:\nUnited States of America", "# Reasoning Path:\nDavid Carradine -> people.person.nationality -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> California\n# Answer:\nUnited States of America", "# Reasoning Path:\nDavid Carradine -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nDavid Carradine -> film.person_or_entity_appearing_in_film.films -> m.0_xwg9m -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nDavid Carradine -> people.person.nationality -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nDavid Carradine -> film.person_or_entity_appearing_in_film.films -> m.0cgnkwb -> film.personal_film_appearance.film -> Ringers: Lord of the Fans\n# Answer:\nRingers: Lord of the Fans", "# Reasoning Path:\nDavid Carradine -> film.person_or_entity_appearing_in_film.films -> m.0crrb92 -> film.personal_film_appearance.film -> Starz Inside: Unforgettably Evil\n# Answer:\nStarz Inside: Unforgettably Evil"], "ground_truth": ["Bad Cop", "Wizards of the Lost Kingdom 2", "Fast Charlie... the Moonbeam Rider", "Bala Perdida", "The Serpent's Egg", "Brothers in Arms", "Miracle at Sage Creek", "Balto II: Wolf Quest", "Last Stand at Saber River", "Kill Bill Volume 1", "Midnight Fear", "Archie's Final Project", "Safari 3000", "On the Line", "Six Days in Paradise", "Kandisha", "Fuego", "Crime Zone", "The Golden Boys", "Kill Bill Volume 2", "An American Tail: The Treasure of Manhattan Island", "The Good Guys and the Bad Guys", "Hair High", "Martial Law", "Guaranteed on Delivery", "Being Michael Madsen", "Money to Burn", "Future Force", "Crank: High Voltage", "Evil Toons", "Boxcar Bertha", "Double Trouble", "The Outsider", "Blackout", "Tropical Snow", "Naked Movie", "Treasure Raiders", "Circle of Iron", "Fall Down Dead", "The New Swiss Family Robinson", "Last Hour", "Night Rhythms", "Thunder and Lightning", "Animal Instincts", "UnConventional", "Macon County Jail", "The Last Sect", "Kill Zone", "Behind Enemy Lines", "Dinocroc vs. Supergator", "High Noon, Part II: The Return of Will Kane", "Bound for Glory", "Permanent Vacation", "Big Stan", "Project Eliminator", "Starz Inside: Unforgettably Evil", "Autumn", "Dead & Breakfast", "Homo Erectus", "Road of No Return", "The Long Riders", "Detention", "Bird on a Wire", "Cloud Dancer", "Dune Warriors", "Death Race 2000", "Jealousy", "Richard III", "Son of the Dragon", "Deathsport", "Kiss of a Stranger", "Children of the Corn V: Fields of Terror", "The Ultimate Enemy", "Try This One for Size", "Six Against the Rock", "By Dawn's Early Light", "Kung Fu: The Movie", "Nightfall", "David Carradine Kung Fu Action Masters", "Max Havoc: Curse of the Dragon", "Run for Your Life", "Lone Wolf McQuade", "Taggart", "Out of the Wilderness", "Nowhere to Run", "Roadside Prophets", "Gray Lady Down", "Kung Fu Killer", "Armed Response", "Hell Ride", "Waxwork II: Lost in Time", "Karate Cop", "Young Billy Young", "The Bad Seed", "Eldorado", "Brotherhood of the Gun", "Sonny Boy", "Maybe I'll Come Home in the Spring", "Kill Bill: The Whole Bloody Affair", "Americana", "American Reel", "Trick or Treats", "David Carradine's Shaolin Cardio Kick Boxing Workout", "David Carradine's AM & PM Tai Chi Workout for Beginners", "Sundown: The Vampire in Retreat", "The Donor", "Absolute Evil", "Macho Callahan", "Night of the Templar", "Mr. Horn", "Break", "Stretch", "Cybercity", "Criminal Desire", "The Monster Hunter", "Camille", "Last Goodbye", "Future Zone", "Cannonball", "How to Rob a Bank", "I Saw What You Did", "Q", "Final Move", "Warlords", "Capital Punishment", "The Rage", "Dark Fields", "Knocking On Death's Door", "Crime School", "All Hell Broke Loose", "Oceans of Fire", "Code Name Jaguar", "The Misfit Brigade", "Mean Streets", "Dangerous Curves", "True Legend", "The Warrior and the Sorceress"], "ans_acc": 0.0072992700729927005, "ans_hit": 1, "ans_f1": 0.013605442176870748, "ans_precission": 0.1, "ans_recall": 0.0072992700729927005, "path_f1": 0.08333333333333333, "path_precision": 0.1, "path_recall": 0.07142857142857142, "path_ans_f1": 0.013605442176870748, "path_ans_precision": 0.1, "path_ans_recall": 0.0072992700729927005}
{"id": "WebQTest-1871", "prediction": ["# Reasoning Path:\nStanley Cup -> symbols.namesake.named_after -> Frederick Stanley, 16th Earl of Derby -> common.topic.article -> m.01q69x\n# Answer:\nFrederick Stanley, 16th Earl of Derby", "# Reasoning Path:\nStanley Cup -> symbols.namesake.named_after -> Frederick Stanley, 16th Earl of Derby -> symbols.name_source.namesakes -> Stanley\n# Answer:\nFrederick Stanley, 16th Earl of Derby", "# Reasoning Path:\nStanley Cup -> symbols.namesake.named_after -> Frederick Stanley, 16th Earl of Derby -> symbols.name_source.namesakes -> Stanley, Nova Scotia\n# Answer:\nFrederick Stanley, 16th Earl of Derby", "# Reasoning Path:\nStanley Cup -> time.recurring_event.current_frequency -> Yearly -> base.events.festival_event.part_of_series -> National Poetry Month\n# Answer:\nYearly", "# Reasoning Path:\nStanley Cup -> time.recurring_event.current_frequency -> Yearly -> common.topic.notable_types -> Unit of frequency\n# Answer:\nYearly", "# Reasoning Path:\nStanley Cup -> symbols.namesake.named_after -> Frederick Stanley, 16th Earl of Derby -> people.person.children -> Arthur Stanley\n# Answer:\nFrederick Stanley, 16th Earl of Derby", "# Reasoning Path:\nStanley Cup -> time.recurring_event.current_frequency -> Yearly -> common.topic.article -> m.02nbmwg\n# Answer:\nYearly", "# Reasoning Path:\nStanley Cup -> symbols.namesake.named_after -> Frederick Stanley, 16th Earl of Derby -> symbols.name_source.namesakes -> Stanley Park\n# Answer:\nFrederick Stanley, 16th Earl of Derby", "# Reasoning Path:\nStanley Cup -> symbols.namesake.named_after -> Frederick Stanley, 16th Earl of Derby -> people.person.children -> Edward Stanley, 17th Earl of Derby\n# Answer:\nFrederick Stanley, 16th Earl of Derby", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1933 Stanley Cup Finals -> sports.sports_championship_event.champion -> New York Rangers\n# Answer:\n1933 Stanley Cup Finals"], "ground_truth": ["Frederick Stanley, 16th Earl of Derby"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1873", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.country.official_language -> Malay Language\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.country.languages_spoken -> English Language\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Sh\u011bn -> common.topic.notable_types -> Country\n# Answer:\nSh\u011bn", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> base.aareas.schema.administrative_area.administrative_area_type -> Sovereign state\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Singapore -> location.country.official_language -> Tamil Language\n# Answer:\nSingapore", "# Reasoning Path:\nChinese language -> language.human_language.main_country -> China -> location.location.containedby -> Eurasia\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Sh\u011bn -> location.country.capital -> Linquan County\n# Answer:\nSh\u011bn", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.country.languages_spoken -> Nepali Language\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Sh\u011bn -> common.topic.article -> m.0pb4_ws\n# Answer:\nSh\u011bn", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Singapore -> location.country.official_language -> Standard Chinese\n# Answer:\nSingapore"], "ground_truth": ["China"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1874", "prediction": ["# Reasoning Path:\nPortugal -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Algeria\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nPortugal -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Tunisia\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nPortugal -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Angola\n# Answer:\nUnitary state", "# Reasoning Path:\nPortugal -> location.statistical_region.places_exported_to -> m.0bhdd35 -> location.imports_and_exports.exported_to -> Tunisia\n# Answer:\nTunisia", "# Reasoning Path:\nPortugal -> location.country.form_of_government -> Semi-presidential system -> common.topic.notable_types -> Form of Government\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nPortugal -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nPortugal -> location.statistical_region.places_exported_to -> m.0bhdd3p -> location.imports_and_exports.exported_to -> Libya\n# Answer:\nLibya", "# Reasoning Path:\nPortugal -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Egypt\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nPortugal -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nPortugal -> location.country.form_of_government -> Unitary state -> common.topic.notable_types -> Form of Government\n# Answer:\nUnitary state"], "ground_truth": ["Angola", "Jordan", "United Arab Emirates", "Tunisia", "Mozambique", "Saudi Arabia", "Morocco", "Egypt", "Algeria", "Libya", "Mexico", "Singapore"], "ans_acc": 0.4166666666666667, "ans_hit": 1, "ans_f1": 0.1818181818181818, "ans_precission": 0.2, "ans_recall": 0.16666666666666666, "path_f1": 0.1643835616438356, "path_precision": 0.6, "path_recall": 0.09523809523809523, "path_ans_f1": 0.49180327868852464, "path_ans_precision": 0.6, "path_ans_recall": 0.4166666666666667}
{"id": "WebQTest-1875", "prediction": ["# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc39sjp\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fm01\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc385hl\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp"], "ground_truth": ["Federal republic", "Presidential system", "Constitutional republic"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1876", "prediction": ["# Reasoning Path:\nBabylon -> location.location.containedby -> Iraq -> location.location.containedby -> Asia\n# Answer:\nIraq", "# Reasoning Path:\nBabylonia -> location.country.languages_spoken -> Akkadian language -> language.human_language.language_family -> Afroasiatic languages\n# Answer:\nAkkadian language", "# Reasoning Path:\nBabylonia -> location.country.languages_spoken -> Akkadian language -> language.human_language.countries_spoken_in -> Assyria\n# Answer:\nAkkadian language", "# Reasoning Path:\nBabylon -> location.location.containedby -> Iraq -> location.location.containedby -> Middle East\n# Answer:\nIraq", "# Reasoning Path:\nBabylon -> location.location.containedby -> Iraq -> location.country.administrative_divisions -> Iraqi Kurdistan\n# Answer:\nIraq", "# Reasoning Path:\nBabylon -> common.topic.image -> Babylon relief -> common.image.appears_in_topic_gallery -> Ishtar\n# Answer:\nBabylon relief", "# Reasoning Path:\nBabylonia -> location.country.languages_spoken -> Akkadian language -> language.human_language.language_family -> East Semitic languages\n# Answer:\nAkkadian language", "# Reasoning Path:\nBabylonia -> location.location.people_born_here -> Hillel the Elder -> people.person.gender -> Male\n# Answer:\nHillel the Elder", "# Reasoning Path:\nBabylon -> location.location.containedby -> Iraq -> location.location.containedby -> Western Asia\n# Answer:\nIraq", "# Reasoning Path:\nBabylon -> location.location.containedby -> Iraq -> common.topic.notable_types -> Country\n# Answer:\nIraq"], "ground_truth": ["Akkadian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1877", "prediction": ["# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> The Body in Pieces\n# Answer:\nModern art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nModern art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Der Blaue Reiter -> common.topic.notable_for -> g.12596yl82\n# Answer:\nDer Blaue Reiter", "# Reasoning Path:\nWassily Kandinsky -> book.author.school_or_movement -> Modernism -> visual_art.art_period_movement.associated_artworks -> American Gothic\n# Answer:\nModernism", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Eyewitness: Reports from an Art World in Crisis\n# Answer:\nModern art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Abstract art -> visual_art.art_period_movement.associated_artists -> Adja Yunkers\n# Answer:\nAbstract art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Abstract art -> common.topic.image -> Black Square\n# Answer:\nAbstract art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Origins, Imitation, Conventions\n# Answer:\nModern art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> visual_art.art_period_movement.associated_artworks -> Elemental: The Power of Illuminated Love\n# Answer:\nModern art", "# Reasoning Path:\nWassily Kandinsky -> book.author.school_or_movement -> Modernism -> visual_art.art_period_movement.associated_artworks -> Automat\n# Answer:\nModernism"], "ground_truth": ["Der Blaue Reiter", "Abstract art", "German Expressionism", "Expressionism", "Modern art"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6857142857142857, "ans_precission": 0.8, "ans_recall": 0.6, "path_f1": 0.6857142857142857, "path_precision": 0.8, "path_recall": 0.6, "path_ans_f1": 0.6857142857142857, "path_ans_precision": 0.8, "path_ans_recall": 0.6}
{"id": "WebQTest-1878", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> South Korea\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> American Samoa\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Northern Mariana Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Mexico\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guam\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Marshall Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Germany\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nbase.schemastaging.context_name.pronunciation"], "ground_truth": ["Federal republic", "Presidential system", "Constitutional republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1879", "prediction": ["# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban convertible peso -> common.topic.notable_for -> g.1257mndy1\n# Answer:\nCuban convertible peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban convertible peso -> common.topic.webpage -> m.04m29wr\n# Answer:\nCuban convertible peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban peso -> common.topic.notable_for -> g.125c74q9f\n# Answer:\nCuban peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban convertible peso -> common.topic.article -> m.049p6m\n# Answer:\nCuban convertible peso", "# Reasoning Path:\nCuba -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc38hmp\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban peso -> common.topic.notable_types -> Currency\n# Answer:\nCuban peso", "# Reasoning Path:\nCuba -> book.book_subject.works -> A simple Habana melody -> book.written_work.subjects -> History\n# Answer:\nA simple Habana melody", "# Reasoning Path:\nCuba -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc3d68y\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nCuba -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc3d9ps\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nCuba -> book.book_subject.works -> White rose -> book.written_work.subjects -> History\n# Answer:\nWhite rose"], "ground_truth": ["Cuban peso", "Cuban convertible peso"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-188", "prediction": ["# Reasoning Path:\nBristol Palin -> people.person.education -> m.0cpk4bl -> education.education.institution -> West Anchorage High School\n# Answer:\nWest Anchorage High School", "# Reasoning Path:\nBristol Palin -> people.person.education -> m.0cpkbwp -> education.education.institution -> Juneau-Douglas High School\n# Answer:\nJuneau-Douglas High School", "# Reasoning Path:\nBristol Palin -> people.person.education -> m.0byl4zl -> education.education.institution -> Wasilla High School\n# Answer:\nWasilla High School", "# Reasoning Path:\nBristol Palin -> people.person.sibling_s -> m.04mvspr -> people.sibling_relationship.sibling -> Trig Palin\n# Answer:\nTrig Palin", "# Reasoning Path:\nBristol Palin -> common.topic.webpage -> m.09ww50l -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nBristol Palin -> people.person.sibling_s -> m.04tll_g -> people.sibling_relationship.sibling -> Track Palin\n# Answer:\nTrack Palin", "# Reasoning Path:\nBristol Palin -> common.topic.webpage -> m.09wv0fg -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nBristol Palin -> common.topic.webpage -> m.0dbrf71 -> common.webpage.category -> Curated Topic\n# Answer:\nCurated Topic", "# Reasoning Path:\nBristol Palin -> people.person.sibling_s -> m.04tllyz -> people.sibling_relationship.sibling -> Willow Palin\n# Answer:\nWillow Palin", "# Reasoning Path:\nBristol Palin -> common.topic.webpage -> m.0dbrf71 -> common.webpage.in_index -> Blissful Celebrities\n# Answer:\nBlissful Celebrities"], "ground_truth": ["West Anchorage High School", "Wasilla High School", "Juneau-Douglas High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1880", "prediction": ["# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.010nnlj1 -> government.government_position_held.office_holder -> Anna Moeller\n# Answer:\nAnna Moeller", "# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.0_gt56s -> government.government_position_held.office_holder -> Art Turner\n# Answer:\nArt Turner", "# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.0wzq9t2 -> government.government_position_held.office_holder -> David Harris\n# Answer:\nDavid Harris", "# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.010nnlj1 -> government.government_position_held.jurisdiction_of_office -> Illinois\n# Answer:\nIllinois", "# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.010nnlj1 -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nBasic title", "# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.0_gt56s -> government.government_position_held.jurisdiction_of_office -> Illinois\n# Answer:\nIllinois", "# Reasoning Path:\nIllinois House of Representatives -> common.topic.image -> Illinoiscapitol -> common.image.appears_in_topic_gallery -> Illinois Senate\n# Answer:\nIllinoiscapitol", "# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.0_gt56s -> government.government_position_held.office_position_or_title -> Member of Illinois House of Representatives\n# Answer:\nMember of Illinois House of Representatives", "# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.0wzq9t2 -> government.government_position_held.basic_title -> State Representative\n# Answer:\nState Representative", "# Reasoning Path:\nIllinois House of Representatives -> common.topic.notable_types -> Governmental Body -> freebase.type_profile.kind -> Definition\n# Answer:\nGovernmental Body"], "ground_truth": ["James T. Londrigan", "Anna Moeller", "Art Turner", "Thaddeus Jones", "Wayne Rosenthal", "David Harris", "Harber H. Hall", "Ann Williams", "Daniel M. Pierce", "Bradley M. Glass", "Tobias Barry", "Terry Deering", "Adam Brown", "John D. Anthony", "Robert W. McCarthy", "Bruce L. Douglas", "Tom Cross", "Ralph C. Capparelli", "Josh Harms", "Bernard B. Wolfe"], "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0.2, "ans_precission": 0.3, "ans_recall": 0.15, "path_f1": 0.20689655172413793, "path_precision": 0.3, "path_recall": 0.15789473684210525, "path_ans_f1": 0.2, "path_ans_precision": 0.3, "path_ans_recall": 0.15}
{"id": "WebQTest-1881", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ll\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22zj\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc378pv\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc37hbq\n# Answer:\nlocation.statistical_region.part_time_employment_percent"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-1882", "prediction": ["# Reasoning Path:\nFranklin D. Roosevelt -> government.politician.government_positions_held -> m.0b_p9hl -> government.government_position_held.basic_title -> State Senator\n# Answer:\nState Senator", "# Reasoning Path:\nFranklin D. Roosevelt -> government.politician.government_positions_held -> m.0b_p9hl -> government.government_position_held.office_position_or_title -> New York State Senator\n# Answer:\nNew York State Senator", "# Reasoning Path:\nFranklin D. Roosevelt -> government.politician.government_positions_held -> m.03q2h_9 -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nFranklin D. Roosevelt -> people.person.profession -> Politician -> freebase.type_profile.kind -> Title\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin D. Roosevelt -> government.politician.government_positions_held -> m.03q2h_j -> government.government_position_held.office_position_or_title -> Assistant Secretary of the Navy\n# Answer:\nAssistant Secretary of the Navy", "# Reasoning Path:\nFranklin D. Roosevelt -> people.person.profession -> Politician -> type.type.domain -> Government\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin D. Roosevelt -> people.person.profession -> Soldier -> common.topic.notable_types -> Profession\n# Answer:\nSoldier", "# Reasoning Path:\nFranklin D. Roosevelt -> government.politician.government_positions_held -> m.03q2h_j -> government.government_position_held.basic_title -> Assistant Secretary\n# Answer:\nAssistant Secretary", "# Reasoning Path:\nFranklin D. Roosevelt -> government.politician.government_positions_held -> m.03q2h_9 -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nFranklin D. Roosevelt -> government.politician.government_positions_held -> m.03q2h_j -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America"], "ground_truth": ["Assistant Secretary of the Navy", "New York State Senator", "Governor of New York"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.30769230769230765, "ans_precission": 0.2, "ans_recall": 0.6666666666666666, "path_f1": 0.28571428571428575, "path_precision": 0.2, "path_recall": 0.5, "path_ans_f1": 0.30769230769230765, "path_ans_precision": 0.2, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1883", "prediction": ["# Reasoning Path:\nRobert Downey Jr. -> tv.tv_actor.guest_roles -> m.040r4ks -> tv.tv_guest_role.episodes_appeared_in -> The Fat Guy Strangler\n# Answer:\nThe Fat Guy Strangler", "# Reasoning Path:\nRobert Downey Jr. -> tv.tv_actor.guest_roles -> m.0bv0lvm -> tv.tv_guest_role.episodes_appeared_in -> Paula DeAnda, Robert Downey Jr.;\n# Answer:\nPaula DeAnda, Robert Downey Jr.;", "# Reasoning Path:\nRobert Downey Jr. -> tv.tv_actor.guest_roles -> m.040r4ks -> tv.tv_guest_role.character -> Patrick Pewterschmidt\n# Answer:\nPatrick Pewterschmidt", "# Reasoning Path:\nRobert Downey Jr. -> tv.tv_actor.guest_roles -> m.0bv0rg1 -> tv.tv_guest_role.episodes_appeared_in -> Show #0534\n# Answer:\nShow #0534", "# Reasoning Path:\nRobert Downey Jr. -> film.actor.film -> m.0126b7kf -> film.performance.film -> Captain America: Civil War\n# Answer:\nCaptain America: Civil War", "# Reasoning Path:\nRobert Downey Jr. -> film.actor.film -> m.0126b7kf -> film.performance.character -> Iron Man\n# Answer:\nIron Man", "# Reasoning Path:\nRobert Downey Jr. -> film.actor.film -> m.01z0q3l -> film.performance.film -> Weird Science\n# Answer:\nWeird Science", "# Reasoning Path:\nRobert Downey Jr. -> music.artist.album -> Smile -> music.single.versions -> Smile (7\\\" version)\n# Answer:\nSmile", "# Reasoning Path:\nRobert Downey Jr. -> music.artist.album -> The Futurist -> music.recording.lyrics_website -> m.0627rlh\n# Answer:\nThe Futurist", "# Reasoning Path:\nRobert Downey Jr. -> film.actor.film -> m.012ky5f9 -> film.performance.film -> Iron Man 2\n# Answer:\nIron Man 2"], "ground_truth": ["California Substance Abuse Treatment Facility and State Prison, Corcoran"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1884", "prediction": ["# Reasoning Path:\nBarack Obama -> people.person.education -> m.0nbyhqc -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.0nbyhqc -> education.education.institution -> Noelani Elementary School\n# Answer:\nNoelani Elementary School", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.0nbyhqc -> freebase.valuenotation.has_no_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> education.education.institution -> Occidental College\n# Answer:\nOccidental College", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.0nbyhqc -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nMinor", "# Reasoning Path:\nBarack Obama -> people.person.ethnicity -> English American -> people.ethnicity.languages_spoken -> American English\n# Answer:\nEnglish American", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.subjects -> 2012 Presidential debate\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Bound Man -> book.written_work.subjects -> United States of America\n# Answer:\nA Bound Man"], "ground_truth": ["Juris Doctor", "Bachelor of Arts"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1885", "prediction": ["# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.deities -> Ayyappan\n# Answer:\nHinduism", "# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.deities -> Yama\n# Answer:\nHinduism", "# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.deities -> Rama\n# Answer:\nHinduism", "# Reasoning Path:\nShiva -> common.topic.notable_for -> g.125bspml8\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.texts -> Upanishads\n# Answer:\nHinduism", "# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.practices -> Astrology\n# Answer:\nHinduism", "# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.practices -> Ayurveda\n# Answer:\nHinduism", "# Reasoning Path:\nShiva -> fictional_universe.person_in_fiction.representations_in_fiction -> Lord Shiva -> common.topic.notable_types -> Film character\n# Answer:\nLord Shiva", "# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.practices -> Bhakti\n# Answer:\nHinduism", "# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.texts -> Aranyaka\n# Answer:\nHinduism"], "ground_truth": ["Hinduism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1886", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove -> common.topic.article -> m.029mn3\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_trwk\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> Glass\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> medicine.medical_treatment.used_to_treat -> Presbyopia\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> Waking up early\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_ty__\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.image -> A bifocal corrective eyeglasses lens\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_wj2d\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.instrument.family -> Crystallophone\n# Answer:\nGlass harmonica"], "ground_truth": ["Bifocals", "Lightning rod", "Glass harmonica", "Franklin stove"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.8, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.8, "path_ans_recall": 0.75}
{"id": "WebQTest-1887", "prediction": ["# Reasoning Path:\nSwitzerland -> location.country.currency_used -> Swiss franc -> finance.currency.countries_used -> Liechtenstein\n# Answer:\nSwiss franc", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d0tc3g\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nSwitzerland -> location.statistical_region.co2_emissions_per_capita -> g.1245_dgvf\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6p27nwl\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.1hhc3d66v\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nSwitzerland -> location.statistical_region.co2_emissions_per_capita -> g.1245_kjqq\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nSwitzerland -> location.statistical_region.co2_emissions_per_capita -> g.1245_kjqr\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita"], "ground_truth": ["Swiss franc"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-1888", "prediction": ["# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> sports.sports_league.teams -> m.0crt479\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> sports.sports_league.teams -> m.0crt4fq\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> organization.organization.child -> m.0ck2dsc\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> sports.sports_league.teams -> m.0crt47r\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> organization.organization.child -> m.0zs6st6\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> business.employer.employees -> m.010vsyz7\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> common.topic.webpage -> m.09w8snm -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> organization.organization.child -> m.0ck2ds6\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> business.employer.employees -> m.0cmxjcn\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> common.topic.image -> Crowd outside the 1903 World Series -> common.image.size -> m.02bpf1c\n# Answer:\nCrowd outside the 1903 World Series"], "ground_truth": ["1982 World Series"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1889", "prediction": ["# Reasoning Path:\nBill Clinton -> government.us_president.vice_president -> Al Gore -> people.person.religion -> Baptists\n# Answer:\nAl Gore", "# Reasoning Path:\nBill Clinton -> base.popstra.celebrity.dated -> m.065pxsm -> base.popstra.dated.participant -> Dolly Kyle Browning\n# Answer:\nDolly Kyle Browning", "# Reasoning Path:\nBill Clinton -> government.us_president.vice_president -> Al Gore -> people.person.gender -> Male\n# Answer:\nAl Gore", "# Reasoning Path:\nBill Clinton -> government.us_president.vice_president -> Al Gore -> people.person.religion -> Southern Baptist Convention\n# Answer:\nAl Gore", "# Reasoning Path:\nBill Clinton -> award.award_nominee.award_nominations -> m.0k08gry -> award.award_nomination.ceremony -> 47th Annual Grammy Awards\n# Answer:\n47th Annual Grammy Awards", "# Reasoning Path:\nBill Clinton -> award.award_nominee.award_nominations -> m.0k08gry -> award.award_nomination.award -> Grammy Award for Best Spoken Word Album\n# Answer:\nGrammy Award for Best Spoken Word Album", "# Reasoning Path:\nBill Clinton -> award.award_nominee.award_nominations -> m.0xnp1nm -> award.award_nomination.nominated_for -> Back To Work: Why We Need Smart Government For A Strong Economy\n# Answer:\nBack To Work: Why We Need Smart Government For A Strong Economy", "# Reasoning Path:\nBill Clinton -> award.award_nominee.award_nominations -> m.0k0tmzp -> award.award_nomination.award_nominee -> Mikhail Sergeyevich Gorbachev\n# Answer:\nMikhail Sergeyevich Gorbachev", "# Reasoning Path:\nBill Clinton -> award.award_nominee.award_nominations -> m.0k0tmzp -> award.award_nomination.ceremony -> 46th Annual Grammy Awards\n# Answer:\n46th Annual Grammy Awards", "# Reasoning Path:\nBill Clinton -> award.award_nominee.award_nominations -> m.0k0tmzp -> award.award_nomination.nominated_for -> Wolf Tracks and Peter and the Wolf\n# Answer:\nWolf Tracks and Peter and the Wolf"], "ground_truth": ["Al Gore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-189", "prediction": ["# Reasoning Path:\nEgypt -> location.location.containedby -> Arab world -> common.topic.subject_of -> As Egypt Howls and History Tweets\n# Answer:\nArab world", "# Reasoning Path:\nEgypt -> location.location.containedby -> Arab world -> periodicals.newspaper_circulation_area.newspapers -> Al-Shorouk\n# Answer:\nArab world", "# Reasoning Path:\nEgypt -> base.locations.countries.continent -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica", "# Reasoning Path:\nEgypt -> location.location.containedby -> Middle East -> location.location.contains -> Saudi Arabia\n# Answer:\nMiddle East", "# Reasoning Path:\nEgypt -> location.location.containedby -> Arab world -> location.location.contains -> Sudan\n# Answer:\nArab world", "# Reasoning Path:\nEgypt -> location.location.containedby -> Arab world -> location.location.contains -> Iraq\n# Answer:\nArab world", "# Reasoning Path:\nEgypt -> location.location.containedby -> Middle East -> travel.travel_destination.tour_operators -> Bunnik Tours\n# Answer:\nMiddle East", "# Reasoning Path:\nEgypt -> location.location.containedby -> Middle East -> location.location.contains -> Iraq\n# Answer:\nMiddle East", "# Reasoning Path:\nEgypt -> location.location.containedby -> Middle East -> location.location.events -> Middle East Theatre of World War II\n# Answer:\nMiddle East", "# Reasoning Path:\nEgypt -> location.location.containedby -> Arab world -> location.location.contains -> Jordan\n# Answer:\nArab world"], "ground_truth": ["Middle East"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1890", "prediction": ["# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.compositional_form.superforms -> Vocal music\n# Answer:\nArt song", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Chamber music -> music.genre.parent_genre -> Classical music\n# Answer:\nChamber music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.compositional_form.compositions -> L'adieu du cavalier\n# Answer:\nArt song", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.compositional_form.superforms -> Song\n# Answer:\nArt song", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.genre.artists -> Aaron Copland\n# Answer:\nArt song", "# Reasoning Path:\nLife of Franz Liszt -> base.kwebbase.kwtopic.connections_to -> achille-claude debussy played for franz liszt -> base.kwebbase.kwconnection.subject -> Claude Debussy\n# Answer:\nachille-claude debussy played for franz liszt", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Classical music -> radio.radio_subject.programs_with_this_subject -> The Collector\u2019s Shelf\n# Answer:\nClassical music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.genre.artists -> Albert Roussel\n# Answer:\nArt song", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Chamber music -> music.genre.albums -> Time Will Pronounce\n# Answer:\nChamber music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.genre.artists -> Alexander Borodin\n# Answer:\nArt song"], "ground_truth": ["Souvenir de la fianc\u00e9e, S. 385/3", "Galop (in A minor), S. 218", "Elegy no. 2, S. 131", "Drei Lieder aus Schillers Wilhelm Tell", "Mephisto Polka", "Ballade aus Der fliegende Holl\u00e4nder, S. 441", "Heroischer Marsch im ungarischem Stil, S. 231", "Adagio in C major, S. 158d", "Chor\u00e4le, S. 506a: No. 2. Jesu Christe: Die f\u00fcnf Wunden", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 9. Andante lagrimoso", "Album-Leaf in A-flat (Portugal), S. 166b", "Tarantelle di bravura d\u2019apr\u00e8s la tarantelle de La muette de Portici, S. 386/1", "\u00c9tude en douze exercices, S. 136: I. Allegro con fuoco", "Douze grandes \u00e9tudes, S. 137: No. 11 in D-flat major (Lento assai)", "Variation \u00fcber einen Walzer von Diabelli, S. 147", "Totentanz, S. 126: III. Variation II", "Die Loreley, S. 273/1", "Schlaflos!, S. 203", "Weihnachtsbaum, S. 185a: VIII. [Alt-provenzalische No\u00ebl]", "Hungarian Battle March, S. 119", "Hungarian Rhapsody for Orchestra no. 3 in D major, S. 359/3", "Die Loreley", "Douze grandes \u00e9tudes, S. 137: No. 2 in A minor (Molto vivace a capriccio)", "Hungarian Rhapsody No. 13", "Symphonisches Zwischenspiel zu \u00dcber allen Zauber Liebe, S. 497", "Totentanz, S. 126: VII. Cadenza", "Reimar der Alte", "Marche hongroise, S425/2b", "Valse m\u00e9lancolique, S. 210a", "Piano Concerto No. 1", "Soir\u00e9es de Vienne, S. 427: No. 3 in E major. Allegro vivace", "Klavierst\u00fcck aus der Bonn Beethoven-Kantate, S. 507", "Chor\u00e4le, S. 506a: No. 6. O Haupt voll Blut und Wunden", "S'il est un charmant gazon, S. 284/2", "Magyar tempo, S. 241b", "Mephisto Waltz no. 4, S. 696", "Geharnischte Lieder, S. 511: No. 2. Nicht gezagt!", "Polonaise aus Tschaikowskys \\\"Eugen Onegin\\\", S. 429", "La Campanella : Nu Rave", "Ungarische Nationalmelodien, S. 243: No. 2. Animato", "Hungarian Rhapsody no. 7 in D minor, S. 244 no. 7", "Consolation in C-sharp minor, S. 171a no. 3", "Album-Leaf: Larghetto in D-flat major, S. 167p", "Was tun?", "Seven brilliant variations on a theme by Rossini, S. 149", "Via Crucis, S. 53: Station IV: Jesus begegnet seiner Heiligen Mutter", "Trois \u00e9tudes de concert, S. 144: III. \\\"Un sospiro\\\" in D-flat major", "Variations on \\\"Tisz\u00e1ntuli sz\u00e9p l\u00e9any\\\", S. 384a", "Five Hungarian Folksongs, S. 245: No. 1. Lassan. Lento", "Variation on a Waltz by Diabelli", "Der alte Vagabund", "Der du von dem Himmel bist, S. 279/3", "Consolation in E major, S. 172 no. 6: Allegretto sempre cantabile", "Cs\u00e1rd\u00e1s", "S'il est un charmant gazon, S. 538", "Angiolin dal biondo crin, S. 269/3", "Fantaisie sur des motifs de La pastorella dell\u2019Alpi e Li marinari des Soir\u00e9es musicales, S. 423", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 16 in E major", "Consolation in E major, S. 171a no. 2: Un poco pi\u00f9 mosso", "Fun\u00e9railles", "A magyarok Istene, S. 543", "Hungarian Rhapsody no. 18 in F-sharp minor, S. 244 no. 18/2", "Un sospiro", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 3 in E major", "Historical Hungarian Portraits, S. 205: No. 3. V\u00f6r\u00f6smarty Mihaly", "Magyar dalok, S. 242: No. 9 in A minor", "Marche fun\u00e8bre et Cavatine de Lucia de Lammermoor, S. 398", "Zigeuner-Epos, S. 695b: No. 1 in C minor. Lento", "Drei St\u00fccke aus der Legende der heiligen Elisabeth, S. 498a: Nr. 2. Marsch der Kreuzritter", "Illustrations du Proph\u00e8te, S. 414: No. 3: Pastorale - Appel aux armes", "Biterolf und der Schmied von Ruhla", "Rosario, S. 670: No. 2. Mysteria dolorosa", "Waldesrauschen (Forest Murmurs)", "Ungarische Nationalmelodien, S. 243: No. 1. Tempo giusto", "Album d'un voyageur, S. 156: I. Impressions et po\u00e9sies: 3. Les Cloches de G*****", "Am Rhein, im sch\u00f6nen Strome, S. 531 no. 2", "Valse, S. 210b", "Album-Leaf in D major, S. 164h", "Scherzo in G minor, S. 153", "Consolation in E major, S. 172 no. 2: Un poco pi\u00f9 mosso", "Grande valse di bravura, S. 209", "Album-Leaf: Preludio, S. 164j", "Les Adieux, r\u00eaverie sur un motif de Rom\u00e9o et Juliette, S. 409", "Hungarian Rhapsody no. 9 in E-flat major, S. 244 no. 9 \\\"Carnival in Pest\\\"", "Historische ungarische Bildnisse, S. 205a: No. 1. Sz\u00e9chenyi Istv\u00e1n", "Via Crucis, S. 504a: Station XIV: J\u00e9sus est mis dans le sepulcre", "Historical Hungarian Portraits, S. 205: No. 5. De\u00e1k Ferenc", "Variation on the March from Bellini's I Puritani", "Totentanz, S. 525, R. 188", "Grande Fantaisie sur des motifs de Soir\u00e9es musicales, S. 422/1", "Spirto gentil, S. 400a", "Resignazione, S. 187a", "Les pr\u00e9ludes", "Gastibelza, S. 540", "Mazeppa, S. 511c", "Feuille morte, S. 428", "Ave Maria (d'Arcadelt), S. 183 no. 2", "Historical Hungarian Portraits, S. 205: No. 7. Mosonyi Mih\u00e1ly", "Freisch\u00fctz Fantasie, S. 451", "Orpheus, S. 672a", "Walzer in A major, S. 208a", "Trauerode (Die Todten), S. 268 no. 2", "Orpheus, S. 98", "Enfant, si j'\u00e9tais roi, S. 537", "\u00c0 la Chapelle Sixtine, S. 360", "O lieb, so lang du lieben kannst!, S. 540a", "Un portrait en musique de la Marquise de Blocqueville, S. 190: III. M\u00eame mouvement mais avec incertitude", "Weihnachtsbaum, S. 185a: III. Die Hirten an der Krippe (In dulci jubilo)", "Albumblatt in Walzerform, S. 166", "Fanfare zur Enth\u00fcllung des Carl-Augusts Monument, S. 542b", "Album-Leaf in E major (Detmold), S. 164d", "Fantasy on Themes from Mozart's Figaro and Don Giovanni", "Spinnerin-Lied, Transkripition aus Wagners \\\"Der fliegende Holl\u00e4nder\\\", S. 440", "Morceau en fa majeur, S. 695", "\u00c0 la Chapelle Sixtine, S. 461/1", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 18 in C-sharp minor", "Introduction des Variations sur une marche du Si\u00e8ge de Corinthe, S. 421a", "Trois Chansons, S. 510a: No. 1. La Consolation", "Suite no. 4 in G major, op. 61 \\\"Mozartiana\\\": III. Preghiera: Andante non tanto", "Grande Fantaisie di bravura sur La Clochette de Paganini, S. 420", "Gaudeamus igitur, S. 240/1", "Sept variations brillantes sur un th\u00e8me de Rossini, op. 2, S. 149", "Allegro maestoso, S. 692c", "Liebestraum No. 3 As-dur", "Ungarischer Marsch in B-flat major, S. 229a", "R\u00e9miniscences de Robert le Diable, S. 413", "Historical Hungarian Portraits, S. 205: No. 1. Sz\u00e9ch\u00e9nyi Istvan", "Tre sonetti di Petrarca, S. 270: III. I' vidi in terra angelici costumi", "Hungarian Rhapsody No. 1", "Fantasia on Hungarian Folk Melodies for piano and orchestra, S. 123", "\u00c9l\u00e9gie sur des motifs du Prince Louis Ferdinand de Prusse, S. 168/1", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 4b in E major", "Ballade no. 2, S. 170a", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 21 in E minor", "Via Crucis, S. 504a: Station VII: J\u00e9sus tombe pour la seconde fois", "Mephisto Waltz no. 4, S. 216b", "Chor\u00e4le, S. 506a: No. 10. Was Gott tut, das ist wohlgetan", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 1 in G minor", "\u00c9tude en douze exercices, S. 136: VIII. Allegro con spirito", "Von der Wiege bis zum Grabe, S. 107: II. Der Kampf ums Dasein", "Dante fragment, S. 701e", "Mephisto Waltz no. 2, S. 515", "En r\u00eave - Nocturne, S. 207", "Tannh\u00e4user Overture", "Lilie, S. 166m/1", "Historische ungarische Bildnisse, S. 205a: No. 7. Mosonyi Mih\u00e1ly", "Ballade No. 2", "Blume und Duft", "Album-Leaf in A major, S. 166s", "Hungarian Rhapsody no. 10 in E major, S. 244 no. 10", "Il penseroso, S. 157b", "Via Crucis, S. 53: Station I: Jesus wird zum Tode verdammt", "Magyar dalok, S. 242: No. 1 in C minor", "\u00c9tude en douze exercices, S. 136: X. Moderato", "Totentanz, S. 126: I. Andante - Allegro - Allegro moderato", "Galop de bal, S. 220", "Klavierst\u00fcck \u00fcber ein fremdes Thema, S. 387a", "Soir\u00e9es de Vienne, S. 427: No. 4 in D-flat major. Andantino a capriccio", "Aus der Musik von Eduard Lassen zu Hebbels Nibelungen und Goethes Faust, S. 496: II. Faust: 2. Hoffest: Marsch und Polonaise", "Deux marches dans le genre hongrois, S. 693: No. 1 in D minor", "Grandes etudes de Paganini", "Der Fischerknabe", "Tre sonetti di Petrarca, S. 158: No. 1. Sonetto XLVII. Benedetto sia il giorno", "Album-Leaf: Lyon Pr\u00e9lude, S. 166d", "Cadenza, S. 695f", "Rondo di bravura, op. 4 no. 2, S. 152", "La romanesca, S. 252a/2", "Via Crucis, S. 504a: Station X: J\u00e9sus est d\u00e9pouill\u00e9 de ses v\u00eatements", "Variations sur Le Carnaval de Venise, S. 700a", "Romancero espagnol, S. 695c: No. 2. Elaboration of an unidentified theme", "Album-Leaf: Andante in E-flat major, S. 167r", "Fantasie \u00fcber Themen aus Beethoven's Ruinen von Athen, S. 388b", "R\u00e1k\u00f3czi-Marsch, S. 244c", "Responsorien und Antiphonen, S. 30: I. In nativitate Domini", "Beethoven Symphonies", "Fantasie und Fuge \u00fcber den Choral Ad nos, ad salutarem undam, S. 259", "Hungaria", "Dante Sonata", "Ang\u00e9lus ! Pri\u00e8re aux anges gardiens, S. 163 no. 1", "Deux \u00e9pisodes d'apres le Faust de Lenau, S. 110: I. Der n\u00e4chtliche Zug", "Der Gl\u00fcckliche", "Von der Wiege bis zum Grabe, S. 107: I. Die Wiege", "Festkl\u00e4nge, S. 511d", "Transcendental \u00c9tude No. 6", "Einleitung, Fuge und Magnificat, S. 672b", "Am Grabe Richard Wagners, S. 202", "Via Crucis, S. 504a: Station IV: J\u00e9sus rencontre sa tr\u00e8s sainte m\u00e8re", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 5a in E major \\\"La Chasse\\\"", "Album-Leaf: Berlin Preludio, S. 164g", "Helge's Treue, S. 686", "Chanson boh\u00e9mienne, S. 250 no. 2", "Cadenza for \\\"Un sospiro\\\", S. 144/3", "La romanesca, S. 252a/1", "Hussitenlied, S. 234", "R\u00e9miniscences de La Scala, S. 458", "A magyarok Istene, S. 543bis", "Weil noch, Sonnenstrahl", "Album-Leaf: Purgatorio. Andante in B minor, S. 166r/1", "Aus der Musik von Eduard Lassen zu Hebbels Nibelungen und Goethes Faust, S. 496: I. Nibelungen: 2. Bechlarn", "Kennst du das Land, S. 531 no. 3", "Wartburglieder", "Paraphrase de concert sur Ernani II, S. 432", "Trauervorspiel und Trauermarsch, S. 206: No. 1. Trauervorspiel", "Aus Lohengrin, S. 446: No. 1. Festspiel und Brautlied", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 6 in A minor", "A magyarok Istene", "Etude in Twelve Exercises", "Magyarische Litanei", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 10. (Hymne)", "Hungarian Rhapsody no. 12 in C-sharp minor, S. 244 no. 12", "Transcendental \u00c9tude No. 9", "Hungaria, S. 511e", "Ab irato, grande \u00e9tude de perfectionnement, S. 143", "Valse oubli\u00e9e no. 2 in A-flat major, S. 215 no. 2", "Hungarian Rhapsody No. 6", "Magyar dalok, S. 242: No. 8 in F minor", "R\u00e9miniscences de \\\"La Juive\\\", S. 409a", "Pastorale, S. 508", "Hungarian Coronation Mass, S. 11: II. Gloria", "Technische Studien, S. 146: No. 62. Spr\u00fcnge mit der Tremolo-Begleitung", "Five Hungarian Folksongs, S. 245: No. 3. Lassan. Andante", "Trois Chansons, S. 510a: No. 3. L'Esp\u00e9rance", "Pri\u00e8re d'un enfant \u00e0 son r\u00e9veil, S. 171c", "Hexam\u00e9ron, S. 365b: VII. Variation V. Vivo e brillante - Fuocoso molto energico - Lento quasi recitativo", "Ora pro nobis, S. 262", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 8a. Andante con sentimento", "Rosario, S. 670: No. 1. Mysteria gaudiosa", "Ave Maria in E major, S. 182 \\\"Die Glocken von Rom\\\"", "Klavierst\u00fcck in F-sharp major, S. 193", "Two Hungarian Recruiting Dances, S. 241 \\\"Zum Andenken\\\": No. 1. Kinizsi n\u00f3t\u00e1ja", "Weihnachtsbaum, S. 186: Nr. 9. Abendglocken", "Ouvert\u00fcre zu Tannh\u00e4user, S. 442", "Adela\u00efde von Beethoven, S. 466: Cadenza ad libitum", "Album Leaf in G major (Dante-Symphony progression), S. 167f", "Historische ungarische Bildnisse, S. 205a: No. 3. Teleki L\u00e1szl\u00f3", "Zwei St\u00fccke aus der heiligen Elisabeth, S. 693a: No. 1. Das Rosenmirakel", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 7 in E major \\\"Alternative\\\"", "\u00c9tude en douze exercices, S. 136: IX. Allegro grazioso", "Oh! quand je dors, S. 282/1", "Transcendental \u00c9tudes", "Elegy no. 2, S. 197", "Album-Leaf in G major, S. 166q", "Ce qu'on entend sur la montagne", "Zigeuner-Epos, S. 695b: No. 10 in F major. Lento", "\u00c9tude en douze exercices, S. 136: XI. Allegro grazioso", "Pilgerchor aus Tannh\u00e4user, S. 443/1", "Soir\u00e9es de Vienne, S. 427: No. 5 in G-flat major. Moderato cantabile con affetto", "Les Jeux d'eaux \u00e0 la Villa d'Este, S. 163 no. 4", "M\u00e9lodies hongroises d'apr\u00e8s Franz Schubert, S. 425: No. 1. Andante", "Via Crucis, S. 504a: Station II: J\u00e9sus est charg\u00e9 de sa croix", "Grande fantaisie de concert, S. 393/2", "Via Crucis, S. 53: Station VIII: Die Frauen von Jerusalem", "Ann\u00e9es de p\u00e8lerinage : Premi\u00e8re ann\u00e9e : Suisse, S. 160", "Consolation in Des-Dur", "Muttergottes-Str\u00e4usslein zum Mai-Monate, S. 316: Nr. 2. Die Schl\u00fcsselblumen", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 22 in E flat major \\\"Pester Carneval\\\"", "2 Cs\u00e1rd\u00e1s, S. 225: No. 2. Cs\u00e1rd\u00e1s obstin\u00e9", "Deux marches dans le genre hongrois, S. 693: No. 2 in B-flat minor", "Douze grandes \u00e9tudes, S. 137: No. 8 in C minor (Presto strepitoso)", "Der 18. Psalm", "Douze grandes \u00e9tudes, S. 137: No. 9 in A-flat major (Andantino)", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 4. Pens\u00e9e des morts", "Im Rhein, im sch\u00f6nen Strome, S. 272/1", "Recueillement, S. 204", "Feuille d'album, S.165", "Impromptu brillant sur des th\u00e8mes de Rossini et Spontini, op. 3, S. 150", "Schlummerlied mit Arabesken, S. 454", "Album-Leaf: Pr\u00e9lude omnitonique, S. 166e", "Historical Hungarian Portraits, S. 205: No. 4. Teleki Laszlo", "Le Rossignol, S. 250 no. 1", "Hungarian Rhapsody no. 2 in C-sharp minor, S. 244 no. 2 bis", "Einleitung und Schlu\u00dftakte zu Tausigs 3. Valse-Caprice, S. 571a", "Mazeppa", "Via Crucis, S. 53: Station VII: Jesus f\u00e4llt zum zweiten Mal", "Evocation \u00e0 la Chapelle Sixtine, S. 658", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 3. Hymne de la nuit", "Berceuse, S. 174/1", "Album-Leaf in E major, S. 167t", "Magyar kir\u00e1ly-dal, S. 544", "Vive Henri IV, S. 239", "Unstern! Sinistre, disastro, S. 208, R. 80", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 14 in A minor", "Trois \u00e9tudes de concert, S. 144: I. \\\"Il lamento\\\" in A-flat minor", "Historical Hungarian Portraits, S. 205: No. 2. E\u00f6tv\u00f6s Jozsef", "Two Hungarian Recruiting Dances, S. 241 \\\"Zum Andenken\\\": No. 2. Lass\u00fa magyar", "Morceau de salon (\u00e9tude de perfectionnement), S. 142", "Album-Leaf: Andantino in A-flat major, S. 166p", "Album-Leaf in A-flat major, S. 166l", "Pastorale, S. 160 no. 3", "Fantasie und Fuge \u00fcber den Choral Ad nos, ad salutarem undam, S. 259: II. Adagio", "Aus der Musik von Eduard Lassen zu Hebbels Nibelungen und Goethes Faust, S. 496: I. Nibelungen: 1. Hagen und Kriemhild", "K\u00fcnstlerfestzug zur Schillerfeier 1859, S. 520/2", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 9a. Allegretto", "Ungarische Nationalmelodien, S. 243: No. 3. Pr\u00e9lude. Allegretto", "Dante Symphony (arrangement for piano)", "Hungarian Coronation Mass, S. 11: IV. Credo", "Canzone, S. 162 no. 2", "J'ai perdu ma force et ma vie, S. 327", "R\u00e1k\u00f3czi-Marsch, S. 692d", "Toccata, S. 197a", "Klavierst\u00fcck in E major, S. 192 no. 1. Sehr langsam", "Au bord d'une source", "Im Rhein, im sch\u00f6nen Strome, S. 272/2", "\u00c0 la Chapelle Sixtine, S. 461/2", "Weihnachtslied \\\"Christus ist geboren\\\", S. 502", "Der Alpenj\u00e4ger", "Chor\u00e4le, S. 506a: No. 9. Vexilla regis", "Benedetto sia 'l giorno", "Sursum corda, S. 163 no. 7", "Das Grab und die Rose", "Die Lorelei, S. 532", "Sz\u00f3zat und Hymnus, S. 486", "L'Id\u00e9e fixe, S. 395", "Concerto path\u00e9tique, S. 365a", "Totentanz, S. 126: IV. Variation III", "Schuberts ungarische Melodien, S. 425a: No. 1. Andante", "Consolation in E major, S. 172 no. 1: Andante con moto", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 2 in C minor \\\"Langueur?\\\"", "Grand solo de concert, S. 175a", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 2. Ave Maria", "Mephisto Waltz no. 3, S. 215a", "Ihr Auge, S. 310/2", "Die Trauer-Gondel (La lugubre gondola), S. 134", "Entwurf der Ramann-Elegie, S. 196a", "Die Schl\u00fcsselblumen", "Valse-caprice no. 6, S.427/6a", "A holt k\u00f6lt\u0151 szerelme, S. 349", "Grande fantaisie sur la tyrolienne de l'op\u00e9ra La fianc\u00e9e de Auber, S. 385/1", "Der tugendhafte Schreiber", "Chanson du B\u00e9arn, S. 236 no. 2", "Grandes \u00e9tudes de Paganini, S. 141: No. 1. Tremolo in G minor", "La campanella", "R\u00e9miniscences des Puritains, S. 390/2", "Krakowiak, S. 166m/4", "Fantaisie romantique sur deux m\u00e9lodies suisses", "Den Schutz-Engeln, S. 162a/1", "Album Leaf in E-flat (Leipzig), S. 164b", "Zigeuner-Epos, S. 695b: No. 6 in G minor. Lento", "Melodie in Dorische Tonart, S. 701d", "Enfant, si j'\u00e9tais roi, S. 283/2", "Postludium, S. 162f", "Einsam bin ich, nicht allein, S. 453", "Glasgow fragment, S. 701f", "La notte, S. 516a", "Transcendental \u00c9tude No. 1", "Dem Andenken Pet\u0151fis", "Via Crucis, S. 504a: Station VI: Sancta Veronica", "Via Crucis, S. 53: Station X: Jesus wird entkleidet", "Pri\u00e8re et Berceuse de La muette de Portici d'Auber, S. 387: Introduction", "Trois morceaux suisses, S. 156a: No. 3. Ranz de ch\u00e8vres", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 1. \u00c9levez-vous, voix de mon \u00e2me", "Tre sonetti di Petrarca, S. 158: No. 2. Sonetto CIV. Pace non trovo", "Gaudeamus igitur, S. 240/2", "Apr\u00e9s une lecture du Dante, S. 161/7 (Ann\u00e9es de P\u00e9l\u00e8rinage II/7)", "P\u00e1sztor Lakodalmas, S. 405a", "Die Ideale", "Glanes de Woronince, S. 249: No. 2. M\u00e9lodies polonaises", "Festpolonaise, S. 619a", "Klavierst\u00fcck in F-sharp major, S. 192 no. 4. Andantino", "Muttergottes-Str\u00e4usslein zum Mai-Monate, S. 316: Nr. 1. Das Veilchen", "Cantico del Sol di San Francesco d'Assisi, S. 499", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 2 in E-flat major", "Marche fun\u00e8bre, S. 226a", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 7b. Lento", "Zwei St\u00fccke aus dem Oratorium Christus, S. 498c: No. 1. Einleitung - Pastorale", "Angiolin dal biondo crin (Arranged for solo piano)", "Zigeuner-Epos, S. 695b: No. 7 in A minor \\\"R\u00e1k\u00f3czi-Marsch\\\". Tempo di marcia", "Weihnachtsbaum, S. 185a: VI. R\u00e9veille-Matin (Wecker)", "O Roma nobilis, S. 546a", "Liebestr\u00e4ume no. 2, S. 541a \\\"Gestorben war ich\\\"", "Album d'un voyageur, S. 156: I. Impressions et po\u00e9sies: 2b. Au bord d'une source", "Apparitions, S. 155: No. 3. Fantaisie sur une valse de Fran\u00e7ois Schubert", "Weihnachtsbaum, S. 186: Nr. 8. Altes provenzalisches Weihnachtslied", "Drei St\u00fccke aus der Legende der heiligen Elisabeth, S. 498a: Nr. 3. Interludium", "Responsorien und Antiphonen, S. 30: IV. Sabbato sancto", "Valse oubli\u00e9e no. 3, S. 215 no. 3a", "Hungarian Rhapsody for Orchestra no. 4 in D minor, S. 359/4", "Via Crucis, S. 53: Station XIV: Jesus wird ins grab gelegt", "Transcendental \u00c9tude No. 3", "Zwei St\u00fccke aus dem Oratorium Christus, S. 498c: No. 2. Das Wunder", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 19 in F-sharp minor", "Carrousel de Madame Pelet-Narbonne, S. 214a", "Magyar dalok, S. 242: No. 7 in E-flat major", "Klavierst\u00fcck in A-flat major, S. 189a", "Von der Wiege bis zum Grabe, S. 107: III. Die Wiege des zukunftigen Lebens", "L\u00e4ndler, S. 211a", "Concerto for Piano and Orchestra no. 1 in E-flat major, S. 124: I. Allegro maestoso", "Angelus! Pri\u00e8re \u00e0 l'ange gardien, S. 162a/3", "Die Rose, S. 571", "Variationen \u00fcber das Motiv von Bach, S. 673", "Album d'un voyageur, S. 156: I. Impressions et po\u00e9sies: 2a. Le Lac de Wallenstadt", "Fantaisie sur des motifs de l'op\u00e9ra Lucrezia Borgia de G. Donizetti, S. 399a", "Transcendental \u00c9tude No. 4", "Ai No Yume", "2 Cs\u00e1rd\u00e1s, S. 225: No. 1. Cs\u00e1rd\u00e1s", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 5 in E major \\\"La Chasse\\\"", "Marche hongroise, S. 425/2c", "Die drei Zigeuner", "Grand galop chromatique", "Pens\u00e9es \\\"Nocturne,\\\" S. 168b", "Valse de concert sur deux motifs de Lucia et Parisina de Donizetti, S. 214 no. 3", "Es war ein K\u00f6nig in Thule, S. 531 no. 4", "Pr\u00e4ludium und Fuge \u00fcber den Namen BACH, S. 260: I. Pr\u00e4ludium", "Valse oubli\u00e9e no. 1, S. 215 no. 1", "Angiolin dal biondo crin", "Hexam\u00e9ron, S. 392: V. Variation III di bravura - Ritornello", "Fantasie und Fuge \u00fcber den Choral Ad nos, ad salutarem undam, S. 259: I. Fantasie", "Weihnachtsbaum, S. 186: Nr. 4. Adeste fideles", "Valse oubli\u00e9e no. 4, S. 215 no. 4", "Via Crucis, S. 504a: Station III: J\u00e9sus tombe pour la premi\u00e8re fois", "Von der Wiege bis zum Grabe, S. 512: I. Die Wiege", "Mazeppa, S. 138", "Die Zelle in Nonnenwerth, S. 534/3", "Muttergottes-Str\u00e4u\u00dflein zum Maimonate", "Valse oubli\u00e9e no. 3, S. 215 no. 3", "Drei St\u00fccke aus der Legende der heiligen Elisabeth, S. 498a: Nr. 1. Orchester Einleitung", "Alleluia, S. 183 no. 1", "Piano Concerto in E-flat major, S. 125a", "Mazurek, S. 166m/3", "Via Crucis, S. 504a: Station V: Simon le Cyr\u00e9n\u00e9en aide J\u00e9sus \u00e0 porter sa croix", "Mazurka brilliante, S. 221", "Via Crucis, S. 504a: Station XI: J\u00e9sus est attach\u00e9 \u00e0 la croix", "Marche des Tcherkesses de l'op\u00e9ra Rouslan et Loudmila de Glinka, S. 406/1", "Vexilla regis prodeunt, S. 185", "Waltz in E-flat major, S. 209a", "Zigeuner-Epos, S. 695b: No. 2 in C major. Andantino", "Konzert-Walzer \u00fcber zwei Themen aus Donizettis \\\"Lucia und Parisina\\\", S. 401", "Drei M\u00e4rsche von Franz Schubert, S. 426: No. 2. Grande Marche", "Mephisto Waltz no. 2, S. 111", "Le Rossignol, S. 249d", "Album-Leaf: Adagio \u2013 religioso in C major, S. 164l", "Weihnachtsbaum, S. 185a: XII. Polnisch", "Harmonie nach Rossini's Carit\u00e0, S. 701j", "Die Lore Ley: neue umgearbeitete Ausg.", "Phantasiest\u00fcck \u00fcber Motive aus Rienzi, S. 439", "Venezia e Napoli, S. 159: No. 1. Lento", "Resignazione, S. 187b", "Consolation in E major, S. 172 no. 5: Andantino", "B\u00fclow-Marsch, S. 230", "Comment, disaient-ils", "Album d'un voyageur, S. 156: III. Paraphrases: 11. Un soir dans les montagnes [de Knop] - Nocturne pastorale", "\u00c9tude en douze exercices, S. 136: VII. Allegretto con molta espressione", "Via Crucis, S. 53: Station XIII: Jesus wird vom Kreuz genommen", "O lieb, so lang du lieben kannst, S. 298/2", "Il penseroso, S. 161 no. 2", "Isoldens Liebestod, S. 447", "Album-Leaf: Fugue chromatique. Allegro in G minor, S. 167j", "Historische ungarische Bildnisse, S. 205a: No. 2. De\u00e1k Ferenc", "Miserere du Trovatore, S. 433", "Orpheus, S. 511b", "Douze grandes \u00e9tudes, S. 137: No. 6 in G minor (Largo patetico)", "O du mein holder Abendstern: Recitativ und Romanze aus Wagners T\u00e4nnhauser", "St. Stanislaus fragment, S. 688a", "Angelus!, S. 162a/2", "Variations de bravoure sur des th\u00e8mes de Paganini, S. 700/2", "Venezia e Napoli, S. 159: No. 2. Allegro", "Six poesies. Buch der Lieder", "Hexam\u00e9ron, S. 365b: VIIIb. Coda", "Fantasy and Fugue on the Theme B-A-C-H", "Petite Valse favorite, S. 212a", "R\u00e1k\u00f3czi-Marsch, S. 244b", "Hexam\u00e9ron, S. 392: VII. Variation V. Vivo e brillante - Fuocoso molto energico - Lento quasi recitativo", "Eine Symphonie zu Dantes Divina Commedia, S. 109: I. Inferno", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 5. Miserere", "Seconda mazurka di Tirindelli, S. 573a", "Geharnischte Lieder, S. 511: No. 3. Es rufet Gott uns mahnend", "Marie-Po\u00e8me, S. 701b", "Hungarian Rhapsody for Orchestra no. 1 in F minor, S. 359/1", "O sacrum convivium, S. 674a", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 5 in G-flat major", "Album-Leaf in E major (Leipzig), S. 163d", "Fantasie \u00fcber englische Themen, S. 694", "Excelsior!, S. 500", "An Frau Minne", "Elegy no. 1, S. 130b", "Halloh!, Jagdchor und Steyrer aus der Oper Tony, S. 404", "Tre sonetti di Petrarca", "Valse-Impromptu", "Via Crucis, S. 53: Station XII: Jesus stirbt am Kreuze", "Album-Leaf: Magyar II in B-flat minor, S. 164e/2", "Album-Leaf in E-flat major, S. 167k", "Anfangs wollt ich fast verzagen", "Nocturne, S. 191/1", "Album-Leaf: Magyar in D-flat major, S. 164e/3", "Totentanz, S. 126: VI. Variation V", "Via Crucis, S. 53: Station XI: Jesus wird ans Kreuz geschlagen", "Via Crucis, S. 504a: Station I: J\u00e9sus est condamn\u00e9 \u00e0 mort", "Piano Concerto No. 2", "Weihnachtsbaum, S. 185a: I. Psallite - Altes Weihnachtslied", "Tre sonetti del Petrarca", "\u00c9tude en douze exercices, S. 136: V. Moderato", "R\u00e9miniscences de Don Juan", "Album d'un voyageur, S. 156: III. Paraphrases: 12. Ranz de chevres [de F. Huber] - Allegro finale", "R\u00e9miniscences de Don Juan, S. 656", "Valse m\u00e9lancolique, S. 210", "Ann\u00e9es de p\u00e8lerinage : Deuxi\u00e8me ann\u00e9e : Italie, S. 161", "Der du von dem Himmel bist, S. 279/1", "R\u00e9miniscences de Boccanegra, S. 438", "Sarabande and Chaconne from Handel's Almira", "Tyrolean Melody, S. 385a", "Marche fun\u00e8bre de Dom S\u00e9bastien, S. 402", "Feierlicher Marsch zum heiligen Gral aus Parsifal, S. 450", "Weihnachtsbaum, S. 185a: X. [Ehemals]", "Responsorien und Antiphonen, S. 30: III. Feria VI in Parasceve", "Nuages gris", "Totentanz, S. 126: VIII. Variation VI", "Weihnachtsbaum, S. 185a: XI. Ungarisch", "R\u00e9miniscences de Norma, S. 394", "Douze grandes \u00e9tudes, S. 137: No. 5 in B-flat major (Equalmente)", "Grande \u00e9tude de perfectionnement", "La Lugubre gondola I, S. 200/1", "Ungarischer Marsch zur Kr\u00f6nungsfeier in Ofen-Pest am 8. Juni 1867, S. 523", "Magyar rapsz\u00f3dia, S. 244/16/1", "Magyar dalok, S. 242: No. 11 in B-flat major", "Ungarische National-Melodien (Im leichten Style bearbeitet), S. 243bis: No. 2 in C major", "Hungarian Rag", "Five Hungarian Folksongs, S. 245: No. 4. Kiss\u00e9 \u00e9l\u00e9nken. Vivace", "Siegesmarsch, S. 233a", "Einzug der G\u00e4ste auf der Wartburg, S. 445 no. 1", "Prol\u00e9gom\u00e8nes \u00e0 la Divina Commedia, S. 158b", "Il m'aimait tant, S. 533", "Via Crucis, S. 53: Station IX: Jesus f\u00e4llt zum dritten Mal", "Seconde marche hongroise, S. 232", "Grand galop chromatique, S. 219 bis", "Via Crucis, S. 53: Station II: Jesus tr\u00e4gt sein Kreuz", "Tr\u00fcbe Wolken (Nuages gris), S. 199, R. 78", "Die Zelle in Nonnenwerth, S. 534/1", "Transcendental \u00c9tude No. 7", "Fantasie und Fuge \u00fcber den Choral Ad nos, ad salutarem undam, S. 259: III. Fugue", "\u00c9tude en douze exercices, S. 136: XII. Allegro non troppo", "Soir\u00e9es de Vienne, S. 427: No. 9 in A-flat major. Preludio a capriccio", "Hungarian Rhapsody no. 18 in F-sharp minor, S. 244 no. 18", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 7. Hymne de l'enfant \u00e0 son r\u00e9veil", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 8c. Allegro moderato", "Bist du", "Mephisto Waltzes", "Hexam\u00e9ron, S. 365b: IV. Variation II. Moderato", "Hexam\u00e9ron, S. 365b: I. Introduction. Extr\u00eamement lent", "Zigeuner-Epos, S. 695b: No. 9 in E-flat major. Andante cantabile quasi adagio", "Elsas Brautzug zum M\u00fcnster, S. 445 no. 2", "Aus Lohengrin, S. 446: No. 2. Elsas Traum", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 3. B\u00e9n\u00e9diction de Dieu dans la solitude", "Transcendental \u00c9tude No. 10", "Weihnachtsbaum, S. 185a: VII. Schlummerlied", "Soir\u00e9es de Vienne, S. 427: No. 1 in A-flat major. Allegretto malinconico", "Weihnachtsbaum, S. 186: Nr. 3. Die Hirten an der Krippe", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 7a. Allegro", "Totentanz, S. 126: II. Variation I", "Comment, disaient-ils, S. 535", "In Liebeslust, S. 318", "Album-Leaf: Freudvoll und leidvoll, S. 166n", "Album-Leaf: Schlusschor des entfesselten Prometheus. Andante solenne in D-flat major, S. 167q", "Weimars Volkslied", "Eglogue, S. 160 no. 7", "Hungarian Coronation Mass, S. 11: V. Offertorium", "Es rauschen die Winde, S. 294/1", "Via Crucis, S. 504a: Station XII: J\u00e9sus meurt sur la croix", "Hungarian Coronation Mass, S. 11: III. Graduale (Psalm 116)", "Wir sind nicht Mumien", "Totentanz, S. 126/1", "Der traurige M\u00f6nch, S. 348", "Douze grandes \u00e9tudes, S. 137: No. 1 in C major (Presto)", "Fantasy and Fugue on the chorale 'Ad nos ad salutarem undam'", "Concerto for Piano No. 1 in E flat major, S 124: III. Allegretto vivace", "Tarantelle di bravura d\u2019apr\u00e8s la tarantelle de La muette de Portici, S. 386/2", "Sunt lacrymae rerum \u2013 in ungarischen Weise, S. 162d", "Don Sanche", "Album Leaf in F-sharp minor, S. 163a/1", "Air cosaque, S. 249c", "Valse m\u00e9lancolique, S. 214 no. 2", "Von der Wiege bis zum Grabe, S. 512: III. Die Wiege des zukunftigen Lebens", "Tre sonetti di Petrarca, S. 270: II. Benedetto sia 'l giorno", "Der Hirt", "Concerto path\u00e9tique", "Hungarian Rhapsody no. 14 in F minor, S. 244 no. 14", "Via Crucis, S. 53: Station III: Jesus f\u00e4llt zum ersten Mal", "Soir\u00e9es de Vienne, S. 427: No. 2 in A-flat major. Poco allegro", "Glanes de Woronince", "Einzug der G\u00e4ste auf der Wartburg, S. 445 no. 1/c", "Legend no. 2: \\\"St. Francis Walking on the Waves\\\"", "H\u00e9ro\u00efde fun\u00e8bre", "Salve Polonia, S. 518", "Pilgerchor aus Tannh\u00e4user, S. 443/2", "Fantasie und Fuge \u00fcber das Thema B-A-C-H, S. 529/2", "Illustrations du Proph\u00e8te, S. 414: No. 1. Pri\u00e8re - Hymne triomphal - Marche du sacre", "Allegro di bravura, op. 4 no. 1, S. 151", "Die Zelle in Nonnenwerth, S. 382", "Schwanengesang und Marsch aus Hunyadi L\u00e1szl\u00f3, S. 405", "La cloche sonne, S. 238", "Piano Concerto No. 3", "Grandes \u00e9tudes de Paganini, S. 141: No. 2. Octave in E-flat major", "Von der Wiege bis zum Grabe, S. 107", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 8. Miserere d'apr\u00e8s Palestrina", "Hungarian Rhapsody No. 2", "Chor\u00e4le, S. 506a: No. 1. Crux ave benedicta", "Ann\u00e9es de p\u00e8lerinage", "Die Macht der Musik", "Elegy no. 1, S. 196", "Am stillen Herd, S. 448", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 4 in E major", "Huit variations, op. 1, S. 148", "Leyer und Schwert, S. 452: II. Schwertlied", "Trois Chansons, S. 510a: No. 2. Avant la bataille", "Lieder aus Schillers Wilhelm Tell, S. 292a: Nr. 1. Der Fischerknabe", "Valse de concert, S. 430", "Sz\u00f3zat und Hymnus, S. 353", "La lugubre gondola, S. 200/2", "Wilde Jagd: Scherzo, S. 176a", "Aus der Musik von Eduard Lassen zu Hebbels Nibelungen und Goethes Faust, S. 496: II. Faust: 1. Osterhymne", "Weihnachtsbaum, S. 186: Nr. 2. O heilige Nacht!", "Magyar dalok, S. 242: No. 3 in D-flat major", "Gondoliera, S. 162 no. 1", "Ein Fichtenbaum steht einsam, S. 309", "Glanes de Woronince, S. 249: No. 1. Ballade ukraine (Dumka)", "Klavierst\u00fcck in A-flat major, S. 189", "Valse \u00e0 capriccio sur deux motifs de Lucia et Parisina de Donizetti, S. 401", "R\u00e9miniscences de Lucia di Lammermoor, S. 397", "K\u00fcnstlerfestzug zur Schillerfeier 1859, S. 520/1", "Album-Leaf, S. 167c", "Magnificat, S. 182a", "Weihnachtsbaum, S. 185a: IV. Adeste fideles (gleichsam als Marsch der heiligen drei K\u00f6nige)", "Korrekturblatt, S. 701k", "R\u00e9miniscences des Huguenots, S. 412/2", "Valse-caprice No. 9 (Sehnsuchtswalzer), S. 427/9", "Introduction et Polonaise de l'op\u00e9ra I puritani, S. 391", "Hamlet, S. 104", "Hungarian Coronation Mass, S. 11: VII. Benedictus", "Via Crucis, S. 53: Station VI: Sancta Veronica", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 4 in D-flat major \\\"Derni\u00e8re illusion\\\"", "I' vidi in terra angelici costumi", "Apparitions, S. 155: No. 1. Senza lentezza quasi allegretto", "Aux anges gardiens, S. 162a/1 bis", "Vergiftet sind mein Lieder, S. 289/3", "Hungarian Rhapsody for Orchestra no. 5 in E minor, S. 359/5", "Eine Faust-Symphonie, S. 108: III. Mephistopheles. Allegro vivace, ironico", "Polnisch, S. 701g", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 15 in D minor", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 6. Hymne de l'enfant \u00e0 son r\u00e9veil", "R\u00e9miniscences de Lucrezia Borgia, S. 400: I. Trio du seconde Acte", "Weihnachtsbaum, S. 185a: II. O heilige Nacht", "Hungarian Rhapsody No. 15", "Tre sonetti di Petrarca, S. 158: No. 3. Sonetto CXXIII. I' vidi in terra angelici costumi", "Ouvert\u00fcre zu Tannh\u00e4user von Richard Wagner", "Deux Polonaises de l'oratorio St. Stanislas, S. 519: Polonaise I", "Hungarian Rhapsody for Orchestra no. 6 in D-flat major \\\"Carnival in Pest\\\", S. 359/6", "Festkl\u00e4nge, S. 101", "La lugubre gondola, S. 199a/1", "Grande paraphrase de la Marche de Donizetti pour le Sultan Abdul-Medjid Khan, S. 403", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 9. La Lampe du temple (Andante lagrimoso)", "Les Morts, S. 516", "Faust Symphony", "Canzonetta del Salvator Rosa, S. 161 no. 3", "Totentanz", "Sonata in B minor", "Und sprich, S. 329", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 8. Prose des morts - De profundis", "Grosse Konzertfantasie \u00fcber spanische Weisen, S. 253", "Die Perle", "L'Hymne du Pape, S. 530", "Jeanne d'Arc au b\u00fbcher, S. 293/3", "Klavierst\u00fcck in A-flat major, S. 192 no. 2. Lento assai", "Hungarian Rhapsodies", "Venezia e Napoli, S. 159", "C\u00e9l\u00e8bre m\u00e9lodie hongroise, S. 243a", "Album-Leaf: Exeter Preludio, S. 164c", "B\u00e9n\u00e9diction et serment de Benvenuto Cellini, S. 396", "Salve Regina, S. 669 no. 1", "Sardanapale", "Album-Leaf: Andante religiosamente in G major, S. 166j", "Konzertparaphrase \u00fcber Mendelssohns Hochzeitsmarsch und Elfenreigen aus der Musik zu Shakespeares Sommernachtstraum, S. 410", "Liebestr\u00e4ume", "Eine Faust-Symphonie, S. 108: II. Gretchen. Andante soave", "Cadenza to the first movement of Beethoven's Piano Concerto no. 3, S. 389a", "Sonetto 47 del Petrarca, S. 161 no. 4", "Grande Fantaisie sur des th\u00e8mes de Paganini, S. 700/1", "Via Crucis, S. 504a: Station VIII: Les Femmes de J\u00e9rusalem", "Magyar dalok, S. 242: No. 6 in G minor", "Chor\u00e4le, S. 506a: No. 5. Nun ruhen all W\u00e4lder", "Grande Fantaisie Symphonique on themes from Berlioz's \\\"L\u00e9lio\\\" for Piano and Orchestra, S. 120", "Douze grandes \u00e9tudes, S. 137: No. 3 in F major (Poco adagio)", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 20 in G minor \\\"Rumanian Rhapsody\\\"", "Album-Leaf: Aus den Mephisto-Walzer. Episode aus Lenaus Faust - Der Tanz in der Dorfschenke, S. 167m", "Illustrations du Proph\u00e8te, S. 414: No. 2: Les Patineurs: Scherzo", "Weinen, Klagen, Sorgen, Zagen, Pr\u00e4ludium nach Johann Sebastian Bach, S. 179", "Am Rhein", "Schlummerlied, S. 186/7a", "Magyar dalok, S. 242: No. 5 in D-flat major", "Einleitung und Coda zu Smetanas Polka, S. 570a", "Ungarische National-Melodien (Im leichten Style bearbeitet), S. 243bis: No. 3 in B-flat major", "Historical Hungarian Portraits, S. 205: No. 6. Pet\u00f6fi Sandor", "Missa solennis zur Einweihung der Basilika in Gran, S. 9 \\\"Graner Messe\\\": III. Credo. Antande maestoso, risoluto", "Zigeuner-Epos, S. 695b: No. 5 in D flat major. Tempo giusto", "Eine Faust-Symphonie, S. 108: I. Faust. Lento assai", "Coro di festa e marcia funebre de Don Carlos, S. 435", "Weihnachtsbaum, S. 186: Nr. 10. Ehemals", "Rondeau fantastique sur un th\u00e8me espagnol \\\"La Contrabandista\\\", S.252", "Litanies de Marie, S. 171e", "Grande Fantaisie sur des motifs de Soir\u00e9es musicales, S. 422/2", "Historische ungarische Bildnisse, S. 205a: No. 4. E\u00f6tv\u00f6s J\u00f3zsef", "Zwei Orchesters\u00e4tze aus dem Oratorium Christus, S. 498b: Nr. 1. Hirtengesang an der Krippe", "Consolation in E major, S. 171a no. 6: Allegretto", "Prozinsky Fragment for piano, S. 701v", "Es muss ein Wunderbares sein, S. 314", "Totentanz, S. 126: V. Variation IV", "Cavatine de Robert le Diable, S. 412a", "R.W.-Venezia, S. 201", "Hungarian Rhapsody no. 3 in B-flat major, S. 244 no. 3", "Liebestraum Es-Dur \\\"Seliger Tod\\\", S. 541 Nr. 2", "Grande fantaisie dramatique sur des th\u00e8mes de l'op\u00e9ra Les Huguenots, S. 412/3, R. 211 \\\"R\u00e9miniscences des Huguenots\\\"", "\u00dcber allen Gipfeln ist Ruh, S. 306", "Requiem f\u00fcr die Orgel, S. 266: VII. Postludium", "Trois morceaux suisses, S. 156a: No. 2. Un soir dans la montagne", "Hungarian Rhapsody for Orchestra no. 2 in C minor, S. 359/2", "Valse-Impromptu, S. 213a", "Der traurige M\u00f6nch", "Album-Leaf: Aus dem Purgatorio des Dante Sinfonie. Lamentoso in B minor, S. 166r/2", "Album-Leaf: Serenade, S. 166g", "Tu es Petrus, S. 664", "Huldigungsmarsch, S. 228ii", "Via Crucis, S. 504a: Station XIII: J\u00e9sus est d\u00e9pos\u00e9 de la croix", "Grandes \u00e9tudes de Paganini, S. 141: No. 4. Arpeggio in E major", "Von der Wiege bis zum Grabe, S. 512: II. Der Kampf ums Dasein", "Romance oubli\u00e9e, S. 132a", "Sunt lacrymae rerum, S. 162c", "Valse de l'op\u00e9ra \\\"Faust\\\", S. 407", "Grand solo de concert, S. 365", "Album-Leaf (Premi\u00e8re Consolation), S. 171b", "Marche fun\u00e8bre, S. 163 no. 6", "In domum Domini ibimus, S. 505", "R\u00e1k\u00f3czi March, S. 608", "Wie entgehn der Gefahr?", "La Mandragore, S. 698", "Ebony Rhapsody", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 7c. Allegro pastorale", "Fantasie und Fuge \u00fcber den Choral Ad nos, ad salutarem undam, S. 624: II. Adagio", "Wieder m\u00f6cht' ich dir begegnen", "Weihnachtsbaum, S. 186: Nr. 12. Polnisch", "Album-Leaf: Agitato in G major, S. 167l", "Drei M\u00e4rsche von Franz Schubert, S. 426: No. 1. Trauermarsch (Grande Marche fun\u00e8bre)", "Soir\u00e9es de Vienne, S. 427: No. 7 in A major. Allegro spiritoso", "g.1235wfk4", "Chapelle de Guillaume Tell, S. 160 no. 1", "Walther von der Vogelweide", "R\u00e1koczy March in A minor", "Ave maris stella, S. 669 no. 2", "Hungarian Rhapsody no. 11 in A minor, S. 244 no. 11", "Symphonic poems", "Magyar dalok, S. 242: No. 2 in C major", "Deux l\u00e9gendes, S. 175: St. Fran\u00e7ois d'Assise: la pr\u00e9dication aux oiseaux", "Grande \u00e9tude d'apr\u00e8s Paganini no. 2 in E-flat major, BV B 70", "Mephisto Waltz no. 3, S. 216", "Aux cypr\u00e8s de la Villa d'Este II : Thr\u00e9nodie, S. 163 no. 3", "Leyer und Schwert, S. 452: I. [Introduction]", "Cantico del sol di San Francisco d'Assisi", "Weihnachtsbaum, S. 186: Nr. 11. Ungarisch", "Wer nie sein Brot mit Tr\u00e4nen a\u00df", "Den Cypressen der Villa d'Este, S. 162b", "Vall\u00e9e d'Obermann, S. 160 no. 6", "Concerto sans orchestre, S. 524a", "Au lac de Wallenstadt, S. 160 no. 2", "Zigeuner-Epos, S. 695b: No. 11 in A minor. Lento", "Album d'un voyageur, S. 156: III. Paraphrases: 10. Ranz de vaches [de F. Huber] - Aufzug auf die Alp - Improvisata", "Operatic aria, S. 701h/1", "Ungarische National-Melodien (Im leichten Style bearbeitet), S. 243bis: No. 1 in D major", "Soir\u00e9es de Vienne, S. 427: No. 6 in A minor. Allegro con strepito", "\u00c9l\u00e9gie sur des motifs du Prince Louis Ferdinand de Prusse, S. 168/2", "Ballade No. 1", "Five Hungarian Folksongs, S. 245: No. 5. B\u00fasongva. Lento", "Album-Leaf: Andantino in E-flat, S. 163a", "Hexam\u00e9ron, S. 392: IX. Finale. Molto vivace quasi prestissimo", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 1. Invocation", "Ave Maria in D major, S. 504/1", "Feuilles d\u2019album, S. 165", "Lenore, S. 346", "Fantasie sur l'\u00f3pera hongroise Sz\u00e9p Ilonka, S. 417", "R\u00e1k\u00f3czy March, S. 117", "Ann\u00e9es de p\u00e8lerinage : Troisi\u00e8me ann\u00e9e, S. 163", "Sposalizio, S. 157a", "Historische ungarische Bildnisse, S. 205a: No. 6. Pet\u00f6fi S\u00e1ndor", "Epithalam, S. 526", "Album-Leaf: Moderato in D-flat major, S. 164k", "Dumka, S. 249b", "Festkantate zur Enth\u00fcllung des Beethoven-Denkmals in Bonn, S. 67: I. Maestoso - Quasi allegretto", "Pr\u00e4ludium und Fuge \u00fcber den Namen BACH, S. 260: II. Fuge", "Album-Leaf, S. 167d", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 8 in D-flat major \\\"M. K.\\\"", "Klavierkonzert No. 1 Es-dur: III. Allegro marciale animato", "Ungarische National-Melodie, S. 242/13 bis", "Drei M\u00e4rsche von Franz Schubert, S. 426: No. 3. Grande Marche caracteristique", "Grande paraphrase de la Marche de Donizetti pour le Sultan Abdul-Medjid Khan, S. 403 bis", "Wolfram von Eschenbach", "Album-Leaf: Quasi mazurek in C major, S. 163e", "Transcendental \u00c9tude No. 8", "Fantasie \u00fcber zwei Motive aus W. A. Mozarts Die Hochzeit des Figaro", "La perla, S. 326/2", "Die Zelle in Nonnenwerth, S. 534/2 bis", "Hungarian Rhapsody no. 5 in E minor, S. 244 no. 5 \\\"H\u00e9ro\u00efde \u00e9l\u00e9giaque\\\"", "Deux l\u00e9gendes, S. 175: II bis. St. Fran\u00e7ois de Paule marchant sur les flots", "Les Pr\u00e9ludes, S. 511a", "Es war ein K\u00f6nig in Thule, S. 278/2", "Andante sensibilissimo, S. 701c", "Apparitions, S. 155: No. 2. Vivamente", "Album-Leaf: Tempo di marcia in E-flat major, S. 167o", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 1 in E-flat major", "Walhall aus Der Ring des Nibelungen, S. 449", "Harmonies po\u00e9tiques et religieuses, S. 154", "Hexam\u00e9ron, S. 392: I. Introduction", "19 Hungarian Rhapsodies for Piano, S 244 No. 15 \\\"R\u00e1k\u00f3czy March\\\"", "Consolation in E major, S. 171a no. 5: Andantino", "Weihnachtsbaum, S. 186: Nr. 5. Scherzoso", "Album-Leaf in A-flat, S. 166c", "Grosses Konzertsolo, S. 176", "Orpheus", "Chor\u00e4le, S. 506a: No. 4. Nun danket alle Gott", "Mal\u00e9diction, S. 121", "Die Zelle in Nonnenwerth, S. 534/2", "Slavimo Slavno Slaveni!", "Variationen \u00fcber das Motiv von Bach, S. 180", "Album Leaf in A major, S. 166k", "Oh! quand je dors, S. 536", "Christmas Again", "\u00c9tude en douze exercices, S. 136: IV. Allegro grazioso", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 13 in A minor \\\"R\u00e1k\u00f3czi-Marsch\\\"", "Aus Lohengrin, S. 446: No. 3. Lohengrins Verweis an Elsa", "Faribolo pastour, S. 236 no. 1", "God Save the Queen, S. 235", "In festo transfigurationis Domini nostri Jesu Christi, S. 188", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 17 in A minor", "Chor\u00e4le, S. 506a: No. 8. O Traurigkeit", "Douze grandes \u00e9tudes, S. 137: No. 12 in B-flat minor (Andantino)", "Stabat mater, S. 172b", "Weihnachtsbaum, S. 186: Nr. 6. Carillon", "Paraphrase de concert sur Ernani I, S. 431a", "Die Lorelei, S. 531 no. 1", "Trauervorspiel und Trauermarsch, S. 206: No. 2. Trauermarsch", "Weihnachtsbaum, S. 186: Nr. 7. Schlummerlied", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 12 in E minor \\\"Hero\u00efde \u00e9l\u00e9giaque\\\"", "Responsorien und Antiphonen, S. 30: II. Feria V in coena Domini", "Dante Symphony", "Canzonetta del Salvator Rosa, S. 157c", "Trois morceaux suisses, S. 156a: No. 1. Ranz de vaches", "Illustrations de l'op\u00e9ra L'Africaine, S. 415: No. 2. Marche indienne", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 11. (B\u00e9n\u00e9diction) (Pr\u00e9lude)", "Sonetto 104 del Petrarca", "Introitus, S. 268 no. 1", "Album-Leaf in G minor, S. 166l/2", "La lugubre gondola", "2 Polonaises, S. 223: no. 1 \\\"Polonaise m\u00e9lancolique\\\" in C minor", "Der n\u00e4chtliche Zug, S. 513a", "Romance, S. 169", "Album-Leaf: Langsam in C-sharp minor, S. 166o", "Fantasie \u00fcber Motive aus Beethovens Ruinen von Athen, S. 122", "Hexam\u00e9ron, S. 392: IV. Variation II. Moderato", "R\u00e9miniscences des Puritains, S. 390/1", "Sospiri!, S. 192 no. 5. Andante", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 6 in A major \\\"Attente\\\"", "\u00c9tude en douze exercices, S. 136: II. Allegro con molto", "Rhapsodie espagnole", "Scherzo und Marsch, S. 177", "Le Mal du pays, S. 160 no. 8", "Sonetto 123 del Petrarca, S. 161 no. 6", "Aus der Ungarischen Kr\u00f6nungsmesse, S. 501: I. Benedictus", "R\u00e1k\u00f3czi-Marsch, S. 244a", "R\u00e9miniscences des Huguenots, S. 412/1", "Klavierst\u00fcck in D-flat major, S. 189b", "Aux cypr\u00e8s de la Villa d'Este I : Thr\u00e9nodie, S. 163 no. 2", "Via Crucis, S. 504a: Station IX: J\u00e9sus tombe une troisi\u00e8me fois", "Impromptu in F-sharp major, S. 191/2, R. 59 \\\"Nocturne\\\"", "Transcendental \u00c9tude No. 12", "Transcendental \u00c9tude No. 5", "Des toten Dichters Liebe", "M\u00e9lodies hongroises d'apr\u00e8s Franz Schubert, S. 425: No. 2. Marche hongroise", "Hungarian Coronation Mass, S. 11: VIII. Agnus Dei", "Album-Leaf: Magyar, S. 164e", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 4. Litanies de Marie", "Danza sacra e duetto finale d\u2019Aida, S. 436", "Le Triomphe fun\u00e8bre du Tasse, S. 517", "Fantaisie sur des motifs favoris de l'op\u00e9ra Somnambula de Bellini, S. 393/1", "Grandes \u00e9tudes de Paganini, S. 141: No. 5. La Chasse in E major", "Album d'un voyageur, S. 156: I. Impressions et po\u00e9sies: 6. Psaume", "Adagio non troppo, S. 151a", "Der blinde S\u00e4nger, S. 546", "Ihr Glocken von Marling, S. 328", "Comment, disaient-ils, S. 276/2", "Hungarian Rhapsody No. 19", "Album-Leaf: Vivace ma non troppo in D-flat major, S. 167g", "Hungarian Rhapsody no. 4 in E-flat major, S. 244 no. 4", "Chor\u00e4le, S. 506a: No. 3. Meine Seele erhebt den Herrn (Der Kirchensegen, Psalm 67)", "Klavierst\u00fcck in F-sharp major, S. 192 no. 3. Sehr langsam", "Des Tages laute Stimmen schweigen", "Pace non trovo", "Marche h\u00e9ro\u00efque, S. 510", "Weihnachtsbaum, S. 185a: IX. [Abendglocken]", "Pri\u00e8re et Berceuse de La muette de Portici d'Auber, S. 387: Pri\u00e8re", "R\u00e9miniscences de Lucrezia Borgia, S. 400: II. Chanson \u00e0 boire (Orgie). Duo-finale", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 9c. Andantino con molto sentimento", "Album-Leaf in A minor (R\u00e1k\u00f3czi-Marsch), S. 164f", "Sposalizio", "Ungarischer Geschwindmarsch, S. 233", "Kling leise, mein Lied, S. 301/1", "Via Crucis, S. 504a: Vexilla regis", "Gaudeamus igitur, S. 509", "Sancta Dorothea, S. 187", "Magyar dalok, S. 242: No. 4 in C-sharp major", "Album-Leaf, S. 167e", "Schlummerlied im Grabe, S. 195a", "Winzerchor aus den entfesselten Prometheus, S. 692e", "\u00c9tude en douze exercices, S. 136: VI. Molto agitato", "Album-Leaf: Andantino in E major, S. 163d/ii", "Hungarian Rhapsody no. 18 in F-sharp minor, S. 244 no. 18/1", "Mosonyis Grabgeleit, S. 194", "Concerto for Piano and Orchestra no. 1 in E-flat major, S. 124: II. Quasi adagio", "Schuberts ungarische Melodien, S. 425a: No. 3. Allegretto", "Consolation No. 3", "L\u00e9gende No. 1: St Fran\u00e7ois d'Assise", "Vom Fels zum Meer!, S. 229", "Lieder aus Schillers Wilhelm Tell, S. 292a: Nr. 3. Der Alpenj\u00e4ger", "Valse de bravoure, S. 214 no. 1", "Hungarian Coronation Mass, S. 11: VI. Sanctus", "Grosse Concert-Fantasie aus der Oper Sonnambula, S. 393/3", "Geharnischte Lieder, S. 511: No. 1. Vor der Schlacht", "Weimars Volkslied, S. 542/2", "Christus", "Deux \u00e9pisodes d'apres le Faust de Lenau, S. 110: II. Der Tanz in der Dorfschenke (Erster Mephisto-Walzer)", "Two Concert \u00c9tudes", "Album d'un voyageur, S. 156: I. Impressions et po\u00e9sies: 4. Vall\u00e9e d'Obermann", "Five Hungarian Folksongs, S. 245: No. 2. M\u00e9rsek\u00e9lve. Allegretto", "Grandes \u00e9tudes de Paganini, S. 141: No. 6. Variations in A minor", "Wie singt die Lerche sch\u00f6n", "Hungarian Rhapsody no. 1 in C-sharp minor, S. 244 no. 1", "Gebet, S. 265", "Hunnenschlacht", "Harmonies po\u00e9tiques et religieuses", "Valse-Impromptu, S. 213 bis", "Three Concert \u00c9tudes", "Totentanz, S. 126: IX. Cadenza", "Canzone napolitana, S. 248a", "Album-Leaf: Introduction to the Grande \u00c9tude de Paganini no. 6, S. 141/6bis", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 1a in G minor", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 10. Cantique d'amour", "Hexam\u00e9ron, S. 392: VIII. Variation VI. Largo - Coda", "Pri\u00e8re et Berceuse de La muette de Portici d'Auber, S. 387: Berceuse", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 2. Hymne du matin", "Leyer und Schwert, S. 452: III. Gebet (vor der Schlacht)", "Via Crucis, S. 53: Station V: Simon von Kyrene hilft Jesus das Kreuz zu tragen", "Orage, S. 160 no. 5", "Ave Maria II, S. 38", "Marche militaire, S. 426a", "Weihnachtsbaum, S. 185a: V. Scherzoso", "Album-Leaf: Braunschweig Preludio, S. 166f", "Eine Faust-Symphonie, S. 108: IV. Chorus mysticus", "Marche hongroise, S. 425/2e", "Album Leaf in E major (Vienna), S. 164a", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 5. Pater noster", "Hexam\u00e9ron, S. 365b: IX. Finale. Molto vivace quasi prestissimo", "Salve Regina", "Venezia e Napoli, S. 159: No. 4. Tarantelles napolitaines", "De Profundis, S. 121a", "Ich liebe dich, S. 542a", "Ungarischer Sturmmarsch, S. 524", "Marche hongroise, S. 425/2b", "Tarantella, S. 162 no. 3", "Responsorien und Antiphonen, S. 30: V. In officio defunctorum", "La Tombe et la rose, S. 539", "M\u00e9lodie polonaise, S. 249a", "Gretchen aus Faust-Symphonie, S. 513", "M\u00e9lodies hongroises d'apr\u00e8s Franz Schubert, S. 425: No. 3. Allegretto", "Zwei Orchesters\u00e4tze aus dem Oratorium Christus, S. 498b: Nr. 2. Die heiligen drei K\u00f6nige \u2013 Marsch", "Freudvoll und Leidvoll, S. 280/2", "Schwebe, schwebe, blaues Auge, S. 305/2", "Album-Leaf, S. 167h", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 9b. Allegretto", "Album d'un voyageur, S. 156: I. Impressions et po\u00e9sies: 1. Lyon", "Der du vom Himmel bist, S. 531 no. 5", "Venezia e Napoli, S. 159: No. 3. Andante placido", "Hungarian Rhapsody no. 13 in A minor, S. 244 no. 13", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 6. Pater noster, d'apr\u00e8s la Psalmodie de l'\u00c9glise", "Album-Leaf: Allegretto in A major, S. 167n", "Schuberts ungarische Melodien, S. 425a: No. 2. Marcia (Marche hongroise)", "Li marinari", "Angiolin, S. 531 no. 6", "Wiegenlied, S. 198", "Fantasie und Fuge \u00fcber den Choral Ad nos, ad salutarem undam, S. 624: I. Fantasy", "Hungarian Rhapsody no. 17 in D minor, S. 244 no. 17", "Hyr\u0107, S. 166m/2", "Einleitung und Coda zu Rubinsteins \u00c9tude in C-Dur, S. 554a", "Prometheus", "Tasso, Lamento e Trionfo", "Album Leaf in E major, S. 166a", "Ave maris stella, S. 506", "Hexam\u00e9ron, S. 365b: V. Variation III di bravura - Ritornello", "Ave Maria in D-flat major, S. 504/2", "Festpolonaise, S. 230a", "Zigeuner-Epos, S. 695b: No. 3 in D-flat major. Sehr langsam", "Album-Leaf in C major, S. 167s \\\"Lyon\\\"", "\u0421\u043b\u0463\u043f\u043e\u0439, S. 350", "Consolation in D-flat major, S. 171a no. 4: Quasi adagio", "Ave Maria IV in G major, S. 545", "Einleitung und Coda zu Raffs Walzer in Des-Dur, S. 551a", "Sunt lacrymae rerum / En mode hongrois, S. 163 no. 5", "Chor\u00e4le, S. 506a: No. 11. Wer nur den lieben Gott l\u00e4sst walten?", "Urbi et orbi, S. 184", "Feuille d\u2019album no. 1 in E major, S. 164", "R\u00e1k\u00f3czi March, S. 242a", "Douze grandes \u00e9tudes, S. 137: No. 10 in F minor (Presto molto agitato)", "L\u00e4ndler in A-flat major, S. 211", "San Francesco, S. 498d", "Abschied, S. 251", "Missa solennis zur Einweihung der Basilika in Gran, S. 9 \\\"Graner Messe\\\": I. Kyrie. Andante solenne", "O du mein holder Abendstern, S. 444", "Kaiser Wilhelm!, S. 197b", "Soir\u00e9es de Vienne, S. 427: No. 8 in D major. Allegro con brio", "Fantasie und Fuge \u00fcber den Choral Ad nos, ad salutarem undam, S. 624: III. Fugue", "Les Sab\u00e9ennes, berceuse de l\u2019op\u00e9ra La Reine de Saba, S. 408", "Tre sonetti di Petrarca, S. 270: I. Pace non trovo", "Mariotte \u2013 Valse de Marie, S. 212b", "Lieder aus Schillers Wilhelm Tell, S. 292a: Nr. 2. Der Hirt", "Pr\u00e4ludium und Fuge \u00fcber das Motiv B.A.C.H., S. 529/1", "Consolation in E-Dur", "Marche hongroise, S. 233b", "Magyar dalok, S. 242: No. 10 in D major", "Album d'un voyageur, S. 156: I. Impressions et po\u00e9sies: 5. La chapelle de Guillaume Tell", "Valse-caprice no. 6, S.427/6b", "Glanes de Woronince, S. 249: No. 3. Complainte (Dumka)", "Consolation in D-flat major, S. 172 no. 4: Quasi adagio", "Canzone napolitana, S. 248", "Album-Leaf in C minor (Pressburg), S. 163c", "Totentanz, S. 126: X. Allegro animato", "Romance oubli\u00e9e, S. 527 bis", "Romancero espagnol, S. 695c: No. 3. Jota aragonesa and coda", "Ungarische Zigeunerweisen", "Zigeuner-Epos, S. 695b: No. 8 in D major. Adagio sostenuto a capriccio", "Tscherkessenmarsch aus Russlan und Ludmilla, S. 406/2", "Bagatelle sans tonalit\u00e9", "Schnitterchor aus den entfesselten Prometheus, S. 507a", "Transcendental \u00c9tude No. 11", "Die stille Wasserrose", "Transcendental \u00c9tude No. 2", "Marche hongroise, S. 425/2e bis", "2 Polonaises, S. 223: no. 2 in E major", "An Edlitam. Zur silbernen Hochzeit", "Hungarian Rhapsody no. 8 in F-sharp minor, S. 244 no. 8", "La lugubre gondola, S. 134b", "Festmarsch zu S\u00e4kularfeier von Goethes Geburtstag, S. 227", "Mephisto Waltz No. 1", "Hungarian Rhapsody no. 16 in A minor, S. 244 no. 16", "Romance oubli\u00e9e, S. 527", "Grande fantaisie sur des th\u00e8mes de l'op\u00e9ra Niobe, S. 419", "Album-Leaf (Ah, vous dirai-je, maman), S. 163b", "Festkantate zur Enth\u00fcllung des Beethoven-Denkmals in Bonn, S. 67: III. Andante mesto - Allegro maestoso - Recitativo - Largo maestoso - Allegro fuocoso", "Douze grandes \u00e9tudes, S. 137: No. 7 in E-flat major (Allegro deciso)", "Huldigungsmarsch, S. 228/1", "Chor\u00e4le, S. 506a: No. 7. O Lamm Gottes", "Leyer und Schwert, S. 452: IV. Lutzows wilde Jagd", "Slavimo slavno, Slaveni!, S. 503", "L'Id\u00e9e fixe, S. 470a no. 1", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 3 in A-flat minor \\\"La Campanella\\\"", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 8b. Andante molto espressivo", "Das Veilchen", "Capriccio alla turca sur des motifs de Beethoven, S. 388", "Via Crucis, S. 53: Einleitung. Vexilla regis", "Consolation in E major, S. 171a no. 1: Andante con moto", "Album-Leaf: Andante religioso, S. 166h", "Symphonic Poem no. 2 \\\"Tasso, Lament and Triumph\\\"", "\u00c9tude en douze exercices, S. 136: III. Allegro sempre legato", "Aus der Ungarischen Kr\u00f6nungsmesse, S. 501: II. Offertorium", "Festkantate zur Enth\u00fcllung des Beethoven-Denkmals in Bonn, S. 67: II. Allegro deciso", "Deux Polonaises de l'oratorio St. Stanislas, S. 519: Polonaise II", "La Marseillaise, S. 237", "Eine Symphonie zu Dantes Divina Commedia, S. 109: II. Purgatorio - Magnificat", "Hungarian Coronation Mass, S. 11: I. Kyrie", "Zigeuner-Epos, S. 695b: No. 4 in C-sharp major. Animato", "Angelus! Pri\u00e8re \u00e0 l'ange gardien, S. 162a/4", "Le Lac de Wallenstadt, S. 156/2a bis", "Ave Maria (d'Arcadelt), S. 659", "Weimars Volkslied, S. 542/1", "Trois \u00e9tudes de concert, S. 144: II. \\\"La leggierezza\\\" in F minor", "Illustrations de l'op\u00e9ra L'Africaine, S. 415: No. 1. Pri\u00e8re des matelots \\\"\u00d4 grand Saint Dominique\\\"", "Zweite Festmarsch nach Motiven von E H z S-C-G, S. 522", "Consolations", "Rosario, S. 670: No. 3. Mysteria gloriosa", "Bist du, S. 277/2", "Douze grandes \u00e9tudes, S. 137: No. 4 in D minor (Allegro patetico)", "Liebestraum As-Dur \\\"Oh Lieb, so lang du lieben kannst\\\", S. 541 Nr. 3", "Paralipom\u00e8nes \u00e0 la Divina Commedia, S. 158a", "Romance oubli\u00e9e, S. 132c", "Weihnachtsbaum, S. 186: Nr. 1. Altes Weihnacthslied (Psalite)", "Apr\u00e8s une lecture du Dante - Fantasia quasi Sonata, S. 158c", "Petite Valse, S. 695d", "Festvorspiel, S. 226", "Album-Leaf: Andantino in E-flat major, S. 163a/2", "Romancero espagnol, S. 695c: No. 1. Introduction and Fandango with variations", "Heinrich von Ofterdingen", "Fantasia quasi Concerto \\\"After Reading Dante\\\"", "Anfang einer Jugendsonate, S. 692b", "Feuille d'album no. 2, S. 167", "Fantasie \u00fcber Motive aus Beethovens Ruinen von Athen, S. 389", "Historische ungarische Bildnisse, S. 205a: No. 5. V\u00f6r\u00f6smarty Mih\u00e1ly", "Rigoletto", "Les Cloches de Gen\u00e8ve : Nocturne, S. 160 no. 9", "Liebestraum As-Dur \\\"Hohe Liebe\\\", S. 541 Nr. 1", "Puszta-Wehmut, S. 246", "Hungarian Rhapsody no. 10 in E major, S. 244 no. 10 bis", "Gnomenreigen", "Paraphrases pour piano sur le th\u00e8me favori et oblig\u00e9: 1a. Pr\u00e9lude \u00e0 la Polka de Borodine, S. 207a", "Wo weilt er? (Heimat)", "Zwei St\u00fccke aus der heiligen Elisabeth, S. 693a: No. 2. Der Sturm", "Festmarsch zur Goethe-Jubil\u00e4umfeier, S. 521", "Au bord d'une source, S. 160 no. 4 bis"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1891", "prediction": ["# Reasoning Path:\nRusso-Japanese War -> military.military_conflict.force_strengths -> m.011pns0q -> military.force_strength.resource -> 28 cm Howitzer L/10\n# Answer:\n28 cm Howitzer L/10", "# Reasoning Path:\nRusso-Japanese War -> time.event.locations -> Korean Peninsula -> common.topic.image -> Location of the Korean Peninsula\n# Answer:\nKorean Peninsula", "# Reasoning Path:\nRusso-Japanese War -> time.event.locations -> Yellow Sea -> common.topic.article -> m.0df4r\n# Answer:\nYellow Sea", "# Reasoning Path:\nRusso-Japanese War -> time.event.includes_event -> Battle of Chemulpo Bay -> time.event.locations -> Incheon\n# Answer:\nBattle of Chemulpo Bay", "# Reasoning Path:\nRusso-Japanese War -> time.event.locations -> Manchuria -> location.location.events -> Battle of Hsimucheng\n# Answer:\nManchuria", "# Reasoning Path:\nRusso-Japanese War -> time.event.locations -> Manchuria -> common.topic.notable_for -> g.125525748\n# Answer:\nManchuria", "# Reasoning Path:\nRusso-Japanese War -> time.event.locations -> Korean Peninsula -> common.topic.image -> Map of the Korean peninsula, as recognized by the Republic of Korea\n# Answer:\nKorean Peninsula", "# Reasoning Path:\nRusso-Japanese War -> time.event.locations -> Korean Peninsula -> common.topic.subject_of -> Crisis on the Korean peninsula : how to deal with a nuclear North Korea\n# Answer:\nKorean Peninsula", "# Reasoning Path:\nRusso-Japanese War -> time.event.includes_event -> Battle of Chemulpo Bay -> time.event.locations -> Korea\n# Answer:\nBattle of Chemulpo Bay", "# Reasoning Path:\nRusso-Japanese War -> time.event.includes_event -> Battle of Hsimucheng -> time.event.locations -> L\u00fcshunkou District\n# Answer:\nBattle of Hsimucheng"], "ground_truth": ["Korean Peninsula", "Manchuria", "Yellow Sea"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1892", "prediction": ["# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup Finals\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> time.event.instance_of_recurring_event -> Stanley Cup Finals\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2012 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n2012 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2012 Stanley Cup Finals -> common.topic.article -> m.0jt52q2\n# Answer:\n2012 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> award.award_nominee.award_nominations -> m.0z1ycgx -> award.award_nomination.award -> Outstanding Team ESPY Award\n# Answer:\nOutstanding Team ESPY Award", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.roster -> g.11b76d7nzz\n# Answer:\nsports.sports_team.roster", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2012 Stanley Cup Finals -> sports.sports_championship_event.runner_up -> New Jersey Devils\n# Answer:\n2012 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> award.award_nominee.award_nominations -> m.0z1ycgx -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work"], "ground_truth": ["2014 Stanley Cup Finals", "2012 Stanley Cup Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1893", "prediction": ["# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3f_h6\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> common.topic.notable_types -> Governmental Body\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> government.governmental_body.component_bodies -> House of Representatives\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> government.governmental_body.component_bodies -> Shura Council\n# Answer:\nParliament of Egypt"], "ground_truth": ["Egyptian pound"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1894", "prediction": ["# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.season -> 1953 Major League Baseball season\n# Answer:\n1953 Major League Baseball season", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.season -> 1954 Major League Baseball season\n# Answer:\n1954 Major League Baseball season", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.season -> 1956 Major League Baseball Season\n# Answer:\n1956 Major League Baseball Season", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> people.place_of_interment.interred_here -> Abraham Cohn\n# Answer:\nCypress Hills Cemetery", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> common.topic.notable_types -> Cemetery\n# Answer:\nCypress Hills Cemetery", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> common.topic.article -> m.03vbmy\n# Answer:\nCypress Hills Cemetery", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> people.place_of_interment.interred_here -> Alexander Walters\n# Answer:\nCypress Hills Cemetery"], "ground_truth": ["Kansas City Monarchs", "Brooklyn Dodgers", "Montreal Royals"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.3157894736842105, "ans_precission": 0.3, "ans_recall": 0.3333333333333333, "path_f1": 0.2608695652173913, "path_precision": 0.3, "path_recall": 0.23076923076923078, "path_ans_f1": 0.3157894736842105, "path_ans_precision": 0.3, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1895", "prediction": ["# Reasoning Path:\nSaint Lawrence River -> geography.river.mouth -> Gulf of Saint Lawrence -> base.scubadiving.ecoregion.contained_by -> Cold Temperate Northwest Atlantic\n# Answer:\nGulf of Saint Lawrence", "# Reasoning Path:\nSaint Lawrence River -> geography.river.mouth -> Gulf of Saint Lawrence -> base.ontologies.ontology_instance.equivalent_instances -> m.09kmh6l\n# Answer:\nGulf of Saint Lawrence", "# Reasoning Path:\nSaint Lawrence River -> geography.river.mouth -> Gulf of Saint Lawrence -> location.location.events -> Action of 8 June 1755\n# Answer:\nGulf of Saint Lawrence", "# Reasoning Path:\nSaint Lawrence River -> geography.river.mouth -> Gulf of Saint Lawrence -> location.location.events -> Battle of the Atlantic\n# Answer:\nGulf of Saint Lawrence", "# Reasoning Path:\nSaint Lawrence River -> location.location.events -> The Beaver Wars -> time.event.locations -> North America\n# Answer:\nThe Beaver Wars", "# Reasoning Path:\nSaint Lawrence River -> geography.river.mouth -> Gulf of Saint Lawrence -> location.location.events -> Battle of the St. Lawrence\n# Answer:\nGulf of Saint Lawrence", "# Reasoning Path:\nSaint Lawrence River -> location.location.partially_contained_by -> m.0wg947r -> location.partial_containment_relationship.partially_contained_by -> Canada\n# Answer:\nCanada", "# Reasoning Path:\nSaint Lawrence River -> location.location.events -> Battle of Neuville -> time.event.included_in_event -> Seven Years' War\n# Answer:\nBattle of Neuville", "# Reasoning Path:\nSaint Lawrence River -> location.location.events -> The Beaver Wars -> time.event.locations -> Great Lakes region\n# Answer:\nThe Beaver Wars", "# Reasoning Path:\nSaint Lawrence River -> location.location.events -> The Beaver Wars -> time.event.includes_event -> Battle of La Prairie\n# Answer:\nThe Beaver Wars"], "ground_truth": ["Gulf of Saint Lawrence"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1896", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> South Korea\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> American Samoa\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Northern Mariana Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Mexico\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guam\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Marshall Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Germany\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nbase.schemastaging.context_name.pronunciation"], "ground_truth": ["Federal republic", "Presidential system", "Constitutional republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1897", "prediction": ["# Reasoning Path:\nAriel -> fictional_universe.fictional_character.children -> Melody -> fictional_universe.fictional_character.parents -> g.12175twq\n# Answer:\nMelody", "# Reasoning Path:\nAriel -> fictional_universe.fictional_character.siblings -> m.065hp2g -> fictional_universe.sibling_relationship_of_fictional_characters.siblings -> Aquata\n# Answer:\nAquata", "# Reasoning Path:\nAriel -> fictional_universe.fictional_character.children -> Melody -> film.film_character.portrayed_in_films -> m.0h5nlyf\n# Answer:\nMelody", "# Reasoning Path:\nAriel -> fictional_universe.fictional_character.children -> Melody -> fictional_universe.fictional_character.occupation -> Crown Princess\n# Answer:\nMelody", "# Reasoning Path:\nAriel -> fictional_universe.fictional_character.siblings -> m.065hpxv -> fictional_universe.sibling_relationship_of_fictional_characters.siblings -> Alana\n# Answer:\nAlana", "# Reasoning Path:\nAriel -> common.topic.article -> m.0cl19n\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nAriel -> fictional_universe.fictional_character.siblings -> m.065hpgf -> fictional_universe.sibling_relationship_of_fictional_characters.siblings -> Arista\n# Answer:\nArista", "# Reasoning Path:\nAriel -> common.topic.article -> m.0j3wxxr -> common.document.content -> m.0j3_hxl\n# Answer:\ncommon.document.content"], "ground_truth": ["Attina", "Arista", "Alana", "Aquata", "Adella", "Andrina"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.42857142857142855, "ans_precission": 0.375, "ans_recall": 0.5, "path_f1": 0.2857142857142857, "path_precision": 0.375, "path_recall": 0.23076923076923078, "path_ans_f1": 0.42857142857142855, "path_ans_precision": 0.375, "path_ans_recall": 0.5}
{"id": "WebQTest-1898", "prediction": ["# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> God\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> fictional_universe.fictional_character_creator.fictional_characters_created -> Glenn Quagmire\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> Mickey McFinnegan\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> fictional_universe.fictional_character_creator.fictional_characters_created -> Stewie Griffin\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> Brian Griffin\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.05tw61d\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> common.topic.notable_for -> g.1255sxsvl\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> fictional_universe.fictional_character_creator.fictional_characters_created -> Peter Griffin\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> award.award_winner.awards_won -> m.0ly_pz4\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.works_set_here -> Family Guy Video Game!\n# Answer:\nFamily Guy Universe"], "ground_truth": ["Dr. Elmer Hartman", "Seamus", "God", "Brian Griffin", "Stewie Griffin", "Kool-aid Guy", "Tom Tucker", "Jasper", "Glenn Quagmire", "Nate Griffin", "Kevin Swanson", "Peter Griffin", "Carter Pewterschmidt", "Jake Tucker", "Mickey McFinnegan"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.3076923076923077, "path_precision": 0.6, "path_recall": 0.20689655172413793, "path_ans_f1": 0.48, "path_ans_precision": 0.6, "path_ans_recall": 0.4}
{"id": "WebQTest-1899", "prediction": ["# Reasoning Path:\nWilliam McKinley -> people.person.spouse_s -> m.0j4k7z3 -> people.marriage.spouse -> Ida Saxton McKinley\n# Answer:\nIda Saxton McKinley", "# Reasoning Path:\nWilliam McKinley -> people.person.spouse_s -> m.0j4k7z3 -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nWilliam McKinley -> people.person.children -> Katherine McKinley -> people.person.parents -> Ida Saxton McKinley\n# Answer:\nKatherine McKinley", "# Reasoning Path:\nWilliam McKinley -> people.person.children -> Ida McKinley -> people.person.parents -> Ida Saxton McKinley\n# Answer:\nIda McKinley", "# Reasoning Path:\nWilliam McKinley -> people.person.children -> Katherine McKinley -> people.person.nationality -> United States of America\n# Answer:\nKatherine McKinley", "# Reasoning Path:\nWilliam McKinley -> people.person.children -> Katherine McKinley -> common.topic.notable_for -> g.125g47nt2\n# Answer:\nKatherine McKinley", "# Reasoning Path:\nWilliam McKinley -> people.person.children -> Ida McKinley -> common.topic.notable_types -> Deceased Person\n# Answer:\nIda McKinley", "# Reasoning Path:\nWilliam McKinley -> people.person.children -> Ida McKinley -> common.topic.notable_for -> g.1259fcsxc\n# Answer:\nIda McKinley", "# Reasoning Path:\nWilliam McKinley -> book.author.works_written -> Papers, ca. 1847-1902 -> common.topic.notable_types -> Book\n# Answer:\nPapers, ca. 1847-1902", "# Reasoning Path:\nWilliam McKinley -> book.author.works_written -> Speeches And Addresses Of William McKinley -> common.topic.notable_types -> Book\n# Answer:\nSpeeches And Addresses Of William McKinley"], "ground_truth": ["Ida Saxton McKinley"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Tickfaw River -> common.topic.notable_types -> River\n# Answer:\nTickfaw River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Tangipahoa River -> location.location.containedby -> United States of America\n# Answer:\nTangipahoa River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Tickfaw River -> geography.river.origin -> Amite County\n# Answer:\nTickfaw River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> location.location.containedby -> United States of America\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wjpmwn -> location.partial_containment_relationship.partially_contains -> Bayou Macon\n# Answer:\nBayou Macon", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wg8__0 -> location.partial_containment_relationship.partially_contains -> Sabine River\n# Answer:\nSabine River"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-190", "prediction": ["# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Television producer -> common.topic.subject_of -> Michael Palance\n# Answer:\nTelevision producer", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Television producer -> base.lightweight.profession.specialization_of -> Producers and Directors\n# Answer:\nTelevision producer", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Visual Artist -> fictional_universe.character_occupation.characters_with_this_occupation -> Eug\u00e8ne Delacroix\n# Answer:\nVisual Artist", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Television producer -> common.topic.subject_of -> Billy Sorrentino\n# Answer:\nTelevision producer", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Television producer -> common.topic.subjects -> Michael Palance\n# Answer:\nTelevision producer", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> theater.theater_production_staff_role.people_who_have_had_this_role -> m.0111n9h6\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> common.topic.subject_of -> Dum-Doodles\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Television producer -> common.topic.subjects -> Billy Sorrentino\n# Answer:\nTelevision producer", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> theater.theater_production_staff_role.people_who_have_had_this_role -> m.0_grv_v\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> book.author.works_written -> ARE YOU MY MOTHER MINI PB -> common.topic.notable_for -> g.125fbm0pz\n# Answer:\nARE YOU MY MOTHER MINI PB"], "ground_truth": ["Poet", "Writer", "Songwriter", "Illustrator", "Cartoonist", "Artist", "Visual Artist", "Film Producer", "Screenwriter", "Television producer", "Animator"], "ans_acc": 0.36363636363636365, "ans_hit": 1, "ans_f1": 0.5179856115107914, "ans_precission": 0.9, "ans_recall": 0.36363636363636365, "path_f1": 0.41860465116279066, "path_precision": 0.9, "path_recall": 0.2727272727272727, "path_ans_f1": 0.5179856115107914, "path_ans_precision": 0.9, "path_ans_recall": 0.36363636363636365}
{"id": "WebQTest-1900", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.film -> Star Wars\n# Answer:\nStar Wars", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Uncredited\n# Answer:\nUncredited", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.film -> Return of the Jedi\n# Answer:\nReturn of the Jedi", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nThe Making of Star Wars"], "ground_truth": ["James Earl Jones", "David Prowse"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.37499999999999994, "ans_precission": 0.3, "ans_recall": 0.5, "path_f1": 0.3, "path_precision": 0.3, "path_recall": 0.3, "path_ans_f1": 0.37499999999999994, "path_ans_precision": 0.3, "path_ans_recall": 0.5}
{"id": "WebQTest-1901", "prediction": ["# Reasoning Path:\nKevin James -> people.person.education -> m.0cr6cbz -> education.education.institution -> Ward Melville High School\n# Answer:\nWard Melville High School", "# Reasoning Path:\nKevin James -> people.person.education -> m.0cr6cc4 -> education.education.institution -> State University of New York at Cortland\n# Answer:\nState University of New York at Cortland", "# Reasoning Path:\nKevin James -> film.actor.film -> m.0wc78q8 -> film.performance.film -> Caf\u00e9 Flesh\n# Answer:\nCaf\u00e9 Flesh", "# Reasoning Path:\nKevin James -> film.actor.film -> m.010_t38m -> film.performance.character -> Will Cooper\n# Answer:\nWill Cooper", "# Reasoning Path:\nKevin James -> film.actor.film -> m.010x46kw -> film.performance.film -> Paul Blart: Mall Cop 2\n# Answer:\nPaul Blart: Mall Cop 2", "# Reasoning Path:\nKevin James -> tv.tv_actor.guest_roles -> m.0bv0l6h -> tv.tv_guest_role.episodes_appeared_in -> Team Nogueira vs Team Mir Finale\n# Answer:\nTeam Nogueira vs Team Mir Finale", "# Reasoning Path:\nKevin James -> film.actor.film -> m.010_t38m -> film.performance.film -> Pixels\n# Answer:\nPixels", "# Reasoning Path:\nKevin James -> tv.tv_actor.guest_roles -> m.0bv13gs -> tv.tv_guest_role.episodes_appeared_in -> Episode 313\n# Answer:\nEpisode 313", "# Reasoning Path:\nKevin James -> film.actor.film -> m.010x46kw -> film.performance.character -> Paul Blart\n# Answer:\nPaul Blart", "# Reasoning Path:\nKevin James -> tv.tv_actor.guest_roles -> m.0bv16cy -> tv.tv_guest_role.episodes_appeared_in -> 1/5/2001\n# Answer:\n1/5/2001"], "ground_truth": ["State University of New York at Cortland"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1902", "prediction": ["# Reasoning Path:\nAdolf Hitler -> people.person.place_of_birth -> Braunau am Inn -> location.location.containedby -> Austria\n# Answer:\nBraunau am Inn", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAdolf Hitler -> people.person.place_of_birth -> Braunau am Inn -> location.location.containedby -> Braunau am Inn District\n# Answer:\nBraunau am Inn", "# Reasoning Path:\nAdolf Hitler -> people.person.place_of_birth -> Braunau am Inn -> common.topic.webpage -> m.03l75z9\n# Answer:\nBraunau am Inn", "# Reasoning Path:\nAdolf Hitler -> people.person.place_of_birth -> Braunau am Inn -> location.location.people_born_here -> Angela Hitler\n# Answer:\nBraunau am Inn", "# Reasoning Path:\nAdolf Hitler -> people.person.place_of_birth -> Braunau am Inn -> location.location.containedby -> Upper Austria\n# Answer:\nBraunau am Inn", "# Reasoning Path:\nAdolf Hitler -> people.person.place_of_birth -> Braunau am Inn -> location.location.people_born_here -> Daniela Raschhofer\n# Answer:\nBraunau am Inn", "# Reasoning Path:\nAdolf Hitler -> people.person.place_of_birth -> Braunau am Inn -> location.location.people_born_here -> David Schie\u00dfl\n# Answer:\nBraunau am Inn", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.resource -> Another Megan Fox letter -- and this one sings her praises!\n# Answer:\nAnother Megan Fox letter -- and this one sings her praises!"], "ground_truth": ["1889-04-20"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1904", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> sports.sports_facility.teams -> Atlanta Falcons\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> United States of America\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola -> location.location.containedby -> Fulton County\n# Answer:\nWorld of Coca-Cola", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola -> common.topic.notable_types -> Museum\n# Answer:\nWorld of Coca-Cola", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> common.topic.notable_types -> Zoo\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.containedby -> United States of America\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> travel.transport_terminus.travel_destinations_served -> m.052zwlt\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola -> location.location.containedby -> 30313\n# Answer:\nWorld of Coca-Cola", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.03ldb41 -> common.webpage.resource -> m.0blf53m\n# Answer:\ncommon.webpage.resource"], "ground_truth": ["Turner Field", "Center for Puppetry Arts", "Atlanta Symphony Orchestra", "Atlanta Marriott Marquis", "Hyatt Regency Atlanta", "Six Flags White Water", "Arbor Place Mall", "Centennial Olympic Park", "Atlanta Cyclorama & Civil War Museum", "Jimmy Carter Library and Museum", "The Tabernacle", "Variety Playhouse", "World of Coca-Cola", "Four Seasons Hotel Atlanta", "Georgia State Capitol", "Woodruff Arts Center", "Atlanta History Center", "Atlanta Ballet", "Cobb Energy Performing Arts Centre", "Masquerade", "Underground Atlanta", "Margaret Mitchell House & Museum", "Fernbank Museum of Natural History", "Fernbank Science Center", "Fox Theatre", "Peachtree Road Race", "CNN Center", "Atlanta Jewish Film Festival", "Martin Luther King, Jr. National Historic Site", "Georgia Aquarium", "Six Flags Over Georgia", "Omni Coliseum", "Georgia Dome", "Philips Arena", "Zoo Atlanta", "Georgia World Congress Center"], "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0.1509433962264151, "ans_precission": 0.8, "ans_recall": 0.08333333333333333, "path_f1": 0.1509433962264151, "path_precision": 0.8, "path_recall": 0.08333333333333333, "path_ans_f1": 0.19512195121951217, "path_ans_precision": 0.8, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-1905", "prediction": ["# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> O\u015bwi\u0119cim -> location.location.containedby -> Lesser Poland\n# Answer:\nO\u015bwi\u0119cim", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> Auschwitz-Birkenau State Museum -> location.location.containedby -> Poland\n# Answer:\nAuschwitz-Birkenau State Museum", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> O\u015bwi\u0119cim -> location.location.containedby -> O\u015bwi\u0119cim County\n# Answer:\nO\u015bwi\u0119cim", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> O\u015bwi\u0119cim -> sports.sports_team_location.teams -> TH Unia O\u015bwi\u0119cim\n# Answer:\nO\u015bwi\u0119cim", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> Poland -> location.location.containedby -> Europe\n# Answer:\nPoland", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> O\u015bwi\u0119cim -> location.location.people_born_here -> Agnieszka Wr\u00f3blewska\n# Answer:\nO\u015bwi\u0119cim", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> Auschwitz-Birkenau State Museum -> location.location.geolocation -> m.0clgyph\n# Answer:\nAuschwitz-Birkenau State Museum", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.people_born_here -> Margherita Horowitz -> people.person.gender -> Female\n# Answer:\nMargherita Horowitz", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> O\u015bwi\u0119cim -> location.location.people_born_here -> Aneta Micha\u0142ek\n# Answer:\nO\u015bwi\u0119cim", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> O\u015bwi\u0119cim -> location.location.people_born_here -> Anna Jurkiewicz\n# Answer:\nO\u015bwi\u0119cim"], "ground_truth": ["O\u015bwi\u0119cim", "Auschwitz-Birkenau State Museum", "Poland"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1907", "prediction": ["# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.article -> m.04lt88\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.webpage -> m.04m2gwk\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.statistical_region.co2_emissions_per_capita -> g.1245_1qfx\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nDominican Republic -> location.statistical_region.internet_users_percent_population -> g.11b60vv5zn\n# Answer:\nlocation.statistical_region.internet_users_percent_population", "# Reasoning Path:\nDominican Republic -> location.statistical_region.co2_emissions_per_capita -> g.1245_22j7\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nDominican Republic -> location.statistical_region.co2_emissions_per_capita -> g.1245_4lf3\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nDominican Republic -> location.statistical_region.internet_users_percent_population -> g.1245_22jn\n# Answer:\nlocation.statistical_region.internet_users_percent_population", "# Reasoning Path:\nDominican Republic -> location.statistical_region.internet_users_percent_population -> g.1245_m66j\n# Answer:\nlocation.statistical_region.internet_users_percent_population"], "ground_truth": ["Dominican peso"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1909", "prediction": ["# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> media_common.netflix_genre.titles -> Adelheid\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> common.topic.notable_types -> Human Language\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.countries_spoken_in -> Ukraine\n# Answer:\nUkrainian Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> media_common.netflix_genre.titles -> All My Loved Ones\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Hungarian language -> language.human_language.countries_spoken_in -> Ukraine\n# Answer:\nHungarian language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Hungarian language -> common.topic.notable_types -> Human Language\n# Answer:\nHungarian language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> media_common.netflix_genre.titles -> Autumn Spring\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Hungarian language -> language.human_language.countries_spoken_in -> Hungary\n# Answer:\nHungarian language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Hungarian language -> language.human_language.region -> Europe\n# Answer:\nHungarian language"], "ground_truth": ["Slovak Language", "Bulgarian Language", "Croatian language", "Serbian language", "German Language", "Greek Language", "Russian Language", "Hungarian language", "Czech Language", "Rusyn Language", "Ukrainian Language", "Polish Language", "Romani language"], "ans_acc": 0.23076923076923078, "ans_hit": 1, "ans_f1": 0.375, "ans_precission": 1.0, "ans_recall": 0.23076923076923078, "path_f1": 0.375, "path_precision": 1.0, "path_recall": 0.23076923076923078, "path_ans_f1": 0.375, "path_ans_precision": 1.0, "path_ans_recall": 0.23076923076923078}
{"id": "WebQTest-191", "prediction": ["# Reasoning Path:\nDrew Brees -> people.person.education -> m.0j_8p_y -> education.education.institution -> Stanford Graduate School of Business\n# Answer:\nStanford Graduate School of Business", "# Reasoning Path:\nDrew Brees -> people.person.education -> m.03gkndb -> education.education.institution -> Purdue University\n# Answer:\nPurdue University", "# Reasoning Path:\nDrew Brees -> people.person.education -> m.052vk4c -> education.education.institution -> Westlake High School\n# Answer:\nWestlake High School", "# Reasoning Path:\nDrew Brees -> american_football.football_player.games -> m.07nvq27 -> american_football.player_game_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints", "# Reasoning Path:\nDrew Brees -> people.person.education -> m.03gkndb -> education.education.major_field_of_study -> Industrial management\n# Answer:\nIndustrial management", "# Reasoning Path:\nDrew Brees -> people.person.parents -> Eugene Wilson Brees II -> people.person.gender -> Male\n# Answer:\nEugene Wilson Brees II", "# Reasoning Path:\nDrew Brees -> american_football.football_player.games -> m.07sh5y9 -> american_football.player_game_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints", "# Reasoning Path:\nDrew Brees -> american_football.football_player.games -> m.07nvq27 -> american_football.player_game_statistics.season -> 2008 NFL season\n# Answer:\n2008 NFL season", "# Reasoning Path:\nDrew Brees -> american_football.football_player.games -> m.07tdyjy -> american_football.player_game_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints", "# Reasoning Path:\nDrew Brees -> people.person.parents -> Eugene Wilson Brees II -> people.person.spouse_s -> m.0w44yvs\n# Answer:\nEugene Wilson Brees II"], "ground_truth": ["Stanford Graduate School of Business", "Purdue University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1910", "prediction": ["# Reasoning Path:\nWilliam Shakespeare -> people.person.place_of_birth -> Stratford-upon-Avon -> location.location.people_born_here -> Thomas Quiney\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.person.place_of_birth -> Stratford-upon-Avon -> location.location.people_born_here -> Gilbert Shakespeare\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.person.place_of_birth -> Stratford-upon-Avon -> travel.travel_destination.tourist_attractions -> Shakespeare's Birthplace\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.deceased_person.place_of_death -> Stratford-upon-Avon -> location.location.people_born_here -> Thomas Quiney\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.person.place_of_birth -> Stratford-upon-Avon -> location.location.people_born_here -> Edmund Shakespeare\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.person.place_of_birth -> Stratford-upon-Avon -> location.location.containedby -> United Kingdom\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.deceased_person.place_of_death -> Stratford-upon-Avon -> location.location.people_born_here -> Gilbert Shakespeare\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.person.place_of_birth -> Stratford-upon-Avon -> travel.travel_destination.tourist_attractions -> Church of the Holy Trinity, Stratford-upon-Avon\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.deceased_person.place_of_death -> Stratford-upon-Avon -> travel.travel_destination.tourist_attractions -> Shakespeare's Birthplace\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.person.place_of_birth -> Stratford-upon-Avon -> travel.travel_destination.tourist_attractions -> Royal Shakespeare Theatre\n# Answer:\nStratford-upon-Avon"], "ground_truth": ["Stratford-upon-Avon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1911", "prediction": ["# Reasoning Path:\nF. Scott Fitzgerald -> people.person.education -> m.02h52nm -> education.education.institution -> Princeton University\n# Answer:\nPrinceton University", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.education -> m.0n0mxgb -> education.education.institution -> St. Paul Academy and Summit School\n# Answer:\nSt. Paul Academy and Summit School", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.education -> m.0n0klvz -> education.education.institution -> Nardin Academy\n# Answer:\nNardin Academy", "# Reasoning Path:\nF. Scott Fitzgerald -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Male\n# Answer:\nheart attack", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.education -> m.02h52nm -> freebase.valuenotation.has_value -> Minor\n# Answer:\nMinor", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.education -> m.02h52nm -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.education -> m.0n0mxgb -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nF. Scott Fitzgerald -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.parent_disease -> Cardiovascular disease\n# Answer:\nheart attack", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.education -> m.02h52nm -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nF. Scott Fitzgerald -> people.deceased_person.cause_of_death -> heart attack -> people.cause_of_death.parent_cause_of_death -> Cardiovascular disease\n# Answer:\nheart attack"], "ground_truth": ["Princeton University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1912", "prediction": ["# Reasoning Path:\nJoan Crawford -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Male\n# Answer:\nheart attack", "# Reasoning Path:\nJoan Crawford -> people.deceased_person.cause_of_death -> heart attack -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nheart attack", "# Reasoning Path:\nJoan Crawford -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Male\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nJoan Crawford -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.symptoms -> Jaundice\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nJoan Crawford -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Diabetes mellitus\n# Answer:\nheart attack", "# Reasoning Path:\nJoan Crawford -> people.deceased_person.cause_of_death -> Pancreatic cancer -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nJoan Crawford -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Diabetes mellitus\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nJoan Crawford -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.symptoms -> Diabetes mellitus\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nJoan Crawford -> film.actor.film -> m.0115tf9q -> film.performance.character -> Della Chappell\n# Answer:\nDella Chappell", "# Reasoning Path:\nJoan Crawford -> tv.tv_actor.guest_roles -> m.0bv_0kc -> tv.tv_guest_role.episodes_appeared_in -> Strange Witness\n# Answer:\nStrange Witness"], "ground_truth": ["Pancreatic cancer", "heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1913", "prediction": ["# Reasoning Path:\nRob Kardashian -> people.person.education -> m.0n1n2gl -> education.education.institution -> USC Marshall School of Business\n# Answer:\nUSC Marshall School of Business", "# Reasoning Path:\nRob Kardashian -> people.person.education -> m.0n1n2gl -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nRob Kardashian -> celebrities.celebrity.sexual_relationships -> m.0nh4s8y -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nDated", "# Reasoning Path:\nRob Kardashian -> people.person.education -> m.0n1n2gl -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nRob Kardashian -> celebrities.celebrity.sexual_relationships -> m.0nh4s8y -> celebrities.romantic_relationship.celebrity -> Rita Ora\n# Answer:\nRita Ora", "# Reasoning Path:\nRob Kardashian -> people.person.education -> m.0n1n2gl -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nRob Kardashian -> people.person.sibling_s -> m.0113v0_l -> people.sibling_relationship.sibling -> Brandon Jenner\n# Answer:\nBrandon Jenner", "# Reasoning Path:\nRob Kardashian -> people.person.sibling_s -> m.04d12sk -> people.sibling_relationship.sibling -> Kim Kardashian\n# Answer:\nKim Kardashian", "# Reasoning Path:\nRob Kardashian -> people.person.sibling_s -> m.071f_w4 -> people.sibling_relationship.sibling -> Kourtney Kardashian\n# Answer:\nKourtney Kardashian", "# Reasoning Path:\nRob Kardashian -> people.person.sibling_s -> m.0113v0_l -> people.sibling_relationship.sibling -> Brody Jenner\n# Answer:\nBrody Jenner"], "ground_truth": ["USC Marshall School of Business"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1914", "prediction": ["# Reasoning Path:\nAllen Iverson -> sports.sports_award_winner.awards -> m.02kbc0l -> sports.sports_award.season -> 1996\u201397 NBA season\n# Answer:\n1996\u201397 NBA season", "# Reasoning Path:\nAllen Iverson -> sports.sports_award_winner.awards -> m.0tlr44k -> sports.sports_award.season -> 2000\u201301 NBA season\n# Answer:\n2000\u201301 NBA season", "# Reasoning Path:\nAllen Iverson -> sports.sports_award_winner.awards -> m.02kbc0l -> sports.sports_award.team -> Philadelphia 76ers\n# Answer:\nPhiladelphia 76ers", "# Reasoning Path:\nAllen Iverson -> sports.sports_award_winner.awards -> m.04ynxpm -> sports.sports_award.season -> 2004\u201305 NBA season\n# Answer:\n2004\u201305 NBA season", "# Reasoning Path:\nAllen Iverson -> basketball.basketball_player.player_statistics -> m.04qrrxn -> basketball.basketball_player_stats.season -> 2000\u201301 NBA season\n# Answer:\n2000\u201301 NBA season", "# Reasoning Path:\nAllen Iverson -> sports.sports_award_winner.awards -> m.0tlr44k -> sports.sports_award.team -> Philadelphia 76ers\n# Answer:\nPhiladelphia 76ers", "# Reasoning Path:\nAllen Iverson -> film.person_or_entity_appearing_in_film.films -> m.0vb4h71 -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nAllen Iverson -> sports.sports_award_winner.awards -> m.0tlr44k -> sports.sports_award.award -> NBA Most Valuable Player Award\n# Answer:\nNBA Most Valuable Player Award", "# Reasoning Path:\nAllen Iverson -> basketball.basketball_player.player_statistics -> m.04qcjf8 -> basketball.basketball_player_stats.season -> 2005\u201306 NBA season\n# Answer:\n2005\u201306 NBA season", "# Reasoning Path:\nAllen Iverson -> film.person_or_entity_appearing_in_film.films -> m.01149f6y -> film.personal_film_appearance.film -> Iverson\n# Answer:\nIverson"], "ground_truth": ["2000\u201301 NBA season"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1915", "prediction": ["# Reasoning Path:\nHarry S. Truman -> people.person.education -> m.0n19ssz -> education.education.institution -> University of Missouri\u2013Kansas City School of Law\n# Answer:\nUniversity of Missouri\u2013Kansas City School of Law", "# Reasoning Path:\nHarry S. Truman -> people.person.education -> m.04t5ls_ -> education.education.institution -> University of Missouri\u2013Kansas City\n# Answer:\nUniversity of Missouri\u2013Kansas City", "# Reasoning Path:\nHarry S. Truman -> people.person.education -> m.04hc6_w -> education.education.institution -> William Chrisman High School\n# Answer:\nWilliam Chrisman High School", "# Reasoning Path:\nHarry S. Truman -> people.person.education -> m.04t5ls_ -> education.education.major_field_of_study -> Law\n# Answer:\nLaw", "# Reasoning Path:\nHarry S. Truman -> government.political_appointer.appointees -> m.03p40b5 -> government.government_position_held.office_holder -> J. Howard Rossbach\n# Answer:\nJ. Howard Rossbach", "# Reasoning Path:\nHarry S. Truman -> book.author.works_written -> Year of Decisions -> freebase.valuenotation.has_value -> Date written\n# Answer:\nYear of Decisions", "# Reasoning Path:\nHarry S. Truman -> government.political_appointer.appointees -> m.03p40b5 -> government.government_position_held.governmental_body -> U.S. Securities and Exchange Commission\n# Answer:\nU.S. Securities and Exchange Commission", "# Reasoning Path:\nHarry S. Truman -> government.political_appointer.appointees -> m.04kqlkp -> government.government_position_held.governmental_body -> Cabinet of the United States\n# Answer:\nCabinet of the United States", "# Reasoning Path:\nHarry S. Truman -> book.author.works_written -> A new era in world affairs -> common.topic.notable_types -> Book\n# Answer:\nA new era in world affairs", "# Reasoning Path:\nHarry S. Truman -> government.political_appointer.appointees -> m.04kqlkt -> government.government_position_held.governmental_body -> Cabinet of the United States\n# Answer:\nCabinet of the United States"], "ground_truth": ["University of Missouri\u2013Kansas City", "University of Missouri\u2013Kansas City School of Law", "William Chrisman High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1916", "prediction": ["# Reasoning Path:\nUtica -> location.location.containedby -> Oneida County -> location.location.containedby -> Utica-Rome, NY Metropolitan Statistical Area\n# Answer:\nOneida County", "# Reasoning Path:\nUtica -> location.hud_county_place.county -> Oneida County -> location.location.containedby -> Utica-Rome, NY Metropolitan Statistical Area\n# Answer:\nOneida County", "# Reasoning Path:\nUtica -> location.location.containedby -> Oneida County -> location.location.containedby -> New York\n# Answer:\nOneida County", "# Reasoning Path:\nUtica -> location.location.containedby -> New York -> location.location.containedby -> United States of America\n# Answer:\nNew York", "# Reasoning Path:\nUtica -> location.location.containedby -> Oneida County -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nOneida County", "# Reasoning Path:\nUtica -> location.location.containedby -> Oneida County -> location.us_county.hud_county_place -> Barneveld\n# Answer:\nOneida County", "# Reasoning Path:\nUtica -> location.hud_county_place.county -> Oneida County -> location.location.containedby -> New York\n# Answer:\nOneida County", "# Reasoning Path:\nUtica -> location.location.containedby -> Oneida County -> location.us_county.hud_county_place -> Boonville\n# Answer:\nOneida County", "# Reasoning Path:\nUtica -> location.location.containedby -> New York -> base.aareas.schema.administrative_area.administrative_children -> Herkimer County\n# Answer:\nNew York", "# Reasoning Path:\nUtica -> location.hud_county_place.county -> Oneida County -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nOneida County"], "ground_truth": ["Oneida County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1917", "prediction": ["# Reasoning Path:\nCharles Babbage -> law.inventor.inventions -> Difference engine -> common.topic.image -> LondonScienceMuseumsReplicaDifferenceEngine\n# Answer:\nDifference engine", "# Reasoning Path:\nCharles Babbage -> law.inventor.inventions -> Difference engine -> common.topic.notable_types -> Invention\n# Answer:\nDifference engine", "# Reasoning Path:\nCharles Babbage -> law.inventor.inventions -> Analytical Engine -> common.topic.article -> m.0ng8\n# Answer:\nAnalytical Engine", "# Reasoning Path:\nCharles Babbage -> law.inventor.inventions -> Difference engine -> common.topic.article -> m.029x2\n# Answer:\nDifference engine", "# Reasoning Path:\nCharles Babbage -> base.argumentmaps.innovator.original_ideas -> Analytical Engine -> common.topic.article -> m.0ng8\n# Answer:\nAnalytical Engine", "# Reasoning Path:\nCharles Babbage -> law.inventor.inventions -> Difference engine -> common.topic.image -> Babbage Difference Engine\n# Answer:\nDifference engine", "# Reasoning Path:\nCharles Babbage -> law.inventor.inventions -> Analytical Engine -> common.topic.image -> AnalyticalMachine Babbage London\n# Answer:\nAnalytical Engine", "# Reasoning Path:\nCharles Babbage -> influence.influence_node.influenced_by -> Blaise Pascal -> law.inventor.inventions -> Pascal's calculator\n# Answer:\nBlaise Pascal", "# Reasoning Path:\nCharles Babbage -> base.argumentmaps.innovator.original_ideas -> Analytical Engine -> common.topic.image -> AnalyticalMachine Babbage London\n# Answer:\nAnalytical Engine", "# Reasoning Path:\nCharles Babbage -> influence.influence_node.influenced_by -> Gottfried Wilhelm von Leibniz -> law.inventor.inventions -> Leibniz wheel\n# Answer:\nGottfried Wilhelm von Leibniz"], "ground_truth": ["Analytical Engine", "Difference engine"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1919", "prediction": ["# Reasoning Path:\nMadagascar -> film.film.starring -> m.03ld02p -> film.performance.actor -> Sacha Baron Cohen\n# Answer:\nSacha Baron Cohen", "# Reasoning Path:\nMadagascar -> film.film.starring -> m.03ld02p -> film.performance.character -> Julien\n# Answer:\nJulien", "# Reasoning Path:\nMadagascar -> film.film.starring -> m.03ld02_ -> film.performance.actor -> Andy Richter\n# Answer:\nAndy Richter", "# Reasoning Path:\nMadagascar -> film.film.starring -> m.03ld02p -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nMadagascar -> film.film.starring -> m.03ld02v -> film.performance.actor -> Cedric the Entertainer\n# Answer:\nCedric the Entertainer", "# Reasoning Path:\nMadagascar -> film.film.rating -> PG (USA) -> film.content_rating.film_rating_system -> Motion Picture Association of America film rating system\n# Answer:\nPG (USA)", "# Reasoning Path:\nMadagascar -> film.film.starring -> m.03ld02v -> film.performance.character -> Maurice\n# Answer:\nMaurice", "# Reasoning Path:\nMadagascar -> film.film.starring -> m.03ld02_ -> film.performance.character -> Mort\n# Answer:\nMort", "# Reasoning Path:\nMadagascar -> film.film.rating -> PG (USA) -> common.topic.webpage -> m.0cpryn3\n# Answer:\nPG (USA)", "# Reasoning Path:\nMadagascar -> film.film.rating -> PG (USA) -> common.topic.article -> m.0kprrm\n# Answer:\nPG (USA)"], "ground_truth": ["Sacha Baron Cohen"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1920", "prediction": ["# Reasoning Path:\nRick Scott -> people.person.profession -> Politician -> common.topic.notable_types -> Profession\n# Answer:\nPolitician", "# Reasoning Path:\nRick Scott -> common.topic.notable_types -> Politician -> common.topic.notable_types -> Profession\n# Answer:\nPolitician", "# Reasoning Path:\nRick Scott -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdn_\n# Answer:\nPolitician", "# Reasoning Path:\nRick Scott -> people.person.profession -> Businessperson -> base.descriptive_names.names.descriptive_name -> m.0100z128\n# Answer:\nBusinessperson", "# Reasoning Path:\nRick Scott -> people.person.profession -> Investor -> common.topic.notable_types -> Profession\n# Answer:\nInvestor", "# Reasoning Path:\nRick Scott -> people.person.profession -> Politician -> people.profession.specializations -> AIDS Policy Specialist\n# Answer:\nPolitician", "# Reasoning Path:\nRick Scott -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdq3\n# Answer:\nPolitician", "# Reasoning Path:\nRick Scott -> people.person.profession -> Businessperson -> base.descriptive_names.names.descriptive_name -> m.0105_bmw\n# Answer:\nBusinessperson", "# Reasoning Path:\nRick Scott -> people.person.profession -> Businessperson -> people.profession.specializations -> Actuary\n# Answer:\nBusinessperson", "# Reasoning Path:\nRick Scott -> people.person.profession -> Investor -> common.topic.notable_for -> g.1256898vb\n# Answer:\nInvestor"], "ground_truth": ["Businessperson", "Executive officer", "Investor", "Politician", "Lawyer"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 1.0, "ans_recall": 0.6, "path_f1": 0.6, "path_precision": 0.6, "path_recall": 0.6, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 1.0, "path_ans_recall": 0.6}
{"id": "WebQTest-1921", "prediction": ["# Reasoning Path:\nCrimean War -> time.event.locations -> Autonomous Republic of Crimea -> base.aareas.schema.administrative_area.administrative_children -> Bakhchysarai Raion\n# Answer:\nAutonomous Republic of Crimea", "# Reasoning Path:\nCrimean War -> time.event.locations -> Autonomous Republic of Crimea -> base.aareas.schema.administrative_area.administrative_children -> Bilohirsk Raion\n# Answer:\nAutonomous Republic of Crimea", "# Reasoning Path:\nCrimean War -> time.event.locations -> Autonomous Republic of Crimea -> location.location.events -> Crimean Campaign\n# Answer:\nAutonomous Republic of Crimea", "# Reasoning Path:\nCrimean War -> time.event.locations -> Black Sea -> common.topic.notable_types -> Body Of Water\n# Answer:\nBlack Sea", "# Reasoning Path:\nCrimean War -> time.event.locations -> White Sea -> location.location.containedby -> Barents Sea\n# Answer:\nWhite Sea", "# Reasoning Path:\nCrimean War -> time.event.locations -> Autonomous Republic of Crimea -> base.aareas.schema.administrative_area.administrative_children -> Chornomorske Raion\n# Answer:\nAutonomous Republic of Crimea", "# Reasoning Path:\nCrimean War -> time.event.locations -> Autonomous Republic of Crimea -> location.location.contains -> Ai-Petri\n# Answer:\nAutonomous Republic of Crimea", "# Reasoning Path:\nCrimean War -> time.event.locations -> Black Sea -> location.location.events -> Action of 4 July 1773\n# Answer:\nBlack Sea", "# Reasoning Path:\nCrimean War -> time.event.includes_event -> The Thin Red Line -> time.event.locations -> Balaklava\n# Answer:\nThe Thin Red Line", "# Reasoning Path:\nCrimean War -> time.event.locations -> Autonomous Republic of Crimea -> location.location.events -> 2014\u201315 Russian military intervention in Ukraine\n# Answer:\nAutonomous Republic of Crimea"], "ground_truth": ["White Sea", "Crimea", "Autonomous Republic of Crimea", "Balkans", "Black Sea", "Caucasus", "Baltic Sea"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.6990291262135921, "ans_precission": 0.9, "ans_recall": 0.5714285714285714, "path_f1": 0.5806451612903225, "path_precision": 0.9, "path_recall": 0.42857142857142855, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 1.0, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-1922", "prediction": ["# Reasoning Path:\nTheodore Roosevelt -> government.politician.government_positions_held -> m.03fx899 -> government.government_position_held.basic_title -> President\n# Answer:\nPresident", "# Reasoning Path:\nTheodore Roosevelt -> government.politician.government_positions_held -> m.03fx899 -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nTheodore Roosevelt -> government.politician.government_positions_held -> m.03q2hyl -> government.government_position_held.basic_title -> Assistant Secretary\n# Answer:\nAssistant Secretary", "# Reasoning Path:\nTheodore Roosevelt -> people.person.employment_history -> m.0w7nv3t -> business.employment_tenure.title -> President\n# Answer:\nPresident", "# Reasoning Path:\nTheodore Roosevelt -> government.politician.government_positions_held -> m.04ml2sj -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nTheodore Roosevelt -> people.person.employment_history -> m.0w7nv3t -> business.employment_tenure.company -> Federal government of the United States\n# Answer:\nFederal government of the United States", "# Reasoning Path:\nTheodore Roosevelt -> common.topic.notable_types -> Book -> freebase.type_profile.published -> Published\n# Answer:\nBook", "# Reasoning Path:\nTheodore Roosevelt -> government.politician.government_positions_held -> m.03q2hyl -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nTheodore Roosevelt -> common.topic.notable_types -> Book -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nBook", "# Reasoning Path:\nTheodore Roosevelt -> common.topic.notable_types -> Film character -> freebase.type_profile.published -> Published\n# Answer:\nFilm character"], "ground_truth": ["Governor", "Member of the Legislative Assembly", "Assistant Secretary", "Vice President", "President"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.5, "ans_recall": 0.6, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.5, "path_ans_recall": 0.6}
{"id": "WebQTest-1923", "prediction": ["# Reasoning Path:\nMiley Cyrus -> people.person.parents -> Tish Cyrus -> film.producer.film -> LOL\n# Answer:\nTish Cyrus", "# Reasoning Path:\nMiley Cyrus -> people.person.parents -> Tish Cyrus -> film.producer.film -> So Undercover\n# Answer:\nTish Cyrus", "# Reasoning Path:\nMiley Cyrus -> people.person.parents -> Tish Cyrus -> film.producer.films_executive_produced -> Miley: The Movement\n# Answer:\nTish Cyrus", "# Reasoning Path:\nMiley Cyrus -> music.artist.album -> Hannah Montana -> media_common.adapted_work.adaptations -> Hannah Montana: The Movie\n# Answer:\nHannah Montana", "# Reasoning Path:\nMiley Cyrus -> people.person.parents -> Billy Ray Cyrus -> film.producer.film -> Hannah Montana: The Movie\n# Answer:\nBilly Ray Cyrus", "# Reasoning Path:\nMiley Cyrus -> film.actor.film -> m.04g4mbv -> film.performance.film -> Bolt\n# Answer:\nBolt", "# Reasoning Path:\nMiley Cyrus -> film.actor.film -> m.010tw6z6 -> film.performance.film -> Wizards on Deck with Hannah Montana\n# Answer:\nWizards on Deck with Hannah Montana", "# Reasoning Path:\nMiley Cyrus -> people.person.parents -> Tish Cyrus -> film.producer.films_executive_produced -> The Last Song\n# Answer:\nTish Cyrus", "# Reasoning Path:\nMiley Cyrus -> people.person.parents -> Tish Cyrus -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nTish Cyrus", "# Reasoning Path:\nMiley Cyrus -> film.actor.film -> m.012zkz2h -> film.performance.film -> Miley Cyrus: Tongue Tied\n# Answer:\nMiley Cyrus: Tongue Tied"], "ground_truth": ["Bolt", "High School Musical 2", "Super Rhino", "Big Fish", "The Last Song", "Sex and the City 2", "Wizards on Deck with Hannah Montana", "So Undercover", "Hannah Montana and Miley Cyrus: Best of Both Worlds Concert", "Rock Mafia Presents: The Big Bang", "Hannah Montana: The Movie", "Family Bond", "Miley Cyrus: Tongue Tied", "Radio Disney Party Jams: The Concert", "The World According to Miley Cyrus", "Sonic the Hedgehog", "LOL"], "ans_acc": 0.4117647058823529, "ans_hit": 1, "ans_f1": 0.22222222222222224, "ans_precission": 0.3, "ans_recall": 0.17647058823529413, "path_f1": 0.3636363636363636, "path_precision": 0.8, "path_recall": 0.23529411764705882, "path_ans_f1": 0.5436893203883495, "path_ans_precision": 0.8, "path_ans_recall": 0.4117647058823529}
{"id": "WebQTest-1925", "prediction": ["# Reasoning Path:\nMilk -> award.award_nominated_work.award_nominations -> m.0b4d54c -> award.award_nomination.award_nominee -> Alison Pill\n# Answer:\nAlison Pill", "# Reasoning Path:\nMilk -> award.award_nominated_work.award_nominations -> m.0b4d54c -> award.award_nomination.ceremony -> 15th Screen Actors Guild Awards\n# Answer:\n15th Screen Actors Guild Awards", "# Reasoning Path:\nMilk -> award.award_nominated_work.award_nominations -> m.0b4d54c -> award.award_nomination.award -> Screen Actors Guild Award for Outstanding Performance by a Cast in a Motion Picture\n# Answer:\nScreen Actors Guild Award for Outstanding Performance by a Cast in a Motion Picture", "# Reasoning Path:\nMilk -> award.award_nominated_work.award_nominations -> m.0b4d54c -> award.award_nomination.award_nominee -> Denis O'Hare\n# Answer:\nDenis O'Hare", "# Reasoning Path:\nMilk -> award.award_nominated_work.award_nominations -> m.05bj54y -> award.award_nomination.ceremony -> 81st Academy Awards\n# Answer:\n81st Academy Awards", "# Reasoning Path:\nMilk -> award.award_nominated_work.award_nominations -> m.0b4d54c -> award.award_nomination.award_nominee -> Diego Luna\n# Answer:\nDiego Luna", "# Reasoning Path:\nMilk -> award.award_nominated_work.award_nominations -> m.05bkr1l -> award.award_nomination.award -> Academy Award for Best Director\n# Answer:\nAcademy Award for Best Director", "# Reasoning Path:\nMilk -> freebase.valuenotation.has_no_value -> Film Series -> type.property.expected_type -> Film series\n# Answer:\nFilm Series", "# Reasoning Path:\nMilk -> award.award_nominated_work.award_nominations -> m.05bj54y -> award.award_nomination.award -> Academy Award for Best Actor\n# Answer:\nAcademy Award for Best Actor", "# Reasoning Path:\nMilk -> film.film.release_date_s -> m.0gl75y_ -> film.film_regional_release_date.film_release_distribution_medium -> Theatrical\n# Answer:\nTheatrical"], "ground_truth": ["Scott Smith"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1926", "prediction": ["# Reasoning Path:\n2011 military intervention in Libya -> time.event.locations -> Libya -> location.country.official_language -> Arabic Language\n# Answer:\nLibya", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.locations -> Libya -> location.location.events -> Operation Brevity\n# Answer:\nLibya", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.locations -> Libya -> location.location.events -> 1986 United States bombing of Libya\n# Answer:\nLibya", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.includes_event -> Operation Unified Protector -> time.event.locations -> Mediterranean Sea\n# Answer:\nOperation Unified Protector", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.locations -> Libya -> location.location.events -> 2010 Arab League summit\n# Answer:\nLibya", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.locations -> Libya -> location.statistical_region.consumer_price_index -> g.11b71rqxvx\n# Answer:\nLibya", "# Reasoning Path:\n2011 military intervention in Libya -> base.culturalevent.event.entity_involved -> \u00c9douard Guillaud -> military.military_person.service -> m.0gy43d4\n# Answer:\n\u00c9douard Guillaud", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.includes_event -> Operation Unified Protector -> time.event.locations -> Libya\n# Answer:\nOperation Unified Protector", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.includes_event -> Operation Ellamy -> base.culturalevent.event.entity_involved -> United Kingdom\n# Answer:\nOperation Ellamy", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.locations -> Libya -> location.statistical_region.consumer_price_index -> g.1hhc37x69\n# Answer:\nLibya"], "ground_truth": ["2011-03-19"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1928", "prediction": ["# Reasoning Path:\nDiana Taurasi -> sports.pro_athlete.teams -> m.0z3t_gj -> sports.sports_team_roster.team -> UMMC Ekaterinburg\n# Answer:\nUMMC Ekaterinburg", "# Reasoning Path:\nDiana Taurasi -> sports.pro_athlete.teams -> m.0z3t_gj -> sports.sports_team_roster.position -> Guard\n# Answer:\nGuard", "# Reasoning Path:\nDiana Taurasi -> sports.pro_athlete.teams -> m.0kc2_tr -> sports.sports_team_roster.team -> Phoenix Mercury\n# Answer:\nPhoenix Mercury", "# Reasoning Path:\nDiana Taurasi -> base.schemastaging.athlete_extra.salary -> m.0ng6vr5 -> base.schemastaging.athlete_salary.team -> Phoenix Mercury\n# Answer:\nPhoenix Mercury", "# Reasoning Path:\nDiana Taurasi -> sports.pro_athlete.teams -> m.0z3t_gj -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nDiana Taurasi -> sports.pro_athlete.teams -> m.0z3t_lb -> sports.sports_team_roster.team -> WBC Spartak Moscow Region\n# Answer:\nWBC Spartak Moscow Region", "# Reasoning Path:\nDiana Taurasi -> base.schemastaging.athlete_extra.salary -> m.0ng6vr5 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nDiana Taurasi -> sports.pro_athlete.teams -> m.0kc2_tr -> sports.sports_team_roster.position -> Guard\n# Answer:\nGuard", "# Reasoning Path:\nDiana Taurasi -> sports.pro_athlete.teams -> m.0kc2_tr -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nDiana Taurasi -> sports.pro_athlete.teams -> m.0z3t_lb -> sports.sports_team_roster.position -> Guard\n# Answer:\nGuard"], "ground_truth": ["Phoenix Mercury", "UMMC Ekaterinburg"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1929", "prediction": ["# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> travel.tourist_attraction.near_travel_destination -> Blois\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> common.topic.notable_for -> g.1256sch6w\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Mus\u00e9e d'Orsay -> base.schemastaging.organization_extra.contact_webpages -> m.010ggx62\n# Answer:\nMus\u00e9e d'Orsay", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> architecture.structure.architect -> Pierre Nepveu\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Mus\u00e9e d'Orsay -> common.topic.image -> Mus\u00c3\u00a9e d'Orsay Clock\n# Answer:\nMus\u00e9e d'Orsay", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Disneyland Park -> location.location.containedby -> France\n# Answer:\nDisneyland Park", "# Reasoning Path:\nParis -> common.topic.webpage -> m.02k8dbw -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Disneyland Park -> common.topic.notable_types -> Amusement Park\n# Answer:\nDisneyland Park", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> architecture.structure.architect -> Domenico da Cortona\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> common.topic.webpage -> m.02k8dbw -> common.webpage.resource -> m.0bkwkw7\n# Answer:\ncommon.webpage.resource"], "ground_truth": ["Mus\u00e9e Maillol", "Centre Georges Pompidou", "Jardin du Luxembourg", "Notre Dame de Paris", "Grande Arche", "Mus\u00e9e d'Orsay", "Bois de Boulogne", "The Louvre", "Les Invalides", "Folies Berg\u00e8re", "Arc de Triomphe", "Parc Ast\u00e9rix", "La Maison Rouge", "Champs-\u00c9lys\u00e9es", "Galerie Claude Bernard", "Ch\u00e2teau de Chambord", "Disneyland Paris", "Mus\u00e9e de l'Orangerie", "Petit Palais", "Place de la Concorde", "Gare d'Orsay", "\u00cele de la Cit\u00e9", "Mus\u00e9e des Arts et M\u00e9tiers", "Eiffel Tower", "Mus\u00e9e du quai Branly", "Panth\u00e9on", "Sacr\u00e9-C\u0153ur, Paris", "Verdon Gorge", "Galerie nationale du Jeu de Paume", "Caf\u00e9 Volpini", "Disneyland Park"], "ans_acc": 0.0967741935483871, "ans_hit": 1, "ans_f1": 0.1726618705035971, "ans_precission": 0.8, "ans_recall": 0.0967741935483871, "path_f1": 0.1726618705035971, "path_precision": 0.8, "path_recall": 0.0967741935483871, "path_ans_f1": 0.1726618705035971, "path_ans_precision": 0.8, "path_ans_recall": 0.0967741935483871}
{"id": "WebQTest-193", "prediction": ["# Reasoning Path:\nBen Roethlisberger -> people.person.education -> m.02wns4p -> education.education.institution -> Miami University\n# Answer:\nMiami University", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.rushing -> m.09tcd8t -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.passing -> m.09tcd5t -> american_football.player_passing_statistics.team -> Pittsburgh Steelers\n# Answer:\nPittsburgh Steelers", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.passing -> m.07916ct -> american_football.player_passing_statistics.season -> 2008 NFL season\n# Answer:\n2008 NFL season", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.passing -> m.09tcd5t -> american_football.player_passing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.rushing -> m.07916fp -> american_football.player_rushing_statistics.season -> 2008 NFL season\n# Answer:\n2008 NFL season", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.passing -> m.07916ct -> american_football.player_passing_statistics.team -> Pittsburgh Steelers\n# Answer:\nPittsburgh Steelers", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.rushing -> m.09tcd8t -> american_football.player_rushing_statistics.team -> Pittsburgh Steelers\n# Answer:\nPittsburgh Steelers", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.rushing -> m.07mmgmj -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.passing -> m.07mmgkg -> american_football.player_passing_statistics.team -> Pittsburgh Steelers\n# Answer:\nPittsburgh Steelers"], "ground_truth": ["Miami University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1932", "prediction": ["# Reasoning Path:\nCanada -> royalty.kingdom.rulers -> Elizabeth II -> people.person.religion -> Protestantism\n# Answer:\nElizabeth II", "# Reasoning Path:\nCanada -> royalty.kingdom.rulers -> Elizabeth II -> people.person.nationality -> United Kingdom\n# Answer:\nElizabeth II", "# Reasoning Path:\nCanada -> royalty.kingdom.rulers -> Elizabeth II -> government.politician.government_positions_held -> m.04g8pm8\n# Answer:\nElizabeth II", "# Reasoning Path:\nCanada -> royalty.kingdom.rulers -> Elizabeth II -> people.person.religion -> Anglicanism\n# Answer:\nElizabeth II", "# Reasoning Path:\nCanada -> government.governmental_jurisdiction.governing_officials -> m.04kc_7_ -> government.government_position_held.governmental_body -> Parliament of Canada\n# Answer:\nParliament of Canada", "# Reasoning Path:\nCanada -> government.governmental_jurisdiction.governing_officials -> m.04kc_7_ -> government.government_position_held.office_position_or_title -> Prime Minister of Canada\n# Answer:\nPrime Minister of Canada", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> South Africa\n# Answer:\nArabic Language", "# Reasoning Path:\nCanada -> government.governmental_jurisdiction.governing_officials -> m.011ktc2z -> freebase.valuenotation.has_no_value -> Appointed By (if Position is Appointed)\n# Answer:\nAppointed By (if Position is Appointed)", "# Reasoning Path:\nCanada -> government.governmental_jurisdiction.governing_officials -> m.011ktc2z -> government.government_position_held.governmental_body -> Legislative Assembly of the Northwest Territories\n# Answer:\nLegislative Assembly of the Northwest Territories", "# Reasoning Path:\nCanada -> government.governmental_jurisdiction.governing_officials -> m.0_xt9b9 -> government.government_position_held.governmental_body -> Legislative Assembly of the Northwest Territories\n# Answer:\nLegislative Assembly of the Northwest Territories"], "ground_truth": ["Parliament of Canada", "House of Commons of Canada"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.16666666666666669, "ans_precission": 0.1, "ans_recall": 0.5, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 0.1, "path_ans_recall": 0.5}
{"id": "WebQTest-1933", "prediction": ["# Reasoning Path:\nAdam Morrison -> sports.drafted_athlete.drafted -> m.04_bzh1 -> sports.sports_league_draft_pick.draft -> 2006 NBA draft\n# Answer:\n2006 NBA draft", "# Reasoning Path:\nAdam Morrison -> sports.drafted_athlete.drafted -> m.04_bzh1 -> sports.sports_league_draft_pick.team -> Charlotte Hornets\n# Answer:\nCharlotte Hornets", "# Reasoning Path:\nAdam Morrison -> sports.drafted_athlete.drafted -> m.04_bzh1 -> sports.sports_league_draft_pick.school -> Gonzaga University\n# Answer:\nGonzaga University", "# Reasoning Path:\nAdam Morrison -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nAdam Morrison -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Alaska\n# Answer:\nUnited States of America", "# Reasoning Path:\nAdam Morrison -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nAdam Morrison -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Arizona\n# Answer:\nUnited States of America", "# Reasoning Path:\nAdam Morrison -> people.person.nationality -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nAdam Morrison -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Alaska\n# Answer:\nUnited States of America", "# Reasoning Path:\nAdam Morrison -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Arizona\n# Answer:\nUnited States of America"], "ground_truth": ["2006 NBA draft"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1934", "prediction": ["# Reasoning Path:\nMichael Jackson -> music.artist.origin -> Gary -> location.location.containedby -> United States of America\n# Answer:\nGary", "# Reasoning Path:\nMichael Jackson -> people.person.place_of_birth -> Gary -> location.location.containedby -> United States of America\n# Answer:\nGary", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> WNEW -> broadcast.content.location -> New York City\n# Answer:\nWNEW", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1.FM Absolute  90's -> common.topic.notable_types -> Broadcast Content\n# Answer:\n1.FM Absolute  90's", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> .977 The '80s Channel -> broadcast.content.genre -> Pop music\n# Answer:\n.977 The '80s Channel", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> .977 The '80s Channel -> common.topic.notable_types -> Broadcast Content\n# Answer:\n.977 The '80s Channel", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> WNEW -> common.topic.notable_types -> Broadcast Content\n# Answer:\nWNEW", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> .977 The '80s Channel -> broadcast.content.genre -> 70's\n# Answer:\n.977 The '80s Channel", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> WNEW -> broadcast.content.artist -> Janet Jackson\n# Answer:\nWNEW", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1.FM Absolute  90's -> common.topic.image -> 1-fm-correct2.PNG\n# Answer:\n1.FM Absolute  90's"], "ground_truth": ["Gary"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1935", "prediction": ["# Reasoning Path:\nMaya Moore -> people.person.education -> m.04mg02t -> education.education.institution -> University of Connecticut\n# Answer:\nUniversity of Connecticut", "# Reasoning Path:\nMaya Moore -> common.topic.notable_for -> g.125cckxj9\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMaya Moore -> people.person.education -> m.040ggtj -> education.education.institution -> Collins Hill High School\n# Answer:\nCollins Hill High School", "# Reasoning Path:\nMaya Moore -> base.schemastaging.athlete_extra.salary -> m.0nghzz3 -> base.schemastaging.athlete_salary.team -> Minnesota Lynx\n# Answer:\nMinnesota Lynx", "# Reasoning Path:\nMaya Moore -> base.schemastaging.athlete_extra.salary -> m.0nghzz3 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar"], "ground_truth": ["University of Connecticut"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1936", "prediction": ["# Reasoning Path:\nLos Angeles Dodgers -> sports.sports_team.championships -> 1988 World Series -> time.event.locations -> Dodger Stadium\n# Answer:\n1988 World Series", "# Reasoning Path:\nLos Angeles Dodgers -> sports.sports_team.championships -> 1988 World Series -> time.event.instance_of_recurring_event -> World Series\n# Answer:\n1988 World Series", "# Reasoning Path:\nLos Angeles Dodgers -> sports.sports_team.championships -> 1988 World Series -> time.event.locations -> Oakland-Alameda County Coliseum\n# Answer:\n1988 World Series", "# Reasoning Path:\nLos Angeles Dodgers -> sports.sports_team.championships -> 1981 World Series -> time.event.locations -> Yankee Stadium\n# Answer:\n1981 World Series", "# Reasoning Path:\nLos Angeles Dodgers -> award.award_nominee.award_nominations -> m.0sgq5v3 -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work", "# Reasoning Path:\nLos Angeles Dodgers -> baseball.baseball_team.team_stats -> m.05n69_9 -> baseball.baseball_team_stats.season -> 1965 Major League Baseball Season\n# Answer:\n1965 Major League Baseball Season", "# Reasoning Path:\nLos Angeles Dodgers -> sports.sports_team.championships -> 1981 World Series -> time.event.instance_of_recurring_event -> World Series\n# Answer:\n1981 World Series", "# Reasoning Path:\nLos Angeles Dodgers -> sports.sports_team.championships -> 1981 World Series -> time.event.locations -> Dodger Stadium\n# Answer:\n1981 World Series", "# Reasoning Path:\nLos Angeles Dodgers -> award.award_nominee.award_nominations -> m.0sgq5v3 -> award.award_nomination.ceremony -> 1989 Kids' Choice Awards\n# Answer:\n1989 Kids' Choice Awards", "# Reasoning Path:\nLos Angeles Dodgers -> sports.sports_team.championships -> 1959 World Series -> time.event.locations -> Los Angeles Memorial Coliseum\n# Answer:\n1959 World Series"], "ground_truth": ["1988 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1937", "prediction": ["# Reasoning Path:\nJames Cook -> people.person.profession -> Sailor -> fictional_universe.character_occupation.characters_with_this_occupation -> Jack Aubrey\n# Answer:\nSailor", "# Reasoning Path:\nJames Cook -> people.person.profession -> Sailor -> base.schemastaging.context_name.pronunciation -> g.125_nnp67\n# Answer:\nSailor", "# Reasoning Path:\nJames Cook -> people.person.profession -> Sailor -> fictional_universe.character_occupation.characters_with_this_occupation -> Alfred King\n# Answer:\nSailor", "# Reasoning Path:\nJames Cook -> people.person.profession -> Sailor -> base.descriptive_names.names.descriptive_name -> m.0105cyc4\n# Answer:\nSailor", "# Reasoning Path:\nJames Cook -> people.person.profession -> Sailor -> fictional_universe.character_occupation.characters_with_this_occupation -> Bussell\n# Answer:\nSailor", "# Reasoning Path:\nJames Cook -> people.person.profession -> Sailor -> base.descriptive_names.names.descriptive_name -> m.0105cyz5\n# Answer:\nSailor", "# Reasoning Path:\nJames Cook -> people.person.profession -> Explorer -> base.descriptive_names.names.descriptive_name -> m.0106bf9p\n# Answer:\nExplorer", "# Reasoning Path:\nJames Cook -> base.kwebbase.kwtopic.connections_to -> claudius ptolemy influenced captain james cook -> base.kwebbase.kwconnection.subject -> Ptolemy\n# Answer:\nclaudius ptolemy influenced captain james cook", "# Reasoning Path:\nJames Cook -> people.person.profession -> Sailor -> base.descriptive_names.names.descriptive_name -> m.0105d0d0\n# Answer:\nSailor", "# Reasoning Path:\nJames Cook -> people.person.profession -> Explorer -> common.topic.notable_types -> Profession\n# Answer:\nExplorer"], "ground_truth": ["Explorer", "Sailor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1938", "prediction": ["# Reasoning Path:\nRobin Williams -> people.person.spouse_s -> m.0j11z6t -> people.marriage.location_of_ceremony -> St. Helena\n# Answer:\nSt. Helena", "# Reasoning Path:\nRobin Williams -> people.person.children -> Zelda Rae Williams -> people.person.place_of_birth -> New York City\n# Answer:\nZelda Rae Williams", "# Reasoning Path:\nRobin Williams -> people.person.spouse_s -> m.0j11z6t -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nRobin Williams -> people.person.spouse_s -> m.0j11z6t -> people.marriage.spouse -> Susan Schneider\n# Answer:\nSusan Schneider", "# Reasoning Path:\nRobin Williams -> people.person.children -> Zak Williams -> people.person.place_of_birth -> San Francisco\n# Answer:\nZak Williams", "# Reasoning Path:\nRobin Williams -> people.person.spouse_s -> m.0h2n9t4 -> people.marriage.spouse -> Marsha Garces\n# Answer:\nMarsha Garces", "# Reasoning Path:\nRobin Williams -> people.person.spouse_s -> m.0hykk0l -> people.marriage.spouse -> Valerie Velardi\n# Answer:\nValerie Velardi", "# Reasoning Path:\nRobin Williams -> people.person.children -> Cody Alan Williams -> people.person.place_of_birth -> New York City\n# Answer:\nCody Alan Williams", "# Reasoning Path:\nRobin Williams -> people.person.spouse_s -> m.0h2n9t4 -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nRobin Williams -> people.person.children -> Zelda Rae Williams -> celebrities.celebrity.celebrity_friends -> m.02_9g30\n# Answer:\nZelda Rae Williams"], "ground_truth": ["St. Helena"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1939", "prediction": ["# Reasoning Path:\nCanada -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.12cp_k6ss\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nCanada -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> United Kingdom\n# Answer:\nParliamentary system", "# Reasoning Path:\nCanada -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.12tb6g_94\n# Answer:\nlocation.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.1hhc38hnd\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nCanada -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Australia\n# Answer:\nParliamentary system", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.1hhc3d2ty\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nCanada -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.1hhc39x0w\n# Answer:\nlocation.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp", "# Reasoning Path:\nCanada -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Belgium\n# Answer:\nParliamentary system", "# Reasoning Path:\nCanada -> location.country.form_of_government -> Federal monarchy -> government.form_of_government.countries -> Australia\n# Answer:\nFederal monarchy"], "ground_truth": ["Parliamentary system", "Constitutional monarchy", "Federal monarchy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-194", "prediction": ["# Reasoning Path:\nIroquois -> people.ethnicity.geographic_distribution -> Ontario -> location.province.capital -> Toronto\n# Answer:\nOntario", "# Reasoning Path:\nIroquois -> people.ethnicity.geographic_distribution -> Ontario -> location.location.containedby -> Canada\n# Answer:\nOntario", "# Reasoning Path:\nIroquois -> people.ethnicity.geographic_distribution -> Ontario -> location.administrative_division.country -> Canada\n# Answer:\nOntario", "# Reasoning Path:\nIroquois -> people.ethnicity.geographic_distribution -> Qu\u00e9bec -> location.administrative_division.country -> Canada\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nIroquois -> people.ethnicity.geographic_distribution -> Qu\u00e9bec -> base.aareas.schema.administrative_area.administrative_parent -> Canada\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nIroquois -> people.ethnicity.included_in_group -> Native Americans in the United States -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nNative Americans in the United States", "# Reasoning Path:\nIroquois -> people.ethnicity.geographic_distribution -> Qu\u00e9bec -> base.ontologies.ontology_instance.equivalent_instances -> m.09klhv1\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nIroquois -> people.ethnicity.included_in_group -> Indigenous peoples of the Americas -> people.ethnicity.geographic_distribution -> Central America\n# Answer:\nIndigenous peoples of the Americas", "# Reasoning Path:\nIroquois -> people.ethnicity.included_in_group -> Indigenous peoples of the Americas -> common.topic.subject_of -> Tribal Gathering 2014\n# Answer:\nIndigenous peoples of the Americas", "# Reasoning Path:\nIroquois -> people.ethnicity.included_in_group -> Indigenous peoples of the Americas -> people.ethnicity.geographic_distribution -> North America\n# Answer:\nIndigenous peoples of the Americas"], "ground_truth": ["Ontario", "Qu\u00e9bec"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1940", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Hinduism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.deities -> Yahweh\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.article -> m.055v4j\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Jainism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.image -> Ramapir01\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> common.topic.article -> m.0jfq\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Sikhism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> God -> common.topic.notable_for -> g.125d2mw4b\n# Answer:\nGod", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.image -> Ramapir22\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> religion.deity.deity_of -> Sunni Islam\n# Answer:\nAllah"], "ground_truth": ["Allah", "God", "Ramdev Pir"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1941", "prediction": ["# Reasoning Path:\nPeru -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> United States of America\n# Answer:\nPresidential system", "# Reasoning Path:\nPeru -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Spain\n# Answer:\nUnitary state", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.11b71n53cl\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nPeru -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Bolivia\n# Answer:\nUnitary state", "# Reasoning Path:\nPeru -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nPeru -> location.statistical_region.energy_use_per_capita -> g.1245_8rt3\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nPeru -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Chile\n# Answer:\nUnitary state", "# Reasoning Path:\nPeru -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Argentina\n# Answer:\nPresidential system", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.1hhc37ctc\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nPeru -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Argentina\n# Answer:\nConstitutional republic"], "ground_truth": ["Unitary state", "Presidential system", "Republic", "Constitutional republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.7241379310344827, "path_precision": 0.7, "path_recall": 0.75, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1942", "prediction": ["# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Yitzhak Rabin -> base.crime.crime_victim.crime_type -> Assassination\n# Answer:\nYitzhak Rabin", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Yitzhak Rabin -> people.person.gender -> Male\n# Answer:\nYitzhak Rabin", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Yitzhak Rabin -> base.nobelprizes.nobel_prize_winner.nobel_honor -> m.065nt14\n# Answer:\nYitzhak Rabin", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Menachem Begin -> people.person.profession -> Politician\n# Answer:\nMenachem Begin", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Menachem Begin -> military.military_person.service -> m.059d8ry\n# Answer:\nMenachem Begin", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Ariel Sharon -> military.military_person.participated_in_conflicts -> 1948 Arab\u2013Israeli War\n# Answer:\nAriel Sharon", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Ariel Sharon -> people.person.sibling_s -> m.0pdsh17\n# Answer:\nAriel Sharon", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Menachem Begin -> book.author.works_written -> White Nights: The story of a prisoner in Russia\n# Answer:\nMenachem Begin", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Ariel Sharon -> military.military_person.participated_in_conflicts -> 1948 Palestine war\n# Answer:\nAriel Sharon", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> book.book_subject.works -> 1948 and After -> book.written_work.author -> Benny Morris\n# Answer:\n1948 and After"], "ground_truth": ["Dalal Mughrabi", "Yitzhak Rabin", "David Ben-Gurion", "Anwar Sadat", "Ahmed Qurei", "Abdullah of Saudi Arabia", "Marwan Barghouti", "Yasser Arafat", "Anthony Zinni", "Hussein of Jordan", "Ahmad Shukeiri", "Menachem Begin", "Nabil Shaath", "Colin Powell", "Ahmed Yassin", "Shimon Peres", "Hanan Ashrawi", "Mahmoud Abbas", "Ariel Sharon", "Chaim Weizmann", "Haj Amin al-Husseini"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0.24657534246575338, "ans_precission": 0.9, "ans_recall": 0.14285714285714285, "path_f1": 0.24657534246575338, "path_precision": 0.9, "path_recall": 0.14285714285714285, "path_ans_f1": 0.24657534246575338, "path_ans_precision": 0.9, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-1943", "prediction": ["# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Lamar Hunt -> people.person.parents -> H. L. Hunt\n# Answer:\nLamar Hunt", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Lamar Hunt -> sports.pro_athlete.teams -> m.0hsx4r3\n# Answer:\nLamar Hunt", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Lamar Hunt -> common.topic.notable_types -> American football player\n# Answer:\nLamar Hunt", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Bud Adams -> award.award_winner.awards_won -> m.0_qxnkc\n# Answer:\nBud Adams", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Bud Adams -> organization.organization_founder.organizations_founded -> Adams Resources & Energy\n# Answer:\nBud Adams", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Bud Adams -> people.deceased_person.cause_of_death -> Natural causes\n# Answer:\nBud Adams", "# Reasoning Path:\nAmerican Football League -> sports.sports_league.sport -> American football -> sports.sport.team_coaches -> Ken Whisenhunt\n# Answer:\nAmerican football", "# Reasoning Path:\nAmerican Football League -> sports.sports_league.teams -> m.0crthm0 -> sports.sports_league_participation.team -> Oakland Raiders\n# Answer:\nOakland Raiders", "# Reasoning Path:\nAmerican Football League -> sports.sports_league.sport -> American football -> sports.sport.team_coaches -> Bob Hewko\n# Answer:\nAmerican football", "# Reasoning Path:\nAmerican Football League -> sports.sports_league.sport -> American football -> film.film_subject.films -> A Saintly Switch\n# Answer:\nAmerican football"], "ground_truth": ["Bud Adams", "Lamar Hunt"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1944", "prediction": ["# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> location.location.nearby_airports -> Adolfo Su\u00e1rez Madrid\u2013Barajas Airport\n# Answer:\nMadrid", "# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> location.location.nearby_airports -> Cuatro Vientos Airport\n# Answer:\nMadrid", "# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> common.topic.webpage -> m.03l85_5\n# Answer:\nMadrid", "# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> location.location.nearby_airports -> Getafe Air Base\n# Answer:\nMadrid", "# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> common.topic.webpage -> m.052jg5b\n# Answer:\nMadrid", "# Reasoning Path:\nDiego Vel\u00e1zquez -> base.kwebbase.kwtopic.has_sentences -> After meeting Rubens and working with him during his nine-month stay in Spain, Vel\u00e1zquez requested  time off from his duties to the royal court so he could visit Italy, for more study, and to collect art for the king's growing collection. -> base.kwebbase.kwsentence.next_sentence -> While in Italy, from August 1629 until 1631, Vel\u00e1zquez studied works by Tintoretto, Titian and Veronese.\n# Answer:\nAfter meeting Rubens and working with him during his nine-month stay in Spain, Vel\u00e1zquez requested  time off from his duties to the royal court so he could visit Italy, for more study, and to collect art for the king's growing collection.", "# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> travel.travel_destination.tourist_attractions -> Museo Nacional Del Prado\n# Answer:\nMadrid", "# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> common.topic.webpage -> m.09wz7pm\n# Answer:\nMadrid", "# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> travel.travel_destination.tourist_attractions -> Almudena Cathedral\n# Answer:\nMadrid", "# Reasoning Path:\nDiego Vel\u00e1zquez -> visual_art.visual_artist.associated_periods_or_movements -> Baroque -> book.school_or_movement.associated_authors -> Zaharije Orfelin\n# Answer:\nBaroque"], "ground_truth": ["Madrid"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1946", "prediction": ["# Reasoning Path:\nMexico -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nMexico -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> United States of America\n# Answer:\nFederal republic", "# Reasoning Path:\nMexico -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Guatemala\n# Answer:\nConstitutional republic", "# Reasoning Path:\nMexico -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> United States of America\n# Answer:\nPresidential system", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_0s31\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_38p7\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guatemala\n# Answer:\nPresidential system", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_569j\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita"], "ground_truth": ["mx"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1947", "prediction": ["# Reasoning Path:\nSwitzerland -> location.country.currency_used -> Swiss franc -> finance.currency.countries_used -> Liechtenstein\n# Answer:\nSwiss franc", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d0tc3g\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nSwitzerland -> location.statistical_region.co2_emissions_per_capita -> g.1245_dgvf\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6p27nwl\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.1hhc3d66v\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nSwitzerland -> location.statistical_region.co2_emissions_per_capita -> g.1245_kjqq\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nSwitzerland -> location.statistical_region.co2_emissions_per_capita -> g.1245_kjqr\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita"], "ground_truth": ["Swiss franc"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-195", "prediction": ["# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Fender Stratocaster -> music.guitar.guitarists -> Tom Petty\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Fender Stratocaster -> common.topic.notable_types -> Guitar\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Fender Stratocaster -> music.guitar.guitarists -> The Edge\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nGeorge Harrison -> music.group_member.instruments_played -> Bass guitar -> common.topic.notable_types -> Musical instrument\n# Answer:\nBass guitar", "# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Rickenbacker 360/12 -> music.guitar.guitarists -> Tom Petty\n# Answer:\nRickenbacker 360/12", "# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Rickenbacker 360/12 -> common.topic.image -> Rickenbacker 360-12WB 12 String\n# Answer:\nRickenbacker 360/12", "# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Fender Stratocaster -> music.guitar.guitarists -> Eric Clapton\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Rickenbacker 360/12 -> music.guitar.guitarists -> The Edge\n# Answer:\nRickenbacker 360/12", "# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Rickenbacker 360/12 -> common.topic.notable_for -> g.125cy8mvw\n# Answer:\nRickenbacker 360/12", "# Reasoning Path:\nGeorge Harrison -> music.group_member.instruments_played -> Bass guitar -> common.topic.subject_of -> Reverb\n# Answer:\nBass guitar"], "ground_truth": ["Rickenbacker 360/12", "Fender Stratocaster"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1950", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_burial -> Arlington National Cemetery -> travel.tourist_attraction.near_travel_destination -> Arlington County\n# Answer:\nArlington National Cemetery", "# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_burial -> Arlington National Cemetery -> location.location.containedby -> Virginia\n# Answer:\nArlington National Cemetery", "# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_burial -> Arlington National Cemetery -> base.schemastaging.context_name.pronunciation -> g.125_rp7jg\n# Answer:\nArlington National Cemetery", "# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_burial -> Arlington National Cemetery -> location.location.containedby -> Arlington County\n# Answer:\nArlington National Cemetery", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr. -> base.popstra.celebrity.friendship -> m.0645k94\n# Answer:\nJohn F. Kennedy Jr.", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> White people -> medicine.risk_factor.diseases -> Breast cancer\n# Answer:\nWhite people", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> time.event.locations -> Dealey Plaza\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School -> common.topic.notable_for -> g.125583hnb\n# Answer:\nBloomington Kennedy High School", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nIrish American", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr. -> base.popstra.celebrity.dated -> m.063t10x\n# Answer:\nJohn F. Kennedy Jr."], "ground_truth": ["Arlington National Cemetery"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1952", "prediction": ["# Reasoning Path:\nMuhammad in Islam -> people.person.place_of_birth -> Mecca -> religion.place_of_worship.religion -> Islam\n# Answer:\nMecca", "# Reasoning Path:\nMuhammad in Islam -> people.person.place_of_birth -> Mecca -> location.location.containedby -> Sharifate of Mecca\n# Answer:\nMecca", "# Reasoning Path:\nMuhammad in Islam -> people.person.place_of_birth -> Mecca -> location.location.containedby -> Saudi Arabia\n# Answer:\nMecca", "# Reasoning Path:\nMuhammad in Islam -> people.person.place_of_birth -> Mecca -> location.location.contains -> Umm al-Qura University\n# Answer:\nMecca", "# Reasoning Path:\nMuhammad in Islam -> influence.influence_node.influenced -> Muhammad ibn Ya'qub al-Kulayni -> influence.influence_node.influenced_by -> Ibn Babawayh\n# Answer:\nMuhammad ibn Ya'qub al-Kulayni", "# Reasoning Path:\nMuhammad in Islam -> book.author.works_written -> Sayings of Mohammed -> common.topic.notable_types -> Book\n# Answer:\nSayings of Mohammed", "# Reasoning Path:\nMuhammad in Islam -> people.person.place_of_birth -> Mecca -> location.location.contains -> Abraj Al Bait\n# Answer:\nMecca", "# Reasoning Path:\nMuhammad in Islam -> book.author.works_written -> Sayings of Mohammed -> common.topic.notable_for -> g.125gry845\n# Answer:\nSayings of Mohammed", "# Reasoning Path:\nMuhammad in Islam -> influence.influence_node.influenced -> Abd Allah ibn Abbas -> people.person.place_of_birth -> Mecca\n# Answer:\nAbd Allah ibn Abbas", "# Reasoning Path:\nMuhammad in Islam -> people.person.place_of_birth -> Mecca -> location.location.contains -> Abraj Al Bait Hajar Tower\n# Answer:\nMecca"], "ground_truth": ["Mecca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1954", "prediction": ["# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.countries_spoken_in -> Russia\n# Answer:\nUkrainian Language", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.countries_spoken_in -> Second Polish Republic\n# Answer:\nUkrainian Language", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.region -> Europe\n# Answer:\nUkrainian Language", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Romanian Language -> language.human_language.countries_spoken_in -> Romania\n# Answer:\nRomanian Language", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Hungarian language -> language.human_language.countries_spoken_in -> Hungary\n# Answer:\nHungarian language", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Hungarian language -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nHungarian language", "# Reasoning Path:\nUkraine -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6dc92bn\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.countries_spoken_in -> Czech Republic\n# Answer:\nUkrainian Language", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Hungarian language -> language.human_language.countries_spoken_in -> Romania\n# Answer:\nHungarian language", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Hungarian language -> common.topic.notable_types -> Human Language\n# Answer:\nHungarian language"], "ground_truth": ["Hungarian language", "Russian Language", "Moldovan language", "Albanian language", "Tatar Language", "Ukrainian Language", "Romanian Language"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5806451612903225, "ans_precission": 0.9, "ans_recall": 0.42857142857142855, "path_f1": 0.5806451612903225, "path_precision": 0.9, "path_recall": 0.42857142857142855, "path_ans_f1": 0.5806451612903225, "path_ans_precision": 0.9, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1955", "prediction": ["# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.written_work.author -> L. A. Scot Powe\n# Answer:\nThe Warren Court and American Politics", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Making Our Democracy Work -> book.written_work.author -> Stephen Breyer\n# Answer:\nMaking Our Democracy Work", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> people.appointed_role.appointment -> m.0hn35mm\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.written_work.author -> Lucas A. Powe\n# Answer:\nThe Warren Court and American Politics", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.written_work.subjects -> Politics of the United States\n# Answer:\nThe Warren Court and American Politics", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> law.judicial_title.judges -> m.046x3bs\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.book.genre -> Non-fiction\n# Answer:\nThe Warren Court and American Politics", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Making Our Democracy Work -> book.written_work.original_language -> English Language\n# Answer:\nMaking Our Democracy Work", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Courtiers of the Marble Palace: The Rise and Influence of the Supreme Court Law Clerk -> book.written_work.subjects -> Law clerk\n# Answer:\nCourtiers of the Marble Palace: The Rise and Influence of the Supreme Court Law Clerk", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.written_work.subjects -> Warren Court\n# Answer:\nThe Warren Court and American Politics"], "ground_truth": ["Stephen Breyer", "Clarence Thomas", "Antonin Scalia", "Anthony Kennedy", "Ruth Bader Ginsburg", "Sonia Sotomayor", "Elena Kagan", "John Roberts", "Samuel Alito"], "ans_acc": 0.1111111111111111, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.10526315789473685, "path_ans_precision": 0.1, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-1956", "prediction": ["# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> United Kingdom -> location.location.containedby -> Eurasia\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nEurope -> location.location.partially_contains -> Kazakhstan -> location.location.partially_contains -> Ural River\n# Answer:\nKazakhstan", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> United Kingdom -> location.country.first_level_divisions -> England\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Russia -> location.location.partially_containedby -> Asia\n# Answer:\nRussia", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> United Kingdom -> location.location.containedby -> Western Europe\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nEurope -> location.location.partially_contains -> Kazakhstan -> location.location.partially_contains -> Caspian Sea\n# Answer:\nKazakhstan", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> United Kingdom -> base.aareas.schema.administrative_area.administrative_area_type -> Sovereign state\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nEurope -> location.location.partially_contains -> Russia -> location.location.partially_containedby -> Asia\n# Answer:\nRussia", "# Reasoning Path:\nEurope -> location.location.partially_contains -> Kazakhstan -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics\n# Answer:\nKazakhstan", "# Reasoning Path:\nEurope -> location.location.partially_contains -> Armenia -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics\n# Answer:\nArmenia"], "ground_truth": ["Russia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1957", "prediction": ["# Reasoning Path:\nKaley Cuoco -> tv.tv_actor.starring_roles -> m.0y4y083 -> tv.regular_tv_appearance.series -> First Monday\n# Answer:\nFirst Monday", "# Reasoning Path:\nKaley Cuoco -> tv.tv_actor.starring_roles -> m.0y4y083 -> tv.regular_tv_appearance.character -> Alyssa\n# Answer:\nAlyssa", "# Reasoning Path:\nKaley Cuoco -> tv.tv_actor.starring_roles -> m.03jqc1n -> tv.regular_tv_appearance.seasons -> Charmed Season 8\n# Answer:\nCharmed Season 8", "# Reasoning Path:\nKaley Cuoco -> tv.tv_actor.starring_roles -> m.03jrq66 -> tv.regular_tv_appearance.seasons -> The Big Bang Theory - Season 6\n# Answer:\nThe Big Bang Theory - Season 6", "# Reasoning Path:\nKaley Cuoco -> film.actor.film -> g.11b8czsxkc\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nKaley Cuoco -> tv.tv_actor.starring_roles -> m.03jrq66 -> tv.regular_tv_appearance.seasons -> The Big Bang Theory - Season 5\n# Answer:\nThe Big Bang Theory - Season 5", "# Reasoning Path:\nKaley Cuoco -> tv.tv_actor.starring_roles -> m.03jqc1n -> tv.regular_tv_appearance.character -> Billie Jenkins\n# Answer:\nBillie Jenkins", "# Reasoning Path:\nKaley Cuoco -> award.award_nominee.award_nominations -> m.010wr05z -> award.award_nomination.award -> Teen Choice Award for Choice TV Actress: Comedy\n# Answer:\nTeen Choice Award for Choice TV Actress: Comedy", "# Reasoning Path:\nKaley Cuoco -> tv.tv_actor.starring_roles -> m.03jqc1n -> tv.regular_tv_appearance.series -> Charmed\n# Answer:\nCharmed", "# Reasoning Path:\nKaley Cuoco -> tv.tv_actor.starring_roles -> m.03jrq66 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["2005-09-25"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1958", "prediction": ["# Reasoning Path:\nChris Rock -> people.person.place_of_birth -> Andrews -> location.location.containedby -> United States of America\n# Answer:\nAndrews", "# Reasoning Path:\nChris Rock -> people.person.place_of_birth -> Andrews -> common.topic.notable_for -> g.125gzsc2j\n# Answer:\nAndrews", "# Reasoning Path:\nChris Rock -> people.person.place_of_birth -> Andrews -> location.statistical_region.population -> g.11b674j202\n# Answer:\nAndrews", "# Reasoning Path:\nChris Rock -> people.person.place_of_birth -> Andrews -> location.location.containedby -> Georgetown County\n# Answer:\nAndrews", "# Reasoning Path:\nChris Rock -> people.person.place_of_birth -> Andrews -> location.statistical_region.population -> g.11x1clr4y\n# Answer:\nAndrews", "# Reasoning Path:\nChris Rock -> people.person.place_of_birth -> Andrews -> location.location.containedby -> South Carolina\n# Answer:\nAndrews", "# Reasoning Path:\nChris Rock -> people.person.place_of_birth -> Andrews -> location.statistical_region.population -> g.11x1cr6w1\n# Answer:\nAndrews", "# Reasoning Path:\nChris Rock -> award.award_winner.awards_won -> m.0sxcwkq -> award.award_honor.award -> Primetime Emmy Award for Outstanding Writing - Variety Series\n# Answer:\nPrimetime Emmy Award for Outstanding Writing - Variety Series", "# Reasoning Path:\nChris Rock -> music.featured_artist.recordings -> You Ain't Gotta Lie (Ta Kick It) -> music.recording.releases -> War & Peace, Volume 2: The Peace Disc\n# Answer:\nYou Ain't Gotta Lie (Ta Kick It)", "# Reasoning Path:\nChris Rock -> music.featured_artist.recordings -> Everybody Hates Chris -> common.topic.notable_for -> g.126sgqhnt\n# Answer:\nEverybody Hates Chris"], "ground_truth": ["Andrews"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-196", "prediction": ["# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> New Zealand\n# Answer:\nEnglish Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> English Language -> education.school_category.schools_of_this_kind -> Piers Midwinter\n# Answer:\nEnglish Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> Navajo Language -> base.rosetta.languoid.document -> Navajo\n# Answer:\nNavajo Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> English Language -> common.topic.subject_of -> Embassy English\n# Answer:\nEnglish Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> Spanish Language -> base.rosetta.languoid.local_name -> Spanish\n# Answer:\nSpanish Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> Navajo Language -> language.human_language.language_family -> Athabaskan languages\n# Answer:\nNavajo Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> English Language -> common.topic.subject_of -> Everything TEFL LTD\n# Answer:\nEnglish Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> Navajo Language -> common.topic.notable_types -> Human Language\n# Answer:\nNavajo Language"], "ground_truth": ["Navajo Language", "English Language", "Spanish Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1960", "prediction": ["# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.championships -> 1967 Stanley Cup Finals -> sports.sports_championship_event.runner_up -> Montreal Canadiens\n# Answer:\n1967 Stanley Cup Finals", "# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.championships -> 1967 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n1967 Stanley Cup Finals", "# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.championships -> 1932 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n1932 Stanley Cup Finals", "# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.arena_stadium -> Air Canada Centre -> sports.sports_facility.teams -> Toronto Rock\n# Answer:\nAir Canada Centre", "# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.championships -> 1942 Stanley Cup Finals -> common.topic.article -> m.03cf4pr\n# Answer:\n1942 Stanley Cup Finals", "# Reasoning Path:\nToronto Maple Leafs -> ice_hockey.hockey_team.conference -> Eastern Conference -> ice_hockey.hockey_conference.teams -> Philadelphia Flyers\n# Answer:\nEastern Conference", "# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.championships -> 1932 Stanley Cup Finals -> common.topic.notable_for -> g.125fpr8l6\n# Answer:\n1932 Stanley Cup Finals", "# Reasoning Path:\nToronto Maple Leafs -> ice_hockey.hockey_team.conference -> Eastern Conference -> common.topic.notable_types -> Ice hockey conference\n# Answer:\nEastern Conference", "# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.championships -> 1932 Stanley Cup Finals -> sports.sports_championship_event.season -> 1931\u201332 NHL season\n# Answer:\n1932 Stanley Cup Finals", "# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.championships -> 1942 Stanley Cup Finals -> sports.sports_championship_event.season -> 1941\u201342 NHL season\n# Answer:\n1942 Stanley Cup Finals"], "ground_truth": ["1967 Stanley Cup Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1961", "prediction": ["# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.region -> Africa\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> base.rosetta.languoid.parent -> Arabic Group\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.writing_system -> Arabic alphabet\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Domari Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nDomari Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.writing_system -> Arabic alphabet\n# Answer:\nArabic Language", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Domari Language -> common.topic.notable_types -> Human Language\n# Answer:\nDomari Language", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Sudan\n# Answer:\nArabic Language"], "ground_truth": ["Modern Standard Arabic"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1962", "prediction": ["# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Brazilian Portuguese -> language.language_dialect.language -> Portuguese Language\n# Answer:\nBrazilian Portuguese", "# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Portuguese Language -> language.human_language.dialects -> Brazilian Portuguese\n# Answer:\nPortuguese Language", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc39sjp\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fm01\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc385hl\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp"], "ground_truth": ["Portuguese Language", "Brazilian Portuguese", "Italian Language"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.36363636363636365, "ans_precission": 0.25, "ans_recall": 0.6666666666666666, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 0.36363636363636365, "path_ans_precision": 0.25, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1963", "prediction": ["# Reasoning Path:\nVince Lombardi -> sports.pro_athlete.sports_played_professionally -> m.0d5jg18 -> sports.pro_sports_played.sport -> American football\n# Answer:\nAmerican football", "# Reasoning Path:\nVince Lombardi -> people.person.quotations -> A man can be as great as he wants to be. If you believe in yourself and have the courage, the determination, the dedication, the competitive drive and if you are willing to sacrifice the little things in life and pay the price for the things that are worthwhile, it can be done. -> media_common.quotation.subjects -> Possibilities\n# Answer:\nA man can be as great as he wants to be. If you believe in yourself and have the courage, the determination, the dedication, the competitive drive and if you are willing to sacrifice the little things in life and pay the price for the things that are worthwhile, it can be done.", "# Reasoning Path:\nVince Lombardi -> film.actor.film -> m.0gchg4q -> film.performance.film -> Paper Lion\n# Answer:\nPaper Lion", "# Reasoning Path:\nVince Lombardi -> people.person.quotations -> A man can be as great as he wants to be. If you believe in yourself and have the courage, the determination, the dedication, the competitive drive and if you are willing to sacrifice the little things in life and pay the price for the things that are worthwhile, it can be done. -> common.topic.notable_types -> Quotation\n# Answer:\nA man can be as great as he wants to be. If you believe in yourself and have the courage, the determination, the dedication, the competitive drive and if you are willing to sacrifice the little things in life and pay the price for the things that are worthwhile, it can be done.", "# Reasoning Path:\nVince Lombardi -> people.person.quotations -> A school without football is in danger of deteriorating into a medieval study hall. -> media_common.quotation.subjects -> Football\n# Answer:\nA school without football is in danger of deteriorating into a medieval study hall.", "# Reasoning Path:\nVince Lombardi -> people.person.quotations -> A school without football is in danger of deteriorating into a medieval study hall. -> common.topic.notable_types -> Quotation\n# Answer:\nA school without football is in danger of deteriorating into a medieval study hall.", "# Reasoning Path:\nVince Lombardi -> people.person.quotations -> After the cheers have died down and the stadium is empty, after the headlines have been written and after you are back in the quiet of your room and the championship ring has been placed on the dresser and all the pomp and fanfare has faded, the enduring things that are left are: the dedication to excellence, the dedication to victory, and the dedication to doing with our lives the very best we can to make the world a better place in which to live. -> media_common.quotation.subjects -> Dedication\n# Answer:\nAfter the cheers have died down and the stadium is empty, after the headlines have been written and after you are back in the quiet of your room and the championship ring has been placed on the dresser and all the pomp and fanfare has faded, the enduring things that are left are: the dedication to excellence, the dedication to victory, and the dedication to doing with our lives the very best we can to make the world a better place in which to live.", "# Reasoning Path:\nVince Lombardi -> film.actor.film -> m.0ggffqj -> film.performance.film -> Second Effort\n# Answer:\nSecond Effort"], "ground_truth": ["Right guard"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1964", "prediction": ["# Reasoning Path:\nA Streetcar Named Desire -> film.film.starring -> m.0w17hf6 -> film.performance.actor -> Dahn Ben-Amotz\n# Answer:\nDahn Ben-Amotz", "# Reasoning Path:\nStanley Kowalski -> film.film_character.portrayed_in_films -> m.0jsxq4 -> film.performance.actor -> Marlon Brando\n# Answer:\nMarlon Brando", "# Reasoning Path:\nA Streetcar Named Desire -> film.film.starring -> m.0w17hf6 -> film.performance.special_performance_type -> Uncredited\n# Answer:\nUncredited", "# Reasoning Path:\nStanley Kowalski -> film.film_character.portrayed_in_films -> m.0jsxq4 -> film.performance.film -> A Streetcar Named Desire\n# Answer:\nfilm.performance.film", "# Reasoning Path:\nA Streetcar Named Desire -> film.film.starring -> m.03l101c -> film.performance.actor -> Alec Baldwin\n# Answer:\nAlec Baldwin", "# Reasoning Path:\nStanley Kowalski -> film.film_character.portrayed_in_films -> m.03l101c -> film.performance.actor -> Alec Baldwin\n# Answer:\nAlec Baldwin", "# Reasoning Path:\nStanley Kowalski -> common.topic.article -> m.03bhwn\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nA Streetcar Named Desire -> film.film.starring -> m.03l1032 -> film.performance.actor -> Treat Williams\n# Answer:\nTreat Williams", "# Reasoning Path:\nStanley Kowalski -> film.film_character.portrayed_in_films -> m.03l1032 -> film.performance.actor -> Treat Williams\n# Answer:\nTreat Williams", "# Reasoning Path:\nA Streetcar Named Desire -> award.award_nominated_work.award_nominations -> m.03mlrqc -> award.award_nomination.award_nominee -> Marlon Brando\n# Answer:\nMarlon Brando"], "ground_truth": ["Marlon Brando"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.26666666666666666, "path_precision": 0.2, "path_recall": 0.4, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1965", "prediction": ["# Reasoning Path:\nLola Bunny -> tv.tv_character.appeared_in_tv_program -> m.0gl1bzt -> tv.regular_tv_appearance.actor -> Kristen Wiig\n# Answer:\nKristen Wiig", "# Reasoning Path:\nLola Bunny -> film.film_character.portrayed_in_films -> m.0qst5_z -> film.performance.actor -> Britt McKillip\n# Answer:\nBritt McKillip", "# Reasoning Path:\nLola Bunny -> common.topic.article -> m.0534rg\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nLola Bunny -> tv.tv_character.appeared_in_tv_program -> m.0gl1bzt -> tv.regular_tv_appearance.series -> The Looney Tunes Show\n# Answer:\nThe Looney Tunes Show", "# Reasoning Path:\nLola Bunny -> tv.tv_character.appeared_in_tv_program -> m.03lyj7h -> tv.regular_tv_appearance.actor -> Britt McKillip\n# Answer:\nBritt McKillip", "# Reasoning Path:\nLola Bunny -> tv.tv_character.appeared_in_tv_program -> m.03lyj7h -> tv.regular_tv_appearance.series -> Baby Looney Tunes\n# Answer:\nBaby Looney Tunes", "# Reasoning Path:\nLola Bunny -> film.film_character.portrayed_in_films -> m.0qst5_z -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nLola Bunny -> film.film_character.portrayed_in_films -> m.0y7rs3h -> film.performance.actor -> Kath Soucie\n# Answer:\nKath Soucie", "# Reasoning Path:\nLola Bunny -> film.film_character.portrayed_in_films -> m.0qst5_z -> film.performance.film -> Baby Looney Tunes' Eggs-traordinary Adventure\n# Answer:\nBaby Looney Tunes' Eggs-traordinary Adventure", "# Reasoning Path:\nLola Bunny -> film.film_character.portrayed_in_films -> m.0y7rs3h -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice"], "ground_truth": ["Britt McKillip", "Kath Soucie", "Kristen Wiig"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1966", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin -> book.author.works_written -> The Autobiography of Charles Darwin\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> award.award_discipline.awards_in_this_discipline -> Copley Medal\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> people.person.children -> William Erasmus Darwin -> people.person.gender -> Male\n# Answer:\nWilliam Erasmus Darwin", "# Reasoning Path:\nCharles Darwin -> base.concepts.concept_developer.concepts_developed -> Natural selection -> book.book_subject.works -> Darwin Loves You: Natural Selection and the Re-Enchantment of the World\n# Answer:\nNatural selection", "# Reasoning Path:\nCharles Darwin -> base.concepts.concept_developer.concepts_developed -> Natural selection -> common.topic.notable_for -> g.12589mw_3\n# Answer:\nNatural selection", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> education.field_of_study.subdisciplines -> Evolution\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.country -> United Kingdom\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Great Britain -> location.location.contains -> London\n# Answer:\nGreat Britain"], "ground_truth": ["Cartas de Darwin 18251859", "To the members of the Down Friendly Club", "Les moyens d'expression chez les animaux", "The action of carbonate of ammonia on the roots of certain plants", "The Correspondence of Charles Darwin, Volume 12: 1864", "Notebooks on transmutation of species", "Part I: Contributions to the Theory of Natural Selection / Part II", "Darwin Darwin", "Charles Darwin's marginalia", "The Correspondence of Charles Darwin, Volume 8: 1860", "Darwin's notebooks on transmutation of species", "The geology of the voyage of H.M.S. Beagle", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Evolution by natural selection", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "vari\u00eberen der huisdieren en cultuurplanten", "The Correspondence of Charles Darwin, Volume 11: 1863", "Charles Darwin", "The Different Forms of Flowers on Plants of the Same Species", "The Correspondence of Charles Darwin, Volume 16: 1868", "Tesakneri tsagume\u030c", "The\u0301orie de l'e\u0301volution", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The Correspondence of Charles Darwin, Volume 15: 1867", "Geological Observations on South America", "On evolution", "Diary of the voyage of H.M.S. Beagle", "Darwin Compendium", "The Voyage of the Beagle", "Leben und Briefe von Charles Darwin", "The Orgin of Species", "Darwin", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "La vie et la correspondance de Charles Darwin", "Questions about the breeding of animals", "The Life of Erasmus Darwin", "Darwinism stated by Darwin himself", "Resa kring jorden", "Gesammelte kleinere Schriften", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "From Darwin's unpublished notebooks", "Del Plata a Tierra del Fuego", "Evolution and natural selection", "On the origin of species by means of natural selection", "On the tendency of species to form varieties", "Die fundamente zur entstehung der arten", "Darwin's journal", "The Correspondence of Charles Darwin, Volume 17: 1869", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The Darwin Reader First Edition", "The portable Darwin", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "The Formation of Vegetable Mould through the Action of Worms", "genese\u014ds t\u014dn eid\u014dn", "The Correspondence of Charles Darwin, Volume 10: 1862", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "The Correspondence of Charles Darwin, Volume 14: 1866", "The Descent of Man, and Selection in Relation to Sex", "Reise um die Welt 1831 - 36", "The living thoughts of Darwin", "Notes on the fertilization of orchids", "Rejse om jorden", "Beagle letters", "A student's introduction to Charles Darwin", "Charles Darwin's natural selection", "Darwin and Henslow", "The Darwin Reader Second Edition", "Fertilisation of Orchids", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Diario del Viaje de Un Naturalista Alrededor", "Insectivorous Plants", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "Proiskhozhdenie vidov", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "The Correspondence of Charles Darwin, Volume 9: 1861", "Darwin for Today", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Darwin's insects", "La facult\u00e9 motrice dans les plantes", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Structure and Distribution of Coral Reefs", "The voyage of Charles Darwin", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "The Power of Movement in Plants", "The principal works", "Origins", "Works", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "The Autobiography of Charles Darwin", "A Darwin Selection", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The foundations of the Origin of species", "Darwin on humus and the earthworm", "Volcanic Islands", "Motsa ha-minim", "Darwin's Ornithological notes", "The Correspondence of Charles Darwin, Volume 13: 1865", "Kleinere geologische Abhandlungen", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "Charles Darwin's letters", "Metaphysics, Materialism, & the evolution of mind", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Reise eines Naturforschers um die Welt", "More Letters of Charles Darwin", "From so simple a beginning", "ontstaan der soorten door natuurlijke teeltkeus", "Die geschlechtliche Zuchtwahl", "Darwin en Patagonia", "On Natural Selection", "On a remarkable bar of sandstone off Pernambuco", "H.M.S. Beagle in South America", "monograph on the sub-class Cirripedia", "Darwin-Wallace", "El Origin De Las Especies", "Wu zhong qi yuan", "On the Movements and Habits of Climbing Plants", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "Evolution", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "The collected papers of Charles Darwin", "The Correspondence of Charles Darwin, Volume 18: 1870", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "red notebook of Charles Darwin", "The Expression of the Emotions in Man and Animals", "Voyage d'un naturaliste autour du monde", "Charles Darwin on the routes of male humble bees", "Memorias y epistolario i\u0301ntimo", "The Essential Darwin", "Human nature, Darwin's view", "The education of Darwin", "Les mouvements et les habitudes des plantes grimpantes", "The Variation of Animals and Plants under Domestication", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Het uitdrukken van emoties bij mens en dier", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Opsht\u0323amung fun menshen"], "ans_acc": 0.035211267605633804, "ans_hit": 1, "ans_f1": 0.02631578947368421, "ans_precission": 0.2, "ans_recall": 0.014084507042253521, "path_f1": 0.13793103448275862, "path_precision": 1.0, "path_recall": 0.07407407407407407, "path_ans_f1": 0.06802721088435375, "path_ans_precision": 1.0, "path_ans_recall": 0.035211267605633804}
{"id": "WebQTest-1967", "prediction": ["# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbs -> people.ethnicity.languages_spoken -> Serbian language -> language.human_language.countries_spoken_in -> Serbia and Montenegro\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Serbia and Montenegro\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbs -> people.ethnicity.languages_spoken -> Serbian language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> South Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> base.rosetta.languoid.local_name -> Serbian\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbs -> people.ethnicity.languages_spoken -> Serbian language -> language.human_language.countries_spoken_in -> Bosnia and Herzegovina\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbs -> people.ethnicity.languages_spoken -> Serbian language -> language.human_language.language_family -> Slavic languages\n# Answer:\nSerbian language"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1968", "prediction": ["# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> common.topic.webpage -> m.09w021v\n# Answer:\nMurder", "# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> common.topic.webpage -> m.09w03yz\n# Answer:\nMurder", "# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> common.topic.webpage -> m.09w1yxt\n# Answer:\nMurder", "# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> base.fight.crime_type.crimes_of_this_type -> 2009 Binghamton Shootings\n# Answer:\nMurder", "# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> base.fight.crime_type.acquittals_of_this_type -> m.05n91mj\n# Answer:\nMurder", "# Reasoning Path:\nLaci Peterson -> base.popstra.celebrity.breakup -> m.0j3sj1q -> base.popstra.breakup.participant -> William Kent Gain\n# Answer:\nWilliam Kent Gain", "# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> base.fight.crime_type.crimes_of_this_type -> 101 California Street shooting\n# Answer:\nMurder", "# Reasoning Path:\nLaci Peterson -> tv.tv_program_guest.appeared_on -> m.0j6p6_g -> tv.tv_guest_personal_appearance.episode -> Scott Peterson\n# Answer:\nScott Peterson", "# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> base.fight.crime_type.crimes_of_this_type -> 1960s London Nude Murders\n# Answer:\nMurder", "# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> base.fight.crime_type.acquittals_of_this_type -> m.05nls80\n# Answer:\nMurder"], "ground_truth": ["Murder"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1969", "prediction": ["# Reasoning Path:\nKris Humphries -> sports.pro_athlete.teams -> m.0115v9w9 -> sports.sports_team_roster.team -> Washington Wizards\n# Answer:\nWashington Wizards", "# Reasoning Path:\nKris Humphries -> sports.pro_athlete.teams -> m.0j2gfzd -> sports.sports_team_roster.team -> Brooklyn Nets\n# Answer:\nBrooklyn Nets", "# Reasoning Path:\nKris Humphries -> sports.pro_athlete.teams -> m.0115v9w9 -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nKris Humphries -> base.schemastaging.athlete_extra.salary -> m.0j2kv6s -> base.schemastaging.athlete_salary.team -> Brooklyn Nets\n# Answer:\nBrooklyn Nets", "# Reasoning Path:\nKris Humphries -> sports.pro_athlete.teams -> m.0115v9w9 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nKris Humphries -> sports.pro_athlete.teams -> m.0j2gfzd -> sports.sports_team_roster.position -> Power forward\n# Answer:\nPower forward", "# Reasoning Path:\nKris Humphries -> sports.pro_athlete.teams -> m.0125nmsc -> sports.sports_team_roster.position -> Center\n# Answer:\nCenter", "# Reasoning Path:\nKris Humphries -> base.schemastaging.athlete_extra.salary -> m.0j2kv6s -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nKris Humphries -> base.schemastaging.athlete_extra.salary -> m.0w7q6my -> base.schemastaging.athlete_salary.team -> Boston Celtics\n# Answer:\nBoston Celtics", "# Reasoning Path:\nKris Humphries -> base.schemastaging.athlete_extra.salary -> m.0w7q6my -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar"], "ground_truth": ["Washington Wizards"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-197", "prediction": ["# Reasoning Path:\nNew England -> location.location.contains -> White Mountain National Forest -> location.location.containedby -> White Mountains\n# Answer:\nWhite Mountain National Forest", "# Reasoning Path:\nNew England -> location.location.contains -> White Mountain National Forest -> location.location.contains -> Franconia Notch State Park\n# Answer:\nWhite Mountain National Forest", "# Reasoning Path:\nNew England -> location.location.contains -> White Mountain National Forest -> location.location.containedby -> Appalachian Mountains\n# Answer:\nWhite Mountain National Forest", "# Reasoning Path:\nNew England -> location.location.contains -> White Mountain National Forest -> location.location.contains -> Franconia\n# Answer:\nWhite Mountain National Forest", "# Reasoning Path:\nNew England -> location.location.contains -> White Mountain National Forest -> location.location.partiallycontains -> m.0wg9cy5\n# Answer:\nWhite Mountain National Forest", "# Reasoning Path:\nNew England -> location.location.contains -> New Hampshire -> location.location.containedby -> United States, with Territories\n# Answer:\nNew Hampshire", "# Reasoning Path:\nNew England -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> Virginia\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nNew England -> location.location.contains -> New Hampshire -> location.location.containedby -> United States of America\n# Answer:\nNew Hampshire", "# Reasoning Path:\nNew England -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> North Carolina\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nNew England -> location.location.contains -> New Hampshire -> location.location.partially_contains -> Wild River\n# Answer:\nNew Hampshire"], "ground_truth": ["Vermont", "Connecticut", "Rhode Island", "Maine", "New Hampshire", "Massachusetts"], "ans_acc": 0.16666666666666666, "ans_hit": 1, "ans_f1": 0.21428571428571427, "ans_precission": 0.3, "ans_recall": 0.16666666666666666, "path_f1": 0.21428571428571427, "path_precision": 0.3, "path_recall": 0.16666666666666666, "path_ans_f1": 0.21428571428571427, "path_ans_precision": 0.3, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTest-1971", "prediction": ["# Reasoning Path:\nGerard Butler -> award.award_nominee.award_nominations -> m.0g8kc21 -> award.award_nomination.nominated_for -> The Bounty Hunter\n# Answer:\nThe Bounty Hunter", "# Reasoning Path:\nGerard Butler -> award.award_nominee.award_nominations -> m.0b3x56s -> award.award_nomination.nominated_for -> 300\n# Answer:\n300", "# Reasoning Path:\nGerard Butler -> award.award_nominee.award_nominations -> m.0g8kc21 -> award.award_nomination.award -> Razzie Award for Worst Actor\n# Answer:\nRazzie Award for Worst Actor", "# Reasoning Path:\nGerard Butler -> award.award_nominee.award_nominations -> m.0z8k5mj -> award.award_nomination.nominated_for -> The Bounty Hunter\n# Answer:\nThe Bounty Hunter", "# Reasoning Path:\nGerard Butler -> award.award_nominee.award_nominations -> m.0g8kc21 -> award.award_nomination.ceremony -> 31st Golden Raspberry Awards\n# Answer:\n31st Golden Raspberry Awards", "# Reasoning Path:\nGerard Butler -> film.actor.film -> m.0jv821 -> film.performance.film -> Beowulf & Grendel\n# Answer:\nBeowulf & Grendel", "# Reasoning Path:\nGerard Butler -> award.award_nominee.award_nominations -> m.0z8k5mj -> award.award_nomination.nominated_for -> The Ugly Truth\n# Answer:\nThe Ugly Truth", "# Reasoning Path:\nGerard Butler -> award.award_nominee.award_nominations -> m.0z8k5mj -> award.award_nomination.award -> Teen Choice Award for Choice Movie Actor: Romantic Comedy\n# Answer:\nTeen Choice Award for Choice Movie Actor: Romantic Comedy", "# Reasoning Path:\nGerard Butler -> film.actor.film -> g.11b6gjwd97\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nGerard Butler -> award.award_nominee.award_nominations -> m.0z8k5mj -> award.award_nomination.ceremony -> 2010 Teen Choice Awards\n# Answer:\n2010 Teen Choice Awards"], "ground_truth": ["Gamer", "Tomorrow Never Dies", "One More Kiss", "300", "How to Train Your Dragon: Legend of the Boneknapper Dragon", "Playing for Keeps", "Nim's Island", "London Has Fallen", "Dracula 2000", "Thunder Run", "Reign of Fire", "Attila", "Fast Food", "Dear Frankie", "Mrs Brown", "The Game of Their Lives", "Olympus Has Fallen", "The Ugly Truth", "Coriolanus", "How to Train Your Dragon 3", "Timeline", "Gods of Egypt", "Jewel of the Sahara", "Trailer for a Remake of Gore Vidal's Caligula", "The Bounty Hunter", "Harrison's Flowers", "How to Train Your Dragon: Gift of the Night Fury", "The Phantom of the Opera", "Butterfly on a Wheel", "Lara Croft Tomb Raider: The Cradle of Life", "Dynamo", "How to Train Your Dragon 2", "Beowulf & Grendel", "Geostorm", "Law Abiding Citizen", "P.S. I Love You", "Machine Gun Preacher", "Chasing Mavericks", "Wrath of Gods", "Justin and the Knights of Valour", "The Cherry Orchard", "Shooters", "Tales of the Black Freighter", "Movie 43", "RocknRolla", "How to Train Your Dragon"], "ans_acc": 0.08695652173913043, "ans_hit": 1, "ans_f1": 0.14814814814814817, "ans_precission": 0.5, "ans_recall": 0.08695652173913043, "path_f1": 0.14285714285714285, "path_precision": 0.5, "path_recall": 0.08333333333333333, "path_ans_f1": 0.14814814814814817, "path_ans_precision": 0.5, "path_ans_recall": 0.08695652173913043}
{"id": "WebQTest-1972", "prediction": ["# Reasoning Path:\nJohn Franklin -> people.person.places_lived -> m.03pqb5g -> people.place_lived.location -> Blue Island\n# Answer:\nBlue Island", "# Reasoning Path:\nJohn Franklin -> people.person.place_of_birth -> Blue Island -> location.statistical_region.population -> g.11b66snzrw\n# Answer:\nBlue Island", "# Reasoning Path:\nJohn Franklin -> people.person.place_of_birth -> Blue Island -> location.hud_foreclosure_area.total_90_day_vacant_residential_addresses -> m.07fg9l6\n# Answer:\nBlue Island", "# Reasoning Path:\nJohn Franklin -> people.person.place_of_birth -> Blue Island -> location.statistical_region.population -> g.11bc88rzdw\n# Answer:\nBlue Island", "# Reasoning Path:\nJohn Franklin -> people.person.place_of_birth -> Blue Island -> location.statistical_region.population -> g.11x1cxrps\n# Answer:\nBlue Island", "# Reasoning Path:\nJohn Franklin -> people.person.place_of_birth -> Blue Island -> location.location.people_born_here -> Tony Lovato\n# Answer:\nBlue Island", "# Reasoning Path:\nJohn Franklin -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nJohn Franklin -> people.person.place_of_birth -> Blue Island -> location.location.people_born_here -> Anthony E. Zuiker\n# Answer:\nBlue Island", "# Reasoning Path:\nJohn Franklin -> people.person.place_of_birth -> Blue Island -> location.location.people_born_here -> Bill Stapleton\n# Answer:\nBlue Island", "# Reasoning Path:\nJohn Franklin -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality"], "ground_truth": ["Blue Island"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1975", "prediction": ["# Reasoning Path:\nSean Connery -> film.actor.film -> g.11b7sqlhrr\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nSean Connery -> film.producer.film -> Looney Tunes: Back in Action -> media_common.netflix_title.netflix_genres -> Family\n# Answer:\nLooney Tunes: Back in Action", "# Reasoning Path:\nSean Connery -> film.producer.film -> Looney Tunes: Back in Action -> film.film.genre -> Adventure Film\n# Answer:\nLooney Tunes: Back in Action", "# Reasoning Path:\nSean Connery -> film.actor.film -> m.021ymvc -> film.performance.film -> Never Say Never Again\n# Answer:\nNever Say Never Again", "# Reasoning Path:\nSean Connery -> film.producer.film -> Entrapment -> film.film.starring -> m.0g8mhpk\n# Answer:\nEntrapment", "# Reasoning Path:\nSean Connery -> film.actor.film -> m.022qj4c -> film.performance.film -> Thunderball\n# Answer:\nThunderball", "# Reasoning Path:\nSean Connery -> film.producer.film -> Looney Tunes: Back in Action -> film.film.genre -> Animation\n# Answer:\nLooney Tunes: Back in Action", "# Reasoning Path:\nSean Connery -> film.producer.film -> Entrapment -> film.film.country -> United States of America\n# Answer:\nEntrapment", "# Reasoning Path:\nSean Connery -> film.producer.film -> Looney Tunes: Back in Action -> film.film.genre -> Comedy\n# Answer:\nLooney Tunes: Back in Action", "# Reasoning Path:\nSean Connery -> film.producer.film -> Looney Tunes: Back in Action -> media_common.netflix_title.netflix_genres -> Family Animation\n# Answer:\nLooney Tunes: Back in Action"], "ground_truth": ["Sir Billi"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1976", "prediction": ["# Reasoning Path:\nDidier Drogba -> base.schemastaging.athlete_extra.salary -> m.0s9fqn7 -> base.schemastaging.athlete_salary.team -> Galatasaray S.K.\n# Answer:\nGalatasaray S.K.", "# Reasoning Path:\nDidier Drogba -> soccer.football_player.statistics -> m.0w9jzjl -> soccer.football_player_stats.team -> Shanghai Greenland Shenhua F.C.\n# Answer:\nShanghai Greenland Shenhua F.C.", "# Reasoning Path:\nDidier Drogba -> base.schemastaging.athlete_extra.salary -> m.0s9fqn7 -> base.schemastaging.athlete_salary.currency -> Euro\n# Answer:\nEuro", "# Reasoning Path:\nDidier Drogba -> base.schemastaging.athlete_extra.salary -> m.0_lykg_ -> base.schemastaging.athlete_salary.team -> Galatasaray S.K.\n# Answer:\nGalatasaray S.K.", "# Reasoning Path:\nDidier Drogba -> soccer.football_player.statistics -> m.0w8wn00 -> soccer.football_player_stats.team -> Galatasaray S.K.\n# Answer:\nGalatasaray S.K.", "# Reasoning Path:\nDidier Drogba -> base.schemastaging.athlete_extra.salary -> m.0_lykg_ -> base.schemastaging.athlete_salary.currency -> Euro\n# Answer:\nEuro", "# Reasoning Path:\nDidier Drogba -> soccer.football_player.statistics -> m.0w8wfyf -> soccer.football_player_stats.team -> Chelsea F.C.\n# Answer:\nChelsea F.C.", "# Reasoning Path:\nDidier Drogba -> base.schemastaging.athlete_extra.salary -> m.0k6zmxh -> base.schemastaging.athlete_salary.team -> Shanghai Greenland Shenhua F.C.\n# Answer:\nShanghai Greenland Shenhua F.C.", "# Reasoning Path:\nDidier Drogba -> base.schemastaging.athlete_extra.salary -> m.0k6zmxh -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nPound sterling", "# Reasoning Path:\nDidier Drogba -> people.person.children -> Iman Drogba -> people.person.sibling_s -> m.0s9fjzd\n# Answer:\nIman Drogba"], "ground_truth": ["Galatasaray S.K.", "Chelsea F.C.", "Ivory Coast national football team", "Le Mans FC", "En Avant de Guingamp", "Shanghai Greenland Shenhua F.C.", "Olympique de Marseille"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.6, "ans_recall": 0.42857142857142855, "path_f1": 0.32432432432432434, "path_precision": 0.6, "path_recall": 0.2222222222222222, "path_ans_f1": 0.5, "path_ans_precision": 0.6, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1977", "prediction": ["# Reasoning Path:\nMichael Crabtree -> people.person.education -> m.040l5fn -> education.education.institution -> David W. Carter High School\n# Answer:\nDavid W. Carter High School", "# Reasoning Path:\nMichael Crabtree -> people.person.education -> m.04m7z4n -> education.education.institution -> Texas Tech University\n# Answer:\nTexas Tech University", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.games -> m.07ych9z -> american_football.player_game_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nMichael Crabtree -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.games -> m.07ych9z -> american_football.player_game_statistics.team -> San Francisco 49ers\n# Answer:\nSan Francisco 49ers", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.games -> m.07z2fhs -> american_football.player_game_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nMichael Crabtree -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nMichael Crabtree -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nMale", "# Reasoning Path:\nMichael Crabtree -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Acute myeloid leukemia\n# Answer:\nMale", "# Reasoning Path:\nMichael Crabtree -> people.person.gender -> Male -> biology.hybrid_parent_gender.hybrids -> m.0blp580\n# Answer:\nMale"], "ground_truth": ["Texas Tech University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1978", "prediction": ["# Reasoning Path:\nFredrik Ljungberg -> people.person.place_of_birth -> Vittsj\u00f6 -> location.location.containedby -> H\u00e4ssleholm Municipality\n# Answer:\nVittsj\u00f6", "# Reasoning Path:\nFredrik Ljungberg -> people.person.place_of_birth -> Vittsj\u00f6 -> location.location.containedby -> Scania\n# Answer:\nVittsj\u00f6", "# Reasoning Path:\nFredrik Ljungberg -> people.person.place_of_birth -> Vittsj\u00f6 -> location.location.containedby -> Sk\u00e5ne County\n# Answer:\nVittsj\u00f6", "# Reasoning Path:\nFredrik Ljungberg -> people.person.place_of_birth -> Vittsj\u00f6 -> location.location.geolocation -> m.0d33bvv\n# Answer:\nVittsj\u00f6", "# Reasoning Path:\nFredrik Ljungberg -> people.person.place_of_birth -> Vittsj\u00f6 -> location.location.people_born_here -> Albin Tingsvall\n# Answer:\nVittsj\u00f6", "# Reasoning Path:\nFredrik Ljungberg -> people.person.place_of_birth -> Vittsj\u00f6 -> location.location.people_born_here -> Gustav Fridolin\n# Answer:\nVittsj\u00f6", "# Reasoning Path:\nFredrik Ljungberg -> people.person.places_lived -> m.0wkctn7 -> people.place_lived.location -> Vittsj\u00f6\n# Answer:\nVittsj\u00f6", "# Reasoning Path:\nFredrik Ljungberg -> film.actor.film -> m.0tllb4x -> film.performance.film -> Goal II: Living the Dream\n# Answer:\nGoal II: Living the Dream", "# Reasoning Path:\nFredrik Ljungberg -> film.actor.film -> m.0tllb4x -> film.performance.special_performance_type -> Him/Herself\n# Answer:\nHim/Herself"], "ground_truth": ["Vittsj\u00f6"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-1979", "prediction": ["# Reasoning Path:\nMiami Dolphins -> sports.sports_team.league -> m.0crthld -> sports.sports_league_participation.league -> American Football League\n# Answer:\nAmerican Football League", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.0tk1ttp -> sports.sports_league_draft_pick.player -> Dion Jordan\n# Answer:\nDion Jordan", "# Reasoning Path:\nMiami Dolphins -> sports.sports_team.league -> m.0crt4zr -> sports.sports_league_participation.league -> National Football League\n# Answer:\nNational Football League", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.010hdmz0 -> sports.sports_league_draft_pick.player -> Jarvis Landry\n# Answer:\nJarvis Landry", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.010q80zg -> sports.sports_league_draft_pick.player -> Dan Marino\n# Answer:\nDan Marino", "# Reasoning Path:\nMiami Dolphins -> sports.sports_team.league -> m.0crt9k9 -> sports.sports_league_participation.league -> American Football Conference\n# Answer:\nAmerican Football Conference", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.0tk1ttp -> sports.sports_league_draft_pick.school -> University of Oregon\n# Answer:\nUniversity of Oregon", "# Reasoning Path:\nMiami Dolphins -> base.schemastaging.organization_extra.contact_webpages -> m.010g8ggp -> freebase.valuenotation.is_reviewed -> Language\n# Answer:\nLanguage", "# Reasoning Path:\nMiami Dolphins -> base.schemastaging.organization_extra.contact_webpages -> m.010g8ggp -> internet.localized_uri.language -> English\n# Answer:\nEnglish", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.010hdmz0 -> sports.sports_league_draft_pick.school -> Louisiana State University\n# Answer:\nLouisiana State University"], "ground_truth": ["Jamar Taylor"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1981", "prediction": ["# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.09ryhrl -> government.government_position_held.district_represented -> Province of Massachusetts Bay\n# Answer:\nProvince of Massachusetts Bay", "# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.04466xb -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.09ryhrl -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nFrom", "# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.04mm9px -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJohn Adams -> people.person.place_of_birth -> Braintree -> location.location.people_born_here -> Anthony Aiello\n# Answer:\nBraintree", "# Reasoning Path:\nJohn Adams -> people.person.place_of_birth -> Braintree -> location.location.containedby -> Massachusetts\n# Answer:\nBraintree", "# Reasoning Path:\nJohn Adams -> people.person.place_of_birth -> Braintree -> common.topic.notable_types -> City/Town/Village\n# Answer:\nBraintree", "# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.09ryhrl -> freebase.valuenotation.is_reviewed -> Governmental body (if position is part of one)\n# Answer:\nGovernmental body (if position is part of one)", "# Reasoning Path:\nJohn Adams -> people.person.place_of_birth -> Braintree -> location.location.people_born_here -> Asa P. French\n# Answer:\nBraintree", "# Reasoning Path:\nJohn Adams -> people.person.place_of_birth -> Braintree -> location.location.containedby -> United States of America\n# Answer:\nBraintree"], "ground_truth": ["Massachusetts"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.11111111111111112, "path_precision": 0.1, "path_recall": 0.125, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1982", "prediction": ["# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> music.genre.parent_genre -> Art music\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> broadcast.genre.content -> Classica\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> broadcast.genre.content -> ORSRADIO.COM Classical\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> radio.radio_subject.programs_with_this_subject -> Adventures in Good Music\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Baroque music -> music.genre.parent_genre -> Classical music\n# Answer:\nBaroque music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Baroque music -> common.topic.image -> Baschenis - Musical Instruments\n# Answer:\nBaroque music", "# Reasoning Path:\nAntonio Vivaldi -> base.ontologies.ontology_instance.equivalent_instances -> m.07ndktx -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nOpenCyc", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> broadcast.genre.content -> 1.FM Otto's  Baroque music\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> radio.radio_subject.programs_with_this_subject -> Music 'Til Dawn\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Baroque music -> music.genre.parent_genre -> Classical music and folk music\n# Answer:\nBaroque music"], "ground_truth": ["Classical music", "Baroque music"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1985", "prediction": ["# Reasoning Path:\nMike Fisher -> base.schemastaging.athlete_extra.salary -> m.0j2c6py -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nMike Fisher -> sports.pro_athlete.teams -> m.0j20pg1 -> sports.sports_team_roster.team -> Ottawa Senators\n# Answer:\nOttawa Senators", "# Reasoning Path:\nMike Fisher -> base.schemastaging.athlete_extra.salary -> m.0j2c6py -> base.schemastaging.athlete_salary.team -> Nashville Predators\n# Answer:\nNashville Predators", "# Reasoning Path:\nMike Fisher -> base.schemastaging.athlete_extra.salary -> m.0t5x4wm -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nMike Fisher -> sports.pro_athlete.teams -> m.0j2c449 -> sports.sports_team_roster.team -> Nashville Predators\n# Answer:\nNashville Predators", "# Reasoning Path:\nMike Fisher -> base.schemastaging.athlete_extra.salary -> m.0ygmwhn -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nMike Fisher -> base.schemastaging.athlete_extra.salary -> m.0t5x4wm -> base.schemastaging.athlete_salary.team -> Nashville Predators\n# Answer:\nNashville Predators", "# Reasoning Path:\nMike Fisher -> people.person.profession -> Actor -> people.profession.specializations -> Extra\n# Answer:\nActor", "# Reasoning Path:\nMike Fisher -> sports.pro_athlete.teams -> m.0j2c449 -> sports.sports_team_roster.position -> Centerman\n# Answer:\nCenterman", "# Reasoning Path:\nMike Fisher -> base.schemastaging.athlete_extra.salary -> m.0ygmwhn -> base.schemastaging.athlete_salary.team -> Nashville Predators\n# Answer:\nNashville Predators"], "ground_truth": ["Nashville Predators"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1986", "prediction": ["# Reasoning Path:\nDaniel Craig -> award.award_nominee.award_nominations -> m.01140w77 -> award.award_nomination.nominated_for -> One Life\n# Answer:\nOne Life", "# Reasoning Path:\nDaniel Craig -> award.award_nominee.award_nominations -> m.07zmwhv -> award.award_nomination.nominated_for -> Casino Royale\n# Answer:\nCasino Royale", "# Reasoning Path:\nDaniel Craig -> award.award_nominee.award_nominations -> m.09zcl3b -> award.award_nomination.nominated_for -> Infamous\n# Answer:\nInfamous", "# Reasoning Path:\nDaniel Craig -> tv.tv_personality.tv_regular_appearances -> m.0zvr46v -> tv.tv_regular_personal_appearance.program -> MTV Europe Music Awards 2006\n# Answer:\nMTV Europe Music Awards 2006", "# Reasoning Path:\nDaniel Craig -> award.award_nominee.award_nominations -> m.01140w77 -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Narrator\n# Answer:\nPrimetime Emmy Award for Outstanding Narrator", "# Reasoning Path:\nDaniel Craig -> film.actor.film -> m.02h7bc8 -> film.performance.film -> The Invasion\n# Answer:\nThe Invasion", "# Reasoning Path:\nDaniel Craig -> award.award_nominee.award_nominations -> m.07zmwhv -> award.award_nomination.award -> BAFTA Award for Best Actor in a Leading Role\n# Answer:\nBAFTA Award for Best Actor in a Leading Role", "# Reasoning Path:\nDaniel Craig -> tv.tv_personality.tv_regular_appearances -> m.0zvr46v -> tv.tv_regular_personal_appearance.appearance_type -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nDaniel Craig -> film.actor.film -> m.02hw79s -> film.performance.film -> Casino Royale\n# Answer:\nCasino Royale", "# Reasoning Path:\nDaniel Craig -> award.award_nominee.award_nominations -> m.09zcl3b -> award.award_nomination.ceremony -> 22nd Independent Spirit Awards\n# Answer:\n22nd Independent Spirit Awards"], "ground_truth": ["A Kid in King Arthur's Court", "Love Is the Devil: Study for a Portrait of Francis Bacon", "Hotel Splendide", "Cowboys & Aliens", "The Jacket", "Flashbacks of a Fool", "Enduring Love", "Sharpe's Eagle", "Infamous", "Lara Croft: Tomb Raider", "Quantum of Solace", "Renaissance", "Layer Cake", "The Trench", "The Girl with the Dragon Tattoo", "The Organ Grinder's Monkey", "The Golden Compass", "The Adventures of Young Indiana Jones: Daredevils of the Desert", "Fateless", "Spectre", "Love and Rage", "Elizabeth", "James Bond Supports International Women's Day", "Skyfall", "The Invasion", "Casino Royale", "Bond 25", "Munich", "Kiss and Tell", "The Mother", "The Girl Who Played with Fire", "Dream House", "Sword of Honour", "Road to Perdition", "I Dreamed of Africa", "Some Voices", "The Adventures of Tintin: The Secret of the Unicorn", "Sylvia", "Defiance", "Obsession", "Archangel", "Saint-Ex", "Copenhagen", "The Power of One"], "ans_acc": 0.06818181818181818, "ans_hit": 1, "ans_f1": 0.11650485436893203, "ans_precission": 0.4, "ans_recall": 0.06818181818181818, "path_f1": 0.10126582278481013, "path_precision": 0.4, "path_recall": 0.057971014492753624, "path_ans_f1": 0.11650485436893203, "path_ans_precision": 0.4, "path_ans_recall": 0.06818181818181818}
{"id": "WebQTest-1987", "prediction": ["# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> visual_art.art_period_movement.associated_artworks -> Elemental: The Power of Illuminated Love\n# Answer:\nModern art", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> visual_art.art_period_movement.associated_artworks -> Flayed Image\n# Answer:\nModern art", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> visual_art.art_period_movement.associated_artworks -> Golden\n# Answer:\nModern art", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nModern art", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Eyewitness: Reports from an Art World in Crisis\n# Answer:\nModern art", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.associated_periods_or_movements -> Pop art -> visual_art.art_period_movement.associated_artists -> Andy Warhol\n# Answer:\nPop art", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.art_forms -> Sculpture -> award.award_discipline.awards_in_this_discipline -> Wolf Prize in Arts\n# Answer:\nSculpture", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.associated_periods_or_movements -> Pop art -> visual_art.art_period_movement.associated_artists -> Robert Rauschenberg\n# Answer:\nPop art", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.art_forms -> Sculpture -> common.topic.subjects -> Artist\n# Answer:\nSculpture", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Origins, Imitation, Conventions\n# Answer:\nModern art"], "ground_truth": ["Bronze"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1988", "prediction": ["# Reasoning Path:\nJoe Biden -> people.person.education -> g.11b8cfq9yc\n# Answer:\npeople.person.education", "# Reasoning Path:\nJoe Biden -> people.person.education -> m.03l4lnx -> education.education.major_field_of_study -> Political Science\n# Answer:\nPolitical Science", "# Reasoning Path:\nJoe Biden -> people.person.education -> m.03l4lnr -> education.education.institution -> Syracuse University\n# Answer:\nSyracuse University", "# Reasoning Path:\nJoe Biden -> common.topic.notable_for -> g.1255qxl7c\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nJoe Biden -> government.politician.government_positions_held -> m.02_x98m -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJoe Biden -> common.topic.notable_for -> g.1259jwgq_\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nJoe Biden -> people.person.education -> m.03l4lnr -> education.education.degree -> Juris Doctor\n# Answer:\nJuris Doctor", "# Reasoning Path:\nJoe Biden -> people.person.education -> m.03l4lnx -> education.education.major_field_of_study -> History\n# Answer:\nHistory", "# Reasoning Path:\nJoe Biden -> government.politician.government_positions_held -> m.02_x98m -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nJoe Biden -> government.politician.government_positions_held -> m.04hzwqx -> government.government_position_held.governmental_body -> New Castle County Council\n# Answer:\nNew Castle County Council"], "ground_truth": ["Political Science", "History"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-199", "prediction": ["# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Iran\n# Answer:\nParliamentary system", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary system -> common.topic.notable_types -> Form of Government\n# Answer:\nParliamentary system", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> United States of America\n# Answer:\nFederal republic", "# Reasoning Path:\nIraq -> location.statistical_region.labor_participation_rate -> g.11b71vj1zg\n# Answer:\nlocation.statistical_region.labor_participation_rate", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Iraqi Kurdistan\n# Answer:\nParliamentary system", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Federal republic -> common.topic.notable_types -> Form of Government\n# Answer:\nFederal republic", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Jordan\n# Answer:\nParliamentary system", "# Reasoning Path:\nIraq -> book.book_subject.works -> A reed shaken by the wind -> book.written_work.subjects -> Middle East\n# Answer:\nA reed shaken by the wind", "# Reasoning Path:\nIraq -> location.statistical_region.labor_participation_rate -> g.12cp_j7p6\n# Answer:\nlocation.statistical_region.labor_participation_rate", "# Reasoning Path:\nIraq -> location.statistical_region.labor_participation_rate -> g.1hhc38qnx\n# Answer:\nlocation.statistical_region.labor_participation_rate"], "ground_truth": ["Parliamentary system", "Parliamentary republic", "Republic", "Federal republic"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.6666666666666665, "ans_precission": 0.6, "ans_recall": 0.75, "path_f1": 0.5454545454545454, "path_precision": 0.6, "path_recall": 0.5, "path_ans_f1": 0.6666666666666665, "path_ans_precision": 0.6, "path_ans_recall": 0.75}
{"id": "WebQTest-1990", "prediction": ["# Reasoning Path:\nThe Lion King -> film.film.starring -> m.0k5qnz -> film.performance.actor -> Nathan Lane\n# Answer:\nNathan Lane", "# Reasoning Path:\nThe Lion King -> film.film.starring -> m.0k5qnb -> film.performance.actor -> Matthew Broderick\n# Answer:\nMatthew Broderick", "# Reasoning Path:\nThe Lion King -> base.wikipedia_infobox.video_game.platforms -> Super Nintendo Entertainment System\n# Answer:\nSuper Nintendo Entertainment System", "# Reasoning Path:\nThe Lion King -> film.film.starring -> m.0k5qnz -> film.performance.character -> Timon\n# Answer:\nTimon", "# Reasoning Path:\nThe Lion King -> film.film.starring -> g.11b82bfkm7\n# Answer:\nfilm.film.starring", "# Reasoning Path:\nThe Lion King -> film.film.starring -> m.0k5qnz -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nThe Lion King -> base.wikipedia_infobox.video_game.platforms -> Amiga\n# Answer:\nAmiga", "# Reasoning Path:\nThe Lion King -> film.film.starring -> m.0k5qnb -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nThe Lion King -> base.wikipedia_infobox.video_game.platforms -> Game Boy\n# Answer:\nGame Boy"], "ground_truth": ["Nathan Lane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.16666666666666666, "path_precision": 0.1111111111111111, "path_recall": 0.3333333333333333, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-1991", "prediction": ["# Reasoning Path:\nOhio -> location.location.containedby -> Midwestern United States -> location.location.containedby -> United States of America\n# Answer:\nMidwestern United States", "# Reasoning Path:\nOhio -> location.location.containedby -> Midwestern United States -> location.location.contains -> East North Central States\n# Answer:\nMidwestern United States", "# Reasoning Path:\nOhio -> location.location.containedby -> Midwestern United States -> book.book_subject.works -> Caught in the Middle: America's Heartland in the Age of Globalism\n# Answer:\nMidwestern United States", "# Reasoning Path:\nOhio -> location.location.containedby -> United States, with Territories -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nOhio -> location.location.containedby -> Contiguous United States -> location.location.containedby -> United States of America\n# Answer:\nContiguous United States", "# Reasoning Path:\nOhio -> location.location.containedby -> Midwestern United States -> location.location.contains -> Avenue of the Saints\n# Answer:\nMidwestern United States", "# Reasoning Path:\nOhio -> location.location.containedby -> United States, with Territories -> base.aareas.schema.earth.sovereign_domain.sovereign_state -> United States of America\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nOhio -> location.location.containedby -> Contiguous United States -> location.location.containedby -> Continental United States\n# Answer:\nContiguous United States", "# Reasoning Path:\nOhio -> location.location.containedby -> United States, with Territories -> location.country.first_level_divisions -> American Samoa\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nOhio -> location.location.containedby -> Midwestern United States -> location.location.contains -> Council Grove\n# Answer:\nMidwestern United States"], "ground_truth": ["Midwestern United States", "East North Central States"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1992", "prediction": ["# Reasoning Path:\nKailua -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nKailua -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Hawaii\n# Answer:\nUnited States of America", "# Reasoning Path:\nKailua -> location.location.containedby -> Hawaii -> location.location.containedby -> United States, with Territories\n# Answer:\nHawaii", "# Reasoning Path:\nKailua -> location.hud_foreclosure_area.ofheo_price_change -> m.07gfw9c\n# Answer:\nlocation.hud_foreclosure_area.ofheo_price_change", "# Reasoning Path:\nKailua -> location.location.containedby -> Hawaii -> location.location.containedby -> Hawaiian Islands\n# Answer:\nHawaii", "# Reasoning Path:\nKailua -> common.topic.notable_for -> g.125f1gfkx\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nKailua -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Hawaii\n# Answer:\nUnited States of America", "# Reasoning Path:\nKailua -> location.location.containedby -> Hawaii County -> location.location.containedby -> Hawaiian Islands\n# Answer:\nHawaii County", "# Reasoning Path:\nKailua -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nKailua -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Alaska\n# Answer:\nUnited States of America"], "ground_truth": ["Oahu"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1993", "prediction": ["# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> language.language_family.geographic_distribution -> Bolivia\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> common.topic.notable_types -> Human Language\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Guaran\u00ed language -> common.topic.notable_types -> Human Language\n# Answer:\nGuaran\u00ed language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> language.language_family.geographic_distribution -> Peru\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nItalian Language", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Paseana -> base.thoroughbredracing.thoroughbred_racehorse.color -> Bay\n# Answer:\nPaseana", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Italian Language -> common.topic.notable_types -> Human Language\n# Answer:\nItalian Language", "# Reasoning Path:\nArgentina -> location.location.partially_contains -> Alto San Juan -> location.location.partially_contained_by -> m.0wg8lvc\n# Answer:\nAlto San Juan", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Paseana -> biology.organism.organism_type -> Horse\n# Answer:\nPaseana", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nItalian Language"], "ground_truth": ["Guaran\u00ed language", "Spanish Language", "Yiddish Language", "Italian Language", "Quechuan languages"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6461538461538462, "ans_precission": 0.7, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.6461538461538462, "path_ans_precision": 0.7, "path_ans_recall": 0.6}
{"id": "WebQTest-1994", "prediction": ["# Reasoning Path:\nWhoopi Goldberg -> award.award_winner.awards_won -> m.0k2vr3p -> award.award_honor.honored_for -> Whoopi Goldberg - Original Broadway Show Recording\n# Answer:\nWhoopi Goldberg - Original Broadway Show Recording", "# Reasoning Path:\nWhoopi Goldberg -> award.award_winner.awards_won -> m.0k2vr3p -> award.award_honor.ceremony -> 28th Annual Grammy Awards\n# Answer:\n28th Annual Grammy Awards", "# Reasoning Path:\nWhoopi Goldberg -> award.award_winner.awards_won -> m.0k2vr3p -> award.award_honor.award -> Grammy Award for Best Comedy Album\n# Answer:\nGrammy Award for Best Comedy Album", "# Reasoning Path:\nWhoopi Goldberg -> award.award_winner.awards_won -> m.04ktfys -> award.award_honor.honored_for -> Ghost\n# Answer:\nGhost", "# Reasoning Path:\nWhoopi Goldberg -> award.award_winner.awards_won -> m.0459r_b -> award.award_honor.award -> Mark Twain Prize for American Humor\n# Answer:\nMark Twain Prize for American Humor", "# Reasoning Path:\nWhoopi Goldberg -> music.artist.track -> Ain't No Mountain High Enough -> common.topic.notable_types -> Musical Recording\n# Answer:\nAin't No Mountain High Enough", "# Reasoning Path:\nWhoopi Goldberg -> music.artist.track -> You Got It -> music.recording.releases -> Boys on the Side\n# Answer:\nYou Got It", "# Reasoning Path:\nWhoopi Goldberg -> award.award_winner.awards_won -> m.04ktfys -> award.award_honor.ceremony -> 63rd Academy Awards\n# Answer:\n63rd Academy Awards", "# Reasoning Path:\nWhoopi Goldberg -> music.artist.track -> Ain't No Mountain High Enough -> music.recording.releases -> Sister Act 2: Back in the Habit\n# Answer:\nAin't No Mountain High Enough", "# Reasoning Path:\nWhoopi Goldberg -> music.artist.track -> Ball of Confusion (That's What the World Is Today) -> music.recording.releases -> Sister Act 2: Back in the Habit\n# Answer:\nBall of Confusion (That's What the World Is Today)"], "ground_truth": ["Grammy Award for Best Comedy Album"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1996", "prediction": ["# Reasoning Path:\nMaggie Simpson -> common.topic.notable_for -> g.1256glm_8\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.parents -> Marge Simpson -> tv.tv_character.appeared_in_tv_program -> m.01z0f47\n# Answer:\nMarge Simpson", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.parents -> Homer Simpson -> fictional_universe.fictional_character.character_created_by -> Matt Groening\n# Answer:\nHomer Simpson", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.parents -> Marge Simpson -> fictional_universe.fictional_character.parents -> Clancy Bouvier\n# Answer:\nMarge Simpson", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.parents -> Marge Simpson -> fictional_universe.fictional_character.married_to -> m.02kx7z6\n# Answer:\nMarge Simpson", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.siblings -> m.02kx80k -> fictional_universe.sibling_relationship_of_fictional_characters.siblings -> Bart Simpson\n# Answer:\nBart Simpson", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.parents -> Homer Simpson -> tv.tv_character.appeared_in_tv_program -> m.01z0f3_\n# Answer:\nHomer Simpson", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.parents -> Marge Simpson -> fictional_universe.fictional_character.parents -> Jacqueline Bouvier\n# Answer:\nMarge Simpson", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.siblings -> m.02kx810 -> fictional_universe.sibling_relationship_of_fictional_characters.siblings -> Lisa Simpson\n# Answer:\nLisa Simpson", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.parents -> Homer Simpson -> fictional_universe.fictional_character.medical_conditions -> Stroke\n# Answer:\nHomer Simpson"], "ground_truth": ["Nancy Cartwright"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1997", "prediction": ["# Reasoning Path:\nOwen Schmitt -> american_football.football_player.receiving -> m.0791jrl -> american_football.player_receiving_statistics.team -> Seattle Seahawks\n# Answer:\nSeattle Seahawks", "# Reasoning Path:\nOwen Schmitt -> common.topic.notable_for -> g.1256s0v84\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nOwen Schmitt -> american_football.football_player.receiving -> m.07sh8x9 -> american_football.player_receiving_statistics.team -> Seattle Seahawks\n# Answer:\nSeattle Seahawks", "# Reasoning Path:\nOwen Schmitt -> american_football.football_player.receiving -> m.07tfchw -> american_football.player_receiving_statistics.team -> Seattle Seahawks\n# Answer:\nSeattle Seahawks", "# Reasoning Path:\nOwen Schmitt -> american_football.football_player.receiving -> m.0791jrl -> american_football.player_receiving_statistics.season -> 2008 NFL season\n# Answer:\n2008 NFL season", "# Reasoning Path:\nOwen Schmitt -> common.topic.notable_types -> American football player -> common.topic.notable_types -> Profession\n# Answer:\nAmerican football player", "# Reasoning Path:\nOwen Schmitt -> american_football.football_player.receiving -> m.07sh8x9 -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nOwen Schmitt -> common.topic.notable_types -> American football player -> people.profession.specialization_of -> Football player\n# Answer:\nAmerican football player", "# Reasoning Path:\nOwen Schmitt -> american_football.football_player.receiving -> m.07tfchw -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nOwen Schmitt -> common.topic.notable_types -> American football player -> fictional_universe.character_occupation.characters_with_this_occupation -> A. A. Ron Balakay\n# Answer:\nAmerican football player"], "ground_truth": ["Seattle Seahawks"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.12765957446808512, "path_precision": 0.3, "path_recall": 0.08108108108108109, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1998", "prediction": ["# Reasoning Path:\nThomas Jefferson -> organization.organization_founder.organizations_founded -> Democratic-Republican Party -> organization.organization.geographic_scope -> United States of America\n# Answer:\nDemocratic-Republican Party", "# Reasoning Path:\nThomas Jefferson -> government.politician.party -> m.03gjg7h -> government.political_party_tenure.party -> Democratic-Republican Party\n# Answer:\nDemocratic-Republican Party", "# Reasoning Path:\nThomas Jefferson -> organization.organization_founder.organizations_founded -> Democratic-Republican Party -> common.topic.image -> TJeffersonrpeale\n# Answer:\nDemocratic-Republican Party", "# Reasoning Path:\nThomas Jefferson -> organization.organization_founder.organizations_founded -> Democratic Party -> organization.organization.geographic_scope -> United States of America\n# Answer:\nDemocratic Party", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> organization.organization_founder.organizations_founded -> Democratic-Republican Party -> government.political_party.ideology -> Republicanism\n# Answer:\nDemocratic-Republican Party", "# Reasoning Path:\nThomas Jefferson -> organization.organization_founder.organizations_founded -> Republican Party -> base.rivalries.rival.rivals -> Red states and blue states\n# Answer:\nRepublican Party", "# Reasoning Path:\nThomas Jefferson -> government.politician.party -> m.03gjg7h -> freebase.valuenotation.is_reviewed -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nThomas Jefferson -> organization.organization_founder.organizations_founded -> Democratic-Republican Party -> common.topic.image -> Rembrandt Peale-Thomas Jefferson\n# Answer:\nDemocratic-Republican Party", "# Reasoning Path:\nThomas Jefferson -> organization.organization_founder.organizations_founded -> Democratic Party -> book.book_subject.works -> The Two Americas: Our Current Political Deadlock and How to Break It\n# Answer:\nDemocratic Party"], "ground_truth": ["Democratic-Republican Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1999", "prediction": ["# Reasoning Path:\nFrance -> location.statistical_region.religions -> m.03q9tjt -> location.religion_percentage.religion -> Catholicism\n# Answer:\nCatholicism", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.11b60tv_69\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nFrance -> location.statistical_region.gni_in_ppp_dollars -> g.11b60wv0lq\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6gn9g\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc37cwf\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nFrance -> location.statistical_region.gni_in_ppp_dollars -> g.1245_0_b7\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nFrance -> location.statistical_region.gni_in_ppp_dollars -> g.1245_3w4h\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars"], "ground_truth": ["Catholicism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.22222222222222224, "path_precision": 0.14285714285714285, "path_recall": 0.5, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> influence.influence_node.influenced_by -> Thomas Aquinas\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.profession -> Philosopher\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.specializations -> Author\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.corresponding_type -> Author\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> influence.influence_node.influenced_by -> Thomas Kuhn\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alvin Plantinga -> influence.influence_node.influenced_by -> Thomas Reid\n# Answer:\nAlvin Plantinga", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Albert Camus -> influence.influence_node.influenced_by -> Edmund Husserl\n# Answer:\nAlbert Camus", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.specializations -> Novelist\n# Answer:\nWriter"], "ground_truth": ["Philosopher", "Physician", "Writer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-200", "prediction": ["# Reasoning Path:\nVietnam War -> book.written_work.subjects -> United States of America -> location.country.administrative_divisions -> New York City\n# Answer:\nUnited States of America", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> United States of America -> location.country.administrative_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nVietnam War -> time.event.locations -> Cambodia -> base.locations.countries.continent -> Asia\n# Answer:\nCambodia", "# Reasoning Path:\nVietnam War -> time.event.locations -> Vietnam -> book.written_work.subjects -> United States of America\n# Answer:\nVietnam", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> United States of America -> location.country.first_level_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nVietnam War -> time.event.locations -> Cambodia -> location.location.events -> Cambodian Campaign\n# Answer:\nCambodia", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> Asia -> base.locations.continents.countries_within -> Vietnam\n# Answer:\nAsia", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> Southeast Asia -> location.location.geolocation -> m.0bg8gdv\n# Answer:\nSoutheast Asia", "# Reasoning Path:\nVietnam War -> time.event.locations -> Cambodia -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nCambodia", "# Reasoning Path:\nVietnam War -> time.event.locations -> Vietnam -> book.written_work.subjects -> Asia\n# Answer:\nVietnam"], "ground_truth": ["Vietnam", "Laos", "Cambodia", "South Vietnam", "North Vietnam", "Southeast Asia"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.6, "ans_recall": 0.5, "path_f1": 0.3636363636363636, "path_precision": 0.4, "path_recall": 0.3333333333333333, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-2000", "prediction": ["# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.0w2d85v -> tv.regular_tv_appearance.actor -> Ron Livingston\n# Answer:\nRon Livingston", "# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.025dgg3 -> tv.regular_tv_appearance.actor -> Sarah Jessica Parker\n# Answer:\nSarah Jessica Parker", "# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.0w2d85v -> tv.regular_tv_appearance.character -> Jack Berger\n# Answer:\nJack Berger", "# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.025dghk -> tv.regular_tv_appearance.actor -> Kim Cattrall\n# Answer:\nKim Cattrall", "# Reasoning Path:\nSex and the City -> film.film.produced_by -> Sarah Jessica Parker -> film.producer.film -> Sex and the City 2\n# Answer:\nSarah Jessica Parker", "# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.025dgg3 -> tv.regular_tv_appearance.seasons -> Sex and the City - Season 6\n# Answer:\nSex and the City - Season 6", "# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.025dgg3 -> tv.regular_tv_appearance.character -> Carrie Bradshaw\n# Answer:\nCarrie Bradshaw", "# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.025dghk -> tv.regular_tv_appearance.seasons -> Sex and the City - Season 6\n# Answer:\nSex and the City - Season 6", "# Reasoning Path:\nSex and the City -> film.film.produced_by -> John P. Melfi -> film.producer.film -> Sex and the City 2\n# Answer:\nJohn P. Melfi", "# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.025dgg3 -> tv.regular_tv_appearance.seasons -> Sex and the City - Season 1\n# Answer:\nSex and the City - Season 1"], "ground_truth": ["Ron Livingston"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-2001", "prediction": ["# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05l1m7b -> sports.sports_award.award_winner -> Liverpool F.C.\n# Answer:\nLiverpool F.C.", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05kkmnq -> sports.sports_award.award_winner -> Manchester United F.C.\n# Answer:\nManchester United F.C.", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05l1m7b -> sports.sports_award.season -> 1988\u201389 FA Cup\n# Answer:\n1988\u201389 FA Cup", "# Reasoning Path:\nFA Cup -> common.topic.notable_for -> g.125f13scs\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.010lkdyj -> sports.sports_award.award_winner -> Arsenal F.C.\n# Answer:\nArsenal F.C.", "# Reasoning Path:\nFA Cup -> freebase.valuenotation.has_no_value -> Date of final occurrence -> rdf-schema#range -> Date/Time\n# Answer:\nDate of final occurrence", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05kkmnq -> sports.sports_award.season -> 2003\u201304 FA Cup\n# Answer:\n2003\u201304 FA Cup", "# Reasoning Path:\nFA Cup -> freebase.valuenotation.has_no_value -> Date of final occurrence -> type.property.schema -> Recurring event\n# Answer:\nDate of final occurrence", "# Reasoning Path:\nFA Cup -> freebase.valuenotation.has_no_value -> Date of final occurrence -> type.property.expected_type -> Date/Time\n# Answer:\nDate of final occurrence", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.010lkdyj -> sports.sports_award.season -> 2013\u201314 FA Cup\n# Answer:\n2013\u201314 FA Cup"], "ground_truth": ["Chelsea F.C."], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-2002", "prediction": ["# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.gender -> Male\n# Answer:\nMichael Joseph Jackson, Jr.", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.nationality -> United States of America\n# Answer:\nMichael Joseph Jackson, Jr.", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.profession -> Actor\n# Answer:\nMichael Joseph Jackson, Jr.", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Paris-Michael Katherine Jackson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nParis-Michael Katherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Paris-Michael Katherine Jackson -> common.topic.article -> m.0j38d_7\n# Answer:\nParis-Michael Katherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Paris-Michael Katherine Jackson -> common.topic.subject_of -> Diamond Ranch Academy\n# Answer:\nParis-Michael Katherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Paris-Michael Katherine Jackson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nParis-Michael Katherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.profession -> TV Personality\n# Answer:\nMichael Joseph Jackson, Jr.", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Paris-Michael Katherine Jackson -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nParis-Michael Katherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Prince Michael Jackson II -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nPrince Michael Jackson II"], "ground_truth": ["Paris-Michael Katherine Jackson", "Michael Joseph Jackson, Jr.", "Prince Michael Jackson II"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-2003", "prediction": ["# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83n09 -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z8jm72 -> award.award_nomination.nominated_for -> The Twilight Saga: New Moon\n# Answer:\nThe Twilight Saga: New Moon", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.award_nominee -> Kristen Stewart\n# Answer:\nKristen Stewart", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.ceremony -> 2012 Teen Choice Awards\n# Answer:\n2012 Teen Choice Awards", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83n09 -> award.award_nomination.award -> Teen Choice Award for Choice Movie Actor: Romance\n# Answer:\nTeen Choice Award for Choice Movie Actor: Romance", "# Reasoning Path:\nRobert Pattinson -> award.award_winner.awards_won -> m.0100q0tx -> award.award_honor.honored_for -> The Twilight Saga: Breaking Dawn - Part 2\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 2", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z8jm72 -> award.award_nomination.award -> Teen Choice Award for Choice Movie Actor: Fantasy\n# Answer:\nTeen Choice Award for Choice Movie Actor: Fantasy", "# Reasoning Path:\nRobert Pattinson -> award.award_winner.awards_won -> m.0b0b974 -> award.award_honor.honored_for -> Twilight\n# Answer:\nTwilight", "# Reasoning Path:\nRobert Pattinson -> people.person.sibling_s -> m.07sh9v2 -> people.sibling_relationship.sibling -> Lizzy Pattinson\n# Answer:\nLizzy Pattinson"], "ground_truth": ["Bel Ami", "The Twilight Saga: Breaking Dawn - Part 1", "Eclipse", "Vanity Fair", "Life", "The Summer House", "Twilight", "Remember Me", "Little Ashes", "Maps to the Stars", "The Childhood of a Leader", "How to Be", "Hold on to Me", "The Twilight Saga: Breaking Dawn - Part 2", "Dark Kingdom: The Dragon King", "Mission: Blacklist", "Queen of the Desert", "Cosmopolis", "The Rover", "The Haunted Airman", "The Twilight Saga: New Moon", "Unbound Captives", "Harry Potter and the Goblet of Fire", "Water for Elephants", "The Bad Mother's Handbook", "Harry Potter and the Order of the Phoenix", "Love & Distrust"], "ans_acc": 0.14814814814814814, "ans_hit": 1, "ans_f1": 0.22857142857142856, "ans_precission": 0.5, "ans_recall": 0.14814814814814814, "path_f1": 0.09803921568627451, "path_precision": 0.5, "path_recall": 0.05434782608695652, "path_ans_f1": 0.22857142857142856, "path_ans_precision": 0.5, "path_ans_recall": 0.14814814814814814}
{"id": "WebQTest-2004", "prediction": ["# Reasoning Path:\nJapan -> location.location.time_zones -> Japan Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nJapan Time Zone", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.11b71r82sc\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_4lv6\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_6fj1\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.12cp_jcjg\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.12tb6g_8v\n# Answer:\nlocation.statistical_region.deposit_interest_rate"], "ground_truth": ["Japan Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-2005", "prediction": ["# Reasoning Path:\nBob Kelso -> tv.tv_character.appeared_in_tv_program -> m.02567sb -> tv.regular_tv_appearance.actor -> Ken Jenkins\n# Answer:\nKen Jenkins", "# Reasoning Path:\nBob Kelso -> tv.tv_character.appeared_in_tv_program -> m.02567sb -> tv.regular_tv_appearance.series -> Scrubs\n# Answer:\nScrubs", "# Reasoning Path:\nBob Kelso -> tv.tv_character.appeared_in_tv_program -> m.02567sb -> tv.regular_tv_appearance.seasons -> Scrubs - Season 1\n# Answer:\nScrubs - Season 1", "# Reasoning Path:\nBob Kelso -> tv.tv_character.appeared_in_tv_program -> m.02567sb -> tv.regular_tv_appearance.seasons -> Scrubs - Season 2\n# Answer:\nScrubs - Season 2", "# Reasoning Path:\nBob Kelso -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Scrubs -> award.award_nominated_work.award_nominations -> m.0lv4677\n# Answer:\nScrubs", "# Reasoning Path:\nBob Kelso -> tv.tv_character.appeared_in_tv_program -> m.02567sb -> tv.regular_tv_appearance.seasons -> Scrubs - Season 3\n# Answer:\nScrubs - Season 3", "# Reasoning Path:\nBob Kelso -> common.topic.notable_types -> TV Character -> type.type.properties -> Appeared In TV Episode Segments\n# Answer:\nTV Character", "# Reasoning Path:\nBob Kelso -> common.topic.notable_types -> TV Character -> type.type.expected_by -> Character\n# Answer:\nTV Character", "# Reasoning Path:\nBob Kelso -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Scrubs -> award.award_nominated_work.award_nominations -> m.0lv4fdx\n# Answer:\nScrubs", "# Reasoning Path:\nBob Kelso -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Scrubs -> fictional_universe.fictional_universe.characters -> Carla Espinosa\n# Answer:\nScrubs"], "ground_truth": ["Ken Jenkins"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-2006", "prediction": ["# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Russian -> people.ethnicity.included_in_group -> Europeans\n# Answer:\nRussian", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Russian -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> L\n# Answer:\nRussian", "# Reasoning Path:\nVanessa Carlton -> music.artist.album -> g.1ywpnw1r_\n# Answer:\nmusic.artist.album", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Russian -> people.ethnicity.included_in_group -> East Slavs\n# Answer:\nRussian", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Russian -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Crimson Dynamo\n# Answer:\nRussian", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Russian -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Billy Boss\n# Answer:\nRussian", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Russian -> common.topic.image -> Kuban Cossack Dance\n# Answer:\nRussian", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Scandinavians -> people.ethnicity.people -> Andreas Sj\u00f6din\n# Answer:\nScandinavians", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Scandinavians -> people.ethnicity.languages_spoken -> North Germanic languages\n# Answer:\nScandinavians", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Russian -> common.topic.image -> Danceinalanya\n# Answer:\nRussian"], "ground_truth": ["Russian", "Scandinavians"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-2007", "prediction": ["# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> media_common.netflix_genre.titles -> Adelheid\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> common.topic.notable_types -> Human Language\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.countries_spoken_in -> Romania\n# Answer:\nUkrainian Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> media_common.netflix_genre.titles -> All My Loved Ones\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Hungarian language -> language.human_language.countries_spoken_in -> Ukraine\n# Answer:\nHungarian language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Hungarian language -> common.topic.notable_types -> Human Language\n# Answer:\nHungarian language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> media_common.netflix_genre.titles -> Autumn Spring\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Hungarian language -> language.human_language.countries_spoken_in -> Hungary\n# Answer:\nHungarian language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Hungarian language -> language.human_language.region -> Europe\n# Answer:\nHungarian language"], "ground_truth": ["Slovak Language", "Bulgarian Language", "Croatian language", "Serbian language", "German Language", "Greek Language", "Russian Language", "Hungarian language", "Czech Language", "Rusyn Language", "Ukrainian Language", "Polish Language", "Romani language"], "ans_acc": 0.23076923076923078, "ans_hit": 1, "ans_f1": 0.375, "ans_precission": 1.0, "ans_recall": 0.23076923076923078, "path_f1": 0.375, "path_precision": 1.0, "path_recall": 0.23076923076923078, "path_ans_f1": 0.375, "path_ans_precision": 1.0, "path_ans_recall": 0.23076923076923078}
{"id": "WebQTest-2008", "prediction": ["# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.location.containedby -> Western Asia\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.location.geolocation -> m.0zwv97z\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.location.containedby -> Central Asia\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Israel -> location.country.languages_spoken -> English Language\n# Answer:\nIsrael", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.location.containedby -> Middle East\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.country.capital -> Ray\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Israel -> location.location.containedby -> Middle East\n# Answer:\nIsrael", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> South Africa -> location.country.languages_spoken -> English Language\n# Answer:\nSouth Africa", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> South Africa -> common.topic.notable_types -> Country\n# Answer:\nSouth Africa", "# Reasoning Path:\nArabic Language -> language.human_language.writing_system -> Arabic alphabet -> language.language_writing_system.languages -> Algerian Arabic\n# Answer:\nArabic alphabet"], "ground_truth": ["Turkey", "Lebanon", "Jordan", "Djibouti", "Morocco", "Mandatory Palestine", "Tanzania", "Iran", "South Africa", "Iraq", "South Yemen", "Canada", "Libya", "Kuwait", "Qatar", "Tunisia", "Egypt", "Syria", "Bahrain", "Yemen", "United Arab Emirates", "Sudan", "Saudi Arabia", "Oman", "Algeria", "Israel", "Mauritania", "Seljuk Empire"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0.19148936170212766, "ans_precission": 0.9, "ans_recall": 0.10714285714285714, "path_f1": 0.19148936170212766, "path_precision": 0.9, "path_recall": 0.10714285714285714, "path_ans_f1": 0.25, "path_ans_precision": 1.0, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-2009", "prediction": ["# Reasoning Path:\nTim Cook -> people.person.nationality -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nTim Cook -> people.person.place_of_birth -> Mobile -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07ghzrz\n# Answer:\nMobile", "# Reasoning Path:\nTim Cook -> people.person.employment_history -> m.04gcn1r -> business.employment_tenure.company -> Intelligent Electronics\n# Answer:\nIntelligent Electronics", "# Reasoning Path:\nTim Cook -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b66g7msm\n# Answer:\nMobile", "# Reasoning Path:\nTim Cook -> people.person.place_of_birth -> Mobile -> location.location.containedby -> United States of America\n# Answer:\nMobile", "# Reasoning Path:\nTim Cook -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Alabama\n# Answer:\nMobile", "# Reasoning Path:\nTim Cook -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b7tn_fd2\n# Answer:\nMobile", "# Reasoning Path:\nTim Cook -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nTim Cook -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Area code 251\n# Answer:\nMobile", "# Reasoning Path:\nTim Cook -> people.person.employment_history -> m.04gcn2l -> business.employment_tenure.company -> Compaq\n# Answer:\nCompaq"], "ground_truth": ["Apple Inc."], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-201", "prediction": ["# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> common.topic.notable_types -> US Vice President\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> people.person.sibling_s -> m.05hlvsz\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> common.topic.notable_types -> Film character\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> people.person.sibling_s -> m.05px29t\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> people.person.quotations -> A strong defense is the surest way to peace. Strength makes detente attainable. Weakness invites war, as my generationmy generationknows from four very bitter experiences. Just as Americas will for peace is second to none, so will Americas strength be second to none. We cannot rely on the forbearance of others to protect this Nation. The power and diversity of the Armed Forces, active Guard and Reserve, the resolve of our fellow citizens, the flexibility in our command to navigate international waters that remain troubled are all essential to our security. -> media_common.quotation.subjects -> Uncategorised\n# Answer:\nA strong defense is the surest way to peace. Strength makes detente attainable. Weakness invites war, as my generationmy generationknows from four very bitter experiences. Just as Americas will for peace is second to none, so will Americas strength be second to none. We cannot rely on the forbearance of others to protect this Nation. The power and diversity of the Armed Forces, active Guard and Reserve, the resolve of our fellow citizens, the flexibility in our command to navigate international waters that remain troubled are all essential to our security.", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> people.person.sibling_s -> m.05px29z\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.political_appointer.appointees -> m.01xrx8h -> government.government_position_held.office_holder -> George H. W. Bush\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nGerald Ford -> government.political_appointer.appointees -> m.01z0qwq -> government.government_position_held.office_holder -> Dick Cheney\n# Answer:\nDick Cheney", "# Reasoning Path:\nGerald Ford -> people.person.quotations -> All of us who served in one war or another know very well that all wars are the glory and the agony of the young. -> common.topic.notable_types -> Quotation\n# Answer:\nAll of us who served in one war or another know very well that all wars are the glory and the agony of the young.", "# Reasoning Path:\nGerald Ford -> government.political_appointer.appointees -> m.04cnx_5 -> government.government_position_held.office_holder -> Donald Rumsfeld\n# Answer:\nDonald Rumsfeld"], "ground_truth": ["Nelson Rockefeller"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-2010", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.degree -> Juris Doctor\n# Answer:\nJuris Doctor", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.major_field_of_study -> Law\n# Answer:\nLaw", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.0123vxrw -> education.educational_institution.students_graduates -> m.0123vxqw\n# Answer:\neducation.educational_institution.students_graduates", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0tk1pyb -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.0125cyb9 -> education.education.institution -> Harvard University\n# Answer:\nHarvard University", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0tk1pyb -> film.personal_film_appearance.film -> 8: The Mormon Proposition\n# Answer:\n8: The Mormon Proposition", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0_gvz4k -> film.personal_film_appearance.film -> Mitt\n# Answer:\nMitt", "# Reasoning Path:\nMitt Romney -> tv.tv_actor.guest_roles -> m.0bvvym_ -> tv.tv_guest_role.episodes_appeared_in -> The Comeback of President Bush\n# Answer:\nThe Comeback of President Bush", "# Reasoning Path:\nMitt Romney -> tv.tv_actor.guest_roles -> m.0bvw9my -> tv.tv_guest_role.episodes_appeared_in -> Mitt Romney, Ryan Sheckler, Jonas Brothers\n# Answer:\nMitt Romney, Ryan Sheckler, Jonas Brothers", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0_gvz4k -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself"], "ground_truth": ["MBA", "Juris Doctor", "Bachelor of Arts"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.15384615384615383, "ans_precission": 0.1, "ans_recall": 0.3333333333333333, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.15384615384615383, "path_ans_precision": 0.1, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-2011", "prediction": ["# Reasoning Path:\nBob Marley -> people.deceased_person.place_of_burial -> Bob Marley Mausoleum -> travel.tourist_attraction.near_travel_destination -> Jamaica\n# Answer:\nBob Marley Mausoleum", "# Reasoning Path:\nBob Marley -> people.deceased_person.place_of_burial -> Bob Marley Mausoleum -> common.topic.notable_types -> Tourist attraction\n# Answer:\nBob Marley Mausoleum", "# Reasoning Path:\nBob Marley -> film.person_or_entity_appearing_in_film.films -> m.0yk0kgr -> film.personal_film_appearance.film -> La deuxi\u00e8me femme\n# Answer:\nLa deuxi\u00e8me femme", "# Reasoning Path:\nBob Marley -> film.person_or_entity_appearing_in_film.films -> m.0y86qzd -> film.personal_film_appearance.film -> Mt. Zion\n# Answer:\nMt. Zion", "# Reasoning Path:\nBob Marley -> common.topic.webpage -> m.0n4vdzx -> common.webpage.resource -> Reggae Guitar Lessons\n# Answer:\nReggae Guitar Lessons", "# Reasoning Path:\nBob Marley -> film.person_or_entity_appearing_in_film.films -> m.0yk0kgr -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nBob Marley -> film.person_or_entity_appearing_in_film.films -> m.0y86qzd -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nBob Marley -> film.person_or_entity_appearing_in_film.films -> m.0crrdhl -> film.personal_film_appearance.film -> Life and Debt\n# Answer:\nLife and Debt", "# Reasoning Path:\nBob Marley -> common.topic.webpage -> discogs page of Bob Marley -> common.webpage.category -> Topic Webpage\n# Answer:\ndiscogs page of Bob Marley", "# Reasoning Path:\nBob Marley -> common.topic.webpage -> m.0n4vdzx -> common.webpage.category -> Musical score\n# Answer:\nMusical score"], "ground_truth": ["Bob Marley Mausoleum"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-2012", "prediction": ["# Reasoning Path:\nLondon -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nGreenwich Mean Time Zone", "# Reasoning Path:\nLondon -> location.location.partially_contains -> River Thames -> location.location.containedby -> United Kingdom\n# Answer:\nRiver Thames", "# Reasoning Path:\nLondon -> periodicals.newspaper_circulation_area.newspapers -> i -> book.periodical.frequency_or_issues_per_year -> m.0jw2n9_\n# Answer:\ni", "# Reasoning Path:\nLondon -> location.location.partially_contains -> River Thames -> location.location.containedby -> England\n# Answer:\nRiver Thames", "# Reasoning Path:\nLondon -> location.location.partially_contains -> River Thames -> location.location.partially_containedby -> Kent\n# Answer:\nRiver Thames", "# Reasoning Path:\nLondon -> location.location.partially_contains -> River Thames -> location.location.partially_contained_by -> m.0wg9mlk\n# Answer:\nRiver Thames", "# Reasoning Path:\nLondon -> periodicals.newspaper_circulation_area.newspapers -> i -> common.topic.article -> m.0dk0p1s\n# Answer:\ni", "# Reasoning Path:\nLondon -> location.location.partially_contains -> River Lea -> location.location.containedby -> United Kingdom\n# Answer:\nRiver Lea", "# Reasoning Path:\nLondon -> periodicals.newspaper_circulation_area.newspapers -> i -> book.newspaper.headquarters -> m.0j0yzj7\n# Answer:\ni", "# Reasoning Path:\nLondon -> location.location.partially_contains -> River Thames -> location.location.containedby -> Europe\n# Answer:\nRiver Thames"], "ground_truth": ["Greenwich Mean Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-2013", "prediction": ["# Reasoning Path:\nChicago -> travel.travel_destination.accommodation -> Hotel Sax Chicago -> common.topic.notable_for -> g.1256hnvy5\n# Answer:\nHotel Sax Chicago", "# Reasoning Path:\nChicago -> travel.travel_destination.accommodation -> Hotel Sax Chicago -> travel.accommodation.accommodation_type -> Boutique hotel\n# Answer:\nHotel Sax Chicago", "# Reasoning Path:\nChicago -> travel.travel_destination.accommodation -> Hotel Sax Chicago -> common.topic.notable_types -> Accommodation\n# Answer:\nHotel Sax Chicago", "# Reasoning Path:\nChicago -> location.statistical_region.population -> g.11b66kxd4l\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nChicago -> location.statistical_region.population -> g.11b7vbyfz4\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nChicago -> location.statistical_region.population -> g.11btt77639\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> American Medical News -> common.topic.article -> m.03mf90t\n# Answer:\nAmerican Medical News", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> American Medical News -> common.topic.notable_for -> g.1254zz3kn\n# Answer:\nAmerican Medical News", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> Arbeiter-Zeitung -> common.topic.notable_types -> Newspaper\n# Answer:\nArbeiter-Zeitung", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> Arbeiter-Zeitung -> book.newspaper.headquarters -> m.0j0pp60\n# Answer:\nArbeiter-Zeitung"], "ground_truth": ["Hotel Sax Chicago"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-2014", "prediction": ["# Reasoning Path:\nSheryl Crow -> medicine.notable_person_with_medical_condition.condition -> Meningioma -> base.schemastaging.context_name.pronunciation -> m.011032cv\n# Answer:\nMeningioma", "# Reasoning Path:\nSheryl Crow -> medicine.notable_person_with_medical_condition.condition -> Meningioma -> people.cause_of_death.people -> Francis Wynne Masters\n# Answer:\nMeningioma", "# Reasoning Path:\nSheryl Crow -> award.award_nominee.award_nominations -> m.0105szp3 -> award.award_nomination.award -> MTV Europe Music Award for Best Female\n# Answer:\nMTV Europe Music Award for Best Female", "# Reasoning Path:\nSheryl Crow -> music.artist.track_contributions -> m.010_wmqq -> music.track_contribution.track -> Love Is All There Is\n# Answer:\nLove Is All There Is", "# Reasoning Path:\nSheryl Crow -> award.award_nominee.award_nominations -> m.0105szp3 -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work", "# Reasoning Path:\nSheryl Crow -> award.award_nominee.award_nominations -> m.05cywhs -> award.award_nomination.nominated_for -> Detours\n# Answer:\nDetours", "# Reasoning Path:\nSheryl Crow -> music.artist.track_contributions -> m.010_wmqq -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nSheryl Crow -> award.award_nominee.award_nominations -> m.0105zmr0 -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work", "# Reasoning Path:\nSheryl Crow -> music.artist.track_contributions -> m.010k1fw6 -> music.track_contribution.track -> Make It Go Away (Radiation Song)\n# Answer:\nMake It Go Away (Radiation Song)", "# Reasoning Path:\nSheryl Crow -> award.award_nominee.award_nominations -> m.0105zmr0 -> award.award_nomination.ceremony -> 1995 MTV Europe Music Awards\n# Answer:\n1995 MTV Europe Music Awards"], "ground_truth": ["Meningioma"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-2015", "prediction": ["# Reasoning Path:\nArgentina -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nArgentina -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> United States of America\n# Answer:\nPresidential system", "# Reasoning Path:\nArgentina -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> United States of America\n# Answer:\nFederal republic", "# Reasoning Path:\nArgentina -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Brazil\n# Answer:\nConstitutional republic", "# Reasoning Path:\nArgentina -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Bolivia\n# Answer:\nPresidential system", "# Reasoning Path:\nArgentina -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Chile\n# Answer:\nConstitutional republic", "# Reasoning Path:\nArgentina -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Brazil\n# Answer:\nPresidential system", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Paseana -> base.thoroughbredracing.thoroughbred_racehorse.color -> Bay\n# Answer:\nPaseana", "# Reasoning Path:\nArgentina -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Brazil\n# Answer:\nFederal republic", "# Reasoning Path:\nArgentina -> location.location.partially_contains -> Alto San Juan -> location.location.partially_contained_by -> m.0wg8lvc\n# Answer:\nAlto San Juan"], "ground_truth": ["Federal republic", "Representative democracy", "Constitutional republic", "Presidential system"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.8, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.8, "path_ans_recall": 0.75}
{"id": "WebQTest-2016", "prediction": ["# Reasoning Path:\nFabio Capello -> people.person.place_of_birth -> San Canzian d'Isonzo -> location.location.containedby -> Friuli-Venezia Giulia\n# Answer:\nSan Canzian d'Isonzo", "# Reasoning Path:\nFabio Capello -> common.topic.article -> m.030_x3\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nFabio Capello -> people.person.place_of_birth -> San Canzian d'Isonzo -> location.location.containedby -> Italy\n# Answer:\nSan Canzian d'Isonzo", "# Reasoning Path:\nFabio Capello -> people.person.place_of_birth -> San Canzian d'Isonzo -> common.topic.article -> m.0fgn_w\n# Answer:\nSan Canzian d'Isonzo", "# Reasoning Path:\nFabio Capello -> people.person.place_of_birth -> San Canzian d'Isonzo -> location.location.containedby -> Province of Gorizia\n# Answer:\nSan Canzian d'Isonzo", "# Reasoning Path:\nFabio Capello -> people.person.place_of_birth -> San Canzian d'Isonzo -> common.topic.notable_types -> City/Town/Village\n# Answer:\nSan Canzian d'Isonzo", "# Reasoning Path:\nFabio Capello -> soccer.football_player.statistics -> m.0w957j9 -> soccer.football_player_stats.team -> Juventus F.C.\n# Answer:\nJuventus F.C.", "# Reasoning Path:\nFabio Capello -> soccer.football_player.statistics -> m.0w957jt -> soccer.football_player_stats.team -> A.S. Roma\n# Answer:\nA.S. Roma", "# Reasoning Path:\nFabio Capello -> soccer.football_player.statistics -> m.0w957k7 -> soccer.football_player_stats.team -> Italy national football team\n# Answer:\nItaly national football team"], "ground_truth": ["San Canzian d'Isonzo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-2017", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> Republican Party -> organization.organization.geographic_scope -> United States of America\n# Answer:\nRepublican Party", "# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> Republican Party -> government.political_party.ideology -> Conservatism in the United States\n# Answer:\nRepublican Party", "# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> Illinois Republican Party -> common.topic.article -> m.0g7ly9\n# Answer:\nIllinois Republican Party", "# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> Republican Party -> organization.organization.founders -> Thomas Jefferson\n# Answer:\nRepublican Party", "# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> United States Secret Service -> organization.organization.founders -> Allan Pinkerton\n# Answer:\nUnited States Secret Service", "# Reasoning Path:\nAbraham Lincoln -> government.politician.party -> m.0lr112v -> government.political_party_tenure.party -> Illinois Republican Party\n# Answer:\nIllinois Republican Party", "# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> United States Secret Service -> base.ontologies.ontology_instance.equivalent_instances -> m.09km072\n# Answer:\nUnited States Secret Service", "# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> Republican Party -> government.political_party.ideology -> Economic liberalism\n# Answer:\nRepublican Party", "# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> Illinois Republican Party -> government.political_party.politicians_in_this_party -> m.0130lyr4\n# Answer:\nIllinois Republican Party", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.04hdfss -> people.place_lived.location -> Illinois\n# Answer:\nIllinois"], "ground_truth": ["Illinois Republican Party", "Whig Party", "National Union Party", "Republican Party"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5833333333333334, "ans_precission": 0.7, "ans_recall": 0.5, "path_f1": 0.48, "path_precision": 0.6, "path_recall": 0.4, "path_ans_f1": 0.5833333333333334, "path_ans_precision": 0.7, "path_ans_recall": 0.5}
{"id": "WebQTest-2018", "prediction": ["# Reasoning Path:\nJack London -> people.person.profession -> Sailor -> film.film_character.portrayed_in_films -> m.0nggq45\n# Answer:\nSailor", "# Reasoning Path:\nJack London -> people.person.profession -> Sailor -> common.topic.notable_types -> Profession\n# Answer:\nSailor", "# Reasoning Path:\nJack London -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Edmund Grey\n# Answer:\nJournalist", "# Reasoning Path:\nJack London -> people.person.profession -> Screenwriter -> freebase.equivalent_topic.equivalent_type -> TV Writer\n# Answer:\nScreenwriter", "# Reasoning Path:\nJack London -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Ernest Pratt\n# Answer:\nJournalist", "# Reasoning Path:\nJack London -> people.person.profession -> Journalist -> common.topic.notable_types -> Profession\n# Answer:\nJournalist", "# Reasoning Path:\nJack London -> people.person.profession -> Journalist -> people.profession.corresponding_type -> Author\n# Answer:\nJournalist", "# Reasoning Path:\nJack London -> people.person.profession -> Screenwriter -> common.topic.notable_types -> Profession\n# Answer:\nScreenwriter", "# Reasoning Path:\nJack London -> common.topic.notable_types -> Author -> fictional_universe.character_occupation.characters_with_this_occupation -> Edmund Grey\n# Answer:\nAuthor", "# Reasoning Path:\nJack London -> common.topic.notable_types -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor"], "ground_truth": ["Novelist", "Screenwriter", "Essayist", "Author", "Journalist", "Sailor"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 1.0, "ans_recall": 0.6666666666666666, "path_f1": 0.6153846153846154, "path_precision": 0.8, "path_recall": 0.5, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-2019", "prediction": ["# Reasoning Path:\nMorgan Freeman -> people.person.spouse_s -> m.0hj6dmb -> people.marriage.spouse -> Myrna Colley-Lee\n# Answer:\nMyrna Colley-Lee", "# Reasoning Path:\nMorgan Freeman -> people.person.spouse_s -> m.0hj6dq1 -> people.marriage.spouse -> Jeanette Adair Bradshaw\n# Answer:\nJeanette Adair Bradshaw", "# Reasoning Path:\nMorgan Freeman -> people.person.children -> Alfonso Freeman -> people.person.children -> Alfonso Rene Freeman II\n# Answer:\nAlfonso Freeman", "# Reasoning Path:\nMorgan Freeman -> people.person.spouse_s -> m.0hj6dmb -> freebase.valuenotation.is_reviewed -> Spouse\n# Answer:\nSpouse", "# Reasoning Path:\nMorgan Freeman -> people.person.children -> Alfonso Freeman -> people.person.children -> Donovan Lee Freeman\n# Answer:\nAlfonso Freeman", "# Reasoning Path:\nMorgan Freeman -> people.person.children -> Alfonso Freeman -> people.person.spouse_s -> m.0j68tts\n# Answer:\nAlfonso Freeman", "# Reasoning Path:\nMorgan Freeman -> people.person.spouse_s -> m.0hj6dmb -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nMorgan Freeman -> people.person.children -> Alfonso Freeman -> people.person.children -> Joshua Caleb Freeman\n# Answer:\nAlfonso Freeman", "# Reasoning Path:\nMorgan Freeman -> people.person.spouse_s -> m.0hj6dq1 -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nMorgan Freeman -> people.person.children -> Alfonso Freeman -> people.person.spouse_s -> m.0kzzbgq\n# Answer:\nAlfonso Freeman"], "ground_truth": ["Myrna Colley-Lee"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-202", "prediction": ["# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Vatican City\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> common.topic.notable_types -> Currency\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Belgium\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37cxr\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Tenerani -> base.thoroughbredracing.thoroughbred_racehorse.foal_if_this_horse_is_sire -> Ribot\n# Answer:\nTenerani", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Tenerani -> biology.organism.sex -> Male\n# Answer:\nTenerani", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Nearco -> biology.organism.sex -> Male\n# Answer:\nNearco"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-2020", "prediction": ["# Reasoning Path:\nApennine Mountains -> geography.mountain_range.passes -> Futa Pass -> common.topic.notable_for -> g.125brlwwv\n# Answer:\nFuta Pass", "# Reasoning Path:\nApennine Mountains -> location.location.containedby -> Italy -> location.country.official_language -> Italian Language\n# Answer:\nItaly", "# Reasoning Path:\nApennine Mountains -> geography.mountain_range.mountains -> Alpe di Succiso -> location.location.containedby -> Province of Reggio Emilia\n# Answer:\nAlpe di Succiso", "# Reasoning Path:\nApennine Mountains -> geography.mountain_range.passes -> Cisa Pass -> location.location.containedby -> Italy\n# Answer:\nCisa Pass", "# Reasoning Path:\nApennine Mountains -> geography.mountain_range.passes -> Futa Pass -> common.topic.article -> m.02qrc4s\n# Answer:\nFuta Pass", "# Reasoning Path:\nApennine Mountains -> location.location.containedby -> Italy -> base.aareas.schema.administrative_area.administrative_children -> Tuscany\n# Answer:\nItaly", "# Reasoning Path:\nApennine Mountains -> geography.mountain_range.mountains -> Corno Grande -> location.location.containedby -> Italy\n# Answer:\nCorno Grande", "# Reasoning Path:\nApennine Mountains -> geography.mountain_range.mountains -> Alpe di Succiso -> common.topic.notable_for -> g.125817bb_\n# Answer:\nAlpe di Succiso", "# Reasoning Path:\nApennine Mountains -> geography.mountain_range.mountains -> Bric del Terma -> geography.geographical_feature.category -> Mountain\n# Answer:\nBric del Terma", "# Reasoning Path:\nApennine Mountains -> location.location.containedby -> Italy -> location.country.languages_spoken -> Italian Language\n# Answer:\nItaly"], "ground_truth": ["Apennine Mountains"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-2021", "prediction": ["# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> influence.influence_node.influenced_by -> Charles Dickens\n# Answer:\nWilliam Faulkner", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> influence.influence_node.influenced_by -> Edgar Allan Poe\n# Answer:\nWilliam Faulkner", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> influence.influence_node.influenced -> Toni Morrison\n# Answer:\nWilliam Faulkner", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> Robert Burns -> influence.influence_node.influenced -> J. D. Salinger\n# Answer:\nRobert Burns", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> influence.influence_node.influenced_by -> Herman Melville\n# Answer:\nWilliam Faulkner", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> people.person.gender -> Male\n# Answer:\nWilliam Faulkner", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> Sherwood Anderson -> influence.influence_node.influenced -> William Faulkner\n# Answer:\nSherwood Anderson", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> Robert Burns -> influence.influence_node.influenced -> James Joyce\n# Answer:\nRobert Burns", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> Sherwood Anderson -> influence.influence_node.influenced_by -> Mark Twain\n# Answer:\nSherwood Anderson", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> influence.influence_node.influenced -> Colum McCann\n# Answer:\nWilliam Faulkner"], "ground_truth": ["Sherwood Anderson", "Robert Burns", "Thomas Malory", "William Faulkner"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.8571428571428571, "path_precision": 1.0, "path_recall": 0.75, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-2022", "prediction": ["# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> Samoan American -> people.ethnicity.people -> Afa Anoa\u02bbi\n# Answer:\nSamoan American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> Samoan American -> people.ethnicity.languages_spoken -> American English\n# Answer:\nSamoan American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> Samoan American -> common.topic.notable_for -> g.12589h52q\n# Answer:\nSamoan American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> Samoan American -> people.ethnicity.people -> Afa Anoa\u02bbi Jr.\n# Answer:\nSamoan American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> African American -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nAfrican American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> African American -> common.topic.subject_of -> The Approaching 100th Anniversary of the Harlem Renaissance (part 2)\n# Answer:\nAfrican American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> Samoan American -> people.ethnicity.people -> Al Harrington\n# Answer:\nSamoan American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> African American -> common.topic.notable_types -> Ethnicity\n# Answer:\nAfrican American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> Samoan American -> people.ethnicity.languages_spoken -> Samoan Language\n# Answer:\nSamoan American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> African American -> people.ethnicity.languages_spoken -> American English\n# Answer:\nAfrican American"], "ground_truth": ["African American", "Samoan American"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-2023", "prediction": ["# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.headquarters -> m.04300hm -> location.mailing_address.citytown -> Brussels\n# Answer:\nBrussels", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.founders -> United States of America -> location.country.administrative_divisions -> Kansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.headquarters -> m.04300hm -> common.topic.notable_types -> Location\n# Answer:\nLocation", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.headquarters -> m.04300hm -> common.topic.notable_for -> g.12571zl_t\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.founders -> Belgium -> location.country.capital -> Brussels\n# Answer:\nBelgium", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.founders -> United States of America -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics\n# Answer:\nUnited States of America", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> military.military_combatant.military_conflicts -> m.07vjs3f -> military.military_combatant_group.conflict -> Cold War\n# Answer:\nCold War", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.founders -> Canada -> base.locations.countries.continent -> North America\n# Answer:\nCanada", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.founders -> United States of America -> olympics.olympic_participating_country.olympics_participated_in -> 1900 Summer Olympics\n# Answer:\nUnited States of America", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.founders -> United States of America -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics\n# Answer:\nUnited States of America"], "ground_truth": ["Brussels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-2024", "prediction": ["# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Sharpsburg -> location.location.containedby -> United States of America\n# Answer:\nSharpsburg", "# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Sharpsburg -> location.hud_foreclosure_area.ofheo_price_change -> m.07gzwmg\n# Answer:\nSharpsburg", "# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Sharpsburg -> location.location.containedby -> Washington County\n# Answer:\nSharpsburg", "# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Sharpsburg -> location.location.containedby -> Maryland\n# Answer:\nSharpsburg", "# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Sharpsburg -> location.statistical_region.population -> g.11b66jxpbg\n# Answer:\nSharpsburg", "# Reasoning Path:\nBattle of Antietam -> base.culturalevent.event.entity_involved -> George B. McClellan -> people.person.nationality -> United States of America\n# Answer:\nGeorge B. McClellan", "# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Maryland -> location.location.adjoin_s -> m.02thq3m\n# Answer:\nMaryland", "# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Maryland -> book.book_subject.works -> Antietam\n# Answer:\nMaryland", "# Reasoning Path:\nBattle of Antietam -> military.military_conflict.military_personnel_involved -> Albion P. Howe -> people.person.nationality -> United States of America\n# Answer:\nAlbion P. Howe", "# Reasoning Path:\nBattle of Antietam -> base.culturalevent.event.entity_involved -> Robert E. Lee -> people.person.place_of_birth -> Stratford Hall\n# Answer:\nRobert E. Lee"], "ground_truth": ["Maryland"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-2025", "prediction": ["# Reasoning Path:\nZambezi -> geography.river.mouth -> Indian Ocean -> geography.geographical_feature.category -> Ocean\n# Answer:\nIndian Ocean", "# Reasoning Path:\nZambezi -> geography.river.mouth -> Indian Ocean -> book.book_subject.works -> A fish caught in time: the search for the coelacanth\n# Answer:\nIndian Ocean", "# Reasoning Path:\nZambezi -> geography.river.mouth -> Indian Ocean -> location.location.events -> Indian Ocean in World War II\n# Answer:\nIndian Ocean", "# Reasoning Path:\nZambezi -> geography.river.mouth -> Indian Ocean -> book.book_subject.works -> Anthology of African and Indian Ocean Photography\n# Answer:\nIndian Ocean", "# Reasoning Path:\nZambezi -> geography.river.mouth -> Indian Ocean -> book.book_subject.works -> Boarding party\n# Answer:\nIndian Ocean", "# Reasoning Path:\nZambezi -> geography.river.mouth -> Indian Ocean -> location.location.events -> Action of 11 January 1944\n# Answer:\nIndian Ocean", "# Reasoning Path:\nZambezi -> geography.river.basin_countries -> Democratic Republic of the Congo -> location.location.containedby -> Africa\n# Answer:\nDemocratic Republic of the Congo", "# Reasoning Path:\nZambezi -> geography.river.cities -> Victoria Falls -> location.location.containedby -> Zambia\n# Answer:\nVictoria Falls", "# Reasoning Path:\nZambezi -> geography.river.cities -> Victoria Falls -> travel.tourist_attraction.near_travel_destination -> Livingstone\n# Answer:\nVictoria Falls", "# Reasoning Path:\nZambezi -> geography.river.mouth -> Indian Ocean -> location.location.events -> Action of 13 November 1943\n# Answer:\nIndian Ocean"], "ground_truth": ["Indian Ocean"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-2027", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m -> soccer.football_player_stats.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9021c -> soccer.football_player_stats.team -> A.C. Milan\n# Answer:\nA.C. Milan", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nPound sterling", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.02nr829 -> sports.sports_team_roster.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.02nr829 -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nMidfielder", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.0bvddcl -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nMidfielder"], "ground_truth": ["Manchester United F.C."], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-2028", "prediction": ["# Reasoning Path:\nFrance -> government.governmental_jurisdiction.governing_officials -> m.0ngn_cw -> government.government_position_held.office_holder -> Nicolas Sarkozy\n# Answer:\nNicolas Sarkozy", "# Reasoning Path:\nFrance -> government.governmental_jurisdiction.governing_officials -> m.0102_81j -> government.government_position_held.appointed_by -> Fran\u00e7ois Hollande\n# Answer:\nFran\u00e7ois Hollande", "# Reasoning Path:\nFrance -> government.governmental_jurisdiction.governing_officials -> m.0102_7kh -> government.government_position_held.appointed_by -> Fran\u00e7ois Hollande\n# Answer:\nFran\u00e7ois Hollande", "# Reasoning Path:\nFrance -> government.governmental_jurisdiction.governing_officials -> m.0ngn_cw -> government.government_position_held.office_position_or_title -> Minister of the Interior\n# Answer:\nMinister of the Interior", "# Reasoning Path:\nFrance -> government.governmental_jurisdiction.governing_officials -> m.0102_81j -> government.government_position_held.office_holder -> Michel Sapin\n# Answer:\nMichel Sapin", "# Reasoning Path:\nFrance -> government.governmental_jurisdiction.governing_officials -> m.0102_7kh -> government.government_position_held.office_holder -> Michel Sapin\n# Answer:\nMichel Sapin", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.11b60tv_69\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nFrance -> location.statistical_region.gni_in_ppp_dollars -> g.11b60wv0lq\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nFrance -> government.governmental_jurisdiction.governing_officials -> m.0102_81j -> freebase.valuenotation.has_no_value -> District represented (if position is district-related)\n# Answer:\nDistrict represented (if position is district-related)", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6gn9g\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita"], "ground_truth": ["Nicolas Sarkozy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.125, "path_precision": 0.1, "path_recall": 0.16666666666666666, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-2029", "prediction": ["# Reasoning Path:\nKnossos -> location.location.containedby -> Greece -> location.location.containedby -> Europe\n# Answer:\nGreece", "# Reasoning Path:\nKnossos -> location.location.containedby -> Greece -> location.location.containedby -> Southeast Europe\n# Answer:\nGreece", "# Reasoning Path:\nKnossos -> location.location.containedby -> Crete -> location.location.containedby -> Greece\n# Answer:\nCrete", "# Reasoning Path:\nKnossos -> location.location.containedby -> Greece -> location.location.containedby -> Eurasia\n# Answer:\nGreece", "# Reasoning Path:\nKnossos -> travel.tourist_attraction.near_travel_destination -> Malia, Crete -> location.location.containedby -> Greece\n# Answer:\nMalia, Crete", "# Reasoning Path:\nKnossos -> location.location.containedby -> Crete -> location.location.containedby -> Mediterranean Sea\n# Answer:\nCrete", "# Reasoning Path:\nKnossos -> location.location.containedby -> Greece -> location.country.administrative_divisions -> Crete\n# Answer:\nGreece", "# Reasoning Path:\nKnossos -> location.location.containedby -> Greece -> location.country.administrative_divisions -> Achaea\n# Answer:\nGreece", "# Reasoning Path:\nKnossos -> location.location.containedby -> Greece -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60nhckb\n# Answer:\nGreece", "# Reasoning Path:\nKnossos -> travel.tourist_attraction.near_travel_destination -> Heraklion -> location.location.containedby -> Greece\n# Answer:\nHeraklion"], "ground_truth": ["Greece", "Crete"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-2030", "prediction": ["# Reasoning Path:\nArea 51 -> aviation.airport.serves -> Nevada -> location.location.containedby -> United States, with Territories\n# Answer:\nNevada", "# Reasoning Path:\nArea 51 -> location.location.containedby -> Lincoln County -> location.location.containedby -> Nevada\n# Answer:\nLincoln County", "# Reasoning Path:\nArea 51 -> aviation.airport.serves -> Nevada -> base.aareas.schema.administrative_area.administrative_children -> White Pine County\n# Answer:\nNevada", "# Reasoning Path:\nArea 51 -> aviation.airport.serves -> Nevada -> location.location.containedby -> United States of America\n# Answer:\nNevada", "# Reasoning Path:\nArea 51 -> aviation.airport.serves -> Nevada -> base.aareas.schema.administrative_area.administrative_children -> Churchill County\n# Answer:\nNevada", "# Reasoning Path:\nArea 51 -> aviation.airport.serves -> Nevada -> base.aareas.schema.administrative_area.administrative_children -> Douglas County\n# Answer:\nNevada", "# Reasoning Path:\nArea 51 -> aviation.airport.serves -> Nevada -> location.statistical_region.religions -> m.04403h4\n# Answer:\nNevada", "# Reasoning Path:\nArea 51 -> location.location.containedby -> Nevada -> location.location.containedby -> United States, with Territories\n# Answer:\nNevada", "# Reasoning Path:\nArea 51 -> common.topic.image -> Wfm area 51 landsat geocover 2000 -> common.image.appears_in_topic_gallery -> Groom Lake\n# Answer:\nWfm area 51 landsat geocover 2000", "# Reasoning Path:\nArea 51 -> location.location.containedby -> Lincoln County -> location.statistical_region.population -> g.11b66dwnht\n# Answer:\nLincoln County"], "ground_truth": ["Nevada", "Lincoln County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-2031", "prediction": ["# Reasoning Path:\nChris Riley -> people.person.place_of_birth -> Echuca -> location.location.containedby -> Australia\n# Answer:\nEchuca", "# Reasoning Path:\nChris Riley -> common.topic.notable_for -> g.1258z_d26\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nChris Riley -> people.person.place_of_birth -> Echuca -> location.location.containedby -> Victoria\n# Answer:\nEchuca", "# Reasoning Path:\nChris Riley -> people.person.place_of_birth -> Echuca -> travel.travel_destination.tourist_attractions -> Billabong Ranch\n# Answer:\nEchuca", "# Reasoning Path:\nChris Riley -> people.person.place_of_birth -> Echuca -> common.topic.image -> Echuca location map in Victoria\n# Answer:\nEchuca", "# Reasoning Path:\nChris Riley -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nChris Riley -> people.person.place_of_birth -> Echuca -> travel.travel_destination.tourist_attractions -> Echuca Historical Society Museum\n# Answer:\nEchuca", "# Reasoning Path:\nChris Riley -> people.person.place_of_birth -> Echuca -> travel.travel_destination.tourist_attractions -> Echuca Regional Park\n# Answer:\nEchuca", "# Reasoning Path:\nChris Riley -> people.person.place_of_birth -> Echuca -> common.topic.image -> Echuca docks Stevage\n# Answer:\nEchuca", "# Reasoning Path:\nChris Riley -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale"], "ground_truth": ["Echuca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-204", "prediction": ["# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo -> location.location.containedby -> Cairo Governorate\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo -> periodicals.newspaper_circulation_area.newspapers -> Al-Ahram Weekly\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.capital -> Cairo -> periodicals.newspaper_circulation_area.newspapers -> Al-Ahram Weekly\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo Governorate -> location.location.containedby -> North Africa\n# Answer:\nCairo Governorate", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo Governorate -> common.topic.notable_types -> Administrative Division\n# Answer:\nCairo Governorate", "# Reasoning Path:\nEgypt -> location.country.capital -> Cairo -> common.topic.subject_of -> As Egypt Howls and History Tweets\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo -> periodicals.newspaper_circulation_area.newspapers -> Almesryoon\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Al Sharqia Governorate -> location.location.containedby -> North Africa\n# Answer:\nAl Sharqia Governorate", "# Reasoning Path:\nEgypt -> location.country.capital -> Cairo -> periodicals.newspaper_circulation_area.newspapers -> Almesryoon\n# Answer:\nCairo"], "ground_truth": ["Cairo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-205", "prediction": ["# Reasoning Path:\nErnest Rutherford -> people.person.education -> m.0n1fbly -> education.education.institution -> University of New Zealand\n# Answer:\nUniversity of New Zealand", "# Reasoning Path:\nErnest Rutherford -> people.person.education -> m.02kq21s -> education.education.institution -> University of Cambridge\n# Answer:\nUniversity of Cambridge", "# Reasoning Path:\nErnest Rutherford -> people.person.education -> m.02kq20k -> education.education.institution -> University of Canterbury\n# Answer:\nUniversity of Canterbury", "# Reasoning Path:\nErnest Rutherford -> people.deceased_person.place_of_death -> Cambridge -> location.location.contains -> Trinity College, Cambridge\n# Answer:\nCambridge", "# Reasoning Path:\nErnest Rutherford -> people.deceased_person.place_of_death -> Cambridge -> location.location.contains -> University of Cambridge\n# Answer:\nCambridge", "# Reasoning Path:\nErnest Rutherford -> people.deceased_person.place_of_death -> Cambridge -> common.topic.notable_types -> City/Town/Village\n# Answer:\nCambridge", "# Reasoning Path:\nErnest Rutherford -> people.deceased_person.place_of_death -> Cambridge -> location.location.containedby -> United Kingdom\n# Answer:\nCambridge", "# Reasoning Path:\nErnest Rutherford -> base.kwebbase.kwtopic.connections_from -> ernest rutherford corresponded with albert einstein -> base.kwebbase.kwconnection.other -> Albert Einstein\n# Answer:\nernest rutherford corresponded with albert einstein", "# Reasoning Path:\nErnest Rutherford -> people.deceased_person.place_of_death -> Cambridge -> location.location.containedby -> England\n# Answer:\nCambridge", "# Reasoning Path:\nErnest Rutherford -> base.kwebbase.kwtopic.connections_from -> ernest rutherford corresponded with enrico fermi -> base.kwebbase.kwconnection.relation -> corresponded with\n# Answer:\nernest rutherford corresponded with enrico fermi"], "ground_truth": ["University of New Zealand", "University of Cambridge", "Nelson College", "University of Canterbury", "Trinity College, Cambridge"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.3, "ans_recall": 0.6, "path_f1": 0.588235294117647, "path_precision": 0.5, "path_recall": 0.7142857142857143, "path_ans_f1": 0.6153846153846154, "path_ans_precision": 0.5, "path_ans_recall": 0.8}
{"id": "WebQTest-206", "prediction": ["# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Shooting guard -> sports.sports_position.sport -> Basketball\n# Answer:\nShooting guard", "# Reasoning Path:\nBrandon Roy -> sports.pro_athlete.sports_played_professionally -> m.0c53rsf -> sports.pro_sports_played.sport -> Basketball\n# Answer:\nBasketball", "# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Point guard -> sports.sports_position.sport -> Basketball\n# Answer:\nPoint guard", "# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Shooting guard -> common.topic.notable_for -> g.1255g6p7n\n# Answer:\nShooting guard", "# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Shooting guard -> common.topic.image -> Jordan by Lipofsky 16577\n# Answer:\nShooting guard", "# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Point guard -> common.topic.notable_for -> g.125c79ttb\n# Answer:\nPoint guard", "# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Small forward -> sports.sports_position.sport -> Basketball\n# Answer:\nSmall forward", "# Reasoning Path:\nBrandon Roy -> sports.drafted_athlete.drafted -> m.04_bzhf -> sports.sports_league_draft_pick.school -> University of Washington\n# Answer:\nUniversity of Washington", "# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Shooting guard -> common.topic.image -> Basketball half-court\n# Answer:\nShooting guard", "# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Point guard -> common.topic.article -> m.02sg00\n# Answer:\nPoint guard"], "ground_truth": ["Small forward", "Point guard", "Shooting guard"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-207", "prediction": ["# Reasoning Path:\nHarvard University -> location.location.containedby -> Massachusetts -> location.location.containedby -> United States of America\n# Answer:\nMassachusetts", "# Reasoning Path:\nHarvard University -> location.location.containedby -> Massachusetts -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMassachusetts", "# Reasoning Path:\nHarvard University -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nHarvard University -> location.location.containedby -> Massachusetts -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nMassachusetts", "# Reasoning Path:\nHarvard University -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nHarvard University -> location.location.containedby -> Cambridge -> location.location.containedby -> Massachusetts\n# Answer:\nCambridge", "# Reasoning Path:\nHarvard University -> education.educational_institution.subsidiary_or_constituent_schools -> Harvard College -> location.location.containedby -> United States of America\n# Answer:\nHarvard College", "# Reasoning Path:\nHarvard University -> location.location.containedby -> Cambridge -> location.location.containedby -> United States of America\n# Answer:\nCambridge", "# Reasoning Path:\nHarvard University -> education.educational_institution.subsidiary_or_constituent_schools -> Harvard Business School -> location.location.containedby -> United States of America\n# Answer:\nHarvard Business School", "# Reasoning Path:\nHarvard University -> location.location.contains -> Massachusetts Hall -> location.location.containedby -> Massachusetts\n# Answer:\nMassachusetts Hall"], "ground_truth": ["Cambridge", "Massachusetts", "United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-209", "prediction": ["# Reasoning Path:\nLaos -> location.location.containedby -> Southeast Asia -> location.location.containedby -> Asia\n# Answer:\nSoutheast Asia", "# Reasoning Path:\nLaos -> base.locations.countries.continent -> Asia -> travel.travel_destination.tour_operators -> Bunnik Tours\n# Answer:\nAsia", "# Reasoning Path:\nLaos -> location.location.containedby -> Southeast Asia -> location.location.events -> Vietnam War\n# Answer:\nSoutheast Asia", "# Reasoning Path:\nLaos -> location.location.containedby -> Asia -> travel.travel_destination.tour_operators -> Bunnik Tours\n# Answer:\nAsia", "# Reasoning Path:\nLaos -> location.location.containedby -> Southeast Asia -> location.location.contains -> Thailand\n# Answer:\nSoutheast Asia", "# Reasoning Path:\nLaos -> location.statistical_region.debt_service_as_percent_of_trade_volume -> g.1hhc39wvj\n# Answer:\nlocation.statistical_region.debt_service_as_percent_of_trade_volume", "# Reasoning Path:\nLaos -> location.location.containedby -> Eurasia -> common.topic.notable_types -> Location\n# Answer:\nEurasia", "# Reasoning Path:\nLaos -> base.locations.countries.continent -> Asia -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nAsia", "# Reasoning Path:\nLaos -> location.location.containedby -> Southeast Asia -> location.location.contains -> Cambodia\n# Answer:\nSoutheast Asia", "# Reasoning Path:\nLaos -> location.location.containedby -> Asia -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nAsia"], "ground_truth": ["Southeast Asia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> people.person.children -> Sean Cole\n# Answer:\nYvonne Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> people.person.spouse_s -> m.0n9hknn\n# Answer:\nYvonne Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> people.person.children -> Neffeteria Pugh\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> common.topic.notable_for -> g.125dlnswt\n# Answer:\nYvonne Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> people.person.children -> Sean Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan -> music.recording.featured_artists -> J. Holiday\n# Answer:\n#1 Fan", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> people.person.spouse_s -> m.0n9hknn\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> common.topic.notable_types -> Person\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> award.award_nominee.award_nominations -> m.05cqdz0 -> award.award_nomination.nominated_for -> Heaven Sent\n# Answer:\nHeaven Sent", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nFrancine Lons"], "ground_truth": ["Francine Lons", "Sal Gibson", "Leon Cole"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.5, "ans_recall": 0.6666666666666666, "path_f1": 0.5714285714285715, "path_precision": 0.5, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.5, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-210", "prediction": ["# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> location.location.people_born_here -> Wanda Coleman\n# Answer:\nWatts", "# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> location.location.people_born_here -> Viola Wills\n# Answer:\nWatts", "# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> common.topic.image -> Watts in 1912\n# Answer:\nWatts", "# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> location.location.people_born_here -> Aaron Meeks\n# Answer:\nWatts", "# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> location.location.containedby -> Los Angeles\n# Answer:\nWatts", "# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> common.topic.image -> Simon Rodia's Watts Towers.\n# Answer:\nWatts", "# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> location.location.containedby -> Los Angeles County\n# Answer:\nWatts", "# Reasoning Path:\nTyrese Gibson -> film.person_or_entity_appearing_in_film.films -> m.0j33q7h -> film.personal_film_appearance.film -> The Con\n# Answer:\nThe Con", "# Reasoning Path:\nTyrese Gibson -> music.composer.compositions -> 2000 Watts -> music.composition.composer -> Edward Theodore Riley\n# Answer:\n2000 Watts", "# Reasoning Path:\nTyrese Gibson -> music.composer.compositions -> How You Gonna Act Like That -> music.composition.composer -> Harvey Mason, Jr.\n# Answer:\nHow You Gonna Act Like That"], "ground_truth": ["Watts"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-211", "prediction": ["# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Albania\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.sports -> Ice Hockey\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Australia\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n1896 Summer Olympics", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> Australia\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.sports -> Ice Hockey\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> Austria\n# Answer:\n2002 Winter Olympics"], "ground_truth": ["France", "Austria", "Slovenia", "Switzerland"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.013333333333333332, "path_precision": 0.1, "path_recall": 0.007142857142857143, "path_ans_f1": 0.14285714285714288, "path_ans_precision": 0.1, "path_ans_recall": 0.25}
{"id": "WebQTest-212", "prediction": ["# Reasoning Path:\nColorado River -> location.location.partially_containedby -> United States of America -> location.location.partially_containedby -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nColorado River -> location.location.partially_containedby -> United States of America -> location.country.first_level_divisions -> Colorado\n# Answer:\nUnited States of America", "# Reasoning Path:\nColorado River -> geography.river.basin_countries -> United States of America -> location.location.partially_containedby -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nColorado River -> location.location.partially_containedby -> United States of America -> base.locations.countries.states_provinces_within -> Colorado\n# Answer:\nUnited States of America", "# Reasoning Path:\nColorado River -> geography.river.basin_countries -> United States of America -> location.country.first_level_divisions -> Colorado\n# Answer:\nUnited States of America", "# Reasoning Path:\nColorado River -> location.location.partially_containedby -> United States of America -> location.country.first_level_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nColorado River -> location.location.partially_containedby -> United States of America -> base.locations.countries.states_provinces_within -> New Mexico\n# Answer:\nUnited States of America", "# Reasoning Path:\nColorado River -> location.location.partially_containedby -> Colorado -> military.military_unit_place_of_origin.military_units -> McLain's Independent Light Artillery Battery\n# Answer:\nColorado", "# Reasoning Path:\nColorado River -> location.location.partially_containedby -> Colorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Cavalry Regiment\n# Answer:\nColorado", "# Reasoning Path:\nColorado River -> location.location.partially_containedby -> Colorado -> location.location.partially_contains -> White River\n# Answer:\nColorado"], "ground_truth": ["Colorado", "Nevada", "California", "Arizona"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.2727272727272727, "ans_precission": 0.3, "ans_recall": 0.25, "path_f1": 0.2727272727272727, "path_precision": 0.3, "path_recall": 0.25, "path_ans_f1": 0.4, "path_ans_precision": 1.0, "path_ans_recall": 0.25}
{"id": "WebQTest-213", "prediction": ["# Reasoning Path:\nScarlett Johansson -> base.popstra.celebrity.dated -> m.065q6ym -> base.popstra.dated.participant -> Ryan Reynolds\n# Answer:\nRyan Reynolds", "# Reasoning Path:\nScarlett Johansson -> base.popstra.celebrity.dated -> m.065q9sh -> base.popstra.dated.participant -> Josh Hartnett\n# Answer:\nJosh Hartnett", "# Reasoning Path:\nScarlett Johansson -> base.popstra.celebrity.dated -> m.064fp5j -> base.popstra.dated.participant -> nm1157013\n# Answer:\nnm1157013", "# Reasoning Path:\nScarlett Johansson -> celebrities.celebrity.sexual_relationships -> m.04xwldp -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nDated", "# Reasoning Path:\nScarlett Johansson -> celebrities.celebrity.sexual_relationships -> m.04xwldp -> celebrities.romantic_relationship.celebrity -> Josh Hartnett\n# Answer:\nJosh Hartnett", "# Reasoning Path:\nScarlett Johansson -> celebrities.celebrity.sexual_relationships -> m.07bb7c3 -> celebrities.romantic_relationship.celebrity -> Benicio del Toro\n# Answer:\nBenicio del Toro", "# Reasoning Path:\nScarlett Johansson -> celebrities.celebrity.sexual_relationships -> m.07bb7cc -> celebrities.romantic_relationship.celebrity -> Jared Leto\n# Answer:\nJared Leto", "# Reasoning Path:\nScarlett Johansson -> celebrities.celebrity.sexual_relationships -> m.07bb7c3 -> celebrities.romantic_relationship.relationship_type -> Slept with\n# Answer:\nSlept with", "# Reasoning Path:\nScarlett Johansson -> common.topic.webpage -> m.05bv55w -> common.webpage.resource -> Scarlett Johansson @ Hollywood Hotties\n# Answer:\nScarlett Johansson @ Hollywood Hotties", "# Reasoning Path:\nScarlett Johansson -> celebrities.celebrity.sexual_relationships -> m.07bb7cc -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nDated"], "ground_truth": ["Justin Timberlake", "nm1157013", "Jared Leto", "Patrick Wilson", "Josh Hartnett", "Benicio del Toro", "Romain Dauriac", "Ryan Reynolds", "Topher Grace"], "ans_acc": 0.5555555555555556, "ans_hit": 1, "ans_f1": 0.576923076923077, "ans_precission": 0.6, "ans_recall": 0.5555555555555556, "path_f1": 0.3870967741935483, "path_precision": 0.6, "path_recall": 0.2857142857142857, "path_ans_f1": 0.576923076923077, "path_ans_precision": 0.6, "path_ans_recall": 0.5555555555555556}
{"id": "WebQTest-214", "prediction": ["# Reasoning Path:\nRoman Empire -> location.location.partially_contained_by -> m.0ws8w7_ -> location.partial_containment_relationship.partially_contained_by -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nRoman Empire -> location.location.partially_contained_by -> m.0ws8wk6 -> location.partial_containment_relationship.partially_contained_by -> Middle East\n# Answer:\nMiddle East", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Latin Language -> language.human_language.region -> Europe\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Greek Language -> language.human_language.region -> Europe\n# Answer:\nGreek Language", "# Reasoning Path:\nRoman Empire -> location.location.partially_contained_by -> m.0ws8w38 -> location.partial_containment_relationship.partially_contained_by -> North Africa\n# Answer:\nNorth Africa", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Latin Language -> language.human_language.countries_spoken_in -> Duchy of Burgundy\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Latin Language -> language.human_language.countries_spoken_in -> Republic of Genoa\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Latin Language -> music.genre.albums -> Acustico\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Latin Language -> language.human_language.countries_spoken_in -> Kingdom of Aragon\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Latin Language -> music.genre.albums -> Amar sin mentiras\n# Answer:\nLatin Language"], "ground_truth": ["North Africa", "Middle East", "Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-215", "prediction": ["# Reasoning Path:\nElvis Costello -> people.person.place_of_birth -> Paddington -> location.location.containedby -> London\n# Answer:\nPaddington", "# Reasoning Path:\nElvis Costello -> people.person.place_of_birth -> Paddington -> location.location.containedby -> United Kingdom\n# Answer:\nPaddington", "# Reasoning Path:\nElvis Costello -> tv.tv_actor.guest_roles -> m.040p_0y -> tv.tv_guest_role.episodes_appeared_in -> How I Spent My Strummer Vacation\n# Answer:\nHow I Spent My Strummer Vacation", "# Reasoning Path:\nElvis Costello -> tv.tv_actor.guest_roles -> m.0b6vqtt -> tv.tv_guest_role.episodes_appeared_in -> Kidney Now!\n# Answer:\nKidney Now!", "# Reasoning Path:\nElvis Costello -> common.topic.webpage -> Discogs page -> common.webpage.category -> Topic Webpage\n# Answer:\nDiscogs page", "# Reasoning Path:\nElvis Costello -> tv.tv_actor.guest_roles -> m.04djjqv -> tv.tv_guest_role.episodes_appeared_in -> Farewell, Nervosa\n# Answer:\nFarewell, Nervosa", "# Reasoning Path:\nElvis Costello -> common.topic.webpage -> m.03kx15c -> common.webpage.resource -> m.0blvlws\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nElvis Costello -> common.topic.webpage -> MusicMoz page for Elvis Costello -> common.webpage.category -> Topic Webpage\n# Answer:\nMusicMoz page for Elvis Costello", "# Reasoning Path:\nElvis Costello -> common.topic.webpage -> m.03kx15c -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website"], "ground_truth": ["Paddington"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-216", "prediction": ["# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> location.location.containedby -> Eurasia\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> location.location.containedby -> Asia\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> location.location.containedby -> East Asia\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> government.governmental_jurisdiction.governing_officials -> m.010g61fb\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> location.statistical_region.external_debt_stock -> g.11b71px2lk\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> government.governmental_jurisdiction.governing_officials -> m.010g61kh\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> geography.geographical_feature.category -> Desert -> fictional_universe.type_of_fictional_setting.settings -> Sandia\n# Answer:\nDesert", "# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> government.governmental_jurisdiction.governing_officials -> m.010g61rc\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> location.statistical_region.external_debt_stock -> g.1hhc390rw\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> geography.geographical_feature.category -> Desert -> geography.geographical_feature_category.features -> Atacama Desert\n# Answer:\nDesert"], "ground_truth": ["Mongolia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-217", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> South Korea\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> American Samoa\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Northern Mariana Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guam\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Mexico\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Marshall Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Germany\n# Answer:\nFederal republic"], "ground_truth": ["Federal republic", "Presidential system", "Constitutional republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-218", "prediction": ["# Reasoning Path:\nGermany -> location.location.containedby -> Eurasia\n# Answer:\nEurasia", "# Reasoning Path:\nGermany -> base.locations.countries.continent -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nGermany -> location.location.containedby -> Western Europe -> location.location.containedby -> Europe\n# Answer:\nWestern Europe", "# Reasoning Path:\nGermany -> location.location.containedby -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nGermany -> base.locations.countries.continent -> Europe -> organization.organization_scope.organizations_with_this_scope -> European Union\n# Answer:\nEurope", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> base.locations.countries.continent -> Europe -> base.locations.continents.planet -> Earth\n# Answer:\nEurope", "# Reasoning Path:\nGermany -> location.location.containedby -> Europe -> organization.organization_scope.organizations_with_this_scope -> European Union\n# Answer:\nEurope", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fdl8\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.location.containedby -> Europe -> base.locations.continents.planet -> Earth\n# Answer:\nEurope"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> France\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Portugal\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Syria\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3f_h6\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives"], "ground_truth": ["Provisional government", "Semi-presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-220", "prediction": ["# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Kurdish languages -> language.human_language.countries_spoken_in -> Armenia\n# Answer:\nKurdish languages", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Ladino Language -> common.topic.notable_types -> Human Language\n# Answer:\nLadino Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nArabic Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Ladino Language -> language.human_language.region -> Asia\n# Answer:\nLadino Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Syria\n# Answer:\nArabic Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Arabic Language -> common.topic.notable_types -> Human Language\n# Answer:\nArabic Language", "# Reasoning Path:\nTurkey -> location.statistical_region.minimum_wage -> g.11b60lkkk3\n# Answer:\nlocation.statistical_region.minimum_wage", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Algeria\n# Answer:\nArabic Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Kurdish languages -> language.human_language.countries_spoken_in -> Iran\n# Answer:\nKurdish languages", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Kurdish languages -> common.topic.notable_types -> Human Language\n# Answer:\nKurdish languages"], "ground_truth": ["Arabic Language", "Kurdish languages", "Zaza language", "Turkish Language", "Ladino Language"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7200000000000001, "ans_precission": 0.9, "ans_recall": 0.6, "path_f1": 0.7200000000000001, "path_precision": 0.9, "path_recall": 0.6, "path_ans_f1": 0.7200000000000001, "path_ans_precision": 0.9, "path_ans_recall": 0.6}
{"id": "WebQTest-221", "prediction": ["# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0ng9xr6 -> base.schemastaging.athlete_salary.team -> Houston Rockets\n# Answer:\nHouston Rockets", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0j2mvjf -> base.schemastaging.athlete_salary.team -> New York Knicks\n# Answer:\nNew York Knicks", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0ng9xr6 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nJeremy Lin -> sports.pro_athlete.teams -> m.0k6s01p -> sports.sports_team_roster.team -> New York Knicks\n# Answer:\nNew York Knicks", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0_qrbd1 -> base.schemastaging.athlete_salary.team -> Houston Rockets\n# Answer:\nHouston Rockets", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0j2mvjf -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nJeremy Lin -> sports.pro_athlete.teams -> m.0k6s01p -> sports.sports_team_roster.position -> Point guard\n# Answer:\nPoint guard", "# Reasoning Path:\nJeremy Lin -> sports.pro_athlete.teams -> m.01145k7g -> sports.sports_team_roster.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nJeremy Lin -> sports.pro_athlete.teams -> m.0j2dfqc -> sports.sports_team_roster.team -> Houston Rockets\n# Answer:\nHouston Rockets", "# Reasoning Path:\nJeremy Lin -> people.person.education -> m.0dw8bzt -> education.education.degree -> Bachelor's degree\n# Answer:\nBachelor's degree"], "ground_truth": ["Los Angeles Lakers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-222", "prediction": ["# Reasoning Path:\nSeychelles -> location.location.containedby -> Southern Sun Hotel Group -> location.location.containedby -> g.1pxpr66mx\n# Answer:\nSouthern Sun Hotel Group", "# Reasoning Path:\nSeychelles -> location.location.containedby -> Africa -> organization.organization_scope.organizations_with_this_scope -> Southern African Development Community\n# Answer:\nAfrica", "# Reasoning Path:\nSeychelles -> base.locations.countries.continent -> Africa -> organization.organization_scope.organizations_with_this_scope -> Southern African Development Community\n# Answer:\nAfrica", "# Reasoning Path:\nSeychelles -> location.statistical_region.high_tech_as_percent_of_manufactured_exports -> g.1hhc3_l38\n# Answer:\nlocation.statistical_region.high_tech_as_percent_of_manufactured_exports", "# Reasoning Path:\nSeychelles -> location.location.containedby -> Africa -> base.locations.continents.countries_within -> Zimbabwe\n# Answer:\nAfrica", "# Reasoning Path:\nSeychelles -> location.location.containedby -> Africa -> location.location.time_zones -> Seychelles Time Zone\n# Answer:\nAfrica", "# Reasoning Path:\nSeychelles -> base.locations.countries.continent -> Africa -> base.locations.continents.countries_within -> Zimbabwe\n# Answer:\nAfrica", "# Reasoning Path:\nSeychelles -> base.locations.countries.continent -> Africa -> location.location.time_zones -> Seychelles Time Zone\n# Answer:\nAfrica", "# Reasoning Path:\nSeychelles -> location.location.containedby -> Africa -> base.locations.continents.countries_within -> Algeria\n# Answer:\nAfrica", "# Reasoning Path:\nSeychelles -> base.locations.countries.continent -> Africa -> base.locations.continents.countries_within -> Algeria\n# Answer:\nAfrica"], "ground_truth": ["Africa", "Southern Sun Hotel Group"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-224", "prediction": ["# Reasoning Path:\nWilliam Morris -> people.person.education -> m.0n1jtn3 -> education.education.institution -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nWilliam Morris -> people.person.education -> m.02wmrt8 -> education.education.institution -> Exeter College, Oxford\n# Answer:\nExeter College, Oxford", "# Reasoning Path:\nWilliam Morris -> people.person.education -> m.04q1gj3 -> education.education.institution -> Marlborough College\n# Answer:\nMarlborough College", "# Reasoning Path:\nWilliam Morris -> base.kwebbase.kwtopic.has_sentences -> After marrying, Morris commissioned an architect friend to build him a home at Bexleyheath in Kent which became known as the Red House  because it was built of red bricks with no stucco finish. -> base.kwebbase.kwsentence.next_sentence -> The Red House was the first of this type of domestic architecture in nineteenth century England.\n# Answer:\nAfter marrying, Morris commissioned an architect friend to build him a home at Bexleyheath in Kent which became known as the Red House  because it was built of red bricks with no stucco finish.", "# Reasoning Path:\nWilliam Morris -> base.kwebbase.kwtopic.connections_from -> william morris a pal of alphonse mucha -> base.kwebbase.kwconnection.relation -> a pal of\n# Answer:\nwilliam morris a pal of alphonse mucha", "# Reasoning Path:\nWilliam Morris -> base.kwebbase.kwtopic.connections_from -> william morris a pal of edward coley burne-jones -> base.kwebbase.kwconnection.sentence -> At university Morris met the future artist Burne-Jones who became a lifelong friend.\n# Answer:\nwilliam morris a pal of edward coley burne-jones", "# Reasoning Path:\nWilliam Morris -> base.kwebbase.kwtopic.connections_from -> william morris a pal of alphonse mucha -> base.kwebbase.kwconnection.sentence -> Was a friend of Mucha and Yeats.\n# Answer:\nwilliam morris a pal of alphonse mucha", "# Reasoning Path:\nWilliam Morris -> base.kwebbase.kwtopic.connections_from -> william morris a pal of william butler yeats -> base.kwebbase.kwconnection.relation -> a pal of\n# Answer:\nwilliam morris a pal of william butler yeats", "# Reasoning Path:\nWilliam Morris -> base.kwebbase.kwtopic.has_sentences -> Among others, he was heard by the future novelist, HG Wells and Marx's daughter, Eleanor Marx. -> base.kwebbase.kwsentence.previous_sentence -> He read \\\"Das Kapital\\\" by Karl Marx  and in 1883 joined the Socialist Democratic Federation and began to tour the industrial areas of Britain, giving lectures on socialism.\n# Answer:\nAmong others, he was heard by the future novelist, HG Wells and Marx's daughter, Eleanor Marx.", "# Reasoning Path:\nWilliam Morris -> base.kwebbase.kwtopic.has_sentences -> After marrying, Morris commissioned an architect friend to build him a home at Bexleyheath in Kent which became known as the Red House  because it was built of red bricks with no stucco finish. -> base.kwebbase.kwsentence.previous_sentence -> In 1858 he published a volume of poems under the title \\\"The Defence of Guenevere and Other Poems\\\".\n# Answer:\nAfter marrying, Morris commissioned an architect friend to build him a home at Bexleyheath in Kent which became known as the Red House  because it was built of red bricks with no stucco finish."], "ground_truth": ["University of Oxford", "Exeter College, Oxford"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-225", "prediction": ["# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.containedby -> United States of America\n# Answer:\nWashington", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.containedby -> Washington\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.containedby -> United States, with Territories\n# Answer:\nWashington", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.containedby -> Contiguous United States\n# Answer:\nWashington", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Cascade Range -> location.location.containedby -> North America\n# Answer:\nCascade Range", "# Reasoning Path:\nMount St. Helens -> geography.mountain.mountain_range -> Cascade Range -> location.location.containedby -> North America\n# Answer:\nCascade Range", "# Reasoning Path:\nMount St. Helens -> common.image.size -> m.02bkwf9\n# Answer:\ncommon.image.size", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> government.political_district.representatives -> m.04j8hdx\n# Answer:\nWashington", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.containedby -> Portland-Vancouver-Beaverton, OR-WA Metropolitan Statistical Area\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.events -> 2009 Cherry Blossom 10-Mile Run\n# Answer:\nWashington"], "ground_truth": ["Washington"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-226", "prediction": ["# Reasoning Path:\nJimmy Savile -> people.person.profession -> Radio personality -> base.descriptive_names.names.descriptive_name -> m.010260w1\n# Answer:\nRadio personality", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Radio personality -> fictional_universe.character_occupation.characters_with_this_occupation -> Frasier Crane\n# Answer:\nRadio personality", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Disc jockey -> people.profession.specialization_of -> Entertainer\n# Answer:\nDisc jockey", "# Reasoning Path:\nJimmy Savile -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Radio personality -> common.topic.subjects -> Chris Free\n# Answer:\nRadio personality", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Presenter -> fictional_universe.character_occupation.characters_with_this_occupation -> Toby Isaacs\n# Answer:\nPresenter", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Radio personality -> base.descriptive_names.names.descriptive_name -> m.0102636p\n# Answer:\nRadio personality", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Radio personality -> base.descriptive_names.names.descriptive_name -> m.010263c4\n# Answer:\nRadio personality", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Radio personality -> fictional_universe.character_occupation.characters_with_this_occupation -> Gil Chesterton\n# Answer:\nRadio personality", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Disc jockey -> common.topic.webpage -> m.046cmsk\n# Answer:\nDisc jockey"], "ground_truth": ["Radio personality", "Disc jockey", "Presenter"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-227", "prediction": ["# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> Ontario -> location.province.capital -> Toronto\n# Answer:\nOntario", "# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> Ontario -> location.location.containedby -> Canada\n# Answer:\nOntario", "# Reasoning Path:\nSeneca people -> people.ethnicity.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> Ontario -> periodicals.newspaper_circulation_area.newspapers -> Toronto Star\n# Answer:\nOntario", "# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> New York -> book.book_subject.works -> All Around the Town\n# Answer:\nNew York", "# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> New York -> location.location.geolocation -> m.046vxf5\n# Answer:\nNew York", "# Reasoning Path:\nSeneca people -> base.casinos.casino_owner.casinos_owned -> Seneca Niagara Casino & Hotel -> common.topic.notable_for -> g.1259vb0h_\n# Answer:\nSeneca Niagara Casino & Hotel", "# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> New York -> book.book_subject.works -> Amusing the Million\n# Answer:\nNew York", "# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> New York -> government.political_district.representatives -> m.011vj0lv\n# Answer:\nNew York", "# Reasoning Path:\nSeneca people -> people.ethnicity.languages_spoken -> English Language -> language.human_language.main_country -> Australia\n# Answer:\nEnglish Language"], "ground_truth": ["New York", "Ontario"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.events -> Assassination of Martin Luther King, Jr.\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.containedby -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Alabama\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> common.topic.article -> m.05nnh0\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nAlpha Man: The Brotherhood of MLK", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0_714v2 -> film.personal_film_appearance.film -> Alice Walker: Beauty in Truth\n# Answer:\nAlice Walker: Beauty in Truth", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Montgomery\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0gbz10_ -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-231", "prediction": ["# Reasoning Path:\nMiami Marlins -> sports.sports_team.league -> m.0crtdb6 -> sports.sports_league_participation.league -> National League East\n# Answer:\nNational League East", "# Reasoning Path:\nMiami Marlins -> sports.sports_team.league -> m.0crt4jw -> sports.sports_league_participation.league -> Major League Baseball\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nMiami Marlins -> sports.sports_team.previously_known_as -> Florida Marlins -> sports.sports_team.championships -> 1997 World Series\n# Answer:\nFlorida Marlins", "# Reasoning Path:\nMiami Marlins -> sports.sports_team.league -> m.0crt79d -> sports.sports_league_participation.league -> National League\n# Answer:\nNational League", "# Reasoning Path:\nMiami Marlins -> sports.sports_team.previously_known_as -> Florida Marlins -> common.topic.notable_types -> Sports Team\n# Answer:\nFlorida Marlins", "# Reasoning Path:\nMiami Marlins -> sports.professional_sports_team.draft_picks -> m.010q_p9h -> sports.sports_league_draft_pick.player -> Tyler Kolek\n# Answer:\nTyler Kolek", "# Reasoning Path:\nMiami Marlins -> sports.professional_sports_team.draft_picks -> m.010q_p9h -> sports.sports_league_draft_pick.draft -> 2014 Major League Baseball season\n# Answer:\n2014 Major League Baseball season", "# Reasoning Path:\nMiami Marlins -> sports.professional_sports_team.draft_picks -> m.04vy3tq -> sports.sports_league_draft_pick.player -> Charles Johnson\n# Answer:\nCharles Johnson", "# Reasoning Path:\nMiami Marlins -> sports.professional_sports_team.draft_picks -> m.04vy3tq -> sports.sports_league_draft_pick.school -> University of Miami\n# Answer:\nUniversity of Miami", "# Reasoning Path:\nMiami Marlins -> sports.professional_sports_team.draft_picks -> m.04vy3tq -> sports.sports_league_draft_pick.draft -> 1992 Major League Baseball Draft\n# Answer:\n1992 Major League Baseball Draft"], "ground_truth": ["1994 Major League Baseball Season"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-232", "prediction": ["# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.country.form_of_government -> Unitary state\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.location.containedby -> Africa\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.country.official_language -> French\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.country.form_of_government -> Presidential system\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Gibraltar -> location.country.form_of_government -> Parliamentary system\n# Answer:\nGibraltar", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.country.form_of_government -> Republic\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Gibraltar -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nGibraltar", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Gibraltar -> travel.travel_destination.tourist_attractions -> Trafalgar Cemetery\n# Answer:\nGibraltar", "# Reasoning Path:\nSpanish Language -> base.rosetta.languoid.local_name -> Spanish -> base.rosetta.local_name.locale -> Puerto Rico\n# Answer:\nSpanish", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.country.official_language -> Portuguese Language\n# Answer:\nEquatorial Guinea"], "ground_truth": ["Saint Lucia", "Spain", "Uruguay", "Vatican City", "Nicaragua", "Colombia", "Andorra", "Costa Rica", "Belize", "Gibraltar", "Canada", "Bolivia", "Equatorial Guinea", "Ecuador", "Cura\u00e7ao", "Panama", "Kingdom of Aragon", "Northern Mariana Islands", "Argentina", "Venezuela", "El Salvador", "Dominican Republic", "Honduras", "Cuba", "Barbados", "Guatemala", "Mexico", "Chile", "Paraguay", "United States of America", "Peru", "Guyana", "Western Sahara", "Puerto Rico"], "ans_acc": 0.08823529411764706, "ans_hit": 1, "ans_f1": 0.11042944785276074, "ans_precission": 0.9, "ans_recall": 0.058823529411764705, "path_f1": 0.11042944785276074, "path_precision": 0.9, "path_recall": 0.058823529411764705, "path_ans_f1": 0.1621621621621622, "path_ans_precision": 1.0, "path_ans_recall": 0.08823529411764706}
{"id": "WebQTest-233", "prediction": ["# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Castle Hot Springs -> location.location.containedby -> Yavapai County\n# Answer:\nCastle Hot Springs", "# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Castle Hot Springs -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.0_ly_wt\n# Answer:\nCastle Hot Springs", "# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Grand Canyon -> travel.tourist_attraction.near_travel_destination -> Grand Canyon National Park\n# Answer:\nGrand Canyon", "# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Ro Ho En -> common.topic.article -> m.03qmw7w\n# Answer:\nRo Ho En", "# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Ro Ho En -> common.topic.image -> RO HO EN 2007 Dec 08 40230-1\n# Answer:\nRo Ho En", "# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Grand Canyon -> travel.tourist_attraction.near_travel_destination -> Lake Powell\n# Answer:\nGrand Canyon", "# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Grand Canyon -> location.location.geolocation -> m.02_ltg5\n# Answer:\nGrand Canyon", "# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Castle Hot Springs -> location.location.containedby -> Arizona\n# Answer:\nCastle Hot Springs", "# Reasoning Path:\nPhoenix -> location.citytown.postal_codes -> 85001 -> location.postal_code.country -> United States of America\n# Answer:\n85001", "# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Castle Hot Springs -> location.location.containedby -> Hieroglyphic Mountains\n# Answer:\nCastle Hot Springs"], "ground_truth": ["Grand Canyon", "Phoenix Mountain Preserve", "Phoenix Art Museum", "Musical Instrument Museum", "Heard Museum", "Pueblo Grande Ruin", "Ro Ho En", "Arizona Science Center", "Mesa Arts Center", "Desert Botanical Garden", "Castle Hot Springs"], "ans_acc": 0.2727272727272727, "ans_hit": 1, "ans_f1": 0.41860465116279066, "ans_precission": 0.9, "ans_recall": 0.2727272727272727, "path_f1": 0.41860465116279066, "path_precision": 0.9, "path_recall": 0.2727272727272727, "path_ans_f1": 0.41860465116279066, "path_ans_precision": 0.9, "path_ans_recall": 0.2727272727272727}
{"id": "WebQTest-234", "prediction": ["# Reasoning Path:\nPanama Canal -> location.location.containedby -> North America -> location.location.containedby -> Northern Hemisphere\n# Answer:\nNorth America", "# Reasoning Path:\nPanama Canal -> location.location.containedby -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nPanama Canal -> location.location.containedby -> Panama -> location.location.containedby -> North America\n# Answer:\nPanama", "# Reasoning Path:\nPanama Canal -> base.engineering.canal.major_cities -> Col\u00f3n -> location.location.containedby -> Panama\n# Answer:\nCol\u00f3n", "# Reasoning Path:\nPanama Canal -> location.location.containedby -> North America -> location.location.containedby -> Western Hemisphere\n# Answer:\nNorth America", "# Reasoning Path:\nPanama Canal -> location.location.containedby -> North America -> base.locations.continents.countries_within -> United States of America\n# Answer:\nNorth America", "# Reasoning Path:\nPanama Canal -> location.location.containedby -> North America -> cvg.computer_game_region.versions_released_in_this_region -> 2Moons\n# Answer:\nNorth America", "# Reasoning Path:\nPanama Canal -> location.location.containedby -> Panama -> location.location.containedby -> Central America\n# Answer:\nPanama", "# Reasoning Path:\nPanama Canal -> location.location.containedby -> Panama -> base.locations.countries.continent -> North America\n# Answer:\nPanama", "# Reasoning Path:\nPanama Canal -> base.engineering.canal.major_cities -> Col\u00f3n -> location.location.containedby -> Col\u00f3n District, Panama\n# Answer:\nCol\u00f3n"], "ground_truth": ["Panama"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-235", "prediction": ["# Reasoning Path:\nCharlie Chaplin -> people.person.profession -> Screenwriter -> common.topic.notable_types -> Profession\n# Answer:\nScreenwriter", "# Reasoning Path:\nCharlie Chaplin -> people.person.profession -> Comedian -> common.topic.notable_types -> Profession\n# Answer:\nComedian", "# Reasoning Path:\nCharlie Chaplin -> people.person.profession -> Film Score Composer -> common.topic.notable_types -> Profession\n# Answer:\nFilm Score Composer", "# Reasoning Path:\nCharlie Chaplin -> people.person.profession -> Film Score Composer -> people.profession.specialization_of -> Composer\n# Answer:\nFilm Score Composer", "# Reasoning Path:\nCharlie Chaplin -> influence.influence_node.influenced -> Benny Hill -> people.person.profession -> Comedian\n# Answer:\nBenny Hill", "# Reasoning Path:\nCharlie Chaplin -> film.editor.film -> A Burlesque on Carmen -> film.film.produced_by -> Jess Robbins\n# Answer:\nA Burlesque on Carmen", "# Reasoning Path:\nCharlie Chaplin -> influence.influence_node.influenced -> Benny Hill -> people.person.profession -> Screenwriter\n# Answer:\nBenny Hill", "# Reasoning Path:\nCharlie Chaplin -> influence.influence_node.influenced -> Blake Clark -> people.person.profession -> Voice Actor\n# Answer:\nBlake Clark", "# Reasoning Path:\nCharlie Chaplin -> influence.influence_node.influenced -> Bob Clampett -> people.person.profession -> Film director\n# Answer:\nBob Clampett", "# Reasoning Path:\nCharlie Chaplin -> film.editor.film -> A Burlesque on Carmen -> film.film.language -> English Language\n# Answer:\nA Burlesque on Carmen"], "ground_truth": ["Film Editor", "Film director", "Comedian", "Composer", "Film Score Composer", "Film Producer", "Screenwriter", "Actor"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.4, "ans_recall": 0.5, "path_f1": 0.38709677419354843, "path_precision": 0.4, "path_recall": 0.375, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.8, "path_ans_recall": 0.75}
{"id": "WebQTest-237", "prediction": ["# Reasoning Path:\nFred Durst -> people.person.profession -> Musician -> fictional_universe.character_occupation.characters_with_this_occupation -> George Harrison\n# Answer:\nMusician", "# Reasoning Path:\nFred Durst -> people.person.profession -> Musician -> freebase.equivalent_topic.equivalent_type -> Musical Artist\n# Answer:\nMusician", "# Reasoning Path:\nFred Durst -> people.person.profession -> Guitarist -> people.profession.specialization_of -> Musician\n# Answer:\nGuitarist", "# Reasoning Path:\nFred Durst -> people.person.profession -> Singer -> common.topic.subject_of -> Alan Motley\n# Answer:\nSinger", "# Reasoning Path:\nFred Durst -> people.person.profession -> Musician -> fictional_universe.character_occupation.characters_with_this_occupation -> John Lennon\n# Answer:\nMusician", "# Reasoning Path:\nFred Durst -> people.person.profession -> Guitarist -> freebase.type_hints.included_types -> Person\n# Answer:\nGuitarist", "# Reasoning Path:\nFred Durst -> people.person.profession -> Musician -> fictional_universe.character_occupation.characters_with_this_occupation -> Paul\n# Answer:\nMusician", "# Reasoning Path:\nFred Durst -> people.person.profession -> Musician -> people.profession.specializations -> Rapper\n# Answer:\nMusician", "# Reasoning Path:\nFred Durst -> people.person.profession -> Singer -> base.lightweight.profession.specialization_of -> Musicians and Singers\n# Answer:\nSinger", "# Reasoning Path:\nFred Durst -> music.producer.tracks_produced -> A Flat -> music.recording.tracks -> g.11b81rr9s9\n# Answer:\nA Flat"], "ground_truth": ["Musician", "Businessperson", "Songwriter", "Film director", "Singer", "Guitarist", "Rapper", "Actor"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5294117647058825, "ans_precission": 0.9, "ans_recall": 0.375, "path_f1": 0.5294117647058825, "path_precision": 0.9, "path_recall": 0.375, "path_ans_f1": 0.6428571428571429, "path_ans_precision": 0.9, "path_ans_recall": 0.5}
{"id": "WebQTest-239", "prediction": ["# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Mississippi\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> location.country.first_level_divisions -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> base.locations.countries.states_provinces_within -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> base.locations.countries.states_provinces_within -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> location.country.first_level_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> location.country.first_level_divisions -> Mississippi\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> base.locations.countries.states_provinces_within -> Alaska\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> base.animal_synopses.animal_synopsis.height -> m.0xn6rcr -> base.qualified_values.qualified_value.unit -> Inch\n# Answer:\nInch"], "ground_truth": ["United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> Maryland\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime. -> media_common.quotation.subjects -> Christians and Christianity\n# Answer:\nA strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime. -> common.topic.image -> Edgar Allan Poe signature\n# Answer:\nA strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.", "# Reasoning Path:\nEdgar Allan Poe -> film.film_story_contributor.film_story_credits -> Beczka amontillado -> film.film.genre -> Short Film\n# Answer:\nBeczka amontillado", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong. -> common.topic.article -> m.05chc0k\n# Answer:\nA wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong.", "# Reasoning Path:\nEdgar Allan Poe -> film.film_story_contributor.film_story_credits -> An Evening of Edgar Allan Poe -> film.film.genre -> Drama\n# Answer:\nAn Evening of Edgar Allan Poe", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong. -> media_common.quotation.subjects -> Revenge\n# Answer:\nA wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong.", "# Reasoning Path:\nEdgar Allan Poe -> film.film_story_contributor.film_story_credits -> An Evening of Edgar Allan Poe -> media_common.adaptation.adapted_from -> The Cask of Amontillado\n# Answer:\nAn Evening of Edgar Allan Poe"], "ground_truth": ["Baltimore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-240", "prediction": ["# Reasoning Path:\nAlbert Einstein -> people.person.employment_history -> m.02kq3y4 -> business.employment_tenure.company -> University of Zurich\n# Answer:\nUniversity of Zurich", "# Reasoning Path:\nAlbert Einstein -> people.person.employment_history -> m.02kq4ch -> business.employment_tenure.company -> Leiden University\n# Answer:\nLeiden University", "# Reasoning Path:\nAlbert Einstein -> people.person.employment_history -> m.02kq3y4 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nAlbert Einstein -> people.person.employment_history -> m.02kq3h6 -> business.employment_tenure.company -> Charles University in Prague\n# Answer:\nCharles University in Prague", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k05ps4 -> education.education.institution -> University of Zurich\n# Answer:\nUniversity of Zurich", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0j_z4g5 -> education.education.institution -> Luitpold Gymnasium\n# Answer:\nLuitpold Gymnasium", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k03_k5 -> education.education.institution -> ETH Zurich\n# Answer:\nETH Zurich", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0j_z4g5 -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k05ps4 -> freebase.valuenotation.has_value -> Minor\n# Answer:\nMinor", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k05ps4 -> freebase.valuenotation.is_reviewed -> Minor\n# Answer:\nMinor"], "ground_truth": ["University of Zurich", "Charles University in Prague", "Institute for Advanced Study", "Leiden University", "ETH Zurich"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.6153846153846154, "ans_precission": 0.5, "ans_recall": 0.8, "path_f1": 0.5263157894736842, "path_precision": 0.5, "path_recall": 0.5555555555555556, "path_ans_f1": 0.6153846153846154, "path_ans_precision": 0.5, "path_ans_recall": 0.8}
{"id": "WebQTest-241", "prediction": ["# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> fictional_universe.fictional_character_creator.fictional_characters_created -> Brian Griffin\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.02ntr0h\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> God\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> common.topic.notable_for -> g.1255sxsvl\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> fictional_universe.fictional_character_creator.fictional_characters_created -> Chris Griffin\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> award.award_winner.awards_won -> m.0ly_pz4\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.works_set_here -> Family Guy Video Game!\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.02kk65p\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> Adam West\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> fictional_universe.fictional_character_creator.fictional_characters_created -> Cleveland Brown\n# Answer:\nSeth MacFarlane"], "ground_truth": ["Seth MacFarlane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-243", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> people.person.place_of_birth -> New Haven -> location.location.containedby -> United States of America\n# Answer:\nNew Haven", "# Reasoning Path:\nGeorge W. Bush -> people.person.place_of_birth -> New Haven -> base.biblioness.bibs_location.state -> Connecticut\n# Answer:\nNew Haven", "# Reasoning Path:\nGeorge W. Bush -> people.person.place_of_birth -> New Haven -> location.location.containedby -> Connecticut\n# Answer:\nNew Haven", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointed_role -> Director of the Bureau of Counterterrorism\n# Answer:\nDirector of the Bureau of Counterterrorism", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pk -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointee -> Henry A. Crumpton\n# Answer:\nHenry A. Crumpton", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1 -> people.appointment.appointed_role -> United States Ambassador to Angola\n# Answer:\nUnited States Ambassador to Angola", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079q3lx -> people.appointment.appointed_role -> United States Ambassador to Brazil\n# Answer:\nUnited States Ambassador to Brazil", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pv -> film.personal_film_appearance.film -> All About Ann: Governor Richards of the Lone Star State\n# Answer:\nAll About Ann: Governor Richards of the Lone Star State", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1 -> people.appointment.appointee -> Dan Mozena\n# Answer:\nDan Mozena"], "ground_truth": ["New Haven"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-245", "prediction": ["# Reasoning Path:\nMiss America -> time.recurring_event.instances -> 1928 Miss America -> time.event.locations -> Atlantic City\n# Answer:\n1928 Miss America", "# Reasoning Path:\nMiss America -> time.recurring_event.instances -> 1928 Miss America -> common.topic.notable_for -> g.125bn860g\n# Answer:\n1928 Miss America", "# Reasoning Path:\nMiss America -> time.recurring_event.instances -> 1928 Miss America -> common.topic.notable_types -> Competition\n# Answer:\n1928 Miss America", "# Reasoning Path:\nMiss America -> time.recurring_event.instances -> 1950 Miss America -> common.topic.notable_for -> g.125ft33p8\n# Answer:\n1950 Miss America", "# Reasoning Path:\nMiss America -> time.recurring_event.instances -> 1950 Miss America -> common.topic.notable_types -> Competition\n# Answer:\n1950 Miss America", "# Reasoning Path:\nMiss America -> time.recurring_event.instances -> 1929 Miss America -> common.topic.notable_types -> Competition\n# Answer:\n1929 Miss America", "# Reasoning Path:\nMiss America -> award.recurring_competition.individual_competitions -> Miss America 1921 -> time.event.locations -> The Pier Shops at Caesars\n# Answer:\nMiss America 1921", "# Reasoning Path:\nMiss America -> time.recurring_event.instances -> 1950 Miss America -> common.topic.article -> m.05h9j4r\n# Answer:\n1950 Miss America", "# Reasoning Path:\nMiss America -> common.topic.webpage -> m.0704g7v -> common.webpage.resource -> Miss America site\n# Answer:\nMiss America site", "# Reasoning Path:\nMiss America -> award.recurring_competition.individual_competitions -> 1964 Miss America -> common.topic.notable_for -> g.125g_pwks\n# Answer:\n1964 Miss America"], "ground_truth": ["1921-09"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-246", "prediction": ["# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Wireless telegraphy -> law.invention.inventor -> Jozef Murga\u0161\n# Answer:\nWireless telegraphy", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Wireless telegraphy -> common.topic.notable_types -> Invention\n# Answer:\nWireless telegraphy", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Alternating current -> common.topic.notable_for -> g.125d8rdgm\n# Answer:\nAlternating current", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Wireless telegraphy -> common.topic.image -> Hertz schematic0\n# Answer:\nWireless telegraphy", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Wireless telegraphy -> law.invention.inventor -> Heinrich Hertz\n# Answer:\nWireless telegraphy", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Alternating current -> common.topic.image -> City lights viewed in a motion blurred exposure. The AC blinking causes the lines to be dotted rather than continuous\n# Answer:\nAlternating current", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Alternating current -> common.topic.notable_types -> Invention\n# Answer:\nAlternating current", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Induction motor -> engineering.engine_category.category_of -> Electric motor\n# Answer:\nInduction motor", "# Reasoning Path:\nNikola Tesla -> book.author.works_written -> 'Death Ray' for Planes -> book.written_work.original_language -> English Language\n# Answer:\n'Death Ray' for Planes", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Induction motor -> base.schemastaging.context_name.pronunciation -> g.125_nxfln\n# Answer:\nInduction motor"], "ground_truth": ["Induction motor", "Three-phase electric power", "Wireless telegraphy", "Alternating current", "Remote control", "Tesla coil", "Neon lamp"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5806451612903225, "ans_precission": 0.9, "ans_recall": 0.42857142857142855, "path_f1": 0.5806451612903225, "path_precision": 0.9, "path_recall": 0.42857142857142855, "path_ans_f1": 0.5806451612903225, "path_ans_precision": 0.9, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-247", "prediction": ["# Reasoning Path:\nFlemish people -> people.ethnicity.included_in_group -> Germanic peoples -> people.ethnicity.includes_groups -> Austrians\n# Answer:\nGermanic peoples", "# Reasoning Path:\nFlemish people -> people.ethnicity.included_in_group -> Germanic peoples -> people.ethnicity.includes_groups -> British people\n# Answer:\nGermanic peoples", "# Reasoning Path:\nFlemish people -> people.ethnicity.included_in_group -> Germanic peoples -> people.ethnicity.people -> \u00c9mil Goeldi\n# Answer:\nGermanic peoples", "# Reasoning Path:\nFlemish people -> people.ethnicity.included_in_group -> Germanic peoples -> people.ethnicity.includes_groups -> Danes\n# Answer:\nGermanic peoples", "# Reasoning Path:\nFlemish people -> people.ethnicity.included_in_group -> Germanic peoples -> book.book_subject.works -> The Roman Empire and Its Germanic Peoples\n# Answer:\nGermanic peoples", "# Reasoning Path:\nFlemish people -> people.ethnicity.geographic_distribution -> United States of America -> olympics.olympic_participating_country.olympics_participated_in -> 1900 Summer Olympics\n# Answer:\nUnited States of America", "# Reasoning Path:\nFlemish people -> people.ethnicity.included_in_group -> Germanic peoples -> people.ethnicity.people -> DJ Lil Syn\n# Answer:\nGermanic peoples", "# Reasoning Path:\nFlemish people -> people.ethnicity.geographic_distribution -> United States of America -> sports.sport_country.multi_event_tournaments_participated_in -> 2012 World Mountain Running Championships\n# Answer:\nUnited States of America", "# Reasoning Path:\nFlemish people -> people.ethnicity.included_in_group -> Germanic peoples -> people.ethnicity.people -> Ricimer\n# Answer:\nGermanic peoples", "# Reasoning Path:\nFlemish people -> people.ethnicity.geographic_distribution -> United States of America -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics\n# Answer:\nUnited States of America"], "ground_truth": ["Canada", "United States of America", "France", "Australia", "Brazil", "Belgium", "South Africa"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0.19354838709677416, "ans_precission": 0.3, "ans_recall": 0.14285714285714285, "path_f1": 0.19354838709677416, "path_precision": 0.3, "path_recall": 0.14285714285714285, "path_ans_f1": 0.19354838709677416, "path_ans_precision": 0.3, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-248", "prediction": ["# Reasoning Path:\nNew York City -> location.location.containedby -> New York -> location.location.containedby -> United States of America\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> base.biblioness.bibs_location.state -> New York -> location.location.containedby -> United States of America\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> base.aareas.schema.administrative_area.administrative_parent -> New York -> location.location.containedby -> United States of America\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> location.location.containedby -> New York -> government.governmental_jurisdiction.government_bodies -> United States Bankruptcy Court for the Southern District of New York\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> New York\n# Answer:\nUnited States of America", "# Reasoning Path:\nNew York City -> location.location.containedby -> Area code 917 -> location.location.containedby -> New York\n# Answer:\nArea code 917", "# Reasoning Path:\nNew York City -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> New York\n# Answer:\nUnited States of America", "# Reasoning Path:\nNew York City -> location.location.containedby -> New York -> travel.travel_destination.tourist_attractions -> Ellis Island\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> location.location.containedby -> New York -> travel.travel_destination.tourist_attractions -> American Museum of Natural History\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> New York\n# Answer:\nUnited States of America"], "ground_truth": ["New York"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-249", "prediction": ["# Reasoning Path:\nBruce Lee -> people.deceased_person.cause_of_death -> Cerebral edema -> common.topic.article -> m.0219c4\n# Answer:\nCerebral edema", "# Reasoning Path:\nBruce Lee -> people.deceased_person.cause_of_death -> Cerebral edema -> people.cause_of_death.people -> Richard Nixon\n# Answer:\nCerebral edema", "# Reasoning Path:\nBruce Lee -> people.deceased_person.cause_of_death -> Cerebral edema -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nCerebral edema", "# Reasoning Path:\nBruce Lee -> people.deceased_person.cause_of_death -> Cerebral edema -> people.cause_of_death.people -> Alan Ladd\n# Answer:\nCerebral edema", "# Reasoning Path:\nBruce Lee -> people.deceased_person.cause_of_death -> Cerebral edema -> people.cause_of_death.people -> David Sharp\n# Answer:\nCerebral edema", "# Reasoning Path:\nBruce Lee -> freebase.valuenotation.is_reviewed -> Martial Art(s) -> type.property.expected_type -> Martial Art\n# Answer:\nMartial Art(s)", "# Reasoning Path:\nBruce Lee -> people.person.profession -> Martial Artist -> people.profession.specialization_of -> Athlete\n# Answer:\nMartial Artist", "# Reasoning Path:\nBruce Lee -> freebase.valuenotation.is_reviewed -> Martial Art(s) -> type.property.reverse_property -> Practitioner\n# Answer:\nMartial Art(s)", "# Reasoning Path:\nBruce Lee -> freebase.valuenotation.is_reviewed -> Cause of death -> rdf-schema#range -> Cause Of Death\n# Answer:\nCause of death", "# Reasoning Path:\nBruce Lee -> freebase.valuenotation.is_reviewed -> Martial Art(s) -> type.property.schema -> Martial Artist\n# Answer:\nMartial Art(s)"], "ground_truth": ["Cerebral edema"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-250", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> South Korea\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> American Samoa\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Northern Mariana Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guam\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Germany\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Marshall Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Mexico\n# Answer:\nFederal republic"], "ground_truth": ["Federal republic", "Presidential system", "Constitutional republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-251", "prediction": ["# Reasoning Path:\nEngland -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Northern Ireland\n# Answer:\nPound sterling", "# Reasoning Path:\nEngland -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Wales\n# Answer:\nPound sterling", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0njwhw5\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nEngland -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Kingdom of England\n# Answer:\nPound sterling", "# Reasoning Path:\nEngland -> location.location.events -> English Civil War -> time.event.locations -> Kingdom of England\n# Answer:\nEnglish Civil War", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0hq0kmy\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nEngland -> location.location.events -> English Civil War -> time.event.people_involved -> Oliver Cromwell\n# Answer:\nEnglish Civil War", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0mv3kv1\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nEngland -> location.location.events -> 1938 lunar eclipse -> common.topic.article -> m.0bj8ptd\n# Answer:\n1938 lunar eclipse", "# Reasoning Path:\nEngland -> location.location.events -> English Civil War -> military.military_conflict.military_personnel_involved -> Vincent Potter\n# Answer:\nEnglish Civil War"], "ground_truth": ["Pound sterling"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-252", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> education.education.institution -> Boston Latin School\n# Answer:\nBoston Latin School", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.is_reviewed -> Institution\n# Answer:\nInstitution", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> American literature\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.has_no_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nMinor", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.is_reviewed -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> :Library and information science\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.size -> m.02bc9fn\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze"], "ground_truth": ["Boston Latin School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-253", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Tyrone\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> base.aareas.schema.administrative_area.administrative_children -> East Midlands\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Scotland -> location.country.first_level_divisions -> Aberdeen\n# Answer:\nScotland", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nChild pornography", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> base.aareas.schema.administrative_area.administrative_children -> East of England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> location.country.first_level_divisions -> East Midlands\n# Answer:\nEngland"], "ground_truth": ["England", "Northern Ireland", "Scotland", "Wales"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.6, "path_precision": 0.5, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-254", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.country.official_language -> Malay Language\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> China -> location.location.containedby -> Eurasia\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.country.languages_spoken -> Nepali Language\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> base.aareas.schema.administrative_area.administrative_area_type -> Sovereign state\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Singapore -> location.country.official_language -> Tamil Language\n# Answer:\nSingapore", "# Reasoning Path:\nChinese language -> language.human_language.main_country -> China -> location.location.containedby -> Eurasia\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.country.languages_spoken -> English Language\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Singapore -> location.country.languages_spoken -> Tamil Language\n# Answer:\nSingapore", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> China -> location.country.official_language -> Standard Chinese\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Singapore -> location.country.official_language -> Standard Chinese\n# Answer:\nSingapore"], "ground_truth": ["Vietnam", "Canada", "Malaysia", "China", "Brunei", "Sh\u011bn", "Singapore"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 1.0, "ans_recall": 0.42857142857142855, "path_f1": 0.5806451612903225, "path_precision": 0.9, "path_recall": 0.42857142857142855, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-255", "prediction": ["# Reasoning Path:\nBarack Obama -> people.person.education -> m.0nbyhqc -> education.education.institution -> Noelani Elementary School\n# Answer:\nNoelani Elementary School", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgp6 -> education.education.institution -> Punahou School\n# Answer:\nPunahou School", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgmw -> education.education.institution -> Columbia University\n# Answer:\nColumbia University", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.0nbyhqc -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgmw -> education.education.degree -> Bachelor of Arts\n# Answer:\nBachelor of Arts", "# Reasoning Path:\nBarack Obama -> people.person.ethnicity -> English American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nEnglish American", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.0nbyhqc -> freebase.valuenotation.has_no_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.subjects -> Guerrilla Decontextualization\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Bound Man -> book.written_work.subjects -> United States of America\n# Answer:\nA Bound Man", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.0nbyhqc -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nMinor"], "ground_truth": ["Columbia University", "Noelani Elementary School", "State Elementary School Menteng 01", "Harvard Law School", "Punahou School", "St. Francis of Assisi Catholic School", "Occidental College"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.3529411764705882, "ans_precission": 0.3, "ans_recall": 0.42857142857142855, "path_f1": 0.3529411764705882, "path_precision": 0.3, "path_recall": 0.42857142857142855, "path_ans_f1": 0.3529411764705882, "path_ans_precision": 0.3, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-256", "prediction": ["# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> language.human_language.countries_spoken_in -> New Zealand\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> common.topic.notable_types -> Human Language\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> language.human_language.countries_spoken_in -> Macau\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.location.partially_contains -> Teram Kangri -> base.schemastaging.disputed_location.claimed_by -> Pakistan\n# Answer:\nTeram Kangri", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Yili horse -> common.topic.notable_types -> Organism Classification\n# Answer:\nYili horse", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Tibetan Terrier -> biology.animal_breed.breed_of -> Dog\n# Answer:\nTibetan Terrier", "# Reasoning Path:\nChina -> location.location.partially_contains -> Amur River -> location.location.partially_containedby -> Russia\n# Answer:\nAmur River", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Tibetan Terrier -> base.petbreeds.dog_breed.color -> Black\n# Answer:\nTibetan Terrier", "# Reasoning Path:\nChina -> location.location.partially_contains -> Teram Kangri -> location.location.partially_containedby -> Xinjiang\n# Answer:\nTeram Kangri"], "ground_truth": ["Standard Chinese"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-257", "prediction": ["# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.office_holder -> Nikita Khrushchev\n# Answer:\nNikita Khrushchev", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.governmental_body -> Central Committee of the Communist Party of the Soviet Union\n# Answer:\nCentral Committee of the Communist Party of the Soviet Union", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_holder -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_6 -> government.government_position_held.office_holder -> Vladimir Ivashko\n# Answer:\nVladimir Ivashko", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_commander -> Nikita Khrushchev\n# Answer:\nNikita Khrushchev", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_conflict -> Cuban Missile Crisis\n# Answer:\nCuban Missile Crisis", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf -> military.military_command.military_commander -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_6 -> government.government_position_held.office_position_or_title -> General Secretary of the Communist Party of the Soviet Union\n# Answer:\nGeneral Secretary of the Communist Party of the Soviet Union", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_position_or_title -> General Secretary of the Communist Party of the Soviet Union\n# Answer:\nGeneral Secretary of the Communist Party of the Soviet Union", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.049y3kf -> military.military_command.military_commander -> Ivan Konev\n# Answer:\nIvan Konev"], "ground_truth": ["Nikita Khrushchev", "Leonid Brezhnev"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.28571428571428575, "ans_precission": 0.2, "ans_recall": 0.5, "path_f1": 0.28571428571428575, "path_precision": 0.2, "path_recall": 0.5, "path_ans_f1": 0.28571428571428575, "path_ans_precision": 0.2, "path_ans_recall": 0.5}
{"id": "WebQTest-258", "prediction": ["# Reasoning Path:\nAustralia -> location.statistical_region.major_exports -> m.0cnqyp9 -> location.imports_exports_by_industry.industry -> Agriculture\n# Answer:\nAgriculture", "# Reasoning Path:\nAustralia -> location.statistical_region.major_exports -> m.0cnqyp5 -> location.imports_exports_by_industry.industry -> Energy industry\n# Answer:\nEnergy industry", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_55mr\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_2hqk\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_6vhn\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp"], "ground_truth": ["Agriculture", "Energy industry"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> sports.sports_facility.teams -> Atlanta Falcons\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> United States of America\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola -> location.location.containedby -> Fulton County\n# Answer:\nWorld of Coca-Cola", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola -> common.topic.notable_types -> Museum\n# Answer:\nWorld of Coca-Cola", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> common.topic.notable_types -> Zoo\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.containedby -> United States of America\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> aviation.airport.serves -> Bullpen Rib House\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> travel.transport_terminus.travel_destinations_served -> m.052zwlt\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Atlanta Regional Airport -> aviation.airport.serves -> Peachtree City\n# Answer:\nAtlanta Regional Airport"], "ground_truth": ["Turner Field", "Center for Puppetry Arts", "Atlanta Symphony Orchestra", "Atlanta Marriott Marquis", "Hyatt Regency Atlanta", "Six Flags White Water", "Arbor Place Mall", "Centennial Olympic Park", "Atlanta Cyclorama & Civil War Museum", "Jimmy Carter Library and Museum", "The Tabernacle", "Variety Playhouse", "World of Coca-Cola", "Four Seasons Hotel Atlanta", "Georgia State Capitol", "Woodruff Arts Center", "Atlanta History Center", "Atlanta Ballet", "Cobb Energy Performing Arts Centre", "Masquerade", "Underground Atlanta", "Margaret Mitchell House & Museum", "Fernbank Museum of Natural History", "Fernbank Science Center", "Fox Theatre", "Peachtree Road Race", "CNN Center", "Atlanta Jewish Film Festival", "Martin Luther King, Jr. National Historic Site", "Georgia Aquarium", "Six Flags Over Georgia", "Omni Coliseum", "Georgia Dome", "Philips Arena", "Zoo Atlanta", "Georgia World Congress Center"], "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0.14893617021276595, "ans_precission": 0.7, "ans_recall": 0.08333333333333333, "path_f1": 0.14893617021276595, "path_precision": 0.7, "path_recall": 0.08333333333333333, "path_ans_f1": 0.19178082191780818, "path_ans_precision": 0.7, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-260", "prediction": ["# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> location.location.containedby -> Dominion of Melchizedek\n# Answer:\nJerusalem", "# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> location.location.containedby -> Jerusalem District\n# Answer:\nJerusalem", "# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> travel.travel_destination.tourist_attractions -> Yad Vashem\n# Answer:\nJerusalem", "# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> location.location.containedby -> Mandatory Palestine\n# Answer:\nJerusalem", "# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> travel.travel_destination.tourist_attractions -> Bethphage\n# Answer:\nJerusalem", "# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> travel.travel_destination.tourist_attractions -> Church of the Holy Sepulchre\n# Answer:\nJerusalem", "# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> location.location.events -> 1929 Palestine riots\n# Answer:\nJerusalem", "# Reasoning Path:\nCity of David -> common.topic.notable_types -> Location -> freebase.type_hints.included_types -> Topic\n# Answer:\nLocation", "# Reasoning Path:\nCity of David -> common.topic.image -> JPF-Ophel - City of David -> common.image.size -> m.025dmvs\n# Answer:\nJPF-Ophel - City of David", "# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> location.location.events -> 1947 Jerusalem riots\n# Answer:\nJerusalem"], "ground_truth": ["Jerusalem"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-261", "prediction": ["# Reasoning Path:\nCindy Sherman -> people.person.profession -> Visual Artist -> fictional_universe.character_occupation.characters_with_this_occupation -> Eug\u00e8ne Delacroix\n# Answer:\nVisual Artist", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Film director -> freebase.type_profile.published -> Published\n# Answer:\nFilm director", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Visual Artist -> type.type.expected_by -> Artist\n# Answer:\nVisual Artist", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Artist -> fictional_universe.character_occupation.characters_with_this_occupation -> Andy Warhol\n# Answer:\nArtist", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Artist -> organization.role.governors -> m.0112k4dj\n# Answer:\nArtist", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Visual Artist -> type.type.expected_by -> producer\n# Answer:\nVisual Artist", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Visual Artist -> base.descriptive_names.names.descriptive_name -> m.0106_1y2\n# Answer:\nVisual Artist", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Artist -> fictional_universe.character_occupation.characters_with_this_occupation -> Byron Montgomery\n# Answer:\nArtist", "# Reasoning Path:\nCindy Sherman -> film.actor.film -> m.0yd601t -> film.performance.film -> !Women Art Revolution\n# Answer:\n!Women Art Revolution", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Visual Artist -> type.type.expected_by -> Artists\n# Answer:\nVisual Artist"], "ground_truth": ["Photographer", "Film director", "Visual Artist", "Artist"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.8181818181818182, "path_precision": 0.9, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-262", "prediction": ["# Reasoning Path:\nShawnee Smith -> tv.tv_actor.starring_roles -> m.0j9dzw6 -> tv.regular_tv_appearance.series -> The Stand\n# Answer:\nThe Stand", "# Reasoning Path:\nShawnee Smith -> film.actor.film -> m.098d405 -> film.performance.film -> The Stand\n# Answer:\nThe Stand", "# Reasoning Path:\nShawnee Smith -> tv.tv_actor.starring_roles -> m.05cmxdm -> tv.regular_tv_appearance.series -> Scream Queens\n# Answer:\nScream Queens", "# Reasoning Path:\nShawnee Smith -> tv.tv_actor.starring_roles -> m.03jq10c -> tv.regular_tv_appearance.series -> Becker\n# Answer:\nBecker", "# Reasoning Path:\nShawnee Smith -> film.actor.film -> m.098d405 -> film.performance.character -> Julie Lawry\n# Answer:\nJulie Lawry", "# Reasoning Path:\nShawnee Smith -> film.actor.film -> m.02vc307 -> film.performance.film -> The Blob\n# Answer:\nThe Blob", "# Reasoning Path:\nShawnee Smith -> film.actor.film -> m.02vd7w0 -> film.performance.film -> Saw II\n# Answer:\nSaw II", "# Reasoning Path:\nShawnee Smith -> film.actor.film -> m.02vc307 -> film.performance.character -> Meg Penny\n# Answer:\nMeg Penny", "# Reasoning Path:\nShawnee Smith -> film.actor.film -> m.02vd7w0 -> film.performance.character -> Amanda Young\n# Answer:\nAmanda Young", "# Reasoning Path:\nShawnee Smith -> common.topic.webpage -> m.0bnt7_g -> common.webpage.in_index -> Blissful Master Index\n# Answer:\nBlissful Master Index"], "ground_truth": ["The Stand", "Arsenio", "Anger Management", "All is Forgiven", "Brand New Life", "Becker", "Scream Queens", "The Tom Show", "30 Days of Night: Dust to Dust"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.4, "ans_recall": 0.3333333333333333, "path_f1": 0.4000000000000001, "path_precision": 0.4, "path_recall": 0.4, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.4, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-264", "prediction": ["# Reasoning Path:\nAustralia -> location.country.currency_used -> Australian dollar -> finance.currency.countries_used -> Tuvalu\n# Answer:\nAustralian dollar", "# Reasoning Path:\nAustralia -> location.country.currency_used -> Australian dollar -> common.topic.notable_types -> Currency\n# Answer:\nAustralian dollar", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.country.currency_used -> Australian dollar -> finance.currency.countries_used -> Christmas Island\n# Answer:\nAustralian dollar", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nAustralia -> location.country.currency_used -> Australian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands\n# Answer:\nAustralian dollar", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_55mr\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_2hqk\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_6vhn\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp"], "ground_truth": ["Australian dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-265", "prediction": ["# Reasoning Path:\nRome -> location.location.nearby_airports -> Urbe Airport -> location.location.geolocation -> m.04fngwt\n# Answer:\nUrbe Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Urbe Airport -> aviation.airport.hub_for -> Ala Littoria\n# Answer:\nUrbe Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> aviation.airport.hub_for -> Ryanair\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> common.topic.article -> m.033_5b\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> aviation.airport.serves -> Ciampino\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> aviation.airport.hub_for -> Aerolinee Itavia\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Italian Met Office Airport -> location.location.geolocation -> m.04fngkg\n# Answer:\nItalian Met Office Airport", "# Reasoning Path:\nRome -> base.popstra.location.vacationers -> m.063hfk9 -> base.popstra.vacation_choice.vacationer -> Eva Longoria\n# Answer:\nEva Longoria", "# Reasoning Path:\nRome -> base.popstra.location.vacationers -> m.0649tj_ -> base.popstra.vacation_choice.vacationer -> David Beckham\n# Answer:\nDavid Beckham", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> aviation.airport.hub_for -> Mistral Air\n# Answer:\nCiampino\u2013G. B. Pastine International Airport"], "ground_truth": ["Ciampino\u2013G. B. Pastine International Airport", "Leonardo da Vinci\u2013Fiumicino Airport"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-266", "prediction": ["# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Worlds of Fun -> location.location.time_zones -> Central Time Zone\n# Answer:\nWorlds of Fun", "# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Worlds of Fun -> amusement_parks.park.rides -> Zambezi Zinger\n# Answer:\nWorlds of Fun", "# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Oceans of Fun -> amusement_parks.park.rides -> Typhoon\n# Answer:\nOceans of Fun", "# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Oceans of Fun -> common.topic.article -> m.08c_19\n# Answer:\nOceans of Fun", "# Reasoning Path:\nKansas City -> location.statistical_region.population -> g.11b66slpck\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Oceans of Fun -> location.location.geolocation -> m.0cmg71w\n# Answer:\nOceans of Fun", "# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Liberty Memorial -> location.location.containedby -> Missouri\n# Answer:\nLiberty Memorial", "# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Worlds of Fun -> amusement_parks.park.rides -> Boomerang\n# Answer:\nWorlds of Fun", "# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Oceans of Fun -> amusement_parks.park.rides -> Aruba Tuba\n# Answer:\nOceans of Fun", "# Reasoning Path:\nKansas City -> location.place_with_neighborhoods.neighborhoods -> Crown Center -> location.location.containedby -> United States of America\n# Answer:\nCrown Center"], "ground_truth": ["Sprint Center", "Kansas City Zoo", "Kemper Museum of Contemporary Art", "Deer Creek Golf Course", "Kauffman Center for the Performing Arts", "Harry S. Truman Presidential Library and Museum", "Liberty Memorial", "Worlds of Fun", "Oceans of Fun", "Crown Center", "Negro Leagues Baseball Museum", "Hotel Phillips", "Ewing and Muriel Kauffman Memorial Garden", "18th and Vine-Downtown East", "Boulevard Brewing Company"], "ans_acc": 0.26666666666666666, "ans_hit": 1, "ans_f1": 0.41142857142857137, "ans_precission": 0.9, "ans_recall": 0.26666666666666666, "path_f1": 0.41142857142857137, "path_precision": 0.9, "path_recall": 0.26666666666666666, "path_ans_f1": 0.41142857142857137, "path_ans_precision": 0.9, "path_ans_recall": 0.26666666666666666}
{"id": "WebQTest-267", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.person.place_of_birth -> Atlanta -> travel.travel_destination.tourist_attractions -> Martin Luther King, Jr. National Historic Site\n# Answer:\nAtlanta", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.place_of_birth -> Atlanta -> location.location.containedby -> United States of America\n# Answer:\nAtlanta", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Alabama\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.place_of_birth -> Atlanta -> location.location.containedby -> Georgia\n# Answer:\nAtlanta", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nAlpha Man: The Brotherhood of MLK", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> common.topic.article -> m.05nnh0\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0_714v2 -> film.personal_film_appearance.film -> Alice Walker: Beauty in Truth\n# Answer:\nAlice Walker: Beauty in Truth", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Montgomery\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0gbz10_ -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage"], "ground_truth": ["Atlanta"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-268", "prediction": ["# Reasoning Path:\nLeBron James -> sports.sports_award_winner.awards -> m.02kbccr -> sports.sports_award.season -> 2003\u201304 NBA season\n# Answer:\n2003\u201304 NBA season", "# Reasoning Path:\nLeBron James -> sports.sports_award_winner.awards -> m.0w0h79l -> sports.sports_award.season -> 2012\u201313 NBA season\n# Answer:\n2012\u201313 NBA season", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04qp97c -> basketball.basketball_player_stats.season -> 2004\u201305 NBA season\n# Answer:\n2004\u201305 NBA season", "# Reasoning Path:\nLeBron James -> sports.sports_award_winner.awards -> m.02kbccr -> sports.sports_award.team -> Cleveland Cavaliers\n# Answer:\nCleveland Cavaliers", "# Reasoning Path:\nLeBron James -> sports.sports_award_winner.awards -> m.04ynxp7 -> sports.sports_award.season -> 2007\u201308 NBA season\n# Answer:\n2007\u201308 NBA season", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04qp97c -> basketball.basketball_player_stats.team -> Cleveland Cavaliers\n# Answer:\nCleveland Cavaliers", "# Reasoning Path:\nLeBron James -> sports.sports_award_winner.awards -> m.0w0h79l -> sports.sports_award.team -> Miami Heat\n# Answer:\nMiami Heat", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04qcjh2 -> basketball.basketball_player_stats.season -> 2005\u201306 NBA season\n# Answer:\n2005\u201306 NBA season", "# Reasoning Path:\nLeBron James -> sports.sports_award_winner.awards -> m.0w0h79l -> sports.sports_award.award -> NBA Most Valuable Player Award\n# Answer:\nNBA Most Valuable Player Award", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04ql_x2 -> basketball.basketball_player_stats.season -> 2007\u201308 NBA season\n# Answer:\n2007\u201308 NBA season"], "ground_truth": ["2003\u201304 NBA season"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-269", "prediction": ["# Reasoning Path:\nDelaware River -> geography.river.origin -> Mount Jefferson -> common.topic.article -> m.036r_d\n# Answer:\nMount Jefferson", "# Reasoning Path:\nDelaware River -> geography.river.origin -> West Branch Delaware River -> geography.river.origin -> Schoharie County\n# Answer:\nWest Branch Delaware River", "# Reasoning Path:\nDelaware River -> geography.river.origin -> Mount Jefferson -> location.location.geolocation -> m.02_f418\n# Answer:\nMount Jefferson", "# Reasoning Path:\nDelaware River -> geography.river.origin -> West Branch Delaware River -> location.location.containedby -> United States of America\n# Answer:\nWest Branch Delaware River", "# Reasoning Path:\nDelaware River -> geography.river.origin -> Mount Jefferson -> location.location.containedby -> New York\n# Answer:\nMount Jefferson", "# Reasoning Path:\nDelaware River -> geography.river.origin -> West Branch Delaware River -> common.topic.notable_for -> g.1258hkxlv\n# Answer:\nWest Branch Delaware River", "# Reasoning Path:\nDelaware River -> common.topic.webpage -> m.09ytwnd -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nDelaware River -> location.location.people_born_here -> Netawatwees -> common.topic.notable_for -> g.125drclq7\n# Answer:\nNetawatwees", "# Reasoning Path:\nDelaware River -> geography.river.origin -> Mount Jefferson -> location.location.containedby -> Jefferson\n# Answer:\nMount Jefferson", "# Reasoning Path:\nDelaware River -> geography.river.origin -> West Branch Delaware River -> location.location.containedby -> North America\n# Answer:\nWest Branch Delaware River"], "ground_truth": ["West Branch Delaware River", "Mount Jefferson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-270", "prediction": ["# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> United Kingdom\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Australia\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.11b71r82sc\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Canada\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_4lv6\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_6fj1\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.12cp_jcjg\n# Answer:\nlocation.statistical_region.deposit_interest_rate"], "ground_truth": ["Parliamentary system", "Constitutional monarchy", "Unitary state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-271", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.parents -> Anna Amelia Romney\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.place_of_birth -> Colonia Dubl\u00e1n\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> Lenore Romney -> people.person.parents -> Harold A. Lafount\n# Answer:\nLenore Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.parents -> Gaskell Romney\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.children -> Jane Romney\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> Lenore Romney -> people.person.place_of_birth -> Logan\n# Answer:\nLenore Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.children -> G. Scott Romney\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0_gvz4k -> film.personal_film_appearance.film -> Mitt\n# Answer:\nMitt", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.children -> Margo Lynn Romney\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> Lenore Romney -> people.person.education -> m.0j4lzcz\n# Answer:\nLenore Romney"], "ground_truth": ["Detroit"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-273", "prediction": ["# Reasoning Path:\nMichael Jordan -> common.image.appears_in_topic_gallery -> Oscar Robertson Trophy\n# Answer:\nOscar Robertson Trophy", "# Reasoning Path:\nMichael Jordan -> common.image.appears_in_topic_gallery -> Atlantic Coast Conference Men's Basketball Player of the Year\n# Answer:\nAtlantic Coast Conference Men's Basketball Player of the Year", "# Reasoning Path:\nMichael Jordan -> sports.drafted_athlete.drafted -> m.02h7cgv -> sports.sports_league_draft_pick.draft -> 1984 NBA draft\n# Answer:\n1984 NBA draft", "# Reasoning Path:\nMichael Jordan -> common.image.appears_in_topic_gallery -> Award share\n# Answer:\nAward share", "# Reasoning Path:\nMichael Jordan -> sports.drafted_athlete.drafted -> m.02h7cgv -> sports.sports_league_draft_pick.team -> Chicago Bulls\n# Answer:\nChicago Bulls", "# Reasoning Path:\nMichael Jordan -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Esophageal cancer\n# Answer:\nMale", "# Reasoning Path:\nMichael Jordan -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Pancreatic cancer\n# Answer:\nMale", "# Reasoning Path:\nMichael Jordan -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Prostate cancer\n# Answer:\nMale"], "ground_truth": ["1984"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-276", "prediction": ["# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> Marin County -> location.location.containedby -> San Francisco Bay Area\n# Answer:\nMarin County", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> Marin County -> location.location.containedby -> San Francisco-San Mateo-Redwood City, CA Metropolitan Division\n# Answer:\nMarin County", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> 94901 -> location.location.containedby -> Marin County\n# Answer:\n94901", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> Marin County -> location.location.containedby -> California\n# Answer:\nMarin County", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> Marin County -> base.localfood.food_producing_region.seasonal_availability -> m.04_jf_1\n# Answer:\nMarin County", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> Marin County -> location.statistical_region.population -> g.11b674q3py\n# Answer:\nMarin County", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> Marin County -> base.localfood.food_producing_region.seasonal_availability -> m.04_jf_p\n# Answer:\nMarin County", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> 94901 -> location.location.geometry -> m.057gz4z\n# Answer:\n94901", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> common.topic.notable_types -> Project focus -> freebase.type_profile.strict_included_types -> Inanimate\n# Answer:\nProject focus", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> common.topic.notable_types -> Project focus -> type.type.properties -> Projects\n# Answer:\nProject focus"], "ground_truth": ["Marin County", "94901"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-277", "prediction": ["# Reasoning Path:\nEsther -> people.person.spouse_s -> m.0130wv8p -> people.marriage.spouse -> Xerxes I\n# Answer:\nXerxes I", "# Reasoning Path:\nEsther -> people.person.spouse_s -> m.0130wv8p -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nEsther -> people.person.spouse_s -> m.012zbnxz -> people.marriage.spouse -> Ahasuerus\n# Answer:\nAhasuerus", "# Reasoning Path:\nEsther -> people.person.spouse_s -> m.012zbnxz -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nEsther -> book.book_subject.works -> Daily Inspiration for Finding Favor With the King -> book.written_work.subjects -> The Bible\n# Answer:\nDaily Inspiration for Finding Favor With the King", "# Reasoning Path:\nEsther -> book.book_subject.works -> Finding Favor With the King -> book.written_work.subjects -> The Bible\n# Answer:\nFinding Favor With the King", "# Reasoning Path:\nEsther -> people.person.religion -> Judaism -> religion.religion.notable_figures -> Theodor Herzl\n# Answer:\nJudaism", "# Reasoning Path:\nEsther -> book.book_subject.works -> Daily Inspiration for Finding Favor With the King -> book.book.genre -> Inspirational\n# Answer:\nDaily Inspiration for Finding Favor With the King", "# Reasoning Path:\nEsther -> book.book_subject.works -> Finding Favor With the King -> book.written_work.author -> Tommy Tenney\n# Answer:\nFinding Favor With the King", "# Reasoning Path:\nEsther -> people.person.religion -> Judaism -> religion.religion.deities -> Elohim\n# Answer:\nJudaism"], "ground_truth": ["Ahasuerus", "Xerxes I"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.26666666666666666, "path_precision": 0.2, "path_recall": 0.4, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-278", "prediction": ["# Reasoning Path:\nClaire Danes -> base.popstra.celebrity.dated -> m.065q9gy -> base.popstra.dated.participant -> Billy Crudup\n# Answer:\nBilly Crudup", "# Reasoning Path:\nClaire Danes -> base.popstra.celebrity.dated -> m.063t679 -> base.popstra.dated.participant -> Matt Damon\n# Answer:\nMatt Damon", "# Reasoning Path:\nClaire Danes -> base.popstra.celebrity.dated -> m.063t67_ -> base.popstra.dated.participant -> Billy Crudup\n# Answer:\nBilly Crudup", "# Reasoning Path:\nClaire Danes -> celebrities.celebrity.sexual_relationships -> m.05cw3vz -> celebrities.romantic_relationship.celebrity -> Hugh Dancy\n# Answer:\nHugh Dancy", "# Reasoning Path:\nClaire Danes -> celebrities.celebrity.sexual_relationships -> m.05cw3vz -> celebrities.romantic_relationship.relationship_type -> Engaged to\n# Answer:\nEngaged to", "# Reasoning Path:\nClaire Danes -> celebrities.celebrity.sexual_relationships -> m.03yxkqm -> celebrities.romantic_relationship.celebrity -> Billy Crudup\n# Answer:\nBilly Crudup", "# Reasoning Path:\nClaire Danes -> film.actor.film -> m.012w8_c9 -> film.performance.film -> Princess Mononoke\n# Answer:\nPrincess Mononoke", "# Reasoning Path:\nClaire Danes -> film.actor.film -> m.0220p8c -> film.performance.character -> Daisy\n# Answer:\nDaisy", "# Reasoning Path:\nClaire Danes -> celebrities.celebrity.sexual_relationships -> m.03yxktc -> celebrities.romantic_relationship.celebrity -> Hugh Dancy\n# Answer:\nHugh Dancy", "# Reasoning Path:\nClaire Danes -> film.actor.film -> m.0220p8c -> film.performance.film -> I Love You, I Love You Not\n# Answer:\nI Love You, I Love You Not"], "ground_truth": ["Hugh Dancy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.23529411764705882, "path_precision": 0.2, "path_recall": 0.2857142857142857, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-279", "prediction": ["# Reasoning Path:\nSt. Paul -> location.location.containedby -> Russell County -> location.location.containedby -> Virginia\n# Answer:\nRussell County", "# Reasoning Path:\nSt. Paul -> location.location.containedby -> Wise County -> location.location.containedby -> Virginia\n# Answer:\nWise County", "# Reasoning Path:\nSt. Paul -> location.location.containedby -> Russell County -> location.location.containedby -> United States of America\n# Answer:\nRussell County", "# Reasoning Path:\nSt. Paul -> location.location.containedby -> Russell County -> location.administrative_division.capital -> m.0jvvsmf\n# Answer:\nRussell County", "# Reasoning Path:\nSt. Paul -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nSt. Paul -> location.citytown.postal_codes -> 24283 -> location.location.containedby -> Wise County\n# Answer:\n24283", "# Reasoning Path:\nSt. Paul -> location.statistical_region.population -> g.11b66hhtgq\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSt. Paul -> location.location.containedby -> Wise County -> location.location.containedby -> United States of America\n# Answer:\nWise County", "# Reasoning Path:\nSt. Paul -> location.location.containedby -> Russell County -> location.location.people_born_here -> W. I. Thomas\n# Answer:\nRussell County", "# Reasoning Path:\nSt. Paul -> location.location.containedby -> Wise County -> location.statistical_region.rent50_3 -> m.05ggjk4\n# Answer:\nWise County"], "ground_truth": ["Russell County", "Wise County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.containedby -> Queensland\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.containedby -> Australia\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.district_represented -> Electoral district of South Brisbane\n# Answer:\nElectoral district of South Brisbane", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> common.topic.notable_types -> City/Town/Village\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Adrienne Pickering\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.appears_in_topic_gallery -> Queensland state election, 2009\n# Answer:\nAnna Bligh crop", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Alan Marshal\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Alison Mosely\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr301h -> government.government_position_held.office_position_or_title -> Premier of Queensland\n# Answer:\nPremier of Queensland"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-281", "prediction": ["# Reasoning Path:\nJapan -> sports.sports_team_location.teams -> Bridgestone-Anchor -> sports.sports_team.sport -> Cycling\n# Answer:\nBridgestone-Anchor", "# Reasoning Path:\nJapan -> sports.sports_team_location.teams -> Burning -> common.topic.notable_for -> g.1yl5j5_sh\n# Answer:\nBurning", "# Reasoning Path:\nJapan -> sports.sports_team_location.teams -> Bridgestone-Anchor -> common.topic.notable_for -> g.1ypjsqnlh\n# Answer:\nBridgestone-Anchor", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> sports.sports_team_location.teams -> Burning -> sports.sports_team.sport -> Professional wrestling\n# Answer:\nBurning", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.11b71r82sc\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nJapan -> sports.sports_team_location.teams -> Burning -> common.topic.notable_types -> Sports Team\n# Answer:\nBurning", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_4lv6\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_6fj1\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.12cp_jcjg\n# Answer:\nlocation.statistical_region.deposit_interest_rate"], "ground_truth": ["Burning", "Japan national handball team", "Japan national football team", "Japan national baseball team", "Japan women's national volleyball team", "Bridgestone-Anchor", "Japan women's national handball team", "Japan men's national volleyball team"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.3333333333333333, "ans_precission": 0.5, "ans_recall": 0.25, "path_f1": 0.3333333333333333, "path_precision": 0.5, "path_recall": 0.25, "path_ans_f1": 0.3333333333333333, "path_ans_precision": 0.5, "path_ans_recall": 0.25}
{"id": "WebQTest-282", "prediction": ["# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Mongolia\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Belarus\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Belgium\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.11b60vv5th\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> Mongolia\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nRussia -> location.statistical_region.co2_emissions_per_capita -> g.1245_8036\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita"], "ground_truth": ["Belarus", "Georgia", "Latvia", "Azerbaijan", "Norway", "Lithuania", "Estonia", "Ukraine", "Finland", "Poland", "Mongolia", "China", "North Korea", "Kazakhstan"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.025531914893617023, "path_precision": 0.3, "path_recall": 0.013333333333333334, "path_ans_f1": 0.19354838709677416, "path_ans_precision": 0.3, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-284", "prediction": ["# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> language.language_family.languages -> Aequian language\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> common.topic.image -> Iron Age Italy\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> common.topic.notable_for -> g.125d12rwn\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> base.rosetta.languoid.parent -> Latino-Faliscan Group -> base.rosetta.languoid.parent -> Italic Group\n# Answer:\nLatino-Faliscan Group", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> language.language_family.languages -> Angevin dialect\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> language.language_family.languages -> Augeron\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Indo-European languages -> language.language_family.sub_families -> Italic languages\n# Answer:\nIndo-European languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Indo-European languages -> common.topic.notable_types -> Language Family\n# Answer:\nIndo-European languages", "# Reasoning Path:\nLatin Language -> base.rosetta.languoid.parent -> Latino-Faliscan Group -> base.rosetta.languoid.languoid_class -> Group\n# Answer:\nLatino-Faliscan Group", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Indo-European languages -> education.field_of_study.students_majoring -> m.05yz7m9\n# Answer:\nIndo-European languages"], "ground_truth": ["Italic languages", "Indo-European languages"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-286", "prediction": ["# Reasoning Path:\nKarl Benz -> people.person.education -> m.04hddz6 -> education.education.institution -> Karlsruhe Institute of Technology\n# Answer:\nKarlsruhe Institute of Technology", "# Reasoning Path:\nKarl Benz -> people.person.education -> m.0n0nxj5 -> education.education.institution -> Karlsruhe Institute of Technology\n# Answer:\nKarlsruhe Institute of Technology", "# Reasoning Path:\nKarl Benz -> people.person.education -> m.04hddz6 -> education.education.degree -> Bachelor of Engineering\n# Answer:\nBachelor of Engineering", "# Reasoning Path:\nKarl Benz -> people.person.education -> m.04hddz6 -> education.education.major_field_of_study -> Mechanical Engineering\n# Answer:\nMechanical Engineering", "# Reasoning Path:\nKarl Benz -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#range -> Person\n# Answer:\nChildren", "# Reasoning Path:\nKarl Benz -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nKarl Benz -> people.person.children -> Thilde Benz -> people.person.gender -> Female\n# Answer:\nThilde Benz", "# Reasoning Path:\nKarl Benz -> people.person.children -> Ellen Benz -> common.topic.notable_for -> g.12h3244gv\n# Answer:\nEllen Benz", "# Reasoning Path:\nKarl Benz -> freebase.valuenotation.is_reviewed -> Children -> type.property.schema -> Person\n# Answer:\nChildren", "# Reasoning Path:\nKarl Benz -> people.person.children -> Clara Benz -> common.topic.notable_types -> Deceased Person\n# Answer:\nClara Benz"], "ground_truth": ["Karlsruhe Institute of Technology"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-287", "prediction": ["# Reasoning Path:\nCarmen Electra -> film.actor.film -> m.0115j6sp -> film.performance.film -> Dragula\n# Answer:\nDragula", "# Reasoning Path:\nCarmen Electra -> film.actor.film -> m.012zh0pk -> film.performance.film -> Chocolate City\n# Answer:\nChocolate City", "# Reasoning Path:\nCarmen Electra -> film.actor.film -> m.02tb13p -> film.performance.film -> Full of It\n# Answer:\nFull of It", "# Reasoning Path:\nCarmen Electra -> film.actor.film -> m.0115j6sp -> film.performance.character -> Jaime\n# Answer:\nJaime", "# Reasoning Path:\nCarmen Electra -> film.actor.film -> m.02tb13p -> film.performance.special_performance_type -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nCarmen Electra -> tv.tv_actor.guest_roles -> m.03lkq75 -> tv.tv_guest_role.episodes_appeared_in -> Mr. Monk and the Panic Room\n# Answer:\nMr. Monk and the Panic Room", "# Reasoning Path:\nCarmen Electra -> base.popstra.celebrity.infidelity_victim -> m.06465lm -> base.popstra.infidelity.victim -> Dave Navarro\n# Answer:\nDave Navarro", "# Reasoning Path:\nCarmen Electra -> tv.tv_actor.guest_roles -> m.040q72p -> tv.tv_guest_role.episodes_appeared_in -> Threat Levels\n# Answer:\nThreat Levels", "# Reasoning Path:\nCarmen Electra -> tv.tv_actor.guest_roles -> m.040q68x -> tv.tv_guest_role.episodes_appeared_in -> Pilot\n# Answer:\nPilot"], "ground_truth": ["The Mating Habits of the Earthbound Human", "Meet the Spartans", "Lil' Pimp", "We Are Family", "Rent Control", "The Axe Boat", "Mardi Gras: Spring Break", "Starsky & Hutch", "Disaster Movie", "The Chosen One: Legend of the Raven", "Get Over It", "Getting Played", "Max Havoc: Curse of the Dragon", "Hot Tamale", "Lap Dance", "The Back Nine", "Uptown Girls", "BThere Dvd Magazine", "Sol Goode", "Scary Movie 4", "Perfume", "Baywatch: Hawaiian Wedding", "American Vampire", "Date Movie", "Chocolate City", "Barry Munday", "My Boss's Daughter", "Monster Island", "Scary Movie", "Epic Movie", "Cheaper by the Dozen 2", "American Dreamz", "Lolo's Cafe", "Book of Fire", "Bedtime Stories", "Mr. 3000", "National Lampoon's Pledge This!", "30 Days Until I'm Famous", "Dragula", "2-Headed Shark Attack", "Carmen Electra's Fit to Strip", "Christmas in Wonderland", "Whacked!", "Searching for Bobby D", "Oy Vey! My Son Is Gay!!", "I Want Candy", "Good Burger", "Starstruck", "Naked Movie", "Dirty Love", "Full of It"], "ans_acc": 0.058823529411764705, "ans_hit": 1, "ans_f1": 0.1, "ans_precission": 0.3333333333333333, "ans_recall": 0.058823529411764705, "path_f1": 0.07894736842105263, "path_precision": 0.3333333333333333, "path_recall": 0.04477611940298507, "path_ans_f1": 0.1, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.058823529411764705}
{"id": "WebQTest-289", "prediction": ["# Reasoning Path:\nWorld War II -> time.event.locations -> Asia -> base.locations.continents.countries_within -> Philippines\n# Answer:\nAsia", "# Reasoning Path:\nWorld War II -> time.event.locations -> Asia -> base.locations.continents.countries_within -> Brunei\n# Answer:\nAsia", "# Reasoning Path:\nWorld War II -> time.event.locations -> Asia -> location.location.partially_contains -> Soviet Union\n# Answer:\nAsia", "# Reasoning Path:\nWorld War II -> time.event.locations -> Africa -> base.locations.continents.countries_within -> South Africa\n# Answer:\nAfrica", "# Reasoning Path:\nWorld War II -> time.event.locations -> Asia -> base.locations.continents.countries_within -> Cambodia\n# Answer:\nAsia", "# Reasoning Path:\nWorld War II -> time.event.locations -> Asia -> location.location.events -> World War I\n# Answer:\nAsia", "# Reasoning Path:\nWorld War II -> time.event.locations -> Asia -> location.location.events -> 2009 flu pandemic\n# Answer:\nAsia", "# Reasoning Path:\nWorld War II -> time.event.locations -> Asia -> location.location.partially_contains -> Armenia\n# Answer:\nAsia", "# Reasoning Path:\nWorld War II -> time.event.locations -> Africa -> base.locations.continents.countries_within -> Algeria\n# Answer:\nAfrica", "# Reasoning Path:\nWorld War II -> time.event.locations -> Africa -> location.location.events -> World War I\n# Answer:\nAfrica"], "ground_truth": ["1945-09-02"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-291", "prediction": ["# Reasoning Path:\nMorgan Freeman -> people.person.education -> m.0n10623 -> education.education.institution -> Los Angeles City College\n# Answer:\nLos Angeles City College", "# Reasoning Path:\nMorgan Freeman -> people.person.education -> m.010665q2 -> education.education.institution -> Pasadena Playhouse\n# Answer:\nPasadena Playhouse", "# Reasoning Path:\nMorgan Freeman -> common.topic.webpage -> m.093_l_6 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nMorgan Freeman -> common.topic.webpage -> m.093zz6z -> common.webpage.resource -> Robert Iler pleads guilty to mugging charge\n# Answer:\nRobert Iler pleads guilty to mugging charge", "# Reasoning Path:\nMorgan Freeman -> tv.tv_producer.tv_episodes_produced -> m.0gfnpbk -> tv.tv_producer_episode_credit.episode -> The 16th Man\n# Answer:\nThe 16th Man", "# Reasoning Path:\nMorgan Freeman -> tv.tv_producer.tv_episodes_produced -> m.0j75_zn -> tv.tv_producer_episode_credit.episode -> Is There a Sixth Sense?\n# Answer:\nIs There a Sixth Sense?", "# Reasoning Path:\nMorgan Freeman -> common.topic.webpage -> m.093z_bp -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nMorgan Freeman -> tv.tv_producer.tv_episodes_produced -> m.0gfnpbk -> tv.tv_producer_episode_credit.role -> g.120lqbyx\n# Answer:\ntv.tv_producer_episode_credit.role", "# Reasoning Path:\nMorgan Freeman -> common.topic.webpage -> m.093z_bp -> common.webpage.resource -> MORGAN FREEMAN\n# Answer:\nMORGAN FREEMAN", "# Reasoning Path:\nMorgan Freeman -> common.topic.webpage -> m.093zz6z -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Los Angeles City College", "Pasadena Playhouse"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-294", "prediction": ["# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Biologist -> people.profession.specialization_of -> Scientist\n# Answer:\nBiologist", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Biologist -> fictional_universe.character_occupation.characters_with_this_occupation -> Professor Archimedes Q. Porter\n# Answer:\nBiologist", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Botanist -> people.profession.specialization_of -> Scientist\n# Answer:\nBotanist", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Biologist -> fictional_universe.character_occupation.characters_with_this_occupation -> Kenneth J. Sullivan\n# Answer:\nBiologist", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Biologist -> people.profession.specializations -> Clinical Molecular Biologist\n# Answer:\nBiologist", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Botanist -> common.topic.notable_for -> g.12568993_\n# Answer:\nBotanist", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Botanist -> base.descriptive_names.names.descriptive_name -> m.0102h0md\n# Answer:\nBotanist", "# Reasoning Path:\nCarl Linnaeus -> base.kwebbase.kwtopic.connections_to -> asa gray replaced system of carolus linnaeus -> base.kwebbase.kwconnection.subject -> Asa Gray\n# Answer:\nasa gray replaced system of carolus linnaeus", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Biologist -> people.profession.specializations -> Computational Biologist\n# Answer:\nBiologist", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Physician -> freebase.type_profile.published -> Published\n# Answer:\nPhysician"], "ground_truth": ["Botanist", "Biologist", "Scientist", "Physician"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.8181818181818182, "path_precision": 0.9, "path_recall": 0.75, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-295", "prediction": ["# Reasoning Path:\nGermany -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Austria\n# Answer:\nFederal republic", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Czech Republic\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_38m_\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fdl8\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Austria\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc38c_4\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_67jr\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Poland\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_jlz4\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp"], "ground_truth": ["Federal republic", "Parliamentary republic", "Representative democracy", "Constitutional republic"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.4, "ans_recall": 0.5, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.4, "path_ans_recall": 0.5}
{"id": "WebQTest-296", "prediction": ["# Reasoning Path:\nLibya -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Egypt\n# Answer:\nProvisional government", "# Reasoning Path:\nLibya -> location.country.form_of_government -> Provisional government -> common.topic.article -> m.02tz44\n# Answer:\nProvisional government", "# Reasoning Path:\nLibya -> location.country.form_of_government -> Provisional government -> common.topic.notable_types -> Form of Government\n# Answer:\nProvisional government", "# Reasoning Path:\nLibya -> location.statistical_region.co2_emissions_per_capita -> g.1245_1hym\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nLibya -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> State of Palestine\n# Answer:\nProvisional government", "# Reasoning Path:\nLibya -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Italy\n# Answer:\nParliamentary republic", "# Reasoning Path:\nLibya -> location.country.form_of_government -> Parliamentary republic -> common.topic.notable_types -> Form of Government\n# Answer:\nParliamentary republic", "# Reasoning Path:\nLibya -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Republic of Kosovo\n# Answer:\nParliamentary republic", "# Reasoning Path:\nLibya -> location.statistical_region.co2_emissions_per_capita -> g.1245_2gxq\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nLibya -> location.statistical_region.co2_emissions_per_capita -> g.1245_55yg\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita"], "ground_truth": ["Parliamentary republic", "Provisional government"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-297", "prediction": ["# Reasoning Path:\nAnaheim -> location.location.time_zones -> Pacific Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nPacific Time Zone", "# Reasoning Path:\nAnaheim -> location.location.time_zones -> Pacific Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nPacific Time Zone", "# Reasoning Path:\nAnaheim -> location.location.time_zones -> Pacific Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nPacific Time Zone", "# Reasoning Path:\nAnaheim -> location.statistical_region.population -> g.11b66d_m1v\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nAnaheim -> location.statistical_region.population -> g.11bc88q060\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nAnaheim -> location.statistical_region.population -> g.11bcdlmd72\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nAnaheim -> location.citytown.postal_codes -> 92899 -> common.topic.notable_types -> Postal Code\n# Answer:\n92899", "# Reasoning Path:\nAnaheim -> location.citytown.postal_codes -> 92899 -> location.postal_code.country -> United States of America\n# Answer:\n92899", "# Reasoning Path:\nAnaheim -> location.citytown.postal_codes -> 92801 -> location.location.geolocation -> m.03dnfqk\n# Answer:\n92801", "# Reasoning Path:\nAnaheim -> location.citytown.postal_codes -> 92899 -> common.topic.notable_for -> g.125h43n1d\n# Answer:\n92899"], "ground_truth": ["Pacific Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-298", "prediction": ["# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1970 World Series -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1970 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1970 World Series -> sports.sports_championship_event.runner_up -> Cincinnati Reds\n# Answer:\n1970 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1970 World Series -> sports.sports_championship_event.season -> 1970 Major League Baseball Season\n# Answer:\n1970 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1983 World Series -> sports.sports_championship_event.championship -> World Series\n# Answer:\n1983 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1966 World Series -> sports.sports_championship_event.season -> 1966 Major League Baseball Season\n# Answer:\n1966 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1966 World Series -> common.topic.notable_for -> g.125br8lv_\n# Answer:\n1966 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1983 World Series -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1983 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.venue -> m.0wz1z2b -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nBaltimore Orioles -> baseball.baseball_team.team_stats -> m.05n60gg -> baseball.baseball_team_stats.season -> 1954 Major League Baseball season\n# Answer:\n1954 Major League Baseball season", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.venue -> m.0wz1z2b -> sports.team_venue_relationship.venue -> Oriole Park at Camden Yards\n# Answer:\nOriole Park at Camden Yards"], "ground_truth": ["1983 World Series", "1966 World Series", "1970 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ken Barlow\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Albert Tatlock\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> people.person.gender -> Male\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Alf Roberts\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> film.actor.film -> m.0h0_mvx\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Drama -> common.topic.subject_of -> Albrecht Behmel\n# Answer:\nDrama", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Comedy -> common.topic.subject_of -> The N Crowd\n# Answer:\nComedy", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Drama -> common.topic.article -> m.02827\n# Answer:\nDrama", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Thursday 10th June 2010 -> common.topic.notable_types -> Multipart TV episode\n# Answer:\nThursday 10th June 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Drama -> common.topic.subject_of -> Vivacity\n# Answer:\nDrama"], "ground_truth": ["William Roache"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-300", "prediction": ["# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> common.topic.subject_of -> Book\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Statesman -> common.topic.article -> m.016fc9\n# Answer:\nStatesman", "# Reasoning Path:\nFrancis Bacon -> book.author.works_written -> Bacon\n# Answer:\nBacon", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> tv.tv_subject.tv_programs -> The Secret Life of Scientists & Engineers\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> common.topic.subject_of -> Autobiography\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> base.descriptive_names.names.descriptive_name -> m.01053kvm\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> common.topic.subject_of -> Biography\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Statesman -> fictional_universe.character_occupation.characters_with_this_occupation -> Thomas Jefferson\n# Answer:\nStatesman", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> base.descriptive_names.names.descriptive_name -> m.01053kyr\n# Answer:\nScientist"], "ground_truth": ["Statesman", "Spy", "Philosopher", "Author", "Scientist"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7200000000000001, "ans_precission": 0.9, "ans_recall": 0.6, "path_f1": 0.8181818181818182, "path_precision": 0.9, "path_recall": 0.75, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 1.0, "path_ans_recall": 0.6}
{"id": "WebQTest-301", "prediction": ["# Reasoning Path:\nAmelia Earhart -> fictional_universe.fictional_character.occupation -> Pilot -> people.profession.specializations -> Aerobatic Pilot\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> fictional_universe.fictional_character.occupation -> Pilot -> people.profession.specializations -> Airman\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> fictional_universe.fictional_character.occupation -> Pilot -> base.firsts.achievement.firsts -> m.0g8qg3r\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> fictional_universe.fictional_character.occupation -> Pilot -> base.descriptive_names.names.descriptive_name -> m.0100tslg\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> fictional_universe.fictional_character.occupation -> Pilot -> people.profession.specializations -> Aircrew\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> common.topic.notable_types -> Film character -> freebase.type_profile.published -> Published\n# Answer:\nFilm character", "# Reasoning Path:\nAmelia Earhart -> fictional_universe.fictional_character.occupation -> Pilot -> base.descriptive_names.names.descriptive_name -> m.0100tthh\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> people.person.profession -> Pilot -> base.descriptive_names.names.descriptive_name -> m.0100tslg\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> people.person.profession -> Writer -> people.profession.specializations -> Author\n# Answer:\nWriter", "# Reasoning Path:\nAmelia Earhart -> people.person.profession -> Pilot -> fictional_universe.character_occupation.characters_with_this_occupation -> Wrongway Feldman\n# Answer:\nPilot"], "ground_truth": ["Pilot", "Writer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-302", "prediction": ["# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Royal blue -> common.topic.notable_types -> Color\n# Answer:\nRoyal blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Royal blue -> common.topic.notable_for -> g.1255wtr8_\n# Answer:\nRoyal blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Silver -> base.schemastaging.context_name.pronunciation -> g.125_rvx57\n# Answer:\nSilver", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Royal blue -> common.topic.article -> m.06kqt8\n# Answer:\nRoyal blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Navy Blue -> base.schemastaging.visual_color_extra.srgb -> m.010q_4pn\n# Answer:\nNavy Blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Silver -> common.topic.notable_types -> Color\n# Answer:\nSilver", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Silver -> common.topic.notable_for -> g.125g6j40s\n# Answer:\nSilver", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Navy Blue -> base.schemastaging.visual_color_extra.cmyk -> m.010q_4qh\n# Answer:\nNavy Blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Navy Blue -> common.topic.notable_types -> Color\n# Answer:\nNavy Blue", "# Reasoning Path:\nDallas Cowboys -> american_football.football_team.away_games -> Dallas Cowboys at Arizona Cardinals, 2008-10-12 -> american_football.football_game.passing -> m.079397s\n# Answer:\nDallas Cowboys at Arizona Cardinals, 2008-10-12"], "ground_truth": ["White", "Silver", "Royal blue", "Blue", "Navy Blue"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.8470588235294118, "ans_precission": 0.9, "ans_recall": 0.8, "path_f1": 0.7200000000000001, "path_precision": 0.9, "path_recall": 0.6, "path_ans_f1": 0.8470588235294118, "path_ans_precision": 0.9, "path_ans_recall": 0.8}
{"id": "WebQTest-303", "prediction": ["# Reasoning Path:\nMargaret Hoover -> people.person.education -> m.0n1gxgt -> education.education.institution -> Davidson College\n# Answer:\nDavidson College", "# Reasoning Path:\nMargaret Hoover -> people.person.education -> g.11c3kps_b6\n# Answer:\npeople.person.education", "# Reasoning Path:\nMargaret Hoover -> common.topic.notable_for -> g.125dcyfk6\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMargaret Hoover -> people.person.profession -> Strategist -> people.profession.people_with_this_profession -> Adam Beashel\n# Answer:\nStrategist", "# Reasoning Path:\nMargaret Hoover -> people.person.profession -> Strategist -> common.topic.notable_for -> g.1254zz38g\n# Answer:\nStrategist", "# Reasoning Path:\nMargaret Hoover -> people.person.profession -> Strategist -> base.descriptive_names.names.descriptive_name -> m.0_z7q_s\n# Answer:\nStrategist", "# Reasoning Path:\nMargaret Hoover -> people.person.profession -> Strategist -> people.profession.people_with_this_profession -> Ann Lewis\n# Answer:\nStrategist", "# Reasoning Path:\nMargaret Hoover -> people.person.profession -> Strategist -> people.profession.people_with_this_profession -> Arun Jaitley\n# Answer:\nStrategist"], "ground_truth": ["Bryn Mawr College", "Davidson College"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.2, "ans_precission": 0.125, "ans_recall": 0.5, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2, "path_ans_precision": 0.125, "path_ans_recall": 0.5}
{"id": "WebQTest-304", "prediction": ["# Reasoning Path:\nJamie Oliver -> people.person.children -> Petal Blossom Rainbow Oliver -> people.person.parents -> Juliette Norton\n# Answer:\nPetal Blossom Rainbow Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.children -> Petal Blossom Rainbow Oliver -> people.person.nationality -> United Kingdom\n# Answer:\nPetal Blossom Rainbow Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.children -> Poppy Honey Rosie Oliver -> common.topic.notable_types -> Person\n# Answer:\nPoppy Honey Rosie Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.children -> Petal Blossom Rainbow Oliver -> people.person.sibling_s -> m.0tlq4hm\n# Answer:\nPetal Blossom Rainbow Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.children -> Daisy Boo Pamela Oliver -> common.topic.notable_types -> Person\n# Answer:\nDaisy Boo Pamela Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.children -> Poppy Honey Rosie Oliver -> common.topic.notable_for -> g.1255bqwws\n# Answer:\nPoppy Honey Rosie Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.children -> Poppy Honey Rosie Oliver -> people.person.sibling_s -> m.0tlq4hm\n# Answer:\nPoppy Honey Rosie Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.children -> Daisy Boo Pamela Oliver -> people.person.parents -> Juliette Norton\n# Answer:\nDaisy Boo Pamela Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.spouse_s -> m.04t57cj -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nJamie Oliver -> tv.tv_producer.programs_produced -> m.0ng0wq_ -> tv.tv_producer_term.producer_type -> Executive Producer\n# Answer:\nExecutive Producer"], "ground_truth": ["Poppy Honey Rosie Oliver", "Daisy Boo Pamela Oliver", "Petal Blossom Rainbow Oliver", "Buddy Bear Maurice Oliver"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.8, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.8, "path_ans_recall": 0.75}
{"id": "WebQTest-305", "prediction": ["# Reasoning Path:\nBradley Walsh -> people.person.places_lived -> m.0h8jzzr -> people.place_lived.location -> Toronto\n# Answer:\nToronto", "# Reasoning Path:\nBradley Walsh -> people.person.places_lived -> m.0h8jzzl -> people.place_lived.location -> Los Angeles\n# Answer:\nLos Angeles", "# Reasoning Path:\nBradley Walsh -> tv.tv_actor.guest_roles -> m.09p0wtd -> tv.tv_guest_role.episodes_appeared_in -> Juliette Lewis, Colin McAllister, Justin Ryan, Oasis\n# Answer:\nJuliette Lewis, Colin McAllister, Justin Ryan, Oasis", "# Reasoning Path:\nBradley Walsh -> tv.tv_actor.starring_roles -> m.03jprnv -> tv.regular_tv_appearance.series -> Wheel of Fortune\n# Answer:\nWheel of Fortune", "# Reasoning Path:\nBradley Walsh -> tv.tv_actor.starring_roles -> m.03js8bs -> tv.regular_tv_appearance.series -> Night and Day\n# Answer:\nNight and Day", "# Reasoning Path:\nBradley Walsh -> tv.tv_actor.guest_roles -> m.09p0wtk -> tv.tv_guest_role.episodes_appeared_in -> Series 4, Show 6\n# Answer:\nSeries 4, Show 6", "# Reasoning Path:\nBradley Walsh -> tv.tv_actor.starring_roles -> m.04dcy12 -> tv.regular_tv_appearance.series -> Torn\n# Answer:\nTorn", "# Reasoning Path:\nBradley Walsh -> tv.tv_actor.guest_roles -> m.09p0wtq -> tv.tv_guest_role.episodes_appeared_in -> ...And A Fist Full of Jack and Jills\n# Answer:\n...And A Fist Full of Jack and Jills"], "ground_truth": ["Toronto", "Los Angeles"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-306", "prediction": ["# Reasoning Path:\nInter Milan -> base.schemastaging.sports_team_extra.managers -> m.0w4j_k_ -> base.schemastaging.sports_team_manager_tenure.manager -> Walter Mazzarri\n# Answer:\nWalter Mazzarri", "# Reasoning Path:\nInter Milan -> base.schemastaging.sports_team_extra.managers -> m.0w4j_k_ -> base.schemastaging.sports_team_manager_tenure.position -> Manager\n# Answer:\nManager", "# Reasoning Path:\nInter Milan -> soccer.football_team.manager -> m.0w2lsb3 -> soccer.football_team_management_tenure.manager -> Walter Mazzarri\n# Answer:\nWalter Mazzarri", "# Reasoning Path:\nInter Milan -> base.schemastaging.sports_team_extra.managers -> m.0w4j_k_ -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nInter Milan -> sports.sports_team.coaches -> m.0w68_yf -> sports.sports_team_coach_tenure.coach -> Walter Mazzarri\n# Answer:\nWalter Mazzarri", "# Reasoning Path:\nInter Milan -> soccer.football_team.manager -> m.0gf984m -> soccer.football_team_management_tenure.manager -> Leonardo Ara\u00fajo\n# Answer:\nLeonardo Ara\u00fajo", "# Reasoning Path:\nInter Milan -> sports.sports_team.coaches -> m.0w68_yf -> sports.sports_team_coach_tenure.position -> Head coach\n# Answer:\nHead coach", "# Reasoning Path:\nInter Milan -> soccer.football_team.manager -> m.0bhj5cb -> soccer.football_team_management_tenure.manager -> Jos\u00e9 Mourinho\n# Answer:\nJos\u00e9 Mourinho", "# Reasoning Path:\nInter Milan -> sports.sports_team.coaches -> m.0nfmm1y -> sports.sports_team_coach_tenure.coach -> Giuseppe Baresi\n# Answer:\nGiuseppe Baresi", "# Reasoning Path:\nInter Milan -> soccer.football_team.manager -> m.0w2lsb3 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["Walter Mazzarri"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-308", "prediction": ["# Reasoning Path:\nReese Witherspoon -> celebrities.celebrity.sexual_relationships -> m.02_90w0 -> celebrities.romantic_relationship.celebrity -> Jake Gyllenhaal\n# Answer:\nJake Gyllenhaal", "# Reasoning Path:\nReese Witherspoon -> celebrities.celebrity.sexual_relationships -> m.02_90w0 -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nDated", "# Reasoning Path:\nReese Witherspoon -> base.popstra.celebrity.dated -> m.065ps9j -> base.popstra.dated.participant -> Jake Gyllenhaal\n# Answer:\nJake Gyllenhaal", "# Reasoning Path:\nReese Witherspoon -> base.popstra.celebrity.dated -> m.063h647 -> base.popstra.dated.participant -> Jake Gyllenhaal\n# Answer:\nJake Gyllenhaal", "# Reasoning Path:\nReese Witherspoon -> people.person.spouse_s -> m.0gxm_yb -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nReese Witherspoon -> base.popstra.celebrity.dated -> m.065pts5 -> base.popstra.dated.participant -> Jeremy Sisto\n# Answer:\nJeremy Sisto", "# Reasoning Path:\nReese Witherspoon -> people.person.spouse_s -> m.0gxm_yb -> people.marriage.spouse -> Jim Toth\n# Answer:\nJim Toth", "# Reasoning Path:\nReese Witherspoon -> people.person.spouse_s -> m.0gxm_yb -> people.marriage.location_of_ceremony -> Ojai\n# Answer:\nOjai", "# Reasoning Path:\nReese Witherspoon -> people.person.spouse_s -> m.0112bp8v -> people.marriage.spouse -> Jake Gyllenhaal\n# Answer:\nJake Gyllenhaal", "# Reasoning Path:\nReese Witherspoon -> people.person.spouse_s -> m.0112bp8v -> freebase.valuenotation.has_no_value -> Location of ceremony\n# Answer:\nLocation of ceremony"], "ground_truth": ["Jim Toth"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> base.biblioness.bibs_location.country -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.administrative_division.second_level_division_of -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> Scotland\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#domain -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Gender -> type.property.schema -> Person\n# Answer:\nGender"], "ground_truth": ["2005"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-310", "prediction": ["# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.image -> 10colonesbcrfront\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.webpage -> m.04m2m2k\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60wmkvh\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.11b71w3_6h\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_5xgy\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_6n42\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.1hhc37pjz\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.1hhc37x5q\n# Answer:\nlocation.statistical_region.external_debt_stock"], "ground_truth": ["Costa Rican col\u00f3n"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-311", "prediction": ["# Reasoning Path:\nUltimate Fighting Championship -> organization.organization.headquarters -> m.03gbyq5 -> location.mailing_address.citytown -> Las Vegas\n# Answer:\nLas Vegas", "# Reasoning Path:\nUltimate Fighting Championship -> organization.organization.headquarters -> m.03gbyq5 -> location.mailing_address.state_province_region -> Nevada\n# Answer:\nNevada", "# Reasoning Path:\nUltimate Fighting Championship -> business.customer.supplier -> 4INFO -> organization.organization.headquarters -> m.03dk0hf\n# Answer:\n4INFO", "# Reasoning Path:\nUltimate Fighting Championship -> organization.organization.headquarters -> m.03gbyq5 -> common.topic.notable_types -> Location\n# Answer:\nLocation", "# Reasoning Path:\nUltimate Fighting Championship -> business.sponsor.sponsorship -> m.0_rf5gv -> business.sponsorship.sponsored_recipient -> UFC 170\n# Answer:\nUFC 170", "# Reasoning Path:\nUltimate Fighting Championship -> business.sponsor.sponsorship -> m.0jwc64x -> business.sponsorship.sponsored_recipient -> UFC 146\n# Answer:\nUFC 146", "# Reasoning Path:\nUltimate Fighting Championship -> business.sponsor.sponsorship -> m.0khmvp3 -> business.sponsorship.sponsored_recipient -> UFC on Fox: Shogun vs. Vera\n# Answer:\nUFC on Fox: Shogun vs. Vera", "# Reasoning Path:\nUltimate Fighting Championship -> business.customer.supplier -> 4INFO -> business.employer.employees -> m.010pmfbd\n# Answer:\n4INFO", "# Reasoning Path:\nUltimate Fighting Championship -> business.customer.supplier -> 4INFO -> computer.software_developer.software -> AdHaven\n# Answer:\n4INFO", "# Reasoning Path:\nUltimate Fighting Championship -> business.customer.supplier -> 4INFO -> business.employer.employees -> m.010qbccp\n# Answer:\n4INFO"], "ground_truth": ["Las Vegas"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-312", "prediction": ["# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0hhzbby -> film.performance.actor -> Chris Pine\n# Answer:\nChris Pine", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8ffj -> film.performance.actor -> William Shatner\n# Answer:\nWilliam Shatner", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8ffv -> film.performance.actor -> William Shatner\n# Answer:\nWilliam Shatner", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0hhzbby -> film.performance.film -> Star Trek Into Darkness\n# Answer:\nStar Trek Into Darkness", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8ffj -> film.performance.film -> Star Trek: The Motion Picture\n# Answer:\nStar Trek: The Motion Picture", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8ffv -> film.performance.film -> Star Trek III: The Search for Spock\n# Answer:\nStar Trek III: The Search for Spock", "# Reasoning Path:\nJames T. Kirk -> common.topic.notable_types -> Film character -> freebase.type_profile.kind -> Annotation\n# Answer:\nFilm character", "# Reasoning Path:\nJames T. Kirk -> common.topic.notable_types -> Film character -> type.type.expected_by -> Character\n# Answer:\nFilm character", "# Reasoning Path:\nJames T. Kirk -> common.topic.webpage -> m.09wcz0z -> common.webpage.resource -> Wolverines! Chris Hemsworth nabs lead in 'Red Dawn'\n# Answer:\nWolverines! Chris Hemsworth nabs lead in 'Red Dawn'", "# Reasoning Path:\nJames T. Kirk -> common.topic.webpage -> m.09wcz0z -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Vic Mignogna", "Jim Carrey", "William Shatner"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.2, "ans_recall": 0.3333333333333333, "path_f1": 0.17391304347826086, "path_precision": 0.2, "path_recall": 0.15384615384615385, "path_ans_f1": 0.25, "path_ans_precision": 0.2, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-313", "prediction": ["# Reasoning Path:\nAlbert Pujols -> base.schemastaging.athlete_extra.salary -> m.0wdd82v -> base.schemastaging.athlete_salary.team -> Los Angeles Angels of Anaheim\n# Answer:\nLos Angeles Angels of Anaheim", "# Reasoning Path:\nAlbert Pujols -> base.schemastaging.athlete_extra.salary -> m.0j2qh8m -> base.schemastaging.athlete_salary.team -> Los Angeles Angels of Anaheim\n# Answer:\nLos Angeles Angels of Anaheim", "# Reasoning Path:\nAlbert Pujols -> base.schemastaging.athlete_extra.salary -> m.0wdd82v -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nAlbert Pujols -> base.schemastaging.athlete_extra.salary -> m.0j2qh8m -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nAlbert Pujols -> sports.pro_athlete.teams -> m.0j2lc1l -> sports.sports_team_roster.team -> St. Louis Cardinals\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\nAlbert Pujols -> sports.pro_athlete.teams -> m.0j2lc1l -> sports.sports_team_roster.position -> Infielder\n# Answer:\nInfielder", "# Reasoning Path:\nAlbert Pujols -> baseball.baseball_player.batting_stats -> m.06s6m0p -> baseball.batting_statistics.season -> 2001 Major League Baseball Season\n# Answer:\n2001 Major League Baseball Season", "# Reasoning Path:\nAlbert Pujols -> sports.pro_athlete.teams -> m.04xhdn1 -> sports.sports_team_roster.team -> Los Angeles Angels of Anaheim\n# Answer:\nLos Angeles Angels of Anaheim", "# Reasoning Path:\nAlbert Pujols -> sports.pro_athlete.teams -> m.05kb0jt -> sports.sports_team_roster.team -> Scottsdale Scorpions\n# Answer:\nScottsdale Scorpions", "# Reasoning Path:\nAlbert Pujols -> sports.pro_athlete.teams -> m.04xhdn1 -> sports.sports_team_roster.position -> Infielder\n# Answer:\nInfielder"], "ground_truth": ["Los Angeles Angels of Anaheim", "Scottsdale Scorpions"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-314", "prediction": ["# Reasoning Path:\nSweden -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nSweden -> location.country.form_of_government -> Unitary state -> common.topic.notable_types -> Form of Government\n# Answer:\nUnitary state", "# Reasoning Path:\nSweden -> location.country.form_of_government -> Constitutional monarchy -> common.topic.notable_types -> Form of Government\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Denmark\n# Answer:\nUnitary state", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nSweden -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nSweden -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> United Kingdom\n# Answer:\nParliamentary system", "# Reasoning Path:\nSweden -> location.country.form_of_government -> Parliamentary system -> common.topic.notable_types -> Form of Government\n# Answer:\nParliamentary system", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ll\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars"], "ground_truth": ["Hereditary monarchy", "Unitary state", "Representative democracy", "Constitutional monarchy", "Parliamentary system"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6461538461538462, "ans_precission": 0.7, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.6461538461538462, "path_ans_precision": 0.7, "path_ans_recall": 0.6}
{"id": "WebQTest-315", "prediction": ["# Reasoning Path:\nGeorge Jones -> music.artist.origin -> Saratoga -> location.location.containedby -> Texas\n# Answer:\nSaratoga", "# Reasoning Path:\nGeorge Jones -> music.artist.origin -> Saratoga -> common.topic.notable_for -> g.1255l1rpv\n# Answer:\nSaratoga", "# Reasoning Path:\nGeorge Jones -> music.artist.origin -> Saratoga -> location.location.geolocation -> m.0kh6hg\n# Answer:\nSaratoga", "# Reasoning Path:\nGeorge Jones -> music.artist.origin -> Saratoga -> location.location.containedby -> Hardin County\n# Answer:\nSaratoga", "# Reasoning Path:\nGeorge Jones -> music.artist.origin -> Vidor -> location.location.containedby -> Texas\n# Answer:\nVidor", "# Reasoning Path:\nGeorge Jones -> award.award_nominee.award_nominations -> m.0__1zjz -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work", "# Reasoning Path:\nGeorge Jones -> music.artist.origin -> Vidor -> common.topic.notable_types -> City/Town/Village\n# Answer:\nVidor", "# Reasoning Path:\nGeorge Jones -> music.artist.genre -> Country -> broadcast.genre.content -> 1.FM Classic Country\n# Answer:\nCountry", "# Reasoning Path:\nGeorge Jones -> music.artist.origin -> Vidor -> location.location.containedby -> Orange County\n# Answer:\nVidor", "# Reasoning Path:\nGeorge Jones -> award.award_nominee.award_nominations -> m.0__1zjz -> award.award_nomination.award -> Academy of Country Music Award for  Vocal Group of the Year\n# Answer:\nAcademy of Country Music Award for  Vocal Group of the Year"], "ground_truth": ["Saratoga"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-316", "prediction": ["# Reasoning Path:\nWilliam Shakespeare -> people.person.profession -> Lyricist -> people.profession.specialization_of -> Writer\n# Answer:\nLyricist", "# Reasoning Path:\nWilliam Shakespeare -> people.person.profession -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor", "# Reasoning Path:\nWilliam Shakespeare -> people.person.profession -> Poet -> people.profession.specialization_of -> Writer\n# Answer:\nPoet", "# Reasoning Path:\nWilliam Shakespeare -> fictional_universe.fictional_character.occupation -> Poet -> people.profession.specialization_of -> Writer\n# Answer:\nPoet", "# Reasoning Path:\nWilliam Shakespeare -> fictional_universe.fictional_character.occupation -> Playwright -> people.profession.specialization_of -> Writer\n# Answer:\nPlaywright", "# Reasoning Path:\nWilliam Shakespeare -> people.person.profession -> Author -> type.type.expected_by -> Playwright\n# Answer:\nAuthor", "# Reasoning Path:\nWilliam Shakespeare -> fictional_universe.fictional_character_creator.fictional_characters_created -> Amleto -> fictional_universe.fictional_character.gender -> Male\n# Answer:\nAmleto", "# Reasoning Path:\nWilliam Shakespeare -> fictional_universe.fictional_character.occupation -> Playwright -> rdf-schema#range -> Author\n# Answer:\nPlaywright", "# Reasoning Path:\nWilliam Shakespeare -> fictional_universe.fictional_character_creator.fictional_characters_created -> Antonio -> fictional_universe.fictional_character.occupation -> Piracy\n# Answer:\nAntonio", "# Reasoning Path:\nWilliam Shakespeare -> fictional_universe.fictional_character.occupation -> Playwright -> people.profession.corresponding_type -> Author\n# Answer:\nPlaywright"], "ground_truth": ["Poet", "Lyricist", "Author", "Playwright", "Actor"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.8000000000000002, "ans_precission": 0.8, "ans_recall": 0.8, "path_f1": 0.48, "path_precision": 0.4, "path_recall": 0.6, "path_ans_f1": 0.8000000000000002, "path_ans_precision": 0.8, "path_ans_recall": 0.8}
{"id": "WebQTest-317", "prediction": ["# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Ukraine\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Belarus\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\n2008 Summer Olympics", "# Reasoning Path:\nUkraine -> location.statistical_region.gender_balance_members_of_parliament -> g.11b71z0kvd\n# Answer:\nlocation.statistical_region.gender_balance_members_of_parliament", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Albanian language -> common.topic.notable_types -> Human Language\n# Answer:\nAlbanian language", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> Ukraine\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Poland\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Albanian language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nAlbanian language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.region -> Europe\n# Answer:\nUkrainian Language", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> Belarus\n# Answer:\n2002 Winter Olympics"], "ground_truth": ["Uzbekistan"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-318", "prediction": ["# Reasoning Path:\nSwitzerland -> location.country.currency_used -> Swiss franc -> finance.currency.countries_used -> Liechtenstein\n# Answer:\nSwiss franc", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d0tc3g\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> Italian Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6p27nwl\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> Italian Language -> language.human_language.main_country -> Italy\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.1hhc3d66v\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> Italian Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> Italian Language -> language.human_language.region -> Europe\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> French -> language.human_language.main_country -> France\n# Answer:\nFrench"], "ground_truth": ["Swiss franc"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.country.form_of_government -> Parliamentary system\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.country.languages_spoken -> Malay Language\n# Answer:\nChristmas Island", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.country.form_of_government -> Federal monarchy\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc4pgdn\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> base.ontologies.ontology_instance.equivalent_instances -> m.09knr56 -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nOpenCyc", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nCocos (Keeling) Islands", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc4y0zl\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands -> location.country.form_of_government -> Federal monarchy\n# Answer:\nCocos (Keeling) Islands", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands -> location.location.time_zones -> Cocos Islands\u00a0Time Zone\n# Answer:\nCocos (Keeling) Islands"], "ground_truth": ["AUD"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-321", "prediction": ["# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> Japan\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars"], "ground_truth": ["Esperanto Language", "English Language", "Lojban"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-322", "prediction": ["# Reasoning Path:\nIan Somerhalder -> film.actor.film -> m.0cw0xkk -> film.performance.film -> The Sensation of Sight\n# Answer:\nThe Sensation of Sight", "# Reasoning Path:\nIan Somerhalder -> film.actor.film -> m.02vc21w -> film.performance.film -> In Enemy Hands\n# Answer:\nIn Enemy Hands", "# Reasoning Path:\nIan Somerhalder -> film.actor.film -> m.0crv_k2 -> film.performance.film -> Lost City Raiders\n# Answer:\nLost City Raiders", "# Reasoning Path:\nIan Somerhalder -> film.actor.film -> m.0cw0xkk -> film.performance.character -> Drifter\n# Answer:\nDrifter", "# Reasoning Path:\nIan Somerhalder -> film.actor.film -> m.02vc21w -> film.performance.character -> Danny Miller\n# Answer:\nDanny Miller", "# Reasoning Path:\nIan Somerhalder -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#range -> Person\n# Answer:\nChildren", "# Reasoning Path:\nIan Somerhalder -> film.actor.film -> m.0crv_k2 -> film.performance.character -> Jack Kubiak\n# Answer:\nJack Kubiak", "# Reasoning Path:\nIan Somerhalder -> freebase.valuenotation.is_reviewed -> Children -> type.property.schema -> Person\n# Answer:\nChildren", "# Reasoning Path:\nIan Somerhalder -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nIan Somerhalder -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#domain -> Person\n# Answer:\nChildren"], "ground_truth": ["Celebrity", "Marco Polo", "The Rules of Attraction", "In Enemy Hands", "How to Make Love to a Woman", "Pulse", "Life as a House", "Changing Hearts", "Anatomy of a Hate Crime", "The Sensation of Sight", "National Lampoon's TV: The Movie", "Time Framed", "The Old Man and the Studio", "Wake", "Fireball", "Caught on Tape", "Fearless", "The Anomaly", "Recess", "The Lost Samaritan", "The Tournament", "Lost City Raiders"], "ans_acc": 0.13636363636363635, "ans_hit": 1, "ans_f1": 0.18749999999999997, "ans_precission": 0.3, "ans_recall": 0.13636363636363635, "path_f1": 0.18181818181818182, "path_precision": 0.3, "path_recall": 0.13043478260869565, "path_ans_f1": 0.18749999999999997, "path_ans_precision": 0.3, "path_ans_recall": 0.13636363636363635}
{"id": "WebQTest-323", "prediction": ["# Reasoning Path:\nRon Howard -> film.producer.film -> Dr. Seuss' How the Grinch Stole Christmas -> film.film.executive_produced_by -> Todd Hallowell\n# Answer:\nDr. Seuss' How the Grinch Stole Christmas", "# Reasoning Path:\nRon Howard -> film.producer.film -> A Beautiful Mind -> film.film.film_casting_director -> Jane Jenkins\n# Answer:\nA Beautiful Mind", "# Reasoning Path:\nRon Howard -> film.producer.film -> Dr. Seuss' How the Grinch Stole Christmas -> film.film.production_companies -> Universal Studios\n# Answer:\nDr. Seuss' How the Grinch Stole Christmas", "# Reasoning Path:\nRon Howard -> film.director.film -> A Beautiful Mind -> film.film.film_casting_director -> Jane Jenkins\n# Answer:\nA Beautiful Mind", "# Reasoning Path:\nRon Howard -> film.producer.film -> Dr. Seuss' How the Grinch Stole Christmas -> film.film.film_casting_director -> Jane Jenkins\n# Answer:\nDr. Seuss' How the Grinch Stole Christmas", "# Reasoning Path:\nRon Howard -> film.director.film -> Night Shift -> film.film.produced_by -> Brian Grazer\n# Answer:\nNight Shift", "# Reasoning Path:\nRon Howard -> film.producer.film -> A Beautiful Mind -> film.film.film_casting_director -> Janet Hirshenson\n# Answer:\nA Beautiful Mind", "# Reasoning Path:\nRon Howard -> film.producer.film -> A Beautiful Mind -> film.film.production_companies -> Universal Studios\n# Answer:\nA Beautiful Mind", "# Reasoning Path:\nRon Howard -> film.director.film -> A Beautiful Mind -> film.film.production_companies -> Universal Studios\n# Answer:\nA Beautiful Mind", "# Reasoning Path:\nRon Howard -> film.producer.film -> The Da Vinci Code -> film.film.production_companies -> Imagine Entertainment\n# Answer:\nThe Da Vinci Code"], "ground_truth": ["Inferno", "Angels & Demons", "Far and Away", "Jay-Z: Made in America", "Night Shift", "Parenthood", "In the Heart of the Sea", "The Da Vinci Code", "Gung Ho", "Rush", "Dr. Seuss' How the Grinch Stole Christmas", "The Dark Tower", "Willow", "The Paper", "Presidential Reunion", "The Missing", "Ransom", "Splash", "Cocoon", "Cinderella Man", "Backdraft", "Grand Theft Auto", "EDtv", "Frost/Nixon", "The Lost Symbol", "The Dilemma", "Apollo 13", "A Beautiful Mind", "Cotton Candy"], "ans_acc": 0.13793103448275862, "ans_hit": 1, "ans_f1": 0.2424242424242424, "ans_precission": 1.0, "ans_recall": 0.13793103448275862, "path_f1": 0.17142857142857143, "path_precision": 0.5, "path_recall": 0.10344827586206896, "path_ans_f1": 0.2424242424242424, "path_ans_precision": 1.0, "path_ans_recall": 0.13793103448275862}
{"id": "WebQTest-326", "prediction": ["# Reasoning Path:\nGerald Ford -> people.deceased_person.cause_of_death -> Cardiovascular disease -> people.cause_of_death.includes_causes_of_death -> Hypertension\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nGerald Ford -> people.deceased_person.cause_of_death -> Cardiovascular disease -> medicine.disease.parent_disease -> Ill-defined descriptions and complications of heart disease\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nGerald Ford -> people.deceased_person.cause_of_death -> Cardiovascular disease -> people.cause_of_death.includes_causes_of_death -> heart attack\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nGerald Ford -> people.deceased_person.cause_of_death -> Cardiovascular disease -> medicine.disease.medical_specialties -> Internal medicine\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nGerald Ford -> people.deceased_person.cause_of_death -> Cardiovascular disease -> people.cause_of_death.includes_causes_of_death -> Angina Pectoris\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nGerald Ford -> people.person.quotations -> A strong defense is the surest way to peace. Strength makes detente attainable. Weakness invites war, as my generationmy generationknows from four very bitter experiences. Just as Americas will for peace is second to none, so will Americas strength be second to none. We cannot rely on the forbearance of others to protect this Nation. The power and diversity of the Armed Forces, active Guard and Reserve, the resolve of our fellow citizens, the flexibility in our command to navigate international waters that remain troubled are all essential to our security. -> media_common.quotation.subjects -> Uncategorised\n# Answer:\nA strong defense is the surest way to peace. Strength makes detente attainable. Weakness invites war, as my generationmy generationknows from four very bitter experiences. Just as Americas will for peace is second to none, so will Americas strength be second to none. We cannot rely on the forbearance of others to protect this Nation. The power and diversity of the Armed Forces, active Guard and Reserve, the resolve of our fellow citizens, the flexibility in our command to navigate international waters that remain troubled are all essential to our security.", "# Reasoning Path:\nGerald Ford -> people.person.quotations -> All of us who served in one war or another know very well that all wars are the glory and the agony of the young. -> media_common.quotation.subjects -> Uncategorised\n# Answer:\nAll of us who served in one war or another know very well that all wars are the glory and the agony of the young.", "# Reasoning Path:\nGerald Ford -> government.political_appointer.appointees -> m.01xrx8h -> government.government_position_held.office_holder -> George H. W. Bush\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nGerald Ford -> government.political_appointer.appointees -> m.01z0qwq -> government.government_position_held.office_holder -> Dick Cheney\n# Answer:\nDick Cheney", "# Reasoning Path:\nGerald Ford -> government.political_appointer.appointees -> m.04cnx_5 -> government.government_position_held.office_holder -> Donald Rumsfeld\n# Answer:\nDonald Rumsfeld"], "ground_truth": ["Cardiovascular disease"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-327", "prediction": ["# Reasoning Path:\nTurkey -> base.locations.countries.continent -> Eurasia -> base.locations.continents.countries_within -> France\n# Answer:\nEurasia", "# Reasoning Path:\nTurkey -> base.locations.countries.continent -> Eurasia -> base.locations.continents.countries_within -> Azerbaijan\n# Answer:\nEurasia", "# Reasoning Path:\nTurkey -> base.locations.countries.continent -> Eurasia -> common.topic.notable_types -> Location\n# Answer:\nEurasia", "# Reasoning Path:\nTurkey -> location.location.partially_containedby -> Asia -> location.location.partiallycontains -> m.0wg8kz1\n# Answer:\nAsia", "# Reasoning Path:\nTurkey -> location.location.partially_containedby -> Asia -> base.locations.continents.countries_within -> Syria\n# Answer:\nAsia", "# Reasoning Path:\nTurkey -> location.location.partially_containedby -> Kurd Mountains -> location.location.partiallycontains -> m.0wg9l7z\n# Answer:\nKurd Mountains", "# Reasoning Path:\nTurkey -> location.location.partially_containedby -> Asia -> location.location.time_zones -> Eastern European Time Zone\n# Answer:\nAsia", "# Reasoning Path:\nTurkey -> location.location.partially_containedby -> Europe -> base.locations.continents.countries_within -> United Kingdom\n# Answer:\nEurope", "# Reasoning Path:\nTurkey -> location.location.partially_containedby -> Asia -> base.locations.continents.countries_within -> Iran\n# Answer:\nAsia", "# Reasoning Path:\nTurkey -> location.location.partially_containedby -> Kurd Mountains -> location.location.partially_contains -> Syria\n# Answer:\nKurd Mountains"], "ground_truth": ["Eurasia", "Europe", "Asia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-328", "prediction": ["# Reasoning Path:\nKevin Love -> people.person.education -> m.04ftwdc -> education.education.institution -> University of California, Los Angeles\n# Answer:\nUniversity of California, Los Angeles", "# Reasoning Path:\nKevin Love -> people.person.education -> m.045z4gx -> education.education.institution -> Lake Oswego High School\n# Answer:\nLake Oswego High School", "# Reasoning Path:\nKevin Love -> common.topic.notable_for -> g.125c8dfyr\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nKevin Love -> freebase.valuenotation.is_reviewed -> Height -> type.property.unit -> Meter\n# Answer:\nHeight", "# Reasoning Path:\nKevin Love -> freebase.valuenotation.is_reviewed -> Height -> type.property.schema -> Person\n# Answer:\nHeight", "# Reasoning Path:\nKevin Love -> freebase.valuenotation.is_reviewed -> Teams -> type.property.expected_type -> Sports team roster\n# Answer:\nTeams", "# Reasoning Path:\nKevin Love -> freebase.valuenotation.is_reviewed -> Height -> rdf-schema#domain -> Person\n# Answer:\nHeight", "# Reasoning Path:\nKevin Love -> freebase.valuenotation.is_reviewed -> Teams -> type.property.schema -> Athlete\n# Answer:\nTeams", "# Reasoning Path:\nKevin Love -> freebase.valuenotation.is_reviewed -> Teams -> rdf-schema#range -> Sports team roster\n# Answer:\nTeams"], "ground_truth": ["University of California, Los Angeles"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.1818181818181818, "path_precision": 0.1111111111111111, "path_recall": 0.5, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-329", "prediction": ["# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Japan\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Albania\n# Answer:\nParliamentary republic", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Brazil\n# Answer:\nConstitutional republic", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37cxr\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic"], "ground_truth": ["Unitary state", "Parliamentary republic", "Constitutional republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ll\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22zj\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc378pv\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc37hbq\n# Answer:\nlocation.statistical_region.part_time_employment_percent"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-331", "prediction": ["# Reasoning Path:\nKarnak -> location.location.containedby -> Egypt -> location.location.containedby -> Middle East\n# Answer:\nEgypt", "# Reasoning Path:\nKarnak -> location.location.containedby -> Egypt -> location.location.containedby -> Arab world\n# Answer:\nEgypt", "# Reasoning Path:\nKarnak -> location.location.containedby -> Egypt -> location.country.administrative_divisions -> Cairo\n# Answer:\nEgypt", "# Reasoning Path:\nKarnak -> location.location.containedby -> Luxor Governorate -> location.location.containedby -> Egypt\n# Answer:\nLuxor Governorate", "# Reasoning Path:\nKarnak -> location.location.containedby -> Egypt -> location.country.administrative_divisions -> Suez Governorate\n# Answer:\nEgypt", "# Reasoning Path:\nKarnak -> travel.tourist_attraction.near_travel_destination -> Luxor -> location.location.containedby -> Egypt\n# Answer:\nLuxor", "# Reasoning Path:\nKarnak -> location.location.containedby -> Egypt -> location.country.administrative_divisions -> Al Sharqia Governorate\n# Answer:\nEgypt", "# Reasoning Path:\nKarnak -> location.location.containedby -> Egypt -> government.governmental_jurisdiction.government_bodies -> Shura Council\n# Answer:\nEgypt", "# Reasoning Path:\nKarnak -> location.location.containedby -> Luxor Governorate -> location.location.containedby -> North Africa\n# Answer:\nLuxor Governorate", "# Reasoning Path:\nKarnak -> location.location.containedby -> Luxor Governorate -> common.topic.notable_types -> Administrative Division\n# Answer:\nLuxor Governorate"], "ground_truth": ["Luxor Governorate", "Egypt"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-333", "prediction": ["# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.0_sv31_ -> award.award_nomination.nominated_for -> Bruce Almighty\n# Answer:\nBruce Almighty", "# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.0_tl4p_ -> award.award_nomination.nominated_for -> Driving Miss Daisy\n# Answer:\nDriving Miss Daisy", "# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.0_sv31_ -> award.award_nomination.ceremony -> 35th NAACP Image Awards\n# Answer:\n35th NAACP Image Awards", "# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.021ydzf -> award.award_nomination.nominated_for -> Driving Miss Daisy\n# Answer:\nDriving Miss Daisy", "# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.0_tl4p_ -> freebase.valuenotation.is_reviewed -> Ceremony\n# Answer:\nCeremony", "# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.021ydzf -> award.award_nomination.ceremony -> 62nd Academy Awards\n# Answer:\n62nd Academy Awards", "# Reasoning Path:\nMorgan Freeman -> common.topic.webpage -> m.093_l_6 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.021ydzf -> award.award_nomination.award -> Academy Award for Best Actor\n# Answer:\nAcademy Award for Best Actor", "# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.0_tl4p_ -> freebase.valuenotation.is_reviewed -> Year\n# Answer:\nYear", "# Reasoning Path:\nMorgan Freeman -> common.topic.webpage -> m.093zz6z -> common.webpage.resource -> Robert Iler pleads guilty to mugging charge\n# Answer:\nRobert Iler pleads guilty to mugging charge"], "ground_truth": ["Evan Almighty", "Johnny Handsome", "The Long Way Home", "Brubaker", "Lean on Me", "America Beyond the Color Line", "Island of Lemurs: Madagascar 3D", "Street Smart", "5 Flights Up", "The Bonfire of the Vanities", "Eyewitness", "The Magic of Belle Isle", "The Art of Romare Bearden", "Transcendence", "Wish Wizard", "Driving Miss Daisy", "Conan the Barbarian", "A Man Called Adam", "The Dark Knight", "Heart Stopper", "Feast of Love", "The Power of One", "Now You See Me: The Second Act", "Guilty by Association", "The Love Guru", "Clinton and Nadine", "The Big Bounce", "The Contract", "Lucky Number Slevin", "Chain Reaction", "Glory", "Resting Place", "For Love of Liberty: The Story of America's Black Patriots", "m.0h0_gsx", "Lucy", "The Hunting of the President", "Unforgiven", "The Dark Knight Rises", "We the People", "Batman Begins", "A Raisin in the Sun", "Oblivion", "Ben-Hur", "Soul Brothas and Sistas: Vol. 4: Quadruple Feature", "The Maiden Heist", "The Civil War", "The Execution of Raymond Graham", "Under Suspicion", "Magnificent Desolation: Walking On The Moon 3D", "Last Knights", "Attica", "Harry & Son", "The Shawshank Redemption", "Thick as Thieves", "War of the Worlds", "Wanted", "Fight for Life", "Edison", "London Has Fallen", "Seven", "Bruce Almighty", "Roll Of Thunder, Hear My Cry", "Now You See Me", "m.0h0zs8c", "RED", "Teachers", "Olympus Has Fallen", "Ted 2", "Nurse Betty", "Along Came a Spider", "The Lego Movie", "Cosmic Voyage", "Deep Impact", "Outbreak", "Million Dollar Baby", "An Unfinished Life", "Last Vegas", "Gone Baby Gone", "Unleashed", "Dolphin Tale 2", "High Crimes", "The Sum of All Fears", "Invictus", "Levity", "Kiss the Girls", "All About Us", "National Geographic: Inside the White House", "The Bucket List", "Dolphin Tale", "Marie", "Death of a Prophet", "Clean and Sober", "Amistad", "Where Were You When the Lights Went Out?", "Robin Hood: Prince of Thieves", "Hard Rain", "Moll Flanders", "Dreamcatcher", "10 Items or Less", "That Was Then... This Is Now", "The Pawnbroker"], "ans_acc": 0.019801980198019802, "ans_hit": 1, "ans_f1": 0.03715170278637771, "ans_precission": 0.3, "ans_recall": 0.019801980198019802, "path_f1": 0.075, "path_precision": 0.3, "path_recall": 0.04285714285714286, "path_ans_f1": 0.03715170278637771, "path_ans_precision": 0.3, "path_ans_recall": 0.019801980198019802}
{"id": "WebQTest-334", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> base.popstra.celebrity.dated -> m.065pzp2 -> base.popstra.dated.participant -> Arlene Dahl\n# Answer:\nArlene Dahl", "# Reasoning Path:\nJohn F. Kennedy -> base.popstra.celebrity.dated -> m.065pt_4 -> base.popstra.dated.participant -> Betty Grable\n# Answer:\nBetty Grable", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> White people -> medicine.risk_factor.diseases -> Breast cancer\n# Answer:\nWhite people", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> 11/22/63\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> common.topic.notable_types -> Event\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School -> common.topic.notable_for -> g.125583hnb\n# Answer:\nBloomington Kennedy High School", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> base.culturalevent.event.entity_involved -> Lee Harvey Oswald\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Cape Canaveral -> common.topic.article -> m.0f76p\n# Answer:\nCape Canaveral", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nIrish American", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> 16 Questions on the Assassination\n# Answer:\nAssassination of John F. Kennedy"], "ground_truth": ["Marilyn Monroe", "Judith Exner"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-335", "prediction": ["# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban peso -> common.topic.notable_for -> g.125c74q9f\n# Answer:\nCuban peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban convertible peso -> common.topic.notable_for -> g.1257mndy1\n# Answer:\nCuban convertible peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban peso -> common.topic.notable_types -> Currency\n# Answer:\nCuban peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban convertible peso -> common.topic.webpage -> m.04m29wr\n# Answer:\nCuban convertible peso", "# Reasoning Path:\nCuba -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc38hmp\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban convertible peso -> common.topic.article -> m.049p6m\n# Answer:\nCuban convertible peso", "# Reasoning Path:\nCuba -> book.book_subject.works -> A simple Habana melody -> book.written_work.subjects -> History\n# Answer:\nA simple Habana melody", "# Reasoning Path:\nCuba -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc3d68y\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nCuba -> book.book_subject.works -> White rose -> book.written_work.subjects -> History\n# Answer:\nWhite rose", "# Reasoning Path:\nCuba -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc3d9ps\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita"], "ground_truth": ["Cuban peso", "Cuban convertible peso"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-336", "prediction": ["# Reasoning Path:\nMorocco -> location.country.currency_used -> Moroccan dirham -> finance.currency.countries_used -> Western Sahara\n# Answer:\nMoroccan dirham", "# Reasoning Path:\nMorocco -> location.country.currency_used -> Moroccan dirham -> common.topic.notable_types -> Currency\n# Answer:\nMoroccan dirham", "# Reasoning Path:\nMorocco -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.1hhc385h5\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nMorocco -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.1hhc39ccx\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nMorocco -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.1hhc3_868\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Taza-Al Hoceima-Taounate -> common.topic.article -> m.0dgr09\n# Answer:\nTaza-Al Hoceima-Taounate", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Chaouia-Ouardigha -> location.location.time_zones -> Western European Time Zone\n# Answer:\nChaouia-Ouardigha", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Doukkala-Abda -> location.location.people_born_here -> Bouchaib Rmail\n# Answer:\nDoukkala-Abda", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Chaouia-Ouardigha -> common.topic.article -> m.0d1426\n# Answer:\nChaouia-Ouardigha", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Doukkala-Abda -> common.topic.notable_types -> Administrative Division\n# Answer:\nDoukkala-Abda"], "ground_truth": ["Moroccan dirham"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-337", "prediction": ["# Reasoning Path:\nMary McLeod Bethune -> people.person.profession -> g.121bkpjb\n# Answer:\npeople.person.profession", "# Reasoning Path:\nMary McLeod Bethune -> symbols.name_source.namesakes -> Mary McLeod Bethune Council House National Historic Site -> travel.tourist_attraction.near_travel_destination -> Washington, D.C.\n# Answer:\nMary McLeod Bethune Council House National Historic Site", "# Reasoning Path:\nMary McLeod Bethune -> symbols.name_source.namesakes -> Mary McLeod Bethune Council House National Historic Site -> common.topic.notable_for -> g.1258mdtk6\n# Answer:\nMary McLeod Bethune Council House National Historic Site", "# Reasoning Path:\nMary McLeod Bethune -> symbols.name_source.namesakes -> Bethune-Cookman University -> education.educational_institution.sports_teams -> Bethune-Cookman Wildcats men's basketball\n# Answer:\nBethune-Cookman University", "# Reasoning Path:\nMary McLeod Bethune -> people.person.education -> m.0n0v47n -> education.education.institution -> Barber\u2013Scotia College\n# Answer:\nBarber\u2013Scotia College", "# Reasoning Path:\nMary McLeod Bethune -> people.person.education -> m.0n1k53t -> education.education.institution -> Moody Bible Institute\n# Answer:\nMoody Bible Institute", "# Reasoning Path:\nMary McLeod Bethune -> symbols.name_source.namesakes -> Bethune-Cookman University -> common.topic.notable_types -> College/University\n# Answer:\nBethune-Cookman University", "# Reasoning Path:\nMary McLeod Bethune -> symbols.name_source.namesakes -> Mary McLeod Bethune Home -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.05h5_mz\n# Answer:\nMary McLeod Bethune Home", "# Reasoning Path:\nMary McLeod Bethune -> symbols.name_source.namesakes -> Bethune-Cookman University -> education.educational_institution.sports_teams -> Bethune-Cookman Wildcats football\n# Answer:\nBethune-Cookman University", "# Reasoning Path:\nMary McLeod Bethune -> symbols.name_source.namesakes -> Bethune-Cookman University -> education.educational_institution.students_graduates -> m.02kyhrl\n# Answer:\nBethune-Cookman University"], "ground_truth": ["Educator"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-338", "prediction": ["# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.11b71r82sc\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_4lv6\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_6fj1\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.12cp_jcjg\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.12tb6g_8v\n# Answer:\nlocation.statistical_region.deposit_interest_rate"], "ground_truth": ["Japanese yen"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-339", "prediction": ["# Reasoning Path:\nMother Teresa -> people.person.nationality -> Ottoman Empire -> common.topic.notable_types -> Country\n# Answer:\nOttoman Empire", "# Reasoning Path:\nMother Teresa -> base.activism.activist.area_of_activism -> Humanitarian -> common.topic.notable_for -> g.1256fnzxx\n# Answer:\nHumanitarian", "# Reasoning Path:\nMother Teresa -> people.person.nationality -> India -> base.aareas.schema.administrative_area.administrative_children -> Tamil Nadu\n# Answer:\nIndia", "# Reasoning Path:\nMother Teresa -> people.person.nationality -> India -> common.topic.notable_types -> Country\n# Answer:\nIndia", "# Reasoning Path:\nMother Teresa -> base.activism.activist.area_of_activism -> Humanitarian -> common.topic.notable_types -> Profession\n# Answer:\nHumanitarian", "# Reasoning Path:\nMother Teresa -> people.person.nationality -> India -> location.country.languages_spoken -> English Language\n# Answer:\nIndia", "# Reasoning Path:\nMother Teresa -> people.person.nationality -> India -> base.aareas.schema.administrative_area.administrative_children -> West Bengal\n# Answer:\nIndia", "# Reasoning Path:\nMother Teresa -> base.activism.activist.area_of_activism -> Humanitarian -> people.profession.people_with_this_profession -> Albert Friedlander\n# Answer:\nHumanitarian", "# Reasoning Path:\nMother Teresa -> base.activism.activist.area_of_activism -> Humanitarian -> people.profession.people_with_this_profession -> Andrew Hunt\n# Answer:\nHumanitarian", "# Reasoning Path:\nMother Teresa -> film.film.release_date_s -> m.0v8_lfk -> film.film_regional_release_date.film_release_region -> United States of America\n# Answer:\nUnited States of America"], "ground_truth": ["India", "Ottoman Empire"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> common.topic.notable_for -> g.125dysc88\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0z23kt0 -> sports.sports_team_roster.team -> Auburn Tigers football\n# Answer:\nAuburn Tigers football", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> common.topic.notable_types -> American football player\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.team -> Carolina Panthers\n# Answer:\nCarolina Panthers", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0z23kt0 -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> people.person.children -> Cecil Newton\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> people.person.gender -> Male\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> people.person.parents -> Jackie Newton -> common.topic.notable_for -> g.125dzwcd6\n# Answer:\nJackie Newton", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["Carolina Panthers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-340", "prediction": ["# Reasoning Path:\nRichard Pryor -> people.person.children -> Rain Pryor -> people.person.children -> Lotus Marie Pryor\n# Answer:\nRain Pryor", "# Reasoning Path:\nRichard Pryor -> people.person.children -> Richard Pryor Jr. -> people.person.parents -> Patricia Price\n# Answer:\nRichard Pryor Jr.", "# Reasoning Path:\nRichard Pryor -> common.topic.notable_for -> g.1257xht0m\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nRichard Pryor -> people.person.children -> Richard Pryor Jr. -> people.person.gender -> Male\n# Answer:\nRichard Pryor Jr.", "# Reasoning Path:\nRichard Pryor -> people.person.children -> Steven Pryor -> people.person.parents -> Flynn Belaine\n# Answer:\nSteven Pryor", "# Reasoning Path:\nRichard Pryor -> people.person.children -> Richard Pryor Jr. -> people.person.sibling_s -> m.0t4tf8x\n# Answer:\nRichard Pryor Jr.", "# Reasoning Path:\nRichard Pryor -> people.person.children -> Rain Pryor -> people.person.profession -> Comedian\n# Answer:\nRain Pryor", "# Reasoning Path:\nRichard Pryor -> people.person.children -> Rain Pryor -> people.person.sibling_s -> m.0t4tf6_\n# Answer:\nRain Pryor", "# Reasoning Path:\nRichard Pryor -> people.person.children -> Steven Pryor -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nSteven Pryor", "# Reasoning Path:\nRichard Pryor -> people.person.children -> Richard Pryor Jr. -> people.person.sibling_s -> m.0t4tf91\n# Answer:\nRichard Pryor Jr."], "ground_truth": ["Franklin Pryor", "Steven Pryor", "Richard Pryor Jr."], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.631578947368421, "ans_precission": 0.6, "ans_recall": 0.6666666666666666, "path_f1": 0.631578947368421, "path_precision": 0.6, "path_recall": 0.6666666666666666, "path_ans_f1": 0.631578947368421, "path_ans_precision": 0.6, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-341", "prediction": ["# Reasoning Path:\nSpain -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n1952 Summer Olympics", "# Reasoning Path:\nSpain -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> Belgium\n# Answer:\n1952 Summer Olympics", "# Reasoning Path:\nSpain -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nSpain -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> France\n# Answer:\n1952 Summer Olympics", "# Reasoning Path:\nSpain -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.sports -> Artistic gymnastics\n# Answer:\n1952 Summer Olympics", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60xs9d2\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc38hhk\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.sports -> Basketball\n# Answer:\n1952 Summer Olympics"], "ground_truth": ["Gibraltar", "Morocco", "France", "Portugal", "Andorra"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.018348623853211014, "path_precision": 0.1, "path_recall": 0.010101010101010102, "path_ans_f1": 0.13333333333333333, "path_ans_precision": 0.1, "path_ans_recall": 0.2}
{"id": "WebQTest-342", "prediction": ["# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.location.containedby -> North America\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Papua New Guinea -> location.country.languages_spoken -> Tok Pisin Language\n# Answer:\nPapua New Guinea", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Papua New Guinea -> location.country.official_language -> Tok Pisin Language\n# Answer:\nPapua New Guinea", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> New Zealand -> location.country.languages_spoken -> Standard Chinese\n# Answer:\nNew Zealand", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.location.containedby -> Americas\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> New Zealand -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nNew Zealand", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> United States of America -> location.location.primarily_containedby -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> New Zealand -> location.country.languages_spoken -> Standard Chinese\n# Answer:\nNew Zealand", "# Reasoning Path:\nEnglish Language -> education.school_category.schools_of_this_kind -> Piers Midwinter -> people.person.quotations -> Learn with a NEST. Please be my GUEST. I will give you my BEST,\n# Answer:\nPiers Midwinter"], "ground_truth": ["Montserrat", "Swaziland", "Jersey", "Saint Lucia", "Uganda", "Hong Kong", "Bermuda", "Malta", "Nigeria", "Tanzania", "Republic of Ireland", "Turks and Caicos Islands", "Marshall Islands", "Belize", "South Africa", "Guam", "Singapore", "Gibraltar", "Cayman Islands", "Canada", "Gambia", "Saint Kitts and Nevis", "Namibia", "Sierra Leone", "Dominica", "Philippines", "United Kingdom", "Kenya", "Nauru", "Liberia", "Ghana", "Vanuatu", "Zambia", "Cook Islands", "Rwanda", "New Zealand", "Pakistan", "Barbados", "Saint Vincent and the Grenadines", "Grenada", "Botswana", "Samoa", "Kiribati", "Cameroon", "Tuvalu", "Papua New Guinea", "Bahamas", "Fiji", "Sudan", "Territory of Papua and New Guinea", "India", "Wales", "Antigua and Barbuda", "Lesotho", "Guyana", "Isle of Man", "Territory of New Guinea", "Puerto Rico", "Zimbabwe"], "ans_acc": 0.05084745762711865, "ans_hit": 1, "ans_f1": 0.0956175298804781, "ans_precission": 0.8, "ans_recall": 0.05084745762711865, "path_f1": 0.09480812641083522, "path_precision": 0.7, "path_recall": 0.05084745762711865, "path_ans_f1": 0.0956175298804781, "path_ans_precision": 0.8, "path_ans_recall": 0.05084745762711865}
{"id": "WebQTest-343", "prediction": ["# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> Shanghai Cooperation Organisation -> organization.organization.founders -> Kazakhstan\n# Answer:\nShanghai Cooperation Organisation", "# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> Shanghai Cooperation Organisation -> organization.organization.founders -> Kyrgyzstan\n# Answer:\nShanghai Cooperation Organisation", "# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> Shanghai Cooperation Organisation -> organization.organization.founders -> Tajikistan\n# Answer:\nShanghai Cooperation Organisation", "# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> UNESCO -> organization.organization.founders -> United States of America\n# Answer:\nUNESCO", "# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> Shanghai Cooperation Organisation -> organization.membership_organization.members -> m.04dj3ty\n# Answer:\nShanghai Cooperation Organisation", "# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> UNESCO -> organization.organization.founders -> Brazil\n# Answer:\nUNESCO", "# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> UNESCO -> organization.membership_organization.members -> m.0w04xd_\n# Answer:\nUNESCO", "# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> BRICS -> organization.organization.founders -> Russia\n# Answer:\nBRICS", "# Reasoning Path:\nChina -> base.locations.countries.continent -> Asia -> location.location.partially_contains -> Russia\n# Answer:\nAsia", "# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> UNESCO -> organization.organization.founders -> France\n# Answer:\nUNESCO"], "ground_truth": ["Shanghai Cooperation Organisation", "UNESCO", "Asia-Pacific Economic Cooperation", "G-20 major economies", "Caribbean Development Bank", "World Bank", "United Nations", "African Development Bank", "Asian Development Bank"], "ans_acc": 0.2222222222222222, "ans_hit": 1, "ans_f1": 0.3478260869565218, "ans_precission": 0.8, "ans_recall": 0.2222222222222222, "path_f1": 0.3478260869565218, "path_precision": 0.8, "path_recall": 0.2222222222222222, "path_ans_f1": 0.3478260869565218, "path_ans_precision": 0.8, "path_ans_recall": 0.2222222222222222}
{"id": "WebQTest-344", "prediction": ["# Reasoning Path:\nWest St. Paul -> location.location.containedby -> Dakota County -> location.location.containedby -> Minnesota\n# Answer:\nDakota County", "# Reasoning Path:\nWest St. Paul -> location.hud_county_place.county -> Dakota County -> location.location.containedby -> Minnesota\n# Answer:\nDakota County", "# Reasoning Path:\nWest St. Paul -> location.location.containedby -> Dakota County -> location.location.containedby -> Minneapolis-St. Paul-Bloomington, MN-WI Metropolitan Statistical Area\n# Answer:\nDakota County", "# Reasoning Path:\nWest St. Paul -> location.location.containedby -> Minnesota -> location.location.containedby -> West North Central States\n# Answer:\nMinnesota", "# Reasoning Path:\nWest St. Paul -> location.location.containedby -> Dakota County -> location.statistical_region.co2_emissions_residential -> m.045jr02\n# Answer:\nDakota County", "# Reasoning Path:\nWest St. Paul -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nWest St. Paul -> location.location.containedby -> Dakota County -> location.statistical_region.population -> g.11b66j64vw\n# Answer:\nDakota County", "# Reasoning Path:\nWest St. Paul -> location.statistical_region.population -> g.11b66g3pg3\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nWest St. Paul -> location.hud_county_place.county -> Dakota County -> location.location.containedby -> Minneapolis-St. Paul-Bloomington, MN-WI Metropolitan Statistical Area\n# Answer:\nDakota County", "# Reasoning Path:\nWest St. Paul -> location.location.containedby -> Dakota County -> location.statistical_region.population -> g.11x1cfpt9\n# Answer:\nDakota County"], "ground_truth": ["Dakota County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-346", "prediction": ["# Reasoning Path:\nThe Sixth Sense -> film.film.starring -> m.0k6gz9 -> film.performance.character -> Vincent Grey\n# Answer:\nVincent Grey", "# Reasoning Path:\nDonnie Wahlberg -> film.actor.film -> m.0k6gz9 -> film.performance.character -> Vincent Grey\n# Answer:\nVincent Grey", "# Reasoning Path:\nThe Sixth Sense -> film.film.starring -> m.0k6gz9 -> film.performance.actor -> Donnie Wahlberg\n# Answer:\nfilm.performance.actor", "# Reasoning Path:\nThe Sixth Sense -> film.film.starring -> m.03jq0nf -> film.performance.character -> Kyra Collins\n# Answer:\nKyra Collins", "# Reasoning Path:\nDonnie Wahlberg -> film.actor.film -> m.02tbg6_ -> film.performance.character -> Mr. Grey\n# Answer:\nMr. Grey", "# Reasoning Path:\nDonnie Wahlberg -> film.actor.film -> m.0k6gz9 -> film.performance.film -> The Sixth Sense\n# Answer:\nfilm.performance.film", "# Reasoning Path:\nThe Sixth Sense -> film.film.starring -> m.03jrcw6 -> film.performance.character -> Sean\n# Answer:\nSean", "# Reasoning Path:\nDonnie Wahlberg -> film.actor.film -> m.02vc00j -> film.performance.character -> Danny Quinn\n# Answer:\nDanny Quinn", "# Reasoning Path:\nThe Sixth Sense -> film.film.starring -> m.03jq0nf -> film.performance.actor -> Mischa Barton\n# Answer:\nMischa Barton", "# Reasoning Path:\nDonnie Wahlberg -> film.actor.film -> m.02tbg6_ -> film.performance.film -> The Taking of Pelham One Two Three\n# Answer:\nThe Taking of Pelham One Two Three"], "ground_truth": ["Vincent Grey"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-347", "prediction": ["# Reasoning Path:\nMontesquieu -> common.topic.notable_for -> g.125fyk02t\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Georg Wilhelm Friedrich Hegel -> influence.influence_node.influenced_by -> Sextus Empiricus\n# Answer:\nGeorg Wilhelm Friedrich Hegel", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Jeremy Bentham -> influence.influence_node.influenced -> William Stanley Jevons\n# Answer:\nJeremy Bentham", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Georg Wilhelm Friedrich Hegel -> influence.influence_node.influenced_by -> Friedrich Wilhelm Joseph Schelling\n# Answer:\nGeorg Wilhelm Friedrich Hegel", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Adam Smith -> influence.influence_node.influenced -> Thomas Sowell\n# Answer:\nAdam Smith", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Georg Wilhelm Friedrich Hegel -> base.kwebbase.kwtopic.connections_to -> arthur schopenhauer opposed views of georg wilhelm friedrich hegel\n# Answer:\nGeorg Wilhelm Friedrich Hegel", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Jeremy Bentham -> influence.influence_node.influenced -> Atilla Yayla\n# Answer:\nJeremy Bentham", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Georg Wilhelm Friedrich Hegel -> influence.influence_node.influenced_by -> Adam Smith\n# Answer:\nGeorg Wilhelm Friedrich Hegel", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Georg Wilhelm Friedrich Hegel -> base.kwebbase.kwtopic.has_sentences -> All Hegel's later work can, to a large extent, be seen as an attempt to intellectualize his conviction of the Absolute as the \\\"geist\\\" (the spirit).\n# Answer:\nGeorg Wilhelm Friedrich Hegel", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Jeremy Bentham -> influence.influence_node.influenced -> David Pearce\n# Answer:\nJeremy Bentham"], "ground_truth": ["Roberto Mangabeira Unger", "\u00c9mile Durkheim", "Friedrich Hayek", "John Adams", "Isaiah Berlin", "Edward Gibbon", "James Madison", "Hannah Arendt", "Louis Althusser", "William Blackstone", "Edmund Burke", "Jean-Jacques Rousseau", "Thomas Jefferson", "Adam Smith", "Paul F\u00e9val, p\u00e8re", "Jeremy Bentham", "Clive James", "Georg Wilhelm Friedrich Hegel", "Thomas Paine", "Alexis de Tocqueville", "David Hume"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0.24657534246575338, "ans_precission": 0.9, "ans_recall": 0.14285714285714285, "path_f1": 0.24657534246575338, "path_precision": 0.9, "path_recall": 0.14285714285714285, "path_ans_f1": 0.24657534246575338, "path_ans_precision": 0.9, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-349", "prediction": ["# Reasoning Path:\nCarlos Boozer -> basketball.basketball_player.player_statistics -> m.04qgd67 -> basketball.basketball_player_stats.team -> Cleveland Cavaliers\n# Answer:\nCleveland Cavaliers", "# Reasoning Path:\nCarlos Boozer -> basketball.basketball_player.player_statistics -> m.04qjzy0 -> basketball.basketball_player_stats.team -> Utah Jazz\n# Answer:\nUtah Jazz", "# Reasoning Path:\nCarlos Boozer -> basketball.basketball_player.player_statistics -> m.04qht47 -> basketball.basketball_player_stats.team -> Utah Jazz\n# Answer:\nUtah Jazz", "# Reasoning Path:\nCarlos Boozer -> basketball.basketball_player.player_statistics -> m.04qgd67 -> basketball.basketball_player_stats.season -> 2002\u201303 NBA season\n# Answer:\n2002\u201303 NBA season", "# Reasoning Path:\nCarlos Boozer -> basketball.basketball_player.player_statistics -> m.04qjzy0 -> basketball.basketball_player_stats.season -> 2005\u201306 NBA season\n# Answer:\n2005\u201306 NBA season", "# Reasoning Path:\nCarlos Boozer -> base.schemastaging.athlete_extra.salary -> m.0j2hs7x -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nCarlos Boozer -> basketball.basketball_player.player_statistics -> m.04qht47 -> basketball.basketball_player_stats.season -> 2007\u201308 NBA season\n# Answer:\n2007\u201308 NBA season", "# Reasoning Path:\nCarlos Boozer -> base.schemastaging.athlete_extra.salary -> m.0j2hs7x -> base.schemastaging.athlete_salary.team -> Chicago Bulls\n# Answer:\nChicago Bulls", "# Reasoning Path:\nCarlos Boozer -> sports.pro_athlete.teams -> m.0115f60_ -> sports.sports_team_roster.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nCarlos Boozer -> sports.pro_athlete.teams -> m.0115f60_ -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["Utah Jazz", "Cleveland Cavaliers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.location.containedby -> Maryland\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Maryland\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.containedby -> United States of America\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.us_county.hud_county_place -> Frederick County / County Place 2402199999\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.location.containedby -> Maryland\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.us_county.hud_county_place -> Frederick County / County Place 2402199999\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.location.containedby -> Bethesda-Frederick-Gaithersburg, MD Metropolitan Division\n# Answer:\nFrederick County"], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-351", "prediction": ["# Reasoning Path:\nPortland Press Herald -> book.newspaper.circulation_areas -> Southern Maine Coast -> common.topic.image -> South Coast (Maine)\n# Answer:\nSouthern Maine Coast", "# Reasoning Path:\nPortland Press Herald -> book.newspaper.circulation_areas -> Portland -> location.location.nearby_airports -> Portland International Jetport\n# Answer:\nPortland", "# Reasoning Path:\nPortland Press Herald -> book.newspaper.owner -> Blethen Maine Newspapers, Inc. -> common.topic.notable_for -> g.1259lvkfc\n# Answer:\nBlethen Maine Newspapers, Inc.", "# Reasoning Path:\nPortland Press Herald -> book.newspaper.circulation_areas -> Southern Maine Coast -> common.topic.notable_types -> Location\n# Answer:\nSouthern Maine Coast", "# Reasoning Path:\nPortland Press Herald -> book.newspaper.circulation_areas -> Portland -> location.location.containedby -> United States of America\n# Answer:\nPortland", "# Reasoning Path:\nPortland Press Herald -> book.newspaper.owner -> Blethen Maine Newspapers, Inc. -> business.employer.employees -> m.0ywzm6d\n# Answer:\nBlethen Maine Newspapers, Inc.", "# Reasoning Path:\nPortland Press Herald -> common.topic.notable_types -> Newspaper -> freebase.type_hints.included_types -> Periodical\n# Answer:\nNewspaper", "# Reasoning Path:\nPortland Press Herald -> common.topic.notable_types -> Newspaper -> freebase.type_hints.included_types -> Topic\n# Answer:\nNewspaper", "# Reasoning Path:\nPortland Press Herald -> common.topic.notable_types -> Newspaper -> type.type.properties -> Circulation\n# Answer:\nNewspaper", "# Reasoning Path:\nPortland Press Herald -> book.newspaper.circulation_areas -> Portland -> location.location.containedby -> Cumberland County\n# Answer:\nPortland"], "ground_truth": ["Blethen Maine Newspapers, Inc."], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-352", "prediction": ["# Reasoning Path:\nAlbert Speer -> architecture.architect.structures_designed -> Reich Chancellery -> location.location.containedby -> Berlin\n# Answer:\nReich Chancellery", "# Reasoning Path:\nAlbert Speer -> architecture.architect.structures_designed -> Reich Chancellery -> architecture.structure.architectural_style -> Nazi architecture\n# Answer:\nReich Chancellery", "# Reasoning Path:\nAlbert Speer -> architecture.architect.structures_designed -> Olympiastadion -> architecture.structure.architect -> Werner March\n# Answer:\nOlympiastadion", "# Reasoning Path:\nAlbert Speer -> people.person.parents -> Albert Friedrich Speer -> common.topic.notable_types -> Deceased Person\n# Answer:\nAlbert Friedrich Speer", "# Reasoning Path:\nAlbert Speer -> people.person.children -> Albert Speer, Jr. -> architecture.architect.structures_designed -> Saoud bin Abdulrahman Stadium\n# Answer:\nAlbert Speer, Jr.", "# Reasoning Path:\nAlbert Speer -> people.person.parents -> Albert Friedrich Speer -> freebase.valuenotation.has_value -> Parents\n# Answer:\nAlbert Friedrich Speer", "# Reasoning Path:\nAlbert Speer -> architecture.architect.structures_designed -> Olympiastadion -> common.topic.image -> 2009-04-07 Berlin Olympiastadion 539\n# Answer:\nOlympiastadion", "# Reasoning Path:\nAlbert Speer -> people.person.parents -> Albert Friedrich Speer -> people.person.gender -> Male\n# Answer:\nAlbert Friedrich Speer", "# Reasoning Path:\nAlbert Speer -> people.person.children -> Albert Speer, Jr. -> people.person.place_of_birth -> Berlin\n# Answer:\nAlbert Speer, Jr.", "# Reasoning Path:\nAlbert Speer -> people.person.parents -> Luise Speer -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nLuise Speer"], "ground_truth": ["Deutsches Stadion", "Volkshalle", "Reich Chancellery", "Olympiastadion"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.4, "ans_recall": 0.5, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.4, "path_ans_recall": 0.5}
{"id": "WebQTest-354", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> High Renaissance -> visual_art.art_period_movement.associated_artists -> Titian\n# Answer:\nHigh Renaissance", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> High Renaissance -> base.schemastaging.context_name.pronunciation -> g.125_krwf6\n# Answer:\nHigh Renaissance", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> High Renaissance -> visual_art.art_period_movement.associated_artists -> Albrecht D\u00fcrer\n# Answer:\nHigh Renaissance", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> High Renaissance -> visual_art.art_period_movement.associated_artworks -> Young Man with an Apple\n# Answer:\nHigh Renaissance", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> Pastel Art -> visual_art.art_period_movement.associated_artists -> Rosalba Carriera\n# Answer:\nPastel Art", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> Pastel Art -> visual_art.art_period_movement.associated_artworks -> Young Girl Holding a Crown of Laurel\n# Answer:\nPastel Art", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> High Renaissance -> visual_art.art_period_movement.associated_artists -> Andrea del Sarto\n# Answer:\nHigh Renaissance", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> Pastel Art -> common.topic.notable_for -> g.11b75r8htr\n# Answer:\nPastel Art", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Raphael and his age\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> High Renaissance -> visual_art.art_period_movement.associated_artworks -> Benois Madonna\n# Answer:\nHigh Renaissance"], "ground_truth": ["Pastel Art", "High Renaissance"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-356", "prediction": ["# Reasoning Path:\nArlington -> location.location.nearby_airports -> Arlington Municipal Airport -> aviation.airport.serves -> TownePlace Suites Arlington Near Six Flags\n# Answer:\nArlington Municipal Airport", "# Reasoning Path:\nArlington -> location.location.nearby_airports -> Arlington Municipal Airport -> location.location.containedby -> Tarrant County\n# Answer:\nArlington Municipal Airport", "# Reasoning Path:\nArlington -> location.location.nearby_airports -> Arlington Municipal Airport -> common.topic.article -> m.02pq2dc\n# Answer:\nArlington Municipal Airport", "# Reasoning Path:\nArlington -> location.statistical_region.population -> g.11b66bywm8\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nArlington -> location.location.nearby_airports -> Arlington Municipal Airport -> location.location.containedby -> 76018\n# Answer:\nArlington Municipal Airport", "# Reasoning Path:\nArlington -> location.statistical_region.population -> g.11b7td7_9_\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nArlington -> location.statistical_region.population -> g.11b7tm_xv7\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nArlington -> location.location.contains -> University of Texas at Arlington School of Architecture -> common.topic.webpage -> m.03ky4f6\n# Answer:\nUniversity of Texas at Arlington School of Architecture", "# Reasoning Path:\nArlington -> location.location.contains -> 22202 -> location.location.containedby -> Virginia\n# Answer:\n22202", "# Reasoning Path:\nArlington -> location.location.contains -> University of Texas at Arlington School of Architecture -> common.topic.notable_types -> Organization\n# Answer:\nUniversity of Texas at Arlington School of Architecture"], "ground_truth": ["Arlington Municipal Airport"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-359", "prediction": ["# Reasoning Path:\nEmile Heskey -> soccer.football_player.statistics -> m.0w90221 -> soccer.football_player_stats.team -> Aston Villa F.C.\n# Answer:\nAston Villa F.C.", "# Reasoning Path:\nEmile Heskey -> sports.pro_athlete.teams -> m.05ckn9m -> sports.sports_team_roster.team -> Aston Villa F.C.\n# Answer:\nAston Villa F.C.", "# Reasoning Path:\nEmile Heskey -> base.schemastaging.athlete_extra.salary -> m.0kdmv43 -> base.schemastaging.athlete_salary.team -> Aston Villa F.C.\n# Answer:\nAston Villa F.C.", "# Reasoning Path:\nEmile Heskey -> soccer.football_player.statistics -> m.0w8wcgz -> soccer.football_player_stats.team -> Wigan Athletic F.C.\n# Answer:\nWigan Athletic F.C.", "# Reasoning Path:\nEmile Heskey -> soccer.football_player.statistics -> m.0w9mysy -> soccer.football_player_stats.team -> Newcastle Jets FC\n# Answer:\nNewcastle Jets FC", "# Reasoning Path:\nEmile Heskey -> sports.pro_athlete.teams -> m.05ckn9m -> sports.sports_team_roster.position -> Forward\n# Answer:\nForward", "# Reasoning Path:\nEmile Heskey -> sports.pro_athlete.teams -> m.04m0_rf -> sports.sports_team_roster.team -> Wigan Athletic F.C.\n# Answer:\nWigan Athletic F.C.", "# Reasoning Path:\nEmile Heskey -> sports.pro_athlete.teams -> m.0n3jm1r -> sports.sports_team_roster.team -> Newcastle Jets FC\n# Answer:\nNewcastle Jets FC", "# Reasoning Path:\nEmile Heskey -> base.schemastaging.athlete_extra.salary -> m.0kdmv43 -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nPound sterling", "# Reasoning Path:\nEmile Heskey -> base.schemastaging.athlete_extra.salary -> m.0n4ph37 -> base.schemastaging.athlete_salary.team -> Newcastle Jets FC\n# Answer:\nNewcastle Jets FC"], "ground_truth": ["Aston Villa F.C.", "Newcastle Jets FC"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7058823529411764, "path_precision": 0.6, "path_recall": 0.8571428571428571, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nHuntingdon College", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> University of Alabama School of Law\n# Answer:\nUniversity of Alabama School of Law", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138 -> education.education.institution -> University of Alabama\n# Answer:\nUniversity of Alabama", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.place_of_birth -> Los Angeles\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.gender -> Female\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_types -> Book Edition\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_for -> g.125920htw\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nHarper Seven Beckham"], "ground_truth": ["Monroe County High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-360", "prediction": ["# Reasoning Path:\nHirohito -> people.person.place_of_birth -> Aoyama, Minato, Tokyo -> location.location.containedby -> Tokyo\n# Answer:\nAoyama, Minato, Tokyo", "# Reasoning Path:\nHirohito -> people.person.place_of_birth -> Aoyama, Minato, Tokyo -> location.neighborhood.neighborhood_of -> Tokyo\n# Answer:\nAoyama, Minato, Tokyo", "# Reasoning Path:\nHirohito -> people.person.place_of_birth -> Aoyama, Minato, Tokyo -> location.location.people_born_here -> Junichi Sasai\n# Answer:\nAoyama, Minato, Tokyo", "# Reasoning Path:\nHirohito -> people.person.spouse_s -> m.0j4k61w -> people.marriage.spouse -> Empress K\u014djun\n# Answer:\nEmpress K\u014djun", "# Reasoning Path:\nHirohito -> people.person.place_of_birth -> Aoyama, Minato, Tokyo -> location.location.people_born_here -> Kenichi Enomoto\n# Answer:\nAoyama, Minato, Tokyo", "# Reasoning Path:\nHirohito -> people.person.sibling_s -> m.0k6ly3v -> people.sibling_relationship.sibling -> Takahito, Prince Mikasa\n# Answer:\nTakahito, Prince Mikasa", "# Reasoning Path:\nHirohito -> people.person.place_of_birth -> Aoyama, Minato, Tokyo -> location.location.people_born_here -> Seinosuke Nakajima\n# Answer:\nAoyama, Minato, Tokyo", "# Reasoning Path:\nHirohito -> people.person.spouse_s -> m.0j4k61w -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nHirohito -> people.person.sibling_s -> m.0k6lh0d -> people.sibling_relationship.sibling -> Nobuhito, Prince Takamatsu\n# Answer:\nNobuhito, Prince Takamatsu", "# Reasoning Path:\nHirohito -> people.person.spouse_s -> m.0j4k61w -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony"], "ground_truth": ["Aoyama, Minato, Tokyo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-361", "prediction": ["# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Sigmund Freud Museum -> common.topic.notable_for -> g.1255jtr81\n# Answer:\nSigmund Freud Museum", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Sigmund Freud Museum -> location.location.geolocation -> m.01225mmz\n# Answer:\nSigmund Freud Museum", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Sch\u00f6nbrunn Palace -> film.film_location.featured_in_films -> Ecstasy\n# Answer:\nSch\u00f6nbrunn Palace", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Sch\u00f6nbrunn Palace -> location.location.geolocation -> m.02_s799\n# Answer:\nSch\u00f6nbrunn Palace", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> St. Stephen's Cathedral, Vienna -> common.topic.webpage -> m.03l8gr3\n# Answer:\nSt. Stephen's Cathedral, Vienna", "# Reasoning Path:\nVienna -> government.governmental_jurisdiction.governing_officials -> m.09c1mpv -> government.government_position_held.office_position_or_title -> Reich Governor\n# Answer:\nReich Governor", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> St. Stephen's Cathedral, Vienna -> location.location.contains -> Ducal Crypt, Vienna\n# Answer:\nSt. Stephen's Cathedral, Vienna", "# Reasoning Path:\nVienna -> government.governmental_jurisdiction.governing_officials -> m.09c1mpv -> government.government_position_held.appointed_by -> Adolf Hitler\n# Answer:\nAdolf Hitler", "# Reasoning Path:\nVienna -> government.governmental_jurisdiction.governing_officials -> m.09c1mpv -> government.government_position_held.office_holder -> Baldur von Schirach\n# Answer:\nBaldur von Schirach", "# Reasoning Path:\nVienna -> government.governmental_jurisdiction.governing_officials -> m.0ng_dh_ -> government.government_position_held.office_position_or_title -> Vice Mayor of Vienna\n# Answer:\nVice Mayor of Vienna"], "ground_truth": ["St. Stephen's Cathedral, Vienna", "Liechtenstein Museum", "St. Peter's Church", "Karlskirche, Vienna", "Haus der Musik", "Belvedere, Vienna", "Imperial Crypt", "Naturhistorisches Museum", "Sch\u00f6nbrunn Palace", "Schloss Hof", "Capuchin Church, Vienna", "Augustinian Church, Vienna", "Jesuit Church, Vienna", "Rathaus, Vienna", "Prater", "Millennium Tower", "Vienna Observatory", "Kunsthistorisches Museum", "Hofburg Palace", "Minoritenkirche", "Sigmund Freud Museum", "Leopold Museum", "Albertina", "Vienna State Opera", "Schottenkirche, Vienna", "Vienna International Centre", "Rock im Park", "mumok", "Museum of Military History, Vienna"], "ans_acc": 0.10344827586206896, "ans_hit": 1, "ans_f1": 0.17647058823529413, "ans_precission": 0.6, "ans_recall": 0.10344827586206896, "path_f1": 0.18181818181818182, "path_precision": 0.6, "path_recall": 0.10714285714285714, "path_ans_f1": 0.17647058823529413, "path_ans_precision": 0.6, "path_ans_recall": 0.10344827586206896}
{"id": "WebQTest-362", "prediction": ["# Reasoning Path:\nIreland -> location.location.events -> 1999 Cricket World Cup -> time.event.locations -> England\n# Answer:\n1999 Cricket World Cup", "# Reasoning Path:\nIreland -> location.location.events -> 1999 Cricket World Cup -> time.event.next_in_series -> 2003 Cricket World Cup\n# Answer:\n1999 Cricket World Cup", "# Reasoning Path:\nIreland -> location.location.events -> 1999 Cricket World Cup -> common.topic.image -> Cricket World Cup trophy 2\n# Answer:\n1999 Cricket World Cup", "# Reasoning Path:\nIreland -> location.location.events -> 1999 Cricket World Cup -> time.event.locations -> Netherlands\n# Answer:\n1999 Cricket World Cup", "# Reasoning Path:\nIreland -> location.location.events -> 2008 Nicky Rackard Cup -> common.topic.article -> m.03hgw2x\n# Answer:\n2008 Nicky Rackard Cup", "# Reasoning Path:\nIreland -> location.location.events -> 1999 Cricket World Cup -> time.event.locations -> Scotland\n# Answer:\n1999 Cricket World Cup", "# Reasoning Path:\nIreland -> location.location.events -> Anglo-Spanish War -> time.event.includes_event -> Action of Faial\n# Answer:\nAnglo-Spanish War", "# Reasoning Path:\nIreland -> location.location.events -> 2008 Nicky Rackard Cup -> common.topic.notable_for -> g.1ylqcrxvw\n# Answer:\n2008 Nicky Rackard Cup", "# Reasoning Path:\nIreland -> location.location.events -> Anglo-Spanish War -> time.event.includes_event -> Battle of Berlengas Islands\n# Answer:\nAnglo-Spanish War", "# Reasoning Path:\nIreland -> organization.organization_scope.organizations_with_this_scope -> Amalgamated Society of Carpenters and Joiners -> organization.organization.geographic_scope -> United States of America\n# Answer:\nAmalgamated Society of Carpenters and Joiners"], "ground_truth": ["Anglo-Spanish War", "Battle of the Curragh", "Guerrilla phase of the Irish Civil War", "Battle of Ros-Mhic-Thri\u00fain", "Jacobite risings", "Battle of Belahoe", "Battle of Affane", "Irish Rebellion of 1641", "Sack of Dun Gallimhe", "Battle of Glentaisie", "Planned French invasion of Britain", "Second Desmond Rebellion", "Battle of Tochar Cruachain-Bri-Ele", "Irish Rebellion of 1798", "Exp\u00e9dition d'Irlande", "Cromwellian conquest of Ireland", "Norman invasion of Ireland", "Nine Years' War", "Irish War of Independence", "Bruce campaign in Ireland", "Siege of Waterford", "Siege of Smerwick", "Battle of Knockdoe", "Desmond Rebellions", "Williamite War in Ireland", "Siege of Drogheda", "Irish Civil War", "Irish Free State offensive", "Wars of the Three Kingdoms", "Siege of Wexford", "Irish Confederate Wars"], "ans_acc": 0.03225806451612903, "ans_hit": 1, "ans_f1": 0.05555555555555555, "ans_precission": 0.2, "ans_recall": 0.03225806451612903, "path_f1": 0.05405405405405406, "path_precision": 0.2, "path_recall": 0.03125, "path_ans_f1": 0.05555555555555555, "path_ans_precision": 0.2, "path_ans_recall": 0.03225806451612903}
{"id": "WebQTest-365", "prediction": ["# Reasoning Path:\nPeoria -> travel.travel_destination.tourist_attractions -> Peoria Zoo -> common.topic.webpage -> m.0cv3kq0\n# Answer:\nPeoria Zoo", "# Reasoning Path:\nPeoria -> travel.travel_destination.tourist_attractions -> Wildlife Prairie State Park -> location.location.geolocation -> m.0cq_w11\n# Answer:\nWildlife Prairie State Park", "# Reasoning Path:\nPeoria -> location.statistical_region.population -> g.11b66dqm1b\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nPeoria -> travel.travel_destination.tourist_attractions -> Peoria Zoo -> zoos.zoo.exhibits -> Africa Exhibit\n# Answer:\nPeoria Zoo", "# Reasoning Path:\nPeoria -> travel.travel_destination.tourist_attractions -> Wildlife Prairie State Park -> common.topic.notable_for -> g.125f33mst\n# Answer:\nWildlife Prairie State Park", "# Reasoning Path:\nPeoria -> travel.travel_destination.tourist_attractions -> Dozer Park -> common.topic.notable_types -> Sports Facility\n# Answer:\nDozer Park", "# Reasoning Path:\nPeoria -> location.location.contains -> John C. Flanagan House Museum -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.05h82bq\n# Answer:\nJohn C. Flanagan House Museum", "# Reasoning Path:\nPeoria -> location.location.contains -> Peoria Civic Center -> location.location.containedby -> United States of America\n# Answer:\nPeoria Civic Center", "# Reasoning Path:\nPeoria -> travel.travel_destination.tourist_attractions -> Dozer Park -> location.location.containedby -> Peoria County\n# Answer:\nDozer Park", "# Reasoning Path:\nPeoria -> location.location.contains -> John C. Flanagan House Museum -> location.location.containedby -> Illinois\n# Answer:\nJohn C. Flanagan House Museum"], "ground_truth": ["Heart of Illinois Fair", "Peoria Civic Center", "John C. Flanagan House Museum", "Wildlife Prairie State Park", "Dozer Park", "Par-A-Dice Hotel and Casino", "Peoria Zoo", "WeaverRidge Golf Club", "Lakeview Museum of Arts and Sciences", "George L. Luthy Memorial Botanical Garden"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6428571428571429, "ans_precission": 0.9, "ans_recall": 0.5, "path_f1": 0.37499999999999994, "path_precision": 0.5, "path_recall": 0.3, "path_ans_f1": 0.6428571428571429, "path_ans_precision": 0.9, "path_ans_recall": 0.5}
{"id": "WebQTest-366", "prediction": ["# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Cornish Language -> language.human_language.countries_spoken_in -> England\n# Answer:\nCornish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> British Sign Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nBritish Sign Language", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nChild pornography", "# Reasoning Path:\nUnited Kingdom -> location.statistical_region.long_term_unemployment_rate -> g.12cp_jvpx\n# Answer:\nlocation.statistical_region.long_term_unemployment_rate", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> Australia\n# Answer:\nEnglish Language"], "ground_truth": ["Esperanto Language", "Cornish Language", "Irish", "Welsh Language", "English Language", "Scots Language", "Scottish Gaelic", "British Sign Language"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.5106382978723405, "ans_precission": 0.8, "ans_recall": 0.375, "path_f1": 0.22222222222222224, "path_precision": 0.2, "path_recall": 0.25, "path_ans_f1": 0.5106382978723405, "path_ans_precision": 0.8, "path_ans_recall": 0.375}
{"id": "WebQTest-367", "prediction": ["# Reasoning Path:\nElvis Presley -> people.person.profession -> Musician -> freebase.equivalent_topic.equivalent_type -> Musical Artist\n# Answer:\nMusician", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Soldier\n# Answer:\nSoldier", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Musician -> people.profession.specializations -> Singer\n# Answer:\nMusician", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Singer -> common.topic.subject_of -> Alan Motley\n# Answer:\nSinger", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Musician -> common.topic.subject_of -> Stephen Melton\n# Answer:\nMusician", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Singer -> common.topic.subject_of -> Justin Bieber\n# Answer:\nSinger", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Singer -> people.profession.specializations -> Singer-songwriter\n# Answer:\nSinger", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Singer -> people.profession.specialization_of -> Musician\n# Answer:\nSinger", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Musician -> people.profession.specializations -> Singer-songwriter\n# Answer:\nMusician", "# Reasoning Path:\nElvis Presley -> common.topic.webpage -> MusicMoz page for Elvis Presley -> common.webpage.category -> Topic Webpage\n# Answer:\nMusicMoz page for Elvis Presley"], "ground_truth": ["Soldier", "Musician", "Actor", "Singer"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> book.written_work.subjects -> History\n# Answer:\nUtah, the right place", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> book.written_work.author -> Thomas G. Alexander\n# Answer:\nUtah, the right place", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> common.topic.notable_for -> g.125dtp7bg\n# Answer:\nUtah, the right place", "# Reasoning Path:\nUtah -> book.book_subject.works -> Bound for Canaan -> book.written_work.author -> Margaret Blair Young\n# Answer:\nBound for Canaan", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> government.government_position_held.office_position_or_title -> Lieutenant Governor of Utah\n# Answer:\nLieutenant Governor of Utah", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010l3l5d -> freebase.valuenotation.has_no_value -> Appointed By (if Position is Appointed)\n# Answer:\nAppointed By (if Position is Appointed)"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-370", "prediction": ["# Reasoning Path:\nNetherlands -> location.country.languages_spoken -> Frisian languages -> language.human_language.language_family -> West Germanic languages\n# Answer:\nFrisian languages", "# Reasoning Path:\nNetherlands -> location.country.languages_spoken -> Frisian languages -> language.human_language.region -> Friesland\n# Answer:\nFrisian languages", "# Reasoning Path:\nNetherlands -> location.country.official_language -> Dutch Language -> language.human_language.dialects -> Frisian languages\n# Answer:\nDutch Language", "# Reasoning Path:\nNetherlands -> location.country.official_language -> Dutch Language -> language.human_language.main_country -> Cura\u00e7ao\n# Answer:\nDutch Language", "# Reasoning Path:\nNetherlands -> location.statistical_region.gni_in_ppp_dollars -> g.11b60prvm_\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nNetherlands -> location.country.languages_spoken -> Dutch Language -> language.human_language.dialects -> Frisian languages\n# Answer:\nDutch Language", "# Reasoning Path:\nNetherlands -> location.country.languages_spoken -> West Flemish -> language.human_language.countries_spoken_in -> France\n# Answer:\nWest Flemish", "# Reasoning Path:\nNetherlands -> location.country.official_language -> Dutch Language -> media_common.netflix_genre.titles -> Zus & Zo\n# Answer:\nDutch Language", "# Reasoning Path:\nNetherlands -> location.country.languages_spoken -> Frisian languages -> language.human_language.language_family -> Germanic languages\n# Answer:\nFrisian languages", "# Reasoning Path:\nNetherlands -> location.country.languages_spoken -> Dutch Language -> language.human_language.main_country -> Cura\u00e7ao\n# Answer:\nDutch Language"], "ground_truth": ["West Flemish", "Dutch Language", "Frisian languages"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-371", "prediction": ["# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Republic of Macedonia\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.main_country -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.region -> Southeast Europe\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Greek Language -> media_common.netflix_genre.titles -> A Girl in Black\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Greek Language -> language.human_language.region -> Europe\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> media_common.netflix_genre.titles -> A Girl in Black\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> language.human_language.region -> Europe\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Italy\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Greek Language -> media_common.netflix_genre.titles -> A Matter of Dignity\n# Answer:\nGreek Language"], "ground_truth": ["Greek Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-374", "prediction": ["# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Printmaking -> visual_art.visual_art_form.artworks -> Dam\n# Answer:\nPrintmaking", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Printmaking -> common.topic.notable_types -> Visual Art Form\n# Answer:\nPrintmaking", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Printmaking -> visual_art.visual_art_form.artworks -> Suite Noa Noa - Noa Noa\n# Answer:\nPrintmaking", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Printmaking -> education.field_of_study.students_majoring -> m.0_w593n\n# Answer:\nPrintmaking", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Eric Ravilious\n# Answer:\nDrawing", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Drawing -> business.consumer_product.brand -> Eyecandyair\n# Answer:\nDrawing", "# Reasoning Path:\nHenri Matisse -> book.book_subject.works -> Matisse -> common.topic.notable_for -> g.125dhx5l1\n# Answer:\nMatisse", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Collage -> visual_art.visual_art_form.artists -> Aberjhani\n# Answer:\nCollage", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Printmaking -> education.field_of_study.students_majoring -> m.0zsrykk\n# Answer:\nPrintmaking", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Henry Moore\n# Answer:\nDrawing"], "ground_truth": ["Painting", "Sculpture", "Collage", "Drawing", "Printmaking"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7200000000000001, "ans_precission": 0.9, "ans_recall": 0.6, "path_f1": 0.7200000000000001, "path_precision": 0.9, "path_recall": 0.6, "path_ans_f1": 0.7200000000000001, "path_ans_precision": 0.9, "path_ans_recall": 0.6}
{"id": "WebQTest-375", "prediction": ["# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> book.written_work.author -> Wilkie Collins\n# Answer:\nA House to Let", "# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> common.topic.notable_for -> g.1255hfpzv\n# Answer:\nA House to Let", "# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> common.topic.article -> m.0273w68\n# Answer:\nA House to Let", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> book.written_work.author -> Adelaide Anne Procter\n# Answer:\nA House to Let", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> common.topic.notable_types -> Film character\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> book.written_work.author -> Elizabeth Gaskell\n# Answer:\nA House to Let", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Abel Magwitch -> book.book_character.appears_in_book -> Great Expectations\n# Answer:\nAbel Magwitch", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced_by -> H. G. Wells\n# Answer:\nAldous Huxley", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nGhost of Christmas Present"], "ground_truth": ["Oliver Twist"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-376", "prediction": ["# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Evil Under the Sun -> film.film.prequel -> Death on the Nile\n# Answer:\nEvil Under the Sun", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Agatha Christie's Miss Marple: 4:50 from Paddington -> film.film.runtime -> m.0ghfzm6\n# Answer:\nAgatha Christie's Miss Marple: 4:50 from Paddington", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Evil Under the Sun -> film.film.language -> French\n# Answer:\nEvil Under the Sun", "# Reasoning Path:\nAgatha Christie -> common.topic.webpage -> m.03lzz8x -> common.webpage.resource -> m.0bl41lc\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Agatha Christie's Miss Marple: 4:50 from Paddington -> film.film.starring -> m.0nd9m31\n# Answer:\nAgatha Christie's Miss Marple: 4:50 from Paddington", "# Reasoning Path:\nAgatha Christie -> book.book_subject.works -> Agatha Christie and the Eleven Missing Days -> book.book.editions -> Agatha Christie and the eleven missing days\n# Answer:\nAgatha Christie and the Eleven Missing Days", "# Reasoning Path:\nAgatha Christie -> book.book_subject.works -> The Getaway Guide to Agatha Christie's England -> book.written_work.subjects -> England\n# Answer:\nThe Getaway Guide to Agatha Christie's England", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Agatha Christie's Miss Marple: 4:50 from Paddington -> film.film.runtime -> m.0nd953j\n# Answer:\nAgatha Christie's Miss Marple: 4:50 from Paddington", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Evil Under the Sun -> film.film.language -> English Language\n# Answer:\nEvil Under the Sun", "# Reasoning Path:\nAgatha Christie -> common.topic.webpage -> m.09y5yvx -> common.webpage.resource -> Bits and Bobs (Vol. 12): Six essential British whodunits\n# Answer:\nBits and Bobs (Vol. 12): Six essential British whodunits"], "ground_truth": ["The secret of Chimneys", "One, Two, Buckle My Shoe (Poirot)", "A murder is announced", "The Golden Ball and Other Stories (G. K. Hall (Large Print))", "By the Pricking of My Thumbs (Tommy & Tuppence Chronology)", "The Sittaford mystery", "Curtain", "The Secret Adversary (G K Hall's Agatha Christie Series)", "They Came to Baghdad (Agatha Christie Collection)", "Pale Horse", "Thirteen Problems", "Cat Among the Pigeons (Ulverscroft Mystery)", "The Mysterious Affair At Styles (Classic Books on Cassettes Collection) (Classic Books on Cassettes Collection)", "Sparkling Cyanide", "4.50 From Paddington", "The Mysterious Affair at Styles, Large-Print Edition", "The Golden Ball and Other Stories", "Curtain (Poirot)", "The mysterious Mr. Quin.", "Passenger to Frankfurt", "The Murder of Roger Ackroyd (Poirot)", "A Pocket Full of Rye (BBC Audio Crime)", "A Pocket Full of Rye (Miss Marple Mysteries (Paperback))", "Sad cypress", "The murder at the vicarage", "Burden", "The Rose and the Yew Tree", "A Murder Is Announced (Miss Marple Mysteries)", "One, Two, Buckle My Shoe", "The Sittaford Mystery (BBC Radio Collection)", "Murder On The Links", "Death on the Nile (Hercule Poirot Mysteries (Audio))", "At Bertram's Hotel", "Curtain (The Agatha Christie mystery collection)", "The Secret of Chimneys (Ulverscroft Mystery)", "Appointment With Death (G K Hall Large Print Book Series)", "Pale horse", "Elephants can remember.", "The Body in the Library (Miss Marple Mysteries)", "Murder On the Links", "Dead man's folly", "Destination Unknown (Agatha Christie Collection)", "Cat among the pigeons.", "Crooked House (Winterbrook Edition)", "Partners in Crime (Listen for Pleasure)", "Peril at End House (Hercule Poirot Mysteries)", "Curtain (G. K. Hall (Large Print))", "The Seven Dials Mystery", "The Man In The Brown Suit", "A Murder is Announced (Miss Marple Mysteries)", "The secret of chimneys", "A Caribbean Mystery (Miss Marple)", "Postern of fate", "The Hound of Death (Agatha Christie Collection)", "A Pocket Full of Rye (Miss Marple)", "Poems", "The Hollow", "By the Pricking of My Thumbs", "The A.B.C. murders", "The Man in the Brown Suit", "Problem at Pollensa Bay and 7 Other Mysteries", "Murder in the Mews", "Destination Unknown (Signature Editions)", "And Then There Were None (Mystery Masters)", "The Mystery of the Blue Train (Poirot)", "Evil Under the Sun (Poirot)", "Hallowe'en party", "They do it with Mirrors", "Sleeping murder.", "Murder in Mesopotamia", "The Body In The Library (A Miss Marple Mystery)", "Sad Cypress", "The Secret Adversary (Tommy & Tuppence Chronology)", "Cat Among the Pigeons (Poirot)", "Death Comes As the End (The Agatha Christie Mystery Collection)", "Three ACT Tragedy", "Dumb Witness", "Hercule Poirot\u2019s Christmas", "The Pale Horse (Agatha Christie Collection)", "Hickory Dickory Dock (Agatha Christie Collection)", "Unfinished Portrait (Westmacott)", "Five Little Pigs", "The Mysterious Mr.Quin (Agatha Christie Signature Edition)", "A Murder Is Announced (Miss Marple)", "The Murder at the Vicarage (BBC Full Cast Dramatization)", "Evil Under the Sun (Agatha Christie Audio Mystery)", "The Pale Horse (The Agatha Christie mystery collection)", "A Murder Is Announced (Agatha Christie Collection)", "Towards zero", "The mysterious affair at Styles", "Appointment With Death", "The Body in the Library (BBC Radio Collection)", "Passenger to Frankfurt.", "Murder on the Orient Express", "Evil Under the Sun (Hercule Poirot Mysteries)", "They Do It With Mirrors (Agatha Christie Audio Mystery)", "A Pocket Full of Rye (Agatha Christie Collection)", "Murder at the Vicarage (Miss Marple Mysteries (Paperback))", "Destination Unknown (St. Martin's Minotaur Mysteries)", "The labours of Hercules", "Sittaford Mystery (St. Martin's Minotaur Mysteries)", "A Murder Is Announced (G. K. Hall (Large Print))", "A Caribbean Mystery (Miss Marple Mysteries)", "A Pocket Full of Rye", "Murder in the mews.", "Hickory Dickory Dock (Poirot)", "The Clocks (Agatha Christie Collection)", "The Under Dog and Other Stories", "A Caribbean Mystery (Greenway E.)", "Elephants Can Remember (Poirot)", "Elephants Can Remember", "A Pocket Full of Rye (A Jane Marple Murder Mystery)", "The Sittaford Mystery (BBC Radio Presents)", "The Sittaford Mystery (Agatha Christie Mystery Collection)", "The Clocks (A Hercule Poirot Murder Mystery)", "One, Two Buckle My Shoe (BBC Radio Collection)", "Come, tell me how you live", "They Came to Baghdad (The Agatha Christie Mystery Collection)", "Murder in Mesopotamia (Hercule Poirot Mysteries)", "CROOKED HOUSE", "They Came to Baghdad (Dell Book)", "Cat Among the Pigeons ( A Hercule Poirot Mystery)", "The Unexpected Guest (BBC Radio Collection)", "By the Pricking of my Thumbs", "Body in the library", "Curtain (Ulverscroft Large Print Ser.)", "The seven dials mystery", "The Man in the Brown Suit (Agatha Christie Mysteries Collection)", "They Do It With Mirrors", "Destination Unknown (Mystery Masters Series)", "4.50 from Paddington (Agatha Christie Collection)", "The Adventure of the Christmas Pudding and The Mystery of the Spanish Chest", "Elephants Can Remember (The Christie Collection)", "The murder at the vicarage.", "And Then There Were None (Agatha Christie Audio Mystery)", "Death in the Clouds (Agatha Christie Collection)", "The Big Four.", "The Murder of Roger Ackroyd (Hercule Poirot Mysteries)", "Witness for the Prosecution and Other Stories (G. K. Hall (Large Print))", "After the Funeral (The Christie Collection)", "Crooked House", "4.50 from Paddington (Miss Marple)", "Cards on the table", "4.50 from Paddington (Agatha Christie Signature Edition)", "Murder on the Links (BBC Audio Crime)", "The adventure of the Christmas pudding", "Sparkling Cyanide (Variant Title = Remembered Death)", "Towards Zero (Agatha Christie Mysteries Collection (Paperback))", "The Clocks", "Postern of Fate (G K Hall Large Print Book Series)", "The Listerdale mystery", "The Murder on the Links (Hercule Poirot Mysteries)", "The Seven Dials Mystery (Agatha Christie Mystery Collection) Bantam Blue Leatherette Edition", "And Then There Were None (Ulverscroft Large Print)", "Sparkling Cyanide (Audio Editions Mystery Masters)", "Appointment With Death (Hercule Poirot Mysteries)", "Lord Edgware dies.", "Partners in Crime", "Mrs. McGinty's dead", "Sleeping Murder", "Third Girl (Poirot)", "Poirot Investigates (Hercule Poirot Mysteries (Paperback))", "Death In The Clouds", "Mystery of the Blue Train", "And Then There Were None", "Nemesis", "The Adventure of the Christmas Pudding (Poirot)", "The witness for the prosecution and other stories", "Murder in the Mews (Agatha Christie Signature Edition)", "And Then There Were None (St. Martin's True Crime Library)", "Third Girl (Agatha Christie Collection)", "Ordeal by Innocence (Signature Editions)", "Rose and the Yew Tree", "The A.B.C. Murders", "Endless Night (Ulverscroft Large Print)", "The under dog and other stories", "Peril At End House", "4.50 from Paddington (BBC Radio Collection)", "Towards Zero (The Agatha Christie Mystery Collection)", "Death in the clouds", "The Big Four (Poirot)", "The Labours of Hercules (Poirot)", "The Big Four", "Lord Edgware Dies (BBC Audio Crime)", "Death on the Nile (Agatha Christie Audio Mystery)", "Parker Pyne investigates", "Absent in the Spring", "A Caribbean Mystery (BBC Radio Presents: An Audio Dramatization)", "Poirot Investigates", "Clocks", "Five Little Pigs (Also published as Murder In Retrospect)", "The Witness for the Prosecution and Other Stories (Mystery Masters Series)", "Cat Among the Pigeons", "The Murder of Roger Ackroyd. (Lernmaterialien)", "The Adventure of the Christmas Pudding (The Crime Club)", "Parker Pyne Investigates", "Clocks (Ulverscroft Large Print)", "Murder in the mews", "The Labours of Hercules (Hercule Poirot Mysteries)", "Murder of Roger Ackroyd", "Secret Adversary (The Agatha Christie Mystery Collection)", "Double Sin and Other Stories", "They Came to Baghdad (G. K. Hall (Large Print))", "Nemesis (BBC Radio Collection)", "Crooked house.", "Star over Bethlehem and other stories by Agatha Christie Mallowan", "The adventure of the Christmas pudding and a selection of entre\u0301es.", "Five Little Pigs (Poirot)", "Sleeping Murder (Large Print Ed)", "Man in the Brown Suit (St. Martin's Minotaur Mysteries)", "The hound of death", "Peril at End House", "Peril at end house.", "The hollow", "Seven Dials Mystery", "The Hollow (Hercule Poirot)", "Endless Night", "Murder Is Easy (St. Martin's Minotaur Mysteries)", "Elephants Can Remember (The Agatha Christie Mystery Collection)", "Lord Edgware Dies (Agatha Christie Collection S.)", "The Hollow (A Hercule Poirot Novel)", "One, two, buckle my shoe", "Sittaford Mystery", "Five little pigs", "Murder in Mesopotamia (BBC Radio Presents)", "Lord Edgware Dies (Poirot)", "Sad Cypress (Poirot)", "Death on the Nile (Hercule Poirot)", "Sleeping murder", "They Do It with Mirrors", "Murder in Mesopotamia (Hercule Poirot)", "Sleeping Murder (The Agatha Christie Mystery Collection)", "Witness for the prosecution, and other stories", "The Secret Adversary", "The Man In The Brown Suit (Agatha Christie Mystery Collection) Bantam Blue Leatherette Edition", "Taken at the Flood (Hercule Poirot)", "The Burden", "The murder of Roger Ackroyd", "Towards Zero (Audio Editions Mystery Masters)", "Murder is Easy (St. Martin's Minotaur Mysteries)", "Endless night", "Sad cypress.", "Mystery Of The Blue Train / The Listerdale Mystery / Murder At The Vicarage", "A pocket full of rye", "Dumb witness", "Crooked house", "Cards on the Table (Agatha Christie Audio Mystery)", "Nemesis (Cover to Cover Classics)", "Five Little Pigs (The Christie Collection)", "The Mysterious Affair at Styles (Large Print Edition)", "Murder at the Vicarage (BBC Radio Presents)", "The Man in the Brown Suit (Mystery Masters)", "Five Little Pigs (Hercule Poirot Mysteries)", "Absent in the Spring (Westmacott)", "Three blind mice and other stories", "Unexpected Guest", "By the Pricking of My Thumbs (Tommy and Tuppence Mysteries)", "The Mystery of the Blue Train (Hercule Poirot Mysteries)", "a Pocket full of Rye", "By The Pricking Of My Thumbs (Tommy and Tuppence Mysteries)", "The Pale Horse (St. Martin's Minotaur Mysteries)", "Third girl", "The Secret of Chimneys (Agatha Christie Signature Edition)", "The unexpected guest", "By the Pricking of My Thumbs (Tommy and Tuppence Mysteries (Paperback))", "Parker Pyne Investigates (Agatha Christie Collection)", "The Man in the Brown Suit (Agatha Christie Collection)", "Evil Under the Sun (BBC Radio Collection)", "Three Act Tragedy (BBC Radio Collection)", "Nemesis (Miss Marple Mysteries)", "The mysterious affair at Styles.", "And Then There Were None (The Christie Collection)", "Murder at the Vicarage (Agatha Christie Mysteries Collection)", "A Pocket Full of Rye (BBC Radio Collection: Crimes and Thrillers)", "Death Comes as the End (Agatha Christie Collection)", "After the Funeral (Hercule Poirot Mysteries)", "They Came to Baghdad", "PASSENGER TO FRANKFURT.", "Death On The Nile (Hercule Poirot Mysteries (Paperback))", "Seven Dials Mystery (Agatha Christie Audio Mystery)", "Towards zero.", "The Murder At the Vicarage", "A Pocket Full of Rye (Miss Marple Mysteries)", "Ordeal by Innocence", "Ordeal by Innocence (Agatha Christie Collection)", "Death Comes as the End", "The Big Four (Hercule Poirot Mysteries)", "The Secret Adversary (Dodo Press)", "Destination unknown (Agatha Christie mystery collection)", "Five Little Pigs (Agatha Christie Mysteries Collection)", "The Seven Dials Mystery (Agatha Christie Signature Edition)", "While the light lasts and other stories", "Come, Tell Me How You Live", "And Then There Were None (Agatha Christie Collection)", "Poirot investigates.", "The Listerdale mystery.", "Appointment with Death (BBC Radio Collection)", "The Pale Horse", "Secret Adversary", "Problem at Pollensa Bay & Other Stories", "And then there were none", "The mysterious Mr. Quin", "Sleeping Murder (Miss Marple)", "Murder is easy", "Partners in Crime (Tommy and Tuppence Mysteries)", "Murder in Mesopotamia.", "The Unexpected Guest (Acting Edition)", "Murder Is Easy", "The Hollow (Winterbrook Edition)", "Mysterious Affair at Styles/Cassettes (1362)", "Peril at End House.", "Sparkling Cyanide (Agatha Christie Signature Edition)", "Sparkling Cyanide (St. Martin's Minotaur Mysteries)", "Passenger to Frankfurt (Winterbrook Edition)", "Curtain (Hercule Poirot Mysteries)", "The Under Dog and Other Stories (G. K. Hall (Large Print))", "4.50 from Paddington (BBC Audio Crime)", "Murder at the Vicarage", "Hickory dickory dock", "A Caribbean Mystery (Audio Editions)", "They came to Baghdad.", "Come tell me how you live", "A CARIBBEAN MYSTERY", "The Unexpected Guest", "Murder in Mesopotamia (Poirot)", "The Mysterious Affair at Styles (Dodo Press)", "4.50 from Paddington (Miss Marple Mysteries (Audio))", "After the Funeral (Hercule Poirot)", "The secret adversary", "They Do It with Mirrors (Miss Marple Mysteries (Audio Partners))", "Murder in Mesopotamia (Agatha Christie Audio Mystery)", "The Murder on the Links (Hercule Poirot)", "After the Funeral", "Appointment with Death (Dell; 10246)", "The Secret of Chimneys (Mystery Masters Series)", "A Pocket Full Of Rye", "Three act tragedy", "The Murder of Roger Ackroyd (Agatha Christie Audio Mystery)", "The harlequin tea set and other stories", "Death Comes As the End", "Evil Under the Sun", "Murder at the Vicarage (G. K. Hall (Large Print))", "Sad Cypress (Hercule Poirot)", "After the Funeral (Hercule Poirot Mysteries (Paperback))", "CLOCKS", "The Murder of Roger Ackroyd (Hercule Poirot Mystery)", "Death on the Nile.", "The Sittaford Mystery (Agatha Christie Signature Edition)", "Peril at End House (BBC Radio Collection)", "Hickory dickory dock.", "The Mysterious Mr.Quin", "Man in the Brown Suit", "Cat Among the Pigeons (Hercule Poirot Mysteries)", "Towards Zero.", "The Murder on the Links", "Cards on the table (Agatha Christie mystery collection)", "Nemesis (Miss Marple)", "Death on the Nile (Hercule Poirot Mysteries)", "The Secret Adversary (Large Print Edition)", "Evil under the sun.", "Partners in Crime (Vol. 1 Finessing the King, Vol 2 The Crackler, Vol 3 The Unbreakable Alibi)", "By the pricking of my thumbs", "Third Girl", "Murder at the Vicarage (Agatha Christie Audio Mystery)", "Death in the Clouds (Hercule Poirot)", "Lord Edgware Dies (Hercule Poirot Mysteries)", "The Thirteen Problems (Miss Marple Mysteries)", "The pale horse.", "Murder on the Links", "Star over Bethlehem, and other stories (The Agatha Christie mystery collection)", "The crooked house", "Endless night.", "A Murder Is Announced", "Sparkling cyanide", "Elephants Can Remember (Hercule Poirot Mysteries (Audio))", "Double sin and other stories", "The murder on the links.", "The murder on the links", "Death comes as the end", "Sad Cypress (BBC Radio Collection)", "Partners in crime.", "The Labours of Hercules", "The Seven Dials mystery.", "The Body in the Library (BBC Radio Collection: Crimes and Thrillers)", "Murder at the vicarage.", "Sleeping Murder (Miss Marple Mysteries)", "The Sittaford Mystery", "Crooked House (Minotaur Mysteries)", "Taken at the Flood (Hercule Poirot Mysteries (Paperback))", "The Murder of Roger Ackroyd", "The Sittaford Mystery (Mystery Masters Series)", "Murder is easy.", "Murder at the Vicarage (Miss Marple Mysteries)", "The Sittaford Mystery (St. Martin's Minotaur Mysteries)", "The regatta mystery and other stories", "Peril at End House (Poirot)", "Poirot investigates", "Death on the Nile (The Christie Collection)", "Partners in Crime (Tommy & Tuppence Chronology)", "The Mysterious Mr. Quin (Ulverscroft Large Print)", "Partners in Crime (Agatha Christie Mysteries Collection)", "The big four", "And Then There Were None (The Agatha Christie Mystery Collection)", "Death in the Clouds (BBC Radio Collection)", "Murder on the links", "The Hollow.", "The Secret Adversary (Classic Books on Cassettes Collection)", "The Murder at the Vicarage (Miss Marple)", "The Sittaford Mystery (Agatha Christie Collection)", "The Big Four (G K Hall Large Print Book Series)", "The body in the library", "The thirteen problems", "Cards on the Table (Hercule Poirot)", "Murder Is Easy (Audio Editions Mystery Masters)", "Death Comes As the End (Mystery Masters)", "The Mysterious Mr Quin", "Evil under the sun", "Destination unknown", "They Came to Baghdad (Mystery Masters)", "Sparkling Cyanide (Mystery Masters Series)", "4.50 from Paddington.", "The Rose and the Yew Tree (Westmacott)", "The Hollow (Poirot)", "Elephants Can Remember (Agatha Christie Mysteries Collection)", "Three blind mice and other stories (Agatha Christie Mystery Collection)", "Poirot Investigates (Hercule Poirot Mysteries)", "The Hollow (Agatha Christie Collection)", "Crooked House (Agatha Christie Collection)", "The \\\"Mysterious Mr Quin\\\"", "Taken at the flood", "The Secret Adversary (Agatha Christie Audio Mystery)", "The golden ball and other stories", "Five little pigs.", "Ordeal by innocence", "Evil Under the Sun (G. K. Hall (Large Print))", "A Caribbean Mystery", "The Body in the Library", "Cards on the Table", "The Mysterious Affair at Styles", "Cards on the Table (Mystery Masters)", "And Then There Were None (Audio Editions Mystery Masters)", "A Murder Is Announced (Miss Marple Mysteries (Paperback))", "The Seven Dials mystery", "Cat among the pigeons", "Taken at the Flood", "The Hound of Death", "After the Funeral (Agatha Christie Collection)", "Appointment with Death (The Christie Collection)", "Sad Cypress (Hercule Poirot Mysteries)", "Problem At Pollensa Bay and Seven Other Mysteries", "The Murder at the Vicarage (Miss Marple Mysteries)", "Appointment with death", "A Murder Is Announced (BBC Radio Collection)", "Mystery of the blue train", "Clocks (Hc Collection)", "Towards Zero (St. Martin's Minotaur Mysteries)", "The Body in the Library (Miss Marple)", "Cards on the Table (Hercule Poirot Mysteries)", "Lord Edgware Dies (BBC Radio Collection)", "Hickory Dickory Dock", "Pale Horse (R)", "Third Girl (Hercule Poirot Mysteries (Paperback))", "Five little pigs (Agatha Christie Mystery Collection)", "The Murder at the Vicarage (Acting Edition)", "Three Act Tragedy", "Postern of Fate (Agatha Christie Audio Mystery)", "Elephants can remember", "A Caribbean Mystery (BBC Radio Collection)", "POIROT INVESTIGATES (Hercule Poirot Mysteries (Paperback))", "Murder on the links (The Agatha Christie mystery collection)", "Lord Edgware dies", "Murder Is Easy (Agatha Christie Collection)", "4.50 from Paddington", "Big Four", "The adventure of the Christmas pudding, and a selection of entr\u00e9es", "Poems (The Agatha Christie mystery collection)", "Mysterious Affair at Styles", "Death in the Clouds (Agatha Christie Audio Mystery)", "CURTAIN", "Appointment with Death (Hercule Poirot Mysteries)", "They came to Baghdad", "Taken at the Flood (Poirot)", "Postern of Fate (Tommy & Tuppence Chronology)", "Partners in Crime (The Agatha Christie Mystery Collection)", "A Caribbean Mystery (G. K. Hall (Large Print))", "Death on the Nile", "One, Two, Buckle My Shoe (Agatha Christie Mysteries Collection (Paperback))", "Poirot Investigates (G. K. Hall (Large Print))", "Cat among the Pigeons", "The Secret of Chimneys", "Murder at the Vicarage (Miss Marple Mystery Series)", "Cards on the Table (A Hercule Poirot Mystery)", "The Mysterious Mr. Quin (Paperback)) (Hercule Poirot Mysteries (Paperback))", "Big Four (Ulverscroft Large Print)", "After The Funeral", "Elephants Can Remember (Hercule Poirot Mysteries)", "The mystery of the blue train", "The Man In The Brown Suit (Classic Books on Cassettes Collection)", "The mysterious Mr Quin", "Nemesis (G. K. Hall (Large Print))", "Taken at the Flood (Radio Collection)", "Secret of Chimneys (St. Martin's Minotaur Mysteries)", "The Regatta Mystery", "Murder In Mesopotamia (Agatha Christie Mystery Collection) Bantam Blue Leatherette Edition", "Poirot Investigates (Hercule Poirot Mysteries (Audio))", "The Man in the Brown Suit (St. Martin's Minotaur Mysteries)", "THE MYSTERY OF THE BLUE TRAIN.", "Poirot Investigates (Poirot)", "Third Girl (Hercule Poirot Mysteries)", "Appointment with Death", "Secret of Chimneys", "The Hollow (Ulverscroft Large Print)", "Death in the Clouds", "Murder in the Mews (Poirot)", "Sparkling cyanide.", "Labours of Hercules", "They Do It With Mirrors (Miss Marple Mysteries)", "The Thirteen Problems", "Appointment with Death (Hercule Poirot)", "Man in the Brown Suit (Ulverscroft Mystery)", "A Pocket Full of Rye (Miss Marple Mysteries (Audio))", "Partners in crime", "Crooked House (Agatha Christie Audio Mystery)", "Three Act Tragedy (Hercule Poirot Mysteries)", "Problem at Pollensa Bay", "Taken at the Flood (Ulverscroft Large Print)", "The secret of chimneys.", "Double Sin and other stories", "After the funeral.", "Towards Zero", "Death in the Clouds (Agatha Christie Mysteries Collection (Paperback))", "Golden Ball and Other Stories (Agatha Christie Mysteries Collection)", "Murder in Mesopotamia (BBC Radio Collection)", "The murder of Roger Ackroyd.", "Ordeal By Innocence", "The Mysterious Mr. Quin", "Death on the Nile (Hercule Poirot Mysteries (Paperback))", "The Adventure of the Christmas Pudding and Other Stories", "The Big Four (Agatha Christie Collection)", "The pale horse", "Cards on the Table (BBC Radio Collection)", "Cat Among the Pigeons (Hercule Poirot Mysteries (Paperback))", "Five Little Pigs.", "Passenger to Frankfurt (The Christie Collection)", "Body in the Library (Miss Marple Mysteries (Paperback))", "Taken at the Flood (Agatha Christie Collection)", "Ordeal by Innocence (Agatha Christie Mysteries Collection)", "Evil under the Sun (Hercule Poirot Mysteries)", "Destination Unknown", "Murder in the Mews (Ulverscroft Large Print)", "Taken At The Flood (A Hercule Poirot Mystery)", "Dumb Witness (Poirot)", "Dumb witness.", "Appointment with death.", "Secret Adversary (Unabridged Classics)", "And Then There Were None Book (Detective English Readers)", "Unfinished Portrait", "A Murder is Announced", "The Mystery of the Blue Train", "They Do It with Mirrors (Miss Marple)", "One, Two, Buckle My Shoe (Ulverscroft Mystery)", "Taken at the Flood (aka There is a Tide...)", "The Clocks (Poirot)", "Postern of Fate", "The A. B. C. Murders", "Partners in Crime (Agatha Christie Collection)", "They do it with mirrors", "Come, Tell Me How You Live (Common Reader Editions: Rediscoveries: LONDON)", "The Clocks (Hercule Poirot Mysteries)", "The Listerdale Mystery", "Death on the Nile (BBC Radio Collection)", "The Mysterious Affair at Styles (Hercule Poirot Mysteries (Audio))", "Evil Under the Sun (Agatha Christie Collection)", "The Listerdale Mystery (Agatha Christie Collection)", "Cards on the Table (Agatha Christie Mysteries Collection (Paperback))", "The A.B.C. Murders (The Christie Collection)", "Peril at End House (The Agatha Christie Mystery Collection) (The Agatha Christie Mystery Collection)", "Three Act Tragedy (Poirot)", "The big four.", "PASSENGER TO FRANKFURT", "Murder On The Links (Classic Books on Cassettes Collection)", "After the Funeral (BBC Audio Crime)", "They Do It with Mirrors (Agatha Christie Collection)", "The Adventure of the Christmas Pudding", "Lord Edgware Dies", "The Murder at the Vicarage", "Towards Zero (Agatha Christie Collection)", "By the Pricking of My Thumbs (Agatha Christie Audio Mystery)", "Hickory Dickory Dock (Hercule Poirot Mysteries (Paperback))", "Cards on the table.", "The Body in the Library (BBC Audio Crime)", "They Do It with Mirrors (BBC Radio Collection)", "Endless Night (Agatha Christie Collection)", "The Witness for the Prosecution and Other Stories", "The Secret Of Chimneys (St. Martin's Minotaur Mysteries)", "Sleeping Murder (Miss Marple Mysteries (Paperback))", "The Thirteen Problems (Miss Marple)", "A Cat Among the Pigeons (Agatha Christie Collection)", "One,two, buckle my shoe", "Sparkling Cyanide (Agatha Christie Collection)"], "ans_acc": 0.013333333333333334, "ans_hit": 1, "ans_f1": 0.019672131147540985, "ans_precission": 0.6, "ans_recall": 0.01, "path_f1": 0.04576976421636615, "path_precision": 0.3, "path_recall": 0.024774774774774775, "path_ans_f1": 0.026086956521739132, "path_ans_precision": 0.6, "path_ans_recall": 0.013333333333333334}
{"id": "WebQTest-377", "prediction": ["# Reasoning Path:\nMichael Jordan -> people.person.education -> m.0jwk2zd -> education.education.institution -> Emsley A. Laney High School\n# Answer:\nEmsley A. Laney High School", "# Reasoning Path:\nMichael Jordan -> people.person.education -> m.04g36q3 -> education.education.institution -> University of North Carolina at Chapel Hill\n# Answer:\nUniversity of North Carolina at Chapel Hill", "# Reasoning Path:\nMichael Jordan -> sports.drafted_athlete.drafted -> m.02h7cgv -> sports.sports_league_draft_pick.school -> University of North Carolina at Chapel Hill\n# Answer:\nUniversity of North Carolina at Chapel Hill", "# Reasoning Path:\nMichael Jordan -> people.person.education -> m.0jwk2zd -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nMichael Jordan -> sports.drafted_athlete.drafted -> m.02h7cgv -> sports.sports_league_draft_pick.team -> Chicago Bulls\n# Answer:\nChicago Bulls", "# Reasoning Path:\nMichael Jordan -> people.person.education -> m.0jwk2zd -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nMichael Jordan -> people.person.education -> m.04g36q3 -> education.education.major_field_of_study -> Cultural geography\n# Answer:\nCultural geography", "# Reasoning Path:\nMichael Jordan -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Esophageal cancer\n# Answer:\nMale", "# Reasoning Path:\nMichael Jordan -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Pancreatic cancer\n# Answer:\nMale", "# Reasoning Path:\nMichael Jordan -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Prostate cancer\n# Answer:\nMale"], "ground_truth": ["University of North Carolina at Chapel Hill", "Emsley A. Laney High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-379", "prediction": ["# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Suicide -> common.topic.notable_types -> Cause Of Death\n# Answer:\nSuicide", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.article -> m.02qnd1f\n# Answer:\nBallistic trauma", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nBallistic trauma", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.notable_for -> g.12558k_cd\n# Answer:\nBallistic trauma", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> film.film_location.featured_in_films -> Valkyrie\n# Answer:\nBerlin", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> base.aareas.schema.administrative_area.administrative_parent -> Germany\n# Answer:\nBerlin", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> film.film_location.featured_in_films -> Inglourious Basterds\n# Answer:\nBerlin"], "ground_truth": ["1945-04-30"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> people.person.ethnicity -> White people\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> freebase.valuenotation.has_value -> Siblings\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> people.person.nationality -> United States of America\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointed_role -> Director of the Bureau of Counterterrorism\n# Answer:\nDirector of the Bureau of Counterterrorism", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pk -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointee -> Henry A. Crumpton\n# Answer:\nHenry A. Crumpton", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1 -> people.appointment.appointed_role -> United States Ambassador to Angola\n# Answer:\nUnited States Ambassador to Angola", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079q3lx -> people.appointment.appointed_role -> United States Ambassador to Brazil\n# Answer:\nUnited States Ambassador to Brazil", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pv -> film.personal_film_appearance.film -> All About Ann: Governor Richards of the Lone Star State\n# Answer:\nAll About Ann: Governor Richards of the Lone Star State", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1 -> people.appointment.appointee -> Dan Mozena\n# Answer:\nDan Mozena"], "ground_truth": ["Ralph Nader", "Michael Peroutka", "Gene Amondson", "John Kerry"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-380", "prediction": ["# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.containedby -> Washington\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.containedby -> Portland-Vancouver-Beaverton, OR-WA Metropolitan Statistical Area\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.containedby -> Contiguous United States\n# Answer:\nWashington", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.partially_contains -> White Salmon River\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Washington\n# Answer:\nUnited States of America", "# Reasoning Path:\nMount St. Helens -> geography.mountain.mountain_range -> Cascade Range -> location.location.containedby -> North America\n# Answer:\nCascade Range", "# Reasoning Path:\nMount St. Helens -> common.image.size -> m.02bkwf9\n# Answer:\ncommon.image.size", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.contains -> Skamania\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.containedby -> Northwestern United States\n# Answer:\nWashington"], "ground_truth": ["Skamania County", "United States of America", "Washington", "Cascade Range", "North America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8470588235294118, "ans_precission": 0.9, "ans_recall": 0.8, "path_f1": 0.6857142857142857, "path_precision": 0.8, "path_recall": 0.6, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-381", "prediction": ["# Reasoning Path:\nNew York City -> tv.tv_location.tv_shows_filmed_here -> Flight of the Conchords -> tv.tv_program.languages -> English Language\n# Answer:\nFlight of the Conchords", "# Reasoning Path:\nNew York City -> tv.tv_location.tv_shows_filmed_here -> The Stand -> film.film.featured_film_locations -> Las Vegas\n# Answer:\nThe Stand", "# Reasoning Path:\nNew York City -> tv.tv_location.tv_shows_filmed_here -> The Stand -> tv.tv_program.filming_locations -> Las Vegas\n# Answer:\nThe Stand", "# Reasoning Path:\nNew York City -> tv.tv_location.tv_shows_filmed_here -> Flight of the Conchords -> tv.tv_program.genre -> Comedy\n# Answer:\nFlight of the Conchords", "# Reasoning Path:\nNew York City -> tv.tv_location.tv_shows_filmed_here -> The Stand -> film.film.country -> United States of America\n# Answer:\nThe Stand", "# Reasoning Path:\nNew York City -> visual_art.art_subject.artwork_on_the_subject -> End of 14th Street Crosstown Line -> visual_art.artwork.art_subject -> Labor unrest\n# Answer:\nEnd of 14th Street Crosstown Line", "# Reasoning Path:\nNew York City -> book.book_subject.works -> A Christmas Caroline -> book.written_work.subjects -> Christmas\n# Answer:\nA Christmas Caroline", "# Reasoning Path:\nNew York City -> book.book_subject.works -> Art Deco New York -> book.written_work.subjects -> Art Deco\n# Answer:\nArt Deco New York", "# Reasoning Path:\nNew York City -> tv.tv_location.tv_shows_filmed_here -> Trial Heat -> common.topic.notable_for -> g.1yl5v08_t\n# Answer:\nTrial Heat", "# Reasoning Path:\nNew York City -> tv.tv_location.tv_shows_filmed_here -> Flight of the Conchords -> tv.tv_program.genre -> Sitcom\n# Answer:\nFlight of the Conchords"], "ground_truth": ["Both Sides", "Flight of the Conchords", "Trial Heat", "The Stand"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7241379310344827, "ans_precission": 0.7, "ans_recall": 0.75, "path_f1": 0.7241379310344827, "path_precision": 0.7, "path_recall": 0.75, "path_ans_f1": 0.7241379310344827, "path_ans_precision": 0.7, "path_ans_recall": 0.75}
{"id": "WebQTest-382", "prediction": ["# Reasoning Path:\nTupac Shakur -> book.author.works_written -> The Rose That Grew from Concrete -> common.topic.notable_for -> g.1254y3l6t\n# Answer:\nThe Rose That Grew from Concrete", "# Reasoning Path:\nTupac Shakur -> common.topic.image -> Tupac -> book.book_edition.publisher -> Simon & Schuster Ltd\n# Answer:\nTupac", "# Reasoning Path:\nTupac Shakur -> book.author.works_written -> The Rose That Grew from Concrete -> common.topic.notable_for -> g.125cjn5d9\n# Answer:\nThe Rose That Grew from Concrete", "# Reasoning Path:\nTupac Shakur -> book.author.works_written -> The Tupac Shakur Collection -> common.topic.notable_types -> Book\n# Answer:\nThe Tupac Shakur Collection", "# Reasoning Path:\nTupac Shakur -> book.author.works_written -> The Tupac Shakur Collection -> book.book.genre -> Music\n# Answer:\nThe Tupac Shakur Collection", "# Reasoning Path:\nTupac Shakur -> common.topic.image -> Tupac -> book.book_edition.number_of_pages -> m.04xl38s\n# Answer:\nTupac", "# Reasoning Path:\nTupac Shakur -> book.author.book_editions_published -> The Rose That Grew from Concrete -> common.topic.notable_for -> g.1254y3l6t\n# Answer:\nThe Rose That Grew from Concrete", "# Reasoning Path:\nTupac Shakur -> book.author.book_editions_published -> Tupac -> book.book_edition.publisher -> Simon & Schuster Ltd\n# Answer:\nTupac", "# Reasoning Path:\nTupac Shakur -> common.topic.image -> Tupac -> book.book_edition.publisher -> Atria Books\n# Answer:\nTupac", "# Reasoning Path:\nTupac Shakur -> common.topic.image -> Meagainsttheworld -> common.image.size -> m.07xyxx2\n# Answer:\nMeagainsttheworld"], "ground_truth": ["Tupac", "The Rose That Grew from Concrete", "The rose that grew from concrete"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-383", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> China -> location.country.languages_spoken -> Yue Chinese\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> China -> location.country.languages_spoken -> English Language\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> China -> location.country.official_language -> Standard Chinese\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.country.official_language -> Malay Language\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Vietnam -> location.country.official_language -> Vietnamese Language\n# Answer:\nVietnam", "# Reasoning Path:\nChinese language -> language.human_language.main_country -> China -> location.country.languages_spoken -> Yue Chinese\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.country.languages_spoken -> English Language\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> China -> location.country.languages_spoken -> Esperanto Language\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> China -> location.country.administrative_divisions -> Shanghai\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Vietnam -> location.country.languages_spoken -> Esperanto Language\n# Answer:\nVietnam"], "ground_truth": ["'Phags-pa script", "N\u00fcshu script", "Simplified Chinese character", "Chinese characters", "Traditional Chinese characters"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-384", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.notable_types -> Profession\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.article -> m.0knj9\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.image -> The Bard\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Bard\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Poet\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Cardiovascular disease\n# Answer:\nMale", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> book.book_subject.works -> Authors Frequently Mentioned on the Web\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> influence.influence_node.influenced_by -> Alexander Pope\n# Answer:\nAnne Bront\u00eb", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.profession -> Writer\n# Answer:\nAnne Bront\u00eb"], "ground_truth": ["Author", "Poet", "Writer", "Bard"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5833333333333334, "ans_precission": 0.7, "ans_recall": 0.5, "path_f1": 0.5833333333333334, "path_precision": 0.7, "path_recall": 0.5, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-385", "prediction": ["# Reasoning Path:\nWilliam McKinley -> people.deceased_person.place_of_death -> Buffalo -> location.location.containedby -> New York\n# Answer:\nBuffalo", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.place_of_death -> Buffalo -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nBuffalo", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.place_of_death -> Buffalo -> location.location.containedby -> Area code 716\n# Answer:\nBuffalo", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.cause_of_death -> Assassination -> people.cause_of_death.includes_causes_of_death -> Assassination by firearm\n# Answer:\nAssassination", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.place_of_death -> Buffalo -> location.location.containedby -> Erie County\n# Answer:\nBuffalo", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.place_of_death -> Buffalo -> location.location.events -> Theodore Roosevelt 1901 presidential inauguration\n# Answer:\nBuffalo", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.place_of_death -> Buffalo -> location.location.events -> Assassination of William McKinley\n# Answer:\nBuffalo", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.cause_of_death -> Assassination -> people.cause_of_death.includes_causes_of_death -> Assassination by 'Accident'\n# Answer:\nAssassination", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.cause_of_death -> Assassination -> base.fight.crime_type.crimes_of_this_type -> Anna Lindh assassination\n# Answer:\nAssassination", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.cause_of_death -> Assassination -> people.cause_of_death.includes_causes_of_death -> Assassination by 'Suicide'\n# Answer:\nAssassination"], "ground_truth": ["1901-09-14"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-386", "prediction": ["# Reasoning Path:\nLake Merritt -> geography.lake.basin_countries -> United States of America -> location.country.first_level_divisions -> Kentucky\n# Answer:\nUnited States of America", "# Reasoning Path:\nLake Merritt -> geography.lake.basin_countries -> United States of America -> location.country.administrative_divisions -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nLake Merritt -> geography.lake.basin_countries -> United States of America -> location.country.first_level_divisions -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nLake Merritt -> geography.lake.basin_countries -> United States of America -> location.country.administrative_divisions -> Kentucky\n# Answer:\nUnited States of America", "# Reasoning Path:\nLake Merritt -> geography.lake.basin_countries -> United States of America -> location.country.first_level_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nLake Merritt -> geography.lake.basin_countries -> United States of America -> location.country.administrative_divisions -> Tennessee\n# Answer:\nUnited States of America", "# Reasoning Path:\nLake Merritt -> geography.lake.basin_countries -> United States of America -> base.locations.countries.states_provinces_within -> Nebraska\n# Answer:\nUnited States of America", "# Reasoning Path:\nLake Merritt -> location.location.containedby -> California -> location.location.partially_contains -> West Walker River\n# Answer:\nCalifornia", "# Reasoning Path:\nLake Merritt -> common.topic.notable_types -> Lake -> freebase.documented_object.documentation -> m.021y5b6\n# Answer:\nLake", "# Reasoning Path:\nLake Merritt -> geography.lake.basin_countries -> United States of America -> base.locations.countries.states_provinces_within -> Kentucky\n# Answer:\nUnited States of America"], "ground_truth": ["3.048"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-388", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.holidays -> Eid al-Fitr Eve -> common.topic.notable_types -> Holiday\n# Answer:\nEid al-Fitr Eve", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Eid al-Fitr Eve -> base.schemastaging.holiday_extra.observance_rule -> Eid al-Fitr Eve observance rule (Turkey, 2000 -now)\n# Answer:\nEid al-Fitr Eve", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Day of Ashura -> time.holiday.featured_in_religions -> Shia Islam\n# Answer:\nDay of Ashura", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Eid al-Fitr Eve -> common.topic.notable_for -> g.1q3sfbb_s\n# Answer:\nEid al-Fitr Eve", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Barat Night -> common.topic.notable_types -> Holiday\n# Answer:\nBarat Night", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Day of Ashura -> base.schemastaging.holiday_extra.observance_rule -> Day of Ashura observance rule (Turkey, 2000 - now)\n# Answer:\nDay of Ashura", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> base.schemastaging.context_name.pronunciation -> g.125_r5my9\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Day of Ashura -> common.topic.notable_for -> g.1259c0slx\n# Answer:\nDay of Ashura", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Barat Night -> common.topic.notable_for -> g.1z2spz3qy\n# Answer:\nBarat Night", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Day of Ashura -> time.holiday.featured_in_religions -> Sunni Islam\n# Answer:\nDay of Ashura"], "ground_truth": ["Mawlid", "Laylat al-Qadr", "Ramadan", "Eid-Ul-Adha Eve", "Eid al-Fitr", "Night of Wishes", "Mourning of Muharram", "Nuzul Al-Quran", "Day of Arafat", "Islamic New Year", "Lailat al Mi'raj", "Eid al-Adha", "Eid al-Fitr Eve", "Day of Ashura", "Barat Night"], "ans_acc": 0.26666666666666666, "ans_hit": 1, "ans_f1": 0.41142857142857137, "ans_precission": 0.9, "ans_recall": 0.26666666666666666, "path_f1": 0.41142857142857137, "path_precision": 0.9, "path_recall": 0.26666666666666666, "path_ans_f1": 0.41142857142857137, "path_ans_precision": 0.9, "path_ans_recall": 0.26666666666666666}
{"id": "WebQTest-389", "prediction": ["# Reasoning Path:\nClaude Debussy -> music.artist.genre -> French opera -> music.genre.artists -> Olivier Messiaen\n# Answer:\nFrench opera", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> French opera -> common.topic.notable_types -> Field of study\n# Answer:\nFrench opera", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> Art song -> music.compositional_form.superforms -> Vocal music\n# Answer:\nArt song", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> French opera -> common.topic.webpage -> m.09wjdym\n# Answer:\nFrench opera", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> 20th-century classical music -> music.genre.parent_genre -> Classical music\n# Answer:\n20th-century classical music", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> French opera -> music.genre.artists -> Francis Poulenc\n# Answer:\nFrench opera", "# Reasoning Path:\nClaude Debussy -> people.person.profession -> Pianist -> music.musician_profession.instrument -> Piano\n# Answer:\nPianist", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> French opera -> music.genre.artists -> Georges Bizet\n# Answer:\nFrench opera", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> Art song -> music.compositional_form.superforms -> Song\n# Answer:\nArt song", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> Art song -> music.genre.artists -> Maurice Ravel\n# Answer:\nArt song"], "ground_truth": ["Classical music", "Art song", "20th-century classical music", "Ballet", "Incidental music", "French opera"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7659574468085106, "ans_precission": 0.9, "ans_recall": 0.6666666666666666, "path_f1": 0.5806451612903225, "path_precision": 0.9, "path_recall": 0.42857142857142855, "path_ans_f1": 0.7659574468085106, "path_ans_precision": 0.9, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> people.marriage.spouse -> Sue Douglas\n# Answer:\nSue Douglas", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.spouse -> Ayaan Hirsi Ali\n# Answer:\nAyaan Hirsi Ali", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0pdthbn -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0pdthbn -> film.personal_film_appearance.film -> Valentino's Ghost\n# Answer:\nValentino's Ghost", "# Reasoning Path:\nNiall Ferguson -> book.author.works_written -> The world's banker -> book.book.editions -> The world's banker: the history of the House of Rothschild\n# Answer:\nThe world's banker", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nNiall Ferguson -> book.author.works_written -> The world's banker -> common.topic.notable_types -> Book\n# Answer:\nThe world's banker"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-390", "prediction": ["# Reasoning Path:\nStanley Cup Finals -> sports.sports_championship.events -> 2014 Stanley Cup Finals -> sports.sports_championship_event.runner_up -> New York Rangers\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup Finals -> sports.sports_championship.events -> 2014 Stanley Cup Finals -> sports.sports_championship_event.season -> 2013\u201314 NHL season\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup Finals -> time.recurring_event.instances -> 2014 Stanley Cup Finals -> sports.sports_championship_event.runner_up -> New York Rangers\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup Finals -> sports.sports_championship.events -> 2014 Stanley Cup Finals -> sports.sports_championship_event.champion -> Los Angeles Kings\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup Finals -> time.recurring_event.instances -> 2014 Stanley Cup Finals -> sports.sports_championship_event.season -> 2013\u201314 NHL season\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup Finals -> time.recurring_event.instances -> 2014 Stanley Cup Finals -> sports.sports_championship_event.champion -> Los Angeles Kings\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup Finals -> common.topic.notable_types -> Sports League Championship -> type.type.properties -> Events\n# Answer:\nSports League Championship", "# Reasoning Path:\nStanley Cup Finals -> time.recurring_event.instances -> 2009 Stanley Cup Finals -> sports.sports_championship_event.champion -> Pittsburgh Penguins\n# Answer:\n2009 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup Finals -> common.topic.notable_types -> Sports League Championship -> freebase.type_profile.equivalent_topic -> Championship\n# Answer:\nSports League Championship", "# Reasoning Path:\nStanley Cup Finals -> time.recurring_event.instances -> 2009 Stanley Cup Finals -> common.topic.notable_for -> g.1258dchbl\n# Answer:\n2009 Stanley Cup Finals"], "ground_truth": ["1967 Stanley Cup Finals"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-392", "prediction": ["# Reasoning Path:\nHank Baskett -> american_football.football_player.receiving -> m.09tcnhz -> american_football.player_receiving_statistics.team -> Philadelphia Eagles\n# Answer:\nPhiladelphia Eagles", "# Reasoning Path:\nHank Baskett -> american_football.football_player.receiving -> m.09rm09r -> american_football.player_receiving_statistics.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nHank Baskett -> american_football.football_player.receiving -> m.09grk6x -> american_football.player_receiving_statistics.team -> Philadelphia Eagles\n# Answer:\nPhiladelphia Eagles", "# Reasoning Path:\nHank Baskett -> american_football.football_player.receiving -> m.09tcnhz -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nHank Baskett -> american_football.football_player.receiving -> m.09rm09r -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nHank Baskett -> people.person.profession -> American football player -> common.topic.notable_types -> Profession\n# Answer:\nAmerican football player", "# Reasoning Path:\nHank Baskett -> american_football.football_player.receiving -> m.09grk6x -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nHank Baskett -> people.person.profession -> Football player -> freebase.type_profile.kind -> Title\n# Answer:\nFootball player", "# Reasoning Path:\nHank Baskett -> sports.pro_athlete.teams -> m.01205rsm -> sports.sports_team_roster.team -> Minnesota Vikings\n# Answer:\nMinnesota Vikings", "# Reasoning Path:\nHank Baskett -> sports.pro_athlete.teams -> m.064w5kf -> sports.sports_team_roster.position -> Wide receiver\n# Answer:\nWide receiver"], "ground_truth": ["Indianapolis Colts", "Philadelphia Eagles", "Minnesota Vikings"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.10526315789473685, "path_precision": 0.4, "path_recall": 0.06060606060606061, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-393", "prediction": ["# Reasoning Path:\nParaguay -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.countries_spoken_in -> Ukraine\n# Answer:\nUkrainian Language", "# Reasoning Path:\nParaguay -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.main_country -> Ukraine\n# Answer:\nUkrainian Language", "# Reasoning Path:\nParaguay -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nParaguay -> location.country.official_language -> Paraguayan Guaran\u00ed -> language.human_language.region -> Americas\n# Answer:\nParaguayan Guaran\u00ed", "# Reasoning Path:\nParaguay -> location.country.languages_spoken -> Ukrainian Language -> common.topic.notable_types -> Human Language\n# Answer:\nUkrainian Language", "# Reasoning Path:\nParaguay -> location.country.official_language -> Paraguayan Guaran\u00ed -> base.rosetta.languoid.parent -> Guarani I Group\n# Answer:\nParaguayan Guaran\u00ed", "# Reasoning Path:\nParaguay -> location.statistical_region.cpi_inflation_rate -> g.11b60sd4vw\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nParaguay -> location.country.official_language -> Paraguayan Guaran\u00ed -> base.rosetta.languoid.local_name -> Guaran\u00ed, Paraguayan\n# Answer:\nParaguayan Guaran\u00ed", "# Reasoning Path:\nParaguay -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Argentina\n# Answer:\nSpanish Language", "# Reasoning Path:\nParaguay -> location.country.languages_spoken -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language"], "ground_truth": ["Paraguayan Guaran\u00ed", "Spanish Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-395", "prediction": ["# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.location -> Pittsburgh -> location.location.containedby -> United States of America\n# Answer:\nPittsburgh", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.location -> Pittsburgh -> base.biblioness.bibs_location.state -> Pennsylvania\n# Answer:\nPittsburgh", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.location -> Pittsburgh -> location.location.containedby -> Allegheny County\n# Answer:\nPittsburgh", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.location -> Pittsburgh -> location.location.containedby -> Area code 412\n# Answer:\nPittsburgh", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.arena_stadium -> Pitt Stadium -> location.location.containedby -> 15213\n# Answer:\nPitt Stadium", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.arena_stadium -> Pitt Stadium -> common.topic.article -> m.038_tx\n# Answer:\nPitt Stadium", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.arena_stadium -> Pitt Stadium -> location.location.containedby -> Allegheny County\n# Answer:\nPitt Stadium", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.arena_stadium -> Forbes Field -> sports.sports_facility.teams -> Pittsburgh Phantoms\n# Answer:\nForbes Field", "# Reasoning Path:\nPittsburgh Steelers -> common.topic.webpage -> m.03nbxbp -> common.webpage.resource -> Offiical site of the PIttsburgh Steelers\n# Answer:\nOffiical site of the PIttsburgh Steelers", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.arena_stadium -> Forbes Field -> location.location.containedby -> Pittsburgh\n# Answer:\nForbes Field"], "ground_truth": ["Pittsburgh"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-396", "prediction": ["# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncxj8d -> education.education.institution -> Trinity College, Cambridge\n# Answer:\nTrinity College, Cambridge", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.02kq1mm -> education.education.institution -> Trinity College, Cambridge\n# Answer:\nTrinity College, Cambridge", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncxj8d -> education.education.degree -> Master of Arts\n# Answer:\nMaster of Arts", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncl4g6 -> education.education.institution -> The King's School, Grantham\n# Answer:\nThe King's School, Grantham", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.02kq1mm -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> Newton -> astronomy.extraterrestrial_location.geolocation -> m.0w2sw5y\n# Answer:\nNewton", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncl4g6 -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nIsaac Newton -> base.kwebbase.kwtopic.connections_to -> abraham de moivre studied work of isaac newton -> base.kwebbase.kwconnection.subject -> Abraham de Moivre\n# Answer:\nabraham de moivre studied work of isaac newton", "# Reasoning Path:\nIsaac Newton -> base.kwebbase.kwtopic.connections_to -> anton van leeuwenhoek royal society member with isaac newton -> base.kwebbase.kwconnection.sentence -> In 1680 Leeuwenhoek was elected a full member of the Royal Society, joining Newton, Hooke, Oldenburg, Boyle, Wren,  Edmond Halley, and other scientific luminaries of his day, who exchanged letters with him, suggesting subjects for his research.\n# Answer:\nanton van leeuwenhoek royal society member with isaac newton", "# Reasoning Path:\nIsaac Newton -> base.kwebbase.kwtopic.connections_to -> abraham de moivre studied work of isaac newton -> base.kwebbase.kwconnection.sentence -> On his release in 1686, he fled to England, where he made a modest living tutoring in mathematics, and continued to study recent mathematical texts, the most notable being Newton's 'Principia Mathematica.' In 1692 he met the astronomer Halley who was so impressed by him that he read his first paper (on Newton's calculus), to the Royal Society in 1695.\n# Answer:\nabraham de moivre studied work of isaac newton"], "ground_truth": ["Trinity College, Cambridge"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-397", "prediction": ["# Reasoning Path:\nHarvard University -> education.educational_institution.colors -> Crimson -> common.topic.notable_types -> Color\n# Answer:\nCrimson", "# Reasoning Path:\nHarvard University -> education.educational_institution.colors -> Crimson -> common.topic.image -> Carminic acid structure\n# Answer:\nCrimson", "# Reasoning Path:\nHarvard University -> education.educational_institution.colors -> Crimson -> base.roses.rose_color.roses_of_this_color -> Rosa 'Chevy Chase'\n# Answer:\nCrimson", "# Reasoning Path:\nHarvard University -> location.location.contains -> Gore Hall -> common.topic.notable_for -> g.1yw9k9gf0\n# Answer:\nGore Hall", "# Reasoning Path:\nHarvard University -> location.location.contains -> Massachusetts Hall -> location.location.containedby -> Cambridge\n# Answer:\nMassachusetts Hall", "# Reasoning Path:\nHarvard University -> education.educational_institution.colors -> Crimson -> base.roses.rose_color.roses_of_this_color -> Rosa 'Chrysler Imperial'\n# Answer:\nCrimson", "# Reasoning Path:\nHarvard University -> location.location.contains -> Gore Hall -> freebase.valuenotation.is_reviewed -> Architect\n# Answer:\nGore Hall", "# Reasoning Path:\nHarvard University -> organization.endowed_organization.endowment -> m.0znsnj0 -> measurement_unit.dated_money_value.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nHarvard University -> location.location.contains -> Massachusetts Hall -> location.location.containedby -> Massachusetts\n# Answer:\nMassachusetts Hall", "# Reasoning Path:\nHarvard University -> location.location.contains -> Massachusetts Hall -> architecture.structure.architectural_style -> Georgian\n# Answer:\nMassachusetts Hall"], "ground_truth": ["Crimson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-398", "prediction": ["# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Colorado\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> Colorado -> location.location.containedby -> United States, with Territories\n# Answer:\nColorado", "# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Colorado\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> Colorado Springs -> location.location.containedby -> United States of America\n# Answer:\nColorado Springs", "# Reasoning Path:\nUniversity of the Rockies -> education.university.domestic_tuition -> m.0nhfxbb -> measurement_unit.dated_money_value.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> Colorado -> location.location.containedby -> United States of America\n# Answer:\nColorado", "# Reasoning Path:\nUniversity of the Rockies -> organization.organization.headquarters -> m.0cnpmv9 -> common.topic.notable_types -> Location\n# Answer:\nLocation", "# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Alaska\n# Answer:\nUnited States of America"], "ground_truth": ["Colorado", "Colorado Springs", "United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-399", "prediction": ["# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Breast cancer -> medicine.disease.notable_people_with_this_condition -> Trisha Goddard\n# Answer:\nBreast cancer", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Breast cancer -> medicine.disease.notable_people_with_this_condition -> Amanda Mealing\n# Answer:\nBreast cancer", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Breast cancer -> medicine.disease.includes_diseases -> Metastatic breast cancer\n# Answer:\nBreast cancer", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Breast cancer -> medicine.disease.notable_people_with_this_condition -> Amy Mickelson\n# Answer:\nBreast cancer", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Breast cancer -> medicine.disease.includes_diseases -> Male breast cancer\n# Answer:\nBreast cancer", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Breast cancer -> common.topic.webpage -> m.04mwg6d\n# Answer:\nBreast cancer", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Myelodysplastic syndrome -> medicine.disease.medical_specialties -> Hematology\n# Answer:\nMyelodysplastic syndrome", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Breast cancer -> medicine.disease.includes_diseases -> HER2-Negative Breast Cancer\n# Answer:\nBreast cancer", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Breast cancer -> common.topic.webpage -> m.09w4cwt\n# Answer:\nBreast cancer", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Myelodysplastic syndrome -> people.cause_of_death.people -> Susan Sontag\n# Answer:\nMyelodysplastic syndrome"], "ground_truth": ["Myelodysplastic syndrome", "Breast cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-400", "prediction": ["# Reasoning Path:\nByzantine Empire -> location.location.containedby -> Near East -> location.location.containedby -> Western Asia\n# Answer:\nNear East", "# Reasoning Path:\nByzantine Empire -> location.location.containedby -> Near East -> location.location.events -> Sixth Crusade\n# Answer:\nNear East", "# Reasoning Path:\nByzantine Empire -> location.location.containedby -> Southern Europe -> location.location.containedby -> Europe\n# Answer:\nSouthern Europe", "# Reasoning Path:\nByzantine Empire -> location.location.containedby -> Near East -> location.location.events -> Third Crusade\n# Answer:\nNear East", "# Reasoning Path:\nByzantine Empire -> location.location.containedby -> Near East -> book.book_subject.works -> Arabic political memoirs and other studies\n# Answer:\nNear East", "# Reasoning Path:\nByzantine Empire -> location.location.events -> Siege of Constantinople -> time.event.locations -> Constantinople\n# Answer:\nSiege of Constantinople", "# Reasoning Path:\nByzantine Empire -> location.location.containedby -> Southern Europe -> location.location.events -> Mediterranean and Middle East theatre of World War II\n# Answer:\nSouthern Europe", "# Reasoning Path:\nByzantine Empire -> location.country.form_of_government -> Monarchy -> base.descriptive_names.names.descriptive_name -> m.0105cxwp\n# Answer:\nMonarchy", "# Reasoning Path:\nByzantine Empire -> location.country.form_of_government -> Monarchy -> book.book_subject.works -> I, the Sun\n# Answer:\nMonarchy", "# Reasoning Path:\nByzantine Empire -> location.location.containedby -> Near East -> location.location.events -> Arab\u2013Israeli conflict\n# Answer:\nNear East"], "ground_truth": ["Constantinople"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-401", "prediction": ["# Reasoning Path:\nEdward Scissorhands -> film.film_character.portrayed_in_films -> m.0jxqvs -> film.performance.actor -> Johnny Depp\n# Answer:\nJohnny Depp"], "ground_truth": ["Johnny Depp"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-402", "prediction": ["# Reasoning Path:\nSlipknot -> music.musical_group.member -> m.0l1wzcp -> music.group_membership.role -> Guitar\n# Answer:\nGuitar", "# Reasoning Path:\nSlipknot -> music.musical_group.member -> m.010ky_w5 -> music.group_membership.role -> Drums\n# Answer:\nDrums", "# Reasoning Path:\nSlipknot -> music.musical_group.member -> m.0l1wzcp -> music.group_membership.member -> Josh Brainard\n# Answer:\nJosh Brainard", "# Reasoning Path:\nJoey Jordison -> music.group_member.membership -> m.0_yyf5f -> music.group_membership.group -> Scar the Martyr\n# Answer:\nScar the Martyr", "# Reasoning Path:\nJoey Jordison -> music.group_member.membership -> m.010ky_w5 -> music.group_membership.role -> Drums\n# Answer:\nDrums", "# Reasoning Path:\nSlipknot -> music.musical_group.member -> g.11b7w8xg0_\n# Answer:\nmusic.musical_group.member", "# Reasoning Path:\nSlipknot -> music.musical_group.member -> m.0l1wzcp -> freebase.valuenotation.is_reviewed -> Member\n# Answer:\nMember", "# Reasoning Path:\nSlipknot -> music.musical_group.member -> m.010ky_w5 -> music.group_membership.member -> Joey Jordison\n# Answer:\nmusic.group_membership.member", "# Reasoning Path:\nSlipknot -> music.musical_group.member -> m.010ky_w5 -> freebase.valuenotation.is_reviewed -> Member\n# Answer:\nMember", "# Reasoning Path:\nJoey Jordison -> music.group_member.membership -> m.01wt4wv -> music.group_membership.role -> Vocals\n# Answer:\nVocals"], "ground_truth": ["Drums"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-403", "prediction": ["# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Kurdish languages -> language.human_language.countries_spoken_in -> Armenia\n# Answer:\nKurdish languages", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Ladino Language -> common.topic.notable_types -> Human Language\n# Answer:\nLadino Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Ladino Language -> language.human_language.region -> Asia\n# Answer:\nLadino Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Kurdish languages -> language.human_language.countries_spoken_in -> Iran\n# Answer:\nKurdish languages", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Kurdish languages -> common.topic.notable_types -> Human Language\n# Answer:\nKurdish languages", "# Reasoning Path:\nTurkey -> location.statistical_region.minimum_wage -> g.11b60lkkk3\n# Answer:\nlocation.statistical_region.minimum_wage", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Kurdish languages -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nKurdish languages", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nArabic Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Arabic Language -> common.topic.notable_types -> Human Language\n# Answer:\nArabic Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Syria\n# Answer:\nArabic Language"], "ground_truth": ["Arabic Language", "Kurdish languages", "Zaza language", "Turkish Language", "Ladino Language"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7200000000000001, "ans_precission": 0.9, "ans_recall": 0.6, "path_f1": 0.7200000000000001, "path_precision": 0.9, "path_recall": 0.6, "path_ans_f1": 0.7200000000000001, "path_ans_precision": 0.9, "path_ans_recall": 0.6}
{"id": "WebQTest-404", "prediction": ["# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04qp97c -> basketball.basketball_player_stats.team -> Cleveland Cavaliers\n# Answer:\nCleveland Cavaliers", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04qmsr4 -> basketball.basketball_player_stats.team -> Cleveland Cavaliers\n# Answer:\nCleveland Cavaliers", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04qp97c -> basketball.basketball_player_stats.season -> 2004\u201305 NBA season\n# Answer:\n2004\u201305 NBA season", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04qcjh2 -> basketball.basketball_player_stats.team -> Cleveland Cavaliers\n# Answer:\nCleveland Cavaliers", "# Reasoning Path:\nLeBron James -> sports.pro_athlete.teams -> m.011461z_ -> sports.sports_team_roster.team -> Cleveland Cavaliers\n# Answer:\nCleveland Cavaliers", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04qmsr4 -> basketball.basketball_player_stats.season -> 2003\u201304 NBA season\n# Answer:\n2003\u201304 NBA season", "# Reasoning Path:\nLeBron James -> sports.pro_athlete.teams -> m.012z6mb1 -> sports.sports_team_roster.position -> Point forward\n# Answer:\nPoint forward", "# Reasoning Path:\nLeBron James -> sports.pro_athlete.teams -> m.011461z_ -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nLeBron James -> sports.pro_athlete.teams -> m.02398d8 -> sports.sports_team_roster.team -> Miami Heat\n# Answer:\nMiami Heat", "# Reasoning Path:\nLeBron James -> award.award_nominee.award_nominations -> m.010_xz3j -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work"], "ground_truth": ["Cleveland Cavaliers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.4210526315789474, "path_precision": 0.4, "path_recall": 0.4444444444444444, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-405", "prediction": ["# Reasoning Path:\nBattle of Gettysburg -> military.military_conflict.military_personnel_involved -> George Meade -> military.military_person.participated_in_conflicts -> American Civil War\n# Answer:\nGeorge Meade", "# Reasoning Path:\nBattle of Gettysburg -> military.military_conflict.military_personnel_involved -> George Meade -> common.topic.article -> m.012pmt\n# Answer:\nGeorge Meade", "# Reasoning Path:\nBattle of Gettysburg -> military.military_conflict.military_personnel_involved -> George Meade -> common.topic.image -> George G. Meade Standing\n# Answer:\nGeorge Meade", "# Reasoning Path:\nBattle of Gettysburg -> military.military_conflict.military_personnel_involved -> George P. Doles -> military.military_person.participated_in_conflicts -> American Civil War\n# Answer:\nGeorge P. Doles", "# Reasoning Path:\nBattle of Gettysburg -> military.military_conflict.military_personnel_involved -> George Meade -> military.military_person.participated_in_conflicts -> Appomattox Campaign\n# Answer:\nGeorge Meade", "# Reasoning Path:\nBattle of Gettysburg -> military.military_conflict.military_personnel_involved -> George Meade -> military.military_person.participated_in_conflicts -> Battle of Antietam\n# Answer:\nGeorge Meade", "# Reasoning Path:\nBattle of Gettysburg -> military.military_conflict.military_personnel_involved -> George Meade -> common.topic.image -> George Gordon Meade\n# Answer:\nGeorge Meade", "# Reasoning Path:\nBattle of Gettysburg -> military.military_conflict.military_personnel_involved -> George P. Doles -> military.military_person.participated_in_conflicts -> Battle of Antietam\n# Answer:\nGeorge P. Doles", "# Reasoning Path:\nBattle of Gettysburg -> military.military_conflict.military_personnel_involved -> George P. Doles -> people.person.nationality -> Confederate States of America\n# Answer:\nGeorge P. Doles", "# Reasoning Path:\nBattle of Gettysburg -> military.military_conflict.military_personnel_involved -> Adolph von Steinwehr -> military.military_person.participated_in_conflicts -> Second Battle of Bull Run\n# Answer:\nAdolph von Steinwehr"], "ground_truth": ["United States of America", "Union", "George Meade", "Robert E. Lee", "Confederate States of America"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.3, "ans_precission": 0.6, "ans_recall": 0.2, "path_f1": 0.019672131147540985, "path_precision": 0.6, "path_recall": 0.01, "path_ans_f1": 0.509090909090909, "path_ans_precision": 0.7, "path_ans_recall": 0.4}
{"id": "WebQTest-406", "prediction": ["# Reasoning Path:\nSan Francisco Giants -> baseball.baseball_team.current_manager -> Bruce Bochy -> baseball.baseball_manager.former_teams_managed -> m.0j7zl02\n# Answer:\nBruce Bochy", "# Reasoning Path:\nSan Francisco Giants -> baseball.baseball_team.current_manager -> Bruce Bochy -> people.person.nationality -> United States of America\n# Answer:\nBruce Bochy", "# Reasoning Path:\nSan Francisco Giants -> baseball.baseball_team.team_stats -> m.05n69_j -> baseball.baseball_team_stats.season -> 1965 Major League Baseball Season\n# Answer:\n1965 Major League Baseball Season", "# Reasoning Path:\nSan Francisco Giants -> baseball.baseball_team.current_manager -> Bruce Bochy -> people.person.nationality -> France\n# Answer:\nBruce Bochy", "# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.league -> m.0crtd80 -> sports.sports_league_participation.league -> National League West\n# Answer:\nNational League West", "# Reasoning Path:\nSan Francisco Giants -> baseball.baseball_team.team_stats -> m.05n69my -> baseball.baseball_team_stats.season -> 1959 Major League Baseball Season\n# Answer:\n1959 Major League Baseball Season", "# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.league -> m.0crt4b6 -> sports.sports_league_participation.league -> Major League Baseball\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nSan Francisco Giants -> baseball.baseball_team.team_stats -> m.05n69l5 -> baseball.baseball_team_stats.season -> 1958 Major League Baseball Season\n# Answer:\n1958 Major League Baseball Season", "# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.league -> m.0crt7c2 -> sports.sports_league_participation.league -> National League\n# Answer:\nNational League"], "ground_truth": ["Dave Righetti", "Mark Gardner", "Tim Flannery", "Bill Hayes", "Roberto Kelly", "Ron Wotus", "Carney Lansford"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-408", "prediction": ["# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Lamar Hunt -> people.person.parents -> H. L. Hunt\n# Answer:\nLamar Hunt", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Lamar Hunt -> sports.pro_athlete.teams -> m.0hsx4r3\n# Answer:\nLamar Hunt", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Lamar Hunt -> common.topic.notable_types -> American football player\n# Answer:\nLamar Hunt", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Bud Adams -> organization.organization_founder.organizations_founded -> Adams Resources & Energy\n# Answer:\nBud Adams", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Bud Adams -> award.award_winner.awards_won -> m.0_qxnkc\n# Answer:\nBud Adams", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Bud Adams -> people.deceased_person.cause_of_death -> Natural causes\n# Answer:\nBud Adams", "# Reasoning Path:\nAmerican Football League -> sports.sports_league.teams -> m.0crthm0 -> sports.sports_league_participation.team -> Oakland Raiders\n# Answer:\nOakland Raiders", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Bud Adams -> organization.organization_founder.organizations_founded -> Adams Resources & Energy, Inc.\n# Answer:\nBud Adams", "# Reasoning Path:\nAmerican Football League -> sports.sports_league.sport -> American football -> sports.sport.team_coaches -> Bob Hewko\n# Answer:\nAmerican football", "# Reasoning Path:\nAmerican Football League -> sports.sports_league.sport -> American football -> film.film_subject.films -> A Saintly Switch\n# Answer:\nAmerican football"], "ground_truth": ["Bud Adams", "Lamar Hunt"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-409", "prediction": ["# Reasoning Path:\nGiuliana Rancic -> people.person.place_of_birth -> Naples -> location.location.events -> 1997 Napoli Film Festival\n# Answer:\nNaples", "# Reasoning Path:\nGiuliana Rancic -> base.popstra.celebrity.dated -> m.065p_0q -> base.popstra.dated.participant -> Jerry O'Connell\n# Answer:\nJerry O'Connell", "# Reasoning Path:\nGiuliana Rancic -> people.person.place_of_birth -> Naples -> travel.travel_destination.tourist_attractions -> Villa Comunale\n# Answer:\nNaples", "# Reasoning Path:\nGiuliana Rancic -> people.person.place_of_birth -> Naples -> travel.travel_destination.tourist_attractions -> Cappella Sansevero\n# Answer:\nNaples", "# Reasoning Path:\nGiuliana Rancic -> people.person.place_of_birth -> Naples -> location.statistical_region.population -> g.11b7t8558d\n# Answer:\nNaples", "# Reasoning Path:\nGiuliana Rancic -> film.person_or_entity_appearing_in_film.films -> m.0w17pwb -> film.personal_film_appearance.type_of_appearance -> Host\n# Answer:\nHost", "# Reasoning Path:\nGiuliana Rancic -> people.person.place_of_birth -> Naples -> location.location.events -> 1998 Napoli Film Festival\n# Answer:\nNaples", "# Reasoning Path:\nGiuliana Rancic -> people.person.place_of_birth -> Naples -> travel.travel_destination.tourist_attractions -> Mount Vesuvius\n# Answer:\nNaples", "# Reasoning Path:\nGiuliana Rancic -> people.person.place_of_birth -> Naples -> location.location.events -> 1999 Napoli Film Festival\n# Answer:\nNaples", "# Reasoning Path:\nGiuliana Rancic -> people.person.place_of_birth -> Naples -> location.statistical_region.population -> g.11btt4_t0l\n# Answer:\nNaples"], "ground_truth": ["Naples"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Pacific Ocean\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> common.topic.article -> m.03660h\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.containedby -> Americas\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> common.topic.notable_for -> g.1255fs0l4\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Pacific Ocean -> location.location.containedby -> World Ocean\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> base.aareas.schema.administrative_area.administrative_children -> Gal\u00e1pagos Province\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Baltra Island -> location.location.containedby -> Santa Cruz Canton, Ecuador\n# Answer:\nBaltra Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Bartolom\u00e9 Island -> location.location.containedby -> Santa Cruz Canton, Ecuador\n# Answer:\nBartolom\u00e9 Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.primarily_containedby -> South America\n# Answer:\nEcuador"], "ground_truth": ["Pacific Ocean", "Ecuador", "Gal\u00e1pagos Province"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-410", "prediction": ["# Reasoning Path:\nUkraine -> location.country.capital -> Kiev -> location.location.containedby -> Kiev Oblast\n# Answer:\nKiev", "# Reasoning Path:\nUkraine -> location.country.capital -> Kiev -> sports.sports_team_location.teams -> BC Budivelnyk\n# Answer:\nKiev", "# Reasoning Path:\nUkraine -> location.country.capital -> Kiev -> travel.travel_destination.tourist_attractions -> St Andrew's Church, Kiev\n# Answer:\nKiev", "# Reasoning Path:\nUkraine -> base.aareas.schema.administrative_area.administrative_children -> Kiev -> travel.travel_destination.tourist_attractions -> St Andrew's Church, Kiev\n# Answer:\nKiev", "# Reasoning Path:\nUkraine -> base.aareas.schema.administrative_area.administrative_children -> Kiev -> sports.sports_team_location.teams -> BC Budivelnyk\n# Answer:\nKiev", "# Reasoning Path:\nUkraine -> base.aareas.schema.administrative_area.administrative_children -> Cherkasy Oblast -> location.location.contains -> Kaniv\n# Answer:\nCherkasy Oblast", "# Reasoning Path:\nUkraine -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6dc92bn\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nUkraine -> location.country.capital -> Kiev -> location.location.containedby -> Rus'\n# Answer:\nKiev", "# Reasoning Path:\nUkraine -> base.aareas.schema.administrative_area.administrative_children -> Kiev -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nKiev", "# Reasoning Path:\nUkraine -> location.country.capital -> Kiev -> travel.travel_destination.tourist_attractions -> Kiev Pechersk Lavra\n# Answer:\nKiev"], "ground_truth": ["Kiev"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-414", "prediction": ["# Reasoning Path:\nBrian Dawkins -> base.schemastaging.athlete_extra.salary -> m.0j3nt3p -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nBrian Dawkins -> base.schemastaging.athlete_extra.salary -> m.0j3nt3p -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nBrian Dawkins -> common.image.size -> m.09hv3d2\n# Answer:\ncommon.image.size", "# Reasoning Path:\nBrian Dawkins -> award.award_nominee.award_nominations -> m.0_qyjj3 -> freebase.valuenotation.has_no_value -> Ceremony\n# Answer:\nCeremony", "# Reasoning Path:\nBrian Dawkins -> award.award_nominee.award_nominations -> m.0_qyjj3 -> award.award_nomination.award -> Walter Payton NFL Man of the Year Award\n# Answer:\nWalter Payton NFL Man of the Year Award", "# Reasoning Path:\nBrian Dawkins -> award.award_nominee.award_nominations -> m.0_qyjj3 -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work"], "ground_truth": ["Denver Broncos"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 0.16666666666666666, "ans_recall": 1.0, "path_f1": 0.08333333333333333, "path_precision": 0.16666666666666666, "path_recall": 0.05555555555555555, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-415", "prediction": ["# Reasoning Path:\nTupac Shakur -> film.actor.film -> m.0j_81z -> film.performance.film -> Juice\n# Answer:\nJuice", "# Reasoning Path:\nTupac Shakur -> film.actor.film -> m.02vcykh -> film.performance.film -> Nothing but Trouble\n# Answer:\nNothing but Trouble", "# Reasoning Path:\nTupac Shakur -> film.actor.film -> m.0j_81z -> film.performance.character -> Bishop\n# Answer:\nBishop", "# Reasoning Path:\nTupac Shakur -> film.actor.film -> m.02vb3h0 -> film.performance.film -> Poetic Justice\n# Answer:\nPoetic Justice", "# Reasoning Path:\nTupac Shakur -> music.featured_artist.recordings -> 2 of Americaz Most Wanted -> music.recording.tracks -> 2 of Amerikaz Most Wanted\n# Answer:\n2 of Americaz Most Wanted", "# Reasoning Path:\nTupac Shakur -> film.actor.film -> m.02vcykh -> film.performance.character -> Digital Underground member\n# Answer:\nDigital Underground member", "# Reasoning Path:\nTupac Shakur -> music.featured_artist.recordings -> 2 of Americaz Most Wanted -> music.recording.artist -> Snoop Dogg\n# Answer:\n2 of Americaz Most Wanted", "# Reasoning Path:\nTupac Shakur -> music.producer.releases_produced -> Gang Related -> common.topic.notable_types -> Film\n# Answer:\nGang Related", "# Reasoning Path:\nTupac Shakur -> film.actor.film -> m.02vb3h0 -> film.performance.character -> Lucky\n# Answer:\nLucky", "# Reasoning Path:\nTupac Shakur -> music.producer.releases_produced -> 15 Years on Death Row -> common.topic.notable_for -> g.1256xn5kv\n# Answer:\n15 Years on Death Row"], "ground_truth": ["Poetic Justice", "Nothing but Trouble", "Gang Related", "Murder Was the Case", "Above the Rim", "Juice", "Bullet", "Gridlock'd"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.4, "ans_recall": 0.5, "path_f1": 0.3, "path_precision": 0.3, "path_recall": 0.3, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.4, "path_ans_recall": 0.5}
{"id": "WebQTest-416", "prediction": ["# Reasoning Path:\nChina -> location.country.currency_used -> Renminbi -> common.topic.notable_types -> Currency\n# Answer:\nRenminbi", "# Reasoning Path:\nChina -> location.location.partially_contains -> Teram Kangri -> base.schemastaging.disputed_location.claimed_by -> Pakistan\n# Answer:\nTeram Kangri", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Yili horse -> common.topic.notable_types -> Organism Classification\n# Answer:\nYili horse", "# Reasoning Path:\nChina -> location.location.partially_contains -> Amur River -> location.location.partially_containedby -> Russia\n# Answer:\nAmur River", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Balikun horse -> common.topic.notable_types -> Organism Classification\n# Answer:\nBalikun horse", "# Reasoning Path:\nChina -> location.location.partially_contains -> Teram Kangri -> location.location.partially_containedby -> Xinjiang\n# Answer:\nTeram Kangri", "# Reasoning Path:\nChina -> location.location.partially_contains -> Argun River -> location.location.partially_containedby -> Russia\n# Answer:\nArgun River", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Black Telescope -> biology.animal_breed.breed_of -> Goldfish\n# Answer:\nBlack Telescope", "# Reasoning Path:\nChina -> location.location.partially_contains -> Teram Kangri -> base.schemastaging.disputed_location.claimed_by -> India\n# Answer:\nTeram Kangri", "# Reasoning Path:\nChina -> location.location.partially_contains -> Teram Kangri -> location.location.containedby -> Karakoram\n# Answer:\nTeram Kangri"], "ground_truth": ["Renminbi"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-417", "prediction": ["# Reasoning Path:\nKenneth Parcell -> tv.tv_character.appeared_in_tv_program -> m.03hbzrb -> tv.regular_tv_appearance.actor -> Jack McBrayer\n# Answer:\nJack McBrayer", "# Reasoning Path:\nKenneth Parcell -> fictional_universe.fictional_character.character_created_by -> Tina Fey -> fictional_universe.fictional_character_creator.fictional_characters_created -> Jack Donaghy\n# Answer:\nTina Fey", "# Reasoning Path:\nKenneth Parcell -> tv.tv_character.appeared_in_tv_program -> m.03hbzrb -> tv.regular_tv_appearance.seasons -> 30 Rock - Season 1\n# Answer:\n30 Rock - Season 1", "# Reasoning Path:\nKenneth Parcell -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nKenneth Parcell -> fictional_universe.fictional_character.character_created_by -> Tina Fey -> people.person.gender -> Female\n# Answer:\nTina Fey", "# Reasoning Path:\nKenneth Parcell -> tv.tv_character.appeared_in_tv_program -> m.03hbzrb -> tv.regular_tv_appearance.seasons -> 30 Rock - Season 2\n# Answer:\n30 Rock - Season 2", "# Reasoning Path:\nKenneth Parcell -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nKenneth Parcell -> fictional_universe.fictional_character.character_created_by -> Tina Fey -> fictional_universe.fictional_character_creator.fictional_characters_created -> Cerie\n# Answer:\nTina Fey", "# Reasoning Path:\nKenneth Parcell -> fictional_universe.fictional_character.character_created_by -> Tina Fey -> people.person.profession -> Screenwriter\n# Answer:\nTina Fey", "# Reasoning Path:\nKenneth Parcell -> tv.tv_character.appeared_in_tv_program -> m.03hbzrb -> tv.regular_tv_appearance.seasons -> 30 Rock - Season 3\n# Answer:\n30 Rock - Season 3"], "ground_truth": ["Jack McBrayer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-418", "prediction": ["# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.division -> AFC South -> american_football.football_division.teams -> Tennessee Titans\n# Answer:\nAFC South", "# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.division -> AFC South -> common.topic.article -> m.068g_7\n# Answer:\nAFC South", "# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.division -> AFC South -> american_football.football_division.teams -> Houston Texans\n# Answer:\nAFC South", "# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.division -> AFC South -> sports.sports_league.teams -> m.0crtf9v\n# Answer:\nAFC South", "# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.away_games -> Indianapolis Colts at Baltimore Ravens, 2009-11-22 -> american_football.football_game.home_team -> Baltimore Ravens\n# Answer:\nIndianapolis Colts at Baltimore Ravens, 2009-11-22", "# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.division -> AFC South -> american_football.football_division.teams -> Jacksonville Jaguars\n# Answer:\nAFC South", "# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.division -> AFC South -> sports.sports_league.teams -> m.0crtfb5\n# Answer:\nAFC South", "# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.away_games -> Indianapolis Colts at Buffalo Bills, 2010-01-03 -> american_football.football_game.season -> 2009 NFL season\n# Answer:\nIndianapolis Colts at Buffalo Bills, 2010-01-03", "# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.division -> AFC South -> sports.sports_league.teams -> m.0crtfbg\n# Answer:\nAFC South", "# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.away_games -> Indianapolis Colts at Arizona Cardinals, 2009-09-27 -> american_football.football_game.season -> 2009 NFL season\n# Answer:\nIndianapolis Colts at Arizona Cardinals, 2009-09-27"], "ground_truth": ["National Football League", "American Football Conference", "AFC South"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.45161290322580644, "ans_precission": 0.7, "ans_recall": 0.3333333333333333, "path_f1": 0.3684210526315789, "path_precision": 0.7, "path_recall": 0.25, "path_ans_f1": 0.45161290322580644, "path_ans_precision": 0.7, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-419", "prediction": ["# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.location.people_born_here -> Afonso Mendes de Melo\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> g.11btv6zfvr\n# Answer:\nprotected_sites.listed_site.designation_as_natural_or_cultural_site", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.location.people_born_here -> Afonso Pires Gato\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> government.governmental_jurisdiction.governing_officials -> m.04_9736\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.location.people_born_here -> Ant\u00f3nio Lu\u00eds de Sousa, 2nd Marquis of Minas\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.country.capital -> Lisbon\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> symbols.name_source.namesakes -> Magellan -> common.topic.article -> m.01m0j_\n# Answer:\nMagellan", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> government.governmental_jurisdiction.governing_officials -> m.04_975f\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.country.capital -> Coimbra\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> government.governmental_jurisdiction.governing_officials -> m.0c0mt_c\n# Answer:\nKingdom of Portugal"], "ground_truth": ["Kingdom of Portugal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> Somebody to Love\n# Answer:\nSomebody to Love", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.featured_artists -> Ludacris\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> music.composition.composer -> Ryan Toby\n# Answer:\nAll Bad", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.artist -> Ludacris\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> music.album.release_type -> Single\n# Answer:\nAll Bad", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All That Matters -> music.composition.composer -> Jason \\\"Poo Bear\\\" Boyd\n# Answer:\nAll That Matters", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All That Matters -> award.award_nominated_work.award_nominations -> m.0_vw6nn\n# Answer:\nAll That Matters", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nJuno Awards of 2014", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Bad -> music.composition.composer -> Andre Harris\n# Answer:\nAll Bad"], "ground_truth": ["Confident", "Eenie Meenie", "Bigger", "Heartbreaker", "Hold Tight", "Thought Of You", "All Around The World", "Die in Your Arms", "As Long as You Love Me", "Pray", "Home to Mama", "Lolly", "Recovery", "All That Matters", "Right Here", "Live My Life", "Change Me", "Somebody to Love", "Never Let You Go", "Beauty And A Beat", "Never Say Never", "Boyfriend", "Bad Day", "All Bad", "Roller Coaster", "#thatPower", "Baby", "Wait for a Minute", "PYD", "First Dance", "Turn to You (Mother's Day Dedication)"], "ans_acc": 0.12903225806451613, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.8, "ans_recall": 0.12903225806451613, "path_f1": 0.05555555555555555, "path_precision": 0.2, "path_recall": 0.03225806451612903, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.8, "path_ans_recall": 0.12903225806451613}
{"id": "WebQTest-420", "prediction": ["# Reasoning Path:\nPerpignan -> location.location.containedby -> Languedoc-Roussillon -> location.location.containedby -> France\n# Answer:\nLanguedoc-Roussillon", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Languedoc-Roussillon -> location.location.containedby -> Metropolitan France\n# Answer:\nLanguedoc-Roussillon", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Languedoc-Roussillon -> location.location.people_born_here -> Miguel Lluch\n# Answer:\nLanguedoc-Roussillon", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Pyr\u00e9n\u00e9es-Orientales -> location.location.containedby -> Languedoc-Roussillon\n# Answer:\nPyr\u00e9n\u00e9es-Orientales", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Languedoc-Roussillon -> location.location.containedby -> Pyrenees\u2013Mediterranean Euroregion\n# Answer:\nLanguedoc-Roussillon", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Pyr\u00e9n\u00e9es-Orientales -> location.location.containedby -> Languedoc-Roussillon-Midi-Pyr\u00e9n\u00e9es\n# Answer:\nPyr\u00e9n\u00e9es-Orientales", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Pyr\u00e9n\u00e9es-Orientales -> common.topic.image -> Location of Pyr\u00e9n\u00e9es-Orientales in France\n# Answer:\nPyr\u00e9n\u00e9es-Orientales", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Languedoc-Roussillon -> common.topic.image -> Flag of Languedoc-Roussillon\n# Answer:\nLanguedoc-Roussillon", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Pyr\u00e9n\u00e9es-Orientales -> location.location.containedby -> France\n# Answer:\nPyr\u00e9n\u00e9es-Orientales", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Pyr\u00e9n\u00e9es-Orientales -> common.topic.notable_for -> g.125b2pjvt\n# Answer:\nPyr\u00e9n\u00e9es-Orientales"], "ground_truth": ["France", "Pyr\u00e9n\u00e9es-Orientales", "Languedoc-Roussillon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 1.0, "ans_recall": 0.6666666666666666, "path_f1": 0.8, "path_precision": 1.0, "path_recall": 0.6666666666666666, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-421", "prediction": ["# Reasoning Path:\nKim Richards -> people.person.spouse_s -> m.0j4kh7g -> people.marriage.spouse -> G. Monty Brinson\n# Answer:\nG. Monty Brinson", "# Reasoning Path:\nKim Richards -> people.person.spouse_s -> m.0j4kh7g -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nKim Richards -> people.person.spouse_s -> m.0kv3rtm -> people.marriage.spouse -> John Jackson\n# Answer:\nJohn Jackson", "# Reasoning Path:\nKim Richards -> people.person.spouse_s -> m.0j4kh7g -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nKim Richards -> people.person.spouse_s -> m.0swszf2 -> people.marriage.spouse -> Gregg Davis\n# Answer:\nGregg Davis", "# Reasoning Path:\nKim Richards -> people.person.parents -> Kathleen Richards -> people.person.spouse_s -> m.0j4js1t\n# Answer:\nKathleen Richards", "# Reasoning Path:\nKim Richards -> people.person.parents -> Kathleen Richards -> people.person.spouse_s -> m.0kv32c8\n# Answer:\nKathleen Richards", "# Reasoning Path:\nKim Richards -> people.person.parents -> Kathleen Richards -> people.person.profession -> Casting Director\n# Answer:\nKathleen Richards", "# Reasoning Path:\nKim Richards -> people.person.parents -> Ken Richards -> people.person.spouse_s -> m.0j4js1t\n# Answer:\nKen Richards", "# Reasoning Path:\nKim Richards -> people.person.spouse_s -> m.0kv3rtm -> people.marriage.type_of_union -> Domestic partnership\n# Answer:\nDomestic partnership"], "ground_truth": ["Gregg Davis", "G. Monty Brinson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.26666666666666666, "path_precision": 0.2, "path_recall": 0.4, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-422", "prediction": ["# Reasoning Path:\nBosnia and Herzegovina -> location.location.events -> Bosnian War -> time.event.includes_event -> Ahatovi\u0107i massacre\n# Answer:\nBosnian War", "# Reasoning Path:\nBosnia and Herzegovina -> location.location.events -> Bosnian War -> time.event.includes_event -> Battle of Kupres\n# Answer:\nBosnian War", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.official_language -> Croatian language -> language.human_language.countries_spoken_in -> Croatia\n# Answer:\nCroatian language", "# Reasoning Path:\nBosnia and Herzegovina -> location.location.events -> Bosnian War -> time.event.includes_event -> Croat\u2013Bosniak War\n# Answer:\nBosnian War", "# Reasoning Path:\nBosnia and Herzegovina -> location.location.events -> Bosnian War -> base.culturalevent.event.entity_involved -> Serbia and Montenegro\n# Answer:\nBosnian War", "# Reasoning Path:\nBosnia and Herzegovina -> location.location.events -> Operation Deliberate Force -> military.military_conflict.military_personnel_involved -> Paul Ray Smith\n# Answer:\nOperation Deliberate Force", "# Reasoning Path:\nBosnia and Herzegovina -> location.location.events -> Bosnian War -> military.military_conflict.military_personnel_involved -> Janko Bobetko\n# Answer:\nBosnian War", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.official_language -> Bosnian language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nBosnian language", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.official_language -> Bosnian language -> language.human_language.writing_system -> Cyrillic script\n# Answer:\nBosnian language", "# Reasoning Path:\nBosnia and Herzegovina -> location.statistical_region.health_expenditure_as_percent_of_gdp -> g.12cp_j3km\n# Answer:\nlocation.statistical_region.health_expenditure_as_percent_of_gdp"], "ground_truth": ["Ahmi\u0107i massacre", "Operation Jackal", "Ahatovi\u0107i massacre", "Operation Tiger", "Stupni Do massacre", "Bosnian War", "Bosnian Genocide", "Operation Sana", "Operation Miracle", "\u0160trpci massacre", "Fo\u010da massacres", "Yugoslav Wars", "Operation Deny Flight", "Operation Deliberate Force", "Operation Spider", "NATO intervention in Bosnia and Herzegovina", "Vi\u0161egrad massacres", "Operation Una", "\u010cemerno massacre", "Glogova massacre", "Operation Southern Move", "Operation Neretva '93", "Operation Summer '95", "Sjeverin massacre", "Croat\u2013Bosniak War", "Massacre in Grabovica", "Sovi\u0107i massacre", "La\u0161va Valley ethnic cleansing", "Battle of Hasselt", "Operation Winter '94", "Operation Mistral 2", "Battle of Kupres", "Operation Corridor 92", "Operation Storm"], "ans_acc": 0.14705882352941177, "ans_hit": 1, "ans_f1": 0.10714285714285714, "ans_precission": 0.6, "ans_recall": 0.058823529411764705, "path_f1": 0.1043478260869565, "path_precision": 0.6, "path_recall": 0.05714285714285714, "path_ans_f1": 0.2362204724409449, "path_ans_precision": 0.6, "path_ans_recall": 0.14705882352941177}
{"id": "WebQTest-423", "prediction": ["# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> Super Bowl XLIII -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XLIII", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> Super Bowl XLIII -> sports.sports_championship_event.runner_up -> Arizona Cardinals\n# Answer:\nSuper Bowl XLIII", "# Reasoning Path:\nPittsburgh Steelers -> american_football.football_team.current_head_coach -> Mike Tomlin -> american_football.football_coach.coaching_history -> m.05cvv1h\n# Answer:\nMike Tomlin", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> 1996 AFC Championship Game -> common.topic.article -> m.0hzps_m\n# Answer:\n1996 AFC Championship Game", "# Reasoning Path:\nPittsburgh Steelers -> common.topic.webpage -> m.0gw54r6 -> common.webpage.in_index -> Blissful Master Index\n# Answer:\nBlissful Master Index", "# Reasoning Path:\nPittsburgh Steelers -> american_football.football_team.current_head_coach -> Mike Tomlin -> sports.pro_athlete.teams -> m.05cvv_4\n# Answer:\nMike Tomlin", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> 1996 AFC Championship Game -> common.topic.notable_for -> g.1q3scnq3r\n# Answer:\n1996 AFC Championship Game", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> 2006 AFC Championship Game -> sports.sports_championship_event.runner_up -> Denver Broncos\n# Answer:\n2006 AFC Championship Game", "# Reasoning Path:\nPittsburgh Steelers -> american_football.football_team.current_head_coach -> Mike Tomlin -> american_football.football_coach.coaching_history -> m.05cvsxx\n# Answer:\nMike Tomlin", "# Reasoning Path:\nPittsburgh Steelers -> common.topic.webpage -> m.0gw54r6 -> common.webpage.category -> Curated Topic\n# Answer:\nCurated Topic"], "ground_truth": ["Super Bowl XLIII"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-425", "prediction": ["# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> United States of America -> location.location.primarily_containedby -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.location.containedby -> North America\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> New Zealand -> location.country.languages_spoken -> Standard Chinese\n# Answer:\nNew Zealand", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> New Zealand -> location.country.languages_spoken -> Standard Chinese\n# Answer:\nNew Zealand", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Australia -> sports.sport_country.multi_event_tournaments_participated_in -> 2010 Commonwealth Games\n# Answer:\nAustralia", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> South Africa -> location.country.languages_spoken -> Xhosa Language\n# Answer:\nSouth Africa", "# Reasoning Path:\nEnglish Language -> education.school_category.schools_of_this_kind -> Piers Midwinter -> people.person.quotations -> Learn with a NEST. Please be my GUEST. I will give you my BEST,\n# Answer:\nPiers Midwinter", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> New Zealand -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nNew Zealand", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Australia -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\nAustralia"], "ground_truth": ["Montserrat", "Mandatory Palestine", "Hong Kong", "Republic of Ireland", "Brunei", "South Africa", "South Yemen", "Cura\u00e7ao", "Sierra Leone", "Kenya", "Honduras", "Gazankulu", "Pakistan", "Cameroon", "Tuvalu", "Transkei", "Cyprus", "Isle of Man", "Saint Lucia", "Vatican City", "China", "Japan", "Vanuatu", "Sri Lanka", "Bonaire", "Rwanda", "Indonesia", "Samoa", "Papua New Guinea", "Fiji", "Territory of Papua and New Guinea", "Antigua and Barbuda", "Zimbabwe", "Swaziland", "England", "Ethiopia", "Uganda", "Nigeria", "Tanzania", "Turks and Caicos Islands", "Marshall Islands", "Belize", "Guam", "Gibraltar", "Australia", "Philippines", "United Kingdom", "Qatar", "Laos", "Malaysia", "Cook Islands", "New Zealand", "Saint Vincent and the Grenadines", "Kiribati", "Kingdom of Great Britain", "United States of America", "Bahamas", "Sudan", "India", "Lesotho", "Guyana", "Israel", "Puerto Rico", "Bangladesh", "Jersey", "Jordan", "Bermuda", "Malta", "Singapore", "Cayman Islands", "Canada", "Gambia", "Saint Kitts and Nevis", "Namibia", "Dominica", "Nauru", "Ghana", "Liberia", "Timor-Leste", "Zambia", "Barbados", "Tokelau", "State of Palestine", "Grenada", "Botswana", "Wales", "Territory of New Guinea"], "ans_acc": 0.05747126436781609, "ans_hit": 1, "ans_f1": 0.10804321728691475, "ans_precission": 0.9, "ans_recall": 0.05747126436781609, "path_f1": 0.08421052631578949, "path_precision": 0.5, "path_recall": 0.04597701149425287, "path_ans_f1": 0.10804321728691475, "path_ans_precision": 0.9, "path_ans_recall": 0.05747126436781609}
{"id": "WebQTest-426", "prediction": ["# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Egypt -> location.location.containedby -> Middle East\n# Answer:\nEgypt", "# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Giza -> travel.travel_destination.tourist_attractions -> Giza Plateau\n# Answer:\nGiza", "# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Egypt -> location.location.containedby -> Arab world\n# Answer:\nEgypt", "# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Egypt -> government.governmental_jurisdiction.government_bodies -> Shura Council\n# Answer:\nEgypt", "# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Giza -> location.location.containedby -> Egypt\n# Answer:\nGiza", "# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Giza -> travel.travel_destination.tourist_attractions -> Great Pyramid of Giza\n# Answer:\nGiza", "# Reasoning Path:\nGiza Necropolis -> travel.tourist_attraction.near_travel_destination -> Cairo -> location.location.containedby -> Egypt\n# Answer:\nCairo", "# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Giza -> location.location.containedby -> Giza Governorate\n# Answer:\nGiza", "# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Egypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nEgypt", "# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Giza -> travel.travel_destination.tourist_attractions -> Al Amir Perfume Palace\n# Answer:\nGiza"], "ground_truth": ["Giza", "Egypt"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-427", "prediction": ["# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79701 -> location.location.contains -> Bank of America Building\n# Answer:\n79701", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79701 -> common.topic.notable_for -> g.1256hz0nz\n# Answer:\n79701", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79702 -> common.topic.notable_for -> g.1257xzgff\n# Answer:\n79702", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79701 -> common.topic.notable_types -> Postal Code\n# Answer:\n79701", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79705 -> location.postal_code.country -> United States of America\n# Answer:\n79705", "# Reasoning Path:\nMidland -> location.statistical_region.population -> g.11b66cwzp7\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79701 -> location.location.contains -> Centennial Tower\n# Answer:\n79701", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79702 -> common.topic.notable_types -> Postal Code\n# Answer:\n79702", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79701 -> location.location.contains -> Petroleum Building\n# Answer:\n79701", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79702 -> location.postal_code.country -> United States of America\n# Answer:\n79702"], "ground_truth": ["79708", "79705", "79704", "79706", "79702", "79701", "79703", "79712", "79707", "79711", "79710"], "ans_acc": 0.2727272727272727, "ans_hit": 1, "ans_f1": 0.41860465116279066, "ans_precission": 0.9, "ans_recall": 0.2727272727272727, "path_f1": 0.41860465116279066, "path_precision": 0.9, "path_recall": 0.2727272727272727, "path_ans_f1": 0.41860465116279066, "path_ans_precision": 0.9, "path_ans_recall": 0.2727272727272727}
{"id": "WebQTest-428", "prediction": ["# Reasoning Path:\nMidwestern United States -> location.location.contains -> Council Grove -> location.location.containedby -> United States of America\n# Answer:\nCouncil Grove", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Council Grove -> location.location.containedby -> Kansas\n# Answer:\nCouncil Grove", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Council Grove -> location.location.contains -> Seth Hays House\n# Answer:\nCouncil Grove", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Missouri -> location.location.containedby -> United States, with Territories\n# Answer:\nMissouri", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Council Grove -> location.location.containedby -> Morris County\n# Answer:\nCouncil Grove", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Missouri -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Council Grove -> location.location.contains -> Butler Community College, Council Grove\n# Answer:\nCouncil Grove", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Missouri -> location.location.containedby -> West North Central States\n# Answer:\nMissouri", "# Reasoning Path:\nMidwestern United States -> book.book_subject.works -> Caught in the Middle: America's Heartland in the Age of Globalism -> book.written_work.subjects -> Economics\n# Answer:\nCaught in the Middle: America's Heartland in the Age of Globalism", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Council Grove -> location.location.contains -> Council Grove Historic District\n# Answer:\nCouncil Grove"], "ground_truth": ["Nebraska", "Illinois", "Iowa", "Kansas", "Wisconsin", "Missouri", "North Dakota", "South Dakota", "Minnesota", "Michigan", "Indiana", "Ohio", "Avenue of the Saints", "West North Central States", "Council Grove", "East North Central States"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.21951219512195125, "ans_precission": 0.9, "ans_recall": 0.125, "path_f1": 0.21951219512195125, "path_precision": 0.9, "path_recall": 0.125, "path_ans_f1": 0.391304347826087, "path_ans_precision": 0.9, "path_ans_recall": 0.25}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> common.topic.notable_types -> Profession\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> common.topic.article -> m.016fc9\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> common.topic.notable_types -> Profession\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Acetylene Lamp\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> people.profession.corresponding_type -> Author\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Physician -> common.topic.notable_types -> Profession\n# Answer:\nPhysician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> type.type.expected_by -> politician\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> symbols.name_source.namesakes -> Champs-\u00c9lys\u00e9es \u2013 Clemenceau -> common.topic.notable_types -> Transit Stop\n# Answer:\nChamps-\u00c9lys\u00e9es \u2013 Clemenceau", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> base.descriptive_names.names.descriptive_name -> m.0101fnw4\n# Answer:\nStatesman"], "ground_truth": ["Statesman", "Writer", "Physician", "Publisher", "Journalist"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.6, "ans_recall": 0.6, "path_f1": 0.6, "path_precision": 0.6, "path_recall": 0.6, "path_ans_f1": 0.6, "path_ans_precision": 0.6, "path_ans_recall": 0.6}
{"id": "WebQTest-430", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> Somebody to Love\n# Answer:\nSomebody to Love", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber's Believe -> film.film.music -> Nathan Lanier\n# Answer:\nJustin Bieber's Believe", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nJuno Awards of 2014", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nWinning work", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0115qhzk -> award.award_honor.award -> MTV Europe Music Voices Award\n# Answer:\nMTV Europe Music Voices Award", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber's Believe -> film.film.executive_produced_by -> Allison Kaye Scarinzi\n# Answer:\nJustin Bieber's Believe", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber: Never Say Never -> film.film.executive_produced_by -> Randy Phillips\n# Answer:\nJustin Bieber: Never Say Never", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> freebase.valuenotation.is_reviewed -> Award category\n# Answer:\nAward category", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> award.award_honor.ceremony -> 2014 Billboard Music Awards\n# Answer:\n2014 Billboard Music Awards"], "ground_truth": ["Vocals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-431", "prediction": ["# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> Japan\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars"], "ground_truth": ["Esperanto Language", "English Language", "Lojban"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-432", "prediction": ["# Reasoning Path:\nAnnie Oakley -> people.person.spouse_s -> m.0j4kg9x -> people.marriage.spouse -> Frank E. Butler\n# Answer:\nFrank E. Butler", "# Reasoning Path:\nAnnie Oakley -> people.person.spouse_s -> m.0j4kg9x -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nAnnie Oakley -> media_common.netflix_title.netflix_genres -> Action/Adventure\n# Answer:\nAction/Adventure", "# Reasoning Path:\nAnnie Oakley -> media_common.netflix_title.netflix_genres -> Classic Action & Adventure\n# Answer:\nClassic Action & Adventure", "# Reasoning Path:\nAnnie Oakley -> media_common.netflix_title.netflix_genres -> Classic Movies\n# Answer:\nClassic Movies"], "ground_truth": ["Frank E. Butler"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-433", "prediction": ["# Reasoning Path:\nAlexander Luthor -> film.film_character.portrayed_in_films -> m.0k3zb8 -> film.performance.actor -> Kevin Spacey\n# Answer:\nKevin Spacey", "# Reasoning Path:\nAlexander Luthor -> film.film_character.portrayed_in_films -> m.04j2wpn -> film.performance.actor -> Clancy Brown\n# Answer:\nClancy Brown", "# Reasoning Path:\nAlexander Luthor -> film.film_character.portrayed_in_films -> m.0k3z3h -> film.performance.actor -> Gene Hackman\n# Answer:\nGene Hackman", "# Reasoning Path:\nAlexander Luthor -> film.film_character.portrayed_in_films -> m.0k3zb8 -> film.performance.film -> Superman Returns\n# Answer:\nSuperman Returns", "# Reasoning Path:\nAlexander Luthor -> film.film_character.portrayed_in_films -> m.04j2wpn -> film.performance.film -> The Batman Superman Movie: World's Finest\n# Answer:\nThe Batman Superman Movie: World's Finest", "# Reasoning Path:\nAlexander Luthor -> fictional_universe.fictional_character.romantically_involved_with -> m.09ds17f -> fictional_universe.romantic_involvement.partner -> Lana Lang\n# Answer:\nLana Lang", "# Reasoning Path:\nAlexander Luthor -> base.fictionaluniverse.fictional_killer.characters_killed -> Lionel Luthor -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Metropolis\n# Answer:\nLionel Luthor", "# Reasoning Path:\nAlexander Luthor -> film.film_character.portrayed_in_films -> m.0k3z3h -> film.performance.film -> Superman II\n# Answer:\nSuperman II", "# Reasoning Path:\nAlexander Luthor -> base.fictionaluniverse.fictional_killer.characters_killed -> Lionel Luthor -> fictional_universe.fictional_character.character_created_by -> Alfred Gough\n# Answer:\nLionel Luthor", "# Reasoning Path:\nAlexander Luthor -> base.fictionaluniverse.fictional_killer.characters_killed -> Lionel Luthor -> fictional_universe.fictional_character.character_created_by -> Miles Millar\n# Answer:\nLionel Luthor"], "ground_truth": ["Clancy Brown", "William Hootkins", "James Marsters", "Kevin Spacey", "Chris Noth", "Anthony LaPaglia", "Powers Boothe", "Gene Hackman", "Jesse Eisenberg"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.3157894736842105, "ans_precission": 0.3, "ans_recall": 0.3333333333333333, "path_f1": 0.2608695652173913, "path_precision": 0.3, "path_recall": 0.23076923076923078, "path_ans_f1": 0.3157894736842105, "path_ans_precision": 0.3, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-434", "prediction": ["# Reasoning Path:\nZambezi -> geography.river.origin -> Mwinilunga -> location.location.containedby -> Zambia\n# Answer:\nMwinilunga", "# Reasoning Path:\nZambezi -> geography.river.origin -> Mwinilunga -> common.topic.notable_types -> Location\n# Answer:\nMwinilunga", "# Reasoning Path:\nZambezi -> geography.river.origin -> Mwinilunga -> common.topic.article -> m.07cdmx\n# Answer:\nMwinilunga", "# Reasoning Path:\nZambezi -> geography.river.cities -> Victoria Falls -> travel.tourist_attraction.near_travel_destination -> Livingstone\n# Answer:\nVictoria Falls", "# Reasoning Path:\nZambezi -> geography.river.cities -> Victoria Falls -> location.location.containedby -> Zambia\n# Answer:\nVictoria Falls", "# Reasoning Path:\nZambezi -> geography.river.basin_countries -> Democratic Republic of the Congo -> location.location.containedby -> Africa\n# Answer:\nDemocratic Republic of the Congo", "# Reasoning Path:\nZambezi -> geography.river.cities -> Victoria Falls -> common.topic.notable_for -> g.1258_gg1d\n# Answer:\nVictoria Falls", "# Reasoning Path:\nZambezi -> geography.river.basin_countries -> Democratic Republic of the Congo -> location.location.time_zones -> West Africa Time Zone\n# Answer:\nDemocratic Republic of the Congo", "# Reasoning Path:\nZambezi -> geography.river.cities -> Victoria Falls -> location.location.containedby -> Livingstone\n# Answer:\nVictoria Falls", "# Reasoning Path:\nZambezi -> geography.river.cities -> Kariba, Zimbabwe -> location.location.containedby -> Mashonaland West Province\n# Answer:\nKariba, Zimbabwe"], "ground_truth": ["Mwinilunga"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-436", "prediction": ["# Reasoning Path:\nNigeria -> location.location.time_zones -> West Africa Time Zone\n# Answer:\nWest Africa Time Zone", "# Reasoning Path:\nNigeria -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nNigeria -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Cameroon\n# Answer:\nEnglish Language", "# Reasoning Path:\nNigeria -> location.country.official_language -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_0sgz\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nNigeria -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Kenya\n# Answer:\nEnglish Language", "# Reasoning Path:\nNigeria -> location.country.official_language -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_22x5\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_2gxr\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nNigeria -> location.country.official_language -> English Language -> language.human_language.main_country -> United Kingdom\n# Answer:\nEnglish Language"], "ground_truth": ["West Africa Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-437", "prediction": ["# Reasoning Path:\nJenni Rivera -> people.deceased_person.place_of_death -> Iturbide -> location.location.containedby -> Nuevo Le\u00f3n\n# Answer:\nIturbide", "# Reasoning Path:\nJenni Rivera -> people.deceased_person.place_of_death -> Iturbide -> location.location.geolocation -> m.0wmwfp5\n# Answer:\nIturbide", "# Reasoning Path:\nJenni Rivera -> people.deceased_person.place_of_death -> Iturbide -> common.topic.notable_for -> g.126t0vkgr\n# Answer:\nIturbide", "# Reasoning Path:\nJenni Rivera -> music.artist.album -> Jenni -> common.topic.notable_for -> g.125b9dwrj\n# Answer:\nJenni", "# Reasoning Path:\nJenni Rivera -> music.artist.album -> Jenni -> music.album.genre -> Pop music\n# Answer:\nJenni", "# Reasoning Path:\nJenni Rivera -> common.topic.webpage -> m.05nq1j8 -> common.webpage.resource -> m.0bkghgr\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nJenni Rivera -> music.artist.album -> Jenni -> music.album.genre -> Banda\n# Answer:\nJenni", "# Reasoning Path:\nJenni Rivera -> music.artist.album -> 1 Vida - 3 Historias: Metamorfosis - Despedida de Culican - Jenni Vive 2013 -> common.topic.notable_for -> g.11b75rhc1z\n# Answer:\n1 Vida - 3 Historias: Metamorfosis - Despedida de Culican - Jenni Vive 2013", "# Reasoning Path:\nJenni Rivera -> music.artist.album -> 1969 - Siempre, En Vivo Desde Monterrey, Parte 1 -> music.album.genre -> Latin music\n# Answer:\n1969 - Siempre, En Vivo Desde Monterrey, Parte 1", "# Reasoning Path:\nJenni Rivera -> common.topic.webpage -> m.04q59kl -> common.webpage.resource -> m.0bjwwyj\n# Answer:\ncommon.webpage.resource"], "ground_truth": ["Iturbide"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-438", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.institution -> Harvard Law School\n# Answer:\nHarvard Law School", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkf4 -> education.education.institution -> Stanford University\n# Answer:\nStanford University", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.major_field_of_study -> Law\n# Answer:\nLaw", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0tk1pyb -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkf4 -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.0123vxrw -> education.educational_institution.students_graduates -> m.0123vxqw\n# Answer:\neducation.educational_institution.students_graduates", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0tk1pyb -> film.personal_film_appearance.film -> 8: The Mormon Proposition\n# Answer:\n8: The Mormon Proposition", "# Reasoning Path:\nMitt Romney -> tv.tv_actor.guest_roles -> m.0bvvym_ -> tv.tv_guest_role.episodes_appeared_in -> The Comeback of President Bush\n# Answer:\nThe Comeback of President Bush", "# Reasoning Path:\nMitt Romney -> tv.tv_actor.guest_roles -> m.0bvw9my -> tv.tv_guest_role.episodes_appeared_in -> Mitt Romney, Ryan Sheckler, Jonas Brothers\n# Answer:\nMitt Romney, Ryan Sheckler, Jonas Brothers", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0_gvz4k -> film.personal_film_appearance.film -> Mitt\n# Answer:\nMitt"], "ground_truth": ["Brigham Young University"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-439", "prediction": ["# Reasoning Path:\nSouth Dakota -> location.location.time_zones -> Mountain Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nSouth Dakota -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nSouth Dakota -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nSouth Dakota -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nSouth Dakota -> government.governmental_jurisdiction.agencies -> South Dakota Public Utilities Commission -> common.topic.article -> m.0b7588m\n# Answer:\nSouth Dakota Public Utilities Commission", "# Reasoning Path:\nSouth Dakota -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nSouth Dakota -> government.governmental_jurisdiction.agencies -> South Dakota Public Utilities Commission -> common.topic.notable_types -> Organization\n# Answer:\nSouth Dakota Public Utilities Commission", "# Reasoning Path:\nSouth Dakota -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nSouth Dakota -> government.governmental_jurisdiction.governing_officials -> m.010hbn99 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nSouth Dakota -> government.governmental_jurisdiction.governing_officials -> m.010hbnc6 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor"], "ground_truth": ["Central Time Zone", "Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nSaguaro", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.symbol -> Petrified wood\n# Answer:\nPetrified wood", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz -> location.location_symbol_relationship.symbol -> Mountain tree frog\n# Answer:\nMountain tree frog", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.Kind_of_symbol -> State flower\n# Answer:\nState flower", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.Kind_of_symbol -> State fossil\n# Answer:\nState fossil", "# Reasoning Path:\nArizona -> location.statistical_region.religions -> m.04403h9 -> location.religion_percentage.religion -> Pentecostalism\n# Answer:\nPentecostalism", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.resource -> ATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!\n# Answer:\nATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1l1s -> common.webpage.resource -> ATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!\n# Answer:\nATV spoilers on a 'Supernatural' war, a 'Grey's' same-sex confession, and more!", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz -> location.location_symbol_relationship.Kind_of_symbol -> State Amphibian\n# Answer:\nState Amphibian"], "ground_truth": ["Saguaro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-440", "prediction": ["# Reasoning Path:\nJason Mraz -> music.artist.origin -> Mechanicsville -> location.location.geolocation -> m.0khpj5\n# Answer:\nMechanicsville", "# Reasoning Path:\nJason Mraz -> music.artist.origin -> Mechanicsville -> location.location.containedby -> Virginia\n# Answer:\nMechanicsville", "# Reasoning Path:\nJason Mraz -> music.artist.origin -> California -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nCalifornia", "# Reasoning Path:\nJason Mraz -> music.artist.origin -> Mechanicsville -> location.location.containedby -> Hanover County\n# Answer:\nMechanicsville", "# Reasoning Path:\nJason Mraz -> music.artist.origin -> Mechanicsville -> location.location.people_born_here -> Wayne Grubb\n# Answer:\nMechanicsville", "# Reasoning Path:\nJason Mraz -> people.person.sibling_s -> m.0whv935 -> people.sibling_relationship.sibling -> Candace Mraz\n# Answer:\nCandace Mraz", "# Reasoning Path:\nJason Mraz -> music.artist.origin -> California -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nCalifornia", "# Reasoning Path:\nJason Mraz -> film.actor.film -> m.0j_z9xh -> film.performance.film -> Singing with the Stars\n# Answer:\nSinging with the Stars", "# Reasoning Path:\nJason Mraz -> music.artist.origin -> Mechanicsville -> location.location.people_born_here -> Billy Parker\n# Answer:\nMechanicsville", "# Reasoning Path:\nJason Mraz -> music.artist.origin -> California -> location.administrative_division.country -> United States of America\n# Answer:\nCalifornia"], "ground_truth": ["Mechanicsville", "California"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-441", "prediction": ["# Reasoning Path:\nJohn Bonham -> people.deceased_person.cause_of_death -> Inhalation of vomit -> people.cause_of_death.people -> Tommy Dorsey\n# Answer:\nInhalation of vomit", "# Reasoning Path:\nJohn Bonham -> people.deceased_person.cause_of_death -> Inhalation of vomit -> people.cause_of_death.parent_cause_of_death -> Pulmonary aspiration\n# Answer:\nInhalation of vomit", "# Reasoning Path:\nJohn Bonham -> people.deceased_person.cause_of_death -> Inhalation of vomit -> common.topic.notable_for -> g.1255jz6dx\n# Answer:\nInhalation of vomit", "# Reasoning Path:\nJohn Bonham -> people.deceased_person.cause_of_death -> Inhalation of vomit -> people.cause_of_death.people -> Bon Scott\n# Answer:\nInhalation of vomit", "# Reasoning Path:\nJohn Bonham -> people.deceased_person.cause_of_death -> Inhalation of vomit -> people.cause_of_death.people -> Crash Holly\n# Answer:\nInhalation of vomit", "# Reasoning Path:\nJohn Bonham -> music.artist.track -> Wailing Sounds -> music.recording.artist -> John Paul Jones\n# Answer:\nWailing Sounds", "# Reasoning Path:\nJohn Bonham -> music.artist.track -> Wailing Sounds -> common.topic.notable_for -> g.12598b5f1\n# Answer:\nWailing Sounds", "# Reasoning Path:\nJohn Bonham -> music.artist.track -> 'Cause I Love You -> common.topic.notable_for -> g.125dz2vd4\n# Answer:\n'Cause I Love You", "# Reasoning Path:\nJohn Bonham -> common.topic.webpage -> m.0bnsrz2 -> common.webpage.category -> Curated Topic\n# Answer:\nCurated Topic", "# Reasoning Path:\nJohn Bonham -> common.topic.webpage -> m.03l65bd -> common.webpage.resource -> Led Zeppelin Website\n# Answer:\nLed Zeppelin Website"], "ground_truth": ["Inhalation of vomit"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-442", "prediction": ["# Reasoning Path:\nWalmart -> business.business_operation.industry -> Retail -> business.industry.child_industry -> Beer, Wine, and Liquor Stores\n# Answer:\nRetail", "# Reasoning Path:\nWalmart -> business.business_operation.industry -> Retail -> common.topic.article -> m.0191_h\n# Answer:\nRetail", "# Reasoning Path:\nWalmart -> business.business_operation.industry -> Retail -> business.industry.child_industry -> Building Material and Garden Equipment and Supplies Dealers\n# Answer:\nRetail", "# Reasoning Path:\nWalmart -> business.business_operation.industry -> Retail -> common.topic.subjects -> Chemist Warehouse\n# Answer:\nRetail", "# Reasoning Path:\nWalmart -> business.business_operation.industry -> Retail -> business.industry.child_industry -> Clothing and Clothing Accessories Stores\n# Answer:\nRetail", "# Reasoning Path:\nWalmart -> business.business_operation.industry -> Department store -> business.industry.child_industry -> Department Stores (except Discount Department Stores)\n# Answer:\nDepartment store", "# Reasoning Path:\nWalmart -> business.business_operation.industry -> Variety Stores -> business.industry.parent_industry -> All Other General Merchandise Stores\n# Answer:\nVariety Stores", "# Reasoning Path:\nWalmart -> organization.organization.sectors -> Retail -> business.industry.child_industry -> Beer, Wine, and Liquor Stores\n# Answer:\nRetail", "# Reasoning Path:\nWalmart -> organization.organization.sectors -> Retail -> common.topic.article -> m.0191_h\n# Answer:\nRetail", "# Reasoning Path:\nWalmart -> business.business_operation.industry -> Retail -> common.topic.subjects -> Cigg-e\n# Answer:\nRetail"], "ground_truth": ["Retail", "Department store", "Variety Stores"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-443", "prediction": ["# Reasoning Path:\nTexarkana, Arkansas -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Arkansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nTexarkana, Arkansas -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Arkansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nTexarkana, Arkansas -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Chicago\n# Answer:\nUnited States of America", "# Reasoning Path:\nTexarkana, Arkansas -> location.location.containedby -> Arkansas -> location.location.containedby -> United States of America\n# Answer:\nArkansas", "# Reasoning Path:\nTexarkana, Arkansas -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Arkansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nTexarkana, Arkansas -> location.location.containedby -> Miller County -> location.location.containedby -> Arkansas\n# Answer:\nMiller County", "# Reasoning Path:\nTexarkana, Arkansas -> location.location.containedby -> Arkansas -> base.aareas.schema.administrative_area.administrative_children -> Miller County\n# Answer:\nArkansas", "# Reasoning Path:\nTexarkana, Arkansas -> location.statistical_region.population -> g.11b66f8sth\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nTexarkana, Arkansas -> location.location.containedby -> Arkansas -> location.location.time_zones -> Central Time Zone\n# Answer:\nArkansas", "# Reasoning Path:\nTexarkana, Arkansas -> location.statistical_region.population -> g.11bymnt5fl\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["Miller County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-446", "prediction": ["# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Classical music -> music.genre.subgenre -> Baroque music\n# Answer:\nClassical music", "# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Classical music -> music.genre.subgenre -> Chamber music\n# Answer:\nClassical music", "# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Classical music -> broadcast.genre.content -> 1.FM Otto's  Baroque music\n# Answer:\nClassical music", "# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Opera -> broadcast.genre.content -> 1.FM Otto's Opera House\n# Answer:\nOpera", "# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Classical music -> music.genre.subgenre -> Pop music\n# Answer:\nClassical music", "# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Classical music -> book.book_subject.works -> The Classical Style: Haydn, Mozart, Beethoven\n# Answer:\nClassical music", "# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Classical music -> broadcast.genre.content -> 1.FM Otto's Classical channel\n# Answer:\nClassical music", "# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Opera -> broadcast.genre.content -> Venice Classic Radio\n# Answer:\nOpera", "# Reasoning Path:\nLudwig van Beethoven -> book.book_subject.works -> Beethoven -> common.image.appears_in_topic_gallery -> Missa solemnis\n# Answer:\nBeethoven", "# Reasoning Path:\nLudwig van Beethoven -> book.book_subject.works -> The Classical Style: Haydn, Mozart, Beethoven -> book.written_work.subjects -> Classical music\n# Answer:\nThe Classical Style: Haydn, Mozart, Beethoven"], "ground_truth": ["Classical music", "Opera"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-447", "prediction": ["# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> The Long Winter -> book.written_work.previous_in_series -> By the Shores of Silver Lake\n# Answer:\nThe Long Winter", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> The Long Winter -> fictional_universe.work_of_fiction.setting -> De Smet\n# Answer:\nThe Long Winter", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> Hard Times on the Prairie -> book.written_work.subjects -> United States of America\n# Answer:\nHard Times on the Prairie", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> Little House on the Prairie -> book.written_work.next_in_series -> On the Banks of Plum Creek\n# Answer:\nLittle House on the Prairie", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> Hard Times on the Prairie -> common.topic.notable_for -> g.1255l13vx\n# Answer:\nHard Times on the Prairie", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> Hard Times on the Prairie -> book.book.editions -> Hard Times on the Prairie (Little House Chapter Books)\n# Answer:\nHard Times on the Prairie", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> Little House on the Prairie -> film.film.runtime -> m.062ydzf\n# Answer:\nLittle House on the Prairie", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> Hard Times on the Prairie -> book.written_work.subjects -> 19th century\n# Answer:\nHard Times on the Prairie", "# Reasoning Path:\nLaura Ingalls Wilder -> film.film_story_contributor.film_story_credits -> Bless All the Dear Children -> film.film.film_set_decoration_by -> Donald E. Webb\n# Answer:\nBless All the Dear Children", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> Hard Times on the Prairie -> book.book.editions -> Hard Times on the Prairie (Little House Chapter Books/the Laura Years, 8)\n# Answer:\nHard Times on the Prairie"], "ground_truth": ["The Deer in the Wood", "The Long Winter (Little House)", "On the Banks of Plum Creek (One Cassette)", "Hard Times on the Prairie (Little House Chapter Books/the Laura Years, 8)", "The Deer in the Wood (My First Little House Books (Sagebrush))", "Little Town on the Prairie (Little House Books)", "Laura's Christmas", "These Happy Golden Years (Little House)", "Sugar Snow (My First Little House Books (Sagebrush))", "Little Town On The Prairie (Little House (Original Series Paperback))", "Writings to young women from Laura Ingalls Wilder", "A Little House Birthday", "My Little House Book of Family", "The Complete Little House on the Prairie", "Laura's Garden", "A Little House Sampler", "The Long Winter (Little House (Original Series Paperback))", "Orillas del r\u00edo Plum", "The Little House Collection Box Set (Full Color) (Little House)", "Long Winter (Little House (Original Series Paperback))", "The adventures of Laura and Jack", "Laura & Mr. Edwards (Little House Chapter Book)", "Christmas in the Big Woods", "Santa Comes to Little House", "A Little House Traveler", "Dance at Grandpa's", "Little House in the Big Woods (Isis Large Print for Children Windrush)", "On the banks of Plum Creek", "Little House on the Prairie (Little House)", "Winter Days in the Big Woods", "On the Banks of Plum Creek (Little House the Laura Years)", "Going West (My First Little House Books (Sagebrush))", "Little house on the prairie", "Little House in the Big Woods (Little House (Original Series Paperback))", "Dear Laura: Letters from Children to Laura Ingalls Wilder", "Farmer Boy (Little House)", "Going West (My First Little House)", "Long Winter (Little House (HarperTrophy))", "Dance at Grandpa's (My First Little House)", "My Little House Chapter Book Collection", "Laura & Mr. Edwards", "A FAMILY COLLECTION", "These Happy Golden Years (Little House on the Prairie)", "Farmer Boy", "Little House In The Big Woods CD (Little House the Laura Years)", "The first four years.", "Christmas Stories (Little House Chapter Books)", "My Book of Little House Christmas Paper Dolls: Christmas on the Prairie", "Little House in the Big Woods (Classic Mammoth)", "Going to Town", "School Days (Little House Chapter Book)", "Little House", "Largo Invierno", "Little House in the Ozarks: The Rediscovered Writings", "These happy golden years", "Little House On The Prairie CD (Little House the Laura Years)", "A Little house traveler", "First Four Years (Little House (HarperTrophy))", "Dear Laura", "Little House Farm Days (Little House Chapter Books)", "Little House Friends (Little House Chapter Book)", "Going to Town (My First Little House Books)", "Little House on the Prairie (Little Brown Notebook Series)", "My Book of Little House Paper Dolls", "My Little House Songbook (My First Little House Books)", "Little Town on the Prairie (Little House)", "Little House Parties (Little House Chapter Books)", "My Little House Songbook", "Farmer Boy Days", "My Little House Birthday Book", "By the Shores of Silver Lake (Little House Books)", "Winter on the Farm (My First Little House)", "Little House (9 Books, Boxed Set)", "Laura Helps Pa", "Sugar Snow (My First Little House Books)", "On the Banks of Plum Creek", "School Days", "The First Four Years (Little House Books)", "Laura & Nellie (Little House Chapter Book)", "Little House Friends (Little House Chapter Books/the Laura Years, 9)", "Winter on the Farm", "Little House on the Prairie Tie-in Edition (Little House)", "Laura's Pa", "These Happy Golden Years (Laura Years)", "Little House on the Prairie (Classic Mammoth)", "Little Town on the Prairie", "These Happy Golden Years", "Little House on the Prairie (Little House (HarperTrophy))", "Long Winter", "The Little House Baby Book", "These Happy Golden Years (Little House Books)", "On the Banks of Plum Creek (Little House (Original Series Paperback))", "Little House Parties (Little House Chapter Book)", "The Long Winter", "Pioneer Sisters (Laura (Econo-Clad))", "Little Town on the Prairie CD", "Little House the Laura Years Boxed Set", "Summertime in the Big Woods", "The First Four Years", "Little House In The Big Woods Unabr CD Low Price (Little House the Laura Years)", "Sugar Snow", "Christmas Stories (Little House Chapter Book)", "Santa comes to little house", "School Days (Laura (Econo-Clad))", "A Farmer Boy Birthday", "The Little House Collection", "Pioneer Sisters (Little House Chapter Book)", "Little House on the Prairie Boxed Set ((9 Books) Little House On the Prairie; Farmer Boy; On the Banks of Plum Creek; the Long Winter; These Happy Golden Years; the First Four Years; By the Shores of Silver Lake; Little House In the Big Woods; Little Town On the Prairie)", "Little House in the Big Woods (Little House)", "By the Shores of Silver Lake", "On the Banks of Plum Creek CD (Little House the Laura Years)", "Animal Adventures (Little House Chapter Books)", "Bedtime for Laura", "My Little House Diary", "Christmas Stories (Little House Chapter Books/the Laura Years, 10)", "Little House Parties", "Little House in the Big Woods Book and Charm (Charming Classics)", "Deer in the Wood", "Little House Parties (Little House Chapter Books/the Laura Years, 14)", "Farmer boy", "A Little Prairie House (Little House)", "Laura's Early Years Collection", "Farmer Boy Days (Little House Chapter Book)", "Little House Sisters", "A Little Prairie House", "Going West (My First Little House Books)", "Little House on the Prairie (Little House Books)", "Going West", "Prairie Day (My First Little House)", "Christmas Stories", "Going to Town (My First Little House)", "Caroline and Her Sister", "Little House On The Prairie (Little House (Original Series Paperback))", "First Four Years (Little House (Original Series Paperback))", "A Farmer Boy Birthday (My First Little House Books)", "The First Four Years (Little House the Laura Years)", "Going to town", "Laura's Little House", "Animal Adventures (Little House Chapter Books/the Laura Years, 3)", "The Deer in the Wood (My First Little House Books)", "Laura Ingalls Wilder's prairie wisdom", "West from Home", "My Little House 123", "My Little House Book of Animals", "By the shores of Silver Lake", "The long winter", "A Farmer Boy", "County Fair (My First Little House)", "School days (Little house chapter book)", "Hard Times on the Prairie (Little House Chapter Books)", "These Happy Golden Years CD", "Little House Farm Days (Little House Chapter Books/the Laura Years, 7)", "Prairie Day", "Little House in the Big Woods 75th Anniversary Edition (Little House)", "The first four years", "Winter on the Farm (My First Little House Books)", "A Little House Collection", "These Happy Golden Years (Little House (Original Series Paperback))", "The First Four Years (Little House (Original Series Library))", "Little House in the Big Woods", "These Happy Golden Years (Little House (HarperTrophy))", "A Little House Reader", "A Day on the Prairie", "Little House in the Big Woods.", "Hard Times on the Prairie", "The deer in the wood (My first Little house books)", "Little House in the Big Woods (Little House Books)", "My Little House Songbook (My First Little House Books, No 1)", "Winter on the Farm (My First Little House Books (Sagebrush))", "By the Shores of Silver Lake (Little House (Original Series Paperback))", "Hello, Laura!", "My Little House Book of Memories", "g.122chq7m", "On the Way Home", "Sugar Snow (My First Little House)", "By the Shores of Silver Lake (Little House)", "Farmer Boy Days (Little House Chapter Books)", "Little House on the Prairie", "Little house in the big woods", "A Little house reader", "On the way home", "Little House Friends", "County Fair (My First Little House Books (Sagebrush))", "My Little House Songbook (My First Little House Books Series)", "Animal Adventures", "The Deer in the Wood (Little House)", "Little House on the Prairie NW 247", "Going West (My First Little Houe Books)", "The Long Winter (Little House Books)", "g.11b60fcs_w", "The Adventures of Laura and Jack (A Little House Chapter Book) (A Little house chapter book)", "The Little House on the Prairie", "1998 Laura Ingalls Wilder Country Engagement Book", "A little house sampler", "Little House On The Prairie (Little House the Laura Years)", "Farmer Boy (Little House (Original Series Paperback))", "A little house treasury", "Words from a fearless heart", "The First Four Years CD (Little House the Laura Years)", "Christmas in the Big Woods (Little House)", "A Little Prairie House (My First Little House Books)", "The First Four Years (Little House)", "On the Banks of Plum Creek (Little House)", "Little house on the prairie.", "Farmer Boy (Little House Books)", "Country Fair", "Little House Friends (Little House Chapter Books)", "On the Banks of Plum Creek (Classic Mammoth)", "By the Shores of Silver Lake CD (Little House)", "Little town on the prairie", "Little House on the Prairie Book and Charm (Charming Classics)", "School Days (Little House Chapter Books/the Laura Years, 4)", "Little House Farm Days", "Pioneer Sisters", "Laura & Nellie", "Farmer Boy Days (Little House Chapter Books/the Laura Years, 6)", "Christmas in the Big Woods (My First Little House Books)", "Little House On The Prairie Low Price CD", "Pioneer Sisters (Little House Chapter Books/the Laura Years, 2)", "West from home", "Saving graces", "Little House On The Prairie"], "ans_acc": 0.07207207207207207, "ans_hit": 1, "ans_f1": 0.08579599618684462, "ans_precission": 0.9, "ans_recall": 0.04504504504504504, "path_f1": 0.09809264305177111, "path_precision": 0.9, "path_recall": 0.05187319884726225, "path_ans_f1": 0.1334569045412419, "path_ans_precision": 0.9, "path_ans_recall": 0.07207207207207207}
{"id": "WebQTest-448", "prediction": ["# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Japan\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Turkey\n# Answer:\nParliamentary republic", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Tenerani -> base.thoroughbredracing.thoroughbred_racehorse.foal_if_this_horse_is_sire -> Ribot\n# Answer:\nTenerani", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Albania\n# Answer:\nParliamentary republic", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37cxr\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp"], "ground_truth": ["Unitary state", "Parliamentary republic", "Constitutional republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-449", "prediction": ["# Reasoning Path:\nJohn Irving -> book.author.works_written -> A Prayer for Owen Meany -> book.written_work.previous_in_series -> The Cider House Rules\n# Answer:\nA Prayer for Owen Meany", "# Reasoning Path:\nJohn Irving -> book.author.works_written -> A Prayer for Owen Meany -> book.written_work.next_in_series -> A Son of the Circus\n# Answer:\nA Prayer for Owen Meany", "# Reasoning Path:\nJohn Irving -> book.author.book_editions_published -> A Prayer for Owen Meany -> book.written_work.previous_in_series -> The Cider House Rules\n# Answer:\nA Prayer for Owen Meany", "# Reasoning Path:\nJohn Irving -> book.author.works_written -> The imaginary girlfriend -> common.topic.notable_for -> g.125925tg1\n# Answer:\nThe imaginary girlfriend", "# Reasoning Path:\nJohn Irving -> book.author.works_written -> A Prayer for Owen Meany -> book.book.characters -> Owen Meany\n# Answer:\nA Prayer for Owen Meany", "# Reasoning Path:\nJohn Irving -> book.author.works_written -> The imaginary girlfriend -> common.topic.notable_types -> Book\n# Answer:\nThe imaginary girlfriend", "# Reasoning Path:\nJohn Irving -> book.author.book_editions_published -> A Prayer for Owen Meany -> book.written_work.next_in_series -> A Son of the Circus\n# Answer:\nA Prayer for Owen Meany", "# Reasoning Path:\nJohn Irving -> book.author.works_written -> A Prayer for Owen Meany -> book.book.characters -> Tabby Wheelwright\n# Answer:\nA Prayer for Owen Meany", "# Reasoning Path:\nJohn Irving -> book.author.works_written -> The Cider House Rules -> film.film.edited_by -> Lisa Zeno Churgin\n# Answer:\nThe Cider House Rules", "# Reasoning Path:\nJohn Irving -> people.person.ethnicity -> White American -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Aria Montgomery\n# Answer:\nWhite American"], "ground_truth": ["A Son of the Circus", "In One Person", "The World According to Garp", "Trying to Save Piggy Sneed", "The Fourth Hand", "Water Method Man", "The Cider House Rules", "3 by Irving", "A Widow for One Year", "A Prayer for Owen Meany", "Setting Free the Bears", "Last Night in Twisted River", "The 158-Pound Marriage", "The imaginary girlfriend", "The Hotel New Hampshire", "A Sound Like Someone Trying Not to Make a Sound", "Until I Find You"], "ans_acc": 0.23529411764705882, "ans_hit": 1, "ans_f1": 0.2950819672131148, "ans_precission": 0.9, "ans_recall": 0.17647058823529413, "path_f1": 0.1818181818181818, "path_precision": 0.4, "path_recall": 0.11764705882352941, "path_ans_f1": 0.37305699481865284, "path_ans_precision": 0.9, "path_ans_recall": 0.23529411764705882}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> base.aareas.schema.administrative_area.administrative_parent -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.location.contains -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.country.administrative_divisions -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> common.topic.notable_types -> Country\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.064_ltw -> base.popstra.infidelity.participant -> Kanye West\n# Answer:\nKanye West", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> m.010_ydxv -> music.track_contribution.track -> If I Never See Your Face Again\n# Answer:\nIf I Never See Your Face Again"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-450", "prediction": ["# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.official_symbols -> m.04st848 -> location.location_symbol_relationship.symbol -> Nene\n# Answer:\nNene", "# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.official_symbols -> m.04stk8g -> location.location_symbol_relationship.symbol -> Reef triggerfish\n# Answer:\nReef triggerfish", "# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.official_symbols -> m.04stpzt -> location.location_symbol_relationship.symbol -> Candlenut\n# Answer:\nCandlenut", "# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.official_symbols -> m.04st848 -> location.location_symbol_relationship.Kind_of_symbol -> State bird\n# Answer:\nState bird", "# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.official_symbols -> m.04stk8g -> location.location_symbol_relationship.Kind_of_symbol -> State Fish\n# Answer:\nState Fish", "# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.official_symbols -> m.04stpzt -> location.location_symbol_relationship.Kind_of_symbol -> State tree\n# Answer:\nState tree", "# Reasoning Path:\nHawaii -> book.book_subject.works -> Anatomy of Paradise -> book.written_work.subjects -> Oceania\n# Answer:\nAnatomy of Paradise", "# Reasoning Path:\nHawaii -> book.book_subject.works -> Anatomy of Paradise -> book.book.editions -> Anatomy of paradise: Hawaii and the islands of the South Seas\n# Answer:\nAnatomy of Paradise", "# Reasoning Path:\nHawaii -> book.book_subject.works -> Anatomy of Paradise -> common.topic.article -> m.0b7n8k9\n# Answer:\nAnatomy of Paradise", "# Reasoning Path:\nHawaii -> book.book_subject.works -> Behind That Curtain -> common.topic.notable_for -> g.12596ylqg\n# Answer:\nBehind That Curtain"], "ground_truth": ["Hawaiian hibiscus"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-451", "prediction": ["# Reasoning Path:\nAlice Stokes Paul -> base.activism.activist.area_of_activism -> Women's suffrage -> common.topic.image -> The argument over women's rights in Victoria was lampooned in this Melbourne Punch cartoon of 1887\n# Answer:\nWomen's suffrage", "# Reasoning Path:\nAlice Stokes Paul -> base.activism.activist.area_of_activism -> Women's suffrage -> tv.tv_subject.tv_episodes -> Sufferin' Until Sufferage\n# Answer:\nWomen's suffrage", "# Reasoning Path:\nAlice Stokes Paul -> people.person.profession -> Author -> book.book_subject.works -> Authors Frequently Mentioned on the Web\n# Answer:\nAuthor", "# Reasoning Path:\nAlice Stokes Paul -> base.activism.activist.area_of_activism -> Women's suffrage -> common.topic.image -> Feminist Suffrage Parade in New York City, 1912\n# Answer:\nWomen's suffrage", "# Reasoning Path:\nAlice Stokes Paul -> people.person.profession -> Author -> common.topic.notable_types -> Profession\n# Answer:\nAuthor", "# Reasoning Path:\nAlice Stokes Paul -> common.topic.notable_types -> Organization founder -> freebase.type_profile.kind -> Role\n# Answer:\nOrganization founder", "# Reasoning Path:\nAlice Stokes Paul -> people.person.profession -> Author -> book.book_subject.works -> C-SPAN's LCV 2011 U.S. Cities Tour Wraps up Savannah Shoot, Now in Charleston\n# Answer:\nAuthor", "# Reasoning Path:\nAlice Stokes Paul -> base.activism.activist.area_of_activism -> Women's suffrage -> common.topic.image -> Rose-Sanderson-Votes-for-Women\n# Answer:\nWomen's suffrage", "# Reasoning Path:\nAlice Stokes Paul -> base.activism.activist.area_of_activism -> Women's suffrage -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Taking Liberties: The struggle for Britain's freedom and rights\n# Answer:\nWomen's suffrage", "# Reasoning Path:\nAlice Stokes Paul -> people.person.profession -> Author -> book.book_subject.works -> Contemporary Authors: A Bio-Bibliographical Guide to Current Writers in Fiction, General Nonfiction, Poetry, Journalism, Drama, Motion Pictures, Television. Vol. 249\n# Answer:\nAuthor"], "ground_truth": ["Organization founder"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-453", "prediction": ["# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Jamaica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.main_country -> Jamaica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Jamaican Creole English Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Venezuela\n# Answer:\nSpanish Language", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60wmkvh\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Colombia\n# Answer:\nSpanish Language", "# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.11b71w3_6h\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Ecuador\n# Answer:\nSpanish Language", "# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Spanish Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nSpanish Language"], "ground_truth": ["Jamaican Creole English Language", "Bribri Language", "Spanish Language", "Limonese Creole"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6153846153846154, "ans_precission": 0.8, "ans_recall": 0.5, "path_f1": 0.6153846153846154, "path_precision": 0.8, "path_recall": 0.5, "path_ans_f1": 0.6153846153846154, "path_ans_precision": 0.8, "path_ans_recall": 0.5}
{"id": "WebQTest-454", "prediction": ["# Reasoning Path:\nHank Marvin -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.geolocation -> m.0239t8w\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nHank Marvin -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.containedby -> North East England\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nHank Marvin -> people.person.place_of_birth -> Newcastle upon Tyne -> base.ontologies.ontology_instance.equivalent_instances -> m.09kmh29\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nHank Marvin -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.containedby -> United Kingdom\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nHank Marvin -> music.lyricist.lyrics_written -> Throw Down a Line -> music.composition.recordings -> Throw Down A Line\n# Answer:\nThrow Down a Line", "# Reasoning Path:\nHank Marvin -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.containedby -> England\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nHank Marvin -> music.artist.contribution -> m.01tnlr0 -> music.recording_contribution.album -> XXV\n# Answer:\nXXV", "# Reasoning Path:\nHank Marvin -> music.lyricist.lyrics_written -> Throw Down a Line -> common.topic.notable_for -> g.12q4p8xlj\n# Answer:\nThrow Down a Line", "# Reasoning Path:\nHank Marvin -> music.artist.contribution -> m.01tnlpx -> music.recording_contribution.album -> Moonlight Shadows\n# Answer:\nMoonlight Shadows", "# Reasoning Path:\nHank Marvin -> music.lyricist.lyrics_written -> Throw Down a Line -> common.topic.notable_for -> g.1z2sp_jvl\n# Answer:\nThrow Down a Line"], "ground_truth": ["Newcastle upon Tyne"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-455", "prediction": ["# Reasoning Path:\nChipper Jones -> baseball.baseball_player.batting_stats -> m.06r0lq6 -> baseball.batting_statistics.season -> 1998 Major League Baseball Season\n# Answer:\n1998 Major League Baseball Season", "# Reasoning Path:\nChipper Jones -> sports.drafted_athlete.drafted -> m.04vy30g -> sports.sports_league_draft_pick.draft -> 1990 Major League Baseball Draft\n# Answer:\n1990 Major League Baseball Draft", "# Reasoning Path:\nChipper Jones -> baseball.baseball_player.batting_stats -> m.06r0l_7 -> baseball.batting_statistics.season -> 2004 Major League Baseball season\n# Answer:\n2004 Major League Baseball season", "# Reasoning Path:\nChipper Jones -> baseball.baseball_player.batting_stats -> m.06r0lj8 -> baseball.batting_statistics.season -> 1993 Major League Baseball Season\n# Answer:\n1993 Major League Baseball Season", "# Reasoning Path:\nChipper Jones -> sports.drafted_athlete.drafted -> m.04vy30g -> sports.sports_league_draft_pick.team -> Atlanta Braves\n# Answer:\nAtlanta Braves", "# Reasoning Path:\nChipper Jones -> baseball.baseball_player.batting_stats -> m.06r0lq6 -> baseball.batting_statistics.team -> Atlanta Braves\n# Answer:\nAtlanta Braves", "# Reasoning Path:\nChipper Jones -> award.award_nominee.award_nominations -> m.0z86p0x -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work", "# Reasoning Path:\nChipper Jones -> baseball.baseball_player.batting_stats -> m.06r0l_7 -> baseball.batting_statistics.team -> Atlanta Braves\n# Answer:\nAtlanta Braves", "# Reasoning Path:\nChipper Jones -> baseball.baseball_player.batting_stats -> m.06r0lj8 -> baseball.batting_statistics.team -> Atlanta Braves\n# Answer:\nAtlanta Braves", "# Reasoning Path:\nChipper Jones -> award.award_nominee.award_nominations -> m.0z86p0x -> award.award_nomination.award -> Best Major League Baseball Player ESPY Award\n# Answer:\nBest Major League Baseball Player ESPY Award"], "ground_truth": ["1990 Major League Baseball Draft"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-456", "prediction": ["# Reasoning Path:\nStar Trek -> fictional_universe.fictional_universe.works_set_here -> Star Trek: The Original Series -> tv.tv_program.theme_song -> Theme from Star Trek\n# Answer:\nStar Trek: The Original Series", "# Reasoning Path:\nStar Trek -> fictional_universe.fictional_universe.works_set_here -> Star Trek: The Original Series -> media_common.netflix_title.netflix_genres -> Television\n# Answer:\nStar Trek: The Original Series", "# Reasoning Path:\nWinona Ryder -> common.image.size -> m.04sl_lr\n# Answer:\ncommon.image.size", "# Reasoning Path:\nStar Trek -> fictional_universe.fictional_universe.works_set_here -> Star Trek: The Original Series -> tv.tv_program.program_creator -> Gene Roddenberry\n# Answer:\nStar Trek: The Original Series", "# Reasoning Path:\nStar Trek -> fictional_universe.fictional_universe.works_set_here -> Star Trek VI: The Undiscovered Country -> film.film.prequel -> Star Trek V: The Final Frontier\n# Answer:\nStar Trek VI: The Undiscovered Country", "# Reasoning Path:\nWinona Ryder -> film.actor.film -> m.02t9_t6 -> film.performance.character -> Charlotte Flax\n# Answer:\nCharlotte Flax", "# Reasoning Path:\nWinona Ryder -> film.actor.film -> m.010fw41j -> film.performance.film -> Turks & Caicos\n# Answer:\nTurks & Caicos", "# Reasoning Path:\nStar Trek -> fictional_universe.fictional_universe.works_set_here -> Star Trek II: The Wrath of Khan -> film.film.prequel -> Star Trek: The Motion Picture\n# Answer:\nStar Trek II: The Wrath of Khan", "# Reasoning Path:\nStar Trek -> fictional_universe.fictional_universe.works_set_here -> Star Trek VI: The Undiscovered Country -> film.film.written_by -> Gene Roddenberry\n# Answer:\nStar Trek VI: The Undiscovered Country", "# Reasoning Path:\nWinona Ryder -> film.actor.film -> m.012zk0ck -> film.performance.film -> Experimenter\n# Answer:\nExperimenter"], "ground_truth": ["Amanda Grayson"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-459", "prediction": ["# Reasoning Path:\nJovan Belcher -> people.deceased_person.place_of_death -> Kansas City -> location.location.containedby -> Missouri\n# Answer:\nKansas City", "# Reasoning Path:\nJovan Belcher -> people.deceased_person.place_of_death -> Kansas City -> location.location.containedby -> United States of America\n# Answer:\nKansas City", "# Reasoning Path:\nJovan Belcher -> people.deceased_person.place_of_death -> Kansas City -> location.hud_county_place.county -> Platte County\n# Answer:\nKansas City", "# Reasoning Path:\nJovan Belcher -> people.deceased_person.place_of_death -> Kansas City -> location.place_with_neighborhoods.neighborhoods -> Crown Center\n# Answer:\nKansas City", "# Reasoning Path:\nJovan Belcher -> people.deceased_person.place_of_death -> Kansas City -> location.place_with_neighborhoods.neighborhoods -> 18th and Vine-Downtown East\n# Answer:\nKansas City", "# Reasoning Path:\nJovan Belcher -> people.deceased_person.place_of_death -> Kansas City -> location.place_with_neighborhoods.neighborhoods -> Brookside Park\n# Answer:\nKansas City", "# Reasoning Path:\nJovan Belcher -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nJovan Belcher -> american_football.football_player.games -> m.07sg_19 -> american_football.player_game_statistics.team -> Kansas City Chiefs\n# Answer:\nKansas City Chiefs", "# Reasoning Path:\nJovan Belcher -> american_football.football_player.games -> m.07tc4tg -> american_football.player_game_statistics.team -> Kansas City Chiefs\n# Answer:\nKansas City Chiefs", "# Reasoning Path:\nJovan Belcher -> american_football.football_player.games -> m.07sg_19 -> american_football.player_game_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season"], "ground_truth": ["Kansas City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> Harrison inauguration\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\ncommon.image.size", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.03fx87q -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.03qtjkt\n# Answer:\ncommon.image.size", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> 222px-Harrison_inauguration1841.jpg\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.0bfmkg2 -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.03fx87q -> government.government_position_held.basic_title -> President\n# Answer:\nPresident", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.04j5skj -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor"], "ground_truth": ["1841-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-461", "prediction": ["# Reasoning Path:\nClay Matthews III -> people.person.education -> m.0hprg7d -> education.education.institution -> University of Southern California\n# Answer:\nUniversity of Southern California", "# Reasoning Path:\nClay Matthews III -> people.person.education -> m.0n0g06t -> education.education.institution -> Agoura High School\n# Answer:\nAgoura High School", "# Reasoning Path:\nClay Matthews III -> people.person.place_of_birth -> Northridge -> common.topic.image -> Northridge Shopping District\n# Answer:\nNorthridge", "# Reasoning Path:\nClay Matthews III -> american_football.football_player.forty_yard_dash_time -> m.0hprhl_\n# Answer:\namerican_football.football_player.forty_yard_dash_time", "# Reasoning Path:\nClay Matthews III -> people.person.place_of_birth -> Northridge -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNorthridge", "# Reasoning Path:\nClay Matthews III -> american_football.football_player.forty_yard_dash_time -> m.0wkcwx3\n# Answer:\namerican_football.football_player.forty_yard_dash_time", "# Reasoning Path:\nClay Matthews III -> people.person.place_of_birth -> Northridge -> location.location.people_born_here -> Adam Fortin\n# Answer:\nNorthridge", "# Reasoning Path:\nClay Matthews III -> people.person.place_of_birth -> Northridge -> location.location.people_born_here -> Alphonse Philippe Mouzon\n# Answer:\nNorthridge", "# Reasoning Path:\nClay Matthews III -> people.person.place_of_birth -> Northridge -> location.location.people_born_here -> Alyssa Diaz\n# Answer:\nNorthridge"], "ground_truth": ["University of Southern California", "Agoura High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-462", "prediction": ["# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.statistical_region.population -> g.11b66fk05m\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.statistical_region.population -> g.11b7tcbqts\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nHouston -> location.place_with_neighborhoods.neighborhoods -> Manchester -> common.topic.notable_for -> g.1255tj187\n# Answer:\nManchester", "# Reasoning Path:\nHouston -> location.statistical_region.population -> g.11btt54h79\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nHouston -> location.place_with_neighborhoods.neighborhoods -> Manchester -> location.neighborhood.neighborhood_of -> Greater Houston\n# Answer:\nManchester", "# Reasoning Path:\nHouston -> location.place_with_neighborhoods.neighborhoods -> Addicks -> location.location.containedby -> Harris County\n# Answer:\nAddicks"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-463", "prediction": ["# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.0wc74kc -> film.performance.actor -> Carrie Fisher\n# Answer:\nCarrie Fisher", "# Reasoning Path:\nLeia Organa -> tv.tv_character.appeared_in_tv_program -> m.0h1255z -> tv.regular_tv_appearance.actor -> Amy Smart\n# Answer:\nAmy Smart", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.01xpnt9 -> film.performance.actor -> Carrie Fisher\n# Answer:\nCarrie Fisher", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.0wc74kc -> film.performance.film -> Star Wars: The Force Awakens\n# Answer:\nStar Wars: The Force Awakens", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.010wvf3v -> film.performance.actor -> Eden Espinosa\n# Answer:\nEden Espinosa", "# Reasoning Path:\nLeia Organa -> tv.tv_character.appeared_in_tv_program -> m.0h1255z -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nLeia Organa -> tv.tv_character.appeared_in_tv_program -> m.0gx626p -> tv.regular_tv_appearance.actor -> Carrie Fisher\n# Answer:\nCarrie Fisher", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.01xpnt9 -> film.performance.film -> Star Wars Episode V: The Empire Strikes Back\n# Answer:\nStar Wars Episode V: The Empire Strikes Back", "# Reasoning Path:\nLeia Organa -> tv.tv_character.appeared_in_tv_program -> m.02t8dyx -> tv.regular_tv_appearance.actor -> Carrie Fisher\n# Answer:\nCarrie Fisher", "# Reasoning Path:\nLeia Organa -> book.book_character.appears_in_book -> The Truce at Bakura -> book.book.genre -> Fiction\n# Answer:\nThe Truce at Bakura"], "ground_truth": ["Carrie Fisher"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-464", "prediction": ["# Reasoning Path:\nChina -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Laos\n# Answer:\nSocialist state", "# Reasoning Path:\nChina -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Vietnam\n# Answer:\nSocialist state", "# Reasoning Path:\nChina -> location.country.form_of_government -> Communist state -> government.form_of_government.countries -> Laos\n# Answer:\nCommunist state", "# Reasoning Path:\nChina -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> North Korea\n# Answer:\nSocialist state", "# Reasoning Path:\nChina -> location.country.form_of_government -> Single-party state -> government.form_of_government.countries -> Vietnam\n# Answer:\nSingle-party state", "# Reasoning Path:\nChina -> location.country.form_of_government -> Communist state -> government.form_of_government.countries -> Vietnam\n# Answer:\nCommunist state", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Balikun horse -> common.topic.notable_types -> Organism Classification\n# Answer:\nBalikun horse", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Black Telescope -> biology.animal_breed.breed_of -> Goldfish\n# Answer:\nBlack Telescope", "# Reasoning Path:\nChina -> location.location.partially_contains -> Amur River -> location.location.partially_containedby -> Russia\n# Answer:\nAmur River", "# Reasoning Path:\nChina -> location.country.form_of_government -> Single-party state -> government.form_of_government.countries -> North Korea\n# Answer:\nSingle-party state"], "ground_truth": ["Communist state", "Single-party state", "Socialist state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-465", "prediction": ["# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> aviation.airport.serves -> Viceroy New York\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> location.location.containedby -> United States of America\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> LaGuardia Airport -> common.topic.notable_types -> Airport\n# Answer:\nLaGuardia Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> travel.transport_terminus.travel_destinations_served -> m.0519ccq\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> Flushing Airport -> common.topic.notable_types -> Airport\n# Answer:\nFlushing Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> aviation.airport.serves -> MBSF - Private Jets\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> LaGuardia Airport -> aviation.airport.serves -> New York metropolitan area\n# Answer:\nLaGuardia Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> aviation.airport.serves -> Queens\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> LaGuardia Airport -> travel.transport_terminus.travel_destinations_served -> m.0519ccq\n# Answer:\nLaGuardia Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> location.location.containedby -> New York\n# Answer:\nJohn F. Kennedy International Airport"], "ground_truth": ["John F. Kennedy International Airport", "LaGuardia Airport", "Newark Liberty International Airport"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7659574468085106, "ans_precission": 0.9, "ans_recall": 0.6666666666666666, "path_f1": 0.7659574468085106, "path_precision": 0.9, "path_recall": 0.6666666666666666, "path_ans_f1": 0.7659574468085106, "path_ans_precision": 0.9, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-466", "prediction": ["# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Brazilian Portuguese -> language.language_dialect.language -> Portuguese Language\n# Answer:\nBrazilian Portuguese", "# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Portuguese Language -> language.human_language.dialects -> Brazilian Portuguese\n# Answer:\nPortuguese Language", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc39sjp\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fm01\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc385hl\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp"], "ground_truth": ["Portuguese Language", "Brazilian Portuguese", "Italian Language"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.36363636363636365, "ans_precission": 0.25, "ans_recall": 0.6666666666666666, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 0.36363636363636365, "path_ans_precision": 0.25, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-467", "prediction": ["# Reasoning Path:\nObi-Wan Kenobi -> film.film_character.portrayed_in_films -> m.0k3qy3 -> film.performance.actor -> Ewan McGregor\n# Answer:\nEwan McGregor", "# Reasoning Path:\nObi-Wan Kenobi -> film.film_character.portrayed_in_films -> m.0k3r05 -> film.performance.actor -> Ewan McGregor\n# Answer:\nEwan McGregor", "# Reasoning Path:\nObi-Wan Kenobi -> film.film_character.portrayed_in_films -> m.0k3qzt -> film.performance.actor -> Ewan McGregor\n# Answer:\nEwan McGregor", "# Reasoning Path:\nObi-Wan Kenobi -> common.topic.notable_for -> g.1258f4t33\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nObi-Wan Kenobi -> film.film_character.portrayed_in_films -> m.0k3qy3 -> film.performance.film -> Star Wars Episode I: The Phantom Menace\n# Answer:\nStar Wars Episode I: The Phantom Menace", "# Reasoning Path:\nObi-Wan Kenobi -> film.film_character.portrayed_in_films -> m.0k3r05 -> film.performance.film -> Star Wars Episode III: Revenge of the Sith\n# Answer:\nStar Wars Episode III: Revenge of the Sith", "# Reasoning Path:\nObi-Wan Kenobi -> film.film_character.portrayed_in_films -> m.0k3qzt -> film.performance.film -> Star Wars Episode II: Attack of the Clones\n# Answer:\nStar Wars Episode II: Attack of the Clones", "# Reasoning Path:\nObi-Wan Kenobi -> common.topic.webpage -> m.09xlg53 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nObi-Wan Kenobi -> common.topic.webpage -> m.09xlg53 -> common.webpage.resource -> Trend Watch: Who has the best celebrity beard?\n# Answer:\nTrend Watch: Who has the best celebrity beard?", "# Reasoning Path:\nObi-Wan Kenobi -> common.topic.webpage -> m.09xsm5h -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Ewan McGregor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-468", "prediction": ["# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.location.containedby -> United States of America\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.location.containedby -> California\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.location.people_born_here -> Ace Mask\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.location.containedby -> Los Angeles County\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.statistical_region.population -> g.11b66hc7d0\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.location.people_born_here -> Art Olivier\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> common.topic.webpage -> m.09xxg8l -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.location.people_born_here -> Big Van Vader\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.statistical_region.population -> g.11bymm5pgf\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> common.topic.webpage -> m.09ykq8v -> common.webpage.resource -> What's your favorite tennis-related movie moment?\n# Answer:\nWhat's your favorite tennis-related movie moment?"], "ground_truth": ["Lynwood"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-469", "prediction": ["# Reasoning Path:\nPanama -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> United States of America\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPanama -> location.country.currency_used -> United States Dollar -> finance.currency.countries_formerly_used -> Cuba\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPanama -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> Ecuador\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPanama -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> El Salvador\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPanama -> location.country.currency_used -> United States Dollar -> base.coinsdaily.coin_type.country -> United States of America\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPanama -> location.statistical_region.official_development_assistance -> g.12cp_k2q2\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nPanama -> location.country.currency_used -> Panamanian balboa -> common.topic.image -> \u00bd balboa(front)\n# Answer:\nPanamanian balboa", "# Reasoning Path:\nPanama -> location.statistical_region.official_development_assistance -> g.1hhc37pjb\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nPanama -> location.country.currency_used -> Panamanian balboa -> common.topic.article -> m.0200cx\n# Answer:\nPanamanian balboa", "# Reasoning Path:\nPanama -> location.statistical_region.official_development_assistance -> g.1hhc37pjc\n# Answer:\nlocation.statistical_region.official_development_assistance"], "ground_truth": ["United States Dollar", "Panamanian balboa"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> United States of America\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> common.topic.image -> Ryan Braun\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> California\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> Los Angeles\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Ryan Braun\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.03lppm1 -> common.webpage.resource -> m.0bjrztw\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Alan Muraoka\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.0948qtd -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Chris Snail\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.03lppm1 -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website"], "ground_truth": ["Mission Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-470", "prediction": ["# Reasoning Path:\nZambezi -> geography.river.origin -> Mwinilunga -> location.location.containedby -> Zambia\n# Answer:\nMwinilunga", "# Reasoning Path:\nZambezi -> geography.river.origin -> Mwinilunga -> common.topic.notable_types -> Location\n# Answer:\nMwinilunga", "# Reasoning Path:\nZambezi -> geography.river.origin -> Mwinilunga -> common.topic.article -> m.07cdmx\n# Answer:\nMwinilunga", "# Reasoning Path:\nZambezi -> geography.river.cities -> Victoria Falls -> travel.tourist_attraction.near_travel_destination -> Livingstone\n# Answer:\nVictoria Falls", "# Reasoning Path:\nZambezi -> geography.river.basin_countries -> Democratic Republic of the Congo -> location.location.containedby -> Africa\n# Answer:\nDemocratic Republic of the Congo", "# Reasoning Path:\nZambezi -> geography.river.cities -> Victoria Falls -> location.location.containedby -> Zambia\n# Answer:\nVictoria Falls", "# Reasoning Path:\nZambezi -> geography.river.cities -> Victoria Falls -> common.topic.notable_for -> g.1258_gg1d\n# Answer:\nVictoria Falls", "# Reasoning Path:\nZambezi -> geography.river.basin_countries -> Democratic Republic of the Congo -> location.location.time_zones -> West Africa Time Zone\n# Answer:\nDemocratic Republic of the Congo", "# Reasoning Path:\nZambezi -> geography.river.basin_countries -> Zimbabwe -> location.location.containedby -> Africa\n# Answer:\nZimbabwe", "# Reasoning Path:\nZambezi -> geography.river.cities -> Kariba, Zimbabwe -> location.location.containedby -> Zimbabwe\n# Answer:\nKariba, Zimbabwe"], "ground_truth": ["Mwinilunga"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-472", "prediction": ["# Reasoning Path:\nRonaldo -> sports.pro_athlete.teams -> m.052b2p8 -> sports.sports_team_roster.team -> Sport Club Corinthians Paulista\n# Answer:\nSport Club Corinthians Paulista", "# Reasoning Path:\nRonaldo -> sports.pro_athlete.teams -> m.0nh6z4r -> sports.sports_team_roster.team -> Cruzeiro Esporte Clube\n# Answer:\nCruzeiro Esporte Clube", "# Reasoning Path:\nRonaldo -> soccer.football_player.statistics -> m.0w8_xw6 -> soccer.football_player_stats.team -> Sport Club Corinthians Paulista\n# Answer:\nSport Club Corinthians Paulista", "# Reasoning Path:\nRonaldo -> sports.pro_athlete.teams -> m.0nh6z71 -> sports.sports_team_roster.team -> PSV Eindhoven\n# Answer:\nPSV Eindhoven", "# Reasoning Path:\nRonaldo -> soccer.football_player.statistics -> m.0w9ntpb -> soccer.football_player_stats.team -> Cruzeiro Esporte Clube\n# Answer:\nCruzeiro Esporte Clube", "# Reasoning Path:\nRonaldo -> sports.pro_athlete.teams -> m.052b2p8 -> sports.sports_team_roster.position -> Forward\n# Answer:\nForward", "# Reasoning Path:\nRonaldo -> sports.pro_athlete.teams -> m.0nh6z4r -> sports.sports_team_roster.position -> Forward\n# Answer:\nForward", "# Reasoning Path:\nRonaldo -> soccer.football_player.statistics -> m.0w9ntpn -> soccer.football_player_stats.team -> PSV Eindhoven\n# Answer:\nPSV Eindhoven", "# Reasoning Path:\nRonaldo -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#range -> Person\n# Answer:\nChildren", "# Reasoning Path:\nRonaldo -> sports.pro_athlete.teams -> m.0nh6z71 -> sports.sports_team_roster.position -> Forward\n# Answer:\nForward"], "ground_truth": ["Real Madrid C.F.", "Brazil national football team"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-473", "prediction": ["# Reasoning Path:\nRobin Williams -> people.person.places_lived -> m.0bh5kjj -> people.place_lived.location -> San Francisco\n# Answer:\nSan Francisco", "# Reasoning Path:\nRobin Williams -> people.person.places_lived -> m.03pqrmt -> people.place_lived.location -> Bloomfield Hills\n# Answer:\nBloomfield Hills", "# Reasoning Path:\nRobin Williams -> film.producer.film -> Mrs. Doubtfire -> film.film.featured_film_locations -> San Francisco\n# Answer:\nMrs. Doubtfire", "# Reasoning Path:\nRobin Williams -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> San Francisco\n# Answer:\nUnited States of America", "# Reasoning Path:\nRobin Williams -> film.producer.film -> Mrs. Doubtfire -> film.film.featured_film_locations -> Oakland\n# Answer:\nMrs. Doubtfire", "# Reasoning Path:\nRobin Williams -> film.producer.film -> Mrs. Doubtfire -> award.award_nominated_work.award_nominations -> m.08__619\n# Answer:\nMrs. Doubtfire", "# Reasoning Path:\nRobin Williams -> people.person.nationality -> United States of America -> common.topic.notable_types -> Country\n# Answer:\nUnited States of America", "# Reasoning Path:\nRobin Williams -> film.producer.film -> Mrs. Doubtfire -> film.film.featured_film_locations -> Transamerica Pyramid\n# Answer:\nMrs. Doubtfire", "# Reasoning Path:\nRobin Williams -> film.producer.film -> Mrs. Doubtfire -> film.film.starring -> m.03js_lj\n# Answer:\nMrs. Doubtfire", "# Reasoning Path:\nRobin Williams -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> California\n# Answer:\nUnited States of America"], "ground_truth": ["San Francisco", "Bloomfield Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.5333333333333333, "path_precision": 0.4, "path_recall": 0.8, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-474", "prediction": ["# Reasoning Path:\nTwilight -> award.award_winning_work.awards_won -> m.0nfjtkv -> award.award_honor.award_winner -> Taylor Lautner\n# Answer:\nTaylor Lautner", "# Reasoning Path:\nJacob Black -> film.film_character.portrayed_in_films -> m.0djz10s -> film.performance.actor -> Taylor Lautner\n# Answer:\nTaylor Lautner", "# Reasoning Path:\nJacob Black -> film.film_character.portrayed_in_films -> m.0b68zn5 -> film.performance.actor -> Taylor Lautner\n# Answer:\nTaylor Lautner", "# Reasoning Path:\nJacob Black -> film.film_character.portrayed_in_films -> m.04z4zmg -> film.performance.actor -> Taylor Lautner\n# Answer:\nTaylor Lautner", "# Reasoning Path:\nTwilight -> award.award_winning_work.awards_won -> m.01086803 -> award.award_honor.award_winner -> Taylor Lautner\n# Answer:\nTaylor Lautner", "# Reasoning Path:\nTwilight -> award.award_winning_work.awards_won -> m.0nfjtkv -> award.award_honor.ceremony -> 2009 Teen Choice Awards\n# Answer:\n2009 Teen Choice Awards", "# Reasoning Path:\nJacob Black -> film.film_character.portrayed_in_films -> m.0djz10s -> film.performance.film -> The Twilight Saga: Breaking Dawn - Part 2\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 2", "# Reasoning Path:\nJacob Black -> book.book_character.appears_in_book -> Twilight -> award.award_winning_work.awards_won -> m.0nfjtkv\n# Answer:\naward.award_winning_work.awards_won", "# Reasoning Path:\nJacob Black -> film.film_character.portrayed_in_films -> m.0b68zn5 -> film.performance.film -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nJacob Black -> film.film_character.portrayed_in_films -> m.04z4zmg -> film.performance.film -> Twilight\n# Answer:\nfilm.performance.film"], "ground_truth": ["Taylor Lautner"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-477", "prediction": ["# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Ministro Pistarini International Airport -> aviation.airport.serves -> Ezeiza, Buenos Aires\n# Answer:\nMinistro Pistarini International Airport", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Ministro Pistarini International Airport -> common.topic.article -> m.01ky3n\n# Answer:\nMinistro Pistarini International Airport", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Ministro Pistarini International Airport -> aviation.airport.serves -> Greater Buenos Aires\n# Answer:\nMinistro Pistarini International Airport", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Ministro Pistarini International Airport -> common.topic.webpage -> m.03l9zc6\n# Answer:\nMinistro Pistarini International Airport", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Don Torcuato Airport -> location.location.containedby -> Argentina\n# Answer:\nDon Torcuato Airport", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Ministro Pistarini International Airport -> common.topic.webpage -> m.040ql4w\n# Answer:\nMinistro Pistarini International Airport", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Aeroparque Jorge Newbery -> aviation.airport.hub_for -> Aerol\u00edneas Argentinas\n# Answer:\nAeroparque Jorge Newbery", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Ministro Pistarini International Airport -> common.topic.webpage -> m.05nn9j7\n# Answer:\nMinistro Pistarini International Airport", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Aeroparque Jorge Newbery -> location.location.containedby -> Palermo, Buenos Aires\n# Answer:\nAeroparque Jorge Newbery", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Don Torcuato Airport -> common.topic.notable_for -> g.12565x5lm\n# Answer:\nDon Torcuato Airport"], "ground_truth": ["Aeroparque Jorge Newbery", "Don Torcuato Airport", "Ministro Pistarini International Airport"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-479", "prediction": ["# Reasoning Path:\nAdrian Peterson -> sports.pro_athlete.teams -> m.03gkk7c -> sports.sports_team_roster.team -> Minnesota Vikings\n# Answer:\nMinnesota Vikings", "# Reasoning Path:\nAdrian Peterson -> sports.pro_athlete.teams -> m.03gkk7c -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nAdrian Peterson -> sports.pro_athlete.teams -> m.03gkk7c -> sports.sports_team_roster.position -> Running back\n# Answer:\nRunning back", "# Reasoning Path:\nAdrian Peterson -> award.award_nominee.award_nominations -> m.0_qwmrv -> award.award_nomination.award -> NFL Play of the Year Award\n# Answer:\nNFL Play of the Year Award", "# Reasoning Path:\nAdrian Peterson -> award.award_winner.awards_won -> m.04kt42p -> award.award_honor.award -> National Football League Rookie of the Year Award\n# Answer:\nNational Football League Rookie of the Year Award", "# Reasoning Path:\nAdrian Peterson -> award.award_nominee.award_nominations -> m.0_qwmrv -> freebase.valuenotation.has_value -> Nominated work\n# Answer:\nNominated work", "# Reasoning Path:\nAdrian Peterson -> award.award_nominee.award_nominations -> m.0_qxjbh -> award.award_nomination.ceremony -> 2nd Annual NFL Honors\n# Answer:\n2nd Annual NFL Honors", "# Reasoning Path:\nAdrian Peterson -> award.award_nominee.award_nominations -> m.0_qwmrv -> award.award_nomination.ceremony -> 3rd Annual NFL Honors\n# Answer:\n3rd Annual NFL Honors", "# Reasoning Path:\nAdrian Peterson -> award.award_nominee.award_nominations -> m.0_qxkv3 -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work", "# Reasoning Path:\nAdrian Peterson -> award.award_winner.awards_won -> m.0_qxb1r -> award.award_honor.ceremony -> 2nd Annual NFL Honors\n# Answer:\n2nd Annual NFL Honors"], "ground_truth": ["University of Oklahoma"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.administrative_division.country -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.containedby -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.citytown -> Suwon\n# Answer:\nSuwon", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.country -> South Korea\n# Answer:\nSouth Korea", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.statistical_region.population -> g.11b7tcbqv4\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.postal_code -> 443-742\n# Answer:\n443-742", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Mahaka Media\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.statistical_region.population -> g.11b7tm_xvh\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Textron\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.statistical_region.population -> g.11b7tm_xwq\n# Answer:\nDaegu"], "ground_truth": ["Suwon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-480", "prediction": ["# Reasoning Path:\nPennsylvania -> symbols.namesake.named_after -> William Penn -> people.person.place_of_birth -> Bristol\n# Answer:\nWilliam Penn", "# Reasoning Path:\nPennsylvania -> symbols.namesake.named_after -> William Penn -> people.person.parents -> Giles Penn\n# Answer:\nWilliam Penn", "# Reasoning Path:\nPennsylvania -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> West Virginia\n# Answer:\nUnited States of America", "# Reasoning Path:\nPennsylvania -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Beryl\n# Answer:\nUnited States of America", "# Reasoning Path:\nPennsylvania -> symbols.namesake.named_after -> William Penn -> people.person.parents -> Joan Gilbert\n# Answer:\nWilliam Penn", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.title -> Director, Office of Health Care Reform\n# Answer:\nDirector, Office of Health Care Reform", "# Reasoning Path:\nPennsylvania -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nPennsylvania -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Arizona\n# Answer:\nUnited States of America", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.person -> Rosemarie B. Greco\n# Answer:\nRosemarie B. Greco", "# Reasoning Path:\nPennsylvania -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> meteorology.cyclone_affected_area.cyclones -> Hurricane Isabel\n# Answer:\nUnited States of America"], "ground_truth": ["William Penn"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-481", "prediction": ["# Reasoning Path:\nGermany -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_38m_\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fdl8\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc38c_4\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_67jr\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_jlz4\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-482", "prediction": ["# Reasoning Path:\nBatman -> film.film.starring -> m.02tb1p0 -> film.performance.character -> g.1232h_22\n# Answer:\nfilm.performance.character", "# Reasoning Path:\nBatman -> film.film.starring -> m.02wk6dq -> film.performance.character -> Bruce Wayne\n# Answer:\nBruce Wayne", "# Reasoning Path:\nBatman -> film.film.film_series -> Batman Film Series -> common.topic.article -> m.02rx1t3\n# Answer:\nBatman Film Series", "# Reasoning Path:\nBatman -> film.film.other_crew -> m.0v9503y -> film.film_crew_gig.crewmember -> Lynda Armstrong\n# Answer:\nLynda Armstrong", "# Reasoning Path:\nBatman -> film.film.other_crew -> m.0v9506s -> film.film_crew_gig.crewmember -> Nick Dudman\n# Answer:\nNick Dudman", "# Reasoning Path:\nBatman -> film.film.starring -> m.02tb1p0 -> film.performance.actor -> Jack Palance\n# Answer:\nJack Palance", "# Reasoning Path:\nBatman -> film.film.film_series -> Batman Film Series -> award.award_nominated_work.award_nominations -> m.0ndvr43\n# Answer:\nBatman Film Series", "# Reasoning Path:\nBatman -> film.film.other_crew -> m.0v9503y -> film.film_crew_gig.film_crew_role -> Makeup Artist\n# Answer:\nMakeup Artist", "# Reasoning Path:\nBatman -> film.film.other_crew -> m.0v9508w -> film.film_crew_gig.film_crew_role -> Chief Makeup Artist\n# Answer:\nChief Makeup Artist", "# Reasoning Path:\nBatman -> film.film.film_series -> Batman Film Series -> common.topic.article -> m.0gyjf78\n# Answer:\nBatman Film Series"], "ground_truth": ["Danny Trejo", "Carlos Alazraqui", "Tom Hardy", "Matthew Wagner"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-484", "prediction": ["# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> Japan\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars"], "ground_truth": ["Esperanto Language", "English Language", "Lojban"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-487", "prediction": ["# Reasoning Path:\nVeronica Lake -> people.person.children -> Andre Michael De Toth III -> people.person.parents -> Andr\u00e9 de Toth\n# Answer:\nAndre Michael De Toth III", "# Reasoning Path:\nVeronica Lake -> people.person.children -> Elaine Detlie -> people.person.parents -> John S. Detlie\n# Answer:\nElaine Detlie", "# Reasoning Path:\nVeronica Lake -> people.person.children -> Andre Michael De Toth III -> people.deceased_person.place_of_death -> Olympia\n# Answer:\nAndre Michael De Toth III", "# Reasoning Path:\nVeronica Lake -> people.person.spouse_s -> m.02kkn6n -> people.marriage.spouse -> Andr\u00e9 de Toth\n# Answer:\nAndr\u00e9 de Toth", "# Reasoning Path:\nVeronica Lake -> people.person.children -> Diana De Toth -> people.person.parents -> Andr\u00e9 de Toth\n# Answer:\nDiana De Toth", "# Reasoning Path:\nVeronica Lake -> people.person.spouse_s -> m.03lf38m -> people.marriage.spouse -> John S. Detlie\n# Answer:\nJohn S. Detlie", "# Reasoning Path:\nVeronica Lake -> people.person.children -> Andre Michael De Toth III -> people.person.sibling_s -> m.0qzbn8m\n# Answer:\nAndre Michael De Toth III", "# Reasoning Path:\nVeronica Lake -> people.person.children -> Elaine Detlie -> people.person.gender -> Female\n# Answer:\nElaine Detlie", "# Reasoning Path:\nVeronica Lake -> people.person.spouse_s -> m.0h2qr4m -> people.marriage.spouse -> Joseph A. McCarthy\n# Answer:\nJoseph A. McCarthy", "# Reasoning Path:\nVeronica Lake -> people.person.parents -> Harry E. Ockelman -> people.person.spouse_s -> m.0ztrgpz\n# Answer:\nHarry E. Ockelman"], "ground_truth": ["John S. Detlie", "Andr\u00e9 de Toth", "Joseph A. McCarthy", "Robert Carleton-Munro"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.4285714285714285, "ans_precission": 0.3, "ans_recall": 0.75, "path_f1": 0.6666666666666665, "path_precision": 0.6, "path_recall": 0.75, "path_ans_f1": 0.6666666666666665, "path_ans_precision": 0.6, "path_ans_recall": 0.75}
{"id": "WebQTest-488", "prediction": ["# Reasoning Path:\nFrancis Drake -> people.person.spouse_s -> m.0k1nmz1 -> people.marriage.spouse -> Mary Newman\n# Answer:\nMary Newman", "# Reasoning Path:\nFrancis Drake -> people.person.spouse_s -> m.0k1nmzh -> people.marriage.spouse -> Elizabeth Sydenham\n# Answer:\nElizabeth Sydenham", "# Reasoning Path:\nFrancis Drake -> people.person.spouse_s -> m.0k1nmz1 -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nFrancis Drake -> people.person.spouse_s -> m.0k1nmzh -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nFrancis Drake -> military.military_commander.military_commands -> m.0j2c6ft -> military.military_command.military_combatant -> Kingdom of England\n# Answer:\nKingdom of England", "# Reasoning Path:\nFrancis Drake -> base.kwebbase.kwtopic.has_sentences -> After a daring raid on the mule train carrying treasure overland from Panama to Nombre de Dios,  Drake felt rich enough to return home. -> base.kwebbase.kwsentence.previous_sentence -> The pinnaces played a vital part in ferrying raiding parties and their plunder on and off the South American mainland.\n# Answer:\nAfter a daring raid on the mule train carrying treasure overland from Panama to Nombre de Dios,  Drake felt rich enough to return home.", "# Reasoning Path:\nFrancis Drake -> base.kwebbase.kwtopic.has_sentences -> After a hostile reception in the Caroline Islands they reached the Moluccas where the Sultan agreed to trade with England and allowed Drake to sail off with a profitable cargo: six tons of cloves. -> base.kwebbase.kwsentence.next_sentence -> \\\"The Golden Hind\\\" survived running aground on a coral reef and Drake visited Java before crossing the Indian Ocean and round the Cape of Good Hope into the Atlantic.\n# Answer:\nAfter a hostile reception in the Caroline Islands they reached the Moluccas where the Sultan agreed to trade with England and allowed Drake to sail off with a profitable cargo: six tons of cloves.", "# Reasoning Path:\nFrancis Drake -> military.military_commander.military_commands -> m.0j2c6ft -> military.military_command.military_conflict -> Battle of Pinos\n# Answer:\nBattle of Pinos", "# Reasoning Path:\nFrancis Drake -> base.kwebbase.kwtopic.has_sentences -> After a daring raid on the mule train carrying treasure overland from Panama to Nombre de Dios,  Drake felt rich enough to return home. -> base.kwebbase.kwsentence.next_sentence -> His ships were loaded up with enough gold to crew and equip thirty warships and he flew the English flag on his mainmast as he sailed (out of gun range) past Cartagena.\n# Answer:\nAfter a daring raid on the mule train carrying treasure overland from Panama to Nombre de Dios,  Drake felt rich enough to return home.", "# Reasoning Path:\nFrancis Drake -> military.military_commander.military_commands -> m.05sp345 -> military.military_command.military_conflict -> Battle of Gravelines (1588)\n# Answer:\nBattle of Gravelines (1588)"], "ground_truth": ["Mary Newman", "Elizabeth Sydenham"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-489", "prediction": ["# Reasoning Path:\nOne for the Money -> film.film.starring -> m.0gv_fdr -> film.performance.actor -> Katherine Heigl\n# Answer:\nKatherine Heigl", "# Reasoning Path:\nOne for the Money -> film.film.starring -> m.0gv_r2n -> film.performance.actor -> Patrick Fischler\n# Answer:\nPatrick Fischler", "# Reasoning Path:\nOne for the Money -> film.film.starring -> m.0gvwwl8 -> film.performance.actor -> Daniel Sunjata\n# Answer:\nDaniel Sunjata", "# Reasoning Path:\nOne for the Money -> film.film.starring -> m.0gv_fdr -> film.performance.character -> Stephanie Plum\n# Answer:\nStephanie Plum", "# Reasoning Path:\nOne for the Money -> film.film.release_date_s -> m.0jsmys9 -> film.film_regional_release_date.film_release_region -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nOne for the Money -> film.film.other_crew -> m.0gwdhjn -> film.film_crew_gig.crewmember -> David Lingenfelser\n# Answer:\nDavid Lingenfelser", "# Reasoning Path:\nOne for the Money -> film.film.starring -> m.0gv_r2n -> film.performance.character -> Vinnie Plum\n# Answer:\nVinnie Plum", "# Reasoning Path:\nOne for the Money -> film.film.release_date_s -> m.0jsmys9 -> film.film_regional_release_date.film_release_distribution_medium -> Theatrical\n# Answer:\nTheatrical", "# Reasoning Path:\nOne for the Money -> film.film.other_crew -> m.0gwdhg0 -> film.film_crew_gig.film_crew_role -> Key Makeup Artist\n# Answer:\nKey Makeup Artist", "# Reasoning Path:\nOne for the Money -> film.film.starring -> m.0gvwwl8 -> film.performance.character -> Ranger\n# Answer:\nRanger"], "ground_truth": ["Katherine Heigl"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Hinduism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.beliefs -> End time\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.article -> m.055v4j\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.image -> Ramapir01\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> common.topic.article -> m.0jfq\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Jainism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> God -> common.topic.notable_for -> g.125d2mw4b\n# Answer:\nGod", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.image -> Ramapir22\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> religion.deity.deity_of -> Sunni Islam\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> base.schemastaging.context_name.pronunciation -> g.125_r5my9\n# Answer:\nAbrahamic religions"], "ground_truth": ["Qiyamah", "Mahdi", "Prophets in Islam", "Monotheism", "\u1e6c\u016bb\u0101", "Islamic view of angels", "Entering Heaven alive", "Tawhid", "Masih ad-Dajjal", "Islamic holy books", "God in Islam", "Predestination in Islam", "Sharia"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-490", "prediction": ["# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Greek Language -> language.human_language.writing_system -> Greek alphabet\n# Answer:\nGreek Language", "# Reasoning Path:\nRoman Empire -> location.country.official_language -> Latin Language -> language.human_language.countries_spoken_in -> Duchy of Burgundy\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Latin Language -> language.human_language.countries_spoken_in -> Duchy of Burgundy\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Latin Language -> language.human_language.language_family -> Italic languages\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> location.country.official_language -> Latin Language -> language.human_language.countries_spoken_in -> Republic of Genoa\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Greek Language -> media_common.netflix_genre.titles -> Young Aphrodites\n# Answer:\nGreek Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Greek Language -> media_common.netflix_genre.titles -> A Girl in Black\n# Answer:\nGreek Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Latin Language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Latin Language -> language.human_language.countries_spoken_in -> Republic of Genoa\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Latin Language -> music.genre.albums -> Acustico\n# Answer:\nLatin Language"], "ground_truth": ["Latin Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-491", "prediction": ["# Reasoning Path:\nRingo Starr -> people.person.profession -> Singer -> common.topic.subject_of -> Alan Motley\n# Answer:\nSinger", "# Reasoning Path:\nRingo Starr -> people.person.parents -> Richard Starkey -> common.topic.notable_for -> g.125h3_b65\n# Answer:\nRichard Starkey", "# Reasoning Path:\nRingo Starr -> music.artist.contribution -> m.0vv1_m9 -> music.recording_contribution.album -> Cloud Nine\n# Answer:\nCloud Nine", "# Reasoning Path:\nRingo Starr -> people.person.profession -> Singer -> base.lightweight.profession.specialization_of -> Musicians and Singers\n# Answer:\nSinger", "# Reasoning Path:\nRingo Starr -> people.person.profession -> Actor -> people.profession.specializations -> Narrator\n# Answer:\nActor", "# Reasoning Path:\nRingo Starr -> music.artist.contribution -> m.0vv1lmv -> music.recording_contribution.album -> Is This What You Want?\n# Answer:\nIs This What You Want?", "# Reasoning Path:\nRingo Starr -> people.person.profession -> Singer -> people.profession.specializations -> Singer-songwriter\n# Answer:\nSinger", "# Reasoning Path:\nRingo Starr -> people.person.parents -> Richard Starkey -> people.person.gender -> Male\n# Answer:\nRichard Starkey", "# Reasoning Path:\nRingo Starr -> music.artist.contribution -> m.0vv1l5w -> music.recording_contribution.album -> Wonderwall Music\n# Answer:\nWonderwall Music", "# Reasoning Path:\nRingo Starr -> people.person.profession -> Cinematographer -> common.topic.subject_of -> Eugenio Polgovsky\n# Answer:\nCinematographer"], "ground_truth": ["You Know It Makes Sense", "Fastest Growing Heartache in the West", "Dear Santa", "Hopeless", "Love Me Do", "Pinocchio Medley (\\\"Do You See the Noses Growing?\\\"): Desolation Theme / When You Wish Upon a Star", "Rory and the Hurricanes", "Who Needs a Heart", "Goodnight Vienna", "Tonight", "Caterwaul", "You Never Know", "The No-No Song", "Gone Are the Days", "Easy for Me", "Wine, Women, and Loud Happy Songs", "Blindman", "Stop and Take the Time to Smell the Roses", "I'm Yours", "Scouse's Dream", "Without Understanding", "Winter Wonderland", "Waiting", "Have You Seen My Baby (Hold On)", "Vertical Man", "Love Is", "Let Love Lead", "Satisfied", "Walk With You", "Elizabeth Reigns", "Sunshine Life for Me (Sail Away Raymond)", "Attention", "Husbands and Wives", "What Love Wants to Be", "Glamorous Life", "I Keep Forgettin'", "A Dose of Rock 'N' Roll", "Oh My Lord", "Spooky Weirdness", "Dead Giveaway", "It's No Secret", "Sweet Little Sixteen", "Call Me", "Don't Know a Thing About Love", "Fading In Fading Out", "OO-WEE", "Running Free", "With a Little Help From My Friends (reprise)", "Don't Go Where the Road Don't Go", "As Far as We Can Go", "Only You (and You Alone)", "Postcards From Paradise", "This Be Called a Song", "Don\u2019t Hang Up", "Give a Little Bit", "Snookeroo", "Right Side of the Road", "Karn Evil 9", "It Don't Come Easy", "Logical Song", "Anthem", "Sure To Fall", "Think About You", "You Belong to Me", "Sure to Fall (In Love With You)", "Christmas Eve", "Pure Gold", "The Christmas Dance", "Alibi", "Instant Amnesia", "I Wanna Be Your Man", "Beaucoups of Blues", "Wine, Women and Loud Happy Songs", "Octopus's Garden", "Six O'Clock (extended version)", "Can't Do It Wrong", "R U Ready", "Out On The Streets", "One", "Pasodobles", "Spooky Wierdness", "Six O\u2019Clock", "I Really Love Her", "Liverpool 8", "If It's Love That You Want", "Don't Hang Up", "Love Is a Many Splendoured Thing", "Harry's Song", "You Don't Know Me at All", "All in the Name of Love", "Mystery of the Night", "Lady Gaye", "Never Without You", "Early 1970", "Don't Be Cruel", "I Still Love Rock 'n' Roll", "Slow Down", "Not Looking Back", "Love Bizarre", "Wonderful", "In My Car", "Woman of the Night", "You and Me (Babe)", "Boat Ride", "Where Did Our Love Go", "Cryin'", "Memphis in Your Mind", "Oo-Wee", "Your Sixteen", "Occapella", "You Can't Fight Lightning", "I'll Be Fine Anywhere", "Mindfield", "Island in the Sun", "All by Myself", "Wings", "Time", "Give Me Back the Beat", "Fastest Growing Heartache In The West", "As Far as We Can Go (original version)", "Hey Baby", "Everybody's in a Hurry but Me", "The Really 'Serious' Introduction", "Goodnight Vienna (reprise)", "Loser's Lounge", "Matchbox", "No One to Blame", "Dream", "I'm the Greatest", "Everlasting Love", "In a Heartbeat", "Fading in Fading Out", "Let the Rest of the World Go By", "Coochy Coochy", "Lay Down Your Arms (feat. Stevie Nicks)", "Lucky Man", "Confirmation", "Stop and Take the Time to Smell the Roses (Original Vocal Version)", "English Garden / I Really Love Her", "Drowning In The Sea Of Love", "Y Not", "Simple Love Song", "Rudolph the Red\u2010Nosed Reindeer", "Can She Do It Like She Dances", "I've Got Blisters...", "Husbands And Wives", "For Love", "Lipstick Traces", "Fading in and Fading Out", "With a Little Help From My Friends / It Don't Come Easy", "Monkey See, Monkey Do", "Las Brisas", "Tommy's Holiday Camp", "Now That She's Gone Away", "Drift Away", "Out on the Streets", "Going Down", "Night and Day", "Trippin' on My Own Tears", "Blue, Turning Grey Over You", "No No Song/Skokiaan", "Choose Love", "Wake Up", "You're Sixteen (You're Beautiful and You're Mine)", "Heart on My Sleeve", "Be My Baby", "Six O'Clock", "Puppet", "King of Broken Hearts", "Without Her", "You\u2019re Sixteen", "After All These Years", "Drumming Is My Madness", "Free Drinks", "Six O'Clock (Extended Version)", "I Wouldn't Have You Any Other Way", "Me and You", "Eye to Eye", "I Think Therefore I Rock 'n Roll", "With a Little Help From My Friends", "The Turnaround", "Sentimental Journey", "I'd Be Talking All the Time", "Fill in the Blanks", "Weight of the World", "All the Young Dudes", "Pax Um Biscum (Peace Be With You)", "You Bring the Party Down", "Think It Over", "Christmas Time Is Here Again", "I Wanna Be Santa Claus", "Tango All Night", "What Goes Around", "Stardust", "A Mouse Like Me", "Peace Dream", "Boys", "Drowning in the Sea of Love", "Devil Woman", "Bamboula", "I Don't Believe You", "Red and Black Blues", "Step Lightly", "Everyone Wins", "Missouri Loves Company", "Wrong All the Time", "Easy For Me", "I'll Still Love You", "All By Myself", "Gave It All Up", "Only You", "Hand Gun Promos", "I'm a Fool to Care", "Bridges", "Living in a Pet Shop", "Nashville Jam", "(It's All Down to) Good Night Vienna", "She's About a Mover", "Iko Iko", "Fiddle About", "Love Don't Last Long", "I'd be Talking all the Time", "$15 Draw", "No No Song", "The Other Side of Liverpool", "Blue Christmas", "I Was Walkin'", "Hard to Be True", "Spooky Weirdiness", "Have You Seen My Baby", "Back Off Boogaloo", "In Liverpool", "Scouse the Mouse", "Some People", "Bad Boy", "Silent Homecoming", "Imagine Me There", "Old Time Relovin'", "White Christmas", "The End", "You Always Hurt the One You Love", "A Man Like Me", "Oh My My", "The Little Drummer Boy", "Don't Pass Me By", "A Dose of Rock 'n' Roll", "(It's All Down to) Good Night Vienna (single version)", "Have I Told You Lately That I Love You?", "Samba", "Down and Out", "Private Property", "Hard Times", "Give It a Try", "Picture Show Life", "Write One for Me", "Sneaking Sally Through the Alley", "Love First, Ask Questions Later", "Only You (And You Alone)", "Wrack My Brain", "Take the Long Way Home", "Runaways", "Come on Christmas, Christmas Come On", "Yellow Submarine", "La De Da", "Gypsies in Flight", "Golden Blunders", "With a Little Help From My Friends (live)", "Gypsies In Flight", "Tuff Love", "Brandy", "Rock Island Line", "Bye Bye Blackbird", "Cookin' (In the Kitchen of Love)", "S.O.S.", "Photograph", "Whispering Grass (Don't Tell the Trees)", "The No No Song", "What in the... World", "Who's Your Daddy", "Nice Way", "I Know a Place", "Honey Don't", "Act Naturally", "You're Sixteen", "Touch and Go"], "ans_acc": 0.0035087719298245615, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.006779661016949153, "path_ans_precision": 0.1, "path_ans_recall": 0.0035087719298245615}
{"id": "WebQTest-492", "prediction": ["# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> travel.travel_destination.tourist_attractions -> Ancient theatre of Taormina\n# Answer:\nSyracuse", "# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> travel.travel_destination.tourist_attractions -> Arethuse\n# Answer:\nSyracuse", "# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> location.location.events -> Siege of Syracuse\n# Answer:\nSyracuse", "# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> travel.travel_destination.tourist_attractions -> Cassibile\n# Answer:\nSyracuse", "# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> location.location.people_born_here -> Archimedes\n# Answer:\nSyracuse", "# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> location.location.events -> Action of 26 June 1625\n# Answer:\nSyracuse", "# Reasoning Path:\nSaint Lucy -> visual_art.art_subject.artwork_on_the_subject -> The Martyrdom and Last Communion of Saint Lucy -> visual_art.artwork.artist -> Paolo Veronese\n# Answer:\nThe Martyrdom and Last Communion of Saint Lucy", "# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> location.location.events -> Operation Ladbroke\n# Answer:\nSyracuse", "# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> location.location.people_born_here -> Achille Majeroni\n# Answer:\nSyracuse", "# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> location.location.people_born_here -> Alessio di Mauro\n# Answer:\nSyracuse"], "ground_truth": ["Syracuse"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-493", "prediction": ["# Reasoning Path:\nAustralia -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> United Kingdom\n# Answer:\nParliamentary system", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Canada\n# Answer:\nParliamentary system", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_55mr\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_2hqk\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Japan\n# Answer:\nParliamentary system", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_6vhn\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp"], "ground_truth": ["Parliamentary system", "Constitutional monarchy", "Federal monarchy"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.4, "ans_recall": 0.6666666666666666, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5, "path_ans_precision": 0.4, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-497", "prediction": ["# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> people.person.parents -> Edward VII\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> people.person.parents -> Alexandra of Denmark\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> Mary of Teck -> people.person.parents -> Princess Mary Adelaide of Cambridge\n# Answer:\nMary of Teck", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> people.deceased_person.cause_of_death -> Euthanasia\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> Mary of Teck -> people.person.parents -> Francis, Duke of Teck\n# Answer:\nMary of Teck", "# Reasoning Path:\nGeorge VI -> people.person.parents -> Mary of Teck -> music.artist.track -> A Message to the Children of the Empire\n# Answer:\nMary of Teck", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> freebase.valuenotation.is_reviewed -> Children\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> Mary of Teck -> people.person.children -> Prince John of the United Kingdom\n# Answer:\nMary of Teck", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nGeorge V"], "ground_truth": ["George V"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-500", "prediction": ["# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08605 -> common.topic.notable_types -> Postal Code\n# Answer:\n08605", "# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08605 -> common.topic.notable_for -> g.1255d7h3l\n# Answer:\n08605", "# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08607 -> location.postal_code.country -> United States of America\n# Answer:\n08607", "# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08605 -> location.postal_code.country -> United States of America\n# Answer:\n08605", "# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08618 -> location.postal_code.country -> United States of America\n# Answer:\n08618", "# Reasoning Path:\nTrenton -> location.statistical_region.population -> g.11b66l5kmz\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08607 -> common.topic.notable_types -> Postal Code\n# Answer:\n08607", "# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08607 -> common.topic.notable_for -> g.125cbpjqx\n# Answer:\n08607", "# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08618 -> location.location.geolocation -> m.03dtqk_\n# Answer:\n08618", "# Reasoning Path:\nTrenton -> location.statistical_region.population -> g.11bymn745l\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["08625", "08607", "08645", "08603", "08650", "08641", "08606", "08666", "08620", "08619", "08604", "08629", "08605", "08602", "08695", "08628", "08691", "08610", "08608", "08609", "08618", "08646", "08647", "08638", "08601", "08690", "08611"], "ans_acc": 0.1111111111111111, "ans_hit": 1, "ans_f1": 0.19512195121951217, "ans_precission": 0.8, "ans_recall": 0.1111111111111111, "path_f1": 0.19512195121951217, "path_precision": 0.8, "path_recall": 0.1111111111111111, "path_ans_f1": 0.19512195121951217, "path_ans_precision": 0.8, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-501", "prediction": ["# Reasoning Path:\nAshley Greene -> film.actor.film -> m.05kdryk -> film.performance.film -> Radio Free Albemuth\n# Answer:\nRadio Free Albemuth", "# Reasoning Path:\nAshley Greene -> film.actor.film -> m.0gbwhrn -> film.performance.film -> Shrink\n# Answer:\nShrink", "# Reasoning Path:\nAshley Greene -> film.actor.film -> m.05kdsj2 -> film.performance.film -> King of California\n# Answer:\nKing of California", "# Reasoning Path:\nAshley Greene -> award.award_nominee.award_nominations -> m.0z83xqy -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nAshley Greene -> film.actor.film -> m.05kdryk -> film.performance.character -> Rhonda\n# Answer:\nRhonda", "# Reasoning Path:\nAshley Greene -> award.award_nominee.award_nominations -> m.0z8hkq8 -> award.award_nomination.nominated_for -> The Twilight Saga: New Moon\n# Answer:\nThe Twilight Saga: New Moon", "# Reasoning Path:\nAshley Greene -> film.actor.film -> m.0gbwhrn -> film.performance.character -> Missy\n# Answer:\nMissy", "# Reasoning Path:\nAshley Greene -> award.award_nominee.award_nominations -> m.0z83xqy -> award.award_nomination.award -> Teen Choice Award for Choice Movie Scene Stealer: Female\n# Answer:\nTeen Choice Award for Choice Movie Scene Stealer: Female", "# Reasoning Path:\nAshley Greene -> people.person.places_lived -> m.0j21l7j -> people.place_lived.location -> Hollywood\n# Answer:\nHollywood", "# Reasoning Path:\nAshley Greene -> film.actor.film -> m.05kdsj2 -> film.performance.character -> MacDonalds Customer\n# Answer:\nMacDonalds Customer"], "ground_truth": ["Shrink", "The Twilight Saga: Breaking Dawn - Part 1", "Eclipse", "The Boom Boom Room", "Otis", "Twilight", "King of California", "LOL", "Skateland", "The Twilight Saga: Breaking Dawn - Part 2", "Butter", "Random", "Radio Free Albemuth", "The Twilight Saga: New Moon", "Urge", "A Warrior's Heart", "CBGB", "Staten Island Summer", "Wish I Was Here", "Burying the Ex", "Americana", "The Apparition", "Summer's Blood"], "ans_acc": 0.2608695652173913, "ans_hit": 1, "ans_f1": 0.3428571428571428, "ans_precission": 0.5, "ans_recall": 0.2608695652173913, "path_f1": 0.24390243902439024, "path_precision": 0.5, "path_recall": 0.16129032258064516, "path_ans_f1": 0.3428571428571428, "path_ans_precision": 0.5, "path_ans_recall": 0.2608695652173913}
{"id": "WebQTest-502", "prediction": ["# Reasoning Path:\nAl-Qaeda -> film.film_subject.films -> Restrepo -> film.film.subjects -> Taliban insurgency\n# Answer:\nRestrepo", "# Reasoning Path:\nAl-Qaeda -> base.terrorism.terrorist_organization.involved_in_attacks -> September 11 attacks -> time.event.includes_event -> American Airlines Flight 11\n# Answer:\nSeptember 11 attacks", "# Reasoning Path:\nAl-Qaeda -> film.film_subject.films -> Killing in the Name -> film.film.subjects -> Terrorism\n# Answer:\nKilling in the Name", "# Reasoning Path:\nAl-Qaeda -> base.terrorism.terrorist_organization.involved_in_attacks -> September 11 attacks -> film.film_subject.films -> 11'09\\\"01 September 11\n# Answer:\nSeptember 11 attacks", "# Reasoning Path:\nAl-Qaeda -> base.terrorism.terrorist_organization.involved_in_attacks -> September 11 attacks -> base.schemastaging.context_name.pronunciation -> g.125_njzfs\n# Answer:\nSeptember 11 attacks", "# Reasoning Path:\nAl-Qaeda -> military.military_combatant.military_conflicts -> m.04yzk7j -> military.military_combatant_group.conflict -> Taliban insurgency\n# Answer:\nTaliban insurgency", "# Reasoning Path:\nAl-Qaeda -> base.terrorism.terrorist_organization.involved_in_attacks -> September 11 attacks -> film.film_subject.films -> 9/11: The Falling Man\n# Answer:\nSeptember 11 attacks", "# Reasoning Path:\nAl-Qaeda -> film.film_subject.films -> Restrepo -> film.film.subjects -> Juan Restrepo\n# Answer:\nRestrepo", "# Reasoning Path:\nAl-Qaeda -> base.terrorism.terrorist_organization.involved_in_attacks -> September 11 attacks -> film.film_subject.films -> 911: In Plane Site\n# Answer:\nSeptember 11 attacks", "# Reasoning Path:\nAl-Qaeda -> film.film_subject.films -> Restrepo -> film.film.subjects -> War in Afghanistan\n# Answer:\nRestrepo"], "ground_truth": ["September 11 attacks", "Taliban insurgency"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7241379310344827, "path_precision": 0.7, "path_recall": 0.75, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-503", "prediction": ["# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> Italian Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> German Language -> language.human_language.countries_spoken_in -> Liechtenstein\n# Answer:\nGerman Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> Italian Language -> language.human_language.main_country -> Italy\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> German Language -> language.human_language.countries_spoken_in -> Liechtenstein\n# Answer:\nGerman Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> Italian Language -> language.human_language.region -> Europe\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> Italian Language -> language.human_language.main_country -> Italy\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> Italian Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nItalian Language"], "ground_truth": ["German Language", "Romansh language", "French", "Italian Language"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.6666666666666665, "path_precision": 0.6, "path_recall": 0.75, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-504", "prediction": ["# Reasoning Path:\nPhilip the Apostle -> people.deceased_person.place_of_death -> Hierapolis -> location.location.people_born_here -> Epictetus\n# Answer:\nHierapolis", "# Reasoning Path:\nPhilip the Apostle -> people.deceased_person.cause_of_death -> Crucifixion -> people.cause_of_death.people -> Andrew the Apostle\n# Answer:\nCrucifixion", "# Reasoning Path:\nPhilip the Apostle -> people.deceased_person.place_of_death -> Hierapolis -> common.topic.image -> The main thoroughfare of Hierapolis\n# Answer:\nHierapolis", "# Reasoning Path:\nPhilip the Apostle -> people.deceased_person.cause_of_death -> Crucifixion -> people.cause_of_death.people -> Artayctes\n# Answer:\nCrucifixion", "# Reasoning Path:\nPhilip the Apostle -> people.deceased_person.place_of_death -> Hierapolis -> common.topic.article -> m.0c4md9\n# Answer:\nHierapolis", "# Reasoning Path:\nPhilip the Apostle -> people.deceased_person.cause_of_death -> Crucifixion -> people.cause_of_death.people -> Bartholom\u00e4us\n# Answer:\nCrucifixion", "# Reasoning Path:\nPhilip the Apostle -> people.deceased_person.cause_of_death -> Crucifixion -> base.ontologies.ontology_instance.equivalent_instances -> m.09klpz1\n# Answer:\nCrucifixion", "# Reasoning Path:\nPhilip the Apostle -> symbols.name_source.namesakes -> St. Philip's Episcopal Church -> people.place_of_interment.interred_here -> Alexander Garden\n# Answer:\nSt. Philip's Episcopal Church", "# Reasoning Path:\nPhilip the Apostle -> symbols.name_source.namesakes -> St. Philip's Episcopal Church -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.06zwg2y\n# Answer:\nSt. Philip's Episcopal Church", "# Reasoning Path:\nPhilip the Apostle -> people.deceased_person.cause_of_death -> Crucifixion -> book.book_subject.works -> Adding Cross to crown\n# Answer:\nCrucifixion"], "ground_truth": ["Apostle Philip"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.0, "path_ans_precision": 0.0, "path_ans_recall": 1.0}
{"id": "WebQTest-506", "prediction": ["# Reasoning Path:\nEngland -> location.location.time_zones -> Greenwich Mean Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nGreenwich Mean Time Zone", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0njwhw5\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0hq0kmy\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0mv3kv1\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Airedale Terrier -> biology.animal_breed.place_of_origin -> United Kingdom\n# Answer:\nAiredale Terrier", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Airedale Terrier -> common.topic.notable_types -> Animal breed\n# Answer:\nAiredale Terrier", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Airedale Terrier -> base.petbreeds.dog_breed.temperament -> Alert\n# Answer:\nAiredale Terrier", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> American Pit Bull Terrier -> biology.animal_breed.coloring -> Black\n# Answer:\nAmerican Pit Bull Terrier", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Bandog -> base.petbreeds.dog_breed.color -> Fawn\n# Answer:\nBandog", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Bandog -> biology.animal_breed.breed_of -> Dog\n# Answer:\nBandog"], "ground_truth": ["Greenwich Mean Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-508", "prediction": ["# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Revaluation of the Turkish Lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nRevaluation of the Turkish Lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.065sxsl -> military.military_combatant_group.conflict -> Korean War\n# Answer:\nKorean War", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.066402y -> military.military_combatant_group.conflict -> Iraqi no-fly zones\n# Answer:\nIraqi no-fly zones"], "ground_truth": ["Turkish lira"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-509", "prediction": ["# Reasoning Path:\nNigeria -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> United States of America\n# Answer:\nPresidential system", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Chad\n# Answer:\nPresidential system", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> United States of America\n# Answer:\nFederal republic", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guinea\n# Answer:\nPresidential system", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_0sgz\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nNigeria -> location.statistical_region.external_debt_stock -> g.11b71rqxzt\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_22x5\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Sudan\n# Answer:\nFederal republic", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_2gxr\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nNigeria -> location.statistical_region.external_debt_stock -> g.1hhc38d1f\n# Answer:\nlocation.statistical_region.external_debt_stock"], "ground_truth": ["Federal republic", "Presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.actor -> Jamie Dornan\n# Answer:\nJamie Dornan", "# Reasoning Path:\nChristian Grey -> common.topic.notable_for -> g.125czvn3w\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.film -> Fifty Shades of Grey\n# Answer:\nFifty Shades of Grey", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Males\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> common.topic.notable_for -> g.1255p_jgc\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Acute myeloid leukemia\n# Answer:\nMale"], "ground_truth": ["Jamie Dornan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-512", "prediction": ["# Reasoning Path:\nMary Shelley -> people.person.profession -> Biographer -> people.profession.specialization_of -> Writer\n# Answer:\nBiographer", "# Reasoning Path:\nMary Shelley -> people.person.profession -> Biographer -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nBiographer", "# Reasoning Path:\nMary Shelley -> people.person.profession -> Writer -> people.profession.specializations -> Biographer\n# Answer:\nWriter", "# Reasoning Path:\nMary Shelley -> influence.influence_node.influenced_by -> William Godwin -> influence.influence_node.influenced -> Thomas Robert Malthus\n# Answer:\nWilliam Godwin", "# Reasoning Path:\nMary Shelley -> people.person.profession -> Biographer -> common.topic.notable_for -> g.1259rk51h\n# Answer:\nBiographer", "# Reasoning Path:\nMary Shelley -> people.person.profession -> Editor -> people.profession.corresponding_type -> Author\n# Answer:\nEditor", "# Reasoning Path:\nMary Shelley -> people.person.profession -> Writer -> common.topic.image -> Edgar Allan Poe portrait B\n# Answer:\nWriter", "# Reasoning Path:\nMary Shelley -> people.person.profession -> Writer -> people.profession.specializations -> Travel writer\n# Answer:\nWriter", "# Reasoning Path:\nMary Shelley -> people.person.profession -> Writer -> book.book_subject.works -> The Passion-Driven Writer and the Digital-Age Literary Marketplace\n# Answer:\nWriter", "# Reasoning Path:\nMary Shelley -> people.person.profession -> Editor -> common.topic.notable_types -> Profession\n# Answer:\nEditor"], "ground_truth": ["Writer", "Novelist", "Travel writer", "Essayist", "Author", "Playwright", "Editor", "Biographer"], "ans_acc": 0.625, "ans_hit": 1, "ans_f1": 0.5294117647058825, "ans_precission": 0.9, "ans_recall": 0.375, "path_f1": 0.5294117647058825, "path_precision": 0.9, "path_recall": 0.375, "path_ans_f1": 0.7377049180327869, "path_ans_precision": 0.9, "path_ans_recall": 0.625}
{"id": "WebQTest-513", "prediction": ["# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> Japan\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars"], "ground_truth": ["Esperanto Language", "English Language", "Lojban"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-514", "prediction": ["# Reasoning Path:\nDenmark -> location.location.containedby -> Northern Europe -> location.location.containedby -> Europe\n# Answer:\nNorthern Europe", "# Reasoning Path:\nDenmark -> location.location.containedby -> Nordic countries -> location.location.contains -> Scandinavia\n# Answer:\nNordic countries", "# Reasoning Path:\nDenmark -> location.location.containedby -> Nordic countries -> location.location.primarily_containedby -> Northern Europe\n# Answer:\nNordic countries", "# Reasoning Path:\nDenmark -> location.location.containedby -> Nordic countries -> location.location.contains -> Sweden\n# Answer:\nNordic countries", "# Reasoning Path:\nDenmark -> location.statistical_region.gender_balance_members_of_parliament -> g.11b71wx6lz\n# Answer:\nlocation.statistical_region.gender_balance_members_of_parliament", "# Reasoning Path:\nDenmark -> location.location.containedby -> Nordic countries -> location.location.contains -> Faroe Islands\n# Answer:\nNordic countries", "# Reasoning Path:\nDenmark -> location.location.containedby -> Eurasia -> location.location.events -> World War II\n# Answer:\nEurasia", "# Reasoning Path:\nDenmark -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> United Kingdom\n# Answer:\nEurope", "# Reasoning Path:\nDenmark -> base.locations.countries.continent -> Europe -> organization.organization_scope.organizations_with_this_scope -> European Union\n# Answer:\nEurope", "# Reasoning Path:\nDenmark -> location.location.containedby -> Northern Europe -> location.location.contains_major_portion_of -> Nordic countries\n# Answer:\nNorthern Europe"], "ground_truth": ["Nordic countries", "Scandinavia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.4, "ans_recall": 0.5, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-515", "prediction": ["# Reasoning Path:\nTheodore Roosevelt -> people.person.places_lived -> m.0108qyxb -> people.place_lived.location -> Washington, D.C.\n# Answer:\nWashington, D.C.", "# Reasoning Path:\nTheodore Roosevelt -> people.person.place_of_birth -> Manhattan -> location.location.containedby -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nTheodore Roosevelt -> symbols.name_source.namesakes -> Roosevelt -> location.location.containedby -> Washington\n# Answer:\nRoosevelt", "# Reasoning Path:\nTheodore Roosevelt -> symbols.name_source.namesakes -> Theodore Roosevelt Island -> location.location.containedby -> Washington, D.C.\n# Answer:\nTheodore Roosevelt Island", "# Reasoning Path:\nTheodore Roosevelt -> people.person.place_of_birth -> Manhattan -> location.location.containedby -> New York\n# Answer:\nManhattan", "# Reasoning Path:\nTheodore Roosevelt -> people.person.place_of_birth -> Manhattan -> base.aareas.schema.administrative_area.administrative_parent -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nTheodore Roosevelt -> people.person.place_of_birth -> Manhattan -> location.location.containedby -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nTheodore Roosevelt -> people.person.place_of_birth -> Manhattan -> location.administrative_division.country -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nTheodore Roosevelt -> people.person.places_lived -> m.04hxhjj -> people.place_lived.location -> New York City\n# Answer:\nNew York City", "# Reasoning Path:\nTheodore Roosevelt -> symbols.name_source.namesakes -> Roosevelt -> common.topic.article -> m.02gt_g\n# Answer:\nRoosevelt"], "ground_truth": ["New York City", "Washington, D.C."], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.2857142857142857, "path_precision": 0.4, "path_recall": 0.2222222222222222, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-516", "prediction": ["# Reasoning Path:\nMalaysia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Thailand\n# Answer:\nParliamentary system", "# Reasoning Path:\nMalaysia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Australia\n# Answer:\nParliamentary system", "# Reasoning Path:\nMalaysia -> location.country.form_of_government -> Parliamentary system -> common.topic.notable_types -> Form of Government\n# Answer:\nParliamentary system", "# Reasoning Path:\nMalaysia -> location.country.form_of_government -> Constitutional monarchy -> common.topic.notable_types -> Form of Government\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nMalaysia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Cambodia\n# Answer:\nParliamentary system", "# Reasoning Path:\nMalaysia -> location.country.form_of_government -> Elective monarchy -> common.topic.notable_types -> Form of Government\n# Answer:\nElective monarchy", "# Reasoning Path:\nMalaysia -> location.statistical_region.size_of_armed_forces -> g.12cp_jm8t\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nMalaysia -> location.statistical_region.gdp_nominal_per_capita -> g.11b60thpy3\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nMalaysia -> location.country.form_of_government -> Elective monarchy -> government.form_of_government.countries -> United Arab Emirates\n# Answer:\nElective monarchy", "# Reasoning Path:\nMalaysia -> location.statistical_region.size_of_armed_forces -> g.1hhc37x7v\n# Answer:\nlocation.statistical_region.size_of_armed_forces"], "ground_truth": ["Democracy", "Parliamentary system", "Constitutional monarchy", "Elective monarchy"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7241379310344827, "ans_precission": 0.7, "ans_recall": 0.75, "path_f1": 0.7241379310344827, "path_precision": 0.7, "path_recall": 0.75, "path_ans_f1": 0.7241379310344827, "path_ans_precision": 0.7, "path_ans_recall": 0.75}
{"id": "WebQTest-517", "prediction": ["# Reasoning Path:\nCanada -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nLojban", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> Lojban -> common.topic.notable_types -> Human Language\n# Answer:\nLojban", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> Lojban -> language.human_language.region -> Americas\n# Answer:\nLojban", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> English Language -> language.human_language.dialects -> Canadian English\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nCanada -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language"], "ground_truth": ["Punjabi language", "Inuktitut", "Urdu Language", "Russian Language", "Persian Language", "Polish Language", "French", "Gwich'in Language", "Korean Language", "Romanian Language", "Gujarati Language", "Tamil Language", "Arabic Language", "Abenaki language", "Chipewyan Language", "Dogrib Language", "Vietnamese Language", "Lojban", "Canadian English", "Cree language", "Inuinnaqtun", "Greek Language", "Chinese language", "Tagalog language", "Portuguese Language", "English Language", "Upper Midwest American English", "German Language", "Slavey language", "Spanish Language", "Western Canadian Inuktitut Language"], "ans_acc": 0.16129032258064516, "ans_hit": 1, "ans_f1": 0.2285714285714286, "ans_precission": 1.0, "ans_recall": 0.12903225806451613, "path_f1": 0.22570532915360503, "path_precision": 0.9, "path_recall": 0.12903225806451613, "path_ans_f1": 0.27777777777777773, "path_ans_precision": 1.0, "path_ans_recall": 0.16129032258064516}
{"id": "WebQTest-518", "prediction": ["# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.image -> 10colonesbcrfront\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.webpage -> m.04m2m2k\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60wmkvh\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.11b71w3_6h\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_5xgy\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_6n42\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.1hhc37pjz\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.1hhc37x5q\n# Answer:\nlocation.statistical_region.external_debt_stock"], "ground_truth": ["Costa Rican col\u00f3n"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-519", "prediction": ["# Reasoning Path:\nNashville -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nNashville -> location.location.time_zones -> Central Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_mmrcv\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nNashville -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nNashville -> common.topic.webpage -> m.03l8rmb -> common.webpage.resource -> m.0bkjqzl\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nNashville -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nNashville -> common.topic.webpage -> m.03l8rmb -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nNashville -> location.location.events -> 1973 NCAA Men's Division I Basketball Tournament -> time.event.locations -> Wichita\n# Answer:\n1973 NCAA Men's Division I Basketball Tournament", "# Reasoning Path:\nNashville -> common.topic.webpage -> m.09w5qrw -> common.webpage.resource -> Jessica Simpson goes a little bit country\n# Answer:\nJessica Simpson goes a little bit country", "# Reasoning Path:\nNashville -> location.location.events -> 1973 NCAA Men's Division I Basketball Tournament -> time.event.locations -> Williamsburg\n# Answer:\n1973 NCAA Men's Division I Basketball Tournament", "# Reasoning Path:\nNashville -> common.topic.webpage -> m.09w5qrw -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Albert Camus\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Anton Chekhov\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> people.cause_of_death.parent_cause_of_death -> Infectious disease\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Antibiotics\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Franz Kafka\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Isoniazid\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Aldous Huxley -> people.deceased_person.cause_of_death -> Laryngeal cancer\n# Answer:\nAldous Huxley", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Pyrazinamide\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion. -> media_common.quotation.subjects -> Jokes and Jokers\n# Answer:\nA dirty joke is a sort of mental rebellion.", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Arthur Koestler -> people.deceased_person.cause_of_death -> Parkinson's disease\n# Answer:\nArthur Koestler"], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-521", "prediction": ["# Reasoning Path:\ng.125_cxx77\n# Answer:\ng.125_cxx77"], "ground_truth": ["Ted Bracewell"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-522", "prediction": ["# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> base.aareas.schema.administrative_area.administrative_parent -> Germany\n# Answer:\nBerlin", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> film.film_location.featured_in_films -> Valkyrie\n# Answer:\nBerlin", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> film.film_location.featured_in_films -> Inglourious Basterds\n# Answer:\nBerlin", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.resource -> Another Megan Fox letter -- and this one sings her praises!\n# Answer:\nAnother Megan Fox letter -- and this one sings her praises!", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wgn1j -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wgn1j -> common.webpage.resource -> Charles Manson: 40 years later, the movie about him you have to see\n# Answer:\nCharles Manson: 40 years later, the movie about him you have to see", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wqfb3 -> common.webpage.resource -> Erykah Badu teases 'New Amerykah Part Two' with trippy, funky clip\n# Answer:\nErykah Badu teases 'New Amerykah Part Two' with trippy, funky clip", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wqfb3 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Berlin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-523", "prediction": ["# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> United Kingdom -> location.location.containedby -> Eurasia\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> United Kingdom -> location.country.languages_spoken -> English Language\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Russia -> location.location.partially_containedby -> Asia\n# Answer:\nRussia", "# Reasoning Path:\nEurope -> location.location.partially_contains -> Russia -> location.location.partially_containedby -> Asia\n# Answer:\nRussia", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> United Kingdom -> location.location.containedby -> Western Europe\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> United Kingdom -> base.aareas.schema.administrative_area.administrative_area_type -> Sovereign state\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Vatican City -> location.location.containedby -> Eurasia\n# Answer:\nVatican City", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Russia -> location.location.partially_contained_by -> m.0wg8l4g\n# Answer:\nRussia", "# Reasoning Path:\nEurope -> location.location.partially_contains -> Russia -> location.location.partially_contained_by -> m.0wg8l4g\n# Answer:\nRussia", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Vatican City -> base.biblioness.bibs_location.country -> Italy\n# Answer:\nVatican City"], "ground_truth": ["Russia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-524", "prediction": ["# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> royalty.kingdom.rulers -> Abdullah of Saudi Arabia\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> location.location.containedby -> Middle East\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> location.country.official_language -> Arabic Language\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> location.location.containedby -> Arabian Peninsula\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.includes_event -> 1991 uprisings in Iraq -> time.event.locations -> Iraq\n# Answer:\n1991 uprisings in Iraq", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> location.location.containedby -> Asia\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.locations -> Arabian Peninsula -> location.location.contains -> Saudi Arabia\n# Answer:\nArabian Peninsula", "# Reasoning Path:\nGulf War -> time.event.locations -> Iraq -> location.location.containedby -> Middle East\n# Answer:\nIraq", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Air engagements of the Gulf War -> time.event.locations -> Kuwait\n# Answer:\nAir engagements of the Gulf War", "# Reasoning Path:\nGulf War -> time.event.locations -> Arabian Peninsula -> location.location.contains -> Yemen Arab Republic\n# Answer:\nArabian Peninsula"], "ground_truth": ["Arabian Peninsula", "Saudi Arabia", "Persian Gulf", "Israel", "Kuwait", "Iraq"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.6428571428571429, "ans_precission": 0.9, "ans_recall": 0.5, "path_f1": 0.7659574468085106, "path_precision": 0.9, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-525", "prediction": ["# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> government.government_position_held.office_holder -> Tim Kaine\n# Answer:\nTim Kaine", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcw0 -> government.government_position_held.office_holder -> Jim Webb\n# Answer:\nJim Webb", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcm1 -> government.government_position_held.office_holder -> Mark Warner\n# Answer:\nMark Warner", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nUnited States Senate", "# Reasoning Path:\nVirginia -> location.location.containedby -> United States of America -> meteorology.cyclone_affected_area.cyclones -> Hurricane Isabel\n# Answer:\nUnited States of America", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcm1 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcw0 -> government.government_position_held.legislative_sessions -> 111th United States Congress\n# Answer:\n111th United States Congress", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcm1 -> government.government_position_held.legislative_sessions -> 111th United States Congress\n# Answer:\n111th United States Congress", "# Reasoning Path:\nVirginia -> location.location.containedby -> Charles Irving Thornton Tombstone -> location.location.geolocation -> m.0zjvs72\n# Answer:\nCharles Irving Thornton Tombstone"], "ground_truth": ["Tim Kaine", "Jim Webb", "Mark Warner"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-526", "prediction": ["# Reasoning Path:\nGeorge Clinton -> music.featured_artist.recordings -> Mothership Reconnection (Daft Punk remix) -> music.recording.artist -> Funkadellic\n# Answer:\nMothership Reconnection (Daft Punk remix)", "# Reasoning Path:\nGeorge Clinton -> music.featured_artist.recordings -> Black People -> music.recording.artist -> Funkadellic\n# Answer:\nBlack People", "# Reasoning Path:\nGeorge Clinton -> music.featured_artist.recordings -> Mothership Reconnection (Daft Punk remix) -> music.recording.artist -> Scott Grooves\n# Answer:\nMothership Reconnection (Daft Punk remix)", "# Reasoning Path:\nGeorge Clinton -> music.featured_artist.recordings -> Mothership Reconnection (Daft Punk remix) -> common.topic.notable_types -> Musical Recording\n# Answer:\nMothership Reconnection (Daft Punk remix)", "# Reasoning Path:\nGeorge Clinton -> music.featured_artist.recordings -> Mothership Reconnection (Daft Punk remix) -> music.recording.canonical_version -> Mothership Connection (Star Child)\n# Answer:\nMothership Reconnection (Daft Punk remix)", "# Reasoning Path:\nGeorge Clinton -> music.featured_artist.recordings -> All I Came to Do -> music.recording.artist -> Messy Marv\n# Answer:\nAll I Came to Do", "# Reasoning Path:\nGeorge Clinton -> music.featured_artist.recordings -> Black People -> common.topic.notable_for -> g.125ffndps\n# Answer:\nBlack People", "# Reasoning Path:\nGeorge Clinton -> music.artist.album -> George Clinton Family Series, Part 1: Go Fer Yer Funk -> music.album.artist -> George Clinton and Family\n# Answer:\nGeorge Clinton Family Series, Part 1: Go Fer Yer Funk", "# Reasoning Path:\nGeorge Clinton -> music.featured_artist.recordings -> Black People -> music.recording.featured_artists -> Belita Woods\n# Answer:\nBlack People", "# Reasoning Path:\nGeorge Clinton -> music.artist.album -> 20th Century Masters \u2013 The Millennium Collection: The Best of Parliament -> music.album.artist -> Parliament\n# Answer:\n20th Century Masters \u2013 The Millennium Collection: The Best of Parliament"], "ground_truth": ["Funkadellic", "George Clinton & Bootsy Collins", "Parliament-Funkadelic", "The Parliaments", "Dolby's Cube", "Parliament"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.125, "ans_precission": 0.1, "ans_recall": 0.16666666666666666, "path_f1": 0.043010752688172046, "path_precision": 0.2, "path_recall": 0.024096385542168676, "path_ans_f1": 0.3157894736842105, "path_ans_precision": 0.3, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-527", "prediction": ["# Reasoning Path:\nSlovakia -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Poland\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nSlovakia -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> Poland\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nSlovakia -> location.location.partially_contains -> Bieszczady Mountains -> location.location.partially_containedby -> Ukraine\n# Answer:\nBieszczady Mountains", "# Reasoning Path:\nSlovakia -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Bulgaria\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nSlovakia -> location.location.partially_contains -> Bieszczady Mountains -> geography.mountain_range.mountains -> Tarnica\n# Answer:\nBieszczady Mountains", "# Reasoning Path:\nSlovakia -> location.statistical_region.co2_emissions_per_capita -> g.1245_9wdf\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nSlovakia -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Czech Republic\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nSlovakia -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.competitions -> Biathlon at the 2014 Winter Olympics - Women's 7.5 km Sprint\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nSlovakia -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> Czech Republic\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nSlovakia -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Poland\n# Answer:\n2010 Winter Olympics"], "ground_truth": ["Austria", "Hungary", "Czech Republic", "Ukraine", "Poland"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.0975609756097561, "path_precision": 0.6, "path_recall": 0.05309734513274336, "path_ans_f1": 0.6, "path_ans_precision": 0.6, "path_ans_recall": 0.6}
{"id": "WebQTest-528", "prediction": ["# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Teacher -> common.topic.subject_of -> The TEFL Academy Ashford\n# Answer:\nTeacher", "# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Teacher -> base.lightweight.profession.professions_similar -> Librarian\n# Answer:\nTeacher", "# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Teacher -> common.topic.subject_of -> Everything TEFL LTD\n# Answer:\nTeacher", "# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Teacher -> common.topic.subject_of -> The TEFL Academy Aberdeen\n# Answer:\nTeacher", "# Reasoning Path:\nJames A. Garfield -> common.topic.notable_types -> US President -> type.type.expected_by -> US Presidents\n# Answer:\nUS President", "# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Teacher -> base.descriptive_names.names.descriptive_name -> m.0102yq_f\n# Answer:\nTeacher", "# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Laity -> people.profession.specializations -> Minister\n# Answer:\nLaity", "# Reasoning Path:\nJames A. Garfield -> common.topic.notable_types -> US President -> type.type.properties -> Vice president\n# Answer:\nUS President", "# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Teacher -> base.descriptive_names.names.descriptive_name -> m.0102yqyn\n# Answer:\nTeacher", "# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Laity -> common.topic.article -> m.01zmq7\n# Answer:\nLaity"], "ground_truth": ["US President"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-529", "prediction": ["# Reasoning Path:\nPaula Deen -> people.person.education -> m.0n1jznt -> education.education.institution -> Albany High School\n# Answer:\nAlbany High School", "# Reasoning Path:\nPaula Deen -> base.schemastaging.person_extra.net_worth -> m.0j8mg8t -> measurement_unit.dated_money_value.source -> celebritynetworth.com\n# Answer:\ncelebritynetworth.com", "# Reasoning Path:\nPaula Deen -> people.person.parents -> Corrie A. Paul -> common.topic.notable_types -> Deceased Person\n# Answer:\nCorrie A. Paul", "# Reasoning Path:\nPaula Deen -> base.schemastaging.person_extra.net_worth -> m.0j8mg8t -> measurement_unit.dated_money_value.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPaula Deen -> people.person.parents -> Corrie A. Paul -> common.topic.notable_for -> g.125d2hh2z\n# Answer:\nCorrie A. Paul", "# Reasoning Path:\nPaula Deen -> people.person.parents -> Earl Wayne Hiers, Sr -> people.person.gender -> Male\n# Answer:\nEarl Wayne Hiers, Sr", "# Reasoning Path:\nPaula Deen -> people.person.parents -> Earl Wayne Hiers, Sr -> common.topic.notable_types -> Deceased Person\n# Answer:\nEarl Wayne Hiers, Sr"], "ground_truth": ["Albany High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-530", "prediction": ["# Reasoning Path:\nRobert Irvine -> common.topic.article -> m.0272y30\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> location.location.containedby -> United Kingdom\n# Answer:\nSalisbury", "# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> travel.travel_destination.tourist_attractions -> The Rifles (Berkshire and Wiltshire) Museum\n# Answer:\nSalisbury", "# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> location.location.containedby -> Wiltshire\n# Answer:\nSalisbury", "# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> common.topic.image -> Salisbury Cathedral\n# Answer:\nSalisbury", "# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> travel.travel_destination.tourist_attractions -> Fisherton Mill\n# Answer:\nSalisbury", "# Reasoning Path:\nRobert Irvine -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> location.location.containedby -> England\n# Answer:\nSalisbury", "# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> travel.travel_destination.tourist_attractions -> Guildhall Salisbury\n# Answer:\nSalisbury", "# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> common.topic.image -> Aston, Flintshire (United Kingdom)\n# Answer:\nSalisbury"], "ground_truth": ["Salisbury"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-531", "prediction": ["# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Contemporary R&B -> common.topic.notable_types -> Musical genre\n# Answer:\nContemporary R&B", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Contemporary R&B -> music.genre.parent_genre -> Rhythm and blues\n# Answer:\nContemporary R&B", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Disco -> music.genre.parent_genre -> Soul music\n# Answer:\nDisco", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Contemporary R&B -> broadcast.genre.content -> 1Club.FM: 80s (Pop)\n# Answer:\nContemporary R&B", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Disco -> music.genre.subgenre -> House music\n# Answer:\nDisco", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Dance music -> broadcast.genre.content -> 1.FM Absolute  90's\n# Answer:\nDance music", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Contemporary R&B -> broadcast.genre.content -> 1Club.FM: Jammin' Oldies\n# Answer:\nContemporary R&B", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Disco -> broadcast.genre.content -> 1.FM Disco Ball\n# Answer:\nDisco", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Contemporary R&B -> broadcast.genre.content -> 1Club.FM: V101\n# Answer:\nContemporary R&B", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Dance music -> broadcast.genre.content -> 1.FM Euro 80's\n# Answer:\nDance music"], "ground_truth": ["Adult contemporary music", "Rock music", "Motown Sound", "Contemporary R&B", "Dance music", "New jack swing", "Electronic music", "Dance-pop", "Pop music", "Disco", "Post-disco", "Funk", "Soul music", "Rhythm and blues", "Pop rock"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 1.0, "ans_recall": 0.2, "path_f1": 0.33333333333333337, "path_precision": 1.0, "path_recall": 0.2, "path_ans_f1": 0.5, "path_ans_precision": 1.0, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-532", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m -> soccer.football_player_stats.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9021c -> soccer.football_player_stats.team -> A.C. Milan\n# Answer:\nA.C. Milan", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nDavid Beckham -> tv.tv_actor.guest_roles -> m.0_z851f -> tv.tv_guest_role.special_performance_type -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nPound sterling"], "ground_truth": ["Football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-533", "prediction": ["# Reasoning Path:\nJulian Lennon -> people.person.profession -> Guitarist -> freebase.type_hints.included_types -> Person\n# Answer:\nGuitarist", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Singer-songwriter -> people.profession.specialization_of -> Singer\n# Answer:\nSinger-songwriter", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Guitarist -> freebase.type_hints.included_types -> Topic\n# Answer:\nGuitarist", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Guitarist -> people.profession.specializations -> Bassist\n# Answer:\nGuitarist", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Guitarist -> fictional_universe.character_occupation.characters_with_this_occupation -> David St. Hubbins\n# Answer:\nGuitarist", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Photographer -> base.descriptive_names.names.descriptive_name -> m.0100_vbb\n# Answer:\nPhotographer", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Guitarist -> people.profession.specializations -> Classical guitarists\n# Answer:\nGuitarist", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Singer-songwriter -> people.profession.specialization_of -> Musician\n# Answer:\nSinger-songwriter", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Singer-songwriter -> common.topic.subject_of -> Anhayla\n# Answer:\nSinger-songwriter", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Guitarist -> people.profession.specializations -> Jazz guitarist\n# Answer:\nGuitarist"], "ground_truth": ["Musician", "Writer", "Singer-songwriter", "Film Score Composer", "Singer", "Record producer", "Photographer", "Guitarist", "Actor"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 1.0, "ans_recall": 0.5555555555555556, "path_f1": 0.6153846153846153, "path_precision": 1.0, "path_recall": 0.4444444444444444, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-534", "prediction": ["# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Bernie Brewer -> common.topic.notable_types -> Mascot\n# Answer:\nBernie Brewer", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Bernie Brewer -> common.topic.image -> Bernie-Brewer.jpg\n# Answer:\nBernie Brewer", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Hank -> common.topic.notable_types -> Mascot\n# Answer:\nHank", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Bonnie Brewer -> common.topic.article -> m.0blvgh\n# Answer:\nBonnie Brewer", "# Reasoning Path:\nMilwaukee Brewers -> base.schemastaging.organization_extra.contact_webpages -> m.010dwkgw -> internet.localized_uri.location -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Bernie Brewer -> common.topic.image -> Bernie Brewer (left) in his dugout at Miller Park.\n# Answer:\nBernie Brewer", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Hank -> common.topic.notable_for -> g.11b5ltzrvv\n# Answer:\nHank", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Bernie Brewer -> common.topic.image -> Bernie brewer in dugout\n# Answer:\nBernie Brewer", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Bonnie Brewer -> common.topic.notable_for -> g.1259jd77m\n# Answer:\nBonnie Brewer", "# Reasoning Path:\nMilwaukee Brewers -> base.schemastaging.organization_extra.contact_webpages -> m.010dwkgw -> internet.localized_uri.language -> English\n# Answer:\nEnglish"], "ground_truth": ["Hank", "Bonnie Brewer", "Bernie Brewer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-535", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Missouri -> location.location.containedby -> United States, with Territories\n# Answer:\nMissouri", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Illinois -> location.location.containedby -> United States, with Territories\n# Answer:\nIllinois", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_children -> Missouri -> location.location.containedby -> United States, with Territories\n# Answer:\nMissouri", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Missouri -> location.location.containedby -> Contiguous United States\n# Answer:\nMissouri", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Illinois -> location.location.containedby -> Contiguous United States\n# Answer:\nIllinois", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Illinois -> base.locations.states_and_provences.cities_within -> Belleville\n# Answer:\nIllinois", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Texas -> location.location.containedby -> Contiguous United States\n# Answer:\nTexas", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Texas -> base.aareas.schema.administrative_area.administrative_area_type -> U.S. state\n# Answer:\nTexas", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_children -> Illinois -> location.location.containedby -> United States, with Territories\n# Answer:\nIllinois"], "ground_truth": ["Texas"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-537", "prediction": ["# Reasoning Path:\nCharlie Hunnam -> film.actor.film -> m.0113ygm8 -> film.performance.film -> Crimson Peak\n# Answer:\nCrimson Peak", "# Reasoning Path:\nCharlie Hunnam -> film.actor.film -> m.0h0yd7r -> film.performance.film -> Pacific Rim\n# Answer:\nPacific Rim", "# Reasoning Path:\nCharlie Hunnam -> film.actor.film -> m.0jy_sj -> film.performance.film -> Green Street\n# Answer:\nGreen Street", "# Reasoning Path:\nCharlie Hunnam -> film.actor.film -> m.0113ygm8 -> film.performance.character -> Dr. Alan McMichael\n# Answer:\nDr. Alan McMichael", "# Reasoning Path:\nCharlie Hunnam -> film.actor.film -> m.0h0yd7r -> film.performance.character -> Raleigh Becket\n# Answer:\nRaleigh Becket", "# Reasoning Path:\nCharlie Hunnam -> film.actor.film -> m.0jy_sj -> film.performance.character -> Pete Dunham\n# Answer:\nPete Dunham", "# Reasoning Path:\nCharlie Hunnam -> people.person.parents -> Jane Hunnam -> people.person.children -> William Hunnam\n# Answer:\nJane Hunnam", "# Reasoning Path:\nCharlie Hunnam -> common.topic.webpage -> m.094hqqj -> common.webpage.resource -> Television\n# Answer:\nTelevision", "# Reasoning Path:\nCharlie Hunnam -> common.topic.webpage -> m.094n9d5 -> common.webpage.resource -> R. Kelly faces police probe over underage sex video\n# Answer:\nR. Kelly faces police probe over underage sex video", "# Reasoning Path:\nCharlie Hunnam -> people.person.parents -> Jane Hunnam -> people.person.spouse_s -> m.0zmn8qr\n# Answer:\nJane Hunnam"], "ground_truth": ["Nicholas Nickleby", "The Ledge", "Whatever Happened to Harold Smith?", "Pacific Rim", "Deadfall", "Abandon", "Children of Men", "Crimson Peak", "Green Street", "Frankie Go Boom", "Cold Mountain", "Knights of the Roundtable: King Arthur"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.2727272727272727, "ans_precission": 0.3, "ans_recall": 0.25, "path_f1": 0.28571428571428564, "path_precision": 0.3, "path_recall": 0.2727272727272727, "path_ans_f1": 0.2727272727272727, "path_ans_precision": 0.3, "path_ans_recall": 0.25}
{"id": "WebQTest-538", "prediction": ["# Reasoning Path:\nTaylor Swift -> award.award_winner.awards_won -> m.0v90n_n -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nWinning work", "# Reasoning Path:\nTaylor Swift -> film.actor.film -> m.0y4q1x7 -> film.performance.film -> The Giver\n# Answer:\nThe Giver", "# Reasoning Path:\nTaylor Swift -> award.award_winner.awards_won -> m.0nfm_g7 -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nWinning work", "# Reasoning Path:\nTaylor Swift -> award.award_winner.awards_won -> m.0v90n_n -> award.award_honor.award -> Billboard Music Award for Top Digital Songs Artist\n# Answer:\nBillboard Music Award for Top Digital Songs Artist", "# Reasoning Path:\nTaylor Swift -> award.award_winner.awards_won -> m.0105d827 -> award.award_honor.honored_for -> Highway Don\u2019t Care\n# Answer:\nHighway Don\u2019t Care", "# Reasoning Path:\nTaylor Swift -> award.award_winner.awards_won -> m.0nfm_g7 -> award.award_honor.award -> American Music Award for Favorite Pop/Rock Female Artist\n# Answer:\nAmerican Music Award for Favorite Pop/Rock Female Artist", "# Reasoning Path:\nTaylor Swift -> film.actor.film -> m.0y4q1x7 -> film.performance.character -> Rosemary\n# Answer:\nRosemary", "# Reasoning Path:\nTaylor Swift -> award.award_winner.awards_won -> m.0nfm_g7 -> award.award_honor.ceremony -> American Music Awards of 2009\n# Answer:\nAmerican Music Awards of 2009", "# Reasoning Path:\nTaylor Swift -> film.actor.film -> m.0y4q1x7 -> freebase.valuenotation.is_reviewed -> Character\n# Answer:\nCharacter", "# Reasoning Path:\nTaylor Swift -> film.actor.film -> m.0znq0tb -> film.performance.film -> Jonas Brothers: The Concert Experience\n# Answer:\nJonas Brothers: The Concert Experience"], "ground_truth": ["Valentine's Day", "Hannah Montana: The Movie", "The Giver", "The Lorax", "Jonas Brothers: The Concert Experience"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.26666666666666666, "ans_precission": 0.2, "ans_recall": 0.4, "path_f1": 0.22222222222222224, "path_precision": 0.2, "path_recall": 0.25, "path_ans_f1": 0.26666666666666666, "path_ans_precision": 0.2, "path_ans_recall": 0.4}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.office_position_or_title -> Chancellor of Germany\n# Answer:\nChancellor of Germany", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.office_position_or_title -> F\u00fchrer\n# Answer:\nF\u00fchrer", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> organization.organization.geographic_scope -> Germany\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.basic_title -> F\u00fchrer\n# Answer:\nF\u00fchrer", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.place_founded -> Nazi Germany\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.founders -> Heinrich Himmler\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> base.schemastaging.context_name.pronunciation -> g.125_pt37m\n# Answer:\nNazi Party"], "ground_truth": ["Nazi Germany"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.28571428571428564, "path_precision": 0.3, "path_recall": 0.2727272727272727, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-540", "prediction": ["# Reasoning Path:\nBrett Favre -> sports.drafted_athlete.drafted -> m.03w8mk1 -> sports.sports_league_draft_pick.school -> University of Southern Mississippi\n# Answer:\nUniversity of Southern Mississippi", "# Reasoning Path:\nBrett Favre -> people.person.education -> m.03gkqk_ -> education.education.institution -> University of Southern Mississippi\n# Answer:\nUniversity of Southern Mississippi", "# Reasoning Path:\nBrett Favre -> sports.drafted_athlete.drafted -> m.03w8mk1 -> sports.sports_league_draft_pick.team -> Atlanta Falcons\n# Answer:\nAtlanta Falcons", "# Reasoning Path:\nBrett Favre -> common.topic.image -> brett-favre.jpg -> common.image.size -> m.063ffjf\n# Answer:\nbrett-favre.jpg", "# Reasoning Path:\nBrett Favre -> common.topic.image -> BrettFavre -> common.image.size -> m.029jwyn\n# Answer:\nBrettFavre", "# Reasoning Path:\nBrett Favre -> common.topic.image -> Brett Favre as Jet1-edit -> common.image.size -> m.04y2174\n# Answer:\nBrett Favre as Jet1-edit", "# Reasoning Path:\nBrett Favre -> common.topic.image -> BrettFavre -> common.image.appears_in_topic_gallery -> Athletes who should hang it up\n# Answer:\nBrettFavre"], "ground_truth": ["University of Southern Mississippi"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.2857142857142857, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.2857142857142857, "path_recall": 1.0, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 1.0}
{"id": "WebQTest-542", "prediction": ["# Reasoning Path:\nAlex Chilton -> people.deceased_person.place_of_death -> New Orleans -> common.topic.subject_of -> Sweet Magnolia Tours\n# Answer:\nNew Orleans", "# Reasoning Path:\nAlex Chilton -> people.deceased_person.place_of_death -> New Orleans -> base.biblioness.bibs_location.state -> Louisiana\n# Answer:\nNew Orleans", "# Reasoning Path:\nAlex Chilton -> people.person.parents -> Sidney Chilton -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nSidney Chilton", "# Reasoning Path:\nAlex Chilton -> people.person.parents -> Sidney Chilton -> people.person.gender -> Male\n# Answer:\nSidney Chilton", "# Reasoning Path:\nAlex Chilton -> music.artist.album -> 19 Years: A Collection of Alex Chilton -> common.topic.notable_for -> g.1256t_zr4\n# Answer:\n19 Years: A Collection of Alex Chilton", "# Reasoning Path:\nAlex Chilton -> music.artist.album -> 19 Years: A Collection of Alex Chilton -> music.album.album_content_type -> Compilation album\n# Answer:\n19 Years: A Collection of Alex Chilton", "# Reasoning Path:\nAlex Chilton -> people.person.parents -> Sidney Chilton -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nSidney Chilton", "# Reasoning Path:\nAlex Chilton -> music.artist.album -> 1970 -> common.topic.notable_types -> Musical Album\n# Answer:\n1970", "# Reasoning Path:\nAlex Chilton -> people.person.parents -> Sidney Chilton -> freebase.valuenotation.has_value -> Parents\n# Answer:\nSidney Chilton", "# Reasoning Path:\nAlex Chilton -> music.artist.album -> 1970 -> common.topic.notable_for -> g.1258ysr17\n# Answer:\n1970"], "ground_truth": ["New Orleans"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-543", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.02wp75f -> education.education.institution -> Boston University\n# Answer:\nBoston University", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.04hddst -> education.education.institution -> Morehouse College\n# Answer:\nMorehouse College", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> education.education.institution -> Crozer Theological Seminary\n# Answer:\nCrozer Theological Seminary", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.02wp75f -> education.education.degree -> PhD\n# Answer:\nPhD", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.04hddst -> education.education.degree -> Bachelor of Arts\n# Answer:\nBachelor of Arts", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> education.education.degree -> Bachelor of Divinity\n# Answer:\nBachelor of Divinity", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nAlpha Man: The Brotherhood of MLK", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> common.topic.article -> m.05nnh0\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> freebase.valuenotation.has_value -> Minor\n# Answer:\nMinor"], "ground_truth": ["Morehouse College", "Boston University", "Crozer Theological Seminary"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-545", "prediction": ["# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.cause_of_death -> Anal cancer -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nAnal cancer", "# Reasoning Path:\nFarrah Fawcett -> common.topic.notable_for -> g.125f8rws1\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.cause_of_death -> Anal cancer -> medicine.disease.parent_disease -> Cancer\n# Answer:\nAnal cancer", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.cause_of_death -> Anal cancer -> common.topic.webpage -> m.09w_c2s\n# Answer:\nAnal cancer", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.cause_of_death -> Anal cancer -> common.topic.webpage -> m.09wx5h4\n# Answer:\nAnal cancer", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.cause_of_death -> Anal cancer -> common.topic.webpage -> m.09wzc81\n# Answer:\nAnal cancer", "# Reasoning Path:\nFarrah Fawcett -> common.topic.webpage -> m.0773vxj -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nFarrah Fawcett -> common.topic.webpage -> m.08wwplb -> common.webpage.resource -> Farrah Fawcett: 16 highlights\n# Answer:\nFarrah Fawcett: 16 highlights", "# Reasoning Path:\nFarrah Fawcett -> common.topic.webpage -> m.0773vxj -> common.webpage.resource -> TV and Movie Trivia Tribute\n# Answer:\nTV and Movie Trivia Tribute", "# Reasoning Path:\nFarrah Fawcett -> common.topic.webpage -> m.094039t -> common.webpage.resource -> '70s Comebacks of the Week\n# Answer:\n'70s Comebacks of the Week"], "ground_truth": ["Anal cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-546", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Schuyler Frances Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSchuyler Frances Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.spouse -> Tracy Pollan\n# Answer:\nTracy Pollan", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Schuyler Frances Fox -> common.topic.notable_types -> Person\n# Answer:\nSchuyler Frances Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Schuyler Frances Fox -> people.person.sibling_s -> m.0j217k5\n# Answer:\nSchuyler Frances Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.profession -> Actor\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.location_of_ceremony -> Arlington\n# Answer:\nArlington", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Schuyler Frances Fox -> people.person.sibling_s -> m.0tjswcf\n# Answer:\nSchuyler Frances Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.sibling_s -> m.0j217jw\n# Answer:\nSam Michael Fox"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-547", "prediction": ["# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.image -> 10colonesbcrfront\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.webpage -> m.04m2m2k\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60wmkvh\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.11b71w3_6h\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_5xgy\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_6n42\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.1hhc37pjz\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.1hhc37x5q\n# Answer:\nlocation.statistical_region.external_debt_stock"], "ground_truth": ["Costa Rican col\u00f3n"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-548", "prediction": ["# Reasoning Path:\nJoe Walsh -> music.group_member.instruments_played -> Bass guitar -> music.instrument.variation -> Kalindula\n# Answer:\nBass guitar", "# Reasoning Path:\nJoe Walsh -> music.group_member.instruments_played -> Bass guitar -> common.topic.webpage -> m.09wnh17\n# Answer:\nBass guitar", "# Reasoning Path:\nJoe Walsh -> music.group_member.instruments_played -> Bass guitar -> common.topic.subject_of -> Reverb\n# Answer:\nBass guitar", "# Reasoning Path:\nJoe Walsh -> music.group_member.instruments_played -> Guitar -> music.instrument.variation -> Bass guitar\n# Answer:\nGuitar", "# Reasoning Path:\nJoe Walsh -> music.guitarist.guitars_played -> Fender Stratocaster -> music.guitar.guitarists -> Adrian Smith\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nJoe Walsh -> music.guitarist.guitars_played -> Fender Stratocaster -> common.topic.image -> Fender strat\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nJoe Walsh -> music.group_member.instruments_played -> Guitar -> music.instrument.variation -> Fretless guitar\n# Answer:\nGuitar", "# Reasoning Path:\nJoe Walsh -> music.guitarist.guitars_played -> Fender Stratocaster -> symbols.namesake.named_after -> Leo Fender\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nJoe Walsh -> music.group_member.instruments_played -> Guitar -> common.topic.subject_of -> Luke Marchie\n# Answer:\nGuitar", "# Reasoning Path:\nJoe Walsh -> music.group_member.instruments_played -> Acoustic guitar -> common.topic.subject_of -> Derek Carter Live - The Spins\n# Answer:\nAcoustic guitar"], "ground_truth": ["Fender Stratocaster"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-549", "prediction": ["# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0fvn -> base.schemastaging.phone_sandbox.service_language -> Spanish Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0fvn -> base.schemastaging.phone_sandbox.service_language -> English Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0d6v -> base.schemastaging.phone_sandbox.service_language -> Spanish Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0d6v -> base.schemastaging.phone_sandbox.service_language -> English Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0fvn -> base.schemastaging.phone_sandbox.service_language -> French\n# Answer:\nFrench", "# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0fvn -> freebase.valuenotation.is_reviewed -> Number\n# Answer:\nNumber", "# Reasoning Path:\nEuropean Union -> location.statistical_region.broadband_penetration_rate -> g.1245_6ndt\n# Answer:\nlocation.statistical_region.broadband_penetration_rate", "# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0fvn -> freebase.valuenotation.has_no_value -> Saturday closing time\n# Answer:\nSaturday closing time", "# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0d6v -> base.schemastaging.phone_sandbox.service_location -> Earth\n# Answer:\nEarth", "# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0d6v -> base.schemastaging.phone_sandbox.service_language -> French\n# Answer:\nFrench"], "ground_truth": ["Greek Language", "German Language", "Swedish Language", "Spanish Language", "Portuguese Language", "English Language", "Dutch Language", "Italian Language", "Polish Language", "French"], "ans_acc": 0.3, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.6, "ans_recall": 0.3, "path_f1": 0.5714285714285713, "path_precision": 0.6, "path_recall": 0.5454545454545454, "path_ans_f1": 0.4, "path_ans_precision": 0.6, "path_ans_recall": 0.3}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> common.topic.notable_types -> Profession\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.specialization_of -> Musician\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.track_contributions -> g.11b7_lvdf2\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.specialization_of -> Musician\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> http://www.discogs.com/artist/Michael+Bubl%E9 -> common.webpage.category -> Topic Webpage\n# Answer:\nhttp://www.discogs.com/artist/Michael+Bubl%E9", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.03lpqjt -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> common.topic.notable_types -> Profession\n# Answer:\nSongwriter"], "ground_truth": ["Actor", "Singer", "Songwriter"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-550", "prediction": ["# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.starring_roles -> m.04f2t4z -> tv.regular_tv_appearance.series -> Reality Check\n# Answer:\nReality Check", "# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.starring_roles -> m.0bmltz0 -> tv.regular_tv_appearance.series -> Gladiators 2000\n# Answer:\nGladiators 2000", "# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.starring_roles -> m.0bmqp0y -> tv.regular_tv_appearance.series -> Click\n# Answer:\nClick", "# Reasoning Path:\nRyan Seacrest -> tv.tv_producer.programs_produced -> g.11byrxg698\n# Answer:\ntv.tv_producer.programs_produced", "# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.starring_roles -> m.04f2t4z -> tv.regular_tv_appearance.character -> Jack Craft\n# Answer:\nJack Craft", "# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.guest_roles -> m.0bgmqnf -> tv.tv_guest_role.episodes_appeared_in -> 5/21/04\n# Answer:\n5/21/04", "# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.starring_roles -> m.0bmltz0 -> tv.regular_tv_appearance.special_performance_type -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.guest_roles -> m.0bgn4x7 -> tv.tv_guest_role.episodes_appeared_in -> The 59th Annual Emmy Awards\n# Answer:\nThe 59th Annual Emmy Awards", "# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.starring_roles -> m.0bmqp0y -> tv.regular_tv_appearance.special_performance_type -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nRyan Seacrest -> tv.tv_producer.programs_produced -> m.0101b9cr -> tv.tv_producer_term.program -> I Wanna Marry Harry\n# Answer:\nI Wanna Marry Harry"], "ground_truth": ["On Air with Ryan Seacrest", "American Top 40"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-552", "prediction": ["# Reasoning Path:\nItaly -> location.country.official_language -> Italian Language -> language.human_language.main_country -> Slovenia\n# Answer:\nItalian Language", "# Reasoning Path:\nItaly -> location.country.languages_spoken -> Italian Language -> language.human_language.main_country -> Slovenia\n# Answer:\nItalian Language", "# Reasoning Path:\nItaly -> location.country.official_language -> Italian Language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nItalian Language", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.official_language -> Italian Language -> language.human_language.countries_spoken_in -> Vatican City\n# Answer:\nItalian Language", "# Reasoning Path:\nItaly -> location.country.languages_spoken -> Italian Language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nItalian Language", "# Reasoning Path:\nItaly -> location.country.official_language -> Italian Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nItalian Language", "# Reasoning Path:\nItaly -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Vatican City\n# Answer:\nItalian Language", "# Reasoning Path:\nItaly -> location.country.languages_spoken -> Occitan language -> language.human_language.main_country -> Monaco\n# Answer:\nOccitan language", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp"], "ground_truth": ["Italian Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-554", "prediction": ["# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Andorra\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Belgium\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60xs9d2\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> France\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc38hhk\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc3dvnk\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.1hhc39shg\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.1hhc3h715\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-555", "prediction": ["# Reasoning Path:\nDrew Barrymore -> base.popstra.celebrity.substance_abuse -> m.063gck7 -> base.popstra.substance_abuse.substance -> alcohol and drugs\n# Answer:\nalcohol and drugs", "# Reasoning Path:\nDrew Barrymore -> film.director.film -> Whip It -> film.film.genre -> Comedy\n# Answer:\nWhip It", "# Reasoning Path:\nDrew Barrymore -> film.director.film -> Whip It -> film.film.language -> English Language\n# Answer:\nWhip It", "# Reasoning Path:\nDrew Barrymore -> film.producer.films_executive_produced -> Olive, the Other Reindeer -> tv.tv_program.tv_producer -> m.0sx3ql4\n# Answer:\nOlive, the Other Reindeer", "# Reasoning Path:\nDrew Barrymore -> film.director.film -> Whip It -> film.film.genre -> Drama\n# Answer:\nWhip It", "# Reasoning Path:\nDrew Barrymore -> film.director.film -> Whip It -> film.film.genre -> Sports films\n# Answer:\nWhip It", "# Reasoning Path:\nDrew Barrymore -> film.director.film -> Whip It -> media_common.netflix_title.netflix_genres -> Comedies\n# Answer:\nWhip It", "# Reasoning Path:\nDrew Barrymore -> film.producer.films_executive_produced -> Olive, the Other Reindeer -> tv.tv_program.tv_producer -> m.0sx3qqq\n# Answer:\nOlive, the Other Reindeer", "# Reasoning Path:\nDrew Barrymore -> film.producer.films_executive_produced -> Olive, the Other Reindeer -> film.film.starring -> m.0g5k629\n# Answer:\nOlive, the Other Reindeer", "# Reasoning Path:\nDrew Barrymore -> film.producer.films_executive_produced -> Olive, the Other Reindeer -> tv.tv_program.tv_producer -> m.0sx3qzt\n# Answer:\nOlive, the Other Reindeer"], "ground_truth": ["Cocaine", "Alcoholic beverage"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-556", "prediction": ["# Reasoning Path:\nSt George's Day -> time.holiday.day_of_year -> April 23 -> common.topic.notable_types -> Day Of Year\n# Answer:\nApril 23", "# Reasoning Path:\nSt George's Day -> time.holiday.day_of_year -> April 23 -> common.topic.article -> m.0tj3\n# Answer:\nApril 23", "# Reasoning Path:\nSt George's Day -> time.holiday.day_of_year -> April 23 -> time.day_of_year.holidays -> World Book Day\n# Answer:\nApril 23", "# Reasoning Path:\nSt George's Day -> time.holiday.day_of_year -> April 23 -> time.day_of_year.holidays -> National Cherry Cheesecake Day\n# Answer:\nApril 23", "# Reasoning Path:\nSt George's Day -> time.holiday.day_of_year -> April 23 -> time.day_of_year.holidays -> All Ukrainian Day of psychologist\n# Answer:\nApril 23", "# Reasoning Path:\nSt George's Day -> time.holiday.featured_in_religions -> Christianity -> religion.religion.deities -> Jesus Christ\n# Answer:\nChristianity", "# Reasoning Path:\nSt George's Day -> common.topic.image -> Shakespeare -> common.image.appears_in_topic_gallery -> ...O brave new world, That has such people in't!\n# Answer:\nShakespeare", "# Reasoning Path:\nSt George's Day -> common.topic.image -> Shakespeare -> common.image.size -> m.02bk77b\n# Answer:\nShakespeare", "# Reasoning Path:\nSt George's Day -> time.holiday.featured_in_religions -> Christianity -> book.periodical_subject.periodicals -> 7ball\n# Answer:\nChristianity", "# Reasoning Path:\nSt George's Day -> time.holiday.featured_in_religions -> Christianity -> religion.religion.deities -> Holy Spirit\n# Answer:\nChristianity"], "ground_truth": ["April 23"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-557", "prediction": ["# Reasoning Path:\nRoman Empire -> location.country.form_of_government -> Autocracy -> common.topic.notable_for -> g.125b8tl9d\n# Answer:\nAutocracy", "# Reasoning Path:\nRoman Empire -> location.country.form_of_government -> Autocracy -> government.form_of_government.countries -> Nazi Germany\n# Answer:\nAutocracy", "# Reasoning Path:\nRoman Empire -> location.country.form_of_government -> Autocracy -> common.topic.notable_types -> Form of Government\n# Answer:\nAutocracy", "# Reasoning Path:\nRoman Empire -> book.book_subject.works -> The Roman Empire and Its Germanic Peoples -> book.written_work.subjects -> Germanic peoples\n# Answer:\nThe Roman Empire and Its Germanic Peoples", "# Reasoning Path:\nRoman Empire -> location.country.form_of_government -> Autocracy -> government.form_of_government.countries -> Byzantine Empire\n# Answer:\nAutocracy", "# Reasoning Path:\nRoman Empire -> location.country.form_of_government -> Autocracy -> government.form_of_government.countries -> Congress Poland\n# Answer:\nAutocracy", "# Reasoning Path:\nRoman Empire -> book.book_subject.works -> The Roman Empire and Its Germanic Peoples -> book.book.editions -> The Roman Empire and its Germanic peoples\n# Answer:\nThe Roman Empire and Its Germanic Peoples", "# Reasoning Path:\nRoman Empire -> military.military_combatant.military_conflicts -> m.048zsyf -> military.military_combatant_group.conflict -> First Jewish\u2013Roman War\n# Answer:\nFirst Jewish\u2013Roman War", "# Reasoning Path:\nRoman Empire -> military.military_combatant.military_conflicts -> m.048zt3v -> military.military_combatant_group.conflict -> Bar Kokhba revolt\n# Answer:\nBar Kokhba revolt", "# Reasoning Path:\nRoman Empire -> book.book_subject.works -> The Roman Empire and Its Germanic Peoples -> common.topic.notable_for -> g.125byq35r\n# Answer:\nThe Roman Empire and Its Germanic Peoples"], "ground_truth": ["Autocracy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-559", "prediction": ["# Reasoning Path:\nRobin Gibb -> music.group_member.instruments_played -> Violin -> common.topic.notable_types -> Musical instrument\n# Answer:\nViolin", "# Reasoning Path:\nRobin Gibb -> music.group_member.instruments_played -> Piano -> common.topic.notable_types -> Musical instrument\n# Answer:\nPiano", "# Reasoning Path:\nRobin Gibb -> music.artist.track_contributions -> m.0110195t -> music.track_contribution.track -> Islands in the Stream\n# Answer:\nIslands in the Stream", "# Reasoning Path:\nRobin Gibb -> music.artist.track_contributions -> m.0110253_ -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nRobin Gibb -> music.artist.label -> Universal Music Group -> common.topic.notable_types -> Record label\n# Answer:\nUniversal Music Group", "# Reasoning Path:\nRobin Gibb -> music.artist.track_contributions -> m.0110195t -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nRobin Gibb -> music.artist.label -> Universal Music Group -> business.business_operation.industry -> Music\n# Answer:\nUniversal Music Group", "# Reasoning Path:\nRobin Gibb -> music.artist.track_contributions -> m.0110253_ -> music.track_contribution.track -> Don't Cry Alone\n# Answer:\nDon't Cry Alone", "# Reasoning Path:\nRobin Gibb -> music.artist.track_contributions -> m.013885yn -> music.track_contribution.track -> Edge of the Universe\n# Answer:\nEdge of the Universe", "# Reasoning Path:\nRobin Gibb -> music.artist.label -> ATCO Records -> common.topic.webpage -> m.04lsq5b\n# Answer:\nATCO Records"], "ground_truth": ["Harmonica", "Piano", "Drum machine", "Vocals", "Violin"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.48, "ans_precission": 0.4, "ans_recall": 0.6, "path_f1": 0.14035087719298245, "path_precision": 0.4, "path_recall": 0.0851063829787234, "path_ans_f1": 0.48, "path_ans_precision": 0.4, "path_ans_recall": 0.6}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.location.containedby -> Kansas City, MO-KS Metropolitan Statistical Area\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas City, MO-KS Metropolitan Statistical Area\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas City, MO-KS Metropolitan Statistical Area\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.statistical_region.co2_emissions_mobile -> m.045l26t\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111 -> location.location.containedby -> Wyandotte County\n# Answer:\n66111", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Wyandotte County -> location.statistical_region.co2_emissions_mobile -> m.045l26t\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.statistical_region.co2_emissions_mobile -> m.045l26t\n# Answer:\nWyandotte County"], "ground_truth": ["Wyandotte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-560", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zsqt -> film.performance.actor -> David Prowse\n# Answer:\nDavid Prowse", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nThe Making of Star Wars", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.film -> Star Wars\n# Answer:\nStar Wars", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zsqt -> film.performance.film -> The Making of Star Wars\n# Answer:\nThe Making of Star Wars", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Uncredited\n# Answer:\nUncredited", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice"], "ground_truth": ["Matt Lanter"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-561", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.04j60k7 -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nUnited States Representative", "# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.0bqspr2 -> government.government_position_held.office_position_or_title -> Member of Illinois House of Representatives\n# Answer:\nMember of Illinois House of Representatives", "# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.04j60k7 -> government.government_position_held.governmental_body -> United States House of Representatives\n# Answer:\nUnited States House of Representatives", "# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.0446bdb -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.0bqspr2 -> government.government_position_held.jurisdiction_of_office -> Illinois\n# Answer:\nIllinois", "# Reasoning Path:\nAbraham Lincoln -> film.film_subject.films -> Abraham Lincoln's Clemency -> film.film.directed_by -> Theodore Wharton\n# Answer:\nAbraham Lincoln's Clemency", "# Reasoning Path:\nAbraham Lincoln -> film.film_subject.films -> Gore Vidal's Lincoln -> film.film.country -> United States of America\n# Answer:\nGore Vidal's Lincoln", "# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.0bqspr2 -> government.government_position_held.governmental_body -> Illinois House of Representatives\n# Answer:\nIllinois House of Representatives", "# Reasoning Path:\nAbraham Lincoln -> film.film_subject.films -> Abraham Lincoln: Vampire Hunter -> film.film.subjects -> American Civil War\n# Answer:\nAbraham Lincoln: Vampire Hunter", "# Reasoning Path:\nAbraham Lincoln -> film.film_subject.films -> Gore Vidal's Lincoln -> film.film.language -> English Language\n# Answer:\nGore Vidal's Lincoln"], "ground_truth": ["Abraham Lincoln 1865 presidential inauguration", "Abraham Lincoln 1861 presidential inauguration"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-562", "prediction": ["# Reasoning Path:\nTexas Rangers -> sports.sports_team.arena_stadium -> Rangers Ballpark in Arlington -> location.location.containedby -> Arlington\n# Answer:\nRangers Ballpark in Arlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.arena_stadium -> Rangers Ballpark in Arlington -> location.location.geolocation -> m.02_t4f8\n# Answer:\nRangers Ballpark in Arlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.arena_stadium -> Rangers Ballpark in Arlington -> location.location.containedby -> Texas\n# Answer:\nRangers Ballpark in Arlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.location -> Arlington -> location.statistical_region.population -> g.11b66bywm8\n# Answer:\nArlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.location -> Arlington -> location.location.contains -> Rangers Ballpark in Arlington\n# Answer:\nArlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.location -> Arlington -> location.metropolitan_area_anchor.metropolitan_area -> Fort Worth-Arlington, TX Metropolitan Division\n# Answer:\nArlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.location -> Arlington -> location.statistical_region.population -> g.11b7td7_9_\n# Answer:\nArlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.location -> Arlington -> location.statistical_region.population -> g.11b7tm_xv7\n# Answer:\nArlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.location -> Arlington -> location.location.contains -> University of Texas at Arlington School of Architecture\n# Answer:\nArlington", "# Reasoning Path:\nTexas Rangers -> sports.professional_sports_team.draft_picks -> m.04vw_l5 -> sports.sports_league_draft_pick.school -> University Yale\n# Answer:\nUniversity Yale"], "ground_truth": ["Rangers Ballpark in Arlington"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-564", "prediction": ["# Reasoning Path:\nArkansas State Capitol -> location.location.containedby -> Little Rock -> location.location.nearby_airports -> Clinton National Airport\n# Answer:\nLittle Rock", "# Reasoning Path:\nArkansas State Capitol -> location.location.containedby -> Little Rock -> location.location.time_zones -> Central Time Zone\n# Answer:\nLittle Rock", "# Reasoning Path:\nArkansas State Capitol -> common.image.appears_in_topic_gallery -> George R. Mann -> people.person.gender -> Male\n# Answer:\nGeorge R. Mann", "# Reasoning Path:\nArkansas State Capitol -> common.image.appears_in_topic_gallery -> George R. Mann -> people.person.profession -> Architect\n# Answer:\nGeorge R. Mann", "# Reasoning Path:\nArkansas State Capitol -> common.topic.image -> The Capitol Building -> common.image.appears_in_topic_gallery -> Arkansas General Assembly\n# Answer:\nThe Capitol Building", "# Reasoning Path:\nArkansas State Capitol -> location.location.containedby -> Arkansas -> location.location.containedby -> Contiguous United States\n# Answer:\nArkansas", "# Reasoning Path:\nArkansas State Capitol -> common.image.appears_in_topic_gallery -> George R. Mann -> common.topic.notable_for -> g.1256qj9r7\n# Answer:\nGeorge R. Mann", "# Reasoning Path:\nArkansas State Capitol -> location.location.containedby -> Arkansas -> location.location.containedby -> United States of America\n# Answer:\nArkansas", "# Reasoning Path:\nArkansas State Capitol -> location.location.containedby -> Arkansas -> base.aareas.schema.administrative_area.administrative_children -> White County\n# Answer:\nArkansas", "# Reasoning Path:\nArkansas State Capitol -> location.location.containedby -> Little Rock -> film.film_location.featured_in_films -> Nine from Little Rock\n# Answer:\nLittle Rock"], "ground_truth": ["Little Rock"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-567", "prediction": ["# Reasoning Path:\nNogales -> location.citytown.postal_codes -> 85621 -> location.location.containedby -> Santa Cruz County\n# Answer:\n85621", "# Reasoning Path:\nNogales -> location.hud_foreclosure_area.ofheo_price_change -> m.07g9y5n\n# Answer:\nlocation.hud_foreclosure_area.ofheo_price_change", "# Reasoning Path:\nNogales -> location.citytown.postal_codes -> 85628 -> location.postal_code.country -> United States of America\n# Answer:\n85628", "# Reasoning Path:\nNogales -> location.statistical_region.population -> g.11b66h2c6g\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nNogales -> location.citytown.postal_codes -> 85621 -> location.location.contains -> Lourdes Catholic School\n# Answer:\n85621", "# Reasoning Path:\nNogales -> location.citytown.postal_codes -> 85621 -> location.postal_code.country -> United States of America\n# Answer:\n85621", "# Reasoning Path:\nNogales -> location.citytown.postal_codes -> 85628 -> common.topic.notable_for -> g.125614f5r\n# Answer:\n85628", "# Reasoning Path:\nNogales -> location.citytown.postal_codes -> 85662 -> common.topic.notable_for -> g.125d1ds3d\n# Answer:\n85662", "# Reasoning Path:\nNogales -> location.statistical_region.population -> g.11bymgs1xf\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nNogales -> location.citytown.postal_codes -> 85628 -> common.topic.notable_types -> Postal Code\n# Answer:\n85628"], "ground_truth": ["85662", "85621", "85628"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-568", "prediction": ["# Reasoning Path:\nMichael Vick -> people.person.education -> m.0n1k_2y -> education.education.institution -> Warwick High School\n# Answer:\nWarwick High School", "# Reasoning Path:\nMichael Vick -> common.topic.image -> VICKpb crop -> common.image.size -> m.03sv987\n# Answer:\nVICKpb crop", "# Reasoning Path:\nMichael Vick -> people.person.education -> m.0hpcsnj -> education.education.institution -> Virginia Tech\n# Answer:\nVirginia Tech", "# Reasoning Path:\nMichael Vick -> award.award_winner.awards_won -> m.0z88l31 -> award.award_honor.ceremony -> 2000 ESPY Awards\n# Answer:\n2000 ESPY Awards", "# Reasoning Path:\nMichael Vick -> award.award_winner.awards_won -> m.0731jvs -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nWinning work", "# Reasoning Path:\nMichael Vick -> common.topic.image -> Vick 6 -> common.image.size -> m.0bct9py\n# Answer:\nVick 6", "# Reasoning Path:\nMichael Vick -> award.award_winner.awards_won -> m.0z88l31 -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nWinning work", "# Reasoning Path:\nMichael Vick -> award.award_winner.awards_won -> m.0731k7n -> award.award_honor.award -> Galloping Gobbler\n# Answer:\nGalloping Gobbler", "# Reasoning Path:\nMichael Vick -> award.award_winner.awards_won -> m.0731jvs -> award.award_honor.award -> Best NFL Player ESPY Award\n# Answer:\nBest NFL Player ESPY Award"], "ground_truth": ["2009"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-570", "prediction": ["# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Lawyer -> common.topic.notable_types -> Profession\n# Answer:\nLawyer", "# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Judge -> common.topic.notable_types -> Profession\n# Answer:\nJudge", "# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Lawyer -> people.profession.specializations -> Criminal defense lawyer\n# Answer:\nLawyer", "# Reasoning Path:\nWilliam Howard Taft -> base.yalebase.secret_society_member.belonged_to -> m.046xl1n -> base.yalebase.secret_society_membership.society -> Skull and Bones\n# Answer:\nSkull and Bones", "# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Lawyer -> common.topic.subjects -> Accident Advice Helpline\n# Answer:\nLawyer", "# Reasoning Path:\nWilliam Howard Taft -> book.author.works_written -> William Howard Taft, 1857-1930 -> common.topic.notable_types -> Book\n# Answer:\nWilliam Howard Taft, 1857-1930", "# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Lawyer -> people.profession.specializations -> Jurist\n# Answer:\nLawyer", "# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Lawyer -> common.topic.subjects -> Brian White & Associates, P.C.\n# Answer:\nLawyer", "# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Jurist -> base.descriptive_names.names.descriptive_name -> m.0105y_7z\n# Answer:\nJurist", "# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Jurist -> common.topic.article -> m.0dl0h\n# Answer:\nJurist"], "ground_truth": ["Jurist", "Judge", "Lawyer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-572", "prediction": ["# Reasoning Path:\nRobin Cook -> common.topic.notable_for -> g.125bt8v5y\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> location.location.containedby -> United Kingdom\n# Answer:\nInverness", "# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> travel.travel_destination.tourist_attractions -> Urquhart Castle\n# Answer:\nInverness", "# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> location.location.containedby -> Inverness-shire\n# Answer:\nInverness", "# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> location.statistical_region.population -> m.0cl9fgl\n# Answer:\nInverness", "# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> travel.travel_destination.tourist_attractions -> Aigas Field Centre\n# Answer:\nInverness", "# Reasoning Path:\nRobin Cook -> tv.tv_actor.guest_roles -> m.09nbpck -> tv.tv_guest_role.episodes_appeared_in -> 06 January 2002\n# Answer:\n06 January 2002", "# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> location.location.containedby -> Scotland\n# Answer:\nInverness", "# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> travel.travel_destination.tourist_attractions -> Beauly Priory\n# Answer:\nInverness", "# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> location.statistical_region.population -> m.0cl9fgq\n# Answer:\nInverness"], "ground_truth": ["Inverness"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-573", "prediction": ["# Reasoning Path:\nMelba Pattillo Beals -> people.person.place_of_birth -> Little Rock -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nLittle Rock", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.place_of_birth -> Little Rock -> base.biblioness.bibs_location.state -> Arkansas\n# Answer:\nLittle Rock", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.places_lived -> m.03pds23 -> people.place_lived.location -> Arkansas\n# Answer:\nArkansas", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.place_of_birth -> Little Rock -> location.citytown.postal_codes -> 72201\n# Answer:\nLittle Rock", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.place_of_birth -> Little Rock -> location.citytown.postal_codes -> 72202\n# Answer:\nLittle Rock", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.education -> m.0n0d4k7 -> education.education.institution -> San Francisco State University\n# Answer:\nSan Francisco State University", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.education -> m.0n1jwpr -> education.education.institution -> Columbia University\n# Answer:\nColumbia University", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.place_of_birth -> Little Rock -> location.citytown.postal_codes -> 72203\n# Answer:\nLittle Rock"], "ground_truth": ["Arkansas"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.125, "path_recall": 0.3333333333333333, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-575", "prediction": ["# Reasoning Path:\nLouisiana -> government.governmental_jurisdiction.governing_officials -> m.04ktp68 -> government.government_position_held.office_holder -> William C. C. Claiborne\n# Answer:\nWilliam C. C. Claiborne", "# Reasoning Path:\nLouisiana -> government.governmental_jurisdiction.governing_officials -> m.010f0pjr -> government.government_position_held.office_holder -> Scott Angelle\n# Answer:\nScott Angelle", "# Reasoning Path:\nLouisiana -> government.political_district.representatives -> m.0bws06r -> government.government_position_held.office_holder -> Huey Long\n# Answer:\nHuey Long", "# Reasoning Path:\nLouisiana -> government.governmental_jurisdiction.governing_officials -> m.010f0pn6 -> government.government_position_held.office_holder -> Jay Dardenne\n# Answer:\nJay Dardenne", "# Reasoning Path:\nLouisiana -> government.political_district.representatives -> m.0bws0xr -> government.government_position_held.office_holder -> Edward Livingston\n# Answer:\nEdward Livingston", "# Reasoning Path:\nLouisiana -> government.governmental_jurisdiction.governing_officials -> m.04ktp68 -> government.government_position_held.office_position_or_title -> Governor of Louisiana\n# Answer:\nGovernor of Louisiana", "# Reasoning Path:\nLouisiana -> government.political_district.representatives -> m.010d1ztj -> government.government_position_held.office_holder -> John Breaux\n# Answer:\nJohn Breaux", "# Reasoning Path:\nLouisiana -> government.governmental_jurisdiction.governing_officials -> m.010f0pjr -> government.government_position_held.basic_title -> Lieutenant Governor\n# Answer:\nLieutenant Governor", "# Reasoning Path:\nLouisiana -> government.political_district.representatives -> m.0bws06r -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nLouisiana -> government.political_district.representatives -> m.0bws0xr -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator"], "ground_truth": ["Robert C. Nicholas", "Eligius Fromentin", "Charles Magill Conrad", "Alexandre Mouton", "Alexander Barrow", "Russell B. Long", "Huey Long", "Jeremiah B. Howell", "Allan B. Magruder", "Pierre Soul\u00e9", "Judah P. Benjamin", "Jean Noel Destr\u00e9han", "Henry Johnson", "Thomas Posey", "Edward Douglass White", "Alexander Porter", "William C. C. Claiborne", "Josiah S. Johnston", "Mary Landrieu", "Charles Dominique Joseph Bouligny", "David Vitter", "John Slidell", "Edward Livingston", "John Breaux", "George A. Waggaman", "James Brown"], "ans_acc": 0.15384615384615385, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.4, "ans_recall": 0.15384615384615385, "path_f1": 0.19047619047619047, "path_precision": 0.4, "path_recall": 0.125, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.4, "path_ans_recall": 0.15384615384615385}
{"id": "WebQTest-576", "prediction": ["# Reasoning Path:\nCentral America -> location.location.contains -> La Amistad International Park -> location.location.contains -> Talamancan montane forests\n# Answer:\nLa Amistad International Park", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.location.containedby -> North America\n# Answer:\nBelize", "# Reasoning Path:\nCentral America -> location.location.contains -> La Amistad International Park -> common.topic.notable_for -> g.12596bzjx\n# Answer:\nLa Amistad International Park", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.location.contains -> Belize District\n# Answer:\nBelize", "# Reasoning Path:\nCentral America -> location.location.contains -> Panama -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nPanama", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.location.contains -> Cayo District\n# Answer:\nBelize", "# Reasoning Path:\nCentral America -> meteorology.cyclone_affected_area.cyclones -> 1924 Cuba hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\n1924 Cuba hurricane", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.location.contains -> Corozal District\n# Answer:\nBelize", "# Reasoning Path:\nCentral America -> location.location.contains -> Panama -> location.location.partiallycontains -> m.0wg9hnr\n# Answer:\nPanama", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.location.containedby -> Americas\n# Answer:\nBelize"], "ground_truth": ["El Salvador", "Honduras", "Gran Colombia", "Costa Rica", "Belize", "Guatemala", "Panama"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.40579710144927533, "ans_precission": 0.7, "ans_recall": 0.2857142857142857, "path_f1": 0.40579710144927533, "path_precision": 0.7, "path_recall": 0.2857142857142857, "path_ans_f1": 0.40579710144927533, "path_ans_precision": 0.7, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-577", "prediction": ["# Reasoning Path:\nDerek Fisher -> sports.drafted_athlete.drafted -> m.0cr2kgn -> sports.sports_league_draft_pick.school -> University of Arkansas at Little Rock\n# Answer:\nUniversity of Arkansas at Little Rock", "# Reasoning Path:\nDerek Fisher -> people.person.education -> m.0sw01cy -> education.education.institution -> University of Arkansas\n# Answer:\nUniversity of Arkansas", "# Reasoning Path:\nDerek Fisher -> sports.drafted_athlete.drafted -> m.0cr2kgn -> sports.sports_league_draft_pick.draft -> 1996 NBA draft\n# Answer:\n1996 NBA draft", "# Reasoning Path:\nDerek Fisher -> sports.drafted_athlete.drafted -> m.0cr2kgn -> sports.sports_league_draft_pick.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nDerek Fisher -> people.person.education -> m.030qjb6 -> education.education.institution -> University of Arkansas at Little Rock\n# Answer:\nUniversity of Arkansas at Little Rock", "# Reasoning Path:\nDerek Fisher -> people.person.education -> m.040jfy4 -> education.education.institution -> Parkview Arts and Science Magnet High School\n# Answer:\nParkview Arts and Science Magnet High School", "# Reasoning Path:\nDerek Fisher -> basketball.basketball_player.player_statistics -> m.04q9tdk -> basketball.basketball_player_stats.team -> Golden State Warriors\n# Answer:\nGolden State Warriors", "# Reasoning Path:\nDerek Fisher -> basketball.basketball_player.player_statistics -> m.04qc3sv -> basketball.basketball_player_stats.season -> 2000\u201301 NBA season\n# Answer:\n2000\u201301 NBA season", "# Reasoning Path:\nDerek Fisher -> basketball.basketball_player.player_statistics -> m.04q9tdk -> basketball.basketball_player_stats.season -> 2004\u201305 NBA season\n# Answer:\n2004\u201305 NBA season", "# Reasoning Path:\nDerek Fisher -> basketball.basketball_player.player_statistics -> m.04qhk3g -> basketball.basketball_player_stats.season -> 2007\u201308 NBA season\n# Answer:\n2007\u201308 NBA season"], "ground_truth": ["University of Arkansas at Little Rock", "University of Arkansas"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-578", "prediction": ["# Reasoning Path:\nKobe Bryant -> sports.drafted_athlete.drafted -> m.02h7cfz -> sports.sports_league_draft_pick.team -> Charlotte Hornets\n# Answer:\nCharlotte Hornets", "# Reasoning Path:\nKobe Bryant -> sports.drafted_athlete.drafted -> m.02h7cfz -> sports.sports_league_draft_pick.draft -> 1996 NBA draft\n# Answer:\n1996 NBA draft", "# Reasoning Path:\nKobe Bryant -> sports.drafted_athlete.drafted -> m.02h7cfz -> sports.sports_league_draft_pick.school -> Lower Merion High School\n# Answer:\nLower Merion High School", "# Reasoning Path:\nKobe Bryant -> basketball.basketball_player.player_statistics -> m.04qb_g9 -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nKobe Bryant -> people.person.places_lived -> m.0kfxz9n -> people.place_lived.location -> Los Angeles\n# Answer:\nLos Angeles", "# Reasoning Path:\nKobe Bryant -> basketball.basketball_player.player_statistics -> m.04qb_g9 -> basketball.basketball_player_stats.season -> 2006\u201307 NBA season\n# Answer:\n2006\u201307 NBA season", "# Reasoning Path:\nKobe Bryant -> basketball.basketball_player.player_statistics -> m.04qbwp0 -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nKobe Bryant -> basketball.basketball_player.player_statistics -> m.04qbwp0 -> basketball.basketball_player_stats.season -> 1997\u201398 NBA season\n# Answer:\n1997\u201398 NBA season", "# Reasoning Path:\nKobe Bryant -> basketball.basketball_player.player_statistics -> m.04qcpcj -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nKobe Bryant -> people.person.places_lived -> m.03pp_xg -> people.place_lived.location -> Newport Beach\n# Answer:\nNewport Beach"], "ground_truth": ["Charlotte Hornets"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-579", "prediction": ["# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.location -> Charlotte -> common.topic.webpage -> m.03lnb3b\n# Answer:\nCharlotte", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.arena_stadium -> The Time Warner Cable Arena -> location.location.containedby -> Charlotte\n# Answer:\nThe Time Warner Cable Arena", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.location -> Charlotte -> common.topic.webpage -> m.07ldsgk\n# Answer:\nCharlotte", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.location -> Charlotte -> location.statistical_region.population -> g.11b674q2v0\n# Answer:\nCharlotte", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.location -> Charlotte -> common.topic.webpage -> m.09w7_w9\n# Answer:\nCharlotte", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.location -> Charlotte -> government.governmental_jurisdiction.governing_officials -> m.0103bhl1\n# Answer:\nCharlotte", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.location -> Charlotte -> location.statistical_region.population -> g.11b7tmvwpg\n# Answer:\nCharlotte", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.arena_stadium -> The Time Warner Cable Arena -> location.location.events -> 2008 NCAA Men's Division I Basketball Tournament\n# Answer:\nThe Time Warner Cable Arena", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.location -> Charlotte -> location.statistical_region.population -> g.11btsv6nnh\n# Answer:\nCharlotte", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.location -> Charlotte -> government.governmental_jurisdiction.governing_officials -> m.0k1bfyb\n# Answer:\nCharlotte"], "ground_truth": ["Charlotte"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> sports.sports_team_roster.team -> Montreal Royals\n# Answer:\nMontreal Royals", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h -> sports.sports_team_roster.team -> UCLA Bruins football\n# Answer:\nUCLA Bruins football", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> sports.sports_team_roster.position -> Second baseman\n# Answer:\nSecond baseman", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0gggrzr -> sports.sports_team_roster.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.season -> 1954 Major League Baseball season\n# Answer:\n1954 Major League Baseball season", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbp_k -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber"], "ground_truth": ["UCLA Bruins football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-580", "prediction": ["# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> Proto-Berber language -> common.topic.article -> m.05zyx79\n# Answer:\nProto-Berber language", "# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> Numidian language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nNumidian language", "# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> Proto-Berber language -> common.topic.notable_types -> Human Language\n# Answer:\nProto-Berber language", "# Reasoning Path:\nAlgeria -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b71sdykm\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> Numidian language -> common.topic.notable_types -> Human Language\n# Answer:\nNumidian language", "# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> French -> language.human_language.countries_spoken_in -> Tunisia\n# Answer:\nFrench", "# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nAlgeria -> location.statistical_region.agriculture_as_percent_of_gdp -> g.1hhc38hg_\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> French -> language.human_language.main_country -> France\n# Answer:\nFrench", "# Reasoning Path:\nAlgeria -> location.statistical_region.agriculture_as_percent_of_gdp -> g.1hhc3__tk\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp"], "ground_truth": ["Arabic Language", "Northern Berber languages", "Proto-Berber language", "Numidian language", "French", "Guanche Language", "Western Berber languages"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5316455696202531, "ans_precission": 0.7, "ans_recall": 0.42857142857142855, "path_f1": 0.5316455696202531, "path_precision": 0.7, "path_recall": 0.42857142857142855, "path_ans_f1": 0.5316455696202531, "path_ans_precision": 0.7, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-581", "prediction": ["# Reasoning Path:\nAnne Boleyn -> people.deceased_person.place_of_death -> Tower of London -> location.location.containedby -> London\n# Answer:\nTower of London", "# Reasoning Path:\nAnne Boleyn -> people.deceased_person.place_of_death -> Tower of London -> travel.tourist_attraction.near_travel_destination -> London\n# Answer:\nTower of London", "# Reasoning Path:\nAnne Boleyn -> people.deceased_person.place_of_death -> Tower of London -> location.location.containedby -> London Borough of Tower Hamlets\n# Answer:\nTower of London", "# Reasoning Path:\nAnne Boleyn -> people.deceased_person.place_of_death -> Tower of London -> base.prison.prison.prisoners -> m.04dn8sr\n# Answer:\nTower of London", "# Reasoning Path:\nAnne Boleyn -> film.film_subject.films -> The Other Boleyn Girl -> film.film.featured_film_locations -> Elstree Studios\n# Answer:\nThe Other Boleyn Girl", "# Reasoning Path:\nAnne Boleyn -> people.deceased_person.place_of_death -> Tower of London -> base.prison.prison.prisoners -> m.04dn8sz\n# Answer:\nTower of London", "# Reasoning Path:\nAnne Boleyn -> common.topic.notable_types -> Noble person -> freebase.type_hints.included_types -> Person\n# Answer:\nNoble person", "# Reasoning Path:\nAnne Boleyn -> people.deceased_person.place_of_death -> Tower of London -> base.prison.prison.prisoners -> m.04dn8t7\n# Answer:\nTower of London", "# Reasoning Path:\nAnne Boleyn -> common.topic.notable_types -> Noble person -> base.descriptive_names.names.descriptive_name -> m.012b_j83\n# Answer:\nNoble person", "# Reasoning Path:\nAnne Boleyn -> common.topic.notable_types -> Play -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nPlay"], "ground_truth": ["Tower of London"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-582", "prediction": ["# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Astronomer -> people.profession.specialization_of -> Scientist\n# Answer:\nAstronomer", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Werner Heisenberg\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Daniel Faraday\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.0101ddrk\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Mathematician -> base.descriptive_names.names.descriptive_name -> m.01066gkh\n# Answer:\nMathematician", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Astronomer -> common.topic.image -> Johannes Hevelius\n# Answer:\nAstronomer", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Emmett Brown\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> common.topic.image -> Albert Einstein 1947\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.01066smw\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Mathematician -> base.descriptive_names.names.descriptive_name -> m.010f3g_r\n# Answer:\nMathematician"], "ground_truth": ["Astronomer", "Mathematician", "Astrologer", "Physicist", "Scientist"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 1.0, "ans_recall": 0.6, "path_f1": 0.7499999999999999, "path_precision": 1.0, "path_recall": 0.6, "path_ans_f1": 0.888888888888889, "path_ans_precision": 1.0, "path_ans_recall": 0.8}
{"id": "WebQTest-583", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.02wp75f -> education.education.institution -> Boston University\n# Answer:\nBoston University", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> education.education.institution -> Crozer Theological Seminary\n# Answer:\nCrozer Theological Seminary", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0n_xlq0 -> education.education.institution -> Washington High School\n# Answer:\nWashington High School", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.02wp75f -> education.education.degree -> PhD\n# Answer:\nPhD", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> education.education.degree -> Bachelor of Divinity\n# Answer:\nBachelor of Divinity", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0n_xlq0 -> freebase.valuenotation.has_no_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nAlpha Man: The Brotherhood of MLK", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> freebase.valuenotation.has_value -> Minor\n# Answer:\nMinor", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> common.topic.article -> m.05nnh0\n# Answer:\nDexter Avenue Baptist Church"], "ground_truth": ["Boston University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-584", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Abhiyaza\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_for -> g.12582gf_5\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_types -> City/Town/Village\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Nanda\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> R\u0101hula\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Thousand Buddha Mountain -> location.location.containedby -> China\n# Answer:\nThousand Buddha Mountain", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> location.location.geolocation -> m.0wmmlzp\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Thousand Buddha Mountain -> location.location.geolocation -> m.02_ttpj\n# Answer:\nThousand Buddha Mountain", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Tooth Relic Temple and Museum -> common.topic.webpage -> m.0gb1hxk\n# Answer:\nBuddha Tooth Relic Temple and Museum", "# Reasoning Path:\nGautama Buddha -> people.person.quotations -> 'He insulted me, he cheated me, he beat me, he robbed me' -- those who are free of resentful thoughts surely find peace. -> media_common.quotation.subjects -> Peace\n# Answer:\n'He insulted me, he cheated me, he beat me, he robbed me' -- those who are free of resentful thoughts surely find peace."], "ground_truth": ["Kapilavastu"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-585", "prediction": ["# Reasoning Path:\nHenry VIII of England -> royalty.monarch.kingdom -> England -> location.country.currency_used -> Pound sterling\n# Answer:\nEngland", "# Reasoning Path:\nHenry VIII of England -> royalty.monarch.kingdom -> England -> location.country.languages_spoken -> Old English\n# Answer:\nEngland", "# Reasoning Path:\nHenry VIII of England -> royalty.monarch.royal_line -> Tudor dynasty -> people.family.country -> Principality of Wales\n# Answer:\nTudor dynasty", "# Reasoning Path:\nHenry VIII of England -> royalty.monarch.royal_line -> Tudor dynasty -> people.family.members -> Edmund Tudor, Duke of Somerset\n# Answer:\nTudor dynasty", "# Reasoning Path:\nHenry VIII of England -> royalty.monarch.kingdom -> England -> location.country.languages_spoken -> English Language\n# Answer:\nEngland", "# Reasoning Path:\nHenry VIII of England -> royalty.monarch.kingdom -> England -> base.aareas.schema.administrative_area.administrative_children -> North East England\n# Answer:\nEngland", "# Reasoning Path:\nHenry VIII of England -> royalty.monarch.royal_line -> Tudor dynasty -> people.family.founder -> Henry VII of England\n# Answer:\nTudor dynasty", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Mary I of England -> people.person.place_of_birth -> Palace of Placentia\n# Answer:\nMary I of England", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Edward VI of England -> royalty.monarch.kingdom -> England\n# Answer:\nEdward VI of England", "# Reasoning Path:\nHenry VIII of England -> royalty.monarch.royal_line -> Tudor dynasty -> people.family.country -> Kingdom of England\n# Answer:\nTudor dynasty"], "ground_truth": ["England"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-586", "prediction": ["# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Leukemia -> people.cause_of_death.parent_cause_of_death -> Hibakusha\n# Answer:\nLeukemia", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Leukemia -> medicine.disease.treatments -> Vincristine\n# Answer:\nLeukemia", "# Reasoning Path:\nLarry Hagman -> film.actor.film -> g.11b7563h44\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Leukemia -> people.cause_of_death.parent_cause_of_death -> Cancer\n# Answer:\nLeukemia", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Leukemia -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nLeukemia", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Leukemia -> medicine.disease.symptoms -> Arthritis\n# Answer:\nLeukemia", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.medical_specialties -> Oncology\n# Answer:\nCancer", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Leukemia -> medicine.disease.treatments -> Cytarabine\n# Answer:\nLeukemia", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Leukemia -> medicine.disease.symptoms -> Bleeding\n# Answer:\nLeukemia", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nCancer"], "ground_truth": ["Leukemia", "Head and neck cancer"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5833333333333334, "ans_precission": 0.7, "ans_recall": 0.5, "path_f1": 0.5833333333333334, "path_precision": 0.7, "path_recall": 0.5, "path_ans_f1": 0.5833333333333334, "path_ans_precision": 0.7, "path_ans_recall": 0.5}
{"id": "WebQTest-588", "prediction": ["# Reasoning Path:\nRobert E. Lee -> military.military_commander.military_commands -> m.04fv8sm -> military.military_command.military_conflict -> Battle of Appomattox Court House\n# Answer:\nBattle of Appomattox Court House", "# Reasoning Path:\nRobert E. Lee -> military.military_commander.military_commands -> m.0h1l91r -> military.military_command.military_conflict -> Western Virginia Campaign\n# Answer:\nWestern Virginia Campaign", "# Reasoning Path:\nRobert E. Lee -> military.military_commander.military_commands -> m.04yv3f8 -> military.military_command.military_conflict -> Siege of Petersburg\n# Answer:\nSiege of Petersburg", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> Robert E. Lee's Civil War -> book.written_work.subjects -> American Civil War\n# Answer:\nRobert E. Lee's Civil War", "# Reasoning Path:\nRobert E. Lee -> people.person.sibling_s -> m.0w4gc9q -> people.sibling_relationship.sibling -> Sydney Smith Lee\n# Answer:\nSydney Smith Lee", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> Robert E. Lee's Civil War -> book.book.genre -> Non-fiction\n# Answer:\nRobert E. Lee's Civil War", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> General Lee's Army -> book.written_work.subjects -> American Civil War\n# Answer:\nGeneral Lee's Army", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> General Lee's Army -> book.book.editions -> General Lee's Army: From Victory to Collapse\n# Answer:\nGeneral Lee's Army", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> General Lee's Army -> book.written_work.author -> Joseph Glatthaar\n# Answer:\nGeneral Lee's Army", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> Uncertain Glory: Lee's Generalship Re-Examined -> common.topic.notable_for -> g.125fs0sd6\n# Answer:\nUncertain Glory: Lee's Generalship Re-Examined"], "ground_truth": ["Second Battle of Petersburg", "Seven Days Battles", "Battle of Cheat Mountain", "Second Battle of Bull Run", "Overland Campaign", "Battle of the Crater", "Battle of Beaver Dam Creek", "Battle of Cumberland Church", "Battle of Williamsport", "Northern Virginia Campaign", "Siege of Fort Pulaski", "Richmond in the American Civil War", "Maryland Campaign", "Battle of Mine Run", "Gettysburg Campaign", "Third Battle of Petersburg", "Mexican\u2013American War", "American Civil War", "Battle of Antietam", "Battle of Fredericksburg", "Battle of Cold Harbor", "Second Battle of Rappahannock Station", "Battle of the Wilderness", "Stoneman's 1863 Raid", "Siege of Petersburg", "Battle of Frederick", "John Brown's raid on Harpers Ferry", "Battle of Oak Grove", "Battle of Chancellorsville", "Battle of Darbytown and New Market Roads", "Battle of Gaines's Mill", "Battle of Franklin's Crossing", "Second Battle of Deep Bottom", "Battle of White Oak Road", "Battle of Spotsylvania Court House", "Western Virginia Campaign", "Battle of Glendale", "Battle of South Mountain", "Battle of Gettysburg", "Battle of Appomattox Court House", "Battle of Fort Stedman", "Battle of Totopotomoy Creek", "Battle of Chaffin's Farm", "Battle of Malvern Hill"], "ans_acc": 0.09090909090909091, "ans_hit": 1, "ans_f1": 0.11111111111111109, "ans_precission": 0.3, "ans_recall": 0.06818181818181818, "path_f1": 0.11111111111111109, "path_precision": 0.3, "path_recall": 0.06818181818181818, "path_ans_f1": 0.15384615384615385, "path_ans_precision": 0.5, "path_ans_recall": 0.09090909090909091}
{"id": "WebQTest-589", "prediction": ["# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> language.human_language.countries_spoken_in -> Ukraine\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> common.topic.notable_types -> Human Language\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> language.human_language.region -> Eurasia\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> language.human_language.countries_spoken_in -> Belarus\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Abaza Language -> language.human_language.region -> Europe\n# Answer:\nAbaza Language", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.11b60vv5th\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> language.human_language.countries_spoken_in -> Georgia\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Abaza Language -> language.human_language.language_family -> North Caucasian languages\n# Answer:\nAbaza Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Yiddish Language -> language.human_language.countries_spoken_in -> Sweden\n# Answer:\nYiddish Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Yiddish Language -> common.topic.notable_types -> Human Language\n# Answer:\nYiddish Language"], "ground_truth": ["Russian Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> people.person.place_of_birth -> Ossining\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> location.location.containedby -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> base.militaryinfiction.location_in_fiction.contained_by -> New York\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Play -> type.type.properties -> Composer\n# Answer:\nPlay", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> people.person.nationality -> United States of America\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.story_by -> Harold Gray -> comic_strips.comic_strip_creator.comic_strips_written -> m.0gwbjrm\n# Answer:\nHarold Gray", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> location.location.containedby -> New York\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Play -> type.type.properties -> Characters\n# Answer:\nPlay", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Choreographer\n# Answer:\nChoreographer", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> award.award_nominee.award_nominations -> m.010bvypw\n# Answer:\nThomas Meehan"], "ground_truth": ["1976-08-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-590", "prediction": ["# Reasoning Path:\nLibya -> location.location.events -> 1793\u201395 Tripolitanian civil war -> base.culturalevent.event.entity_involved -> Tripolitania\n# Answer:\n1793\u201395 Tripolitanian civil war", "# Reasoning Path:\nLibya -> government.governmental_jurisdiction.governing_officials -> m.0105y7bt -> government.government_position_held.office_holder -> Abdullah al-Thani\n# Answer:\nAbdullah al-Thani", "# Reasoning Path:\nLibya -> location.location.events -> 1793\u201395 Tripolitanian civil war -> common.topic.notable_for -> g.12577rzkz\n# Answer:\n1793\u201395 Tripolitanian civil war", "# Reasoning Path:\nLibya -> location.location.events -> 1835\u201358 revolt in Ottoman Tripolitania -> common.topic.notable_types -> Military Conflict\n# Answer:\n1835\u201358 revolt in Ottoman Tripolitania", "# Reasoning Path:\nLibya -> government.governmental_jurisdiction.governing_officials -> m.010gfn62 -> government.government_position_held.office_holder -> Ahmed Maiteeq\n# Answer:\nAhmed Maiteeq", "# Reasoning Path:\nLibya -> location.statistical_region.co2_emissions_per_capita -> g.1245_1hym\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nLibya -> location.location.events -> 1793\u201395 Tripolitanian civil war -> time.event.locations -> Ottoman Tripolitania\n# Answer:\n1793\u201395 Tripolitanian civil war", "# Reasoning Path:\nLibya -> location.location.events -> 1920 Jabal al-Gharbi civil war -> common.topic.article -> m.0gw_q9y\n# Answer:\n1920 Jabal al-Gharbi civil war", "# Reasoning Path:\nLibya -> government.governmental_jurisdiction.governing_officials -> m.011n4mp5 -> government.government_position_held.office_holder -> Abdullah al-Thani\n# Answer:\nAbdullah al-Thani", "# Reasoning Path:\nLibya -> location.location.events -> 1793\u201395 Tripolitanian civil war -> base.culturalevent.event.entity_involved -> Rebellion\n# Answer:\n1793\u201395 Tripolitanian civil war"], "ground_truth": ["Abdullah al-Thani"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-592", "prediction": ["# Reasoning Path:\nShays' Rebellion -> time.event.locations -> Massachusetts -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nMassachusetts", "# Reasoning Path:\nShays' Rebellion -> time.event.locations -> Massachusetts -> location.location.time_zones -> UTC\u221205:00\n# Answer:\nMassachusetts", "# Reasoning Path:\nShays' Rebellion -> time.event.locations -> Massachusetts -> location.location.events -> 2014 Boston Tornado\n# Answer:\nMassachusetts", "# Reasoning Path:\nShays' Rebellion -> base.schemastaging.context_name.pronunciation -> g.125_lt26z\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nShays' Rebellion -> time.event.locations -> Massachusetts -> location.location.events -> Actions of 5\u20136 May 1945\n# Answer:\nMassachusetts", "# Reasoning Path:\nShays' Rebellion -> common.topic.notable_types -> Event -> freebase.type_profile.published -> Published\n# Answer:\nEvent", "# Reasoning Path:\nShays' Rebellion -> time.event.locations -> Massachusetts -> location.location.events -> Chappaquiddick Incident\n# Answer:\nMassachusetts", "# Reasoning Path:\nShays' Rebellion -> time.event.locations -> Massachusetts -> symbols.name_source.namesakes -> 4547 Massachusetts\n# Answer:\nMassachusetts", "# Reasoning Path:\nShays' Rebellion -> time.event.locations -> Massachusetts -> symbols.name_source.namesakes -> USS Massachusetts\n# Answer:\nMassachusetts", "# Reasoning Path:\nShays' Rebellion -> common.topic.notable_types -> Event -> type.type.expected_by -> event\n# Answer:\nEvent"], "ground_truth": ["1786"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-593", "prediction": ["# Reasoning Path:\nMontel Williams -> common.topic.article -> m.018pv0\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> medicine.disease.symptoms -> Dysphagia\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> medicine.disease.symptoms -> Vision loss\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> people.cause_of_death.people -> Zeki Kuneralp\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> medicine.disease.symptoms -> Amnesia\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> medicine.disease.notable_people_with_this_condition -> Aaron Solowoniuk\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> people.cause_of_death.people -> Anne Volant Rowling\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> people.cause_of_death.people -> Annette Funicello\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> medicine.disease.notable_people_with_this_condition -> Abigail McLellan\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> medicine.disease.notable_people_with_this_condition -> Adam Riedy\n# Answer:\nMultiple sclerosis"], "ground_truth": ["Multiple sclerosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-595", "prediction": ["# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.article -> m.0642vqz\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> 2001 AFC Championship Game -> sports.sports_championship_event.runner_up -> Oakland Raiders\n# Answer:\n2001 AFC Championship Game", "# Reasoning Path:\nBaltimore Ravens -> film.film_subject.films -> The Band That Wouldn't Die -> film.film.music -> Marcello Zavros\n# Answer:\nThe Band That Wouldn't Die", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> 2001 AFC Championship Game -> sports.sports_championship_event.season -> 2000 NFL season\n# Answer:\n2001 AFC Championship Game", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> 2013 AFC Championship Game -> time.event.instance_of_recurring_event -> AFC Championship Game\n# Answer:\n2013 AFC Championship Game", "# Reasoning Path:\nBaltimore Ravens -> film.film_subject.films -> The Band That Wouldn't Die -> film.film.subjects -> Baltimore's Marching Ravens\n# Answer:\nThe Band That Wouldn't Die", "# Reasoning Path:\nBaltimore Ravens -> american_football.football_team.away_games -> Mile High Miracle -> common.topic.article -> m.0s8vgqc\n# Answer:\nMile High Miracle", "# Reasoning Path:\nBaltimore Ravens -> film.film_subject.films -> The Band That Wouldn't Die -> film.film.subjects -> Baltimore Colts\n# Answer:\nThe Band That Wouldn't Die", "# Reasoning Path:\nBaltimore Ravens -> film.film_subject.films -> The Band That Wouldn't Die -> common.topic.article -> m.07shzm2\n# Answer:\nThe Band That Wouldn't Die"], "ground_truth": ["Super Bowl XLVII"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-596", "prediction": ["# Reasoning Path:\nKansas City Royals -> sports.sports_team.location -> Kansas City -> location.location.containedby -> United States of America\n# Answer:\nKansas City", "# Reasoning Path:\nKansas City Royals -> sports.sports_team.location -> Kansas City -> location.location.containedby -> Missouri\n# Answer:\nKansas City", "# Reasoning Path:\nKansas City Royals -> sports.sports_team.location -> Kansas City -> travel.travel_destination.tourist_attractions -> Crown Center\n# Answer:\nKansas City", "# Reasoning Path:\nKansas City Royals -> organization.organization.leadership -> m.0z45pkh -> organization.leadership.role -> CEO\n# Answer:\nCEO", "# Reasoning Path:\nKansas City Royals -> sports.sports_team.location -> Kansas City -> location.place_with_neighborhoods.neighborhoods -> Crown Center\n# Answer:\nKansas City", "# Reasoning Path:\nKansas City Royals -> sports.sports_team.league -> m.0crt4ht -> sports.sports_league_participation.league -> Major League Baseball\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nKansas City Royals -> organization.organization.leadership -> m.0z45pkh -> organization.leadership.person -> David D. Glass\n# Answer:\nDavid D. Glass", "# Reasoning Path:\nKansas City Royals -> sports.sports_team.location -> Kansas City -> travel.travel_destination.tourist_attractions -> 18th and Vine-Downtown East\n# Answer:\nKansas City", "# Reasoning Path:\nKansas City Royals -> sports.sports_team.league -> m.0crtd4s -> sports.sports_league_participation.league -> American League Central\n# Answer:\nAmerican League Central", "# Reasoning Path:\nKansas City Royals -> organization.organization.leadership -> m.0z45pkh -> freebase.valuenotation.has_value -> From\n# Answer:\nFrom"], "ground_truth": ["Kansas City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-597", "prediction": ["# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Country rock -> music.genre.parent_genre -> Country\n# Answer:\nCountry rock", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Folk music -> broadcast.genre.content -> Acoustic Caf.\n# Answer:\nFolk music", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Soul music -> broadcast.genre.content -> SoulfulSmoothJazz.com\n# Answer:\nSoul music", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Soul music -> music.genre.parent_genre -> Blues\n# Answer:\nSoul music", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Soul music -> music.genre.subgenre -> Blue-eyed soul\n# Answer:\nSoul music", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Folk music -> broadcast.genre.content -> mvyradio\n# Answer:\nFolk music", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Soul music -> broadcast.genre.content -> ACB Radio Interactive\n# Answer:\nSoul music", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Folk music -> broadcast.genre.content -> radioNtenna\n# Answer:\nFolk music", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Folk music -> common.topic.notable_types -> Musical genre\n# Answer:\nFolk music", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Country rock -> music.genre.parent_genre -> Neotraditional country\n# Answer:\nCountry rock"], "ground_truth": ["Rock music", "Soul music", "Alternative rock", "Soft rock", "Indie rock", "Acoustic music", "Pop music", "Soul rock", "Blues rock", "Country", "Folk rock", "Blue-eyed soul", "Blues", "Country rock", "Pop rock", "Folk music"], "ans_acc": 0.4375, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 1.0, "ans_recall": 0.25, "path_f1": 0.4, "path_precision": 1.0, "path_recall": 0.25, "path_ans_f1": 0.6086956521739131, "path_ans_precision": 1.0, "path_ans_recall": 0.4375}
{"id": "WebQTest-599", "prediction": ["# Reasoning Path:\nThomas Edison -> base.argumentmaps.innovator.original_ideas -> Phonograph -> common.topic.article -> m.063mq\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> base.argumentmaps.innovator.original_ideas -> Phonograph -> law.invention.inventor -> Eldridge R. Johnson\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Phonograph -> common.topic.article -> m.063mq\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> common.image.size -> m.02ctc4t\n# Answer:\ncommon.image.size", "# Reasoning Path:\nThomas Edison -> base.argumentmaps.innovator.original_ideas -> Phonograph -> base.argumentmaps.original_idea.innovator -> Eldridge R. Johnson\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Phonograph -> law.invention.inventor -> Eldridge R. Johnson\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Carbon microphone -> common.topic.article -> m.0d9fl1\n# Answer:\nCarbon microphone", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Phonograph -> base.argumentmaps.original_idea.innovator -> Eldridge R. Johnson\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> base.argumentmaps.innovator.original_ideas -> Phonograph -> law.invention.inventor -> Emile Berliner\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> common.image.size -> m.01xjmzn\n# Answer:\ncommon.image.size"], "ground_truth": ["Movie camera", "Mimeograph", "Incandescent light bulb", "Carbon microphone", "Phonograph", "Electric Power Distribution", "Quadruplex telegraph", "Phonograph cylinder"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.38095238095238093, "ans_precission": 0.8, "ans_recall": 0.25, "path_f1": 0.3076923076923077, "path_precision": 0.4, "path_recall": 0.25, "path_ans_f1": 0.38095238095238093, "path_ans_precision": 0.8, "path_ans_recall": 0.25}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> United States of America\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07ghzrz\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Alabama\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b66g7msm\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.education -> m.03gkqtp -> education.education.institution -> Louisiana State University\n# Answer:\nLouisiana State University", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Area code 251\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b7tn_fd2\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11btt6sf_l\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07919ln -> american_football.player_passing_statistics.team -> Oakland Raiders\n# Answer:\nOakland Raiders", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07mmjx4 -> american_football.player_passing_statistics.team -> Oakland Raiders\n# Answer:\nOakland Raiders"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> base.aareas.schema.administrative_area.administrative_parent -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.administrative_division.country -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> John Aspinwall Roosevelt -> people.deceased_person.place_of_death -> New York City\n# Answer:\nJohn Aspinwall Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> You learn by living -> book.book_edition.book -> You Learn by Living\n# Answer:\nYou learn by living", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> What I hope to leave behind -> book.book_edition.book -> What I Hope to Leave Behind\n# Answer:\nWhat I hope to leave behind", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> John Aspinwall Roosevelt -> people.person.place_of_birth -> Hyde Park\n# Answer:\nJohn Aspinwall Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Anna Roosevelt Halsted -> people.deceased_person.place_of_death -> New York City\n# Answer:\nAnna Roosevelt Halsted"], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-600", "prediction": ["# Reasoning Path:\nNovak Djokovic -> people.person.place_of_birth -> Belgrade -> base.biblioness.bibs_location.country -> Yugoslavia\n# Answer:\nBelgrade", "# Reasoning Path:\nNovak Djokovic -> people.person.place_of_birth -> Belgrade -> location.location.containedby -> Serbia\n# Answer:\nBelgrade", "# Reasoning Path:\nNovak Djokovic -> people.person.place_of_birth -> Belgrade -> location.location.containedby -> Europe\n# Answer:\nBelgrade", "# Reasoning Path:\nNovak Djokovic -> people.person.place_of_birth -> Belgrade -> periodicals.newspaper_circulation_area.newspapers -> Danas\n# Answer:\nBelgrade", "# Reasoning Path:\nNovak Djokovic -> people.person.nationality -> Serbia -> sports.sports_team_location.teams -> KK Crvena zvezda\n# Answer:\nSerbia", "# Reasoning Path:\nNovak Djokovic -> people.person.place_of_birth -> Belgrade -> location.location.containedby -> Yugoslavia\n# Answer:\nBelgrade", "# Reasoning Path:\nNovak Djokovic -> people.person.place_of_birth -> Belgrade -> periodicals.newspaper_circulation_area.newspapers -> Politika\n# Answer:\nBelgrade", "# Reasoning Path:\nNovak Djokovic -> people.person.nationality -> Serbia -> sports.sports_team_location.teams -> KK Mega Basket\n# Answer:\nSerbia", "# Reasoning Path:\nNovak Djokovic -> people.person.nationality -> Serbia -> location.location.containedby -> Europe\n# Answer:\nSerbia", "# Reasoning Path:\nNovak Djokovic -> people.person.place_of_birth -> Belgrade -> periodicals.newspaper_circulation_area.newspapers -> Start\n# Answer:\nBelgrade"], "ground_truth": ["Serbia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-602", "prediction": ["# Reasoning Path:\nNew England -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nNew England -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Vermont\n# Answer:\nUnited States of America", "# Reasoning Path:\nNew England -> location.location.containedby -> Northeastern United States -> location.location.containedby -> United States of America\n# Answer:\nNortheastern United States", "# Reasoning Path:\nNew England -> location.location.contains -> Hillsborough County -> location.location.containedby -> United States of America\n# Answer:\nHillsborough County", "# Reasoning Path:\nNew England -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> West Virginia\n# Answer:\nUnited States of America", "# Reasoning Path:\nNew England -> location.location.containedby -> Eastern United States -> location.location.containedby -> United States of America\n# Answer:\nEastern United States", "# Reasoning Path:\nNew England -> location.location.contains -> White Mountain National Forest -> location.location.containedby -> White Mountains\n# Answer:\nWhite Mountain National Forest", "# Reasoning Path:\nNew England -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nNew England -> location.location.containedby -> Northeastern United States -> location.location.contains -> Hillsborough County\n# Answer:\nNortheastern United States", "# Reasoning Path:\nNew England -> location.location.contains -> Hillsborough County -> location.statistical_region.co2_emissions_commercial -> m.045jfqg\n# Answer:\nHillsborough County"], "ground_truth": ["Northeastern United States"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-603", "prediction": ["# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Revaluation of the Turkish Lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nRevaluation of the Turkish Lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.065sxsl -> military.military_combatant_group.conflict -> Korean War\n# Answer:\nKorean War", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.066402y -> military.military_combatant_group.conflict -> Iraqi no-fly zones\n# Answer:\nIraqi no-fly zones"], "ground_truth": ["Turkish lira"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-604", "prediction": ["# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Rock music -> common.topic.notable_types -> Musical genre\n# Answer:\nRock music", "# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Acid rock -> common.topic.notable_for -> g.1257tm1jy\n# Answer:\nAcid rock", "# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Acid rock -> common.topic.notable_types -> Musical genre\n# Answer:\nAcid rock", "# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Acid rock -> common.topic.webpage -> m.09xphgr\n# Answer:\nAcid rock", "# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Blues -> music.genre.subgenre -> Soul music\n# Answer:\nBlues", "# Reasoning Path:\nJimi Hendrix -> music.artist.label -> Vee-Jay Records -> music.record_label.artist -> The Beatles\n# Answer:\nVee-Jay Records", "# Reasoning Path:\nJimi Hendrix -> music.artist.label -> Vee-Jay Records -> common.topic.notable_types -> Record label\n# Answer:\nVee-Jay Records", "# Reasoning Path:\nJimi Hendrix -> music.artist.label -> Barclay Records -> music.record_label.artist -> The Jimi Hendrix Experience\n# Answer:\nBarclay Records", "# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Blues -> music.genre.subgenre -> Garage rock\n# Answer:\nBlues", "# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Blues -> broadcast.genre.content -> 1.FM Blues\n# Answer:\nBlues"], "ground_truth": ["Hard rock", "Funk rock", "Rock music", "Psychedelic rock", "Heavy metal", "Experimental rock", "Blues rock", "Acid rock", "Progressive rock", "Blues", "Rhythm and blues", "Psychedelia"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.3684210526315789, "ans_precission": 0.7, "ans_recall": 0.25, "path_f1": 0.3684210526315789, "path_precision": 0.7, "path_recall": 0.25, "path_ans_f1": 0.3684210526315789, "path_ans_precision": 0.7, "path_ans_recall": 0.25}
{"id": "WebQTest-605", "prediction": ["# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgmw -> education.education.degree -> Bachelor of Arts\n# Answer:\nBachelor of Arts", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgmw -> education.education.major_field_of_study -> Political Science\n# Answer:\nPolitical Science", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgn2 -> education.education.degree -> Juris Doctor\n# Answer:\nJuris Doctor", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> education.education.institution -> Occidental College\n# Answer:\nOccidental College", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.subjects -> 2012 Presidential debate\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Bound Man -> book.written_work.subjects -> United States of America\n# Answer:\nA Bound Man", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgn2 -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Writer's Journey to Selma, Alabama -> book.written_work.subjects -> United States of America\n# Answer:\nA Writer's Journey to Selma, Alabama"], "ground_truth": ["Juris Doctor", "Bachelor of Arts"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-606", "prediction": ["# Reasoning Path:\nAlexander Graham Bell -> people.person.sibling_s -> m.0w17rt4 -> people.sibling_relationship.sibling -> Edward Charles Bell\n# Answer:\nEdward Charles Bell", "# Reasoning Path:\nAlexander Graham Bell -> people.person.children -> Edward Bell -> people.person.parents -> Mabel Gardiner Hubbard\n# Answer:\nEdward Bell", "# Reasoning Path:\nAlexander Graham Bell -> people.person.sibling_s -> m.0k257qs -> people.sibling_relationship.sibling -> Melville James Bell\n# Answer:\nMelville James Bell", "# Reasoning Path:\nAlexander Graham Bell -> people.person.children -> Edward Bell -> people.person.sibling_s -> m.0w17pb4\n# Answer:\nEdward Bell", "# Reasoning Path:\nAlexander Graham Bell -> people.person.children -> Elsie Bell -> people.person.sibling_s -> m.0w17p51\n# Answer:\nElsie Bell", "# Reasoning Path:\nAlexander Graham Bell -> people.person.children -> Marian Hubbard Bell -> people.person.sibling_s -> m.0w17p51\n# Answer:\nMarian Hubbard Bell", "# Reasoning Path:\nAlexander Graham Bell -> people.person.children -> Marian Hubbard Bell -> people.person.parents -> Mabel Gardiner Hubbard\n# Answer:\nMarian Hubbard Bell", "# Reasoning Path:\nAlexander Graham Bell -> people.person.children -> Edward Bell -> people.person.sibling_s -> m.0w1c1b0\n# Answer:\nEdward Bell", "# Reasoning Path:\nAlexander Graham Bell -> people.person.children -> Elsie Bell -> people.person.sibling_s -> m.0w17pb4\n# Answer:\nElsie Bell", "# Reasoning Path:\nAlexander Graham Bell -> people.person.children -> Elsie Bell -> people.person.children -> Mabel H. Grosvenor\n# Answer:\nElsie Bell"], "ground_truth": ["Edward Charles Bell", "Melville James Bell"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.2, "path_recall": 0.3333333333333333, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-608", "prediction": ["# Reasoning Path:\nLiverpool F.C. -> sports.sports_team.championships -> 2012 Football League Cup Final -> time.event.locations -> Wembley Stadium\n# Answer:\n2012 Football League Cup Final", "# Reasoning Path:\nLiverpool F.C. -> sports.sports_team.championships -> 2012 Football League Cup Final -> sports.sports_championship_event.championship -> Football League Cup\n# Answer:\n2012 Football League Cup Final", "# Reasoning Path:\nLiverpool F.C. -> sports.sports_team.championships -> 1964\u201365 FA Cup -> sports.sports_league_season.awards -> m.05l1m64\n# Answer:\n1964\u201365 FA Cup", "# Reasoning Path:\nLiverpool F.C. -> sports.sports_team.championships -> 1965 FA Cup Final -> soccer.football_match.teams -> Leeds United F.C.\n# Answer:\n1965 FA Cup Final", "# Reasoning Path:\nLiverpool F.C. -> sports.sports_team.championships -> 1964\u201365 FA Cup -> sports.sports_league_season.championship -> 1965 FA Cup Final\n# Answer:\n1964\u201365 FA Cup", "# Reasoning Path:\nLiverpool F.C. -> sports.sports_team.championships -> 1964\u201365 FA Cup -> common.topic.notable_for -> g.1256p9c3r\n# Answer:\n1964\u201365 FA Cup", "# Reasoning Path:\nLiverpool F.C. -> sports.sports_team.championships -> 1965 FA Cup Final -> sports.sports_championship_event.championship -> FA Cup\n# Answer:\n1965 FA Cup Final", "# Reasoning Path:\nLiverpool F.C. -> soccer.football_team.manager -> m.0k2m_12 -> soccer.football_team_management_tenure.manager -> Ronnie Moran\n# Answer:\nRonnie Moran", "# Reasoning Path:\nLiverpool F.C. -> business.employer.employees -> m.0lfqzgs -> business.employment_tenure.person -> John Achterberg\n# Answer:\nJohn Achterberg", "# Reasoning Path:\nLiverpool F.C. -> business.employer.employees -> m.05l1p44 -> business.employment_tenure.title -> Assistant manager\n# Answer:\nAssistant manager"], "ground_truth": ["2012 Football League Cup Final"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-609", "prediction": ["# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.regular_cast -> m.02t8rjb -> tv.regular_tv_appearance.actor -> Bill Daily\n# Answer:\nBill Daily", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.regular_cast -> m.02t8sfq -> tv.regular_tv_appearance.actor -> Larry Hagman\n# Answer:\nLarry Hagman", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.regular_cast -> m.02t90y8 -> tv.regular_tv_appearance.actor -> Hayden Rorke\n# Answer:\nHayden Rorke", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.theme_song -> Jeannie -> tv.tv_character.appeared_in_tv_program -> m.04lqxmt\n# Answer:\nJeannie", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.regular_cast -> m.02t8rjb -> tv.regular_tv_appearance.seasons -> I Dream of Jeannie, Season 5\n# Answer:\nI Dream of Jeannie, Season 5", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.regular_cast -> m.02t8rjb -> tv.regular_tv_appearance.character -> Roger Healey\n# Answer:\nRoger Healey", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.regular_cast -> m.02t8sfq -> tv.regular_tv_appearance.seasons -> I Dream of Jeannie, Season 5\n# Answer:\nI Dream of Jeannie, Season 5", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.tv_producer -> m.05kk2dn -> tv.tv_producer_term.producer -> Sidney Sheldon\n# Answer:\nSidney Sheldon", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.regular_cast -> m.02t8sfq -> tv.regular_tv_appearance.character -> Tony Nelson\n# Answer:\nTony Nelson", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.regular_cast -> m.02t90y8 -> tv.regular_tv_appearance.seasons -> I Dream of Jeannie, Season 5\n# Answer:\nI Dream of Jeannie, Season 5"], "ground_truth": ["Barbara Eden"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.064szk2 -> location.religion_percentage.religion -> Catholicism\n# Answer:\nCatholicism", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf2_w -> location.religion_percentage.religion -> Islam\n# Answer:\nIslam", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf301 -> location.religion_percentage.religion -> Protestantism\n# Answer:\nProtestantism", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60ptk2z\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.12cp_j7n1\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_4m6h\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_67l9\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.1hhc378kt\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.1hhc38d0l\n# Answer:\nlocation.statistical_region.official_development_assistance"], "ground_truth": ["Islam", "Protestantism", "Hinduism", "Catholicism"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.46153846153846156, "ans_precission": 0.3333333333333333, "ans_recall": 0.75, "path_f1": 0.42857142857142855, "path_precision": 0.3333333333333333, "path_recall": 0.6, "path_ans_f1": 0.46153846153846156, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.75}
{"id": "WebQTest-611", "prediction": ["# Reasoning Path:\nSaint Peter -> people.deceased_person.place_of_death -> Rome -> travel.travel_destination.tourist_attractions -> St. Peter's Basilica\n# Answer:\nRome", "# Reasoning Path:\nSaint Peter -> people.deceased_person.place_of_death -> Rome -> location.location.time_zones -> Central European Time Zone\n# Answer:\nRome", "# Reasoning Path:\nSaint Peter -> people.deceased_person.place_of_death -> Rome -> travel.travel_destination.tourist_attractions -> Vatican City\n# Answer:\nRome", "# Reasoning Path:\nSaint Peter -> people.deceased_person.place_of_death -> Rome -> base.biblioness.bibs_location.country -> Italy\n# Answer:\nRome", "# Reasoning Path:\nSaint Peter -> people.deceased_person.place_of_death -> Rome -> travel.travel_destination.tourist_attractions -> Santa Maria sopra Minerva\n# Answer:\nRome", "# Reasoning Path:\nSaint Peter -> visual_art.art_subject.artwork_on_the_subject -> Virgin and Child Enthroned with Saints Peter, Paul, John the Baptist, Dominic and a Donor -> visual_art.artwork.art_subject -> Saint Dominic\n# Answer:\nVirgin and Child Enthroned with Saints Peter, Paul, John the Baptist, Dominic and a Donor", "# Reasoning Path:\nSaint Peter -> symbols.name_source.namesakes -> Basilica of Saints Peter and Paul -> common.topic.notable_for -> g.1256kb1hr\n# Answer:\nBasilica of Saints Peter and Paul", "# Reasoning Path:\nSaint Peter -> visual_art.art_subject.artwork_on_the_subject -> Virgin and Child Enthroned with Saints Peter, Paul, John the Baptist, Dominic and a Donor -> visual_art.artwork.art_subject -> Paul the Apostle\n# Answer:\nVirgin and Child Enthroned with Saints Peter, Paul, John the Baptist, Dominic and a Donor", "# Reasoning Path:\nSaint Peter -> visual_art.art_subject.artwork_on_the_subject -> Virgin and Child Enthroned with Saints Peter, Paul, John the Baptist, Dominic and a Donor -> visual_art.artwork.artist -> Ugolino di Nerio (Attributed to)\n# Answer:\nVirgin and Child Enthroned with Saints Peter, Paul, John the Baptist, Dominic and a Donor", "# Reasoning Path:\nSaint Peter -> symbols.name_source.namesakes -> Basilica of Saints Peter and Paul -> location.location.containedby -> Lewiston\n# Answer:\nBasilica of Saints Peter and Paul"], "ground_truth": ["Rome"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-612", "prediction": ["# Reasoning Path:\nJeremy Shockey -> american_football.football_player.receiving -> m.09tctgy -> american_football.player_receiving_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints", "# Reasoning Path:\nJeremy Shockey -> american_football.football_player.receiving -> m.09rmcz9 -> american_football.player_receiving_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints", "# Reasoning Path:\nJeremy Shockey -> american_football.football_player.receiving -> m.09tctgy -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nJeremy Shockey -> american_football.football_player.receiving -> m.0791gjj -> american_football.player_receiving_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints", "# Reasoning Path:\nJeremy Shockey -> american_football.football_player.receiving -> m.09rmcz9 -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nJeremy Shockey -> sports.pro_athlete.teams -> m.0jb8dd7 -> sports.sports_team_roster.team -> Carolina Panthers\n# Answer:\nCarolina Panthers", "# Reasoning Path:\nJeremy Shockey -> american_football.football_player.games -> m.09rmcbm -> american_football.player_game_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nJeremy Shockey -> american_football.football_player.receiving -> m.0791gjj -> american_football.player_receiving_statistics.season -> 2008 NFL season\n# Answer:\n2008 NFL season", "# Reasoning Path:\nJeremy Shockey -> sports.pro_athlete.teams -> m.0jb8dd7 -> sports.sports_team_roster.position -> Tight end\n# Answer:\nTight end", "# Reasoning Path:\nJeremy Shockey -> american_football.football_player.games -> m.09rmcbm -> american_football.player_game_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints"], "ground_truth": ["Carolina Panthers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-613", "prediction": ["# Reasoning Path:\nDeion Sanders -> people.person.education -> m.02kygcm -> education.education.institution -> Florida State University\n# Answer:\nFlorida State University", "# Reasoning Path:\nDeion Sanders -> sports.pro_athlete.teams -> m.04_7n1k -> sports.sports_team_roster.team -> Cincinnati Reds\n# Answer:\nCincinnati Reds", "# Reasoning Path:\nDeion Sanders -> people.person.education -> m.04_7nf0 -> education.education.institution -> North Fort Myers High School\n# Answer:\nNorth Fort Myers High School", "# Reasoning Path:\nDeion Sanders -> sports.pro_athlete.teams -> m.04_7n19 -> sports.sports_team_roster.team -> New York Yankees\n# Answer:\nNew York Yankees", "# Reasoning Path:\nDeion Sanders -> sports.pro_athlete.teams -> m.04_7n21 -> sports.sports_team_roster.team -> San Francisco Giants\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nDeion Sanders -> sports.pro_athlete.teams -> m.04_7n1k -> sports.sports_team_roster.position -> Outfielder\n# Answer:\nOutfielder", "# Reasoning Path:\nDeion Sanders -> baseball.baseball_player.position_s -> Center fielder -> sports.sports_position.players -> g.1q4kq6m3k\n# Answer:\nCenter fielder", "# Reasoning Path:\nDeion Sanders -> baseball.baseball_player.position_s -> Center fielder -> common.topic.article -> m.02sg4k\n# Answer:\nCenter fielder", "# Reasoning Path:\nDeion Sanders -> baseball.baseball_player.position_s -> Center fielder -> sports.sports_position.sport -> Baseball\n# Answer:\nCenter fielder", "# Reasoning Path:\nDeion Sanders -> baseball.baseball_player.position_s -> Outfielder -> common.topic.notable_types -> Baseball Position\n# Answer:\nOutfielder"], "ground_truth": ["Florida State University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-614", "prediction": ["# Reasoning Path:\nBattle of the Alamo -> time.event.locations -> San Antonio -> location.location.time_zones -> Central Time Zone\n# Answer:\nSan Antonio", "# Reasoning Path:\nBattle of the Alamo -> time.event.locations -> San Antonio -> location.citytown.postal_codes -> 78201\n# Answer:\nSan Antonio", "# Reasoning Path:\nBattle of the Alamo -> military.military_conflict.military_personnel_involved -> William B. Travis -> military.military_person.participated_in_conflicts -> Texas Revolution\n# Answer:\nWilliam B. Travis", "# Reasoning Path:\nBattle of the Alamo -> time.event.locations -> San Antonio -> location.statistical_region.population -> g.11b66cbs7_\n# Answer:\nSan Antonio", "# Reasoning Path:\nBattle of the Alamo -> time.event.locations -> San Antonio -> location.citytown.postal_codes -> 78202\n# Answer:\nSan Antonio", "# Reasoning Path:\nBattle of the Alamo -> military.military_conflict.military_personnel_involved -> William B. Travis -> people.deceased_person.cause_of_death -> Firearm\n# Answer:\nWilliam B. Travis", "# Reasoning Path:\nBattle of the Alamo -> military.military_conflict.military_personnel_involved -> Manuel Fern\u00e1ndez Castrill\u00f3n -> military.military_person.participated_in_conflicts -> Texas Revolution\n# Answer:\nManuel Fern\u00e1ndez Castrill\u00f3n", "# Reasoning Path:\nBattle of the Alamo -> time.event.locations -> San Antonio -> location.citytown.postal_codes -> 78203\n# Answer:\nSan Antonio", "# Reasoning Path:\nBattle of the Alamo -> time.event.locations -> San Antonio -> location.statistical_region.population -> g.11b7sw44tx\n# Answer:\nSan Antonio", "# Reasoning Path:\nBattle of the Alamo -> military.military_conflict.military_personnel_involved -> William B. Travis -> common.topic.image -> William B\n# Answer:\nWilliam B. Travis"], "ground_truth": ["1836-02-23"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-615", "prediction": ["# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> common.topic.article -> m.02sx6l\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.spouse_s -> m.03mlj1z\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> common.topic.article -> m.0hn9rcf\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.place_of_birth -> Blackmore\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Mary I of England -> people.person.religion -> Catholicism\n# Answer:\nMary I of England", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> people.person.nationality -> Kingdom of England\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> symbols.name_source.namesakes -> King Henry's Drive tram stop -> common.topic.image -> King Henry's Drive tramstop look south\n# Answer:\nKing Henry's Drive tram stop", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> royalty.noble_person.titles -> m.0hqncyh\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Mary I of England -> people.deceased_person.cause_of_death -> Cancer\n# Answer:\nMary I of England", "# Reasoning Path:\nHenry VIII of England -> symbols.name_source.namesakes -> King Henry's Drive tram stop -> common.topic.notable_for -> g.1257k6j6b\n# Answer:\nKing Henry's Drive tram stop"], "ground_truth": ["Elizabeth I of England", "Henry FitzRoy, 1st Duke of Richmond and Somerset", "Edward VI of England", "Henry, Duke of Cornwall", "Mary I of England"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6857142857142857, "ans_precission": 0.8, "ans_recall": 0.6, "path_f1": 0.6857142857142857, "path_precision": 0.8, "path_recall": 0.6, "path_ans_f1": 0.6857142857142857, "path_ans_precision": 0.8, "path_ans_recall": 0.6}
{"id": "WebQTest-616", "prediction": ["# Reasoning Path:\nElizabeth II -> people.person.children -> Charles, Prince of Wales -> people.person.children -> Prince Harry\n# Answer:\nCharles, Prince of Wales", "# Reasoning Path:\nElizabeth II -> people.person.children -> Charles, Prince of Wales -> people.person.parents -> Prince Philip, Duke of Edinburgh\n# Answer:\nCharles, Prince of Wales", "# Reasoning Path:\nElizabeth II -> people.person.ethnicity -> English people -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nEnglish people", "# Reasoning Path:\nElizabeth II -> people.person.children -> Charles, Prince of Wales -> people.person.children -> Prince William, Duke of Cambridge\n# Answer:\nCharles, Prince of Wales", "# Reasoning Path:\nElizabeth II -> people.person.children -> Prince Edward, Earl of Wessex -> people.person.children -> James, Viscount Severn\n# Answer:\nPrince Edward, Earl of Wessex", "# Reasoning Path:\nElizabeth II -> people.person.children -> Prince Edward, Earl of Wessex -> people.person.place_of_birth -> Buckingham Palace\n# Answer:\nPrince Edward, Earl of Wessex", "# Reasoning Path:\nElizabeth II -> people.person.children -> Charles, Prince of Wales -> people.person.religion -> Anglicanism\n# Answer:\nCharles, Prince of Wales", "# Reasoning Path:\nElizabeth II -> people.person.children -> Anne, Princess Royal -> people.person.children -> Zara Phillips\n# Answer:\nAnne, Princess Royal", "# Reasoning Path:\nElizabeth II -> people.person.children -> Prince Edward, Earl of Wessex -> people.person.children -> Lady Louise Windsor\n# Answer:\nPrince Edward, Earl of Wessex", "# Reasoning Path:\nElizabeth II -> people.person.children -> Prince Edward, Earl of Wessex -> people.person.parents -> Prince Philip, Duke of Edinburgh\n# Answer:\nPrince Edward, Earl of Wessex"], "ground_truth": ["Lilibet", "Elizabeth II"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-617", "prediction": ["# Reasoning Path:\nTim Tebow -> people.person.place_of_birth -> Makati -> location.location.containedby -> Philippines\n# Answer:\nMakati", "# Reasoning Path:\nTim Tebow -> people.person.place_of_birth -> Makati -> location.location.containedby -> Metro Manila\n# Answer:\nMakati", "# Reasoning Path:\nTim Tebow -> people.person.place_of_birth -> Makati -> location.location.contains -> Alphaland Makati Tower\n# Answer:\nMakati", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> book.written_work.author -> Nathan Whitaker\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> people.person.place_of_birth -> Makati -> location.location.people_born_here -> Adolf Alix, Jr.\n# Answer:\nMakati", "# Reasoning Path:\nTim Tebow -> people.person.place_of_birth -> Makati -> location.location.contains -> Asia Pacific College\n# Answer:\nMakati", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.notable_types -> Book\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> people.person.place_of_birth -> Makati -> location.location.contains -> Asian Institute for Distance Education\n# Answer:\nMakati", "# Reasoning Path:\nTim Tebow -> people.person.place_of_birth -> Makati -> location.location.people_born_here -> Albie Casi\u00f1o\n# Answer:\nMakati", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.article -> m.0j4d5g4\n# Answer:\nThrough My Eyes"], "ground_truth": ["Makati"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-618", "prediction": ["# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Queen Anne's County -> location.statistical_region.population -> g.11b66hy55n\n# Answer:\nQueen Anne's County", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Queen Anne's County -> common.topic.article -> m.0g83b\n# Answer:\nQueen Anne's County", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.body_of_water_spanned -> Chesapeake Bay -> location.location.containedby -> United States of America\n# Answer:\nChesapeake Bay", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Queen Anne's County -> location.statistical_region.population -> g.11x1cgn71\n# Answer:\nQueen Anne's County", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Queen Anne's County -> location.statistical_region.population -> g.11x1cvdsr\n# Answer:\nQueen Anne's County", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Anne Arundel County -> location.location.containedby -> Maryland\n# Answer:\nAnne Arundel County", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Anne Arundel County -> location.statistical_region.co2_emissions_residential -> m.045k33p\n# Answer:\nAnne Arundel County", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Anne Arundel County -> location.statistical_region.population -> g.11b66cbs7h\n# Answer:\nAnne Arundel County", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.body_of_water_spanned -> Chesapeake Bay -> location.location.containedby -> North America\n# Answer:\nChesapeake Bay", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.body_of_water_spanned -> Chesapeake Bay -> geography.body_of_water.islands -> Barren Island\n# Answer:\nChesapeake Bay"], "ground_truth": ["Anne Arundel County", "Queen Anne's County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-619", "prediction": ["# Reasoning Path:\nThe Big Valley -> tv.tv_program.regular_cast -> m.04d576g -> tv.regular_tv_appearance.actor -> Barbara Stanwyck\n# Answer:\nBarbara Stanwyck", "# Reasoning Path:\nThe Big Valley -> award.award_nominated_work.award_nominations -> m.07tws31 -> award.award_nomination.award_nominee -> Barbara Stanwyck\n# Answer:\nBarbara Stanwyck", "# Reasoning Path:\nThe Big Valley -> tv.tv_program.regular_cast -> m.04d576g -> tv.regular_tv_appearance.character -> Victoria Barkley\n# Answer:\nVictoria Barkley", "# Reasoning Path:\nThe Big Valley -> tv.tv_program.regular_cast -> m.04d576m -> tv.regular_tv_appearance.actor -> Linda Evans\n# Answer:\nLinda Evans", "# Reasoning Path:\nThe Big Valley -> tv.tv_program.regular_cast -> m.04d576s -> tv.regular_tv_appearance.actor -> Lee Majors\n# Answer:\nLee Majors", "# Reasoning Path:\nThe Big Valley -> award.award_nominated_work.award_nominations -> m.0n1xrhl -> award.award_nomination.award_nominee -> Barbara Stanwyck\n# Answer:\nBarbara Stanwyck", "# Reasoning Path:\nThe Big Valley -> award.award_nominated_work.award_nominations -> m.07tws31 -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Lead Actress in a Drama Series\n# Answer:\nPrimetime Emmy Award for Outstanding Lead Actress in a Drama Series", "# Reasoning Path:\nThe Big Valley -> award.award_nominated_work.award_nominations -> m.07tws78 -> award.award_nomination.award_nominee -> Barbara Stanwyck\n# Answer:\nBarbara Stanwyck", "# Reasoning Path:\nThe Big Valley -> tv.tv_program.regular_cast -> m.04d576m -> tv.regular_tv_appearance.character -> Audra Barkley\n# Answer:\nAudra Barkley", "# Reasoning Path:\nThe Big Valley -> tv.tv_program.regular_cast -> m.04d576s -> tv.regular_tv_appearance.character -> Heath Barkley\n# Answer:\nHeath Barkley"], "ground_truth": ["Barbara Stanwyck"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.380952380952381, "path_precision": 0.4, "path_recall": 0.36363636363636365, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Countryway Gunshop\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> TinkerSource\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.image -> M&Prevolver\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.statistical_region.population -> g.11b66mljjm\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Projectile weapon\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> base.weapons.weapon.subtypes -> Semi-automatic firearm\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.hud_foreclosure_area.total_90_day_vacant_residential_addresses -> m.07h2xbq\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.image -> HK USP 45\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.statistical_region.population -> g.11bc87r407\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.citytown.postal_codes -> 64501\n# Answer:\nSaint Joseph"], "ground_truth": ["Firearm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-620", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> people.person.place_of_birth -> Brookline -> travel.travel_destination.tourist_attractions -> John Fitzgerald Kennedy National Historic Site\n# Answer:\nBrookline", "# Reasoning Path:\nJohn F. Kennedy -> base.inaugurations.inauguration_speaker.inauguration -> John F. Kennedy 1961 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nJohn F. Kennedy 1961 presidential inauguration", "# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_death -> Dallas -> travel.travel_destination.tourist_attractions -> Dealey Plaza\n# Answer:\nDallas", "# Reasoning Path:\nJohn F. Kennedy -> people.person.place_of_birth -> Brookline -> business.business_location.parent_company -> 16 Handles brookline\n# Answer:\nBrookline", "# Reasoning Path:\nJohn F. Kennedy -> people.person.place_of_birth -> Brookline -> location.location.containedby -> United States of America\n# Answer:\nBrookline", "# Reasoning Path:\nJohn F. Kennedy -> base.inaugurations.inauguration_speaker.inauguration -> John F. Kennedy 1961 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nJohn F. Kennedy 1961 presidential inauguration", "# Reasoning Path:\nJohn F. Kennedy -> base.inaugurations.inauguration_speaker.inauguration -> John F. Kennedy 1961 presidential inauguration -> base.inaugurations.inauguration.inaugural_address -> Inaugural address of John F. Kennedy\n# Answer:\nJohn F. Kennedy 1961 presidential inauguration", "# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_death -> Dallas -> location.location.containedby -> United States of America\n# Answer:\nDallas", "# Reasoning Path:\nJohn F. Kennedy -> people.person.place_of_birth -> Brookline -> location.location.containedby -> Massachusetts\n# Answer:\nBrookline", "# Reasoning Path:\nJohn F. Kennedy -> base.inaugurations.inauguration_speaker.inauguration -> John F. Kennedy 1961 presidential inauguration -> base.culturalevent.event.entity_involved -> Sam Rayburn\n# Answer:\nJohn F. Kennedy 1961 presidential inauguration"], "ground_truth": ["United States Capitol", "Washington, D.C."], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.23529411764705882, "path_precision": 0.2, "path_recall": 0.2857142857142857, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-621", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_used -> East German mark\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.containedby -> Central and Eastern Europe\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> West Germany -> location.country.currency_used -> Deutsche Mark\n# Answer:\nWest Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_used -> Deutsche Mark\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Vaduz\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Belgium\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0102xvg7 -> education.education.student -> Sigurd Burckhardt\n# Answer:\nSigurd Burckhardt", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> West Germany -> location.location.containedby -> Western Europe\n# Answer:\nWest Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Andreas Bielau\n# Answer:\nEast Germany"], "ground_truth": ["Austria", "Liechtenstein", "Germany", "Luxembourg", "Switzerland", "East Germany", "Belgium"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.5581395348837209, "ans_precission": 0.8, "ans_recall": 0.42857142857142855, "path_f1": 0.3870967741935483, "path_precision": 0.6, "path_recall": 0.2857142857142857, "path_ans_f1": 0.6990291262135921, "path_ans_precision": 0.9, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-622", "prediction": ["# Reasoning Path:\nTaliban -> common.topic.article -> m.07jqt\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nTaliban -> organization.organization.founders -> Baitullah Mehsud -> people.person.place_of_birth -> Bannu District\n# Answer:\nBaitullah Mehsud", "# Reasoning Path:\nTaliban -> organization.organization.geographic_scope -> Afghanistan -> location.country.first_level_divisions -> Badakhshan Province\n# Answer:\nAfghanistan", "# Reasoning Path:\nTaliban -> organization.organization.founders -> Baitullah Mehsud -> people.person.sibling_s -> m.0w4g4xj\n# Answer:\nBaitullah Mehsud", "# Reasoning Path:\nTaliban -> organization.organization.founders -> Abdul Ghani Baradar -> people.person.place_of_birth -> Deh Rahwod District\n# Answer:\nAbdul Ghani Baradar", "# Reasoning Path:\nTaliban -> organization.organization.geographic_scope -> Afghanistan -> location.country.first_level_divisions -> Badghis Province\n# Answer:\nAfghanistan", "# Reasoning Path:\nTaliban -> organization.organization.geographic_scope -> Afghanistan -> base.aareas.schema.administrative_area.administrative_children -> Badakhshan Province\n# Answer:\nAfghanistan", "# Reasoning Path:\nTaliban -> organization.organization.founders -> Baitullah Mehsud -> people.person.profession -> Warlord\n# Answer:\nBaitullah Mehsud", "# Reasoning Path:\nTaliban -> organization.organization.founders -> Mohammed Omar -> government.politician.government_positions_held -> m.0726f2h\n# Answer:\nMohammed Omar", "# Reasoning Path:\nTaliban -> organization.organization.geographic_scope -> Afghanistan -> location.country.first_level_divisions -> Baghlan Province\n# Answer:\nAfghanistan"], "ground_truth": ["Afghanistan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-624", "prediction": ["# Reasoning Path:\nElena Gilbert -> tv.tv_character.appeared_in_tv_program -> m.0790rw5 -> tv.regular_tv_appearance.actor -> Nina Dobrev\n# Answer:\nNina Dobrev", "# Reasoning Path:\nElena Gilbert -> tv.tv_character.appeared_in_tv_program -> m.0790rw5 -> tv.regular_tv_appearance.series -> The Vampire Diaries\n# Answer:\nThe Vampire Diaries", "# Reasoning Path:\nElena Gilbert -> fictional_universe.fictional_character.romantically_involved_with -> m.0bfmsv6 -> fictional_universe.romantic_involvement.partner -> Stefan Salvatore\n# Answer:\nStefan Salvatore", "# Reasoning Path:\nElena Gilbert -> fictional_universe.fictional_character.romantically_involved_with -> m.0790spf -> fictional_universe.romantic_involvement.partner -> g.12176g5r\n# Answer:\nfictional_universe.romantic_involvement.partner", "# Reasoning Path:\nElena Gilbert -> book.book_character.appears_in_book -> Dark Reunion -> book.written_work.original_language -> English Language\n# Answer:\nDark Reunion", "# Reasoning Path:\nElena Gilbert -> book.book_character.appears_in_book -> Dark Reunion -> book.book.editions -> The Vampire Diaries Volume IV: Dark Reunion\n# Answer:\nDark Reunion", "# Reasoning Path:\nElena Gilbert -> book.book_character.appears_in_book -> The Fury -> book.book.characters -> Damon Salvatore\n# Answer:\nThe Fury", "# Reasoning Path:\nElena Gilbert -> book.book_character.appears_in_book -> Dark Reunion -> book.book.characters -> Stefan Salvatore\n# Answer:\nDark Reunion", "# Reasoning Path:\nElena Gilbert -> book.book_character.appears_in_book -> The Return: Nightfall -> book.book.characters -> Damon Salvatore\n# Answer:\nThe Return: Nightfall", "# Reasoning Path:\nElena Gilbert -> book.book_character.appears_in_book -> The Fury -> book.book.characters -> Stefan Salvatore\n# Answer:\nThe Fury"], "ground_truth": ["Nina Dobrev"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-626", "prediction": ["# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Missouri -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Missouri -> location.location.partially_contains -> White River\n# Answer:\nMissouri", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> South Dakota -> location.location.people_born_here -> Adam Emerson\n# Answer:\nSouth Dakota", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Missouri -> location.location.partially_contains -> Black River\n# Answer:\nMissouri", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Missouri -> book.book_subject.works -> The 100 Greatest Moments in St. Louis Sports\n# Answer:\nMissouri", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> South Dakota -> location.location.people_born_here -> Adam Johnson\n# Answer:\nSouth Dakota", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> South Dakota -> location.location.partiallycontains -> m.0wg90p7\n# Answer:\nSouth Dakota", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Missouri -> location.location.partially_contains -> Blue River\n# Answer:\nMissouri", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Missouri -> book.book_subject.works -> The Adventures of Tom Sawyer\n# Answer:\nMissouri", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> North Dakota -> location.location.containedby -> Contiguous United States\n# Answer:\nNorth Dakota"], "ground_truth": ["Nebraska", "Iowa", "Montana", "Kansas", "Missouri", "North Dakota", "South Dakota"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 1.0, "ans_recall": 0.42857142857142855, "path_f1": 0.6, "path_precision": 1.0, "path_recall": 0.42857142857142855, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-627", "prediction": ["# Reasoning Path:\nTaylor Lautner -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Michigan\n# Answer:\nUnited States of America", "# Reasoning Path:\nTaylor Lautner -> people.person.place_of_birth -> Grand Rapids -> location.location.containedby -> United States of America\n# Answer:\nGrand Rapids", "# Reasoning Path:\nTaylor Lautner -> base.popstra.celebrity.friendship -> m.06524mx -> base.popstra.friendship.participant -> Cayden Boyd\n# Answer:\nCayden Boyd", "# Reasoning Path:\nTaylor Lautner -> people.person.place_of_birth -> Grand Rapids -> location.location.containedby -> Kent County\n# Answer:\nGrand Rapids", "# Reasoning Path:\nTaylor Lautner -> people.person.place_of_birth -> Grand Rapids -> travel.travel_destination.tourist_attractions -> DeVos Place Convention Center\n# Answer:\nGrand Rapids", "# Reasoning Path:\nTaylor Lautner -> people.person.place_of_birth -> Grand Rapids -> location.location.containedby -> Michigan\n# Answer:\nGrand Rapids", "# Reasoning Path:\nTaylor Lautner -> people.person.place_of_birth -> Grand Rapids -> location.location.contains -> DeVos Place Convention Center\n# Answer:\nGrand Rapids", "# Reasoning Path:\nTaylor Lautner -> people.person.place_of_birth -> Grand Rapids -> travel.travel_destination.tourist_attractions -> Gerald R. Ford Presidential Museum\n# Answer:\nGrand Rapids", "# Reasoning Path:\nTaylor Lautner -> base.popstra.celebrity.friendship -> m.064c5y7 -> base.popstra.friendship.participant -> Taylor Dooley\n# Answer:\nTaylor Dooley", "# Reasoning Path:\nTaylor Lautner -> people.person.place_of_birth -> Grand Rapids -> travel.travel_destination.tourist_attractions -> Heritage Hill Historic District\n# Answer:\nGrand Rapids"], "ground_truth": ["United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-628", "prediction": ["# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.official_symbols -> m.0462792 -> location.location_symbol_relationship.symbol -> California grizzly bear\n# Answer:\nCalifornia grizzly bear", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.official_symbols -> m.046279k -> location.location_symbol_relationship.symbol -> I Love You, California\n# Answer:\nI Love You, California", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.official_symbols -> m.046279y -> location.location_symbol_relationship.symbol -> Square dance\n# Answer:\nSquare dance", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.official_symbols -> m.0462792 -> location.location_symbol_relationship.Kind_of_symbol -> State animal\n# Answer:\nState animal", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.official_symbols -> m.046279k -> location.location_symbol_relationship.Kind_of_symbol -> State song\n# Answer:\nState song", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.official_symbols -> m.046279y -> location.location_symbol_relationship.Kind_of_symbol -> State folk dance\n# Answer:\nState folk dance", "# Reasoning Path:\nCalifornia -> location.location.events -> 1987 Wine Country Film Festival -> freebase.valuenotation.has_value -> Closing date\n# Answer:\n1987 Wine Country Film Festival", "# Reasoning Path:\nCalifornia -> location.location.events -> 1987 Wine Country Film Festival -> time.event.locations -> Sonoma Valley\n# Answer:\n1987 Wine Country Film Festival", "# Reasoning Path:\nCalifornia -> location.location.events -> 1987 Wine Country Film Festival -> time.event.instance_of_recurring_event -> Wine Country Film Festival\n# Answer:\n1987 Wine Country Film Festival", "# Reasoning Path:\nCalifornia -> location.location.events -> 1989 Wine Country Film Festival -> time.event.instance_of_recurring_event -> Wine Country Film Festival\n# Answer:\n1989 Wine Country Film Festival"], "ground_truth": ["California grizzly bear"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-629", "prediction": ["# Reasoning Path:\nNike, Inc. -> organization.organization.founders -> Phil Knight -> organization.organization_founder.organizations_founded -> Laika\n# Answer:\nPhil Knight", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Sports equipment -> business.industry.child_industry -> Fabricated Rubber Products, NEC (wet suits)\n# Answer:\nSports equipment", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Sports equipment -> business.industry.parent_industry -> Other Miscellaneous Manufacturing\n# Answer:\nSports equipment", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Sports equipment -> freebase.equivalent_topic.equivalent_type -> Sports Equipment\n# Answer:\nSports equipment", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Sports equipment -> business.industry.child_industry -> Ice hockey equipment\n# Answer:\nSports equipment", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Footwear Manufacturing -> business.industry.child_industry -> Athletic Shoes & Apparel\n# Answer:\nFootwear Manufacturing", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Sports equipment -> business.industry.child_industry -> Sporting and Athletic Goods\n# Answer:\nSports equipment", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Footwear Manufacturing -> common.topic.notable_for -> g.1257xhlq7\n# Answer:\nFootwear Manufacturing", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Athletic Shoes & Apparel -> business.industry.companies -> Crocs\n# Answer:\nAthletic Shoes & Apparel", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Athletic Shoes & Apparel -> common.topic.notable_types -> Industry\n# Answer:\nAthletic Shoes & Apparel"], "ground_truth": ["Twin Dragons Holding B.V.", "NIKE de Mexico S de R.L. de C.V.", "NIKE Waffle", "USISL Inc", "Yugen Kaisha Hurley Japan", "NIKE Canada Holding B.V.", "Exeter Brands Group LLC", "NIKE New Zealand Co", "NIKE Israel Ltd", "Nike Brand Kitchen", "American NIKE SL", "NIKE Lavadome", "NIKE Hellas EPE", "NIKE Flight", "Umbro Sportwear Ltd", "Umbro HK Ltd", "NIKE Australia Holding B.V.", "PT NIKE Indonesia", "NIKE (Switzerland) GmbH", "Futbol Club Barcelona SL", "NIKE Servicios de Mexico S. de R.L. de C.V.", "NIKE Global Trading PTE. LTD", "NIKE NZ Holding B.V.", "NIKE International Holding Inc", "NIKE Retail Poland sp. z o. o.", "PT Hurley Indonesia", "Juventus Merchandising S.r.l.", "NIKE South Africa Holdings LLC", "Umbro Worldwide Ltd", "PMG International Ltd", "Cole Haan", "NIKE International Holding B.V.", "Converse Canada Holding B.V.", "NIKE UK Holding B.V.", "Hurley 999 SL", "NIKE Finance Ltd", "NIKE Global Services PTE. LTD", "NIKE Australia Pty. Ltd", "Converse Footwear Technical Service (Zhongshan) Co Ltd", "NIKE Retail B.V.", "Umbro International JV", "NIKE Denmark ApS", "NIKE IHM Inc", "NIKE Licenciamentos do Brasil Ltda", "NIKE Italy S.R.L.", "NIKE TN Inc", "NIKE Vision Timing & Techlab LP", "NIKE CA LLC", "Converse Sporting Goods (China) Co Ltd", "Converse", "NIKE India Holding B.V.", "Umbro International Holdings Ltd", "NIKE USA Inc", "NIKE Mexico Holdings LLC", "NIKE Retail Services Inc", "Hurley Australia Pty Ltd", "NIKE GmbH", "NIKE Sports Korea Co Ltd", "NIKE Offshore Holding B.V.", "Umbro", "Twin Dragons Global Ltd", "NIKE Suzhou Holding HK Ltd", "Cole Haan Japan", "NIKE Global Holding B.V.", "NIKE Zoom LLC", "BRS NIKE Taiwan Inc", "Savier Inc", "NIKE do Brasil Comercio e Participacoes Ltda", "Converse (Asia Pacific) Ltd", "Umbro Schweiz Ltd", "NIKE Philippines Inc", "NIKE Japan Corp", "Cole Haan Hong Kong Ltd", "NIKE BH B.V.", "NIKE Sourcing India Private Ltd", "NIKE Sweden AB", "NIKE Holding LLC", "Converse Canada Corp", "Converse Trading Co B.V.", "NIKE India Private Ltd", "NIKE Sports (China) Co Ltd", "NIKE Retail LLC", "NIKE Dunk Holding B.V.", "Hurley International Holding B.V.", "NIKE Canada Corp", "NIKE SALES (MALAYSIA) SDN. BHD.", "NIKE Tailwind", "NIKE (Thailand) Ltd", "Converse Hong Kong Ltd", "Bragano Trading S.r.l.", "NIKE (UK) Ltd", "NIKE South Africa Ltd", "NIKE Deutschland GmbH", "Manchester United Merchandising Ltd", "NIKE Europe Holding B.V.", "NIKE Chile B.V.", "NIKE Galaxy Holding B.V.", "NIKE 360 Holding B.V.", "Umbro Asia Sourcing Ltd", "NIKE Jump Ltd", "NIKE China Holding HK Ltd", "NIKE de Chile Ltda", "NIKE Pegasus", "NIKE Laser Holding B.V.", "NIKE European Operations Netherlands B.V.", "NIKE (Suzhou) Sports Co Ltd", "NIKE Argentina Srl", "NIKE Cortez", "NIKE International LLC", "Umbro.com", "NIKE Logistics Yugen Kaisha", "Converse Netherlands B.V.", "NIKE Max LLC", "NIKE Hong Kong Ltd", "NIKE Vietnam Co", "Umbro JV Ltd", "Umbro International Ltd", "NIKE Poland Sp.zo.o", "NIKE Africa Ltd", "NIKE Trading Co B.V.", "Nike Vision", "Umbro Finance Ltd", "NIKE Huarache", "NIKE Russia LLC", "Nike Golf", "NIKE SINGAPORE PTE LTD", "Triax Insurance Inc", "NIKE Group Holding B.V.", "Hurley999 UK Ltd", "NIKE International Ltd", "NIKE Vapor Ltd", "Umbro Ltd", "NIKE Asia Holding B.V.", "NIKE Finland OY", "Cole Haan Co Store", "Hurley International", "Exeter Hong Kong Ltd", "Converse Hong Kong Holding Co Ltd", "NIKE France SAS.", "Umbro Licensing Ltd"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> people.person.children -> Charles Hamlin\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> common.topic.article -> m.03mpv\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.nationality -> United States of America\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.04hdfss -> people.place_lived.location -> Illinois\n# Answer:\nIllinois", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.profession -> Politician\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pgr_5 -> people.place_lived.location -> Kentucky\n# Answer:\nKentucky", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pn4x_ -> people.place_lived.location -> Springfield\n# Answer:\nSpringfield"], "ground_truth": ["Hannibal Hamlin", "Andrew Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7272727272727273, "ans_precission": 0.5714285714285714, "ans_recall": 1.0, "path_f1": 0.7272727272727273, "path_precision": 0.5714285714285714, "path_recall": 1.0, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 1.0}
{"id": "WebQTest-630", "prediction": ["# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Belize\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Achawa language -> language.human_language.language_family -> Macro-Arawakan languages\n# Answer:\nAchawa language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Achawa language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nAchawa language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Chile\n# Answer:\nSpanish Language"], "ground_truth": ["Guahibo language", "Cumeral Language", "Siona Language", "Cubeo Language", "Cuiba language", "Desano Language", "Andoque Language", "Nukak language", "Ember\u00e1, Northern Language", "Cof\u00e1n Language", "Uwa language", "Awa-Cuaiquer Language", "Minica Huitoto", "Cams\u00e1 Language", "Totoro Language", "Tunebo, Central Language", "Malayo Language", "Nonuya language", "Piapoco Language", "Cabiyar\u00ed Language", "Romani, Vlax Language", "Quechua, Napo Lowland Language", "Carijona Language", "Bora Language", "Chipiajes Language", "Piratapuyo Language", "Yucuna Language", "Guanano Language", "Siriano Language", "Baudo language", "Anserma Language", "Wayuu Language", "Ocaina Language", "Ponares Language", "Macagu\u00e1n Language", "Barasana Language", "Arhuaco Language", "Yukpa Language", "S\u00e1liba Language", "Inga, Jungle Language", "Achawa language", "Koreguaje Language", "Spanish Language", "Tinigua language", "Puinave Language", "Pijao Language", "Runa Language", "Guambiano Language", "Tama Language", "Macaguaje Language", "Piaroa Language", "Murui Huitoto language", "Cagua Language", "Curripaco Language", "Palenquero Language", "Kogi Language", "Ticuna language", "Tunebo, Western Language", "Macuna Language", "Andaqui Language", "Tuyuca language", "Muinane Language", "Carabayo Language", "Providencia Sign Language", "Nheengatu language", "Natagaimas Language", "Playero language", "Coxima Language", "P\u00e1ez language", "Tucano Language", "Coyaima Language", "Colombian Sign Language", "Cocama language", "Hupd\u00eb Language", "Inga Language", "Islander Creole English", "Kuna, Border Language", "Waimaj\u00e3 Language", "Tomedes Language", "Bar\u00ed Language", "Tanimuca-Retuar\u00e3 Language", "Tunebo, Barro Negro Language", "Guayabero Language", "Omejes Language", "Catio language", "Tunebo, Angosturas Language"], "ans_acc": 0.023255813953488372, "ans_hit": 1, "ans_f1": 0.04545454545454545, "ans_precission": 1.0, "ans_recall": 0.023255813953488372, "path_f1": 0.04552845528455284, "path_precision": 0.7, "path_recall": 0.023529411764705882, "path_ans_f1": 0.04545454545454545, "path_ans_precision": 1.0, "path_ans_recall": 0.023255813953488372}
{"id": "WebQTest-631", "prediction": ["# Reasoning Path:\nRay Allen -> basketball.basketball_player.player_statistics -> m.04qbrjm -> basketball.basketball_player_stats.season -> 2006\u201307 NBA season\n# Answer:\n2006\u201307 NBA season", "# Reasoning Path:\nRay Allen -> basketball.basketball_player.player_statistics -> m.04qcnpp -> basketball.basketball_player_stats.season -> 2002\u201303 NBA season\n# Answer:\n2002\u201303 NBA season", "# Reasoning Path:\nRay Allen -> people.person.sibling_s -> m.0j2h3vr -> people.sibling_relationship.sibling -> Kristie Raye\n# Answer:\nKristie Raye", "# Reasoning Path:\nRay Allen -> basketball.basketball_player.player_statistics -> m.04qbrjm -> basketball.basketball_player_stats.team -> Seattle Supersonics\n# Answer:\nSeattle Supersonics", "# Reasoning Path:\nRay Allen -> basketball.basketball_player.player_statistics -> m.04qdv8z -> basketball.basketball_player_stats.season -> 1997\u201398 NBA season\n# Answer:\n1997\u201398 NBA season", "# Reasoning Path:\nRay Allen -> sports.pro_athlete.teams -> m.05cf4bc -> sports.sports_team_roster.team -> Connecticut Huskies men's basketball\n# Answer:\nConnecticut Huskies men's basketball", "# Reasoning Path:\nRay Allen -> basketball.basketball_player.player_statistics -> m.04qcnpp -> basketball.basketball_player_stats.team -> Seattle Supersonics\n# Answer:\nSeattle Supersonics", "# Reasoning Path:\nRay Allen -> people.person.sibling_s -> m.0ztqw9x -> people.sibling_relationship.sibling -> John Allen\n# Answer:\nJohn Allen", "# Reasoning Path:\nRay Allen -> basketball.basketball_player.player_statistics -> m.04qdv8z -> basketball.basketball_player_stats.team -> Milwaukee Bucks\n# Answer:\nMilwaukee Bucks", "# Reasoning Path:\nRay Allen -> people.person.sibling_s -> m.0ztqwb8 -> people.sibling_relationship.sibling -> Kim Allen\n# Answer:\nKim Allen"], "ground_truth": ["1996"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-633", "prediction": ["# Reasoning Path:\nClaire Danes -> people.person.profession -> Television producer -> common.topic.subject_of -> Michael Palance\n# Answer:\nTelevision producer", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Voice Actor -> people.profession.specialization_of -> Actor\n# Answer:\nVoice Actor", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Television producer -> common.topic.notable_types -> Profession\n# Answer:\nTelevision producer", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Television producer -> common.topic.subject_of -> Billy Sorrentino\n# Answer:\nTelevision producer", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Television producer -> common.topic.subjects -> Michael Palance\n# Answer:\nTelevision producer", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Television producer -> common.topic.subjects -> Billy Sorrentino\n# Answer:\nTelevision producer", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Voice Actor -> common.topic.notable_types -> Profession\n# Answer:\nVoice Actor", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Actor -> common.topic.subjects -> Michael Palance\n# Answer:\nActor", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Voice Actor -> film.film_job.films_with_this_crew_job -> m.0y8734m\n# Answer:\nVoice Actor", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Actor -> people.profession.specializations -> Voice Actor\n# Answer:\nActor"], "ground_truth": ["Voice Actor", "Actor", "Television producer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-634", "prediction": ["# Reasoning Path:\nSan Antonio -> location.location.containedby -> Bexar County -> location.location.containedby -> San Antonio, TX Metropolitan Statistical Area\n# Answer:\nBexar County", "# Reasoning Path:\nSan Antonio -> location.hud_county_place.county -> Comal County -> location.location.containedby -> United States of America\n# Answer:\nComal County", "# Reasoning Path:\nSan Antonio -> location.location.containedby -> Bexar County -> location.location.containedby -> Texas\n# Answer:\nBexar County", "# Reasoning Path:\nSan Antonio -> location.location.containedby -> Bexar County -> location.hud_foreclosure_area.ofheo_price_change -> m.07cs268\n# Answer:\nBexar County", "# Reasoning Path:\nSan Antonio -> location.location.containedby -> Bexar County -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nBexar County", "# Reasoning Path:\nSan Antonio -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> San Francisco\n# Answer:\nUnited States of America", "# Reasoning Path:\nSan Antonio -> location.location.containedby -> Area code 210 -> location.location.containedby -> Texas\n# Answer:\nArea code 210", "# Reasoning Path:\nSan Antonio -> location.hud_county_place.county -> Comal County -> location.location.containedby -> San Antonio, TX Metropolitan Statistical Area\n# Answer:\nComal County", "# Reasoning Path:\nSan Antonio -> location.hud_county_place.county -> Comal County -> base.aareas.schema.administrative_area.administrative_area_type -> U.S. county\n# Answer:\nComal County", "# Reasoning Path:\nSan Antonio -> location.hud_county_place.county -> Comal County -> location.location.containedby -> Edwards Plateau\n# Answer:\nComal County"], "ground_truth": ["Comal County", "Medina County", "Bexar County"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7272727272727272, "ans_precission": 0.8, "ans_recall": 0.6666666666666666, "path_f1": 0.3636363636363636, "path_precision": 0.4, "path_recall": 0.3333333333333333, "path_ans_f1": 0.7272727272727272, "path_ans_precision": 0.8, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-635", "prediction": ["# Reasoning Path:\nMatt Stone -> film.actor.film -> m.0bthgvx -> film.performance.character -> Wizard of Oz\n# Answer:\nWizard of Oz", "# Reasoning Path:\nMatt Stone -> film.actor.film -> m.0bthgvx -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nMatt Stone -> film.actor.film -> m.02t9_gm -> film.performance.character -> Dave the Lighting Guy\n# Answer:\nDave the Lighting Guy", "# Reasoning Path:\nMatt Stone -> film.actor.film -> m.0bthgvx -> film.performance.film -> South Park: Bigger, Longer & Uncut\n# Answer:\nSouth Park: Bigger, Longer & Uncut", "# Reasoning Path:\nMatt Stone -> film.actor.film -> m.0bmzsb6 -> film.performance.character -> Kyle / Kenny / Jesus\n# Answer:\nKyle / Kenny / Jesus", "# Reasoning Path:\nMatt Stone -> fictional_universe.fictional_character_creator.fictional_characters_created -> Eric Cartman -> fictional_universe.fictional_character.character_created_by -> Trey Parker\n# Answer:\nEric Cartman", "# Reasoning Path:\nMatt Stone -> fictional_universe.fictional_character_creator.fictional_characters_created -> Mr. Garrison -> fictional_universe.fictional_character.character_created_by -> Trey Parker\n# Answer:\nMr. Garrison", "# Reasoning Path:\nMatt Stone -> fictional_universe.fictional_character_creator.fictional_characters_created -> Eric Cartman -> fictional_universe.fictional_character.parents -> Ms. Cartman\n# Answer:\nEric Cartman", "# Reasoning Path:\nMatt Stone -> tv.tv_actor.starring_roles -> m.02s9hwb -> tv.regular_tv_appearance.character -> Kyle Broflovski\n# Answer:\nKyle Broflovski", "# Reasoning Path:\nMatt Stone -> film.actor.film -> m.02t9_gm -> film.performance.film -> Orgazmo\n# Answer:\nOrgazmo"], "ground_truth": ["George Clooney", "Doug Remer", "Man toasting drinks at MCA outdoor bar", "Kyle / Kenny / Jesus", "Saddam Hussein", "Kyle Broflovski", "Ethan Hawke", "Hermaphrodite", "Terrance Henry Stoot", "Dave the Lighting Guy", "Wizard of Oz", "Gerald Broflovski", "Former Columbine Student", "Jimbo Kearn", "Kenny McCormick", "Ticket Taker", "Danny Glover", "Bill Gates", "Taper Guy", "Chris", "James Humphries"], "ans_acc": 0.19047619047619047, "ans_hit": 1, "ans_f1": 0.25806451612903225, "ans_precission": 0.4, "ans_recall": 0.19047619047619047, "path_f1": 0.17647058823529413, "path_precision": 0.3, "path_recall": 0.125, "path_ans_f1": 0.25806451612903225, "path_ans_precision": 0.4, "path_ans_recall": 0.19047619047619047}
{"id": "WebQTest-636", "prediction": ["# Reasoning Path:\nMichigan -> location.location.time_zones -> Eastern Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Eastern Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Eastern Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> UTC\u221205:00 -> common.topic.notable_types -> Time Zone\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nMichigan -> government.governmental_jurisdiction.governing_officials -> m.010f1h5h -> government.government_position_held.basic_title -> Attorney general\n# Answer:\nAttorney general", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nMichigan -> government.governmental_jurisdiction.governing_officials -> m.010f1h5h -> government.government_position_held.office_position_or_title -> Michigan Attorney General\n# Answer:\nMichigan Attorney General"], "ground_truth": ["Central Time Zone", "Eastern Time Zone", "UTC\u221205:00"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-637", "prediction": ["# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Vietnam\n# Answer:\nSocialist state", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Socialist state -> common.topic.notable_types -> Form of Government\n# Answer:\nSocialist state", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Venezuela\n# Answer:\nSocialist state", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Socialist state -> common.topic.image -> Socialist states all\n# Answer:\nSocialist state", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Albania\n# Answer:\nParliamentary republic", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Parliamentary republic -> common.topic.article -> m.0glnt1\n# Answer:\nParliamentary republic", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Azerbaijan People's Government\n# Answer:\nSocialist state", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Parliamentary republic -> common.topic.image -> Form of government parliamentary\n# Answer:\nParliamentary republic", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Communist state -> common.topic.image -> Communist States\n# Answer:\nCommunist state", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Austria\n# Answer:\nParliamentary republic"], "ground_truth": ["Communist state", "Parliamentary republic", "Socialist state", "Soviet democracy"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.8571428571428571, "path_precision": 1.0, "path_recall": 0.75, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-638", "prediction": ["# Reasoning Path:\nJapan -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nJapan -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nJapan -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nJapan -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nEnglish Language", "# Reasoning Path:\nJapan -> location.country.languages_spoken -> English Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nJapan -> location.country.languages_spoken -> Japanese Language -> language.human_language.region -> Asia\n# Answer:\nJapanese Language", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.11b71r82sc\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nJapan -> location.country.languages_spoken -> Korean Language -> language.human_language.countries_spoken_in -> South Korea\n# Answer:\nKorean Language", "# Reasoning Path:\nJapan -> location.country.languages_spoken -> Korean Language -> language.human_language.region -> Asia\n# Answer:\nKorean Language"], "ground_truth": ["Esperanto Language", "Ainu Language", "Japanese Language", "English Language", "Korean Language"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6857142857142857, "ans_precission": 0.8, "ans_recall": 0.6, "path_f1": 0.6857142857142857, "path_precision": 0.8, "path_recall": 0.6, "path_ans_f1": 0.6857142857142857, "path_ans_precision": 0.8, "path_ans_recall": 0.6}
{"id": "WebQTest-639", "prediction": ["# Reasoning Path:\nDennis Rodman -> sports.pro_athlete.teams -> m.0_qx2_s -> sports.sports_team_roster.team -> Detroit Pistons\n# Answer:\nDetroit Pistons", "# Reasoning Path:\nDennis Rodman -> sports.pro_athlete.teams -> m.0_qwx1g -> sports.sports_team_roster.team -> Chicago Bulls\n# Answer:\nChicago Bulls", "# Reasoning Path:\nDennis Rodman -> people.person.profession -> Athlete -> common.topic.notable_types -> Profession\n# Answer:\nAthlete", "# Reasoning Path:\nDennis Rodman -> sports.pro_athlete.teams -> m.0_qx2_s -> sports.sports_team_roster.position -> Power forward\n# Answer:\nPower forward", "# Reasoning Path:\nDennis Rodman -> sports.pro_athlete.teams -> m.0_qwx1g -> sports.sports_team_roster.position -> Power forward\n# Answer:\nPower forward", "# Reasoning Path:\nDennis Rodman -> people.person.profession -> Athlete -> people.profession.specializations -> Basketball player\n# Answer:\nAthlete", "# Reasoning Path:\nDennis Rodman -> sports.pro_athlete.teams -> m.0_qwyt6 -> sports.sports_team_roster.team -> San Antonio Spurs\n# Answer:\nSan Antonio Spurs", "# Reasoning Path:\nDennis Rodman -> award.award_winner.awards_won -> m.059pgqh -> award.award_honor.award -> Razzie Award for Worst Supporting Actor\n# Answer:\nRazzie Award for Worst Supporting Actor", "# Reasoning Path:\nDennis Rodman -> sports.pro_athlete.teams -> m.0_qwyt6 -> sports.sports_team_roster.position -> Power forward\n# Answer:\nPower forward", "# Reasoning Path:\nDennis Rodman -> award.award_winner.awards_won -> m.059pgqh -> award.award_honor.ceremony -> 18th Golden Raspberry Awards\n# Answer:\n18th Golden Raspberry Awards"], "ground_truth": ["1995"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> award.award_nominee.award_nominations -> m.011lncpm -> award.award_nomination.nominated_for -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> common.topic.notable_types -> Film character\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Oliver Twist -> theater.theater_character.plays_appears_in -> Oliver!\n# Answer:\nOliver Twist", "# Reasoning Path:\nCharles Dickens -> award.award_nominee.award_nominations -> m.011lncpm -> award.award_nomination.award_nominee -> Orson Welles\n# Answer:\nOrson Welles", "# Reasoning Path:\nCharles Dickens -> award.award_nominee.award_nominations -> m.011lncpm -> award.award_nomination.award -> Retro Hugo for Best Dramatic Presentation, Short Form\n# Answer:\nRetro Hugo for Best Dramatic Presentation, Short Form", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Oliver Twist -> common.topic.image -> \\\"Please, sir, I want some more.\\\" Illustration by George Cruikshank.\n# Answer:\nOliver Twist", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> film.film_character.portrayed_in_films -> m.010p_33b\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced_by -> H. G. Wells\n# Answer:\nAldous Huxley"], "ground_truth": ["A Tale of Two Cities (40th Anniversary Edition)", "A Tale Of Two Cities (Adult Classics in Audio)", "A Tale of Two Cities (Everyman's Library (Paper))", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "A Tale of Two Cities (Adopted Classic)", "A Christmas Carol (Classic Fiction)", "The Mystery of Edwin Drood", "A Christmas Carol (Young Reading Series 2)", "Dombey and Son.", "A Tale of Two Cities (Naxos AudioBooks)", "A Christmas Carol (Reissue)", "A Tale of Two Cities (Konemann Classics)", "A Tale of Two Cities (Penguin Popular Classics)", "A Christmas Carol (Children's Theatre Playscript)", "A Christmas Carol (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Compact English Classics)", "A Tale of Two Cities (Everyman's Library Classics)", "A Christmas Carol (Apple Classics)", "A Tale of Two Cities (Collected Works of Charles Dickens)", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "David Copperfield", "Hard times", "A Christmas Carol (The Kennett Library)", "A Christmas Carol (Thornes Classic Novels)", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Progressive English)", "A Tale of Two Cities (Bookcassette(r) Edition)", "A Christmas Carol (Classic Collection)", "A Christmas Carol (Scholastic Classics)", "A Christmas Carol (Dramascripts Classic Texts)", "A Tale of Two Cities (Tor Classics)", "A Christmas Carol (Radio Theatre)", "A Tale of Two Cities (Signet Classics)", "A Tale of Two Cities (Barnes & Noble Classics Series)", "Our mutual friend.", "The cricket on the hearth", "A Christmas Carol (Gollancz Children's Classics)", "Great expectations.", "A Christmas Carol (Ladybird Children's Classics)", "A Tale of Two Cities (Large Print Edition)", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "A Christmas Carol (Tor Classics)", "The old curiosity shop", "A Tale of Two Cities (Prentice Hall Science)", "A Tale of Two Cities (Piccolo Books)", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Christmas Carol (Cover to Cover)", "A Tale of Two Cities (Pacemaker Classics)", "A CHRISTMAS CAROL", "A Christmas Carol (Nelson Graded Readers)", "A Christmas Carol (Pacemaker Classic)", "A Christmas Carol (Family Classics)", "Great Expectations.", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "A Christmas Carol (Pacemaker Classics)", "Bleak House", "A Christmas Carol (Limited Editions)", "A Tale of Two Cities (Classic Retelling)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "A Christmas Carol (Cp 1135)", "David Copperfield.", "A Christmas Carol (Children's Classics)", "A Christmas Carol (Illustrated Classics)", "A Tale of Two Cities (Cassette (1 Hr).)", "A Tale of Two Cities (Dramatized)", "A Christmas Carol (Read & Listen Books)", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "Dombey and Son", "A Tale Of Two Cities (Adult Classics)", "A Christmas Carol (Enriched Classics)", "A Christmas Carol (Penguin Student Editions)", "Oliver Twist", "A Christmas Carol (Penguin Readers, Level 2)", "The mystery of Edwin Drood", "A Christmas Carol (Classics Illustrated)", "A Christmas Carol (R)", "The old curiosity shop.", "A Christmas Carol (Acting Edition)", "A Christmas Carol (Green Integer, 50)", "A Christmas Carol (Classics for Young Adults and Adults)", "A Christmas Carol (Watermill Classics)", "Great Expectations", "A Tale of Two Cities (Dodo Press)", "A Christmas Carol", "A Christmas Carol (Saddleback Classics)", "A Tale of Two Cities (Classic Fiction)", "The Pickwick papers", "A Tale of Two Cities (Penguin Classics)", "Great expectations", "A Tale of Two Cities (Longman Fiction)", "A TALE OF TWO CITIES", "A Tale of Two Cities (Unabridged Classics)", "A Christmas Carol (Dramascripts)", "A Tale of Two Cities (Isis Clear Type Classic)", "A Tale of Two Cities (Bantam Classic)", "A Christmas Carol. (Lernmaterialien)", "A Tale of Two Cities (Ultimate Classics)", "A Tale of Two Cities (Wordsworth Classics)", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Courage Literary Classics)", "A Tale of Two Cities (Everyman Paperbacks)", "A Tale of Two Cities (Penguin Readers, Level 5)", "A Tale of Two Cities (The Classic Collection)", "A Christmas Carol (Soundings)", "A Tale of Two Cities (Dramascripts S.)", "A Tale of Two Cities (Silver Classics)", "A Tale of Two Cities (Simple English)", "A Tale of Two Cities (Illustrated Classics)", "A Christmas Carol (Bantam Classic)", "A Christmas Carol (Clear Print)", "A Christmas Carol (Oxford Bookworms Library)", "A Christmas Carol (Classic, Picture, Ladybird)", "A Christmas Carol (Through the Magic Window Series)", "A Tale of Two Cities (Paperback Classics)", "Bleak House.", "A Tale of Two Cities (Collector's Library)", "A Christmas Carol (Aladdin Classics)", "A Tale of Two Cities (Classics Illustrated Notes)", "A Christmas Carol (Chrysalis Children's Classics Series)", "A Tale of Two Cities (Ladybird Children's Classics)", "A Tale of Two Cities (Student's Novels)", "A Tale of Two Cities (Soundings)", "Martin Chuzzlewit", "A Christmas Carol (Watermill Classic)", "A Christmas Carol (Great Stories)", "A Christmas Carol (Value Books)", "A Tale of Two Cities (Classics Illustrated)", "A Christmas Carol (Usborne Young Reading)", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Longman Classics, Stage 2)", "Dombey and son", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "A Tale of Two Cities (Dover Thrift Editions)", "The life and adventures of Nicholas Nickleby", "The Pickwick Papers", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "Sketches by Boz", "A Christmas Carol (Audio Editions)", "A Tale of Two Cities", "A Tale of Two Cities (Cover to Cover Classics)", "Little Dorrit", "A Tale of Two Cities (BBC Audio Series)", "A Tale of Two Cities (The Greatest Historical Novels)", "Our mutual friend", "A Tale of Two Cities (Saddleback Classics)", "A Christmas Carol (Everyman's Library Children's Classics)", "A Christmas Carol (Take Part)", "A Christmas Carol (Puffin Classics)", "A Tale of Two Cities (10 Cassettes)", "A Tale of Two Cities (Cyber Classics)", "A Tale of Two Cities (Oxford Playscripts)", "A Christmas Carol (Large Print)", "A Tale of Two Cities (Masterworks)", "A Tale of Two Cities (Illustrated Junior Library)", "A Tale of Two Cities (Clear Print)", "A Christmas Carol (Ladybird Classics)", "A Tale of Two Cities (Oxford Bookworms Library)", "A Tale of Two Cities (Classic Literature with Classical Music)", "A Tale of Two Cities (Macmillan Students' Novels)", "A Christmas Carol (New Longman Literature)", "The Old Curiosity Shop", "A Tale of Two Cities (Puffin Classics)", "Bleak house", "A Tale of Two Cities (Acting Edition)", "A Tale of Two Cities (Enriched Classic)", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "A Christmas Carol (Whole Story)", "A Christmas Carol (Puffin Choice)"], "ans_acc": 0.023668639053254437, "ans_hit": 1, "ans_f1": 0.033519553072625705, "ans_precission": 0.3, "ans_recall": 0.01775147928994083, "path_f1": 0.3037974683544304, "path_precision": 0.5, "path_recall": 0.21818181818181817, "path_ans_f1": 0.03428571428571429, "path_ans_precision": 0.5, "path_ans_recall": 0.01775147928994083}
{"id": "WebQTest-640", "prediction": ["# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Tentmaker -> common.topic.notable_types -> Profession\n# Answer:\nTentmaker", "# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Tentmaker -> common.topic.notable_for -> g.1254xhyd0\n# Answer:\nTentmaker", "# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Missionary -> media_common.quotation_subject.quotations_about_this_subject -> As each Sister is to become a Co-Worker of Christ in the slums, each ought to understand what God and the Missionaries of Charity expect from her. Let Christ radiate and live his life in her and through her in the slums. Let the poor, seeing her, be drawn to Christ and invite him to enter their homes and their lives. Let the sick and suffering find in her a real angel of comfort and consolation. Let the little ones of the streets cling to her because she reminds them of him, the friend of the little ones.\n# Answer:\nMissionary", "# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Missionary -> base.ontologies.ontology_instance.equivalent_instances -> m.07ngj4q\n# Answer:\nMissionary", "# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Missionary -> base.schemastaging.context_name.pronunciation -> g.125_p_yp_\n# Answer:\nMissionary", "# Reasoning Path:\nPaul the Apostle -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nPaul the Apostle -> book.book_subject.works ->  1 Corinthians 14:21: Paul's reflection on gl\u014dssa -> book.journal_article.journal_publication -> m.012bzlpk\n# Answer:\n 1 Corinthians 14:21: Paul's reflection on gl\u014dssa", "# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Missionary -> media_common.quotation_subject.quotations_about_this_subject -> Firm, faithful, and devoted, full of energy and zeal, and truth, he labors for his race; he clears their painful way to improvement; he hews down like a giant the prejudices of creed and caste that encumber it. He may be stern; he may be exacting; he may be ambitious yet; but his is the sternness of the warrior Greatheart, who guards his pilgrim convoy from the onslaught of Apollyon. His is the exaction of the apostle, who speaks but for Christ, when he says, Whosoever will come after Me, let him deny himself, and take up his cross and follow Me. His is the ambition of the high master-spirit, which aims to fill a place in the first rank of those who are redeemed from the earth -- who stand without fault before the throne of God, who share the last mighty victories of the Lamb, who are called, and chosen, and faithful.\n# Answer:\nMissionary", "# Reasoning Path:\nPaul the Apostle -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nMale", "# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Prophet -> fictional_universe.character_occupation.characters_with_this_occupation -> Wundarr the Aquarian\n# Answer:\nProphet"], "ground_truth": ["Missionary", "Prophet", "Writer", "Tentmaker"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7241379310344827, "ans_precission": 0.7, "ans_recall": 0.75, "path_f1": 0.7241379310344827, "path_precision": 0.7, "path_recall": 0.75, "path_ans_f1": 0.7241379310344827, "path_ans_precision": 0.7, "path_ans_recall": 0.75}
{"id": "WebQTest-642", "prediction": ["# Reasoning Path:\nVin Scully -> people.person.education -> m.02wnq86 -> education.education.institution -> Fordham University\n# Answer:\nFordham University", "# Reasoning Path:\nVin Scully -> people.person.education -> m.0svtrlm -> education.education.institution -> Fordham Preparatory School\n# Answer:\nFordham Preparatory School", "# Reasoning Path:\nVin Scully -> common.topic.image -> Scully pub -> common.image.size -> m.029sr_k\n# Answer:\nScully pub", "# Reasoning Path:\nVin Scully -> film.actor.film -> m.0gdlst9 -> film.performance.film -> Play Ball!: Official Rules & Regulations\n# Answer:\nPlay Ball!: Official Rules & Regulations", "# Reasoning Path:\nVin Scully -> common.topic.image -> VinScully0308 -> common.image.appears_in_topic_gallery -> Los Angeles Dodgers Radio Network\n# Answer:\nVinScully0308", "# Reasoning Path:\nVin Scully -> film.actor.film -> m.0gdlwjf -> film.performance.film -> For Love of the Game\n# Answer:\nFor Love of the Game", "# Reasoning Path:\nVin Scully -> common.topic.image -> VinScully0308 -> common.image.size -> m.03t8x2n\n# Answer:\nVinScully0308", "# Reasoning Path:\nVin Scully -> film.actor.film -> m.0h11hq9 -> film.performance.film -> Bluetopia: The L.A. Dodgers Movie\n# Answer:\nBluetopia: The L.A. Dodgers Movie", "# Reasoning Path:\nVin Scully -> film.actor.film -> m.0gdlwjf -> film.performance.special_performance_type -> Him/Herself\n# Answer:\nHim/Herself"], "ground_truth": ["Fordham University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-643", "prediction": ["# Reasoning Path:\nLeia Organa -> fictional_universe.fictional_character.places_lived -> Alderaan -> fictional_universe.fictional_setting.setting_type -> Planet\n# Answer:\nAlderaan", "# Reasoning Path:\nLeia Organa -> fictional_universe.fictional_character.places_lived -> Alderaan -> fictional_universe.fictional_setting.characters_that_have_lived_here -> Breha Organa\n# Answer:\nAlderaan", "# Reasoning Path:\nLeia Organa -> fictional_universe.fictional_character.places_lived -> Alderaan -> common.topic.notable_for -> g.1254xtwfh\n# Answer:\nAlderaan", "# Reasoning Path:\nLeia Organa -> fictional_universe.fictional_character.places_lived -> Alderaan -> fictional_universe.fictional_setting.characters_that_have_lived_here -> g.1237vxbv\n# Answer:\nAlderaan", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.0wc74kc -> film.performance.film -> Star Wars: The Force Awakens\n# Answer:\nStar Wars: The Force Awakens", "# Reasoning Path:\nLeia Organa -> book.book_character.appears_in_book -> The Truce at Bakura -> book.written_work.subjects -> Star Wars\n# Answer:\nThe Truce at Bakura", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.0wc74kc -> film.performance.actor -> Carrie Fisher\n# Answer:\nCarrie Fisher", "# Reasoning Path:\nLeia Organa -> book.book_character.appears_in_book -> The Truce at Bakura -> book.book.genre -> Fiction\n# Answer:\nThe Truce at Bakura", "# Reasoning Path:\nLeia Organa -> book.book_character.appears_in_book -> Fury -> book.written_work.previous_in_series -> Inferno\n# Answer:\nFury", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.010wvf3v -> film.performance.film -> Robot Chicken: Star Wars Episode III\n# Answer:\nRobot Chicken: Star Wars Episode III"], "ground_truth": ["Alderaan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-644", "prediction": ["# Reasoning Path:\nEl Salvador -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nEl Salvador -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nEl Salvador -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Belize\n# Answer:\nSpanish Language", "# Reasoning Path:\nEl Salvador -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nEl Salvador -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> United States of America\n# Answer:\nPresidential system", "# Reasoning Path:\nEl Salvador -> location.statistical_region.poverty_rate_2dollars_per_day -> g.11b6c_pzzl\n# Answer:\nlocation.statistical_region.poverty_rate_2dollars_per_day", "# Reasoning Path:\nEl Salvador -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nSpanish Language", "# Reasoning Path:\nEl Salvador -> location.statistical_region.poverty_rate_2dollars_per_day -> g.11b6cn0xc8\n# Answer:\nlocation.statistical_region.poverty_rate_2dollars_per_day", "# Reasoning Path:\nEl Salvador -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Colombia\n# Answer:\nConstitutional republic", "# Reasoning Path:\nEl Salvador -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Colombia\n# Answer:\nPresidential system"], "ground_truth": ["Norman Quijano", "Mario Montoya", "Mauricio Alonso Rodr\u00edguez", "Saturnino Osorio", "Rub\u00e9n Zamora", "Julio Adalberto Rivera Carballo", "Pedro Chavarria", "Ernesto Aparicio", "Edwin Ramos", "Robert Renderos", "Doroteo Vasconcelos", "Miguel Angel Deras", "Elena Diaz", "Jose B. Gonzalez", "Malin Arvidsson", "Prudencia Ayala", "Jos\u00e9 Castellanos Contreras", "Jos\u00e9 Inocencio Alas", "Roberto Carlos Martinez", "Damaris Qu\u00e9les", "Xenia Estrada", "Erwin McManus", "Jos\u00e9 Manfredi Portillo", "F\u00e9lix Pineda", "Laura Molina", "Patricia Chica", "Francisco Due\u00f1as", "Am\u00e9rico Gonz\u00e1lez", "Santiago \\\"Jimmy\\\" Mellado", "Ra\u00fal Cicero", "Wilfredo Iraheta", "Jose Solis", "Francisco Gavidia", "\u00c1ngel Orellana", "Eva Dimas", "Milton Palacios", "Guillermo Garc\u00eda", "William L\u00f3pez", "Steve Montenegro", "Emilio Guardado", "DJ Quest", "Rafael Campo", "Sarah Ramos", "William Armando", "Tom\u00e1s Medina", "Enrique \u00c1lvarez C\u00f3rdova", "Isa\u00edas Choto", "Gerardo Barrios", "Arturo Rivera y Damas", "Armando Chac\u00f3n", "Francisco Funes", "Arturo Armando Molina", "Mario Wilfredo Contreras", "Salvador Castaneda Castro", "Jaime Portillo", "Diego Vel\u00e1zquez", "Mauricio Alvarenga", "Alexander M\u00e9ndoza", "Eduardo Hern\u00e1ndez", "Gualberto Fern\u00e1ndez", "Papa A.P.", "Jorge Rivera", "Rutilio Grande", "Ruben Cedillos", "Juan Ram\u00f3n S\u00e1nchez", "Rafael Menj\u00edvar Ochoa", "Pedro Jos\u00e9 Escal\u00f3n", "Carlos Linares", "\u00d3scar Antonio Ulloa", "Ana Sol Gutierrez", "Rene Moran", "Victor Lopez", "Bobby Rivas", "Nicolas F. Shi", "Bernard Lewinsky", "Claudia Lars", "g.11b8058v7j", "Elmer Acevedo", "Ricardo L\u00f3pez Tenorio", "Francisco Men\u00e9ndez", "Victor Manuel Ochoa", "Paula Heredia", "Jorge Mel\u00e9ndez", "Johnny Lopez", "Juan Rafael Bustillo", "Mauricio Alfaro", "William Renderos Iraheta", "Marlon Menj\u00edvar", "Carlos Barrios", "V\u00edctor Ram\u00edrez", "Genaro Serme\u00f1o", "Jorge B\u00facaro", "Consuelo de Saint Exup\u00e9ry", "Andr\u00e9s Eduardo Men\u00e9ndez", "Selvin Gonz\u00e1lez", "Keoki", "Roberto Rivas", "Manuel Enrique Araujo", "Miguel Cruz", "Alexander Campos", "Joel Aguilar", "Richard Oriani", "Fausto Omar V\u00e1squez", "Jos\u00e9 Mar\u00eda Ca\u00f1as", "Jos\u00e9 Francisco Valiente", "Takeshi Fujiwara", "Alfredo Ruano", "Jose Orlando Martinez", "Eduardo \\\"Volkswagen\\\" Hern\u00e1ndez", "Melvin Barrera", "Pedro Geoffroy Rivas", "Camilo Minero", "Ana Maria de Martinez", "Francisca Gonz\u00e1lez", "Jos\u00e9 Luis Rugamas", "Miguel Ca\u00f1izalez"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-645", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Leonardo da Vinci and the Art of Sculpture: Inspiration and Invention\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Raphael and his age\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> people.profession.people_with_this_profession -> Sandro Botticelli\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi -> visual_art.artwork.art_subject -> Biblical Magi\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> influence.influence_node.influenced_by -> Melozzo da Forl\u00ec\n# Answer:\nAntonio da Correggio", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi -> visual_art.artwork.locations -> m.0pcqbm3\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Annunciation -> common.topic.notable_types -> Artwork\n# Answer:\nAnnunciation", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Bernardino Luini -> visual_art.visual_artist.art_forms -> Painting\n# Answer:\nBernardino Luini", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> common.topic.subject_of -> BRS Custom Painting\n# Answer:\nPainting"], "ground_truth": ["Painting", "Sculpture", "Drawing"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-646", "prediction": ["# Reasoning Path:\nMary Bell -> common.topic.notable_for -> g.125bm4y8j\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMary Bell -> people.person.place_of_birth -> Newcastle upon Tyne -> base.aareas.schema.administrative_area.administrative_area_type -> English metropolitan borough city\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nMary Bell -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.containedby -> United Kingdom\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nMary Bell -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.containedby -> England\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nMary Bell -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.contains -> Woolsington\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nMary Bell -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.containedby -> North East England\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nMary Bell -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.contains -> Basil's Kitchen\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nMary Bell -> book.book_subject.works -> Cries Unheard: Why Children Kill - The Story of Mary Bell -> book.book.genre -> Non-fiction\n# Answer:\nCries Unheard: Why Children Kill - The Story of Mary Bell", "# Reasoning Path:\nMary Bell -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.contains -> Blakelaw and North Fenham\n# Answer:\nNewcastle upon Tyne"], "ground_truth": ["Newcastle upon Tyne"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-648", "prediction": ["# Reasoning Path:\nKimberly Williams-Paisley -> people.person.spouse_s -> m.02kkn0v -> people.marriage.spouse -> Brad Paisley\n# Answer:\nBrad Paisley", "# Reasoning Path:\nKimberly Williams-Paisley -> people.person.spouse_s -> m.02kkn0v -> people.marriage.location_of_ceremony -> Pepperdine University\n# Answer:\nPepperdine University", "# Reasoning Path:\nKimberly Williams-Paisley -> common.topic.article -> m.05d5ml\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nKimberly Williams-Paisley -> people.person.spouse_s -> m.02kkn0v -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nKimberly Williams-Paisley -> common.topic.webpage -> m.062tr2x -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nKimberly Williams-Paisley -> common.topic.webpage -> m.062tr2x -> common.webpage.resource -> m.0h24qs_\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nKimberly Williams-Paisley -> common.topic.webpage -> m.0948l9p -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nKimberly Williams-Paisley -> common.topic.webpage -> m.094dn22 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nKimberly Williams-Paisley -> common.topic.webpage -> m.0948l9p -> common.webpage.resource -> Matched Sets\n# Answer:\nMatched Sets", "# Reasoning Path:\nKimberly Williams-Paisley -> common.topic.webpage -> m.094dn22 -> common.webpage.resource -> Kimberly Williams' looks familiar\n# Answer:\nKimberly Williams' looks familiar"], "ground_truth": ["Brad Paisley"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-649", "prediction": ["# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0hnjpjf -> award.award_nomination.nominated_for -> Crazy, Stupid, Love.\n# Answer:\nCrazy, Stupid, Love.", "# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0z83ls_ -> award.award_nomination.nominated_for -> Crazy, Stupid, Love.\n# Answer:\nCrazy, Stupid, Love.", "# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0hn5tnq -> award.award_nomination.nominated_for -> Drive\n# Answer:\nDrive", "# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0hnjpjf -> award.award_nomination.ceremony -> 69th Golden Globe Awards\n# Answer:\n69th Golden Globe Awards", "# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0z83ls_ -> award.award_nomination.award -> Teen Choice Award for Choice Movie Actor: Comedy\n# Answer:\nTeen Choice Award for Choice Movie Actor: Comedy", "# Reasoning Path:\nRyan Gosling -> film.actor.film -> g.11b6hsfq8c\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0z83ls_ -> award.award_nomination.ceremony -> 2012 Teen Choice Awards\n# Answer:\n2012 Teen Choice Awards", "# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0hn5tnq -> award.award_nomination.ceremony -> 17th Critics' Choice Awards\n# Answer:\n17th Critics' Choice Awards", "# Reasoning Path:\nRyan Gosling -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#range -> Person\n# Answer:\nChildren", "# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0hn5tnq -> award.award_nomination.award -> Critics' Choice Movie Award for Best Actor\n# Answer:\nCritics' Choice Movie Award for Best Actor"], "ground_truth": ["Nothing Too Good for a Cowboy", "Drunk History Christmas - Feat. Ryan Gosling, Jim Carrey & Eva Mendes", "Touch of Evil", "The Nice Guys", "Blue Valentine", "The Big Short", "The Ides of March", "Only God Forgives", "The Unbelievables", "Gangster Squad", "Remember the Titans", "Logan's Run", "Weightless", "The United States of Leland", "The Notebook", "Murder by Numbers", "I'm Still Here: Real Diaries of Young People Who Lived During the Holocaust", "All Good Things", "Half Nelson", "The Place Beyond the Pines", "Crazy, Stupid, Love.", "Lars and the Real Girl", "Stay", "The Believer", "Fracture", "Drive", "The Slaughter Rule"], "ans_acc": 0.07407407407407407, "ans_hit": 1, "ans_f1": 0.1188118811881188, "ans_precission": 0.3, "ans_recall": 0.07407407407407407, "path_f1": 0.06593406593406594, "path_precision": 0.3, "path_recall": 0.037037037037037035, "path_ans_f1": 0.1188118811881188, "path_ans_precision": 0.3, "path_ans_recall": 0.07407407407407407}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_commander -> Nikita Khrushchev\n# Answer:\nNikita Khrushchev", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.office_holder -> Nikita Khrushchev\n# Answer:\nNikita Khrushchev", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_conflict -> Cuban Missile Crisis\n# Answer:\nCuban Missile Crisis", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf -> military.military_command.military_commander -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_6 -> government.government_position_held.office_holder -> Vladimir Ivashko\n# Answer:\nVladimir Ivashko", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.049y3kf -> military.military_command.military_commander -> Ivan Konev\n# Answer:\nIvan Konev", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.governmental_body -> Central Committee of the Communist Party of the Soviet Union\n# Answer:\nCentral Committee of the Communist Party of the Soviet Union", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_holder -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf -> military.military_command.military_conflict -> World War II\n# Answer:\nWorld War II", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.049y3kf -> military.military_command.military_conflict -> Battle of Berlin\n# Answer:\nBattle of Berlin"], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-650", "prediction": ["# Reasoning Path:\nJulia Gillard -> government.politician.party -> m.046vpjr -> government.political_party_tenure.party -> Australian Labor Party\n# Answer:\nAustralian Labor Party", "# Reasoning Path:\nJulia Gillard -> government.politician.party -> m.0lr1xnw -> government.political_party_tenure.party -> Queensland Labor Party\n# Answer:\nQueensland Labor Party", "# Reasoning Path:\nJulia Gillard -> common.topic.article -> m.02kx9y\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nJulia Gillard -> people.person.parents -> John Gillard -> people.deceased_person.place_of_death -> Adelaide\n# Answer:\nJohn Gillard", "# Reasoning Path:\nJulia Gillard -> people.person.parents -> John Gillard -> people.person.children -> Alison Gillard\n# Answer:\nJohn Gillard", "# Reasoning Path:\nJulia Gillard -> people.person.parents -> Moira Gillard -> people.person.children -> Alison Gillard\n# Answer:\nMoira Gillard", "# Reasoning Path:\nJulia Gillard -> people.person.parents -> John Gillard -> people.person.profession -> Registered nurse\n# Answer:\nJohn Gillard", "# Reasoning Path:\nJulia Gillard -> people.person.parents -> Moira Gillard -> people.person.nationality -> Australia\n# Answer:\nMoira Gillard", "# Reasoning Path:\nJulia Gillard -> people.person.parents -> Moira Gillard -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nMoira Gillard", "# Reasoning Path:\nJulia Gillard -> people.person.parents -> Moira Gillard -> freebase.valuenotation.has_value -> Parents\n# Answer:\nMoira Gillard"], "ground_truth": ["Queensland Labor Party", "Australian Labor Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-651", "prediction": ["# Reasoning Path:\nGermany -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_38m_\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fdl8\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc38c_4\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_67jr\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_jlz4\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-652", "prediction": ["# Reasoning Path:\nSelena Gomez -> people.person.ethnicity -> Italian American -> common.topic.notable_types -> Ethnicity\n# Answer:\nItalian American", "# Reasoning Path:\nSelena Gomez -> music.artist.album -> g.1hc0gf71l\n# Answer:\nmusic.artist.album", "# Reasoning Path:\nSelena Gomez -> people.person.ethnicity -> Italian American -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Alex Russo\n# Answer:\nItalian American", "# Reasoning Path:\nSelena Gomez -> people.person.ethnicity -> Mexicans -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nMexicans", "# Reasoning Path:\nSelena Gomez -> people.person.ethnicity -> Italian American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nItalian American", "# Reasoning Path:\nSelena Gomez -> people.person.ethnicity -> Mexicans -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nMexicans", "# Reasoning Path:\nSelena Gomez -> award.award_nominee.award_nominations -> m.010wr51v -> award.award_nomination.award -> Teen Choice Award for Choice Smile\n# Answer:\nTeen Choice Award for Choice Smile", "# Reasoning Path:\nSelena Gomez -> people.person.ethnicity -> Mexicans -> people.ethnicity.geographic_distribution -> Mexico\n# Answer:\nMexicans", "# Reasoning Path:\nSelena Gomez -> music.artist.album -> Another Cinderella Story EP -> common.topic.notable_for -> g.1ydnq94r6\n# Answer:\nAnother Cinderella Story EP", "# Reasoning Path:\nSelena Gomez -> award.award_nominee.award_nominations -> m.010wr51v -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work"], "ground_truth": ["Italian American", "Mexicans"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-653", "prediction": ["# Reasoning Path:\nDenise van Outen -> people.person.spouse_s -> m.062thbp -> people.marriage.spouse -> Lee Mead\n# Answer:\nLee Mead", "# Reasoning Path:\nDenise van Outen -> people.person.spouse_s -> m.062thbp -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nDenise van Outen -> people.person.spouse_s -> m.062thbp -> people.marriage.location_of_ceremony -> Seychelles\n# Answer:\nSeychelles", "# Reasoning Path:\nDenise van Outen -> music.artist.track -> Tyler King -> music.recording.releases -> Tell Me on a Sunday\n# Answer:\nTyler King", "# Reasoning Path:\nDenise van Outen -> base.popstra.celebrity.dated -> m.065pw0h -> base.popstra.dated.participant -> Nick Moran\n# Answer:\nNick Moran", "# Reasoning Path:\nDenise van Outen -> music.artist.track -> Tyler King -> music.recording.artist -> Andrew Lloyd Webber\n# Answer:\nTyler King", "# Reasoning Path:\nDenise van Outen -> base.popstra.celebrity.dated -> m.065pybf -> base.popstra.dated.participant -> Richard Traviss\n# Answer:\nRichard Traviss", "# Reasoning Path:\nDenise van Outen -> base.popstra.celebrity.dated -> m.065pzwy -> base.popstra.dated.participant -> James Lance\n# Answer:\nJames Lance", "# Reasoning Path:\nDenise van Outen -> music.artist.track -> Capped Teeth and Caesar Salad -> music.recording.contributions -> m.0njmjp8\n# Answer:\nCapped Teeth and Caesar Salad", "# Reasoning Path:\nDenise van Outen -> music.artist.track -> Can't Take My Eyes Off You -> music.recording.artist -> Andy Williams\n# Answer:\nCan't Take My Eyes Off You"], "ground_truth": ["Lee Mead"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-654", "prediction": ["# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3f_h6\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> common.topic.notable_types -> Governmental Body\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> government.governmental_body.component_bodies -> Shura Council\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> government.governmental_body.component_bodies -> House of Representatives\n# Answer:\nParliament of Egypt"], "ground_truth": ["Egyptian pound"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-655", "prediction": ["# Reasoning Path:\nWyoming -> location.citytown.postal_codes -> 49519 -> common.topic.notable_types -> Postal Code\n# Answer:\n49519", "# Reasoning Path:\nWyoming -> location.citytown.postal_codes -> 49519 -> common.topic.notable_for -> g.1256qhz91\n# Answer:\n49519", "# Reasoning Path:\nWyoming -> location.citytown.postal_codes -> 49509 -> location.location.geolocation -> m.03ds4y_\n# Answer:\n49509", "# Reasoning Path:\nWyoming -> location.citytown.postal_codes -> 49519 -> location.postal_code.country -> United States of America\n# Answer:\n49519", "# Reasoning Path:\nWyoming -> location.statistical_region.population -> g.11b674hfwb\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nWyoming -> location.citytown.postal_codes -> 49509 -> common.topic.notable_for -> g.125dngrry\n# Answer:\n49509", "# Reasoning Path:\nWyoming -> location.citytown.postal_codes -> 49509 -> location.location.containedby -> Kent County\n# Answer:\n49509", "# Reasoning Path:\nWyoming -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Michigan\n# Answer:\nUnited States of America", "# Reasoning Path:\nWyoming -> location.statistical_region.population -> g.11bymkw_85\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nWyoming -> location.statistical_region.population -> g.11x1chmfg\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["49509", "49519"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-656", "prediction": ["# Reasoning Path:\nCosta Rica -> government.governmental_jurisdiction.governing_officials -> m.010g6lmd -> government.government_position_held.office_holder -> \u00d3scar Arias\n# Answer:\n\u00d3scar Arias", "# Reasoning Path:\nCosta Rica -> government.governmental_jurisdiction.governing_officials -> m.010g6ln4 -> government.government_position_held.office_holder -> Abel Pacheco\n# Answer:\nAbel Pacheco", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60wmkvh\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> government.governmental_jurisdiction.governing_officials -> m.0105mtsf -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nBasic title", "# Reasoning Path:\nCosta Rica -> government.governmental_jurisdiction.governing_officials -> m.0105mtsf -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.11b71w3_6h\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nCosta Rica -> government.governmental_jurisdiction.governing_officials -> m.0105mtsf -> government.government_position_held.office_position_or_title -> President of Costa Rica\n# Answer:\nPresident of Costa Rica", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_5xgy\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> government.governmental_jurisdiction.governing_officials -> m.0105mtsf -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nFrom", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_6n42\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars"], "ground_truth": ["Laura Chinchilla"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-657", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.events -> Assassination of Martin Luther King, Jr.\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.containedby -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Alabama\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nAlpha Man: The Brotherhood of MLK", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> common.topic.article -> m.05nnh0\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0_714v2 -> film.personal_film_appearance.film -> Alice Walker: Beauty in Truth\n# Answer:\nAlice Walker: Beauty in Truth", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Montgomery\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0gbz10_ -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-658", "prediction": ["# Reasoning Path:\nGospel of John -> book.written_work.author -> John the Evangelist -> people.person.parents -> Salome\n# Answer:\nJohn the Evangelist", "# Reasoning Path:\nGospel of John -> common.topic.notable_for -> g.125brlxm_\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nGospel of John -> book.written_work.author -> John the Evangelist -> symbols.name_source.namesakes -> Anglican Church of St. John the Evangelist\n# Answer:\nJohn the Evangelist", "# Reasoning Path:\nGospel of John -> book.written_work.author -> John the Evangelist -> common.topic.image -> John-the-evangelist-francesco-furini\n# Answer:\nJohn the Evangelist", "# Reasoning Path:\nGospel of John -> book.written_work.subjects -> Jesus Christ -> people.person.ethnicity -> Jewish people\n# Answer:\nJesus Christ", "# Reasoning Path:\nGospel of John -> book.written_work.author -> John the Evangelist -> symbols.name_source.namesakes -> Cathedral of St. John the Evangelist\n# Answer:\nJohn the Evangelist", "# Reasoning Path:\nGospel of John -> book.written_work.author -> John the Evangelist -> symbols.name_source.namesakes -> Church of St John the Evangelist, Poulton-le-Fylde\n# Answer:\nJohn the Evangelist", "# Reasoning Path:\nGospel of John -> book.written_work.author -> John the Evangelist -> common.topic.image -> JohnEvangelistReni\n# Answer:\nJohn the Evangelist", "# Reasoning Path:\nGospel of John -> book.written_work.subjects -> Healing the man blind from birth -> religion.religious_text.religious_text_of -> Christianity\n# Answer:\nHealing the man blind from birth", "# Reasoning Path:\nGospel of John -> book.written_work.subjects -> Jesus Christ -> common.topic.webpage -> m.09w3_0l\n# Answer:\nJesus Christ"], "ground_truth": ["John the Evangelist"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-659", "prediction": ["# Reasoning Path:\nUnited Kingdom -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Wales\n# Answer:\nParliamentary system", "# Reasoning Path:\nUnited Kingdom -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nUnited Kingdom -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Republic of Ireland\n# Answer:\nParliamentary system", "# Reasoning Path:\nUnited Kingdom -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Anguilla\n# Answer:\nParliamentary system", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nChild pornography", "# Reasoning Path:\nUnited Kingdom -> location.statistical_region.long_term_unemployment_rate -> g.12cp_jvpx\n# Answer:\nlocation.statistical_region.long_term_unemployment_rate", "# Reasoning Path:\nUnited Kingdom -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Sierra Leone\n# Answer:\nUnitary state", "# Reasoning Path:\nUnited Kingdom -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Angola\n# Answer:\nUnitary state", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.arrested_person -> Gary Glitter\n# Answer:\nGary Glitter", "# Reasoning Path:\nUnited Kingdom -> location.statistical_region.long_term_unemployment_rate -> g.1hhc390lx\n# Answer:\nlocation.statistical_region.long_term_unemployment_rate"], "ground_truth": ["Parliamentary system", "Constitutional monarchy", "Unitary state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_formerly_used -> Cuba\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> United States of America\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> American Samoa\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.statistical_region.gdp_deflator_change -> g.11b6ddwl64\n# Answer:\nlocation.statistical_region.gdp_deflator_change", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> Bonaire\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.target_of_exchange -> m.03x8_16\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Working People's Party of Puerto Rico -> common.topic.notable_types -> Political party\n# Answer:\nWorking People's Party of Puerto Rico", "# Reasoning Path:\nPuerto Rico -> location.statistical_region.gdp_deflator_change -> g.124x8g_1y\n# Answer:\nlocation.statistical_region.gdp_deflator_change", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.target_of_exchange -> m.04kq24y\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Working People's Party of Puerto Rico -> common.topic.article -> m.0k0w3qq\n# Answer:\nWorking People's Party of Puerto Rico"], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-661", "prediction": ["# Reasoning Path:\nSamir Nasri -> base.schemastaging.athlete_extra.salary -> m.0j6tdy1 -> base.schemastaging.athlete_salary.team -> Manchester City F.C.\n# Answer:\nManchester City F.C.", "# Reasoning Path:\nSamir Nasri -> sports.pro_athlete.teams -> m.04mb0rc -> sports.sports_team_roster.team -> Arsenal F.C.\n# Answer:\nArsenal F.C.", "# Reasoning Path:\nSamir Nasri -> sports.pro_athlete.teams -> m.04mb0rc -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nMidfielder", "# Reasoning Path:\nSamir Nasri -> sports.pro_athlete.teams -> m.0j52lrl -> sports.sports_team_roster.team -> France national football team\n# Answer:\nFrance national football team", "# Reasoning Path:\nSamir Nasri -> sports.pro_athlete.teams -> m.0j6tdwm -> sports.sports_team_roster.team -> Manchester City F.C.\n# Answer:\nManchester City F.C.", "# Reasoning Path:\nSamir Nasri -> base.schemastaging.athlete_extra.salary -> m.0j6tdy1 -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nPound sterling", "# Reasoning Path:\nSamir Nasri -> soccer.football_player.statistics -> m.0w8yfb2 -> soccer.football_player_stats.team -> Arsenal F.C.\n# Answer:\nArsenal F.C.", "# Reasoning Path:\nSamir Nasri -> sports.pro_athlete.teams -> m.0j52lrl -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nMidfielder", "# Reasoning Path:\nSamir Nasri -> sports.pro_athlete.teams -> m.0j52lrl -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nSamir Nasri -> sports.pro_athlete.teams -> m.0j6tdwm -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nMidfielder"], "ground_truth": ["Olympique de Marseille"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-662", "prediction": ["# Reasoning Path:\nCzech Republic -> location.country.currency_used -> Czech koruna -> common.topic.notable_types -> Currency\n# Answer:\nCzech koruna", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> Beskids -> location.location.partially_containedby -> Ukraine\n# Answer:\nBeskids", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> Beskids -> location.location.partially_contained_by -> m.0wg8sdh\n# Answer:\nBeskids", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> B\u00f3br -> geography.river.mouth -> Oder\n# Answer:\nB\u00f3br", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> Carpathian Mountains -> location.location.partially_containedby -> Ukraine\n# Answer:\nCarpathian Mountains", "# Reasoning Path:\nCzech Republic -> media_common.netflix_genre.titles -> A Walk Worthwhile -> film.film.language -> Czech Language\n# Answer:\nA Walk Worthwhile", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> Beskids -> location.location.partially_containedby -> Poland\n# Answer:\nBeskids", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> B\u00f3br -> location.location.partially_containedby -> Poland\n# Answer:\nB\u00f3br", "# Reasoning Path:\nCzech Republic -> media_common.netflix_genre.titles -> Adelheid -> film.film.country -> Czechoslovakia\n# Answer:\nAdelheid", "# Reasoning Path:\nCzech Republic -> media_common.netflix_genre.titles -> All My Loved Ones -> film.film.country -> Slovakia\n# Answer:\nAll My Loved Ones"], "ground_truth": ["Czech koruna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-663", "prediction": ["# Reasoning Path:\nWilliam Penn -> people.person.spouse_s -> m.0j4kvc6 -> people.marriage.spouse -> Hannah Callowhill Penn\n# Answer:\nHannah Callowhill Penn", "# Reasoning Path:\nWilliam Penn -> people.person.spouse_s -> m.0j6bdtn -> people.marriage.spouse -> Gulielma Maria Springett\n# Answer:\nGulielma Maria Springett", "# Reasoning Path:\nWilliam Penn -> symbols.name_source.namesakes -> William Penn High School -> education.educational_institution.students_graduates -> m.0svv28m\n# Answer:\nWilliam Penn High School", "# Reasoning Path:\nWilliam Penn -> people.person.parents -> Joan Gilbert\n# Answer:\nJoan Gilbert", "# Reasoning Path:\nWilliam Penn -> people.person.spouse_s -> m.0j4kvc6 -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nWilliam Penn -> people.person.parents -> Giles Penn\n# Answer:\nGiles Penn", "# Reasoning Path:\nWilliam Penn -> people.person.spouse_s -> m.0j6bdtn -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nWilliam Penn -> symbols.name_source.namesakes -> William Penn High School -> education.educational_institution.students_graduates -> m.0svzm7k\n# Answer:\nWilliam Penn High School", "# Reasoning Path:\nWilliam Penn -> symbols.name_source.namesakes -> William Penn High School -> common.topic.article -> m.028c5q8\n# Answer:\nWilliam Penn High School", "# Reasoning Path:\nWilliam Penn -> symbols.name_source.namesakes -> William Penn High School -> education.educational_institution.students_graduates -> m.0w55xpc\n# Answer:\nWilliam Penn High School"], "ground_truth": ["1672-04-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-664", "prediction": ["# Reasoning Path:\nFirefox -> computer.software.software_genre -> Web browser -> base.ontologies.ontology_instance.equivalent_instances -> m.07ng00y\n# Answer:\nWeb browser", "# Reasoning Path:\nFirefox -> computer.software.software_genre -> Web browser -> type.type.expected_by -> Works on web browser\n# Answer:\nWeb browser", "# Reasoning Path:\nFirefox -> computer.software.software_genre -> Web browser -> computer.software_genre.subgenres -> Text-based web browser\n# Answer:\nWeb browser", "# Reasoning Path:\nFirefox -> computer.web_browser.extensions -> Zotero -> computer.software.license -> Affero General Public License\n# Answer:\nZotero", "# Reasoning Path:\nFirefox -> computer.software.software_genre -> News aggregator -> internet.website_category.sites -> World News Network\n# Answer:\nNews aggregator", "# Reasoning Path:\nFirefox -> computer.web_browser.extensions -> Zotero -> common.topic.article -> m.026lvyb\n# Answer:\nZotero", "# Reasoning Path:\nFirefox -> computer.web_browser.extensions -> Bing Bar -> computer.software.developer -> Microsoft Corporation\n# Answer:\nBing Bar", "# Reasoning Path:\nFirefox -> computer.software.software_genre -> Web browser -> type.type.expected_by -> Browsers\n# Answer:\nWeb browser", "# Reasoning Path:\nFirefox -> computer.software.software_genre -> File Transfer Protocol -> computer.software_genre.software_in_genre -> 3D-FTP\n# Answer:\nFile Transfer Protocol", "# Reasoning Path:\nFirefox -> computer.software.software_genre -> File Transfer Protocol -> computer.internet_protocol.software_used_by -> Vim\n# Answer:\nFile Transfer Protocol"], "ground_truth": ["XUL", "C++", "XBL", "Cascading Style Sheets", "JavaScript"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.20000000000000004, "ans_precission": 0.2, "ans_recall": 0.2, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 1.0, "path_ans_recall": 0.2}
{"id": "WebQTest-665", "prediction": ["# Reasoning Path:\nNew Zealand -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> United Kingdom\n# Answer:\nParliamentary system", "# Reasoning Path:\nNew Zealand -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nNew Zealand -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Australia\n# Answer:\nParliamentary system", "# Reasoning Path:\nNew Zealand -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Canada\n# Answer:\nParliamentary system", "# Reasoning Path:\nNew Zealand -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nNew Zealand -> film.film_location.featured_in_films -> Yogi Bear -> media_common.netflix_title.netflix_genres -> Family\n# Answer:\nYogi Bear", "# Reasoning Path:\nNew Zealand -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nNew Zealand -> film.film_location.featured_in_films -> Yogi Bear -> film.film.genre -> Family\n# Answer:\nYogi Bear", "# Reasoning Path:\nNew Zealand -> olympics.olympic_participating_country.medals_won -> m.04gdvs5 -> olympics.olympic_medal_honor.medal -> Gold medal\n# Answer:\nGold medal", "# Reasoning Path:\nNew Zealand -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Japan\n# Answer:\nUnitary state"], "ground_truth": ["Parliamentary system", "Constitutional monarchy", "Unitary state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-666", "prediction": ["# Reasoning Path:\nIsthmus of Panama -> location.location.containedby -> South America -> location.location.containedby -> Americas\n# Answer:\nSouth America", "# Reasoning Path:\nIsthmus of Panama -> location.location.containedby -> South America -> location.location.containedby -> Western Hemisphere\n# Answer:\nSouth America", "# Reasoning Path:\nIsthmus of Panama -> common.topic.article -> m.04zwg3\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nIsthmus of Panama -> location.location.containedby -> South America -> location.location.containedby -> DVD Region 4\n# Answer:\nSouth America", "# Reasoning Path:\nIsthmus of Panama -> location.location.containedby -> South America -> location.location.events -> War of the Pacific\n# Answer:\nSouth America", "# Reasoning Path:\nIsthmus of Panama -> location.location.containedby -> South America -> book.book_subject.works -> A Brazilian Alphabet for the Younger Reader\n# Answer:\nSouth America", "# Reasoning Path:\nIsthmus of Panama -> common.topic.image -> Pm-map -> common.image.size -> m.02bh8qh\n# Answer:\nPm-map", "# Reasoning Path:\nIsthmus of Panama -> location.location.containedby -> South America -> location.location.events -> 2009 flu pandemic\n# Answer:\nSouth America", "# Reasoning Path:\nIsthmus of Panama -> location.location.containedby -> South America -> location.location.events -> 2013 Infinite 1st World Tour\n# Answer:\nSouth America", "# Reasoning Path:\nIsthmus of Panama -> location.location.containedby -> South America -> book.book_subject.works -> A History of Venice\n# Answer:\nSouth America"], "ground_truth": ["South America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-667", "prediction": ["# Reasoning Path:\nRepublic of Ireland -> location.country.capital -> Dublin -> location.location.containedby -> Ireland\n# Answer:\nDublin", "# Reasoning Path:\nRepublic of Ireland -> location.country.capital -> Dublin -> location.location.time_zones -> Irish Time Zone\n# Answer:\nDublin", "# Reasoning Path:\nRepublic of Ireland -> location.country.capital -> Dublin -> common.topic.notable_types -> City/Town/Village\n# Answer:\nDublin", "# Reasoning Path:\nRepublic of Ireland -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdmsw\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nRepublic of Ireland -> location.statistical_region.co2_emissions_per_capita -> g.1245_0s2z\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nRepublic of Ireland -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fm09\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nRepublic of Ireland -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37x9c\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nRepublic of Ireland -> location.statistical_region.co2_emissions_per_capita -> g.1245_0sgs\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nRepublic of Ireland -> location.statistical_region.co2_emissions_per_capita -> g.1245_1hyj\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita"], "ground_truth": ["Dublin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-668", "prediction": ["# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0z83n2p -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0sgl9q8 -> award.award_nomination.nominated_for -> The Twilight Saga: New Moon\n# Answer:\nThe Twilight Saga: New Moon", "# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0z83n2p -> award.award_nomination.ceremony -> 2012 Teen Choice Awards\n# Answer:\n2012 Teen Choice Awards", "# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.award_nominee -> Robert Pattinson\n# Answer:\nRobert Pattinson", "# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.ceremony -> 2012 Teen Choice Awards\n# Answer:\n2012 Teen Choice Awards", "# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0sgl9q8 -> award.award_nomination.award_nominee -> Taylor Lautner\n# Answer:\nTaylor Lautner", "# Reasoning Path:\nKristen Stewart -> base.popstra.celebrity.dated -> m.064tjrc -> base.popstra.dated.participant -> Michael Angarano\n# Answer:\nMichael Angarano", "# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0sgl9q8 -> award.award_nomination.award -> Blimp Award for Cutest Couple\n# Answer:\nBlimp Award for Cutest Couple", "# Reasoning Path:\nKristen Stewart -> base.popstra.celebrity.bought -> m.064tkql -> base.popstra.product_choice.product -> Chrysler White Aspen\n# Answer:\nChrysler White Aspen"], "ground_truth": ["Zathura", "Speak", "The Twilight Saga: Breaking Dawn - Part 1", "The Flintstones in Viva Rock Vegas", "Eclipse", "Fierce People", "Anesthesia", "Catch That Kid", "Twilight", "Panic Room", "The Messengers", "Welcome to the Rileys", "In the Land of Women", "Cold Creek Manor", "The Twilight Saga: Breaking Dawn - Part 2", "Billy Lynn's Long Halftime Walk", "Cutlass", "On the Road", "What Just Happened", "The Thirteenth Year", "Camp X-Ray", "Undertow", "The Runaways", "Into the Wild", "The Yellow Handkerchief", "The Twilight Saga: New Moon", "Snow White and the Huntsman", "Adventureland", "Jumper", "Equals", "The Safety of Objects", "The Big Shoe", "Clouds of Sils Maria", "Still Alice", "Snow White and the Huntsman 2", "The Cake Eaters", "American Ultra"], "ans_acc": 0.08108108108108109, "ans_hit": 1, "ans_f1": 0.12765957446808512, "ans_precission": 0.3, "ans_recall": 0.08108108108108109, "path_f1": 0.04477611940298508, "path_precision": 0.3, "path_recall": 0.024193548387096774, "path_ans_f1": 0.12765957446808512, "path_ans_precision": 0.3, "path_ans_recall": 0.08108108108108109}
{"id": "WebQTest-669", "prediction": ["# Reasoning Path:\nJosef Mengele -> people.person.profession -> Physician -> media_common.quotation_subject.quotations_about_this_subject -> A doctor, like anyone else who has to deal with human beings, each of them unique, cannot be a scientist; he is either, like the surgeon, a craftsman, or, like the physician and the psychologist, an artist. This means that in order to be a good doctor a man must also have a good character, that is to say, whatever weaknesses and foibles he may have, he must love his fellow human beings in the concrete and desire their good before his own.\n# Answer:\nPhysician", "# Reasoning Path:\nJosef Mengele -> people.person.profession -> Physician -> people.profession.specializations -> Psychologist\n# Answer:\nPhysician", "# Reasoning Path:\nJosef Mengele -> people.person.profession -> Physician -> media_common.quotation_subject.quotations_about_this_subject -> A patient going to a doctor for his first visit was asked, And whom did you consult before coming to me? Only the village druggist, was the answer. And what sort of foolish advice did that numbskull give you? asked the doctor, his tone and manner denoting his contempt for the advice of the layman. Oh, replied his patient, with no malice aforethought, he told me to come and see you.\n# Answer:\nPhysician", "# Reasoning Path:\nJosef Mengele -> people.person.profession -> Physician -> media_common.quotation_subject.quotations_about_this_subject -> A skilful leech is better far, than half a hundred men of war.\n# Answer:\nPhysician", "# Reasoning Path:\nJosef Mengele -> people.person.profession -> Physician -> base.descriptive_names.names.descriptive_name -> m.0105bg71\n# Answer:\nPhysician", "# Reasoning Path:\nJosef Mengele -> people.person.profession -> Physician -> people.profession.specializations -> Anesthesiologist\n# Answer:\nPhysician", "# Reasoning Path:\nJosef Mengele -> people.person.profession -> Physician -> people.profession.specializations -> Army Doctor\n# Answer:\nPhysician", "# Reasoning Path:\nJosef Mengele -> people.person.profession -> Physician -> base.descriptive_names.names.descriptive_name -> m.0105yyv9\n# Answer:\nPhysician", "# Reasoning Path:\nJosef Mengele -> people.person.education -> m.0n1bzk1 -> education.education.degree -> PhD\n# Answer:\nPhD", "# Reasoning Path:\nJosef Mengele -> people.person.places_lived -> m.0b9tg52 -> people.place_lived.location -> G\u00fcnzburg\n# Answer:\nG\u00fcnzburg"], "ground_truth": ["Physician"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Dementia\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> people.cause_of_death.parent_cause_of_death -> Cancer\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Cholecalciferol\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Epilepsy\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease.medical_specialties -> Oncology\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Head pressing\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Adjuvant\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease.medical_specialties -> Neurology\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.symptoms -> Air crescent sign\n# Answer:\nLung cancer"], "ground_truth": ["Lung cancer", "Brain tumor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-670", "prediction": ["# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.region -> Africa\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> base.rosetta.languoid.parent -> Arabic Group\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.writing_system -> Arabic alphabet\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.writing_system -> Arabic alphabet\n# Answer:\nArabic Language", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Siwi Language -> language.human_language.language_family -> Afroasiatic languages\n# Answer:\nSiwi Language", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Sudan\n# Answer:\nArabic Language", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nHouse of Representatives"], "ground_truth": ["Modern Standard Arabic"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-671", "prediction": ["# Reasoning Path:\nHawaii -> government.political_district.representatives -> m.030qq8b -> government.government_position_held.office_holder -> Daniel Kahikina Akaka\n# Answer:\nDaniel Kahikina Akaka", "# Reasoning Path:\nHawaii -> government.political_district.representatives -> m.030r1fw -> government.government_position_held.office_holder -> Daniel Inouye\n# Answer:\nDaniel Inouye", "# Reasoning Path:\nHawaii -> government.political_district.representatives -> m.09dm85m -> government.government_position_held.office_holder -> Hiram Fong\n# Answer:\nHiram Fong", "# Reasoning Path:\nHawaii -> government.political_district.representatives -> m.030qq8b -> freebase.valuenotation.is_reviewed -> To\n# Answer:\nTo", "# Reasoning Path:\nHawaii -> government.political_district.representatives -> m.030qq8b -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nHawaii -> government.political_district.representatives -> m.030r1fw -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nUnited States Senate", "# Reasoning Path:\nHawaii -> government.political_district.representatives -> m.09dm85m -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nHawaii -> government.political_district.representatives -> m.030qq8b -> freebase.valuenotation.is_reviewed -> Appointed By (if Position is Appointed)\n# Answer:\nAppointed By (if Position is Appointed)", "# Reasoning Path:\nHawaii -> government.political_district.representatives -> m.030r1fw -> government.government_position_held.legislative_sessions -> 100th United States Congress\n# Answer:\n100th United States Congress", "# Reasoning Path:\nHawaii -> book.book_subject.works -> Anatomy of Paradise -> book.book.editions -> Anatomy of paradise: Hawaii and the islands of the South Seas\n# Answer:\nAnatomy of Paradise"], "ground_truth": ["Mazie Hirono"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-672", "prediction": ["# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Vietnam\n# Answer:\nSocialist state", "# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Socialist state -> common.topic.notable_types -> Form of Government\n# Answer:\nSocialist state", "# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Socialist state -> common.topic.image -> Socialist states all\n# Answer:\nSocialist state", "# Reasoning Path:\nNorth Korea -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f719\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Azerbaijan People's Government\n# Answer:\nSocialist state", "# Reasoning Path:\nNorth Korea -> location.statistical_region.electricity_consumption_per_capita -> g.1245_0sg2\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Bukharan People's Soviet Republic\n# Answer:\nSocialist state", "# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Single-party state -> base.schemastaging.context_name.pronunciation -> g.125_klj_x\n# Answer:\nSingle-party state", "# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Juche -> government.political_ideology.political_parties -> Anti-Imperialist National Democratic Front\n# Answer:\nJuche", "# Reasoning Path:\nNorth Korea -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6fdwt\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment"], "ground_truth": ["Socialist state", "Single-party state", "Juche"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-673", "prediction": ["# Reasoning Path:\nRobert Kardashian -> people.person.education -> m.04hwvgz -> education.education.institution -> University of San Diego\n# Answer:\nUniversity of San Diego", "# Reasoning Path:\nRobert Kardashian -> people.person.education -> m.0646hdy -> education.education.institution -> University of Southern California\n# Answer:\nUniversity of Southern California", "# Reasoning Path:\nRobert Kardashian -> people.person.education -> m.0glw5zj -> education.education.institution -> University of San Diego School of Law\n# Answer:\nUniversity of San Diego School of Law", "# Reasoning Path:\nRobert Kardashian -> people.person.children -> Rob Kardashian -> people.person.education -> m.0n1n2gl\n# Answer:\nRob Kardashian", "# Reasoning Path:\nRobert Kardashian -> people.person.children -> Khlo\u00e9 Kardashian -> people.person.education -> m.0n5my85\n# Answer:\nKhlo\u00e9 Kardashian", "# Reasoning Path:\nRobert Kardashian -> people.person.profession -> Businessperson -> base.descriptive_names.names.descriptive_name -> m.0100z128\n# Answer:\nBusinessperson", "# Reasoning Path:\nRobert Kardashian -> people.person.children -> Kim Kardashian -> people.person.education -> m.0gx8_97\n# Answer:\nKim Kardashian", "# Reasoning Path:\nRobert Kardashian -> people.person.profession -> Businessperson -> base.descriptive_names.names.descriptive_name -> m.0105_bmw\n# Answer:\nBusinessperson", "# Reasoning Path:\nRobert Kardashian -> people.person.profession -> Businessperson -> people.profession.specializations -> Actuary\n# Answer:\nBusinessperson", "# Reasoning Path:\nRobert Kardashian -> people.person.profession -> Lawyer -> base.descriptive_names.names.descriptive_name -> m.0101pjk9\n# Answer:\nLawyer"], "ground_truth": ["University of San Diego", "University of San Diego School of Law", "University of Southern California"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-675", "prediction": ["# Reasoning Path:\nSt. James City -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nSt. James City -> location.hud_county_place.county -> Lee County -> location.location.containedby -> Florida\n# Answer:\nLee County", "# Reasoning Path:\nSt. James City -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Florida\n# Answer:\nUnited States of America", "# Reasoning Path:\nSt. James City -> location.location.containedby -> Florida -> location.location.containedby -> United States of America\n# Answer:\nFlorida", "# Reasoning Path:\nSt. James City -> location.location.containedby -> Lee County -> location.location.containedby -> Florida\n# Answer:\nLee County", "# Reasoning Path:\nSt. James City -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Florida\n# Answer:\nUnited States of America", "# Reasoning Path:\nSt. James City -> location.hud_county_place.county -> Lee County -> location.location.containedby -> South Florida water management district\n# Answer:\nLee County", "# Reasoning Path:\nSt. James City -> common.topic.image -> Location of St. James City, Florida -> common.image.size -> m.0kl4r6\n# Answer:\nLocation of St. James City, Florida", "# Reasoning Path:\nSt. James City -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nSt. James City -> location.location.containedby -> Florida -> location.location.containedby -> Contiguous United States\n# Answer:\nFlorida"], "ground_truth": ["Lee County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-676", "prediction": ["# Reasoning Path:\nAkkadian Empire -> location.location.contains -> Akkad -> location.location.geolocation -> m.0wmpxx7\n# Answer:\nAkkad", "# Reasoning Path:\nAkkadian Empire -> location.location.containedby -> Iraq -> location.location.containedby -> Western Asia\n# Answer:\nIraq", "# Reasoning Path:\nAkkadian Empire -> location.location.contains -> Akkad -> common.topic.notable_types -> City/Town/Village\n# Answer:\nAkkad", "# Reasoning Path:\nAkkadian Empire -> location.location.containedby -> Iraq -> location.location.containedby -> Middle East\n# Answer:\nIraq", "# Reasoning Path:\nAkkadian Empire -> location.location.containedby -> Iraq -> location.country.administrative_divisions -> Nineveh Governorate\n# Answer:\nIraq", "# Reasoning Path:\nAkkadian Empire -> location.location.containedby -> Baghdad Governorate -> location.location.containedby -> Iraq\n# Answer:\nBaghdad Governorate", "# Reasoning Path:\nAkkadian Empire -> location.location.containedby -> Iraq -> location.location.containedby -> Arab world\n# Answer:\nIraq", "# Reasoning Path:\nAkkadian Empire -> common.topic.notable_types -> Country -> freebase.type_profile.strict_included_types -> Location\n# Answer:\nCountry", "# Reasoning Path:\nAkkadian Empire -> location.location.containedby -> Iraq -> location.country.administrative_divisions -> Al Anbar Governorate\n# Answer:\nIraq", "# Reasoning Path:\nAkkadian Empire -> location.location.containedby -> Iraq -> location.country.administrative_divisions -> Al Diwaniyah Governorate\n# Answer:\nIraq"], "ground_truth": ["Akkad"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-677", "prediction": ["# Reasoning Path:\nOlympic Games -> time.recurring_event.current_frequency -> Quadrennial -> common.topic.notable_types -> Unit of frequency\n# Answer:\nQuadrennial", "# Reasoning Path:\nOlympic Games -> time.recurring_event.current_frequency -> Quadrennial -> common.topic.notable_for -> g.125d3_wnj\n# Answer:\nQuadrennial", "# Reasoning Path:\nOlympic Games -> time.recurring_event.current_frequency -> Quadrennial -> freebase.unit_profile.dimension -> Time\n# Answer:\nQuadrennial", "# Reasoning Path:\nOlympic Games -> common.topic.notable_types -> Website Category -> common.topic.article -> m.02ht13p\n# Answer:\nWebsite Category", "# Reasoning Path:\nOlympic Games -> common.topic.notable_types -> Website Category -> freebase.type_profile.kind -> Classification\n# Answer:\nWebsite Category", "# Reasoning Path:\nOlympic Games -> common.topic.notable_types -> Website Category -> type.type.properties -> Websites of this genre\n# Answer:\nWebsite Category", "# Reasoning Path:\nOlympic Games -> book.book_subject.works -> Berlin Games -> book.written_work.subjects -> World War II\n# Answer:\nBerlin Games", "# Reasoning Path:\nOlympic Games -> book.book_subject.works -> All That Glitters Is Not Gold: The Olympic Game -> book.written_work.author -> William O. Johnson Jr.\n# Answer:\nAll That Glitters Is Not Gold: The Olympic Game", "# Reasoning Path:\nOlympic Games -> book.book_subject.works -> Olympic dreams -> book.book.genre -> Children's literature\n# Answer:\nOlympic dreams", "# Reasoning Path:\nOlympic Games -> book.book_subject.works -> Berlin Games -> common.topic.notable_types -> Book\n# Answer:\nBerlin Games"], "ground_truth": ["Quadrennial"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-678", "prediction": ["# Reasoning Path:\nGuyana -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Andorra\n# Answer:\nUnitary state", "# Reasoning Path:\nGuyana -> location.country.form_of_government -> Unitary state -> common.topic.notable_types -> Form of Government\n# Answer:\nUnitary state", "# Reasoning Path:\nGuyana -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Angola\n# Answer:\nUnitary state", "# Reasoning Path:\nGuyana -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Algeria\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nGuyana -> location.country.form_of_government -> Semi-presidential system -> common.topic.image -> States with semi-presidential systems are shown in yellow\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nGuyana -> location.statistical_region.gni_in_ppp_dollars -> g.11b60z05j3\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nGuyana -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Bahamas\n# Answer:\nUnitary state", "# Reasoning Path:\nGuyana -> location.country.form_of_government -> Semi-presidential system -> base.schemastaging.context_name.pronunciation -> g.125_l584v\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nGuyana -> location.country.form_of_government -> Republic -> common.topic.image -> Roman Empire\n# Answer:\nRepublic", "# Reasoning Path:\nGuyana -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Burkina Faso\n# Answer:\nSemi-presidential system"], "ground_truth": ["Unitary state", "Republic", "Semi-presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nWilliam Daniels", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.0kv9d2d -> tv.regular_tv_appearance.actor -> Peter Parros\n# Answer:\nPeter Parros", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 4\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4 -> tv.tv_series_season.regular_cast -> m.03lj4m5\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.02h9cb0 -> tv.regular_tv_appearance.actor -> David Hasselhoff\n# Answer:\nDavid Hasselhoff", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 3\n# Answer:\nKnight Rider - Season 3", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.0kv9d2d -> tv.regular_tv_appearance.character -> g.1213vfbl\n# Answer:\ntv.regular_tv_appearance.character", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4 -> tv.tv_series_season.regular_cast -> m.02h9cb0\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 0 -> tv.tv_series_season.episodes -> Behind the Scenes\n# Answer:\nKnight Rider - Season 0"], "ground_truth": ["William Daniels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-680", "prediction": ["# Reasoning Path:\nSergio Ag\u00fcero -> sports.pro_athlete.teams -> m.0j_9hm9 -> sports.sports_team_roster.team -> Club Atl\u00e9tico Independiente\n# Answer:\nClub Atl\u00e9tico Independiente", "# Reasoning Path:\nSergio Ag\u00fcero -> sports.pro_athlete.teams -> m.0j3_f1z -> sports.sports_team_roster.team -> Manchester City F.C.\n# Answer:\nManchester City F.C.", "# Reasoning Path:\nSergio Ag\u00fcero -> soccer.football_player.statistics -> m.0w9hrct -> soccer.football_player_stats.team -> Club Atl\u00e9tico Independiente\n# Answer:\nClub Atl\u00e9tico Independiente", "# Reasoning Path:\nSergio Ag\u00fcero -> sports.pro_athlete.teams -> m.0j_9hm9 -> sports.sports_team_roster.position -> Forward\n# Answer:\nForward", "# Reasoning Path:\nSergio Ag\u00fcero -> sports.pro_athlete.teams -> m.04m4tkx -> sports.sports_team_roster.team -> Atl\u00e9tico Madrid\n# Answer:\nAtl\u00e9tico Madrid", "# Reasoning Path:\nSergio Ag\u00fcero -> soccer.football_player.matches_played -> m.0g9n9jg -> soccer.football_player_match_participation.team -> Argentina national football team\n# Answer:\nArgentina national football team", "# Reasoning Path:\nSergio Ag\u00fcero -> soccer.football_player.statistics -> m.0w8x3j2 -> soccer.football_player_stats.team -> Atl\u00e9tico Madrid\n# Answer:\nAtl\u00e9tico Madrid", "# Reasoning Path:\nSergio Ag\u00fcero -> sports.pro_athlete.teams -> m.0j3_f1z -> sports.sports_team_roster.position -> Forward\n# Answer:\nForward", "# Reasoning Path:\nSergio Ag\u00fcero -> soccer.football_player.statistics -> m.0w9d7pl -> soccer.football_player_stats.team -> Argentina national football team\n# Answer:\nArgentina national football team", "# Reasoning Path:\nSergio Ag\u00fcero -> soccer.football_player.matches_played -> m.0c0ltrg -> soccer.football_player_match_participation.team -> Argentina national football team\n# Answer:\nArgentina national football team"], "ground_truth": ["Manchester City F.C.", "Argentina national football team", "Atl\u00e9tico Madrid", "Club Atl\u00e9tico Independiente"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.5517241379310345, "path_precision": 0.8, "path_recall": 0.42105263157894735, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-681", "prediction": ["# Reasoning Path:\nEuropean Union -> government.governmental_jurisdiction.governing_officials -> m.0g5k82m -> government.government_position_held.office_holder -> Jerzy Buzek\n# Answer:\nJerzy Buzek", "# Reasoning Path:\nEuropean Union -> government.governmental_jurisdiction.governing_officials -> m.0g5k82w -> government.government_position_held.office_holder -> Jerzy Buzek\n# Answer:\nJerzy Buzek", "# Reasoning Path:\nEuropean Union -> government.governmental_jurisdiction.governing_officials -> m.0g5k82m -> government.government_position_held.office_position_or_title -> President of the European Parliament\n# Answer:\nPresident of the European Parliament", "# Reasoning Path:\nEuropean Union -> government.governmental_jurisdiction.governing_officials -> m.010p8k3t -> government.government_position_held.office_holder -> Nick Griffin\n# Answer:\nNick Griffin", "# Reasoning Path:\nEuropean Union -> location.statistical_region.broadband_penetration_rate -> g.1245_6ndt\n# Answer:\nlocation.statistical_region.broadband_penetration_rate", "# Reasoning Path:\nEuropean Union -> government.governmental_jurisdiction.governing_officials -> m.0g5k82w -> government.government_position_held.district_represented -> Silesian\n# Answer:\nSilesian", "# Reasoning Path:\nEuropean Union -> government.governmental_jurisdiction.governing_officials -> m.0g5k82w -> government.government_position_held.governmental_body -> European Parliament\n# Answer:\nEuropean Parliament", "# Reasoning Path:\nEuropean Union -> internet.website_owner.websites_owned -> http://europa.eu/index_en.htm -> common.topic.notable_types -> Website\n# Answer:\nhttp://europa.eu/index_en.htm", "# Reasoning Path:\nEuropean Union -> location.statistical_region.broadband_penetration_rate -> g.1245yvl64\n# Answer:\nlocation.statistical_region.broadband_penetration_rate", "# Reasoning Path:\nEuropean Union -> location.statistical_region.broadband_penetration_rate -> g.1245ywrjj\n# Answer:\nlocation.statistical_region.broadband_penetration_rate"], "ground_truth": ["Jerzy Buzek"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-683", "prediction": ["# Reasoning Path:\nMiss USA -> time.recurring_event.instances -> Miss USA 2000 -> award.competition.competitors -> Tara Wilson\n# Answer:\nMiss USA 2000", "# Reasoning Path:\nMiss USA -> time.recurring_event.instances -> Miss USA 1952 -> time.event.locations -> Long Beach\n# Answer:\nMiss USA 1952", "# Reasoning Path:\nMiss USA -> time.recurring_event.instances -> Miss USA 2000 -> time.event.locations -> Branson\n# Answer:\nMiss USA 2000", "# Reasoning Path:\nMiss USA -> time.recurring_event.instances -> Miss USA 1952 -> award.competition.type_of_competition -> Beauty Pageant\n# Answer:\nMiss USA 1952", "# Reasoning Path:\nMiss USA -> award.recurring_competition.individual_competitions -> Miss USA 2001 -> award.competition.competitors -> Tiffany Fallon\n# Answer:\nMiss USA 2001", "# Reasoning Path:\nMiss USA -> time.recurring_event.instances -> Miss USA 1952 -> common.topic.article -> m.04g28r2\n# Answer:\nMiss USA 1952", "# Reasoning Path:\nMiss USA -> time.recurring_event.instances -> Miss USA 2000 -> award.competition.competitors -> Jamie Kern\n# Answer:\nMiss USA 2000", "# Reasoning Path:\nMiss USA -> time.recurring_event.instances -> Miss USA 1954 -> award.competition.winner -> Miriam Stevenson\n# Answer:\nMiss USA 1954", "# Reasoning Path:\nMiss USA -> award.recurring_competition.individual_competitions -> Miss USA 2001 -> common.topic.notable_for -> g.1256p97kg\n# Answer:\nMiss USA 2001", "# Reasoning Path:\nMiss USA -> award.recurring_competition.individual_competitions -> Miss USA 1960 -> award.competition.competitors -> Linda Bement\n# Answer:\nMiss USA 1960"], "ground_truth": ["Miss USA 1993"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-684", "prediction": ["# Reasoning Path:\nNigeria -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> United States of America\n# Answer:\nPresidential system", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Chad\n# Answer:\nPresidential system", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> United States of America\n# Answer:\nFederal republic", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guinea\n# Answer:\nPresidential system", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_0sgz\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nNigeria -> location.statistical_region.external_debt_stock -> g.11b71rqxzt\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_22x5\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Sudan\n# Answer:\nFederal republic", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_2gxr\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nNigeria -> location.statistical_region.external_debt_stock -> g.1hhc38d1f\n# Answer:\nlocation.statistical_region.external_debt_stock"], "ground_truth": ["Federal republic", "Presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-685", "prediction": ["# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> travel.travel_destination.tourist_attractions -> Brooklyn Botanic Garden\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> people.person.places_lived -> m.04hv_n0 -> people.place_lived.location -> Brooklyn\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> travel.travel_destination.tourist_attractions -> Brooklyn Bridge\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> location.place_with_neighborhoods.neighborhoods -> Bath Beach\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> travel.travel_destination.tourist_attractions -> Brooklyn Historical Society Building\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> film.film_location.featured_in_films -> A Tree Grows in Brooklyn\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> location.place_with_neighborhoods.neighborhoods -> Bay Ridge\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> location.place_with_neighborhoods.neighborhoods -> Bedford\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> film.film_location.featured_in_films -> Annie Hall\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> film.film_location.featured_in_films -> Arranged\n# Answer:\nBrooklyn"], "ground_truth": ["Brooklyn"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-686", "prediction": ["# Reasoning Path:\nAtlanta Braves -> sports.sports_team.championships -> 1995 World Series -> sports.sports_championship_event.season -> 1995 Major League Baseball Season\n# Answer:\n1995 World Series", "# Reasoning Path:\nAtlanta Braves -> sports.sports_team.championships -> 1995 World Series -> common.topic.article -> m.04j717\n# Answer:\n1995 World Series", "# Reasoning Path:\nAtlanta Braves -> sports.professional_sports_team.owner_s -> Liberty Media Corporation -> sports.sports_team_owner.teams_owned -> Danville Braves\n# Answer:\nLiberty Media Corporation", "# Reasoning Path:\nAtlanta Braves -> sports.professional_sports_team.draft_picks -> m.04vw__5 -> sports.sports_league_draft_pick.draft -> 1982 Major League Baseball Draft\n# Answer:\n1982 Major League Baseball Draft", "# Reasoning Path:\nAtlanta Braves -> sports.professional_sports_team.draft_picks -> m.04vw_lk -> sports.sports_league_draft_pick.draft -> 1981 Major League Baseball Draft\n# Answer:\n1981 Major League Baseball Draft", "# Reasoning Path:\nAtlanta Braves -> sports.professional_sports_team.draft_picks -> m.04vw__5 -> sports.sports_league_draft_pick.player -> Duane Ward\n# Answer:\nDuane Ward", "# Reasoning Path:\nAtlanta Braves -> sports.professional_sports_team.draft_picks -> m.04vws5t -> sports.sports_league_draft_pick.draft -> 1965 Major League Baseball Draft\n# Answer:\n1965 Major League Baseball Draft", "# Reasoning Path:\nAtlanta Braves -> sports.professional_sports_team.owner_s -> Liberty Media Corporation -> internet.website_owner.websites_owned_new -> m.0rzl6cf\n# Answer:\nLiberty Media Corporation", "# Reasoning Path:\nAtlanta Braves -> sports.professional_sports_team.draft_picks -> m.04vw_lk -> sports.sports_league_draft_pick.player -> Jason Grimsley\n# Answer:\nJason Grimsley", "# Reasoning Path:\nAtlanta Braves -> sports.professional_sports_team.owner_s -> John C. Malone -> organization.organization_founder.organizations_founded -> Liberty Media Corporation\n# Answer:\nJohn C. Malone"], "ground_truth": ["1995 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-688", "prediction": ["# Reasoning Path:\nIran -> location.country.official_language -> Persian Language -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.official_language -> Persian Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Turkmen Language -> common.topic.notable_types -> Human Language\n# Answer:\nTurkmen Language", "# Reasoning Path:\nIran -> location.country.official_language -> Persian Language -> common.topic.notable_types -> Human Language\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Turkmen Language -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nTurkmen Language", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal_per_capita -> g.11b60rrlgr\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Turkmen Language -> language.human_language.region -> Asia\n# Answer:\nTurkmen Language", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nArabic Language", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6fdrh\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Afshar language -> common.topic.notable_types -> Human Language\n# Answer:\nAfshar language"], "ground_truth": ["Mazanderani Language", "Arabic Language", "Balochi language", "Kurdish languages", "Armenian Language", "Azerbaijani language", "Afshar language", "Gilaki Language", "Qashqa'i Language", "Talysh language", "Luri language", "Pashto language", "Khalaj, Turkic Language", "Persian Language", "Khorasani Turkish Language", "Turkmen Language", "Assyrian Neo-Aramaic Language", "Kumzari Language"], "ans_acc": 0.2222222222222222, "ans_hit": 1, "ans_f1": 0.3478260869565218, "ans_precission": 0.8, "ans_recall": 0.2222222222222222, "path_f1": 0.3478260869565218, "path_precision": 0.8, "path_recall": 0.2222222222222222, "path_ans_f1": 0.3478260869565218, "path_ans_precision": 0.8, "path_ans_recall": 0.2222222222222222}
{"id": "WebQTest-689", "prediction": ["# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.region -> Europe\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.region -> Europe\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Basque Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nBasque Language", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> Andorra\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> Gibraltar\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc38hhk\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Andorra\n# Answer:\nSpanish Language"], "ground_truth": ["Spanish Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.location.containedby -> Tennessee\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.location.containedby -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County -> location.location.containedby -> Tennessee\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.location.containedby -> Nashville-Davidson--Murfreesboro--Franklin, TN Metropolitan Statistical Area\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.statistical_region.co2_emissions_total -> m.045hnmm\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.statistical_region.population -> g.11b674pwdr\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.country -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11b66dwnl4\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County -> location.location.containedby -> Nashville-Davidson--Murfreesboro--Franklin, TN Metropolitan Statistical Area\n# Answer:\nWilliamson County"], "ground_truth": ["Williamson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-690", "prediction": ["# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Piano -> music.instrument.family -> Struck string instruments\n# Answer:\nPiano", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Piano -> interests.collectable_item.in_collections -> m.0104pjzp\n# Answer:\nPiano", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Piano -> education.field_of_study.students_majoring -> m.02wmsgv\n# Answer:\nPiano", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Jazz piano -> music.performance_role.regular_performances -> m.0z84xb_\n# Answer:\nJazz piano", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Jazz piano -> common.topic.article -> m.06ch5b\n# Answer:\nJazz piano", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Jazz piano -> common.topic.webpage -> m.09xz26q\n# Answer:\nJazz piano", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Piano -> education.field_of_study.students_majoring -> m.04hm3p5\n# Answer:\nPiano", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Jazz piano -> music.performance_role.regular_performances -> m.0z84xd1\n# Answer:\nJazz piano", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Piano -> education.field_of_study.students_majoring -> m.04hwb7q\n# Answer:\nPiano", "# Reasoning Path:\nScott Joplin -> book.author.works_written -> Treemonisha -> music.album.artist -> The Paragon Ragtime Orchestra\n# Answer:\nTreemonisha"], "ground_truth": ["Piano", "Jazz piano"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-691", "prediction": ["# Reasoning Path:\nMiley Cyrus -> people.person.education -> m.0h2yk16 -> education.education.institution -> Heritage Elementary School\n# Answer:\nHeritage Elementary School", "# Reasoning Path:\nMiley Cyrus -> people.person.education -> m.0h2yk16 -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nMiley Cyrus -> music.artist.album -> Hannah Montana -> tv.tv_program.country_of_origin -> United States of America\n# Answer:\nHannah Montana", "# Reasoning Path:\nMiley Cyrus -> people.person.education -> m.0h2yk16 -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nMiley Cyrus -> people.person.education -> m.0h2yk16 -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nMiley Cyrus -> music.featured_artist.recordings -> 23 -> music.composition.language -> English Language\n# Answer:\n23", "# Reasoning Path:\nMiley Cyrus -> music.featured_artist.recordings -> A Day in the Life -> music.recording.artist -> The Flaming Lips\n# Answer:\nA Day in the Life", "# Reasoning Path:\nMiley Cyrus -> music.artist.album -> Hannah Montana -> music.album.genre -> Bubblegum pop\n# Answer:\nHannah Montana", "# Reasoning Path:\nMiley Cyrus -> music.artist.album -> 23 -> music.composition.composer -> Juicy J\n# Answer:\n23", "# Reasoning Path:\nMiley Cyrus -> music.artist.album -> Hannah Montana -> common.topic.notable_types -> Award-Winning Work\n# Answer:\nHannah Montana"], "ground_truth": ["Heritage Elementary School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-693", "prediction": ["# Reasoning Path:\nAfghanistan -> location.country.official_language -> Farsi, Eastern Language -> common.topic.image -> Dari lang\n# Answer:\nFarsi, Eastern Language", "# Reasoning Path:\nAfghanistan -> location.country.official_language -> Farsi, Eastern Language -> base.rosetta.languoid.local_name -> Farsi, Eastern\n# Answer:\nFarsi, Eastern Language", "# Reasoning Path:\nAfghanistan -> location.country.official_language -> Farsi, Eastern Language -> base.rosetta.languoid.parent -> Persian Group\n# Answer:\nFarsi, Eastern Language", "# Reasoning Path:\nAfghanistan -> location.country.languages_spoken -> Farsi, Eastern Language -> common.topic.image -> Dari lang\n# Answer:\nFarsi, Eastern Language", "# Reasoning Path:\nAfghanistan -> location.statistical_region.cpi_inflation_rate -> g.11b60q43bt\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nAfghanistan -> location.country.official_language -> Pashto language -> language.human_language.countries_spoken_in -> Pakistan\n# Answer:\nPashto language", "# Reasoning Path:\nAfghanistan -> location.country.languages_spoken -> Farsi, Eastern Language -> base.rosetta.languoid.local_name -> Farsi, Eastern\n# Answer:\nFarsi, Eastern Language", "# Reasoning Path:\nAfghanistan -> location.country.languages_spoken -> Farsi, Eastern Language -> base.rosetta.languoid.parent -> Persian Group\n# Answer:\nFarsi, Eastern Language", "# Reasoning Path:\nAfghanistan -> location.statistical_region.cpi_inflation_rate -> g.12tb6gn6w\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nAfghanistan -> location.country.official_language -> Pashto language -> common.topic.notable_types -> Human Language\n# Answer:\nPashto language"], "ground_truth": ["Farsi, Eastern Language", "Pashto language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-694", "prediction": ["# Reasoning Path:\nPrime Minister of Jamaica -> government.government_office_or_title.office_holders -> m.0hqg83f -> government.government_position_held.office_holder -> Portia Simpson-Miller\n# Answer:\nPortia Simpson-Miller", "# Reasoning Path:\nPrime Minister of Jamaica -> common.topic.article -> m.01sm6d\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nPrime Minister of Jamaica -> government.government_office_or_title.office_holders -> m.0hqg83f -> government.government_position_held.jurisdiction_of_office -> Jamaica\n# Answer:\nJamaica", "# Reasoning Path:\nPrime Minister of Jamaica -> government.government_office_or_title.office_holders -> m.010fp6mv -> government.government_position_held.office_holder -> Alexander Bustamante\n# Answer:\nAlexander Bustamante", "# Reasoning Path:\nPrime Minister of Jamaica -> government.government_office_or_title.office_holders -> m.0hqg83f -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nPrime Minister of Jamaica -> government.government_office_or_title.office_holders -> m.010fp7qb -> government.government_position_held.office_holder -> Donald Sangster\n# Answer:\nDonald Sangster", "# Reasoning Path:\nPrime Minister of Jamaica -> common.topic.image -> Coat of Arms of Jamaica -> common.image.appears_in_topic_gallery -> Coat of arms of Jamaica\n# Answer:\nCoat of Arms of Jamaica", "# Reasoning Path:\nPrime Minister of Jamaica -> government.government_office_or_title.office_holders -> m.010fp6mv -> government.government_position_held.jurisdiction_of_office -> Jamaica\n# Answer:\nJamaica", "# Reasoning Path:\nPrime Minister of Jamaica -> common.topic.image -> Coat of Arms of Jamaica -> common.image.appears_in_topic_gallery -> Governor-General of Jamaica\n# Answer:\nCoat of Arms of Jamaica", "# Reasoning Path:\nPrime Minister of Jamaica -> common.topic.image -> Coat of Arms of Jamaica -> common.image.size -> m.02bfsxv\n# Answer:\nCoat of Arms of Jamaica"], "ground_truth": ["Portia Simpson-Miller"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-695", "prediction": ["# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> m.02hrk87 -> music.group_membership.role -> Bass\n# Answer:\nBass", "# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> m.02hrk87 -> music.group_membership.role -> Lead Vocals\n# Answer:\nLead Vocals", "# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> m.02hrk87 -> music.group_membership.role -> Backing vocalist\n# Answer:\nBacking vocalist", "# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> g.11b7_rbdpp\n# Answer:\nmusic.group_member.membership", "# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> m.02hrk87 -> music.group_membership.group -> The Beatles\n# Answer:\nThe Beatles", "# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> g.11b7_s0vtk\n# Answer:\nmusic.group_member.membership", "# Reasoning Path:\nPaul McCartney -> base.activism.activist.area_of_activism -> Animal rights\n# Answer:\nAnimal rights", "# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> m.02hrk87 -> freebase.valuenotation.is_reviewed -> Member\n# Answer:\nMember", "# Reasoning Path:\nPaul McCartney -> award.award_nominee.award_nominations -> m.01062q3w -> freebase.valuenotation.is_reviewed -> Ceremony\n# Answer:\nCeremony", "# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> m.02hrk87 -> freebase.valuenotation.is_reviewed -> Group\n# Answer:\nGroup"], "ground_truth": ["Backing vocalist", "Bass", "Lead Vocals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-696", "prediction": ["# Reasoning Path:\nJuventus F.C. -> sports.sports_team.championships -> 2014 Supercoppa Italiana -> time.event.instance_of_recurring_event -> Supercoppa Italiana\n# Answer:\n2014 Supercoppa Italiana", "# Reasoning Path:\nJuventus F.C. -> sports.sports_team.championships -> 2014 Supercoppa Italiana -> time.event.next_in_series -> g.11btrbw12v\n# Answer:\n2014 Supercoppa Italiana", "# Reasoning Path:\nJuventus F.C. -> soccer.football_team.matches -> 2014 A. S. Roma vs. Juventus F.C -> soccer.football_match.teams -> A.S. Roma\n# Answer:\n2014 A. S. Roma vs. Juventus F.C", "# Reasoning Path:\nJuventus F.C. -> sports.sports_team.championships -> 1977 UEFA Cup Final -> time.event.locations -> Stadio Olimpico di Torino\n# Answer:\n1977 UEFA Cup Final", "# Reasoning Path:\nJuventus F.C. -> sports.sports_team.championships -> 1977 UEFA Cup Final -> common.topic.notable_types -> Football Match\n# Answer:\n1977 UEFA Cup Final", "# Reasoning Path:\nJuventus F.C. -> sports.sports_team.championships -> 1984 European Cup Winners' Cup Final -> soccer.football_match.held_at -> St. Jakob-Park\n# Answer:\n1984 European Cup Winners' Cup Final", "# Reasoning Path:\nJuventus F.C. -> soccer.football_team.matches -> 2014 A. S. Roma vs. Juventus F.C -> common.topic.notable_for -> g.1s05cp90q\n# Answer:\n2014 A. S. Roma vs. Juventus F.C", "# Reasoning Path:\nJuventus F.C. -> soccer.football_team.matches -> 2014 Juventus vs. Benfica -> soccer.football_match.teams -> S.L. Benfica\n# Answer:\n2014 Juventus vs. Benfica", "# Reasoning Path:\nJuventus F.C. -> sports.sports_team.championships -> 1984 European Cup Winners' Cup Final -> sports.sports_championship_event.championship -> UEFA Cup Winners' Cup\n# Answer:\n1984 European Cup Winners' Cup Final", "# Reasoning Path:\nJuventus F.C. -> soccer.football_team.matches -> 2014 A. S. Roma vs. Juventus F.C -> soccer.football_match.held_at -> Stadio Olimpico\n# Answer:\n2014 A. S. Roma vs. Juventus F.C"], "ground_truth": ["1996 UEFA Champions League Final"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-698", "prediction": ["# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lawrence E. Roberts -> people.person.children -> Lawrence E. Roberts, Jr.\n# Answer:\nLawrence E. Roberts", "# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lawrence E. Roberts -> common.topic.notable_types -> Military Person\n# Answer:\nLawrence E. Roberts", "# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lawrence E. Roberts -> people.person.children -> Sally-Ann Roberts\n# Answer:\nLawrence E. Roberts", "# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lawrence E. Roberts -> freebase.valuenotation.has_value -> Parents\n# Answer:\nLawrence E. Roberts", "# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lawrence E. Roberts -> people.person.children -> Dorothy Roberts McEwen\n# Answer:\nLawrence E. Roberts", "# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lucimarian Tolliver -> common.topic.notable_for -> g.125fjdxy2\n# Answer:\nLucimarian Tolliver", "# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lucimarian Tolliver -> people.person.children -> Sally-Ann Roberts\n# Answer:\nLucimarian Tolliver", "# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lucimarian Tolliver -> freebase.valuenotation.has_value -> Parents\n# Answer:\nLucimarian Tolliver", "# Reasoning Path:\nRobin Roberts -> common.topic.image -> Robin Roberts at Heart Truth 2010 cropped -> common.image.size -> m.0gzz6wy\n# Answer:\nRobin Roberts at Heart Truth 2010 cropped", "# Reasoning Path:\nRobin Roberts -> award.award_nominee.award_nominations -> m.010g1g1v -> freebase.valuenotation.is_reviewed -> Award\n# Answer:\nAward"], "ground_truth": ["Lawrence E. Roberts"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-699", "prediction": ["# Reasoning Path:\nVictoria -> location.administrative_division.capital -> m.0jvvh95 -> location.administrative_division_capital_relationship.capital -> Melbourne\n# Answer:\nMelbourne", "# Reasoning Path:\nVictoria -> base.aareas.schema.administrative_area.capital -> Melbourne -> travel.travel_destination.tourist_attractions -> Yarra Valley\n# Answer:\nMelbourne", "# Reasoning Path:\nVictoria -> base.aareas.schema.administrative_area.administrative_children -> City of Ballarat -> location.location.containedby -> Australia\n# Answer:\nCity of Ballarat", "# Reasoning Path:\nVictoria -> base.aareas.schema.administrative_area.capital -> Melbourne -> common.topic.image -> Melbourne Infobox Montage\n# Answer:\nMelbourne", "# Reasoning Path:\nVictoria -> base.aareas.schema.administrative_area.capital -> Melbourne -> travel.travel_destination.tourist_attractions -> Melbourne Museum\n# Answer:\nMelbourne", "# Reasoning Path:\nVictoria -> base.aareas.schema.administrative_area.administrative_children -> Shire of Colac Otway -> location.location.containedby -> Australia\n# Answer:\nShire of Colac Otway", "# Reasoning Path:\nVictoria -> base.aareas.schema.administrative_area.capital -> Melbourne -> location.statistical_region.population -> g.11bcdlkx9x\n# Answer:\nMelbourne", "# Reasoning Path:\nVictoria -> base.aareas.schema.administrative_area.administrative_children -> City of Ballarat -> common.topic.notable_for -> g.125fxg37y\n# Answer:\nCity of Ballarat", "# Reasoning Path:\nVictoria -> base.aareas.schema.administrative_area.capital -> Melbourne -> common.topic.image -> Melbourneskyline\n# Answer:\nMelbourne", "# Reasoning Path:\nVictoria -> base.aareas.schema.administrative_area.capital -> Melbourne -> travel.travel_destination.tourist_attractions -> Abbotsford Convent\n# Answer:\nMelbourne"], "ground_truth": ["Melbourne"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> United States of America\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.geolocation -> m.0kdmvf\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Jasper County\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Missouri\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11b66mljn1\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_for -> g.1255959wd\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11x1chmhk\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> astronomy.extraterrestrial_location.on_celestial_object -> Moon\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> USS George Washington Carver (SSBN-656) -> common.topic.notable_types -> Ship\n# Answer:\nUSS George Washington Carver (SSBN-656)", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11x1ct3wq\n# Answer:\nDiamond"], "ground_truth": ["Diamond"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-700", "prediction": ["# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Rwanda -> location.country.official_language -> English Language\n# Answer:\nRwanda", "# Reasoning Path:\nFrench -> language.human_language.main_country -> France -> location.country.first_level_divisions -> Martinique\n# Answer:\nFrance", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Rwanda -> common.topic.notable_types -> Country\n# Answer:\nRwanda", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Rwanda -> location.country.form_of_government -> Parliamentary system\n# Answer:\nRwanda", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Gabon -> common.topic.notable_types -> Country\n# Answer:\nGabon", "# Reasoning Path:\nFrench -> language.human_language.main_country -> France -> organization.organization_founder.organizations_founded -> World Trade Organization\n# Answer:\nFrance", "# Reasoning Path:\nFrench -> language.human_language.countries_spoken_in -> Rwanda -> location.country.official_language -> English Language\n# Answer:\nRwanda", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Gabon -> location.location.time_zones -> West Africa Time Zone\n# Answer:\nGabon", "# Reasoning Path:\nFrench -> language.human_language.main_country -> France -> base.locations.countries.planet -> Earth\n# Answer:\nFrance", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Rwanda -> location.country.form_of_government -> Presidential system\n# Answer:\nRwanda"], "ground_truth": ["Seychelles", "Jersey", "Comoros", "Lebanon", "Djibouti", "Vatican City", "Monaco", "Morocco", "Central African Republic", "France", "Haiti", "Mali", "Mauritius", "Guinea", "Equatorial Guinea", "Niger", "Martinique", "Burkina Faso", "Holy Roman Empire", "Benin", "Dominica", "Switzerland", "Qatar", "Ivory Coast", "Congo", "Laos", "Cambodia", "Senegal", "Luxembourg", "Tunisia", "Vanuatu", "Habsburg Netherlands", "Rwanda", "Gabon", "Barbados", "Chad", "Madagascar", "Cameroon", "South Vietnam", "Algeria", "Burundi", "Cyprus", "Belgium"], "ans_acc": 0.09302325581395349, "ans_hit": 1, "ans_f1": 0.13043478260869565, "ans_precission": 1.0, "ans_recall": 0.06976744186046512, "path_f1": 0.03773584905660377, "path_precision": 0.1, "path_recall": 0.023255813953488372, "path_ans_f1": 0.1702127659574468, "path_ans_precision": 1.0, "path_ans_recall": 0.09302325581395349}
{"id": "WebQTest-701", "prediction": ["# Reasoning Path:\nReggie Bush -> american_football.football_player.rushing -> m.09tctbs -> american_football.player_rushing_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints", "# Reasoning Path:\nReggie Bush -> american_football.football_player.rushing -> m.09grpy_ -> american_football.player_rushing_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints", "# Reasoning Path:\nReggie Bush -> american_football.football_player.rushing -> m.07z2cq4 -> american_football.player_rushing_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints", "# Reasoning Path:\nReggie Bush -> american_football.football_player.rushing -> m.09tctbs -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nReggie Bush -> common.topic.image -> reggiebush12.jpg -> common.image.size -> m.029rqjc\n# Answer:\nreggiebush12.jpg", "# Reasoning Path:\nReggie Bush -> american_football.football_player.rushing -> m.09grpy_ -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nReggie Bush -> american_football.football_player.rushing -> m.07z2cq4 -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nReggie Bush -> american_football.football_player.games -> m.07nvq3h -> american_football.player_game_statistics.season -> 2008 NFL season\n# Answer:\n2008 NFL season", "# Reasoning Path:\nReggie Bush -> common.topic.image -> Reggiebush2 -> common.image.size -> m.0kk6_4\n# Answer:\nReggiebush2", "# Reasoning Path:\nReggie Bush -> american_football.football_player.games -> m.07nvq3h -> american_football.player_game_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints"], "ground_truth": ["New Orleans Saints", "Miami Dolphins"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.4, "ans_recall": 0.5, "path_f1": 0.11940298507462686, "path_precision": 0.4, "path_recall": 0.07017543859649122, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.4, "path_ans_recall": 0.5}
{"id": "WebQTest-702", "prediction": ["# Reasoning Path:\nEmily Osment -> people.person.education -> m.0n1fn0k -> education.education.institution -> Flintridge Preparatory School\n# Answer:\nFlintridge Preparatory School", "# Reasoning Path:\nEmily Osment -> people.person.education -> g.11b62v8w8j\n# Answer:\npeople.person.education", "# Reasoning Path:\nEmily Osment -> people.person.place_of_birth -> Los Angeles -> fictional_universe.fictional_setting.characters_that_have_lived_here -> Winifred Burkle\n# Answer:\nLos Angeles", "# Reasoning Path:\nEmily Osment -> people.person.place_of_birth -> Los Angeles -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nLos Angeles", "# Reasoning Path:\nEmily Osment -> people.person.place_of_birth -> Los Angeles -> fictional_universe.fictional_setting.characters_that_have_lived_here -> Linwood Murrow\n# Answer:\nLos Angeles", "# Reasoning Path:\nEmily Osment -> award.award_winner.awards_won -> m.0t5t4lr -> award.award_honor.award -> Canadian Screen Award for Best Performance by an Actress in a Leading Role in a Dramatic Program or Mini-Series\n# Answer:\nCanadian Screen Award for Best Performance by an Actress in a Leading Role in a Dramatic Program or Mini-Series", "# Reasoning Path:\nEmily Osment -> award.award_winner.awards_won -> m.0z915m5 -> award.award_honor.ceremony -> 2009 Teen Choice Awards\n# Answer:\n2009 Teen Choice Awards", "# Reasoning Path:\nEmily Osment -> award.award_winner.awards_won -> m.0t5t4lr -> award.award_honor.honored_for -> Cyberbully\n# Answer:\nCyberbully", "# Reasoning Path:\nEmily Osment -> award.award_winner.awards_won -> m.0z915m5 -> award.award_honor.award -> Teen Choice Award for Choice TV: Sidekick\n# Answer:\nTeen Choice Award for Choice TV: Sidekick"], "ground_truth": ["Flintridge Preparatory School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-703", "prediction": ["# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_0s31\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.11b60v4rn6\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_38p7\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_569j\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.12tb6hbx9\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.1hhc378k5\n# Answer:\nlocation.statistical_region.cpi_inflation_rate"], "ground_truth": ["Mexican peso"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-704", "prediction": ["# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Scientist -> common.topic.subject_of -> Book\n# Answer:\nScientist", "# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Scientist -> tv.tv_subject.tv_programs -> The Secret Life of Scientists & Engineers\n# Answer:\nScientist", "# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Scientist -> common.topic.subject_of -> Autobiography\n# Answer:\nScientist", "# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Scientist -> base.descriptive_names.names.descriptive_name -> m.01053kvm\n# Answer:\nScientist", "# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Film director -> common.topic.subject_of -> Eugenio Polgovsky\n# Answer:\nFilm director", "# Reasoning Path:\nThor Heyerdahl -> freebase.valuenotation.is_reviewed -> Profession -> rdf-schema#domain -> Person\n# Answer:\nProfession", "# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Scientist -> common.topic.subject_of -> Biography\n# Answer:\nScientist", "# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Scientist -> base.descriptive_names.names.descriptive_name -> m.01053kyr\n# Answer:\nScientist", "# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Film director -> common.topic.subject_of -> Lew M. Parry\n# Answer:\nFilm director", "# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Film director -> fictional_universe.character_occupation.characters_with_this_occupation -> Chad Dylan Cooper\n# Answer:\nFilm director"], "ground_truth": ["Film director", "Adventurer", "Cinematographer", "Ethnographer", "Film Producer", "Scientist"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.48648648648648646, "ans_precission": 0.9, "ans_recall": 0.3333333333333333, "path_f1": 0.48648648648648646, "path_precision": 0.9, "path_recall": 0.3333333333333333, "path_ans_f1": 0.48648648648648646, "path_ans_precision": 0.9, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-706", "prediction": ["# Reasoning Path:\nTroy Aikman -> sports.pro_athlete.teams -> m.0hpxsvm -> sports.sports_team_roster.team -> Dallas Cowboys\n# Answer:\nDallas Cowboys", "# Reasoning Path:\nTroy Aikman -> sports.pro_athlete.teams -> m.0hpxsvm -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nTroy Aikman -> award.award_nominee.award_nominations -> m.0zgx48n -> freebase.valuenotation.is_reviewed -> Year\n# Answer:\nYear", "# Reasoning Path:\nTroy Aikman -> award.award_nominee.award_nominations -> m.0zgx48n -> award.award_nomination.award -> Sports Emmy Award for Outstanding Sports Personality - Sports Event Analyst\n# Answer:\nSports Emmy Award for Outstanding Sports Personality - Sports Event Analyst", "# Reasoning Path:\nTroy Aikman -> award.award_nominee.award_nominations -> m.0zgx48n -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work", "# Reasoning Path:\nTroy Aikman -> award.award_nominee.award_nominations -> m.0zgx48n -> freebase.valuenotation.is_reviewed -> Award\n# Answer:\nAward", "# Reasoning Path:\nTroy Aikman -> award.award_nominee.award_nominations -> m.0zgx48n -> freebase.valuenotation.is_reviewed -> Award Nominee\n# Answer:\nAward Nominee", "# Reasoning Path:\nTroy Aikman -> award.award_nominee.award_nominations -> m.0zcps9m -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work", "# Reasoning Path:\nTroy Aikman -> award.award_nominee.award_nominations -> m.0_qzp0y -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work", "# Reasoning Path:\nTroy Aikman -> award.award_nominee.award_nominations -> m.0_qzp0y -> award.award_nomination.award -> Walter Payton NFL Man of the Year Award\n# Answer:\nWalter Payton NFL Man of the Year Award"], "ground_truth": ["Dallas Cowboys"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-707", "prediction": ["# Reasoning Path:\nCletus Hogg -> tv.tv_character.appeared_in_tv_program -> m.03hqvml -> tv.regular_tv_appearance.actor -> Rick Hurst\n# Answer:\nRick Hurst", "# Reasoning Path:\nCletus Hogg -> film.film_character.portrayed_in_films -> m.03hqvvw -> film.performance.actor -> Jack Polick\n# Answer:\nJack Polick", "# Reasoning Path:\nCletus Hogg -> common.topic.article -> m.0bxccc\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nCletus Hogg -> film.film_character.portrayed_in_films -> m.03hqvvw -> film.performance.film -> The Dukes of Hazzard\n# Answer:\nThe Dukes of Hazzard"], "ground_truth": ["Jack Polick"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-708", "prediction": ["# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05l1m7b -> sports.sports_award.award_winner -> Liverpool F.C.\n# Answer:\nLiverpool F.C.", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05l1m7b -> sports.sports_award.season -> 1988\u201389 FA Cup\n# Answer:\n1988\u201389 FA Cup", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.0bfpn_5 -> sports.sports_award.award_winner -> Manchester United F.C.\n# Answer:\nManchester United F.C.", "# Reasoning Path:\nFA Cup -> common.topic.notable_for -> g.125f13scs\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.0bfpn_5 -> sports.sports_award.season -> 1984\u201385 FA Cup\n# Answer:\n1984\u201385 FA Cup", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.010lkdyj -> sports.sports_award.award_winner -> Arsenal F.C.\n# Answer:\nArsenal F.C.", "# Reasoning Path:\nFA Cup -> freebase.valuenotation.has_no_value -> Date of final occurrence -> rdf-schema#range -> Date/Time\n# Answer:\nDate of final occurrence", "# Reasoning Path:\nFA Cup -> freebase.valuenotation.has_no_value -> Date of final occurrence -> type.property.schema -> Recurring event\n# Answer:\nDate of final occurrence", "# Reasoning Path:\nFA Cup -> freebase.valuenotation.has_no_value -> Date of final occurrence -> type.property.expected_type -> Date/Time\n# Answer:\nDate of final occurrence", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.010lkdyj -> sports.sports_award.season -> 2013\u201314 FA Cup\n# Answer:\n2013\u201314 FA Cup"], "ground_truth": ["Southampton F.C."], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-709", "prediction": ["# Reasoning Path:\nRobert E. Lee -> people.deceased_person.cause_of_death -> Pneumonia -> people.cause_of_death.includes_causes_of_death -> Aspiration pneumonia\n# Answer:\nPneumonia", "# Reasoning Path:\nRobert E. Lee -> people.deceased_person.cause_of_death -> Pneumonia -> medicine.disease.includes_diseases -> Aspiration pneumonia\n# Answer:\nPneumonia", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> Robert E. Lee's Civil War -> book.written_work.subjects -> American Civil War\n# Answer:\nRobert E. Lee's Civil War", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> Robert E. Lee's Civil War -> book.book.genre -> Non-fiction\n# Answer:\nRobert E. Lee's Civil War", "# Reasoning Path:\nRobert E. Lee -> military.military_commander.military_commands -> m.048z_8v -> military.military_command.military_conflict -> Battle of Gettysburg\n# Answer:\nBattle of Gettysburg", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> Uncertain Glory: Lee's Generalship Re-Examined -> common.topic.notable_for -> g.125fs0sd6\n# Answer:\nUncertain Glory: Lee's Generalship Re-Examined", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> Uncertain Glory: Lee's Generalship Re-Examined -> book.book.editions -> Uncertain glory : Lee's generalship re-examined\n# Answer:\nUncertain Glory: Lee's Generalship Re-Examined", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> General Lee's Army -> book.written_work.subjects -> Army of Northern Virginia\n# Answer:\nGeneral Lee's Army", "# Reasoning Path:\nRobert E. Lee -> military.military_commander.military_commands -> m.048z_8v -> military.military_command.military_combatant -> Confederate States of America\n# Answer:\nConfederate States of America", "# Reasoning Path:\nRobert E. Lee -> military.military_commander.military_commands -> m.049y372 -> military.military_command.military_conflict -> Battle of Fredericksburg\n# Answer:\nBattle of Fredericksburg"], "ground_truth": ["Pneumonia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Schuyler Frances Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSchuyler Frances Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.spouse -> Tracy Pollan\n# Answer:\nTracy Pollan", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Schuyler Frances Fox -> common.topic.notable_types -> Person\n# Answer:\nSchuyler Frances Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Schuyler Frances Fox -> people.person.sibling_s -> m.0j217k5\n# Answer:\nSchuyler Frances Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.location_of_ceremony -> Arlington\n# Answer:\nArlington", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.profession -> Actor\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> common.topic.notable_types -> Person\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-710", "prediction": ["# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Neo-impressionism -> visual_art.art_period_movement.associated_artists -> Th\u00e9o van Rysselberghe\n# Answer:\nNeo-impressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Neo-impressionism -> visual_art.art_period_movement.associated_artists -> Albert Dubois-Pillet\n# Answer:\nNeo-impressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Neo-impressionism -> visual_art.art_period_movement.associated_artworks -> Sunday\n# Answer:\nNeo-impressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Fauvism -> visual_art.art_period_movement.associated_artworks -> Bathers by a River\n# Answer:\nFauvism", "# Reasoning Path:\nHenri Matisse -> book.book_subject.works -> Matisse -> common.topic.notable_for -> g.125dhx5l1\n# Answer:\nMatisse", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Neo-impressionism -> visual_art.art_period_movement.associated_artists -> Alfred William Finch\n# Answer:\nNeo-impressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Neo-impressionism -> common.topic.image -> Georges Seurat - Un dimanche apr\u00c3\u00a8s-midi \u00c3\u00a0 l'\u00c3\u008ele de la Grande Jatte v2\n# Answer:\nNeo-impressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Fauvism -> common.topic.notable_for -> g.1257w3_9g\n# Answer:\nFauvism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Eyewitness: Reports from an Art World in Crisis\n# Answer:\nModern art", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nModern art"], "ground_truth": ["Impressionism", "Neo-impressionism", "Modernism", "Modern art", "Fauvism"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.8470588235294118, "ans_precission": 0.9, "ans_recall": 0.8, "path_f1": 0.7200000000000001, "path_precision": 0.9, "path_recall": 0.6, "path_ans_f1": 0.8470588235294118, "path_ans_precision": 0.9, "path_ans_recall": 0.8}
{"id": "WebQTest-711", "prediction": ["# Reasoning Path:\nGossip Girl -> tv.tv_character.appeared_in_tv_program -> m.0dk0lbd -> tv.regular_tv_appearance.actor -> Kristen Bell\n# Answer:\nKristen Bell", "# Reasoning Path:\nChuck Bass -> tv.tv_character.appeared_in_tv_program -> m.04hby1k -> tv.regular_tv_appearance.actor -> Ed Westwick\n# Answer:\nEd Westwick", "# Reasoning Path:\nGossip Girl -> tv.tv_character.appeared_in_tv_program -> m.0dk0lbd -> tv.regular_tv_appearance.seasons -> Gossip Girl - Season 6\n# Answer:\nGossip Girl - Season 6", "# Reasoning Path:\nChuck Bass -> tv.tv_character.appeared_in_tv_program -> m.04hby1k -> tv.regular_tv_appearance.seasons -> Gossip Girl - Season 6\n# Answer:\nGossip Girl - Season 6", "# Reasoning Path:\nGossip Girl -> tv.tv_program.regular_cast -> m.010blsdj -> tv.regular_tv_appearance.actor -> Margaret Colin\n# Answer:\nMargaret Colin", "# Reasoning Path:\nGossip Girl -> tv.tv_program.regular_cast -> m.03jqr3k -> tv.regular_tv_appearance.actor -> Leighton Meester\n# Answer:\nLeighton Meester", "# Reasoning Path:\nChuck Bass -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Multiple myeloma\n# Answer:\nMale", "# Reasoning Path:\nGossip Girl -> tv.tv_program.regular_cast -> m.03jtc3r -> tv.regular_tv_appearance.character -> Dan Humphrey\n# Answer:\nDan Humphrey", "# Reasoning Path:\nGossip Girl -> tv.tv_character.appeared_in_tv_program -> m.0dk0lbd -> tv.regular_tv_appearance.seasons -> Gossip Girl - Season 1\n# Answer:\nGossip Girl - Season 1", "# Reasoning Path:\nChuck Bass -> tv.tv_character.appeared_in_tv_program -> m.04hby1k -> tv.regular_tv_appearance.seasons -> Gossip Girl - Season 5\n# Answer:\nGossip Girl - Season 5"], "ground_truth": ["Ed Westwick"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-712", "prediction": ["# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.location.contains -> Phoenix Islands\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.location.containedby -> Pacific Ocean\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.location.contains -> Tarawa\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.country.languages_spoken -> English Language\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Australia -> location.country.currency_used -> Australian dollar\n# Answer:\nAustralia", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.location.contains -> Line Islands\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.containedby -> DVD Region 4 -> location.location.contains -> South America\n# Answer:\nDVD Region 4", "# Reasoning Path:\nOceania -> location.location.contains -> Australia -> sports.sport_country.multi_event_tournaments_participated_in -> 2010 Commonwealth Games\n# Answer:\nAustralia", "# Reasoning Path:\nOceania -> location.location.contains -> Australia -> olympics.olympic_participating_country.olympics_participated_in -> 2000 Summer Olympics\n# Answer:\nAustralia", "# Reasoning Path:\nOceania -> location.location.containedby -> Pacific Ocean -> location.location.contains -> Kiribati\n# Answer:\nPacific Ocean"], "ground_truth": ["Niue", "French Polynesia", "Norfolk Island", "Marshall Islands", "Palau", "Guam", "Pitcairn Islands", "Australia", "Solomon Islands", "Nauru", "Northern Mariana Islands", "Vanuatu", "Timor-Leste", "Tonga", "Cook Islands", "Federated States of Micronesia", "New Caledonia", "New Zealand", "Tokelau", "Indonesia", "Wallis and Futuna", "Samoa", "Kiribati", "Tuvalu", "Papua New Guinea", "Fiji", "American Samoa"], "ans_acc": 0.07407407407407407, "ans_hit": 1, "ans_f1": 0.13559322033898305, "ans_precission": 0.8, "ans_recall": 0.07407407407407407, "path_f1": 0.13559322033898305, "path_precision": 0.8, "path_recall": 0.07407407407407407, "path_ans_f1": 0.13688212927756654, "path_ans_precision": 0.9, "path_ans_recall": 0.07407407407407407}
{"id": "WebQTest-713", "prediction": ["# Reasoning Path:\nSt. Louis Park -> location.location.containedby -> Hennepin County -> location.location.containedby -> Minnesota\n# Answer:\nHennepin County", "# Reasoning Path:\nSt. Louis Park -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nSt. Louis Park -> location.hud_county_place.county -> Hennepin County -> location.location.containedby -> Minnesota\n# Answer:\nHennepin County", "# Reasoning Path:\nSt. Louis Park -> location.location.containedby -> Hennepin County -> location.location.containedby -> Minneapolis-St. Paul-Bloomington, MN-WI Metropolitan Statistical Area\n# Answer:\nHennepin County", "# Reasoning Path:\nSt. Louis Park -> location.location.containedby -> Hennepin County -> location.statistical_region.population -> g.11b674hc3d\n# Answer:\nHennepin County", "# Reasoning Path:\nSt. Louis Park -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Minnesota\n# Answer:\nUnited States of America", "# Reasoning Path:\nSt. Louis Park -> location.statistical_region.population -> g.11b66k1kc0\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSt. Louis Park -> location.location.containedby -> Hennepin County -> location.us_county.hud_county_place -> Woodland\n# Answer:\nHennepin County", "# Reasoning Path:\nSt. Louis Park -> location.location.containedby -> Minneapolis\u2013Saint Paul -> location.location.containedby -> Washington County\n# Answer:\nMinneapolis\u2013Saint Paul", "# Reasoning Path:\nSt. Louis Park -> location.hud_county_place.county -> Hennepin County -> location.location.containedby -> Minneapolis-St. Paul-Bloomington, MN-WI Metropolitan Statistical Area\n# Answer:\nHennepin County"], "ground_truth": ["Hennepin County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-714", "prediction": ["# Reasoning Path:\nSouth Africa -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nSouth Africa -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Botswana\n# Answer:\nParliamentary republic", "# Reasoning Path:\nSouth Africa -> location.statistical_region.electricity_consumption_per_capita -> g.1245_4m5f\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nSouth Africa -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nSouth Africa -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f730\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nSouth Africa -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Greece\n# Answer:\nConstitutional republic", "# Reasoning Path:\nSouth Africa -> location.statistical_region.electricity_consumption_per_capita -> g.1245_f78y\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nSouth Africa -> location.statistical_region.electricity_consumption_per_capita -> g.1245_f78z\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nSouth Africa -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6fdy5\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nSouth Africa -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6ft1s\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment"], "ground_truth": ["Constitutional republic", "Parliamentary republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-716", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.language_family -> West Germanic languages -> common.topic.article -> m.0140r1\n# Answer:\nWest Germanic languages", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> Germanic languages -> language.language_family.geographic_distribution -> Northern Europe\n# Answer:\nGermanic languages", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> West Germanic languages -> common.topic.image -> m.029ftls\n# Answer:\nWest Germanic languages", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> West Germanic languages -> common.topic.notable_for -> g.125b7l2y7\n# Answer:\nWest Germanic languages", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> West Germanic languages -> common.topic.image -> Europe germanic languages\n# Answer:\nWest Germanic languages", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> West Germanic languages -> common.topic.image -> Europe germanic-languages 2\n# Answer:\nWest Germanic languages", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0102xvg7 -> education.education.student -> Sigurd Burckhardt\n# Answer:\nSigurd Burckhardt", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> Indo-European languages -> language.language_family.sub_families -> Germanic languages\n# Answer:\nIndo-European languages", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> Germanic languages -> base.schemastaging.context_name.pronunciation -> g.125_qp31h\n# Answer:\nGermanic languages", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> Indo-European languages -> common.topic.article -> m.03v0m\n# Answer:\nIndo-European languages"], "ground_truth": ["Germanic languages", "Indo-European languages", "West Germanic languages"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-717", "prediction": ["# Reasoning Path:\nTucson -> location.location.time_zones -> Mountain Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nTucson -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nTucson -> location.location.nearby_airports -> Tucson International Airport -> location.location.containedby -> United States of America\n# Answer:\nTucson International Airport", "# Reasoning Path:\nTucson -> location.location.time_zones -> Mountain Time Zone -> common.topic.notable_for -> g.125brs1ct\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nTucson -> location.location.nearby_airports -> Tucson International Airport -> aviation.airport.serves -> Miraval Resort\n# Answer:\nTucson International Airport", "# Reasoning Path:\nTucson -> location.location.nearby_airports -> Tucson International Airport -> aviation.airport.serves -> JW Marriott Tucson Starr Pass Resort & Spa\n# Answer:\nTucson International Airport", "# Reasoning Path:\nTucson -> location.location.nearby_airports -> Davis\u2013Monthan Air Force Base -> common.topic.notable_for -> g.1257ryx5p\n# Answer:\nDavis\u2013Monthan Air Force Base", "# Reasoning Path:\nTucson -> location.citytown.postal_codes -> 85701 -> common.topic.notable_types -> Postal Code\n# Answer:\n85701", "# Reasoning Path:\nTucson -> location.location.nearby_airports -> Tucson International Airport -> aviation.airport.serves -> Pima County\n# Answer:\nTucson International Airport", "# Reasoning Path:\nTucson -> location.location.nearby_airports -> Tucson International Airport -> common.topic.webpage -> m.03l4jzn\n# Answer:\nTucson International Airport"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-718", "prediction": ["# Reasoning Path:\nLucy Lawless -> people.person.place_of_birth -> Mount Albert, New Zealand -> location.location.containedby -> New Zealand\n# Answer:\nMount Albert, New Zealand", "# Reasoning Path:\nLucy Lawless -> people.person.place_of_birth -> Mount Albert, New Zealand -> location.location.containedby -> Auckland\n# Answer:\nMount Albert, New Zealand", "# Reasoning Path:\nLucy Lawless -> people.person.place_of_birth -> Mount Albert, New Zealand -> common.topic.article -> m.04jnl_\n# Answer:\nMount Albert, New Zealand", "# Reasoning Path:\nLucy Lawless -> people.person.place_of_birth -> Mount Albert, New Zealand -> common.topic.notable_types -> Location\n# Answer:\nMount Albert, New Zealand", "# Reasoning Path:\nLucy Lawless -> freebase.valuenotation.has_value -> Siblings -> rdf-schema#domain -> Person\n# Answer:\nSiblings", "# Reasoning Path:\nLucy Lawless -> freebase.valuenotation.has_value -> Siblings -> type.property.schema -> Person\n# Answer:\nSiblings", "# Reasoning Path:\nLucy Lawless -> freebase.valuenotation.has_value -> Siblings -> type.property.master_property -> Sibling\n# Answer:\nSiblings", "# Reasoning Path:\nLucy Lawless -> tv.tv_actor.starring_roles -> m.02ht1fx -> tv.regular_tv_appearance.seasons -> Xena: Warrior Princess - Season 5\n# Answer:\nXena: Warrior Princess - Season 5", "# Reasoning Path:\nLucy Lawless -> tv.tv_actor.starring_roles -> m.02ht1fx -> tv.regular_tv_appearance.series -> Xena: Warrior Princess\n# Answer:\nXena: Warrior Princess", "# Reasoning Path:\nLucy Lawless -> tv.tv_actor.starring_roles -> m.02t8f60 -> tv.regular_tv_appearance.series -> Tarzan\n# Answer:\nTarzan"], "ground_truth": ["1968-03-29"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-719", "prediction": ["# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.party -> Democratic Party\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> government.politician.party -> m.03gjhww -> government.political_party_tenure.party -> Democratic Party\n# Answer:\nDemocratic Party", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.venues -> Pepsi Center\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.speeches -> m.04j00hr\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> government.politician.party -> m.03gjhww -> freebase.valuenotation.is_reviewed -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.venues -> Sports Authority Field at Mile High\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.speeches -> m.04j00j0\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.subjects -> 2012 Presidential debate\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Bound Man -> book.written_work.subjects -> United States of America\n# Answer:\nA Bound Man", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.speeches -> m.04j00j9\n# Answer:\n2008 Democratic National Convention"], "ground_truth": ["Democratic Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.2, "path_recall": 0.3333333333333333, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04yvq68 -> military.military_command.military_conflict -> How Few Remain\n# Answer:\nHow Few Remain", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9q6 -> military.military_command.military_conflict -> Battle of McDowell\n# Answer:\nBattle of McDowell", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9nd -> military.military_command.military_conflict -> Battle of Port Republic\n# Answer:\nBattle of Port Republic", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Foot cavalry\n# Answer:\nFoot cavalry", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> time.event.includes_event -> Battle of McDowell\n# Answer:\nJackson's Valley Campaign", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Winfield Scott\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> book.book_subject.musical_compositions_about_this_topic -> the CIVIL warS: a tree is best measured when it is down\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Abraham Lincoln\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> visual_art.art_subject.artwork_on_the_subject -> Battle of Mobile Bay, 5 August 1864\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Antietam -> military.military_conflict.military_personnel_involved -> Albion P. Howe\n# Answer:\nBattle of Antietam"], "ground_truth": ["Battle of Hoke's Run", "Romney Expedition", "First Battle of Kernstown", "Battle of Harpers Ferry", "Second Battle of Bull Run", "Battle of Front Royal", "American Civil War", "Battle of Port Republic", "Battle of McDowell", "Battle of Cedar Mountain", "Battle of Hancock", "Jackson's Valley Campaign", "Battle of Chancellorsville", "Manassas Station Operations", "How Few Remain", "First Battle of Winchester", "Battle of White Oak Swamp", "First Battle of Rappahannock Station", "Battle of Chantilly"], "ans_acc": 0.2631578947368421, "ans_hit": 1, "ans_f1": 0.396039603960396, "ans_precission": 0.8, "ans_recall": 0.2631578947368421, "path_f1": 0.2445414847161572, "path_precision": 0.7, "path_recall": 0.14814814814814814, "path_ans_f1": 0.396039603960396, "path_ans_precision": 0.8, "path_ans_recall": 0.2631578947368421}
{"id": "WebQTest-721", "prediction": ["# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> Leukemia -> people.cause_of_death.parent_cause_of_death -> Cancer\n# Answer:\nLeukemia", "# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> Leukemia -> people.cause_of_death.parent_cause_of_death -> Hibakusha\n# Answer:\nLeukemia", "# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> B-cell chronic lymphocytic leukemia -> medicine.disease.notable_people_with_this_condition -> Clive James\n# Answer:\nB-cell chronic lymphocytic leukemia", "# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> Leukemia -> medicine.disease.treatments -> Vincristine\n# Answer:\nLeukemia", "# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> Leukemia -> medicine.disease.symptoms -> Arthritis\n# Answer:\nLeukemia", "# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> B-cell chronic lymphocytic leukemia -> common.topic.article -> m.02bg7v\n# Answer:\nB-cell chronic lymphocytic leukemia", "# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> B-cell chronic lymphocytic leukemia -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nB-cell chronic lymphocytic leukemia", "# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> Leukemia -> medicine.disease.symptoms -> Bleeding\n# Answer:\nLeukemia", "# Reasoning Path:\nJill Clayburgh -> film.actor.film -> m.01z0gn4 -> film.performance.film -> Starting Over\n# Answer:\nStarting Over", "# Reasoning Path:\nJill Clayburgh -> film.actor.film -> m.02tb0z1 -> film.performance.character -> Barbara Jane Bookman\n# Answer:\nBarbara Jane Bookman"], "ground_truth": ["B-cell chronic lymphocytic leukemia", "Leukemia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-723", "prediction": ["# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Luna Park Sydney -> location.location.time_zones -> Australian Eastern Time Zone\n# Answer:\nLuna Park Sydney", "# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Wild Life Sydney -> location.location.geolocation -> m.0cldn30\n# Answer:\nWild Life Sydney", "# Reasoning Path:\nSydney -> freebase.valuenotation.is_reviewed -> Time zone(s) -> type.property.master_property -> Locations in this time zone\n# Answer:\nTime zone(s)", "# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Luna Park Sydney -> location.location.events -> 2013 Sydney Darts Masters\n# Answer:\nLuna Park Sydney", "# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Sydney Opera House -> location.location.events -> 17th Australian Film Institute Awards\n# Answer:\nSydney Opera House", "# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Sydney Opera House -> projects.project_focus.projects -> Design and Construction of the Sydney Opera House\n# Answer:\nSydney Opera House", "# Reasoning Path:\nSydney -> freebase.valuenotation.is_reviewed -> Time zone(s) -> rdf-schema#range -> Time Zone\n# Answer:\nTime zone(s)", "# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Sydney Opera House -> location.location.containedby -> Australia\n# Answer:\nSydney Opera House", "# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Luna Park Sydney -> location.location.events -> 1979 Sydney Ghost Train fire\n# Answer:\nLuna Park Sydney", "# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Wild Life Sydney -> zoos.zoo.memberships -> Zoo and Aquarium Association Australasia\n# Answer:\nWild Life Sydney"], "ground_truth": ["Kamay Botany Bay National Park", "Rose Seidler House", "Australian National Maritime Museum", "Sydney Harbour Bridge", "Hillsong Church", "City2Surf", "Sydney Markets", "Sydney Opera House", "North Head Quarantine Station", "The Rocks", "St Mary's Cathedral, Sydney", "Bondi Beach", "Port Jackson", "Darling Harbour", "Wild Life Sydney", "Oatley Park, New South Wales", "Sydney Film Festival", "Featherdale Wildlife Park", "Powerhouse Museum", "Luna Park Sydney"], "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0.2526315789473684, "ans_precission": 0.8, "ans_recall": 0.15, "path_f1": 0.2526315789473684, "path_precision": 0.8, "path_recall": 0.15, "path_ans_f1": 0.2526315789473684, "path_ans_precision": 0.8, "path_ans_recall": 0.15}
{"id": "WebQTest-724", "prediction": ["# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.place_of_death -> Brentwood -> location.location.containedby -> Los Angeles\n# Answer:\nBrentwood", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.place_of_death -> Brentwood -> freebase.valuenotation.is_reviewed -> Image\n# Answer:\nBrentwood", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.place_of_death -> Brentwood -> location.location.containedby -> California\n# Answer:\nBrentwood", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.place_of_death -> Brentwood -> organization.organization_scope.organizations_with_this_scope -> Brentwood Public School\n# Answer:\nBrentwood", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.place_of_death -> Brentwood -> location.location.containedby -> Greater Los Angeles Area\n# Answer:\nBrentwood", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.place_of_death -> Brentwood -> organization.organization_scope.organizations_with_this_scope -> Saketini\n# Answer:\nBrentwood", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.07zn09y -> award.award_nomination.ceremony -> 9th British Academy Film Awards\n# Answer:\n9th British Academy Film Awards", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.07zn0h0 -> award.award_nomination.award -> BAFTA Award for Best Foreign Actress\n# Answer:\nBAFTA Award for Best Foreign Actress", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.07zn09y -> award.award_nomination.award -> BAFTA Award for Best Foreign Actress\n# Answer:\nBAFTA Award for Best Foreign Actress", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.0909kmj -> award.award_nomination.award -> Golden Globe Award for Best Actress \u2013 Motion Picture \u2013 Musical or Comedy\n# Answer:\nGolden Globe Award for Best Actress \u2013 Motion Picture \u2013 Musical or Comedy"], "ground_truth": ["Brentwood"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-725", "prediction": ["# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> Nevada -> location.location.containedby -> United States, with Territories\n# Answer:\nNevada", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> Nevada -> location.location.people_born_here -> A. J. Haglund\n# Answer:\nNevada", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> Nevada -> location.location.containedby -> United States of America\n# Answer:\nNevada", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> Nevada -> location.location.partiallycontains -> m.0wg93c0\n# Answer:\nNevada", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> California -> location.location.partially_contains -> West Walker River\n# Answer:\nCalifornia", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> Nevada -> location.location.people_born_here -> Adrian C. Louis\n# Answer:\nNevada", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> California -> location.location.partially_contains -> Amargosa River\n# Answer:\nCalifornia", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> California -> location.location.events -> 1987 Wine Country Film Festival\n# Answer:\nCalifornia", "# Reasoning Path:\nSierra Nevada -> location.location.contains -> Bass Lake -> geography.lake.outflow -> Willow Creek\n# Answer:\nBass Lake", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> Nevada -> location.location.containedby -> Contiguous United States\n# Answer:\nNevada"], "ground_truth": ["Nevada", "California"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-726", "prediction": ["# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> media_common.netflix_genre.titles -> A Girl in Black\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> language.human_language.region -> Europe\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> media_common.netflix_genre.titles -> A Matter of Dignity\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Republic of Macedonia\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.main_country -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> media_common.netflix_genre.titles -> A Touch of Spice\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.region -> Southeast Europe\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Greek Language -> media_common.netflix_genre.titles -> A Girl in Black\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Greek Language -> language.human_language.region -> Europe\n# Answer:\nGreek Language"], "ground_truth": ["Albanian language", "Greek Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-728", "prediction": ["# Reasoning Path:\nSpeaker of the United States House of Representatives -> government.government_office_or_title.office_holders -> m.0g4_6k1 -> government.government_position_held.office_holder -> John Boehner\n# Answer:\nJohn Boehner", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> government.government_office_or_title.office_holders -> m.03hcgxq -> government.government_position_held.office_holder -> Nancy Pelosi\n# Answer:\nNancy Pelosi", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> government.government_office_or_title.office_holders -> m.0g4_6k1 -> government.government_position_held.governmental_body -> United States House of Representatives\n# Answer:\nUnited States House of Representatives", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> government.government_office_or_title.office_holders -> m.0g4_6k1 -> government.government_position_held.basic_title -> Speaker\n# Answer:\nSpeaker", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> government.government_office_or_title.office_holders -> m.03hcgxq -> government.government_position_held.basic_title -> Speaker\n# Answer:\nSpeaker", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> government.government_office_or_title.office_holders -> m.04mmc8c -> government.government_position_held.office_holder -> Theodore Sedgwick\n# Answer:\nTheodore Sedgwick", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> common.topic.image -> Nancy Pelosig -> common.image.appears_in_topic_gallery -> Nancy Pelosi\n# Answer:\nNancy Pelosig", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> base.schemastaging.context_name.plural -> m.0w0qtlx -> base.schemastaging.plural_form.language -> Hebrew\n# Answer:\nHebrew", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> government.government_office_or_title.office_holders -> m.04mmc8c -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> common.topic.image -> Nancy Pelosig -> common.image.size -> m.03rxy3k\n# Answer:\nNancy Pelosig"], "ground_truth": ["Nancy Pelosi", "John Boehner"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-729", "prediction": ["# Reasoning Path:\nStanley Hudson -> tv.tv_character.appeared_in_tv_program -> m.05nphmt -> tv.regular_tv_appearance.actor -> Leslie David Baker\n# Answer:\nLeslie David Baker", "# Reasoning Path:\nStanley Hudson -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Attention deficit hyperactivity disorder\n# Answer:\nMale", "# Reasoning Path:\nStanley Hudson -> tv.tv_character.appeared_in_tv_program -> m.05nphmt -> tv.regular_tv_appearance.series -> The Office\n# Answer:\nThe Office", "# Reasoning Path:\nStanley Hudson -> tv.tv_character.appeared_in_tv_program -> m.05nphmt -> tv.regular_tv_appearance.seasons -> The Office - Season 9\n# Answer:\nThe Office - Season 9", "# Reasoning Path:\nStanley Hudson -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Kidney cancer\n# Answer:\nMale", "# Reasoning Path:\nStanley Hudson -> tv.tv_character.appeared_in_tv_program -> m.05nphmt -> tv.regular_tv_appearance.seasons -> The Office\n# Answer:\nThe Office", "# Reasoning Path:\nStanley Hudson -> tv.tv_character.appeared_in_tv_program -> m.05nphmt -> tv.regular_tv_appearance.seasons -> The Office - Season 7\n# Answer:\nThe Office - Season 7", "# Reasoning Path:\nStanley Hudson -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nStanley Hudson -> common.topic.notable_types -> TV Character -> type.type.properties -> Appeared In TV Episode Segments\n# Answer:\nTV Character", "# Reasoning Path:\nStanley Hudson -> common.topic.notable_types -> TV Character -> type.type.expected_by -> Character\n# Answer:\nTV Character"], "ground_truth": ["Leslie David Baker"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.countries_spoken_in -> Tanzania\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.parent -> Ongamo-Maa Group\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> common.topic.article -> m.064klf\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.countries_spoken_in -> Kenya\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai -> common.image.size -> m.02bgrp_\n# Answer:\nMaasai", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai women and children -> common.image.size -> m.03qy0z9\n# Answer:\nMaasai women and children", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai-jump -> common.image.size -> m.02br_p7\n# Answer:\nMaasai-jump"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-730", "prediction": ["# Reasoning Path:\nKelly Clarkson -> music.artist.label -> RCA Records -> award.award_nominee.award_nominations -> m.0sgj7wg\n# Answer:\nRCA Records", "# Reasoning Path:\nKelly Clarkson -> music.artist.label -> RCA Records -> common.topic.notable_types -> Record label\n# Answer:\nRCA Records", "# Reasoning Path:\nKelly Clarkson -> music.artist.label -> Sony Music Entertainment -> business.business_operation.industry -> Music\n# Answer:\nSony Music Entertainment", "# Reasoning Path:\nKelly Clarkson -> base.saturdaynightlive.snl_musical_guest.musical_performance -> m.04p4_dv -> base.saturdaynightlive.snl_musical_performance.episode -> SNL - 30.12\n# Answer:\nSNL - 30.12", "# Reasoning Path:\nKelly Clarkson -> music.artist.label -> Sony BMG -> business.defunct_company.reason_for_ceasing_operations -> Buyout\n# Answer:\nSony BMG", "# Reasoning Path:\nKelly Clarkson -> music.artist.label -> Sony BMG -> common.topic.article -> m.03mp8v\n# Answer:\nSony BMG", "# Reasoning Path:\nKelly Clarkson -> music.artist.label -> Sony Music Entertainment -> business.business_operation.industry -> Arts, Entertainment, and Recreation\n# Answer:\nSony Music Entertainment", "# Reasoning Path:\nKelly Clarkson -> music.artist.label -> Sony BMG -> business.defunct_company.reason_for_ceasing_operations -> Rebranding\n# Answer:\nSony BMG", "# Reasoning Path:\nKelly Clarkson -> music.artist.label -> Sony BMG -> organization.organization.child -> m.04kjv9q\n# Answer:\nSony BMG", "# Reasoning Path:\nKelly Clarkson -> music.artist.label -> Sony BMG -> organization.organization.child -> m.04m0wdw\n# Answer:\nSony BMG"], "ground_truth": ["19 Recordings", "RCA Records", "Sony Music Entertainment", "Sony BMG"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.8181818181818182, "path_precision": 0.9, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-731", "prediction": ["# Reasoning Path:\nTim Roth -> people.person.place_of_birth -> Dulwich -> location.location.containedby -> London\n# Answer:\nDulwich", "# Reasoning Path:\nTim Roth -> people.person.place_of_birth -> Dulwich -> travel.travel_destination.tourist_attractions -> Dulwich Picture Gallery\n# Answer:\nDulwich", "# Reasoning Path:\nTim Roth -> people.person.place_of_birth -> Dulwich -> common.topic.notable_types -> Location\n# Answer:\nDulwich", "# Reasoning Path:\nTim Roth -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nTim Roth -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nTim Roth -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nTim Roth -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#domain -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nTim Roth -> freebase.valuenotation.is_reviewed -> Gender -> type.property.schema -> Person\n# Answer:\nGender", "# Reasoning Path:\nTim Roth -> tv.tv_actor.guest_roles -> m.09nx7fy -> tv.tv_guest_role.episodes_appeared_in -> Jeremy Piven, Morgan Spurlock, & Esthero\n# Answer:\nJeremy Piven, Morgan Spurlock, & Esthero", "# Reasoning Path:\nTim Roth -> freebase.valuenotation.is_reviewed -> Date of birth -> type.property.expected_type -> Date/Time\n# Answer:\nDate of birth"], "ground_truth": ["Dulwich"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-733", "prediction": ["# Reasoning Path:\nVietnam -> location.country.currency_used -> Vietnamese dong -> common.topic.image -> 1 \u0111\u1ed3ng 1976\n# Answer:\nVietnamese dong", "# Reasoning Path:\nVietnam -> location.country.currency_used -> Vietnamese dong -> common.topic.notable_types -> Currency\n# Answer:\nVietnamese dong", "# Reasoning Path:\nVietnam -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d1ntkd\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nVietnam -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71w7lzm\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nVietnam -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6p1vnp7\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nVietnam -> location.statistical_region.renewable_freshwater_per_capita -> g.1hhc3bvgz\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nVietnam -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6f73_\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nVietnam -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc3b60b\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp"], "ground_truth": ["Vietnamese dong"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-734", "prediction": ["# Reasoning Path:\nRoger Federer -> people.person.children -> Charlene Riva Federer -> people.person.parents -> Mirka Federer\n# Answer:\nCharlene Riva Federer", "# Reasoning Path:\nRoger Federer -> people.person.children -> Myla Rose Federer -> people.person.parents -> Mirka Federer\n# Answer:\nMyla Rose Federer", "# Reasoning Path:\nRoger Federer -> people.person.spouse_s -> m.0j12_p2 -> people.marriage.spouse -> Mirka Federer\n# Answer:\nMirka Federer", "# Reasoning Path:\nRoger Federer -> people.person.children -> Charlene Riva Federer -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nCharlene Riva Federer", "# Reasoning Path:\nRoger Federer -> people.person.children -> Charlene Riva Federer -> people.person.sibling_s -> m.010h346t\n# Answer:\nCharlene Riva Federer", "# Reasoning Path:\nRoger Federer -> people.person.children -> Lenny Federer -> people.person.parents -> Mirka Federer\n# Answer:\nLenny Federer", "# Reasoning Path:\nRoger Federer -> people.person.children -> Myla Rose Federer -> common.topic.notable_for -> g.125gwrm0m\n# Answer:\nMyla Rose Federer", "# Reasoning Path:\nRoger Federer -> people.person.children -> Myla Rose Federer -> common.topic.notable_types -> Person\n# Answer:\nMyla Rose Federer", "# Reasoning Path:\nRoger Federer -> people.person.spouse_s -> m.0j12_p2 -> people.marriage.location_of_ceremony -> Riehen\n# Answer:\nRiehen", "# Reasoning Path:\nRoger Federer -> people.person.children -> Charlene Riva Federer -> freebase.valuenotation.has_value -> Place of birth\n# Answer:\nCharlene Riva Federer"], "ground_truth": ["Mirka Federer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.5333333333333333, "path_precision": 0.4, "path_recall": 0.8, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-735", "prediction": ["# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.image -> 10colonesbcrfront\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.webpage -> m.04m2m2k\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60wmkvh\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.11b71w3_6h\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_5xgy\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_6n42\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.1hhc37pjz\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.1hhc37x5q\n# Answer:\nlocation.statistical_region.external_debt_stock"], "ground_truth": ["Costa Rican col\u00f3n"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-737", "prediction": ["# Reasoning Path:\nBelgium -> location.country.capital -> Brussels -> location.location.contains -> Laeken\n# Answer:\nBrussels", "# Reasoning Path:\nBelgium -> location.country.capital -> Brussels -> location.location.time_zones -> Central European Time Zone\n# Answer:\nBrussels", "# Reasoning Path:\nBelgium -> location.statistical_region.gni_in_ppp_dollars -> g.11b61jk1pw\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nBelgium -> location.country.capital -> City of Brussels -> location.location.time_zones -> Central European Time Zone\n# Answer:\nCity of Brussels", "# Reasoning Path:\nBelgium -> location.country.capital -> City of Brussels -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nCity of Brussels", "# Reasoning Path:\nBelgium -> location.statistical_region.gni_in_ppp_dollars -> g.1245__rg8\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nBelgium -> location.statistical_region.gni_in_ppp_dollars -> g.1245_dgwn\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nBelgium -> media_common.netflix_genre.titles -> 22nd of May -> film.film.country -> Germany\n# Answer:\n22nd of May", "# Reasoning Path:\nBelgium -> media_common.netflix_genre.titles -> 22nd of May -> film.film.directed_by -> Koen Mortier\n# Answer:\n22nd of May", "# Reasoning Path:\nBelgium -> media_common.netflix_genre.titles -> 22nd of May -> film.film.country -> Netherlands\n# Answer:\n22nd of May"], "ground_truth": ["Brussels", "City of Brussels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-738", "prediction": ["# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> United Kingdom\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Australia\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.11b71r82sc\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Canada\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_4lv6\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_6fj1\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita"], "ground_truth": ["Parliamentary system", "Constitutional monarchy", "Unitary state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-739", "prediction": ["# Reasoning Path:\nClaude Debussy -> common.topic.image -> Claude Debussy, ca. 1908 (photo by F\u00e9lix Nadar) -> common.image.appears_in_topic_gallery -> Claude\n# Answer:\nClaude Debussy, ca. 1908 (photo by F\u00e9lix Nadar)", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> French opera -> music.genre.artists -> Olivier Messiaen\n# Answer:\nFrench opera", "# Reasoning Path:\nClaude Debussy -> common.topic.image -> Claude Debussy, ca. 1908 (photo by F\u00e9lix Nadar) -> common.image.appears_in_topic_gallery -> Estampes\n# Answer:\nClaude Debussy, ca. 1908 (photo by F\u00e9lix Nadar)", "# Reasoning Path:\nClaude Debussy -> common.topic.image -> Claude Debussy, ca. 1908 (photo by F\u00e9lix Nadar) -> common.image.size -> m.02bcsxr\n# Answer:\nClaude Debussy, ca. 1908 (photo by F\u00e9lix Nadar)", "# Reasoning Path:\nClaude Debussy -> common.topic.image -> Claude Debussy, ca. 1908 (photo by F\u00e9lix Nadar) -> common.image.appears_in_topic_gallery -> Images pour orchestre\n# Answer:\nClaude Debussy, ca. 1908 (photo by F\u00e9lix Nadar)", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> French opera -> common.topic.notable_types -> Field of study\n# Answer:\nFrench opera", "# Reasoning Path:\nClaude Debussy -> common.topic.image -> Debussy at the Villa M\u00e9dici in Rome, 1885, at centre in the white jacket -> common.image.size -> m.041td0y\n# Answer:\nDebussy at the Villa M\u00e9dici in Rome, 1885, at centre in the white jacket", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> French opera -> common.topic.webpage -> m.09wjdym\n# Answer:\nFrench opera", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> French opera -> music.genre.artists -> Francis Poulenc\n# Answer:\nFrench opera", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> 20th-century classical music -> music.genre.albums -> Octet / Music for a Large Ensemble / Violin Phase\n# Answer:\n20th-century classical music"], "ground_truth": ["Sim\u00e9on's Recitative and Aria", "Brouillards", "Deux arabesques, L. 66, CD 74, pour piano : No. 2 en sol majeur, Allegretto scherzando", "Masques, L. 105", "Petite Suite, L. 65, CD 71b, pour orchestre : I. En bateau", "Six \u00e9pigraphes antiques, L. 131: IV. Pour la danseuse aux crotales", "Hommage \u00e0 S. Pickwick Esq. P.P.M.P.C.", "Cort\u00e8ge et Air de danse de \\\"L\u2019Enfant prodigue\\\"", "Dialogue of the Wind and the Sea", "Pell\u00e9as et M\u00e9lisande: Act V. \\\"Ce n'est pas de cette petite blessure qu'elle peut mourir ...\\\" (Le m\u00e9decin, Arkel, Golaud)", "Images pour orchestre, L 122: III. Rondes de printemps", "Cort\u00e8ge et Air de danse de \\\"L\u2019Enfant prodigue\\\": Cort\u00e8ge", "Pagodes from Estampes", "Hommage \u00e0 Joseph Haydn, L. 115", "Pr\u00e9ludes, Livre II, L. 123: IX. Hommage \u00e0 S. Pickwick Esq. P.P.M.P.C.. Grave", "Je tremble en voyant ton visage", "Proses lyriques, L. 84, CD 90 : II. De Gr\u00e8ve, m\u00e9lodie \\\"Sur la mer les cr\u00e9puscules tombent\\\"", "Pierrot", "Proses lyriques, L. 84, CD 90 : IV. De Soirs, m\u00e9lodie \\\"Dimanche sur les villes\\\"", "La Bo\u00eete \u00e0 joujoux : V. 4e tableau. Apr\u00e8s fortune faite", "Pell\u00e9as et M\u00e9lisande - Concert Suite: Acte III, sc\u00e8ne 2. Les souterrains du ch\u00e2teau / Acte IV, sc\u00e8ne 2. Un appartement dans le ch\u00e2teau", "Premi\u00e8re Rapsodie pour clarinette en si b\u00e9mol, avec accompagnement d'orchestre, L. 116, CD 124b", "Estampes II: La soir e dans Grenade", "Premi\u00e8re Suite d\u2019orchestre, L. 50, CD 46 : I. F\u00eate", "Trois Chansons de France, L. 102: III. Rondel: Pour ce que Plaisance est morte", "Mazurka, L. 67, CD 75, pour piano", "Preludes, Book I: 11. La Danse de Puck", "Carry", "Pell\u00e9as et M\u00e9lisande: Act II, Scene I. \\\"Vous ne savez pas o\u00f9 je vous ai men\u00e9e?\\\" (Pell\u00e9as, M\u00e9lisande)", "Les Chansons de Bilitis, L. 96: No. 7. Le Tombeau sans nom", "Preludes, Book I: 7. Ce qu'a vu le vent d'ouest", "En sourdine", "Pell\u00e9as et M\u00e9lisande: Act I, Scene II. \\\"Je n'en dis rien\\\" (Arkel, Genevi\u00e8ve)", "\u00c9tudes, L. 136: XI. Pour les arp\u00e8ges compos\u00e9s", "Children's Corner, L. 113: IV. The Snow is Dancing", "Nocturnes, L 91: I. Nuages", "La Bo\u00eete \u00e0 joujoux : VI. \u00c9pilogue", "Bilitis", "Trois M\u00e9lodies, L. 81, CD 85, pour une voix avec accompagnement de piano : II. Le son du cor s\u2019afflige vers les bois.", "Rondel chinois", "Il dort encore", "Cello sonata in D minor: I. Prologue", "Estampes", "Des pas sur la neige", "La pluie au matin", "Pell\u00e9as et M\u00e9lisande: Act I, Scene III. \\\"Ho\u00e9! Hisse ho\u00e9! Ho\u00e9! Ho\u00e9!\\\" (Ch\u0153r, M\u00e9lisande, Pell\u00e9as, Genevi\u00e8ve)", "Pell\u00e9as et M\u00e9lisande: Act III, Scene II. \\\"Prenez garde; par ici, par ici\\\" (Golaud, Pell\u00e9as)", "Six \u00e9pigraphes antiques, L. 131", "\u00c9tudes, L. 136: V. Pour les octaves", "Pour le piano: Sarabande", "Le promenoir des deux amants, L. 118, CD 129, m\u00e9lodies pour voix et piano : II. \\\"Crois mon conseil, ch\u00e8re Clim\u00e8ne\\\"", "Musique pour \u201cLe Roi Lear\u201d, L. 107: Le Sommeil de Lear", "Voici que le printemps", "Six \u00e9pigraphes antiques, L. 131: III. Pour que la nuit soit propice", "Le Martyre de Saint-S\u00e9bastien: I. La cour des lys: N\u00b02", "Trois chansons de Charles d'Orl\u00e9ans, pour ch\u0153ur \u00e0 quatre voix mixtes, L. 92, CD 99: II. \\\"Quand j'ay ouy le tabourin\\\"", "F\u00eates galantes (Premier recueil), L. 80, CD 86 : II. Fantoches, m\u00e9lodie pour voix et piano \\\"Scaramouche et Pulcinella\\\"", "Apparition, L. 53, CD 57, m\u00e9lodie pour voix et piano \\\"La lune s\u2019attristait. Des s\u00e9raphins en pleurs\\\"", "La Romance d\u2019Ariel, L. 54, CD 58, m\u00e9lodie pour voix et piano \\\"Au long de ces montagnes douces\\\"", "Pell\u00e9as et M\u00e9lisande: Act V. \\\"Non, non, nous n'avons pas \u00e9t\u00e9 coupables\\\" (M\u00e9lisande, Golaud)", "Suite Bergamasque - Menuet", "Fantaisie pour piano et orchestre, L. 73, CD 72 : I. Andante - Allegro", "Prelude No. 8: The Girl with the Flaxen Hair", "Cinq po\u00e8mes de Baudelaire, L. 64, CD 70: IV. Recueillement", "En blanc et noir", "Sonate pour violoncelle et piano: II. S\u00e9r\u00e9nade", "Le Martyre de Saint-S\u00e9bastien, fragments symphoniques: III. La Passion", "Dans le jardin, L. 78, CD 107, m\u00e9lodie pour voix et piano \\\"Je regardais dans le jardin\\\"", "Premi\u00e8re rhapsodie", "Coquetterie posthume", "Pell\u00e9as et M\u00e9lisande: Act I, Scene I. \\\"Je ne pourrai plus sortir de cette for\u00eat\\\" (Golaud, M\u00e9lisande)", "Lia's Recitative and Aria", "L\u2019Archet", "Suite bergamasque, L. 75, CD 82, pour orchestre : III. Clair de lune", "Cinq po\u00e8mes de Baudelaire, L. 64, CD 70: V. La Mort des amants", "My Reverie", "F\u00eates galantes II, L. 104: III. Colloque sentimental", "Pell\u00e9as et M\u00e9lisande - Concert Suite: Acte I. Une for\u00eat", "F\u00eates galantes II, L. 104: II. Le Faune", "Les Chansons de Bilitis, L. 96: No. 10. La Danseuse aux crotales", "Bilitis: VI. Pour remercier la pluie au matin", "Preludes, Book I: 2. Voiles", "Le Martyre de Saint-S\u00e9bastien: III. Le concile des faux dieux: N\u00b05", "Le Martyre de Saint-S\u00e9bastien: IV. Le laurier bless\u00e9: N\u00b02", "Pell\u00e9as et M\u00e9lisande: Act V. \\\"Qu'avez-vous fait? Vous allez la tuer\\\" (Arkel, Golaud, M\u00e9lisande)", "Khamma", "La Bo\u00eete \u00e0 joujoux : III. 2e tableau. Le Champ de bataille", "La Romance d'Ariel", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene I. \\\"Maintenant que le p\u00e8re de Pell\u00e9as est sauv\u00e9 ...\\\" (Arkel, M\u00e9lisande)", "Khamma, L 125", "De r\u00eave", "Clair de lune, L. 32, CD 45, m\u00e9lodie pour voix et piano \\\"Votre \u00e2me est un paysage choisi\\\"", "En blanc et noir, L. 134, CD 142", "Preludes, Book I: 6. Des pas sur la neige", "Six \u00e9pigraphes antiques, L. 131: VI. Pour remercier la pluie au matin", "\u00c9tudes, L. 136: III. Pour les quartes", "Preludes, Book II: 1. Brouillards", "Le Martyre de Saint-S\u00e9bastien, fragments symphoniques: IV. Le Bon Pasteur", "Les Elfes, CD 25, m\u00e9lodie pour voix et piano \\\"Du sentier des bois aux daims familiers\\\"", "Spleen", "Pi\u00e8ce pour le V\u00eatement du bless\u00e9 (Page d'album), L. 133", "Images I: II. Hommages Rameau", "Preludes, Book II: 10. Canope", "\u00c9tudes, L. 136: XII. Pour les accords", "Nocturnes, L. 91, CD 98: I. Nuages", "Petite Suite, L. 65, CD 71b, pour orchestre : III. Menuet", "Ariettes oubli\u00e9es, L. 60, CD 63a : VI. Aquarelles, 2. Spleen, m\u00e9lodie pour voix et piano \\\"Les roses \u00e9taient toutes rouges\\\"", "La Mer: II. Jeux de vagues", "Rapsodie pour saxophone et piano, L. 98", "Premi\u00e8re Suite d\u2019orchestre, L. 50, CD 46 : IV. Bacchanale (Cort\u00e8ge et Bacchanale)", "Romance (Deux Romances, No. 2, 1891)", "Chanson", "Ariettes Oubli\u00e9es", "D'un cahier d'esquisses, L. 99", "Cort\u00e8ge et Air de danse de \\\"L\u2019Enfant prodigue\\\": Air de danse", "Images pour orchestre", "Children's Corner, L. 113: II. Jimbo's Lullaby", "F\u00eates galantes II, L. 104: I. Les Ing\u00e9nus", "Bilitis: III. Pour que la nuit soit propice", "F\u00eates galantes (Premier recueil), L. 80, CD 86 : III. Clair de lune, m\u00e9lodie pour voix et piano \\\"Votre \u00e2me est un paysage choisi\\\"", "Fleur des Bl\u00e9s", "Quatuor \u00e0 cordes en sol mineur, op. 10, L. 85, CD 91 : IV. Tr\u00e8s mod\u00e9r\u00e9", "Three Preludes: Feux d'artifice", "Mandoline", "Musique, L. 44, CD 54, m\u00e9lodie pour voix et piano \\\"La lune se levait, pure, mais plus glac\u00e9e\\\"", "Pour le piano", "Ballade des femmes de Paris", "Ballade (slave), L. 70, CD 78, pour piano", "No\u00ebl des enfants qui n'ont plus de maisons", "Le Martyre de Saint-S\u00e9bastien, L. 124: Prologue", "F\u00eate galante", "Trois Chansons de France", "Pr\u00e9ludes", "Ariettes oubli\u00e9es, L. 60, CD 63a : III. Le rossignol qui, du haut d\u2019une branche, m\u00e9lodie pour voix et piano \\\"L\u2019ombre des arbres dans la rivi\u00e8re embrum\u00e9e\\\"", "\u00c9tudes", "Coquetterie posthume, L. 39, CD 50, m\u00e9lodie pour voix et piano \\\"Quand je mourrai, que l\u2019on me mette\\\"", "Danse boh\u00e9mienne, L. 9, CD 4, pour piano", "Jardins sous la pluie", "Suite: Pour le Piano, L.95: III. Toccata (vif)", "Pell\u00e9as et M\u00e9lisande: Act V. \\\"Qu'y-a-t-il? Qu'est-ce que toutes ces femmes viennent faire ici?\\\" (Golaud, Le m\u00e9decin, Arkel)", "Sonata for Violin and Piano in G Minor, L 140: I. Allegro vivo", "Les Chansons de Bilitis, L. 96: No. 11. Le Souvenir de Mnasidika", "Le petit N\u00e8gre, L. 114", "Nuit d\u2019\u00e9toiles, L. 4, CD 2, m\u00e9lodie pour voix et piano \\\"Nuit d\u2019\u00e9toiles, sous tes voiles\\\"", "Chansons de Bilitis, L. 90: La Fl\u00fbte de Pan \u00abPour le jour des Hyacinthies\u00bb", "g.11b821q1dm", "Le promenoir des deux amants, L. 118, CD 129, m\u00e9lodies pour voix et piano", "Marche \u00e9cossaise sur un th\u00e8me populaire, L. 77, CD 83a (Marche des anciens comtes de Ross, d\u00e9di\u00e9e \u00e0 leur descendant le G\u00e9n\u00e9ral Meredith Reid, grand-croix de l'ordre royal du R\u00e9dempteur)", "\u00c9tudes, L. 136: X. Pour les sonorit\u00e9s oppos\u00e9es", "Fantaisie pour piano et orchestre, L. 73, CD 72 : III. Allegro molto", "Engulfed Cathedral (Debussy)", "Il pleure dans mon c\u0153ur", "Bilitis: V. Pour l'\u00e9gyptienne", "Le Martyre de Saint-S\u00e9bastien: III. Le concile des faux dieux: N\u00b06", "Le Martyre de Saint-S\u00e9bastien: V. Le paradis: N\u00b01", "Le temps a laiss\u00e9 son manteau", "Chanson triste", "Le Martyre de Saint-S\u00e9bastien, L. 124: Acte III \\\"Le Concile des faux dieux\\\". Fanfare no. 2", "Deux danses pour Harpe, L. 103: Danse sacr\u00e9e", "Pell\u00e9as et M\u00e9lisande", "Nocturnes, L. 91, CD 98: III. Sir\u00e8nes", "Pell\u00e9as et M\u00e9lisande: Act II, Scene II. \\\"Je suis ... je suis malade ici\\\" (M\u00e9lisande, Golaud)", "Pour le piano, L. 95: I. Pr\u00e9lude. \u00c0 Mademoiselle M.W. de Romilly. Assez anim\u00e9 et tr\u00e8s rythm\u00e9 - Tempo di cadenza - Tempo I", "Printemps, L. 61, CD 68b : II. Mod\u00e9r\u00e9", "Trio in G Major for Violin, Cello and Piano: III. Andante espressivo", "F\u00eates Galantes I", "Trois Chansons de France, L. 102: II. La Grotte: Aupr\u00e8s de cette grotte sombre", "Preludes, Book II: 12. Feux d'artifice", "Soupir", "Clair de lune Samba", "Trag\u00e9die", "Rondeau: 'Fut-il jamais'", "Le Tombeau sans nom", "Khamma: Sc\u00e8ne 3", "Aupr\u00e8s de cette grotte sombre", "Chant pastoral", "Le Martyre de Saint-S\u00e9bastien: II. La chambre magique: N\u00b03", "Rhapsodie pour saxophone et orchestre, L. 98", "L'enfant prodigue", "R\u00eaverie, L. 8, CD 3, m\u00e9lodie pour voix et piano \\\"Le Z\u00e9phir \u00e0 la douce haleine\\\"", "Trois po\u00e8mes de St\u00e9phane Mallarm\u00e9, L. 127, CD 135: III. \u00c9ventail \\\"\u00d4 r\u00eaveuse pour que je plonge\\\"", "La Damoiselle \u00e9lue, L. 62, CD 69, po\u00e8me lyrique pour voix de femmes, solo, ch\u0153ur et orchestre", "En blanc et noir: I. Avec emportement", "Fantaisie pour piano et orchestre, L. 73, CD 72", "Children's Corner, L. 113: V. The Little Shepherd", "Two movements from \\\"L\u2019Enfant prodigue\\\": Pr\u00e9lude", "No\u00ebl des enfants qui n'ont plus de maison, L. 139, CD 147, m\u00e9lodie pour voix et piano \\\"Nous n\u2019avons plus de maison\\\"", "F\u00eate galante, L. 23, CD 31, m\u00e9lodie pour voix et piano \\\"Voil\u00e0 Sylandre et Lycas et Myrtil\\\"", "Images pour orchestre, L 122: I. Gigues", "Images oubli\u00e9es, L. 87: Quelques aspects de 'Nous n'irons plus au bois' parce qu'il fait un temps insupportable. Tr\u00e8s vite - Mod\u00e9r\u00e9 - Premier Mouvement (vit et joyeax)", "Lorsqu'elle est entr\u00e9e", "Les Chansons de Bilitis, L. 96: No. 6. Bilitis", "Le Martyre de Saint-S\u00e9bastien, L. 124: Acte III \\\"Le Concile des faux dieux\\\". Fanfare no. 1", "Ariettes oubli\u00e9es, L. 60, CD 63a : V. Aquarelles, 1. Green, m\u00e9lodie pour voix et piano \\\"Voici des fruits, des fleurs, des feuilles\\\"", "Ministrels for Cello and Piano", "L'eau pure du bassin", "Trois Ballades de Fran\u00e7ois Villon, L. 119: II. Ballade que Villon feit \u00e0 la requeste de sa m\u00e8re pour prier Nostre-Dame", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene II. \\\"Oh! Cette pierre est lourde ...\\\" (Yniold, Le Berger)", "La mort des amants", "Trois Chansons de France, L. 102, CD 115: I. Rondel: Le temps a laiss\u00e9 son manteau", "Printemps, L. 61, CD 68b : I. Tr\u00e8s mod\u00e9r\u00e9", "Le Souvenir de Mnasidika", "Preludes, Book II: 7. La Terrasse des audiences du clair de lune", "Ariettes oubli\u00e9es, L. 60, CD 63a : I. Le vent dans la plaine suspend son haleine, m\u00e9lodie pour voix et piano \\\"C'est l'extase langoureuse\\\"", "Beau soir", "\u00c9tudes, L. 136: IV. Pour les sixtes", "Iberia No. 2 from \\\"Images\\\" for Orchestra: I. Through the Streets and Roads", "Regret", "Le Martyre de Saint-S\u00e9bastien: III. Le concile des faux dieux: N\u00b07", "Le Martyre de Saint-S\u00e9bastien: III. Le concile des faux dieux: N\u00b04", "La Fille aux cheveux de lin, L. 33, CD 15, m\u00e9lodie pour voix et piano \\\"Sur la luzerne en fleur\\\"", "Bilitis: I. Pour invoquer Pan, dieu du vent d'\u00e9t\u00e9", "Les Chansons de Bilitis, L. 96: No. 1. Chant pastoral", "Petite Suite, L. 65, CD 71a, pour piano \u00e0 4 mains : III. Menuet", "Images, Livre 2, L. 111: No. 3. Poissons d'or", "Bilitis: II. Pour un tombeau sans nom", "Pell\u00e9as et M\u00e9lisande - Concert Suite: Acte V. Une chambre dans le ch\u00e2teau", "Printemps, L. 61, CD 68b, suite symphonique en mi majeur pour 2 pianos et orchestre", "S\u00e9r\u00e9nade", "Petite Suite: Menuet", "Rondel chinois, L. 17, CD 11, m\u00e9lodie pour voix et piano \\\"Sur le lac bord\u00e9 d\u2019azal\u00e9e\\\"", "Preludes, Book II: 6. General Lavine - Eccentric", "Pell\u00e9as et M\u00e9lisande: Act V. \\\"Ouvrez la fen\u00eatre, ouvrez la fen\u00eatre ...\\\" (M\u00e9lisande, Arkel, Le m\u00e9decin)", "\u00c9tudes, L. 136: IX. Pour les notes r\u00e9p\u00e9t\u00e9es", "Sonata for flute, viola and harp", "Images pour orchestre, L 122: II. Iberia", "Proses Lyriques", "Les papillons", "Petite Suite, L. 65, CD 71, pour violon et piano : III. Menuet", "Printemps, L. 61, CD 68a, suite symphonique en mi majeur pour 2 pianos et ch\u0153ur", "Calme dans le demi-jour", "My Reverie (Debussy's \\\"Reverie\\\")", "Pell\u00e9as et M\u00e9lisande - Concert Suite: Acte II, sc\u00e8ne 1. Une fontaine dans le parc", "Valse romantique", "Trio in G Major for Violin, Cello and Piano: IV. Finale. Appassionato", "Trois po\u00e8mes de St\u00e9phane Mallarm\u00e9, L. 127, CD 135: I. Soupir \\\"Mon \u00e2me vers ton front o\u00f9 r\u00eave, \u00f4 calme s\u0153ur\\\"", "Colloque sentimental", "Chansons de Bilitis, L. 90: Le Tombeau des na\u00efades: \u00abLe long du bois couvert de givre\u00bb", "Proses lyriques, L. 84, CD 90 : I. De R\u00eave, m\u00e9lodie \\\"La nuit a des douceurs de femme\\\"", "Trois Po\u00e8mes de St\u00e9phane Mallarm\u00e9", "Deux danses pour Harpe, L. 103", "Pell\u00e9as et M\u00e9lisande: Act V. \\\"Attention... attention\\\" (Arkel, Le m\u00e9decin)", "Regret, L. 55, CD 59, m\u00e9lodie pour voix et piano \\\"Devant le ciel d\u2019\u00e9t\u00e9, ti\u00e8de et calme\\\"", "Images oubli\u00e9es, L. 87: Dans le mouvement d'une 'Sarabande', c'est-\u00e0-dire avec une \u00e9l\u00e9gance gr\u00e2ve et lente, m\u00eame un peu vieux portrait, souvenir du Louvre, etc.", "Pantomime, L. 31, CD 47, m\u00e9lodie pour voix et piano \\\"Pierrot, qui n\u2019a rien d\u2019un Clitandre\\\"", "Souhait", "En blanc et noir: III. Scherzando", "\u00c9ventail", "L\u2019Enfant prodigue, L. 57, CD 61, sc\u00e8ne lyrique \\\"L\u2019ann\u00e9e, en vain chasse l\u2019ann\u00e9e\\\"", "Deux arabesques", "Pell\u00e9as et M\u00e9lisande: Act II, Scene II. \\\"Ah! Ah! Tout va bien, ce la ne sera rien\\\" (Golaud, M\u00e9lisande)", "Les Chansons de Bilitis, L. 96: No. 5. La Partie d'osselets", "Streichquartett in g-Moll, Op. 10: Andantino, doucement expressif (Endress-Quartett)", "Morceau de concours, L. 108", "Nocturne, L. 82, CD 89, pour piano", "Berceuse h\u00e9ro\u00efque pour orchestre, L. 132", "Premi\u00e8re Rapsodie pour clarinette en si b\u00e9mol, avec accompagnement de piano, L. 116, CD 124a", "La Danseuse aux crotales", "Children's Corner", "Pell\u00e9as et M\u00e9lisande: Act III, Scene III. \\\"Ah! Je respire enfin!\\\" (Pell\u00e9as, Golaud)", "Estampes - I. Pagodes", "Le Martyre de Saint-S\u00e9bastien: II. La chambre magique: N\u00b02", "Petite Suite, L. 65, CD 71a, pour piano \u00e0 4 mains : I. En bateau", "Prelude (From Suite Bergamasque)", "\u00c9l\u00e9gie, L. 138", "Children's Corner, L. 113: III. Serenade for the Doll", "Pell\u00e9as et M\u00e9lisande: Act I, Scene II. \\\"Voici ce qu'il \u00e9crit \u00e0 son fr\u00e8re Pell\u00e9as: 'Un soir, je l'ai trouv\u00e9e'\\\" (Genevi\u00e8ve)", "Premi\u00e8re Suite d\u2019orchestre, L. 50, CD 46 : III. R\u00eave", "Les Chansons de Bilitis, L. 96: No. 8. Les Courtisanes \u00e9gyptiennes", "Minstrels", "Khamma: Au Mouvement \u2013", "Trois Ballades de Fran\u00e7ois Villon", "\u00c9tudes, L. 136: II. Pour les tierces", "Preludes, Book II: 3. La Puerta del Vino", "Les Chansons de Bilitis, L. 96: No. 9. L'Eau pure du bassin", "Ariettes oubli\u00e9es, L. 60, CD 63a : IV. Paysages belges. Chevaux de bois, m\u00e9lodie pour voix et piano \\\"Tournez, tournez, bons chevaux de bois\\\"", "Dans le Jardin", "Syrinx huilulle, L 129", "Pell\u00e9as et M\u00e9lisande: Act I, Scene I. \\\"Qu'est-ce qui brille ainsi, au fond de l'eau?\\\" (Golaud, M\u00e9lisande)", "Ce qu'a vu le vent d'ouest", "Le Martyre de Saint-S\u00e9bastien: III. Le concile des faux dieux: N\u00b01", "Trio in G Major for Violin, Cello and Piano: I. Andantino con moto allegro", "Tarantelle styrienne, L. 69, CD 77a, (Danse) pour piano", "Pell\u00e9as et M\u00e9lisande: Act III, Scene IV. \\\"Viens, nous allons nous asseoir ici, Yniold\\\" (Golaud, Yniold)", "Le Martyre de Saint-S\u00e9bastien, fragments symphoniques: II. Danse extatique et final du premier acte", "Piano Trio", "Chansons de Bilitis, L. 90: \u00abLa Chevelure \u00abIl m'a dit \u00abCette nuit d'ai r\u00eav\u00e9\u00bb\u00bb", "Pr\u00e9ludes, Livre II, L. 123: IV. \u00abLes f\u00e9es sont d'exquises danseuses\u00bb. Rapide et l\u00e9ger", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene II. \\\"On dirait que ta voix a pass\u00e9 sur la mer au printemps!\\\" (Choeur, M\u00e9lisande, Pell\u00e9as, Genevi\u00e8ve)", "Le Matelot qui tombe \u00e0 l\u2019eau", "L\u2019Enfant prodigue : 2b. Pourquoi m'as-tu quitt\u00e9e (Lia)", "Khamma: Deuxi\u00e8me Danse. Assez anim\u00e9 \u2013 Plus anim\u00e9 peu \u00e0 peu \u2013 Tr\u00e8s anim\u00e9 \u2013", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene I. \\\"Pell\u00e9as part ce soir\\\" (Golaud, Arkel, M\u00e9lisande)", "De soir", "Le Martyre de Saint-S\u00e9bastien: IV. Le laurier bless\u00e9: N\u00b01", "Fantaisie for piano and orchestra", "Trois chansons de Charles d'Orl\u00e9ans, pour ch\u0153ur \u00e0 quatre voix mixtes, L. 92, CD 99: III. \\\"Yver, vous n'estes qu'un vilain \\\"", "\u00c9tudes, L. 136: I. Pour les \u00ab cinq doigts \u00bb d'apr\u00e8s monsieur Czerny", "Nocturnes, L 91: III. Sir\u00e8nes", "Pell\u00e9as et M\u00e9lisande: Act II, Scene III. \\\"Oui, c'est ici, nous y sommes\\\" (Pell\u00e9as, M\u00e9lisande)", "Reflets dans l'eau", "La chute de la maison Usher", "Trois Ballades de Fran\u00e7ois Villon, L. 119: III. Ballade des femmes de Paris", "Children's Corner, L. 113 No. 6: Golliwog's Cake-walk", "Les Chansons de Bilitis, L. 96: No. 3. Les Contes", "Mes longs cheveux", "Khamma: Sc\u00e8ne 2 \u2013", "R\u00eaverie", "Le Martyre de Saint-S\u00e9bastien: IV. Le laurier bless\u00e9: N\u00b03", "Le diable dans le beffroi", "Iberia No. 2 from \\\"Images\\\" for Orchestra: III. The Morning of a Holiday", "Bilitis: IV. Pour la danseuse aux crotales", "Petite Suite, L. 65, CD 71a, pour piano \u00e0 4 mains : IV. Ballet", "Les contes", "Nuits blanches", "Paysage sentimental", "String Quartet", "L'ombre des arbres", "Trois M\u00e9lodies, L. 81, CD 85, pour une voix avec accompagnement de piano : I. La mer est plus belle que les cath\u00e9drales.", "Lindaraja", "Le Martyre de saint S\u00e9bastien", "Duo", "Petite Suite, L. 65, CD 71b, pour orchestre : IV. Ballet", "La plus que lente", "Prelude to the Afternoon of a Faun", "Pell\u00e9as et M\u00e9lisande: Act III, Scene I. \\\"Je les noue, je les noue aux branches du saule\\\" (Pell\u00e9as, M\u00e9lisande, Golaud)", "F\u00eates Galantes II", "Jeux", "Images, Livre 1, L. 110: No. 3. Mouvement", "Ariettes oubli\u00e9es, L. 60, CD 63a : II. Il pleut doucement sur la ville, m\u00e9lodie pour voix et piano \\\"Il pleure dans mon c\u0153ur comme il pleure sur la ville\\\"", "Sonata for flute, viola and harp, L. 137: III. Finale", "Children's Corner, L. 113: VI. Golliwogg's Cake-Walk", "Cinq po\u00e8mes de Baudelaire, L. 64, CD 70: II. Harmonie du soir", "Preludes, Book I: 1. Danseuses de Delphes", "Petite Suite, L. 65, CD 71b, pour orchestre : II. Cort\u00e8ge", "Romance, L. 52, CD 56, m\u00e9lodie pour voix et piano \\\"Voici que le printemps, ce fil l\u00e9ger d\u2019avril\\\"", "Three Preludes: Ce qu'a vu le vent de l'ouest", "Ballade de Villon a s'amye", "De gr\u00e8ve", "Pell\u00e9as et M\u00e9lisande: Act III, Scene IV. \\\"Ah! Ah! Petite m\u00e8re a allum\u00e9 sa lampe\\\" (Yniold, Golaud)", "Le promenoir des deux amants, L. 118, CD 129, m\u00e9lodies pour voix et piano : III. \\\"Je tremble en voyant ton visage\\\"", "Sonata for flute, viola and harp, L. 137: II. Interlude", "Harmonie du soir", "Nocturnes", "Nocturnes, L 91: II. F\u00eates", "Petite Suite", "Trois Chansons de Charles d'Orl\u00e9ans", "Six \u00e9pigraphes antiques, L. 131: V. Pour l'\u00c9gyptienne", "Quant j'ai ouy le tabourin", "Musique", "Dieu! qu'il la fait bon regarder!", "Pell\u00e9as et M\u00e9lisande: Act I, Scene II. \\\"Grand-p\u00e8re, j'ai re\u00e7u en m\u00eame temps que la lettre de mon fr\u00e8re...\\\" (Pell\u00e9as, Arkel, Genevi\u00e8ve)", "Pr\u00e9ludes, Livre 1, L. 117 No. 9: La s\u00e9r\u00e9nade interrompue", "Khamma: Premi\u00e8re Danse. Grave et lent \u2013 Animez \u2013 Revenez au Mouvement \u2013 Plus lent \u2013 Animez peu \u00e0 peu \u2013", "Le son du cor s'afflige", "Chevaux de bois", "Star Gazers Theme Song", "Premi\u00e8re Suite d\u2019orchestre, L. 50, CD 46 : II. Ballet", "III. Le Vent dans la plaine", "La Belle au Bois dormant", "Six \u00e9pigraphes antiques, L. 131: II. Pour un tombeau sans nom", "Deux Romances", "Images oubli\u00e9es, L. 87: Lent (m\u00e9lancolique et doux)", "g.1234nfvz", "Le promenoir des deux amants", "Berceuse h\u00e9ro\u00efque pour piano, L. 132", "Pagodes", "Trois Ballades de Fran\u00e7ois Villon, L. 119: I. Ballade de Villon \u00e0 s'Amye", "En blanc et noir: II. Lent, sombre", "Le Tombeau des Na\u00efades", "Petite Suite, L. 65, CD 71a, pour piano \u00e0 4 mains : II. Cort\u00e8ge", "\u00c9tudes, L. 136: VII. Pour les degr\u00e9s chromatiques", "Suite Bergamasque: Passepied: Allegretto ma non troppo", "Quatuor \u00e0 cordes en sol mineur, op. 10, L. 85, CD 91 : I. Anim\u00e9 et tr\u00e8s d\u00e9cid\u00e9", "Pell\u00e9as et M\u00e9lisande: Act II, Scene I. \\\"C'est au bord d'une fontaine aussi qu'il vous a trouv\u00e9e?\\\" (Pell\u00e9as, M\u00e9lisande)", "Suite bergamasque, L. 75, CD 82a, pour orchestre : III. Clair de lune", "Le Martyre de Saint-S\u00e9bastien, L. 124: Acte III \\\"Le Concile des faux dieux\\\"", "\u00c9tudes, L. 136: VIII. Pour les agr\u00e9ments", "Rodrigue et Chim\u00e8ne", "Les roses", "From Dawn Till Noon on the Sea", "Les ing\u00e9nus", "Fantaisie pour piano et orchestre, L. 73, CD 72 : II. Lento e molto espressivo", "Le Martyre de Saint-S\u00e9bastien: II. La chambre magique: N\u00b01", "Pour le piano: II. Sarabande", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene I. \\\"O\u00f9 vas-tu? Il faut que je te parle ce soir\\\" (Pell\u00e9as, M\u00e9lisande)", "Chanson espagnole, L. 42, CD 49, duo pour 2 voix \u00e9gales \\\"Nous venions de voir le taureau\\\"", "Nocturnes, L. 91, CD 98: II. F\u00eates", "Preludes, Book II: 2. Feuilles mortes", "Le Martyre de Saint-S\u00e9bastien: III. Le concile des faux dieux: N\u00b02", "Suite bergamasque, L. 75, CD 82b, pour orchestre : III. Clair de lune", "\u00c9tudes, L. 136: VI. Pour les huits doigts", "Voiles", "Chansons de Bilitis", "Khamma: Pr\u00e9lude. Mod\u00e9r\u00e9ment anim\u00e9 (comme un lointain tumulte) \u2013", "Petite Suite, L. 65: I. En bateau", "Trois M\u00e9lodies, L. 81, CD 85, pour une voix avec accompagnement de piano : III. L\u2019\u00e9chelonnement des haies moutonne \u00e0 l\u2019infini", "Pr\u00e9lude \u00e0 l'apr\u00e8s-midi d'un faune, L. 86, CD 87", "Les Chansons de Bilitis, L. 96: No. 4. Chanson", "3 Preludes from Book II: V. Bruyer\u00e8s. Calme", "Paysage sentimental, L 45, CD 55, m\u00e9lodie pour voix et piano \\\"Le ciel d\u2019hiver si doux, si triste, si dormant\\\"", "Triolet \u00e0 Philis \\\"Z\u00e9phir\\\", L. 12, CD 19, m\u00e9lodie pour voix et piano \\\"Si j\u2019\u00e9tais le z\u00e9phir ail\u00e9\\\"", "Les Chansons de Bilitis, L. 96: No. 2. Les Comparaisons", "La mer est plus belle", "La mer", "Petite Suite, L. 65, CD 71a, pour piano \u00e0 4 mains", "Danse profane", "Sonate f\u00fcr Violine und Klavier in g-Moll: Finale: Tr\u00e8s anim\u00e9", "Les Chansons de Bilitis, L. 96: No. 12. La Pluie au matin", "Nuit sans fin", "Iberia No. 2 from \\\"Images\\\" for Orchestra: II. The Perfumes of the Night", "Le jet d'eau", "\u00c9tude retrouv\u00e9e", "Pell\u00e9as et M\u00e9lisande, L. 88, CD 93: Acte IV", "Les Comparaisons", "Pantomime", "Pell\u00e9as et M\u00e9lisande: Act II, Scene II. \\\"Tiens, o\u00f9 est l'anneau que je t'avais donn\u00e9?\\\" (Golaud, M\u00e9lisande)", "Les Courtisanes \u00e9gyptiennes", "Fantoches", "Pell\u00e9as et M\u00e9lisande: Act III, Scene I. \\\"Mes longs cheveux descendent\\\"", "Fleurs des bl\u00e9s, L. 7, CD 16, m\u00e9lodie pour voix et piano \\\"Le long des bl\u00e9s que la brise fait onduler\\\"", "Pell\u00e9as et M\u00e9lisande: Act III, Scene I. Une des tours du ch\u00e2teau", "L'\u00e9chelonnement des haies", "La damoiselle \u00e9lue", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene II. \\\"C'est le dernier soir ...\\\" (Pell\u00e9as, M\u00e9lisande)", "La Belle au bois dormant, L. 74, CD 81, m\u00e9lodie \\\"Des trous \u00e0 son pourpoint vermeil\\\"", "Mandoline, L. 29, CD 43, m\u00e9lodie pour voix et piano \\\"Les donneurs de s\u00e9r\u00e9nades\\\"", "Le promenoir des deux amants, L. 118, CD 129, m\u00e9lodies pour voix et piano : I. \\\"Aupr\u00e8s de cette grotte sombre\\\"", "Rapsodie pour orchestre et saxophone", "Violin Sonata", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene I. \\\"Apporte-la\\\" (Golaud, Arkel, M\u00e9lisande)", "Chanson espagnole", "Pell\u00e9as et M\u00e9lisande: Act III, Scene I. \\\"Oh! Oh! Mes cheveux descendent de la tour!\\\" (M\u00e9lisande, Pell\u00e9as)", "Green", "Petite Pi\u00e8ce", "Menuet", "De fleurs", "Placet futile", "Sonate pour violoncelle et piano: III. Finale", "Khamma: Sc\u00e8ne 1. Le Temple int\u00e9rieur du Grand-Dieu Amun-Ra \u2013", "La Bo\u00eete \u00e0 Joujoux : I. Pr\u00e9lude. Le Sommeil de la bo\u00eete", "Suite bergamasque, L. 75, CD 82 : III. Clair de lune, pour violon et piano", "La grotte", "La Bo\u00eete \u00e0 joujoux : II. 1er tableau. Le Magasin de jouets", "Yver, vous n'estes qu'un vilain", "Le Martyre de Saint-S\u00e9bastien: III. Le concile des faux dieux: N\u00b03", "UTSUKUSHII YUUGURE", "N\u00b0 5: The Film: Clair de Lune", "Le Martyre de Saint-S\u00e9bastien, fragments symphoniques: I. La Cour de lys", "Proses lyriques, L. 84, CD 90 : III. De Fleurs, m\u00e9lodie \\\"Dans l\u2019ennui si d\u00e9sol\u00e9ment vert\\\"", "Flots, palmes, sables, L. 25, CD 38, m\u00e9lodie pour voix et piano (ou harpe) \\\"Loin des yeux du monde\\\"", "Arabesque 1", "Nuit d'\u00c9toiles", "Trois po\u00e8mes de St\u00e9phane Mallarm\u00e9, L. 127, CD 135: II. Placet futile \\\"Princesse! \u00c0 jalouser le destin d'une H\u00e9b\u00e9\\\"", "Cinq po\u00e8mes de Baudelaire, L. 64, CD 70: I. Le Balcon", "g.1234bndn", "L'isle joyeuse", "Les Ang\u00e9lus", "Khamma: Troisi\u00e8me Danse. Tr\u00e8s lent \u2013 Plus p\u00e9n\u00e9trant \u2013 Doucement contenu \u2013", "La Partie d'osselets", "Sonata for Violin and Piano in G Minor, L 140: II. Interm\u00e8de: fantasque et l\u00e9ger", "Images, Livre 2, L. 111: No. 2. Et la lune descend sur le temple qui fut", "Recueillement", "Images II: I. Cloches trevers les feuilles", "Three Preludes: Feuilles mortes", "Cinq Po\u00e8mes de Baudelaire", "Ballade que Villon feit \u00e0 la requeste de sa m\u00e8re pour prier Nostre-Dame", "Suite bergamasque", "Sonate pour violon et piano en sol mineur, L. 140", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene II. \\\"Quel est ce bruit? On ferme les portes\\\" (Pell\u00e9as, M\u00e9lisande)", "Les soirs illumin\u00e9s par l'ardeur du charbon", "Romance", "La Fl\u00fbte de Pan", "Pour ce que Plaisance est morte", "Pr\u00e9ludes, Livre I, L. 117: VIII. La fille aux cheveux de lin. Tr\u00e8s calme et doucement expressif", "Trio in G Major for Violin, Cello and Piano: II. Scherzo - Intermezzo. Moderato con allegro", "Children's Corner, L. 113 No. 5: The Little Shepherd", "Apparition", "Romance, L. 43, CD 53, m\u00e9lodie pour voix et piano \\\"Silence ineffable de l\u2019heure\\\"", "Les cloches", "Musique pour \u201cLe Roi Lear\u201d, L. 107: Fanfare d\u2019ouverture", "Le Martyre de Saint-S\u00e9bastien: I. La cour des lys: N\u00b01", "Danse: Tarantelle styrienne", "Le lilas", "Le balcon", "Crois mon conseil, ch\u00e8re Clim\u00e8ne", "Cello Sonata", "Le faune", "Marche \u00e9cossaise sur un th\u00e8me populaire, L. 77, CD 83b (Marche des anciens comtes de Ross, d\u00e9di\u00e9e \u00e0 leur descendant le G\u00e9n\u00e9ral Meredith Reid, grand-croix de l'ordre royal du R\u00e9dempteur)", "Aimons-nous et dormons, L. 16, CD 7, m\u00e9lodie pour voix et piano \\\"Aimons-nous et dormons, sans songer au reste du monde\\\"", "La chevelure", "C'est l'extase", "Le Martyre de Saint-S\u00e9bastien: V. Le paradis: N\u00b02", "Clair de lune", "The Tears of Billie Blue", "Pr\u00e9ludes, Livre II, L. 123: XI. Les tierces altern\u00e9es. Mod\u00e9r\u00e9ment anim\u00e9", "Preludes, Book II: 8. Ondine", "La Bo\u00eete \u00e0 joujoux : IV. 3e tableau. La Bergerie \u00e0 vendre", "Pell\u00e9as et M\u00e9lisande: Act V. \\\"M\u00e9lisande...M\u00e9lisande ...\\\" - \\\"Est-ce vous, Golaud?\\\" (Golaud, M\u00e9lisande)", "\u590f\u306e\u98a8\u306e\u795e\uff08\u300c\uff16\u3064\u306e\u53e4\u4ee3\u5893\u7891\u540d\u300d\u3088\u308a\uff09", "Syrinx", "Children's Corner - I. Doctor Gradus ad Parnassum", "Estampes, L. 100: III. Jardins sous la pluie. Net et vif", "Quatuor \u00e0 cordes en sol mineur, op. 10, L. 85, CD 91 : II. Assez vif et bien rythm\u00e9", "Sonata for flute, viola and harp, L. 137: I. Pastorale", "Pell\u00e9as et M\u00e9lisande: Act I, Scene III. \\\"Il fait sombre dans les jardins\\\" (M\u00e9lisande, Genevi\u00e8ve, Pell\u00e9as)"], "ans_acc": 0.003952569169960474, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.08888888888888888, "path_precision": 0.2, "path_recall": 0.05714285714285714, "path_ans_f1": 0.007751937984496123, "path_ans_precision": 0.2, "path_ans_recall": 0.003952569169960474}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> common.topic.image -> WilliamFranklin\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Francis Folger Franklin -> people.person.parents -> Deborah Read\n# Answer:\nFrancis Folger Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> people.person.gender -> Male\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Sarah Franklin Bache -> people.person.children -> Benjamin Franklin Bache\n# Answer:\nSarah Franklin Bache", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Sarah Franklin Bache -> people.person.spouse_s -> m.0j4k6gy\n# Answer:\nSarah Franklin Bache", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> American literature\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Francis Folger Franklin -> people.deceased_person.place_of_burial -> Christ Church Burial Ground\n# Answer:\nFrancis Folger Franklin", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Francis Folger Franklin -> common.topic.notable_for -> g.125by3nfc\n# Answer:\nFrancis Folger Franklin", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> :Library and information science\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze"], "ground_truth": ["Deborah Read"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-740", "prediction": ["# Reasoning Path:\nSelena Gomez -> tv.tv_actor.starring_roles -> m.0bgyvq1 -> tv.regular_tv_appearance.character -> Gianna\n# Answer:\nGianna", "# Reasoning Path:\nBarney & Friends -> tv.tv_program.regular_cast -> m.0bgyvq1 -> tv.regular_tv_appearance.character -> Gianna\n# Answer:\nGianna", "# Reasoning Path:\nSelena Gomez -> tv.tv_actor.starring_roles -> g.11b7vzwxql\n# Answer:\ntv.tv_actor.starring_roles", "# Reasoning Path:\nSelena Gomez -> tv.tv_actor.starring_roles -> m.0bgyvq1 -> tv.regular_tv_appearance.series -> Barney & Friends\n# Answer:\ntv.regular_tv_appearance.series", "# Reasoning Path:\nBarney & Friends -> tv.tv_program.regular_cast -> m.0y55_3z -> tv.regular_tv_appearance.character -> Angela\n# Answer:\nAngela", "# Reasoning Path:\nSelena Gomez -> tv.tv_actor.starring_roles -> m.03lyrfq -> tv.regular_tv_appearance.character -> Alex Russo\n# Answer:\nAlex Russo", "# Reasoning Path:\nSelena Gomez -> tv.tv_actor.guest_roles -> m.0y5cxrq -> tv.tv_guest_role.character -> Alex Russo\n# Answer:\nAlex Russo", "# Reasoning Path:\nSelena Gomez -> music.artist.album -> g.1hc0gf71l\n# Answer:\nmusic.artist.album", "# Reasoning Path:\nBarney & Friends -> tv.tv_program.regular_cast -> m.0bgyvq1 -> tv.regular_tv_appearance.actor -> Selena Gomez\n# Answer:\ntv.regular_tv_appearance.actor", "# Reasoning Path:\nBarney & Friends -> tv.tv_program.regular_cast -> m.0bng6c0 -> tv.regular_tv_appearance.character -> Barney\n# Answer:\nBarney"], "ground_truth": ["Gianna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.14814814814814817, "path_precision": 0.2, "path_recall": 0.11764705882352941, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-741", "prediction": ["# Reasoning Path:\nThe Hunger Games -> film.film.starring -> m.0gw7kw1 -> film.performance.actor -> Stanley Tucci\n# Answer:\nStanley Tucci", "# Reasoning Path:\nThe Hunger Games -> film.film.starring -> m.0gw7kw1 -> film.performance.character -> Caesar Flickerman\n# Answer:\nCaesar Flickerman", "# Reasoning Path:\nThe Hunger Games -> film.film.starring -> m.0gw7kv0 -> film.performance.actor -> Jennifer Lawrence\n# Answer:\nJennifer Lawrence", "# Reasoning Path:\nThe Hunger Games -> film.film.starring -> m.0gw7kv7 -> film.performance.actor -> Liam Hemsworth\n# Answer:\nLiam Hemsworth", "# Reasoning Path:\nThe Hunger Games -> film.film.starring -> m.0gw7kv0 -> film.performance.character -> Katniss Everdeen\n# Answer:\nKatniss Everdeen", "# Reasoning Path:\nThe Hunger Games -> film.film.songs -> m.0j6q1bx -> film.film_song_relationship.performers -> Sting\n# Answer:\nSting", "# Reasoning Path:\nThe Hunger Games -> film.film.starring -> m.0gw7kv7 -> film.performance.character -> Gale Hawthorne\n# Answer:\nGale Hawthorne", "# Reasoning Path:\nThe Hunger Games -> award.award_nominated_work.award_nominations -> m.0z83x3f -> award.award_nomination.ceremony -> 2012 Teen Choice Awards\n# Answer:\n2012 Teen Choice Awards", "# Reasoning Path:\nThe Hunger Games -> film.film.songs -> m.0j6q1bx -> film.film_song_relationship.composition -> Deep in the Meadow (Lullaby)\n# Answer:\nDeep in the Meadow (Lullaby)", "# Reasoning Path:\nThe Hunger Games -> award.award_nominated_work.award_nominations -> m.0z83x3f -> award.award_nomination.award_nominee -> Jennifer Lawrence\n# Answer:\nJennifer Lawrence"], "ground_truth": ["Stanley Tucci"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-742", "prediction": ["# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> location.location.containedby -> Adams County\n# Answer:\nGettysburg", "# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> location.location.containedby -> Pennsylvania\n# Answer:\nGettysburg", "# Reasoning Path:\nGettysburg Address -> common.topic.notable_for -> g.1255f923w\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nGettysburg Address -> common.topic.article -> m.037z4\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> location.location.contains -> Sheads House\n# Answer:\nGettysburg", "# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> travel.travel_destination.tourist_attractions -> Gettysburg National Cemetery\n# Answer:\nGettysburg", "# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> location.location.contains -> Dobbin House Tavern\n# Answer:\nGettysburg", "# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> location.location.contains -> Eisenhower National Historic Site\n# Answer:\nGettysburg", "# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> travel.travel_destination.tourist_attractions -> American Civil War Museum\n# Answer:\nGettysburg", "# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> travel.travel_destination.tourist_attractions -> Eisenhower National Historic Site\n# Answer:\nGettysburg"], "ground_truth": ["Gettysburg"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-743", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.sibling_s -> m.0pbyccm\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.sibling_s -> m.0pbycd1\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Caroline Kennedy -> people.person.sibling_s -> m.07ydm_0\n# Answer:\nCaroline Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.sibling_s -> m.0pbycd7\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> John F. Kennedy Jr. -> people.person.sibling_s -> m.07ydm_0\n# Answer:\nJohn F. Kennedy Jr.", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> John F. Kennedy Jr. -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nJohn F. Kennedy Jr.", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> White people -> medicine.risk_factor.diseases -> Breast cancer\n# Answer:\nWhite people", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Caroline Kennedy -> people.person.sibling_s -> m.07ydm_5\n# Answer:\nCaroline Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> freebase.valuenotation.has_no_value -> Children\n# Answer:\nArabella Kennedy"], "ground_truth": ["Ted Kennedy", "Joseph P. Kennedy, Jr.", "Robert F. Kennedy"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-744", "prediction": ["# Reasoning Path:\nTupac Shakur -> music.artist.label -> Interscope Records -> common.topic.notable_types -> Record label\n# Answer:\nInterscope Records", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Death Row Records -> common.topic.notable_types -> Record label\n# Answer:\nDeath Row Records", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Death Row Records -> music.record_label.artist -> Young Noble\n# Answer:\nDeath Row Records", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Death Row Records -> organization.organization.founders -> Suge Knight\n# Answer:\nDeath Row Records", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Amaru Entertainment -> common.topic.webpage -> m.04m3s72\n# Answer:\nAmaru Entertainment", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Amaru Entertainment -> common.topic.notable_for -> g.1256c6brp\n# Answer:\nAmaru Entertainment", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Death Row Records -> music.record_label.artist -> Bad Azz\n# Answer:\nDeath Row Records", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Death Row Records -> music.record_label.artist -> Big Syke\n# Answer:\nDeath Row Records", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Amaru Entertainment -> common.topic.webpage -> m.09wt6dk\n# Answer:\nAmaru Entertainment", "# Reasoning Path:\nTupac Shakur -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality"], "ground_truth": ["Interscope Records", "Amaru Entertainment", "Death Row Records"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-745", "prediction": ["# Reasoning Path:\nAugustus -> people.person.profession -> Roman emperor -> people.profession.people_with_this_profession -> Vitellius\n# Answer:\nRoman emperor", "# Reasoning Path:\nAugustus -> people.person.profession -> Roman emperor -> common.topic.article -> m.01g152\n# Answer:\nRoman emperor", "# Reasoning Path:\nAugustus -> people.person.profession -> Roman emperor -> common.topic.notable_types -> Character Occupation\n# Answer:\nRoman emperor", "# Reasoning Path:\nAugustus -> people.person.profession -> Roman emperor -> people.profession.people_with_this_profession -> Antoninus Pius\n# Answer:\nRoman emperor", "# Reasoning Path:\nAugustus -> people.person.profession -> Roman emperor -> people.profession.people_with_this_profession -> Caligula\n# Answer:\nRoman emperor", "# Reasoning Path:\nAugustus -> people.person.profession -> Politician -> people.profession.specializations -> Tyrant\n# Answer:\nPolitician", "# Reasoning Path:\nAugustus -> people.person.profession -> Politician -> people.profession.specializations -> AIDS Policy Specialist\n# Answer:\nPolitician", "# Reasoning Path:\nAugustus -> people.person.profession -> Politician -> common.topic.subjects -> Nathalie Kosciusko-Morizet\n# Answer:\nPolitician", "# Reasoning Path:\nAugustus -> people.person.profession -> Politician -> people.profession.specializations -> Advocacy\n# Answer:\nPolitician", "# Reasoning Path:\nAugustus -> common.topic.image -> Statue-Augustus -> common.image.appears_in_topic_gallery -> Augustus of Prima Porta\n# Answer:\nStatue-Augustus"], "ground_truth": ["Politician", "Roman emperor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-747", "prediction": ["# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Belize\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Achawa language -> language.human_language.language_family -> Macro-Arawakan languages\n# Answer:\nAchawa language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Achawa language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nAchawa language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Chile\n# Answer:\nSpanish Language"], "ground_truth": ["Guahibo language", "Cumeral Language", "Siona Language", "Cubeo Language", "Cuiba language", "Desano Language", "Andoque Language", "Nukak language", "Ember\u00e1, Northern Language", "Cof\u00e1n Language", "Uwa language", "Awa-Cuaiquer Language", "Minica Huitoto", "Cams\u00e1 Language", "Totoro Language", "Tunebo, Central Language", "Malayo Language", "Nonuya language", "Piapoco Language", "Cabiyar\u00ed Language", "Romani, Vlax Language", "Quechua, Napo Lowland Language", "Carijona Language", "Bora Language", "Chipiajes Language", "Piratapuyo Language", "Yucuna Language", "Guanano Language", "Siriano Language", "Baudo language", "Anserma Language", "Wayuu Language", "Ocaina Language", "Ponares Language", "Macagu\u00e1n Language", "Barasana Language", "Arhuaco Language", "Yukpa Language", "S\u00e1liba Language", "Inga, Jungle Language", "Achawa language", "Koreguaje Language", "Spanish Language", "Tinigua language", "Puinave Language", "Pijao Language", "Runa Language", "Guambiano Language", "Tama Language", "Macaguaje Language", "Piaroa Language", "Murui Huitoto language", "Cagua Language", "Curripaco Language", "Palenquero Language", "Kogi Language", "Ticuna language", "Tunebo, Western Language", "Macuna Language", "Andaqui Language", "Tuyuca language", "Muinane Language", "Carabayo Language", "Providencia Sign Language", "Nheengatu language", "Natagaimas Language", "Playero language", "Coxima Language", "P\u00e1ez language", "Tucano Language", "Coyaima Language", "Colombian Sign Language", "Cocama language", "Hupd\u00eb Language", "Inga Language", "Islander Creole English", "Kuna, Border Language", "Waimaj\u00e3 Language", "Tomedes Language", "Bar\u00ed Language", "Tanimuca-Retuar\u00e3 Language", "Tunebo, Barro Negro Language", "Guayabero Language", "Omejes Language", "Catio language", "Tunebo, Angosturas Language"], "ans_acc": 0.023255813953488372, "ans_hit": 1, "ans_f1": 0.04545454545454545, "ans_precission": 1.0, "ans_recall": 0.023255813953488372, "path_f1": 0.04552845528455284, "path_precision": 0.7, "path_recall": 0.023529411764705882, "path_ans_f1": 0.04545454545454545, "path_ans_precision": 1.0, "path_ans_recall": 0.023255813953488372}
{"id": "WebQTest-748", "prediction": ["# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12301 -> common.topic.notable_for -> g.125fmlxn3\n# Answer:\n12301", "# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12301 -> location.postal_code.country -> United States of America\n# Answer:\n12301", "# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12302 -> location.location.geometry -> m.058lwr7\n# Answer:\n12302", "# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12301 -> common.topic.notable_types -> Postal Code\n# Answer:\n12301", "# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12303 -> location.location.geometry -> m.058lwmk\n# Answer:\n12303", "# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12302 -> location.postal_code.country -> United States of America\n# Answer:\n12302", "# Reasoning Path:\nSchenectady -> location.statistical_region.population -> g.11b66fk06n\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12302 -> location.location.contains -> Schenectady County Airport\n# Answer:\n12302", "# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12303 -> common.topic.notable_for -> g.125bxt773\n# Answer:\n12303", "# Reasoning Path:\nSchenectady -> location.location.contains -> Bayou Cafe -> common.topic.notable_for -> g.125bv1wbt\n# Answer:\nBayou Cafe"], "ground_truth": ["12325", "12306", "12303", "12308", "12304", "12345", "12307", "12302", "12309", "12301", "12305"], "ans_acc": 0.2727272727272727, "ans_hit": 1, "ans_f1": 0.4067796610169491, "ans_precission": 0.8, "ans_recall": 0.2727272727272727, "path_f1": 0.4067796610169491, "path_precision": 0.8, "path_recall": 0.2727272727272727, "path_ans_f1": 0.4067796610169491, "path_ans_precision": 0.8, "path_ans_recall": 0.2727272727272727}
{"id": "WebQTest-749", "prediction": ["# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx8cf -> government.government_position_held.office_holder -> Richard Nixon\n# Answer:\nRichard Nixon", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx817 -> government.government_position_held.office_holder -> Harry S. Truman\n# Answer:\nHarry S. Truman", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03q2h_9 -> government.government_position_held.office_holder -> Franklin D. Roosevelt\n# Answer:\nFranklin D. Roosevelt", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx8cf -> government.government_position_held.basic_title -> President\n# Answer:\nPresident", "# Reasoning Path:\nPresident of the United States -> freebase.equivalent_topic.equivalent_type -> US President -> freebase.type_profile.kind -> Title\n# Answer:\nUS President", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx817 -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03q2h_9 -> government.government_position_held.basic_title -> President\n# Answer:\nPresident", "# Reasoning Path:\nPresident of the United States -> freebase.equivalent_topic.equivalent_type -> US President -> type.type.properties -> Vice president\n# Answer:\nUS President", "# Reasoning Path:\nPresident of the United States -> freebase.equivalent_topic.equivalent_type -> US President -> type.type.expected_by -> US Presidents\n# Answer:\nUS President", "# Reasoning Path:\nPresident of the United States -> freebase.equivalent_topic.equivalent_type -> US President -> freebase.type_profile.kind -> Significant\n# Answer:\nUS President"], "ground_truth": ["Ronald Reagan"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Old age\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Tobacco smoking\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.risk_factors -> Old age\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> African American\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.treatments -> Surgery\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.risk_factors -> Tobacco smoking\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf9g\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nPancreatic cancer"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-751", "prediction": ["# Reasoning Path:\nGoogle -> organization.organization.founders -> Sergey Brin -> people.person.ethnicity -> Jewish people\n# Answer:\nSergey Brin", "# Reasoning Path:\nGoogle -> organization.organization.founders -> Sergey Brin -> law.inventor.inventions -> Google Search\n# Answer:\nSergey Brin", "# Reasoning Path:\nGoogle -> organization.organization.founders -> Larry Page -> influence.influence_node.influenced -> Mukil Elango\n# Answer:\nLarry Page", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> organization.organization.founders -> Larry Page\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> organization.organization.founders -> Sergey Brin -> law.inventor.inventions -> PageRank\n# Answer:\nSergey Brin", "# Reasoning Path:\nGoogle -> organization.organization.founders -> Larry Page -> people.person.nationality -> United States of America\n# Answer:\nLarry Page", "# Reasoning Path:\nGoogle -> internet.website_owner.websites_owned_new -> m.010g4by1 -> internet.website_ownership.website -> Like.com\n# Answer:\nLike.com", "# Reasoning Path:\nGoogle -> organization.organization.founders -> Larry Page -> business.board_member.leader_of -> Google Apps for Work\n# Answer:\nLarry Page", "# Reasoning Path:\nGoogle -> organization.organization.founders -> Larry Page -> business.board_member.leader_of -> m.0w2mf_w\n# Answer:\nLarry Page", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> organization.organization_partnership.members -> Redcorus\n# Answer:\nGoogle Apps for Work"], "ground_truth": ["Nikesh Arora", "Vic Gundotra", "David Drummond", "Patrick Pichette", "Larry Page"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.26666666666666666, "ans_precission": 0.4, "ans_recall": 0.2, "path_f1": 0.23529411764705882, "path_precision": 0.4, "path_recall": 0.16666666666666666, "path_ans_f1": 0.28571428571428575, "path_ans_precision": 0.5, "path_ans_recall": 0.2}
{"id": "WebQTest-752", "prediction": ["# Reasoning Path:\nJohn Pemberton -> law.inventor.inventions -> Coca-Cola -> food.food.nutrients -> m.07xb760\n# Answer:\nCoca-Cola", "# Reasoning Path:\nJohn Pemberton -> law.inventor.inventions -> Coca-Cola -> base.popstra.product.sold_to -> m.064hhfy\n# Answer:\nCoca-Cola", "# Reasoning Path:\nJohn Pemberton -> law.inventor.inventions -> Coca-Cola -> food.food.nutrients -> m.07xb76d\n# Answer:\nCoca-Cola", "# Reasoning Path:\nJohn Pemberton -> law.inventor.inventions -> Coca-Cola -> common.topic.webpage -> m.02sr4y1\n# Answer:\nCoca-Cola", "# Reasoning Path:\nJohn Pemberton -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nJohn Pemberton -> law.inventor.inventions -> Coca-Cola -> food.food.nutrients -> m.07xb76s\n# Answer:\nCoca-Cola", "# Reasoning Path:\nJohn Pemberton -> symbols.name_source.namesakes -> Pemberton Place -> location.location.geolocation -> m.01227nwv\n# Answer:\nPemberton Place", "# Reasoning Path:\nJohn Pemberton -> law.inventor.inventions -> Coca-Cola -> common.topic.webpage -> m.09x_pq4\n# Answer:\nCoca-Cola", "# Reasoning Path:\nJohn Pemberton -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Alaska\n# Answer:\nUnited States of America", "# Reasoning Path:\nJohn Pemberton -> law.inventor.inventions -> Coca-Cola -> common.topic.webpage -> m.09xl5g4\n# Answer:\nCoca-Cola"], "ground_truth": ["Coca-Cola"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-753", "prediction": ["# Reasoning Path:\nSir Stanley Matthews -> soccer.football_player.position_s -> Forward -> sports.sports_position.sport -> Football\n# Answer:\nForward", "# Reasoning Path:\nSir Stanley Matthews -> sports.pro_athlete.sports_played_professionally -> m.0d5fbrd -> sports.pro_sports_played.sport -> Football\n# Answer:\nFootball", "# Reasoning Path:\nSir Stanley Matthews -> common.topic.article -> m.013s75\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nSir Stanley Matthews -> soccer.football_player.position_s -> Forward -> sports.sport.pro_athletes -> m.011p3wnv\n# Answer:\nForward", "# Reasoning Path:\nSir Stanley Matthews -> soccer.football_player.position_s -> Forward -> common.topic.webpage -> m.09w2grw\n# Answer:\nForward", "# Reasoning Path:\nSir Stanley Matthews -> soccer.football_player.position_s -> Outside forward -> sports.sports_position.players -> m.011l0tb4\n# Answer:\nOutside forward", "# Reasoning Path:\nSir Stanley Matthews -> soccer.football_player.position_s -> Outside forward -> common.topic.image -> m.02gydjp\n# Answer:\nOutside forward", "# Reasoning Path:\nSir Stanley Matthews -> sports.pro_athlete.sports_played_professionally -> m.0gvc2fg -> sports.pro_sports_played.sport -> Tennis\n# Answer:\nTennis", "# Reasoning Path:\nSir Stanley Matthews -> soccer.football_player.position_s -> Outside forward -> common.topic.notable_types -> Football Position\n# Answer:\nOutside forward", "# Reasoning Path:\nSir Stanley Matthews -> soccer.football_player.position_s -> Midfielder -> sports.sports_position.sport -> Football\n# Answer:\nMidfielder"], "ground_truth": ["Outside forward", "Forward", "Midfielder"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-755", "prediction": ["# Reasoning Path:\nCharles Edward Stuart -> people.person.parents -> James Francis Edward Stuart -> people.person.parents -> Mary of Modena\n# Answer:\nJames Francis Edward Stuart", "# Reasoning Path:\nCharles Edward Stuart -> people.person.parents -> James Francis Edward Stuart -> people.person.children -> Henry Benedict Stuart\n# Answer:\nJames Francis Edward Stuart", "# Reasoning Path:\nCharles Edward Stuart -> people.person.parents -> Maria Clementina Sobieska -> people.person.children -> Henry Benedict Stuart\n# Answer:\nMaria Clementina Sobieska", "# Reasoning Path:\nCharles Edward Stuart -> people.person.parents -> James Francis Edward Stuart -> people.person.parents -> James II of England\n# Answer:\nJames Francis Edward Stuart", "# Reasoning Path:\nCharles Edward Stuart -> people.person.parents -> James Francis Edward Stuart -> people.person.nationality -> England\n# Answer:\nJames Francis Edward Stuart", "# Reasoning Path:\nCharles Edward Stuart -> people.person.parents -> Maria Clementina Sobieska -> common.topic.image -> Maria Klementyna Sobieska\n# Answer:\nMaria Clementina Sobieska", "# Reasoning Path:\nCharles Edward Stuart -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nCharles Edward Stuart -> common.topic.notable_for -> g.125fbmkkn\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCharles Edward Stuart -> people.person.parents -> Maria Clementina Sobieska -> common.topic.article -> m.0155dl\n# Answer:\nMaria Clementina Sobieska", "# Reasoning Path:\nCharles Edward Stuart -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Kidney cancer\n# Answer:\nMale"], "ground_truth": ["Charles Edward Stuart"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-756", "prediction": ["# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.02ntr0h\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> people.person.profession -> Voice Actor\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.010gysx9\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> tv.tv_producer.programs_produced -> m.010hsx_r\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.02kk65p\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nStewie Griffin -> tv.tv_character.appeared_in_tv_program -> m.02kk65p -> tv.regular_tv_appearance.seasons -> Family Guy - Season 1\n# Answer:\nFamily Guy - Season 1", "# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.siblings -> m.05sr55v -> fictional_universe.sibling_relationship_of_fictional_characters.siblings -> Bertram Griffin\n# Answer:\nBertram Griffin", "# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> tv.tv_producer.programs_produced -> m.01140l3r\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nStewie Griffin -> tv.tv_character.appeared_in_tv_program -> m.02kk65p -> tv.regular_tv_appearance.series -> Family Guy\n# Answer:\nFamily Guy", "# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> tv.tv_producer.programs_produced -> m.0bgp7jt\n# Answer:\nSeth MacFarlane"], "ground_truth": ["Seth MacFarlane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-757", "prediction": ["# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Marcheline Bertrand -> people.person.parents -> Rolland F. Bertrand\n# Answer:\nMarcheline Bertrand", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Marcheline Bertrand -> people.person.parents -> Lois June Gouwens\n# Answer:\nMarcheline Bertrand", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Marcheline Bertrand -> tv.tv_actor.guest_roles -> m.09p2yk0\n# Answer:\nMarcheline Bertrand", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Jon Voight -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nJon Voight", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Marcheline Bertrand -> people.person.children -> James Haven\n# Answer:\nMarcheline Bertrand", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Jon Voight -> people.person.children -> James Haven\n# Answer:\nJon Voight", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Jon Voight -> freebase.valuenotation.is_reviewed -> Children\n# Answer:\nJon Voight", "# Reasoning Path:\nAngelina Jolie -> award.award_nominee.award_nominations -> m.010wr37v -> award.award_nomination.nominated_for -> Maleficent\n# Answer:\nMaleficent", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Jon Voight -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nJon Voight", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Jon Voight -> people.person.profession -> Film Producer\n# Answer:\nJon Voight"], "ground_truth": ["Jon Voight", "Marcheline Bertrand"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-758", "prediction": ["# Reasoning Path:\nGovernor of Indiana -> common.topic.image -> Mitch Daniels -> government.politician.government_positions_held -> m.04l4slb\n# Answer:\nMitch Daniels", "# Reasoning Path:\nGovernor of Indiana -> common.topic.image -> Mitch Daniels -> common.image.appears_in_topic_gallery -> Indiana gubernatorial election, 2008\n# Answer:\nMitch Daniels", "# Reasoning Path:\nGovernor of Indiana -> common.topic.image -> Mitch Daniels -> government.political_appointer.appointees -> m.010hbc1b\n# Answer:\nMitch Daniels", "# Reasoning Path:\nGovernor of Indiana -> government.government_office_or_title.office_holders -> m.04l4slb -> government.government_position_held.office_holder -> Mitch Daniels\n# Answer:\nMitch Daniels", "# Reasoning Path:\nGovernor of Indiana -> government.government_office_or_title.office_holders -> m.04l4s5v -> government.government_position_held.office_holder -> James B. Ray\n# Answer:\nJames B. Ray", "# Reasoning Path:\nGovernor of Indiana -> government.government_office_or_title.jurisdiction -> Indiana -> common.topic.image -> Indiana.png\n# Answer:\nIndiana", "# Reasoning Path:\nGovernor of Indiana -> government.government_office_or_title.office_holders -> m.04l4slb -> government.government_position_held.jurisdiction_of_office -> Indiana\n# Answer:\nIndiana", "# Reasoning Path:\nGovernor of Indiana -> government.government_office_or_title.office_holders -> m.04l4s5v -> government.government_position_held.jurisdiction_of_office -> Indiana\n# Answer:\nIndiana", "# Reasoning Path:\nGovernor of Indiana -> government.government_office_or_title.office_holders -> m.04l4s5g -> government.government_position_held.office_holder -> Jonathan Jennings\n# Answer:\nJonathan Jennings", "# Reasoning Path:\nGovernor of Indiana -> government.government_office_or_title.jurisdiction -> Indiana -> common.topic.image -> Vista della riserva del fiume Salamonie nella Contea di Huntington, Indiana nord-orientale\n# Answer:\nIndiana"], "ground_truth": ["Mitch Daniels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-759", "prediction": ["# Reasoning Path:\nBrad Pitt -> base.popstra.celebrity.dated -> m.063g4f2 -> base.popstra.dated.participant -> Angelina Jolie\n# Answer:\nAngelina Jolie", "# Reasoning Path:\nBrad Pitt -> base.popstra.celebrity.canoodled -> m.0652slx -> base.popstra.canoodled.participant -> Uma Thurman\n# Answer:\nUma Thurman", "# Reasoning Path:\nBrad Pitt -> base.popstra.celebrity.canoodled -> m.063h8xj -> base.popstra.canoodled.participant -> Julia Ormond\n# Answer:\nJulia Ormond", "# Reasoning Path:\nBrad Pitt -> film.actor.film -> g.11b6r79157\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nBrad Pitt -> base.popstra.celebrity.canoodled -> m.064bhn3 -> base.popstra.canoodled.participant -> Angelina Jolie\n# Answer:\nAngelina Jolie", "# Reasoning Path:\nBrad Pitt -> film.actor.film -> g.11bynvk8jk\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nBrad Pitt -> base.popstra.celebrity.dated -> m.064bhq6 -> base.popstra.dated.participant -> Gwyneth Paltrow\n# Answer:\nGwyneth Paltrow", "# Reasoning Path:\nBrad Pitt -> base.popstra.celebrity.dated -> m.064bhr2 -> base.popstra.dated.participant -> Thandie Newton\n# Answer:\nThandie Newton", "# Reasoning Path:\nBrad Pitt -> base.popstra.celebrity.dated -> m.064bhq6 -> base.popstra.dated.participant -> Geena Davis\n# Answer:\nGeena Davis", "# Reasoning Path:\nBrad Pitt -> base.popstra.celebrity.dated -> m.064bhr2 -> base.popstra.dated.participant -> Shalane McCall\n# Answer:\nShalane McCall"], "ground_truth": ["Thandie Newton", "Robin Givens", "Sinitta", "Shalane McCall", "Juliette Lewis"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.26666666666666666, "ans_precission": 0.2, "ans_recall": 0.4, "path_f1": 0.15384615384615385, "path_precision": 0.2, "path_recall": 0.125, "path_ans_f1": 0.26666666666666666, "path_ans_precision": 0.2, "path_ans_recall": 0.4}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Leonardo da Vinci and the Art of Sculpture: Inspiration and Invention\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Raphael and his age\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> people.profession.people_with_this_profession -> Sandro Botticelli\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper -> visual_art.artwork.art_subject -> Jesus Christ\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> influence.influence_node.influenced_by -> Melozzo da Forl\u00ec\n# Answer:\nAntonio da Correggio", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper -> visual_art.artwork.art_form -> Mural\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> St. John the Baptist -> visual_art.artwork.art_subject -> John the Baptist\n# Answer:\nSt. John the Baptist", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Bernardino Luini -> visual_art.visual_artist.art_forms -> Painting\n# Answer:\nBernardino Luini", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper -> visual_art.artwork.art_form -> Painting\n# Answer:\nThe Last Supper"], "ground_truth": ["Leda and the Swan", "Portrait of Isabella d'Este", "Lady with an Ermine", "Ginevra de' Benci", "g.1239jd9p", "Madonna Litta", "g.1224tf0c", "Portrait of a Musician", "Madonna of the Carnation", "The Holy Infants Embracing", "g.12314dm1", "The Virgin and Child with St. Anne", "St. John the Baptist", "Benois Madonna", "Madonna of the Yarnwinder", "g.121yh91r", "Madonna and Child with St Joseph", "Bacchus", "Medusa", "Salvator Mundi", "Vitruvian Man", "The Last Supper", "Head of a Woman", "g.121wt37c", "Adoration of the Magi", "Drapery for a Seated Figure", "g.120vt1gz", "Portrait of a man in red chalk", "Virgin of the Rocks", "La belle ferronni\u00e8re", "Sala delle Asse", "Portrait of a Young Fianc\u00e9e", "Lucan portrait of Leonardo da Vinci", "g.1213jb_b", "The Battle of Anghiari", "Horse and Rider", "g.1219sb0g", "Annunciation", "The Virgin and Child with St Anne and St John the Baptist", "Madonna of Laroque", "g.12215rxg", "Leonardo's horse", "St. Jerome in the Wilderness", "The Baptism of Christ", "Mona Lisa"], "ans_acc": 0.044444444444444446, "ans_hit": 1, "ans_f1": 0.07999999999999999, "ans_precission": 0.4, "ans_recall": 0.044444444444444446, "path_f1": 0.0816326530612245, "path_precision": 0.4, "path_recall": 0.045454545454545456, "path_ans_f1": 0.07999999999999999, "path_ans_precision": 0.4, "path_ans_recall": 0.044444444444444446}
{"id": "WebQTest-760", "prediction": ["# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.party -> Democratic Party\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> government.politician.party -> m.03gjhww -> government.political_party_tenure.party -> Democratic Party\n# Answer:\nDemocratic Party", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.venues -> Pepsi Center\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.speeches -> m.04j00hr\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> government.politician.party -> m.03gjhww -> freebase.valuenotation.is_reviewed -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.venues -> Sports Authority Field at Mile High\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.speeches -> m.04j00j0\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.subjects -> 2012 Presidential debate\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> A Bound Man -> book.written_work.subjects -> United States of America\n# Answer:\nA Bound Man", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.speeches -> m.04j00j9\n# Answer:\n2008 Democratic National Convention"], "ground_truth": ["Democratic Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.2, "path_recall": 0.3333333333333333, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-761", "prediction": ["# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94107 -> location.location.containedby -> United States of America\n# Answer:\n94107", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94107 -> common.topic.notable_types -> Postal Code\n# Answer:\n94107", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94120 -> common.topic.notable_for -> g.125925x8_\n# Answer:\n94120", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94107 -> location.location.containedby -> California\n# Answer:\n94107", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94107 -> location.postal_code.country -> United States of America\n# Answer:\n94107", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94118 -> location.location.containedby -> California\n# Answer:\n94118", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94120 -> location.postal_code.country -> United States of America\n# Answer:\n94120", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94107 -> location.location.containedby -> South of Market\n# Answer:\n94107", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94120 -> common.topic.notable_types -> Postal Code\n# Answer:\n94120", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94118 -> location.location.containedby -> United States of America\n# Answer:\n94118"], "ground_truth": ["94123", "94111", "94160", "94129", "94151", "94132", "94118-3208", "94125", "94117", "94145", "94118", "94131", "94115", "94137", "94117-2212", "94199", "94121", "94142", "94114", "94177", "94109", "94107-7621", "94144", "94140", "94127", "94116", "94102-4733", "94104", "94105", "94122", "94172", "94153", "94120", "94188", "94124", "94154", "94163", "94130", "94119", "94108", "94101", "94147", "94139", "94107", "94134", "94171", "94126", "94158", "94102", "94110", "94143", "94146", "94133", "94161", "94159", "94112", "94156", "94164", "94162", "94103", "94141", "94128"], "ans_acc": 0.04838709677419355, "ans_hit": 1, "ans_f1": 0.09230769230769231, "ans_precission": 1.0, "ans_recall": 0.04838709677419355, "path_f1": 0.09230769230769231, "path_precision": 1.0, "path_recall": 0.04838709677419355, "path_ans_f1": 0.09230769230769231, "path_ans_precision": 1.0, "path_ans_recall": 0.04838709677419355}
{"id": "WebQTest-762", "prediction": ["# Reasoning Path:\nStephen Ireland -> soccer.football_player.statistics -> m.0w9mhjv -> soccer.football_player_stats.team -> Newcastle United F.C.\n# Answer:\nNewcastle United F.C.", "# Reasoning Path:\nStephen Ireland -> base.schemastaging.athlete_extra.salary -> m.0lh5l0q -> base.schemastaging.athlete_salary.team -> Aston Villa F.C.\n# Answer:\nAston Villa F.C.", "# Reasoning Path:\nStephen Ireland -> soccer.football_player.statistics -> m.0w8wygw -> soccer.football_player_stats.team -> Manchester City F.C.\n# Answer:\nManchester City F.C.", "# Reasoning Path:\nStephen Ireland -> soccer.football_player.statistics -> m.0w9mf8d -> soccer.football_player_stats.team -> Aston Villa F.C.\n# Answer:\nAston Villa F.C.", "# Reasoning Path:\nStephen Ireland -> base.schemastaging.athlete_extra.salary -> m.0lh5l0q -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nPound sterling", "# Reasoning Path:\nStephen Ireland -> sports.pro_athlete.teams -> m.04m44tt -> sports.sports_team_roster.team -> Manchester City F.C.\n# Answer:\nManchester City F.C.", "# Reasoning Path:\nStephen Ireland -> sports.pro_athlete.teams -> m.0lf9x6n -> sports.sports_team_roster.team -> Aston Villa F.C.\n# Answer:\nAston Villa F.C.", "# Reasoning Path:\nStephen Ireland -> sports.pro_athlete.teams -> m.04m44tt -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nStephen Ireland -> sports.pro_athlete.teams -> m.0lf9x6n -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nMidfielder", "# Reasoning Path:\nStephen Ireland -> sports.pro_athlete.teams -> m.0lgppdy -> sports.sports_team_roster.team -> Newcastle United F.C.\n# Answer:\nNewcastle United F.C."], "ground_truth": ["Stoke City F.C.", "Aston Villa F.C.", "Newcastle United F.C.", "Manchester City F.C."], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7241379310344827, "ans_precission": 0.7, "ans_recall": 0.75, "path_f1": 0.7777777777777777, "path_precision": 0.7, "path_recall": 0.875, "path_ans_f1": 0.7241379310344827, "path_ans_precision": 0.7, "path_ans_recall": 0.75}
{"id": "WebQTest-764", "prediction": ["# Reasoning Path:\nJohn Mayer -> people.person.education -> m.0n0v79l -> education.education.institution -> Fairfield Warde High School\n# Answer:\nFairfield Warde High School", "# Reasoning Path:\nJohn Mayer -> people.person.education -> m.02wnjyy -> education.education.institution -> Berklee College of Music\n# Answer:\nBerklee College of Music", "# Reasoning Path:\nJohn Mayer -> people.person.education -> m.0h30083 -> education.education.institution -> Brien McMahon High School\n# Answer:\nBrien McMahon High School", "# Reasoning Path:\nJohn Mayer -> common.topic.webpage -> http://www.discogs.com/artist/John+Mayer -> common.webpage.category -> Topic Webpage\n# Answer:\nhttp://www.discogs.com/artist/John+Mayer", "# Reasoning Path:\nJohn Mayer -> tv.tv_actor.guest_roles -> m.0bv_4q4 -> tv.tv_guest_role.episodes_appeared_in -> Show #2662\n# Answer:\nShow #2662", "# Reasoning Path:\nJohn Mayer -> common.topic.webpage -> m.09w5t6j -> common.webpage.resource -> Celebrity date night at John Mayer's 'revue'\n# Answer:\nCelebrity date night at John Mayer's 'revue'", "# Reasoning Path:\nJohn Mayer -> common.topic.webpage -> http://www.discogs.com/artist/John+Mayer -> common.webpage.resource -> m.0bm4m94\n# Answer:\nhttp://www.discogs.com/artist/John+Mayer", "# Reasoning Path:\nJohn Mayer -> common.topic.webpage -> m.09w5t6j -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nJohn Mayer -> common.topic.webpage -> m.09w6d63 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nJohn Mayer -> tv.tv_actor.guest_roles -> m.0bv_shf -> tv.tv_guest_role.episodes_appeared_in -> Dave Chapelle, John Mayer\n# Answer:\nDave Chapelle, John Mayer"], "ground_truth": ["Berklee College of Music"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-765", "prediction": ["# Reasoning Path:\nBrody Jenner -> base.popstra.celebrity.canoodled -> m.0652s8g -> base.popstra.canoodled.participant -> Paris Hilton\n# Answer:\nParis Hilton", "# Reasoning Path:\nBrody Jenner -> base.popstra.celebrity.dated -> m.065q5tn -> base.popstra.dated.participant -> Lauren Conrad\n# Answer:\nLauren Conrad", "# Reasoning Path:\nBrody Jenner -> base.popstra.celebrity.canoodled -> m.063k2n8 -> base.popstra.canoodled.participant -> Paris Hilton\n# Answer:\nParis Hilton", "# Reasoning Path:\nBrody Jenner -> base.popstra.celebrity.canoodled -> m.063k2nm -> base.popstra.canoodled.participant -> Lauren Conrad\n# Answer:\nLauren Conrad", "# Reasoning Path:\nBrody Jenner -> base.popstra.celebrity.dated -> m.063g5df -> base.popstra.dated.participant -> Paris Hilton\n# Answer:\nParis Hilton", "# Reasoning Path:\nBrody Jenner -> base.popstra.celebrity.dated -> m.065pvwb -> base.popstra.dated.participant -> Ryan J. Parker\n# Answer:\nRyan J. Parker", "# Reasoning Path:\nBrody Jenner -> tv.tv_actor.guest_roles -> m.09nm66k -> tv.tv_guest_role.episodes_appeared_in -> You Know What You Did\n# Answer:\nYou Know What You Did", "# Reasoning Path:\nBrody Jenner -> tv.tv_actor.guest_roles -> m.0g0f2wz -> tv.tv_guest_role.episodes_appeared_in -> Brody Jenner, Steve Harvey, Chuck Nice, Patrick Meagher\n# Answer:\nBrody Jenner, Steve Harvey, Chuck Nice, Patrick Meagher"], "ground_truth": ["Cora Skinner", "Lauren Conrad", "Jayde Nicole", "Nicole Richie", "Kristin Cavallari", "Ryan J. Parker", "Haylie Duff", "Paris Hilton"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.75, "ans_recall": 0.375, "path_f1": 0.48, "path_precision": 0.75, "path_recall": 0.35294117647058826, "path_ans_f1": 0.5, "path_ans_precision": 0.75, "path_ans_recall": 0.375}
{"id": "WebQTest-766", "prediction": ["# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.webpage -> m.03lfx0d\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.notable_types -> Currency\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.notable_for -> g.1258v8707\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.statistical_region.size_of_armed_forces -> g.12cp_jm8t\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.webpage -> m.04m10rm\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.statistical_region.gdp_nominal_per_capita -> g.11b60thpy3\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nMalaysia -> location.statistical_region.size_of_armed_forces -> g.1hhc37x7v\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nMalaysia -> location.statistical_region.size_of_armed_forces -> g.1hhc39wxf\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nMalaysia -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6gh1z\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nMalaysia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc38hmv\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita"], "ground_truth": ["Malaysian ringgit"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-768", "prediction": ["# Reasoning Path:\nVolkswagen Passenger Cars -> organization.organization.parent -> m.04kjktn -> organization.organization_relationship.parent -> Volkswagen Group\n# Answer:\nVolkswagen Group", "# Reasoning Path:\nVolkswagen Passenger Cars -> organization.organization.leadership -> m.0vbmqq2 -> organization.leadership.person -> Martin Winterkorn\n# Answer:\nMartin Winterkorn", "# Reasoning Path:\nVolkswagen Passenger Cars -> organization.organization.leadership -> m.0vbmqq2 -> organization.leadership.role -> Chairman\n# Answer:\nChairman", "# Reasoning Path:\nVolkswagen Passenger Cars -> organization.organization.leadership -> m.0vbmqq2 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nVolkswagen Passenger Cars -> common.topic.webpage -> m.09vk8xf -> common.webpage.resource -> Volkswagen Reviews and Pricing\n# Answer:\nVolkswagen Reviews and Pricing", "# Reasoning Path:\nVolkswagen Passenger Cars -> common.topic.webpage -> m.01xwtq5 -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nVolkswagen Passenger Cars -> common.topic.webpage -> m.01xwtq5 -> common.webpage.resource -> m.0bkfyrf\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nVolkswagen Passenger Cars -> common.topic.webpage -> m.09vk8xf -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage"], "ground_truth": ["Volkswagen Group"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.125, "path_recall": 0.3333333333333333, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-769", "prediction": ["# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> aviation.airport.serves -> Ciampino\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> aviation.airport.hub_for -> Ryanair\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> common.topic.article -> m.033_5b\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> aviation.airport.hub_for -> Aerolinee Itavia\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Urbe Airport -> location.location.geolocation -> m.04fngwt\n# Answer:\nUrbe Airport", "# Reasoning Path:\nRome -> travel.travel_destination.how_to_get_here -> m.0p7qds5 -> travel.transportation.transport_operator -> ATAC SpA\n# Answer:\nATAC SpA", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> aviation.airport.hub_for -> Mistral Air\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Urbe Airport -> aviation.airport.hub_for -> Ala Littoria\n# Answer:\nUrbe Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Italian Met Office Airport -> location.location.geolocation -> m.04fngkg\n# Answer:\nItalian Met Office Airport", "# Reasoning Path:\nRome -> travel.travel_destination.how_to_get_here -> m.052lyyd -> travel.transportation.mode_of_transportation -> Train\n# Answer:\nTrain"], "ground_truth": ["Roma Termini railway station"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vorarlberg -> base.aareas.schema.administrative_area.administrative_children -> Feldkirch District\n# Answer:\nVorarlberg", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vorarlberg -> base.aareas.schema.administrative_area.administrative_children -> Bludenz District\n# Answer:\nVorarlberg", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vorarlberg -> base.aareas.schema.administrative_area.administrative_children -> Bregenz District\n# Answer:\nVorarlberg", "# Reasoning Path:\nAustria -> location.country.capital -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vorarlberg -> base.aareas.schema.administrative_area.administrative_children -> Feldkirch District\n# Answer:\nVorarlberg", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vorarlberg -> location.location.partially_contains -> Drei Schwestern\n# Answer:\nVorarlberg", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Burgenland -> base.aareas.schema.administrative_area.administrative_area_type -> Austrian state\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vorarlberg -> base.aareas.schema.administrative_area.administrative_children -> Bludenz District\n# Answer:\nVorarlberg"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-770", "prediction": ["# Reasoning Path:\nSteve McNair -> people.deceased_person.cause_of_death -> Ballistic trauma -> base.pethealth.cause.pet_diseases_or_conditions_caused -> Head Trauma in Animals\n# Answer:\nBallistic trauma", "# Reasoning Path:\nSteve McNair -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.article -> m.02qnd1f\n# Answer:\nBallistic trauma", "# Reasoning Path:\nSteve McNair -> people.deceased_person.cause_of_death -> Ballistic trauma -> base.schemastaging.context_name.pronunciation -> g.125_s3hjb\n# Answer:\nBallistic trauma", "# Reasoning Path:\nSteve McNair -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nSteve McNair -> people.person.gender -> Male -> medicine.risk_factor.diseases -> heart attack\n# Answer:\nMale", "# Reasoning Path:\nSteve McNair -> people.person.education -> m.02kybw3 -> education.education.institution -> Alcorn State University\n# Answer:\nAlcorn State University", "# Reasoning Path:\nSteve McNair -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nMale", "# Reasoning Path:\nSteve McNair -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nSteve McNair -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_98hd\n# Answer:\nMale", "# Reasoning Path:\nSteve McNair -> people.person.gender -> Male -> biology.hybrid_parent_gender.hybrids -> m.0zm9_69\n# Answer:\nMale"], "ground_truth": ["Ballistic trauma"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-771", "prediction": ["# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.agencies -> California Conservation Corps -> organization.organization.founders -> Jerry Brown\n# Answer:\nCalifornia Conservation Corps", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.agencies -> California Conservation Corps -> common.topic.notable_for -> g.1257jlrc9\n# Answer:\nCalifornia Conservation Corps", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.agencies -> 11-99 Foundation -> common.topic.notable_for -> g.1254xhy5c\n# Answer:\n11-99 Foundation", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.agencies -> Alameda County Superior Court -> location.location.street_address -> m.08d08r9\n# Answer:\nAlameda County Superior Court", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.agencies -> 11-99 Foundation -> common.topic.notable_types -> Government Agency\n# Answer:\n11-99 Foundation", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.agencies -> 11-99 Foundation -> common.topic.article -> m.027gbf3\n# Answer:\n11-99 Foundation", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.agencies -> Alameda County Superior Court -> location.location.containedby -> Alameda County\n# Answer:\nAlameda County Superior Court", "# Reasoning Path:\nCalifornia -> location.location.events -> 1987 Wine Country Film Festival -> freebase.valuenotation.has_value -> Closing date\n# Answer:\n1987 Wine Country Film Festival", "# Reasoning Path:\nCalifornia -> location.location.events -> 1987 Wine Country Film Festival -> time.event.locations -> Sonoma Valley\n# Answer:\n1987 Wine Country Film Festival", "# Reasoning Path:\nCalifornia -> location.location.events -> 1987 Wine Country Film Festival -> time.event.instance_of_recurring_event -> Wine Country Film Festival\n# Answer:\n1987 Wine Country Film Festival"], "ground_truth": ["Arnold Schwarzenegger"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-772", "prediction": ["# Reasoning Path:\nGhost Rider -> film.film.starring -> m.0jyrrc -> film.performance.actor -> Eva Mendes\n# Answer:\nEva Mendes", "# Reasoning Path:\nGhost Rider -> film.film.starring -> m.0hdy101 -> film.performance.actor -> Raquel Alessi\n# Answer:\nRaquel Alessi", "# Reasoning Path:\nGhost Rider -> film.film.starring -> m.0jyrrc -> film.performance.character -> Roxanne Simpson\n# Answer:\nRoxanne Simpson", "# Reasoning Path:\nGhost Rider -> film.film.starring -> m.03jrkkx -> film.performance.actor -> Brett Cullen\n# Answer:\nBrett Cullen", "# Reasoning Path:\nGhost Rider -> film.film.starring -> m.0hdy101 -> film.performance.character -> Roxanne Simpson\n# Answer:\nRoxanne Simpson", "# Reasoning Path:\nGhost Rider -> common.topic.webpage -> m.03l8z6p -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nGhost Rider -> film.film.starring -> m.03jrkkx -> film.performance.character -> Barton Blaze\n# Answer:\nBarton Blaze", "# Reasoning Path:\nGhost Rider -> common.topic.webpage -> m.03l8z6p -> common.webpage.resource -> m.0blx2k4\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nGhost Rider -> common.topic.webpage -> m.08xfwt6 -> common.webpage.resource -> ''Ghost Rider'' is No. 1 again\n# Answer:\n''Ghost Rider'' is No. 1 again", "# Reasoning Path:\nGhost Rider -> film.film.genre -> Thriller -> film.film_subject.films -> The Inspector\n# Answer:\nThriller"], "ground_truth": ["Eva Mendes", "Raquel Alessi"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-773", "prediction": ["# Reasoning Path:\nSony Mobile Communications -> organization.organization.headquarters -> m.040r1dz -> location.mailing_address.citytown -> Minato\n# Answer:\nMinato", "# Reasoning Path:\nSony Mobile Communications -> organization.organization.headquarters -> m.040r1dz -> location.mailing_address.state_province_region -> Kant\u014d region\n# Answer:\nKant\u014d region", "# Reasoning Path:\nSony Mobile Communications -> organization.organization.headquarters -> m.040r1dz -> common.topic.notable_types -> Location\n# Answer:\nLocation", "# Reasoning Path:\nSony Mobile Communications -> base.schemastaging.organization_extra.phone_number -> m.0109_8hh -> base.schemastaging.phone_sandbox.service_location -> Costa Rica\n# Answer:\nCosta Rica", "# Reasoning Path:\nSony Mobile Communications -> computer.computer_manufacturer_brand.computer_models -> Sony Ericsson Live with Walkman -> common.topic.article -> m.0n49dzq\n# Answer:\nSony Ericsson Live with Walkman", "# Reasoning Path:\nSony Mobile Communications -> computer.computer_manufacturer_brand.computer_models -> Sony Ericsson Xperia X10 -> common.topic.notable_types -> Consumer product\n# Answer:\nSony Ericsson Xperia X10", "# Reasoning Path:\nSony Mobile Communications -> base.schemastaging.organization_extra.phone_number -> m.0109_8hh -> base.schemastaging.phone_sandbox.service_language -> Spanish Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nSony Mobile Communications -> base.schemastaging.organization_extra.phone_number -> m.0109_8m_ -> base.schemastaging.phone_sandbox.service_language -> English Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nSony Mobile Communications -> base.schemastaging.organization_extra.phone_number -> m.0109_8hh -> base.schemastaging.phone_sandbox.contact_category -> Customer Service\n# Answer:\nCustomer Service", "# Reasoning Path:\nSony Mobile Communications -> base.schemastaging.organization_extra.phone_number -> m.0109_8t3 -> base.schemastaging.phone_sandbox.service_location -> Dominican Republic\n# Answer:\nDominican Republic"], "ground_truth": ["Minato"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-774", "prediction": ["# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.article -> m.04lt88\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.webpage -> m.04m2gwk\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.statistical_region.co2_emissions_per_capita -> g.1245_1qfx\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nDominican Republic -> location.statistical_region.internet_users_percent_population -> g.11b60vv5zn\n# Answer:\nlocation.statistical_region.internet_users_percent_population", "# Reasoning Path:\nDominican Republic -> location.statistical_region.co2_emissions_per_capita -> g.1245_22j7\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nDominican Republic -> location.statistical_region.co2_emissions_per_capita -> g.1245_4lf3\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nDominican Republic -> location.statistical_region.internet_users_percent_population -> g.1245_22jn\n# Answer:\nlocation.statistical_region.internet_users_percent_population", "# Reasoning Path:\nDominican Republic -> location.statistical_region.internet_users_percent_population -> g.1245_m66j\n# Answer:\nlocation.statistical_region.internet_users_percent_population"], "ground_truth": ["Dominican peso"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-775", "prediction": ["# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Manchester Museum -> common.topic.image -> The Manchester Museum\n# Answer:\nManchester Museum", "# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Bedford Mall\n# Answer:\nBedford Mall", "# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Manchester Museum -> location.location.containedby -> England\n# Answer:\nManchester Museum", "# Reasoning Path:\nManchester -> location.statistical_region.population -> g.11b66bsm6f\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Albert Hall, Manchester -> location.location.containedby -> England\n# Answer:\nAlbert Hall, Manchester", "# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Manchester Museum -> common.topic.image -> Manchester Museum by Nick Higham\n# Answer:\nManchester Museum", "# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Albert Hall, Manchester -> freebase.valuenotation.has_value -> Floors\n# Answer:\nAlbert Hall, Manchester", "# Reasoning Path:\nManchester -> location.statistical_region.population -> g.11btt603nx\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Manchester Museum -> location.location.containedby -> Europe\n# Answer:\nManchester Museum", "# Reasoning Path:\nManchester -> common.topic.webpage -> m.03l15rs -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website"], "ground_truth": ["Imperial War Museum North", "Old Trafford", "Manchester Jewish Museum", "Urbis", "Wheel of Manchester", "The Moon Under Water, Manchester", "Great Manchester Run", "Albert Hall, Manchester", "Manchester Art Gallery", "Manchester Museum", "Manchester Cathedral", "Chinatown, Manchester", "Whitworth Art Gallery"], "ans_acc": 0.15384615384615385, "ans_hit": 1, "ans_f1": 0.2448979591836735, "ans_precission": 0.6, "ans_recall": 0.15384615384615385, "path_f1": 0.2448979591836735, "path_precision": 0.6, "path_recall": 0.15384615384615385, "path_ans_f1": 0.2448979591836735, "path_ans_precision": 0.6, "path_ans_recall": 0.15384615384615385}
{"id": "WebQTest-776", "prediction": ["# Reasoning Path:\nItaly -> sports.sports_team_location.teams -> Italy national football team -> sports.sports_team.sport -> Football\n# Answer:\nItaly national football team", "# Reasoning Path:\nItaly -> sports.sports_team_location.teams -> Dinamo Basket Sassari -> sports.sports_team.sport -> Basketball\n# Answer:\nDinamo Basket Sassari", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> sports.sports_team_location.teams -> AS Vicenza -> sports.sports_team.sport -> Basketball\n# Answer:\nAS Vicenza", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37cxr\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> sports.sports_team_location.teams -> AS Vicenza -> common.topic.notable_types -> Basketball Team\n# Answer:\nAS Vicenza", "# Reasoning Path:\nItaly -> sports.sports_team_location.teams -> Dinamo Basket Sassari -> common.topic.notable_types -> Basketball Team\n# Answer:\nDinamo Basket Sassari", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Tenerani -> base.thoroughbredracing.thoroughbred_racehorse.foal_if_this_horse_is_sire -> Ribot\n# Answer:\nTenerani", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Tenerani -> biology.organism.sex -> Male\n# Answer:\nTenerani"], "ground_truth": ["Italy national football team", "Italy national cricket team", "Italy national handball team", "Italy national rugby league team", "Italy men's national pitch and putt team", "Italy national futsal team", "Italy national speedway team", "Italy national baseball team", "Italy national rugby union team", "Fortitudo Pallacanestro Bologna", "Italy women's national water polo team", "Victoria Libertas Pesaro", "Team Liquigas-Cannondale", "Italy women's national football team", "Partenope Napoli", "Italy women's national volleyball team", "Italy Davis Cup team", "Italy Fed Cup team", "Juvecaserta Basket", "Pallacanestro Virtus Roma", "Italy men's national volleyball team", "Italy women's national beach handball team", "S.S. Felice Scandone", "Pallacanestro Varese", "Olimpia Milano", "g.11x7vhqcx", "Italy women's national rugby union team", "Mens Sana 1871 Basket", "Pallacanestro Treviso", "Pallacanestro Cant\u00f9", "Italy men's national ice hockey team", "Virtus Pallacanestro Bologna", "Dinamo Basket Sassari", "Italy national basketball team", "Italy women's national ice hockey team", "AS Vicenza"], "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0.14285714285714285, "ans_precission": 0.5, "ans_recall": 0.08333333333333333, "path_f1": 0.14285714285714285, "path_precision": 0.5, "path_recall": 0.08333333333333333, "path_ans_f1": 0.14285714285714285, "path_ans_precision": 0.5, "path_ans_recall": 0.08333333333333333}
{"id": "WebQTest-777", "prediction": ["# Reasoning Path:\nGoogle -> organization.organization.headquarters -> m.030qthd -> location.mailing_address.citytown -> Mountain View\n# Answer:\nMountain View", "# Reasoning Path:\nGoogle -> organization.organization.headquarters -> m.030qthd -> common.topic.notable_types -> Location\n# Answer:\nLocation", "# Reasoning Path:\nGoogle -> organization.organization.headquarters -> m.030qthd -> location.mailing_address.state_province_region -> California\n# Answer:\nCalifornia", "# Reasoning Path:\nGoogle -> organization.organization.place_founded -> Menlo Park -> location.location.containedby -> United States of America\n# Answer:\nMenlo Park", "# Reasoning Path:\nGoogle -> organization.organization.place_founded -> Menlo Park -> location.location.containedby -> California\n# Answer:\nMenlo Park", "# Reasoning Path:\nGoogle -> organization.organization.place_founded -> Menlo Park -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMenlo Park", "# Reasoning Path:\nGoogle -> organization.organization.place_founded -> Menlo Park -> base.biblioness.bibs_location.state -> California\n# Answer:\nMenlo Park", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> organization.organization.headquarters -> m.012cdccs\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> organization.organization_partnership.members -> Redcorus\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> common.topic.subject_of -> Google Chrome\n# Answer:\nGoogle Apps for Work"], "ground_truth": ["Mountain View"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-778", "prediction": ["# Reasoning Path:\nJesse Owens -> people.person.education -> m.02wnc1b -> education.education.institution -> Ohio State University\n# Answer:\nOhio State University", "# Reasoning Path:\nJesse Owens -> people.person.education -> m.0n1kct6 -> education.education.institution -> East Technical High School\n# Answer:\nEast Technical High School", "# Reasoning Path:\nJesse Owens -> people.person.education -> m.02wnc1b -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nJesse Owens -> people.person.education -> m.02wnc1b -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nJesse Owens -> people.person.education -> m.0n1kct6 -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nJesse Owens -> people.person.places_lived -> m.03pfl8z -> people.place_lived.location -> Cleveland\n# Answer:\nCleveland", "# Reasoning Path:\nJesse Owens -> people.person.education -> m.02wnc1b -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nJesse Owens -> film.person_or_entity_appearing_in_film.films -> m.0v9dxnk -> film.personal_film_appearance.film -> Salute\n# Answer:\nSalute", "# Reasoning Path:\nJesse Owens -> people.person.education -> m.0n1kct6 -> freebase.valuenotation.has_no_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nJesse Owens -> people.person.places_lived -> m.0wk2xk3 -> people.place_lived.location -> Oakville, Alabama\n# Answer:\nOakville, Alabama"], "ground_truth": ["East Technical High School", "Ohio State University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-779", "prediction": ["# Reasoning Path:\nCarlos Santana -> people.person.places_lived -> m.0h3dwg3 -> people.place_lived.location -> San Francisco\n# Answer:\nSan Francisco", "# Reasoning Path:\nCarlos Santana -> people.person.places_lived -> m.03psmj7 -> people.place_lived.location -> Jalisco\n# Answer:\nJalisco", "# Reasoning Path:\nCarlos Santana -> symbols.name_source.namesakes -> Santana -> music.artist.origin -> San Francisco\n# Answer:\nSantana", "# Reasoning Path:\nCarlos Santana -> people.person.places_lived -> m.0h3dwfk -> people.place_lived.location -> Tijuana\n# Answer:\nTijuana", "# Reasoning Path:\nCarlos Santana -> people.person.place_of_birth -> Autl\u00e1n -> location.location.people_born_here -> Alfonso Corona Blake\n# Answer:\nAutl\u00e1n", "# Reasoning Path:\nCarlos Santana -> people.person.place_of_birth -> Autl\u00e1n -> common.topic.article -> m.027941n\n# Answer:\nAutl\u00e1n", "# Reasoning Path:\nCarlos Santana -> symbols.name_source.namesakes -> Santana -> award.award_winner.awards_won -> m.05btgv1\n# Answer:\nSantana", "# Reasoning Path:\nCarlos Santana -> people.person.place_of_birth -> Autl\u00e1n -> common.topic.image -> Municipality location in Jalisco\n# Answer:\nAutl\u00e1n", "# Reasoning Path:\nCarlos Santana -> symbols.name_source.namesakes -> Santana -> music.artist.label -> Arista Records\n# Answer:\nSantana", "# Reasoning Path:\nCarlos Santana -> people.person.place_of_birth -> Autl\u00e1n -> location.location.people_born_here -> Antonio Alatorre\n# Answer:\nAutl\u00e1n"], "ground_truth": ["Tijuana", "Jalisco", "San Francisco"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.33333333333333326, "path_precision": 0.3, "path_recall": 0.375, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.treatments -> Radiation therapy\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.treatments -> Cervical conization\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> common.topic.webpage -> m.04tnn4f\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> people.cause_of_death.people -> Yvette Wilson\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.person.gender -> Female -> medicine.risk_factor.diseases -> Ovarian cancer\n# Answer:\nFemale", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> common.topic.webpage -> m.09wbnt0\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.treatments -> Erlotinib\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> common.topic.webpage -> m.09ybml5\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> people.cause_of_death.people -> Angelique Pettyjohn\n# Answer:\nCervical cancer"], "ground_truth": ["Cervical cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-780", "prediction": ["# Reasoning Path:\nJohn Carter -> film.film.release_date_s -> m.0gwq3ht -> film.film_regional_release_date.film_release_distribution_medium -> Theatrical\n# Answer:\nTheatrical", "# Reasoning Path:\nJohn Carter -> film.film.release_date_s -> m.0gwq3ht -> film.film_regional_release_date.film_release_region -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJohn Carter -> film.film.release_date_s -> m.0k27srr -> film.film_regional_release_date.film_release_distribution_medium -> DVD\n# Answer:\nDVD", "# Reasoning Path:\nJohn Carter -> film.film.release_date_s -> m.0vxb_0b -> film.film_regional_release_date.film_release_distribution_medium -> Theatrical\n# Answer:\nTheatrical", "# Reasoning Path:\nJohn Carter -> film.film.produced_by -> Lindsey Collins -> common.topic.notable_for -> g.1256468vn\n# Answer:\nLindsey Collins", "# Reasoning Path:\nJohn Carter -> film.film.produced_by -> Jim Morris -> people.person.profession -> Film Producer\n# Answer:\nJim Morris", "# Reasoning Path:\nJohn Carter -> film.film.starring -> m.064xypb -> film.performance.actor -> Taylor Kitsch\n# Answer:\nTaylor Kitsch", "# Reasoning Path:\nJohn Carter -> film.film.release_date_s -> m.0k27srr -> film.film_regional_release_date.film_release_region -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJohn Carter -> film.film.produced_by -> Lindsey Collins -> film.actor.film -> m.0v300yp\n# Answer:\nLindsey Collins", "# Reasoning Path:\nJohn Carter -> film.film.produced_by -> Colin Wilson -> film.producer.film -> Amistad\n# Answer:\nColin Wilson"], "ground_truth": ["Science Fiction", "Action Film", "Adventure Film", "Fantasy"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-782", "prediction": ["# Reasoning Path:\nNorth American Free Trade Agreement -> organization.membership_organization.members -> m.03w60c3 -> organization.organization_membership.member -> Mexico\n# Answer:\nMexico", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.membership_organization.members -> m.03w60cf -> organization.organization_membership.member -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.membership_organization.members -> m.03w60c8 -> organization.organization_membership.member -> Canada\n# Answer:\nCanada", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.organization.founders -> George H. W. Bush -> people.person.nationality -> United States of America\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nNorth American Free Trade Agreement -> freebase.valuenotation.has_value -> Place founded -> rdf-schema#range -> Location\n# Answer:\nPlace founded", "# Reasoning Path:\nNorth American Free Trade Agreement -> freebase.valuenotation.has_value -> Place founded -> rdf-schema#domain -> Organization\n# Answer:\nPlace founded", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.organization.founders -> George H. W. Bush -> people.person.profession -> Politician\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nNorth American Free Trade Agreement -> freebase.valuenotation.has_value -> Place founded -> type.property.expected_type -> Location\n# Answer:\nPlace founded", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.organization.founders -> George H. W. Bush -> government.political_appointer.appointees -> m.048zk8w\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.organization.founders -> George H. W. Bush -> people.person.profession -> Businessperson\n# Answer:\nGeorge H. W. Bush"], "ground_truth": ["Canada", "United States of America", "Mexico"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-783", "prediction": ["# Reasoning Path:\nHouston -> location.hud_county_place.county -> Houston County -> location.location.containedby -> Texas\n# Answer:\nHouston County", "# Reasoning Path:\nHouston -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nHouston -> location.location.containedby -> Area code 281 -> location.location.containedby -> Texas\n# Answer:\nArea code 281", "# Reasoning Path:\nHouston -> location.location.containedby -> Area code 713 -> location.location.containedby -> Texas\n# Answer:\nArea code 713", "# Reasoning Path:\nHouston -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> New York\n# Answer:\nUnited States of America", "# Reasoning Path:\nHouston -> location.hud_county_place.county -> Houston County -> location.location.time_zones -> Central Time Zone\n# Answer:\nHouston County", "# Reasoning Path:\nHouston -> location.hud_county_place.county -> Montgomery County -> location.location.containedby -> Texas\n# Answer:\nMontgomery County", "# Reasoning Path:\nHouston -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nHouston -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Oklahoma\n# Answer:\nUnited States of America", "# Reasoning Path:\nHouston -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> New York\n# Answer:\nUnited States of America"], "ground_truth": ["Montgomery County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-784", "prediction": ["# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.arena_stadium -> CenturyLink Field -> sports.sports_facility.teams -> Seattle Sounders FC\n# Answer:\nCenturyLink Field", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.location -> Seattle -> travel.travel_destination.tourist_attractions -> Experience Music Project Museum\n# Answer:\nSeattle", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.arena_stadium -> CenturyLink Field -> soccer.football_pitch.matches -> 2010 Lamar Hunt U.S. Open Cup Final\n# Answer:\nCenturyLink Field", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.location -> Seattle -> travel.travel_destination.tourist_attractions -> Pike Place Market\n# Answer:\nSeattle", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.arena_stadium -> CenturyLink Field -> location.location.events -> 2006 NFC Championship Game\n# Answer:\nCenturyLink Field", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.location -> Seattle -> location.hud_county_place.county -> King County\n# Answer:\nSeattle", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.arena_stadium -> CenturyLink Field -> soccer.football_pitch.matches -> 2011 Lamar Hunt U.S. Open Cup Final\n# Answer:\nCenturyLink Field", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.arena_stadium -> CenturyLink Field -> soccer.football_pitch.matches -> 2014 LA Galaxy vs Seattle Sounders\n# Answer:\nCenturyLink Field", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.arena_stadium -> CenturyLink Field -> location.location.events -> 2010\u201311 NFL playoffs\n# Answer:\nCenturyLink Field", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.season_record -> m.075fsd1 -> sports.sports_team_season_record.season -> 2008 NFL season\n# Answer:\n2008 NFL season"], "ground_truth": ["CenturyLink Field"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-785", "prediction": ["# Reasoning Path:\nMichael Keaton -> film.actor.film -> m.0jw4g9 -> film.performance.character -> Chick Hicks\n# Answer:\nChick Hicks", "# Reasoning Path:\nCars -> film.film.starring -> m.0jw4g9 -> film.performance.character -> Chick Hicks\n# Answer:\nChick Hicks", "# Reasoning Path:\nMichael Keaton -> film.actor.film -> g.11b6dn6h31\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nMichael Keaton -> film.actor.film -> m.0jw4g9 -> film.performance.film -> Cars\n# Answer:\nfilm.performance.film", "# Reasoning Path:\nCars -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> World of Cars -> fictional_universe.fictional_universe.characters -> Chick Hicks\n# Answer:\nWorld of Cars", "# Reasoning Path:\nMichael Keaton -> base.popstra.celebrity.dated -> m.065q84p -> base.popstra.dated.participant -> Rachel Ryan\n# Answer:\nRachel Ryan", "# Reasoning Path:\nMichael Keaton -> film.actor.film -> g.11b6pd5lf0\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nMichael Keaton -> film.actor.film -> m.0jw4g9 -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nMichael Keaton -> base.popstra.celebrity.dated -> m.065q4rt -> base.popstra.dated.participant -> Michelle Pfeiffer\n# Answer:\nMichelle Pfeiffer", "# Reasoning Path:\nCars -> film.film.starring -> m.04fdc8_ -> film.performance.character -> Junior\n# Answer:\nJunior"], "ground_truth": ["Chick Hicks"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-787", "prediction": ["# Reasoning Path:\nRiviera -> location.location.containedby -> Las Vegas -> location.location.containedby -> United States of America\n# Answer:\nLas Vegas", "# Reasoning Path:\nRiviera -> location.location.containedby -> Las Vegas -> location.location.containedby -> Area codes 702 and 725\n# Answer:\nLas Vegas", "# Reasoning Path:\nRiviera -> location.location.containedby -> Las Vegas -> location.location.containedby -> Clark County\n# Answer:\nLas Vegas", "# Reasoning Path:\nRiviera -> common.image.size -> m.02930wg\n# Answer:\ncommon.image.size", "# Reasoning Path:\nRiviera -> location.location.containedby -> Las Vegas -> location.location.nearby_airports -> North Las Vegas Airport\n# Answer:\nLas Vegas", "# Reasoning Path:\nRiviera -> location.location.containedby -> Las Vegas -> location.location.nearby_airports -> Henderson Executive Airport\n# Answer:\nLas Vegas", "# Reasoning Path:\nRiviera -> location.location.containedby -> Las Vegas -> film.film_location.featured_in_films -> Vegas Vacation\n# Answer:\nLas Vegas", "# Reasoning Path:\nRiviera -> common.image.size -> m.03s8648\n# Answer:\ncommon.image.size", "# Reasoning Path:\nRiviera -> location.location.containedby -> Las Vegas -> location.location.nearby_airports -> Ivanpah Valley Airport\n# Answer:\nLas Vegas", "# Reasoning Path:\nRiviera -> common.image.size -> m.041x0l3\n# Answer:\ncommon.image.size"], "ground_truth": ["Las Vegas"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-788", "prediction": ["# Reasoning Path:\nBuddhism -> religion.religion.texts -> \u015ar\u012bm\u0101l\u0101dev\u012b Si\u1e43han\u0101da S\u016btra -> common.topic.article -> m.036z9j\n# Answer:\n\u015ar\u012bm\u0101l\u0101dev\u012b Si\u1e43han\u0101da S\u016btra", "# Reasoning Path:\nBuddhism -> religion.religion.texts -> \u015ar\u012bm\u0101l\u0101dev\u012b Si\u1e43han\u0101da S\u016btra -> common.topic.notable_types -> Religious Text\n# Answer:\n\u015ar\u012bm\u0101l\u0101dev\u012b Si\u1e43han\u0101da S\u016btra", "# Reasoning Path:\nBuddhism -> base.skosbase.skos_concept.narrower -> Buddhist civilization\n# Answer:\nBuddhist civilization", "# Reasoning Path:\nBuddhism -> religion.religion.texts -> Chinese Buddhist canon -> common.topic.article -> m.02l3dq\n# Answer:\nChinese Buddhist canon", "# Reasoning Path:\nBuddhism -> religion.religion.texts -> Mah\u0101vastu -> common.topic.notable_types -> Religious Text\n# Answer:\nMah\u0101vastu", "# Reasoning Path:\nBuddhism -> base.skosbase.skos_concept.narrower -> Buddhist fundamentalism\n# Answer:\nBuddhist fundamentalism", "# Reasoning Path:\nBuddhism -> religion.religion.texts -> Mah\u0101vastu -> common.topic.article -> m.04_13vy\n# Answer:\nMah\u0101vastu", "# Reasoning Path:\nBuddhism -> base.skosbase.skos_concept.narrower -> Buddhist gods\n# Answer:\nBuddhist gods", "# Reasoning Path:\nBuddhism -> religion.religion.texts -> Chinese Buddhist canon -> common.topic.image -> IMG 0275 Song\n# Answer:\nChinese Buddhist canon", "# Reasoning Path:\nBuddhism -> religion.religion.practices -> Astrology -> common.topic.notable_for -> g.1255v8zv6\n# Answer:\nAstrology"], "ground_truth": ["Longchen Nyingthig", "Mah\u0101vastu", "Chinese Buddhist canon", "Amitabha Sutra", "N\u012blaka\u1e47\u1e6dha Dh\u0101ra\u1e47\u012b", "Mah\u0101y\u0101na s\u016btras", "Tibetan Buddhist canon", "\u015ar\u012bm\u0101l\u0101dev\u012b Si\u1e43han\u0101da S\u016btra", "U\u1e63\u1e47\u012b\u1e63a Vijaya Dh\u0101ra\u1e47\u012b S\u016btra", "\u0100gama", "Vimalakirti Sutra", "P\u0101li Canon", "Gang\u014dji Garan Engi", "Dhammapada"], "ans_acc": 0.21428571428571427, "ans_hit": 1, "ans_f1": 0.3157894736842105, "ans_precission": 0.6, "ans_recall": 0.21428571428571427, "path_f1": 0.3157894736842105, "path_precision": 0.6, "path_recall": 0.21428571428571427, "path_ans_f1": 0.3157894736842105, "path_ans_precision": 0.6, "path_ans_recall": 0.21428571428571427}
{"id": "WebQTest-789", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> people.deceased_person.place_of_death -> Washington, D.C. -> symbols.namesake.named_after -> George Washington\n# Answer:\nWashington, D.C.", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.children -> Tazewell Tyler\n# Answer:\nJohn Tyler", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.nationality -> United States of America\n# Answer:\nJohn Tyler", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.religion -> Episcopal Church\n# Answer:\nJohn Tyler", "# Reasoning Path:\nWilliam Henry Harrison -> people.deceased_person.place_of_death -> Washington, D.C. -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nWashington, D.C.", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\ncommon.image.size", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.children -> Alice Tyler\n# Answer:\nJohn Tyler", "# Reasoning Path:\nWilliam Henry Harrison -> people.deceased_person.place_of_death -> Washington, D.C. -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nWashington, D.C.", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.religion -> Deism\n# Answer:\nJohn Tyler", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.children -> Anne Contesse Tyler\n# Answer:\nJohn Tyler"], "ground_truth": ["John Tyler"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> R\u0101hula\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_for -> g.12582gf_5\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> book.book_subject.works -> Remaking Buddhism for Medieval Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Abhiyaza\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_types -> City/Town/Village\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> location.location.time_zones -> Nepal Time Zone\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Nanda\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> location.location.geolocation -> m.0wmmlzp\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> common.topic.article -> m.0hznzjg\n# Answer:\nBuddha Memorial Center", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Tooth Relic Temple and Museum -> common.topic.webpage -> m.0gb1hxk\n# Answer:\nBuddha Tooth Relic Temple and Museum"], "ground_truth": ["Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-790", "prediction": ["# Reasoning Path:\nRichard Arkwright -> people.person.place_of_birth -> Preston, Lancashire -> location.location.containedby -> Lancashire\n# Answer:\nPreston, Lancashire", "# Reasoning Path:\nRichard Arkwright -> people.person.place_of_birth -> Preston, Lancashire -> location.location.containedby -> United Kingdom\n# Answer:\nPreston, Lancashire", "# Reasoning Path:\nRichard Arkwright -> people.person.place_of_birth -> Preston, Lancashire -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nPreston, Lancashire", "# Reasoning Path:\nRichard Arkwright -> people.person.place_of_birth -> Preston, Lancashire -> location.location.contains -> St Joseph's Church, Preston\n# Answer:\nPreston, Lancashire", "# Reasoning Path:\nRichard Arkwright -> people.person.place_of_birth -> Preston, Lancashire -> location.location.containedby -> City of Preston, Lancashire\n# Answer:\nPreston, Lancashire", "# Reasoning Path:\nRichard Arkwright -> people.person.place_of_birth -> Preston, Lancashire -> location.location.contains -> Preston Central Methodist Church\n# Answer:\nPreston, Lancashire", "# Reasoning Path:\nRichard Arkwright -> people.person.children -> Richard Arkwright junior -> common.topic.notable_for -> g.1258b76mr\n# Answer:\nRichard Arkwright junior", "# Reasoning Path:\nRichard Arkwright -> people.person.place_of_birth -> Preston, Lancashire -> location.location.contains -> Avenham Park\n# Answer:\nPreston, Lancashire", "# Reasoning Path:\nRichard Arkwright -> people.person.children -> Richard Arkwright junior -> common.topic.article -> m.06pf3h\n# Answer:\nRichard Arkwright junior", "# Reasoning Path:\nRichard Arkwright -> people.person.children -> Susanna Arkwright -> common.topic.notable_types -> Deceased Person\n# Answer:\nSusanna Arkwright"], "ground_truth": ["Preston, Lancashire"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-791", "prediction": ["# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> film.film.production_companies -> Walt Disney Pictures\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> common.topic.article -> m.03tgtz\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> The Boy Who Lost His Face -> common.topic.notable_types -> Book\n# Answer:\nThe Boy Who Lost His Face", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> The Cardturner -> award.award_nominated_work.award_nominations -> m.0g2978l\n# Answer:\nThe Cardturner", "# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> common.topic.article -> m.0dq62d\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> The Cardturner -> common.topic.article -> m.0fq08x4\n# Answer:\nThe Cardturner", "# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> film.film.production_companies -> Phoenix Pictures\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> The Cardturner -> common.topic.notable_for -> g.12556956x\n# Answer:\nThe Cardturner", "# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> film.film.production_companies -> The Walt Disney Company\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> film.writer.film -> Holes -> film.film.production_companies -> Walt Disney Pictures\n# Answer:\nHoles"], "ground_truth": ["Small Steps (Readers Circle)", "Wayside School Gets A Little Stranger", "Someday Angeline", "A Flying Birthday Cake?", "Holes", "Marvin Redpost", "Johnny's in the Basement", "Class President (A Stepping Stone Book(TM))", "Wayside School Gets a Little Stranger (rack) (Wayside School)", "Stanley Yelnats' Survival Guide to Camp Green Lake", "Why Pick on Me?", "Marvin Redpost.", "There's a Boy in the Girls' Bathroom", "A magic crystal?", "More Sideways Arithmetic from Wayside School", "Wayside School is falling down (Celebrate reading, Scott Foresman)", "Boy Who Lost His Face", "Hoyos/Holes", "Wayside School is Falling Down", "Monkey soup", "Holes (Readers Circle)", "There's a boy in the girls bathroom", "Alone in His Teacher's House", "Der Fluch des David Ballinger. ( Ab 11 J.).", "Sixth Grade Secrets (Apple Paperbacks)", "Holes. (Lernmaterialien)", "Why Pick on Me? (A Stepping Stone Book(TM))", "Wayside School Is Falling Down", "Super Fast, Out of Control! (A Stepping Stone Book(TM))", "Sideways Stories from Wayside School", "A Flying Birthday Cake? (A Stepping Stone Book(TM))", "Sideways stories from Wayside School", "Wayside School Gets a Little Stranger", "Wayside School is falling down", "Someday Angeline (Avon/Camelot Book)", "Hay Un Chico En El Bano De Las Chicas", "Class President", "Wayside School Boxed Set", "Il y a un gar\u00e7on dans les toilettes des filles", "Holes (with \\\"Connections\\\") HRW Library (HRW library)", "Stanley Yelnats Survival Guide to Camp Green Lake", "Kidnapped at Birth? (A Stepping Stone Book(TM))", "L\u00f6cher", "Holes (Listening Library)", "Holes (Cascades)", "Sideways Arithmetic from Wayside School", "Holes (Yearling Books)", "Holes (World Book Day 2001)", "Small steps", "Dogs Don't Tell Jokes", "Holes Activity Pack", "Kidnapped at Birth?", "Sixth grade secrets", "Wayside School Collection", "The Cardturner", "Sixth Grade Secrets", "Louis Sacher Collection", "More Sideways Arithmetic From Wayside School", "Wayside School gets a little stranger", "Sideways Arithmetic From Wayside School", "Pequenos Pasos/ Small Steps", "g.1218f5g0", "Small Steps", "Super Fast, Out of Control!", "Johnny's in the basement", "The boy who lost his face", "Sixth Grade Secrets (An Apple Paperback)", "The Boy Who Lost His Face"], "ans_acc": 0.07352941176470588, "ans_hit": 1, "ans_f1": 0.13698630136986303, "ans_precission": 1.0, "ans_recall": 0.07352941176470588, "path_f1": 0.046511627906976744, "path_precision": 0.4, "path_recall": 0.024691358024691357, "path_ans_f1": 0.13698630136986303, "path_ans_precision": 1.0, "path_ans_recall": 0.07352941176470588}
{"id": "WebQTest-792", "prediction": ["# Reasoning Path:\nRoth IRA -> symbols.namesake.named_after -> William V. Roth, Jr. -> people.person.place_of_birth -> Great Falls\n# Answer:\nWilliam V. Roth, Jr.", "# Reasoning Path:\nRoth IRA -> common.topic.article -> m.023_lv\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nRoth IRA -> symbols.namesake.named_after -> William V. Roth, Jr. -> book.author.works_written -> Complexity of the Individual Income Tax\n# Answer:\nWilliam V. Roth, Jr.", "# Reasoning Path:\nRoth IRA -> symbols.namesake.named_after -> William V. Roth, Jr. -> book.author.works_written -> Consequences of the Asian Financial Crisis\n# Answer:\nWilliam V. Roth, Jr.", "# Reasoning Path:\nRoth IRA -> symbols.namesake.named_after -> William V. Roth, Jr. -> book.author.works_written -> Increasing Savings for Retirement\n# Answer:\nWilliam V. Roth, Jr."], "ground_truth": ["William V. Roth, Jr."], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-794", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nThe Making of Star Wars", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.film -> Star Wars\n# Answer:\nStar Wars", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Uncredited\n# Answer:\nUncredited", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.film -> Return of the Jedi\n# Answer:\nReturn of the Jedi", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice"], "ground_truth": ["Matt Lanter", "Zac Efron", "Dr. Smoov", "James Earl Jones", "Abraham Benrubi"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.24, "ans_precission": 0.3, "ans_recall": 0.2, "path_f1": 0.28571428571428564, "path_precision": 0.3, "path_recall": 0.2727272727272727, "path_ans_f1": 0.24, "path_ans_precision": 0.3, "path_ans_recall": 0.2}
{"id": "WebQTest-796", "prediction": ["# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Mathematician -> fictional_universe.character_occupation.characters_with_this_occupation -> Beremiz Samir\n# Answer:\nMathematician", "# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Mathematician -> fictional_universe.character_occupation.characters_with_this_occupation -> Dyhianna Selei\n# Answer:\nMathematician", "# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Mathematician -> base.descriptive_names.names.descriptive_name -> m.01066gkh\n# Answer:\nMathematician", "# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Economist -> base.descriptive_names.names.descriptive_name -> m.0101nv80\n# Answer:\nEconomist", "# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Economist -> base.schemastaging.context_name.pronunciation -> g.125_pmh72\n# Answer:\nEconomist", "# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Mathematician -> fictional_universe.character_occupation.characters_with_this_occupation -> Hari Seldon\n# Answer:\nMathematician", "# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Economist -> common.topic.notable_for -> g.12562254w\n# Answer:\nEconomist", "# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Demographer -> common.topic.notable_types -> Profession\n# Answer:\nDemographer", "# Reasoning Path:\nThomas Robert Malthus -> common.topic.image -> Thomas Malthus -> common.image.appears_in_topic_gallery -> Albert Allen Bartlett\n# Answer:\nThomas Malthus", "# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Mathematician -> base.descriptive_names.names.descriptive_name -> m.010f3g_r\n# Answer:\nMathematician"], "ground_truth": ["Demographer", "Scientist", "Mathematician", "Economist"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.8181818181818182, "path_precision": 0.9, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-797", "prediction": ["# Reasoning Path:\nNewark -> location.citytown.postal_codes -> 94560 -> location.location.geolocation -> m.03dnk73\n# Answer:\n94560", "# Reasoning Path:\nNewark -> location.citytown.postal_codes -> 94560 -> location.location.containedby -> United States of America\n# Answer:\n94560", "# Reasoning Path:\nNewark -> location.statistical_region.population -> g.11b66cggvh\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nNewark -> location.citytown.postal_codes -> 94560 -> location.location.contains -> NewPark Mall\n# Answer:\n94560", "# Reasoning Path:\nNewark -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> California\n# Answer:\nUnited States of America", "# Reasoning Path:\nNewark -> location.statistical_region.population -> g.11bymn221g\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nNewark -> location.citytown.postal_codes -> 94560 -> location.location.containedby -> Alameda County\n# Answer:\n94560", "# Reasoning Path:\nNewark -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> California\n# Answer:\nUnited States of America", "# Reasoning Path:\nNewark -> location.statistical_region.population -> g.11x1cbcyw\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nNewark -> location.citytown.postal_codes -> 94560 -> location.location.containedby -> California\n# Answer:\n94560"], "ground_truth": ["94560"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-798", "prediction": ["# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> Mississippi\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> Kentucky\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> meteorology.tropical_cyclone.affected_areas -> Kentucky\n# Answer:\nHurricane Bob", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> Alabama\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.tropical_cyclone_season -> 1982 Atlantic hurricane season\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> meteorology.tropical_cyclone.affected_areas -> Mississippi\n# Answer:\nHurricane Bob", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> common.topic.notable_for -> g.1255tjcrg\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> 1915 New Orleans hurricane -> meteorology.tropical_cyclone.affected_areas -> Mississippi\n# Answer:\n1915 New Orleans hurricane", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> meteorology.tropical_cyclone.affected_areas -> Alabama\n# Answer:\nHurricane Bob", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> common.topic.notable_for -> g.1259xs2jv\n# Answer:\nHurricane Bob"], "ground_truth": ["North Carolina", "Alabama", "Missouri", "Arkansas", "Virginia", "Mississippi", "Georgia", "Kentucky"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.0736842105263158, "path_precision": 0.7, "path_recall": 0.03888888888888889, "path_ans_f1": 0.48837209302325574, "path_ans_precision": 0.7, "path_ans_recall": 0.375}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove -> common.topic.article -> m.029mn3\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_trwk\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> Glass\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> medicine.medical_treatment.used_to_treat -> Presbyopia\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_ty__\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.image -> A bifocal corrective eyeglasses lens\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> Waking up early\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_wj2d\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.instrument.family -> Crystallophone\n# Answer:\nGlass harmonica"], "ground_truth": ["Bifocals", "Lightning rod", "Glass harmonica", "Franklin stove"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.8, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.8, "path_ans_recall": 0.75}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpsq_\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Colorado\n# Answer:\nUnited States of America", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.us_county.hud_county_place -> Aristocrat Ranchettes\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.contains -> Academy of Natural Therapy -> location.location.containedby -> United States of America\n# Answer:\nAcademy of Natural Therapy", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpst3\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County -> location.location.adjoin_s -> m.03jpsq_\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpt45\n# Answer:\nWeld County"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-800", "prediction": ["# Reasoning Path:\nUnited States of America -> military.military_combatant.casualties -> m.043wpr_ -> military.casualties.type_of_casualties -> Military - Dead\n# Answer:\nMilitary - Dead", "# Reasoning Path:\nUnited States of America -> military.military_combatant.casualties -> m.010h8rpj -> military.casualties.military_conflict -> Battle of the Little Bighorn\n# Answer:\nBattle of the Little Bighorn", "# Reasoning Path:\nUnited States of America -> military.military_combatant.casualties -> m.010h8rq6 -> military.casualties.military_conflict -> Battle of the Little Bighorn\n# Answer:\nBattle of the Little Bighorn", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> military.military_combatant.casualties -> m.010h8rpj -> military.casualties.type_of_casualties -> Military - Dead\n# Answer:\nMilitary - Dead", "# Reasoning Path:\nUnited States of America -> military.military_combatant.casualties -> m.010h8rq6 -> military.casualties.type_of_casualties -> Military - Wounded\n# Answer:\nMilitary - Wounded", "# Reasoning Path:\nUnited States of America -> periodicals.newspaper_circulation_area.newspapers -> American Free Press -> book.periodical.language -> English Language\n# Answer:\nAmerican Free Press", "# Reasoning Path:\nUnited States of America -> periodicals.newspaper_circulation_area.newspapers -> China Daily -> book.newspaper.circulation_areas -> Washington, D.C.\n# Answer:\nChina Daily", "# Reasoning Path:\nUnited States of America -> periodicals.newspaper_circulation_area.newspapers -> American Free Press -> common.topic.notable_types -> Newspaper\n# Answer:\nAmerican Free Press"], "ground_truth": ["World War II"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-801", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> France\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Portugal\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Syria\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3f_h6\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives"], "ground_truth": ["Provisional government", "Semi-presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-802", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> Somebody to Love\n# Answer:\nSomebody to Love", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber's Believe -> film.film.music -> Nathan Lanier\n# Answer:\nJustin Bieber's Believe", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nJuno Awards of 2014", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nWinning work", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0115qhzk -> award.award_honor.award -> MTV Europe Music Voices Award\n# Answer:\nMTV Europe Music Voices Award", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber: Never Say Never -> film.film.music -> Edvard Grieg\n# Answer:\nJustin Bieber: Never Say Never", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> freebase.valuenotation.is_reviewed -> Award category\n# Answer:\nAward category", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> award.award_honor.ceremony -> 2014 Billboard Music Awards\n# Answer:\n2014 Billboard Music Awards", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber's Believe -> film.film.executive_produced_by -> Allison Kaye Scarinzi\n# Answer:\nJustin Bieber's Believe"], "ground_truth": ["Vocals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-805", "prediction": ["# Reasoning Path:\nJohn Lennon -> people.deceased_person.place_of_death -> New York City -> music.composition.composer -> Yoko Ono\n# Answer:\nNew York City", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 1Club.FM: 70s (Pop) -> broadcast.content.location -> Chicago\n# Answer:\n1Club.FM: 70s (Pop)", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis -> film.film.directed_by -> Yoko Ono\n# Answer:\nApotheosis", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 1Club.FM: 70s (Pop) -> common.topic.image -> 1clubfm.jpg\n# Answer:\n1Club.FM: 70s (Pop)", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Up Your Legs Forever -> film.film.directed_by -> Yoko Ono\n# Answer:\nUp Your Legs Forever", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 1Club.FM: 70s (Pop) -> common.topic.notable_types -> Broadcast Content\n# Answer:\n1Club.FM: 70s (Pop)", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 181-greatoldies -> common.topic.notable_types -> Broadcast Content\n# Answer:\n181-greatoldies", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis -> film.film.starring -> m.0w0m5l0\n# Answer:\nApotheosis", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> .997 Radiostorm Oldies -> broadcast.content.genre -> Oldies\n# Answer:\n.997 Radiostorm Oldies", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> .997 Radiostorm Oldies -> common.topic.notable_types -> Broadcast Content\n# Answer:\n.997 Radiostorm Oldies"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-809", "prediction": ["# Reasoning Path:\nBangalore -> location.location.time_zones -> India Time Zone -> common.topic.image -> IST-CIA-TZ\n# Answer:\nIndia Time Zone", "# Reasoning Path:\nBangalore -> location.location.time_zones -> India Time Zone -> common.topic.notable_for -> g.1257lwlvq\n# Answer:\nIndia Time Zone", "# Reasoning Path:\nBangalore -> location.location.time_zones -> India Time Zone -> common.topic.image -> IST-Mirzapur\n# Answer:\nIndia Time Zone", "# Reasoning Path:\nBangalore -> location.location.time_zones -> India Time Zone -> freebase.valuenotation.has_no_value -> DST offset from UTC\n# Answer:\nIndia Time Zone", "# Reasoning Path:\nBangalore -> location.location.time_zones -> India Time Zone -> freebase.valuenotation.has_no_value -> Day DST begins\n# Answer:\nIndia Time Zone", "# Reasoning Path:\nBangalore -> location.location.time_zones -> India Time Zone -> freebase.valuenotation.has_no_value -> Day DST ends\n# Answer:\nIndia Time Zone", "# Reasoning Path:\nBangalore -> travel.travel_destination.tourist_attractions -> World 10K Bangalore -> common.topic.image -> Sree Kanteerava Stadium in 2010\n# Answer:\nWorld 10K Bangalore", "# Reasoning Path:\nBangalore -> travel.travel_destination.tourist_attractions -> Bannerghatta National Park -> location.location.containedby -> India\n# Answer:\nBannerghatta National Park", "# Reasoning Path:\nBangalore -> travel.travel_destination.tourist_attractions -> World 10K Bangalore -> common.topic.webpage -> m.0chbxyy\n# Answer:\nWorld 10K Bangalore", "# Reasoning Path:\nBangalore -> travel.travel_destination.tourist_attractions -> Bangalore Palace -> common.topic.notable_for -> g.12564hvs2\n# Answer:\nBangalore Palace"], "ground_truth": ["India Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-810", "prediction": ["# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Republic of Macedonia\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.main_country -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.region -> Southeast Europe\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Greek Language -> media_common.netflix_genre.titles -> A Girl in Black\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Greek Language -> language.human_language.region -> Europe\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> media_common.netflix_genre.titles -> A Girl in Black\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> language.human_language.region -> Europe\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Italy\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Greek Language -> media_common.netflix_genre.titles -> A Matter of Dignity\n# Answer:\nGreek Language"], "ground_truth": ["Albanian language", "Greek Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-811", "prediction": ["# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Citizens Bank Park -> location.location.events -> 2008 National League Championship Series\n# Answer:\nCitizens Bank Park", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Citizens Bank Park -> architecture.structure.architect -> Stanley Cole\n# Answer:\nCitizens Bank Park", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Citizens Bank Park -> location.location.events -> 2008 World Series\n# Answer:\nCitizens Bank Park", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Baker Bowl -> common.topic.article -> m.038js6\n# Answer:\nBaker Bowl", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Citizens Bank Park -> location.location.events -> 2009 National League Championship Series\n# Answer:\nCitizens Bank Park", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Citizens Bank Park -> architecture.structure.contractor -> Don Todd Associates, Inc\n# Answer:\nCitizens Bank Park", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Bright House Field -> base.schemastaging.sports_facility_extra.training_ground_for -> m.0x25ctr\n# Answer:\nBright House Field", "# Reasoning Path:\nPhiladelphia Phillies -> sports.professional_sports_team.draft_picks -> m.04vw__p -> sports.sports_league_draft_pick.school -> University of Oklahoma\n# Answer:\nUniversity of Oklahoma", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Citizens Bank Park -> architecture.structure.contractor -> Hunt Construction\n# Answer:\nCitizens Bank Park", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Baker Bowl -> sports.sports_facility.home_venue_for -> m.0wz2_kv\n# Answer:\nBaker Bowl"], "ground_truth": ["Bright House Field"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-812", "prediction": ["# Reasoning Path:\nMatt Dallas -> film.actor.film -> m.0cg5qxx -> film.performance.film -> As Good as Dead\n# Answer:\nAs Good as Dead", "# Reasoning Path:\nMatt Dallas -> film.actor.film -> m.0h2kydt -> film.performance.film -> The Ghost of Goodnight Lane\n# Answer:\nThe Ghost of Goodnight Lane", "# Reasoning Path:\nMatt Dallas -> film.actor.film -> m.0h2kycv -> film.performance.film -> Wannabe\n# Answer:\nWannabe", "# Reasoning Path:\nMatt Dallas -> film.actor.film -> m.0cg5qxx -> film.performance.character -> Jake\n# Answer:\nJake", "# Reasoning Path:\nMatt Dallas -> film.actor.film -> m.0h2kydt -> film.performance.character -> Ben\n# Answer:\nBen", "# Reasoning Path:\nMatt Dallas -> people.person.spouse_s -> m.0vsbrys -> people.marriage.spouse -> Blue Hamilton\n# Answer:\nBlue Hamilton", "# Reasoning Path:\nMatt Dallas -> tv.tv_actor.guest_roles -> m.09nyhkx -> tv.tv_guest_role.episodes_appeared_in -> Matt Dallas, Geena Davis, Cesar Millan, guest co-host Mo'nique\n# Answer:\nMatt Dallas, Geena Davis, Cesar Millan, guest co-host Mo'nique", "# Reasoning Path:\nMatt Dallas -> film.actor.film -> m.0h2kycv -> film.performance.character -> Monkee Dancer #1\n# Answer:\nMonkee Dancer #1", "# Reasoning Path:\nMatt Dallas -> people.person.spouse_s -> m.0vsbrys -> freebase.valuenotation.has_value -> From\n# Answer:\nFrom", "# Reasoning Path:\nMatt Dallas -> people.person.spouse_s -> m.0vsbrys -> people.marriage.type_of_union -> Domestic partnership\n# Answer:\nDomestic partnership"], "ground_truth": ["Naughty or Nice", "Camp Slaughter", "Wannabe", "In Between Days", "As Good as Dead", "Wyatt Earp's Revenge", "The Indian", "You, Me & The Circus", "Hot Dudes with Kittens", "Beauty & the Briefcase", "Living the Dream", "The Story of Bonnie and Clyde", "Way of the Vampire", "Babysitter Wanted", "The Ghost of Goodnight Lane"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.24, "ans_precission": 0.3, "ans_recall": 0.2, "path_f1": 0.23076923076923075, "path_precision": 0.3, "path_recall": 0.1875, "path_ans_f1": 0.24, "path_ans_precision": 0.3, "path_ans_recall": 0.2}
{"id": "WebQTest-813", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Rose Kennedy -> people.person.parents -> John F. Fitzgerald\n# Answer:\nRose Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Rose Kennedy -> people.person.children -> Robert F. Kennedy\n# Answer:\nRose Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Rose Kennedy -> people.person.parents -> Mary Josephine Hannon\n# Answer:\nRose Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Rose Kennedy -> people.person.children -> Eunice Kennedy Shriver\n# Answer:\nRose Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Joseph P. Kennedy, Sr. -> people.person.children -> Robert F. Kennedy\n# Answer:\nJoseph P. Kennedy, Sr.", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Caroline Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nCaroline Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Rose Kennedy -> people.person.children -> Jean Kennedy Smith\n# Answer:\nRose Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Rose Kennedy -> people.person.sibling_s -> m.0v_6_p3\n# Answer:\nRose Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Joseph P. Kennedy, Sr. -> people.person.children -> Ted Kennedy\n# Answer:\nJoseph P. Kennedy, Sr."], "ground_truth": ["Kathleen Cavendish", "Ted Kennedy", "Robert F. Kennedy", "Arabella Kennedy", "Rosemary Kennedy", "Patricia Kennedy Lawford", "Caroline Kennedy", "Patrick Bouvier Kennedy", "Joseph P. Kennedy, Sr.", "Rose Kennedy", "Eunice Kennedy Shriver", "Jean Kennedy Smith", "John F. Kennedy Jr.", "Joseph P. Kennedy, Jr."], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 1.0, "ans_recall": 0.2857142857142857, "path_f1": 0.36, "path_precision": 1.0, "path_recall": 0.21951219512195122, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 1.0, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-814", "prediction": ["# Reasoning Path:\nHarley-Davidson Motor Company -> organization.organization.headquarters -> m.03v5b_z -> location.mailing_address.citytown -> Milwaukee\n# Answer:\nMilwaukee", "# Reasoning Path:\nHarley-Davidson Motor Company -> organization.organization.place_founded -> Milwaukee -> location.location.containedby -> United States of America\n# Answer:\nMilwaukee", "# Reasoning Path:\nHarley-Davidson Motor Company -> organization.organization.headquarters -> m.03v5b_z -> location.mailing_address.postal_code -> 53208\n# Answer:\n53208", "# Reasoning Path:\nHarley-Davidson Motor Company -> organization.organization.headquarters -> m.03v5b_z -> location.location.geolocation -> m.04np6h4\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nHarley-Davidson Motor Company -> organization.organization.place_founded -> Milwaukee -> location.location.containedby -> Area code 414\n# Answer:\nMilwaukee", "# Reasoning Path:\nHarley-Davidson Motor Company -> organization.organization.place_founded -> Milwaukee -> location.location.containedby -> Milwaukee County\n# Answer:\nMilwaukee", "# Reasoning Path:\nHarley-Davidson Motor Company -> organization.organization.place_founded -> Milwaukee -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMilwaukee", "# Reasoning Path:\nHarley-Davidson Motor Company -> base.motorcycle.motorcycle_part_manufacturer.part_numbers_of_manufactured_products -> 25343-04A -> base.motorcycle.motorcycle_part_number_manufacturer.motorcycle_part -> Derby cover\n# Answer:\n25343-04A", "# Reasoning Path:\nHarley-Davidson Motor Company -> base.motorcycle.motorcycle_part_manufacturer.part_numbers_of_manufactured_products -> 25343-04A -> common.topic.webpage -> m.03hqz98\n# Answer:\n25343-04A", "# Reasoning Path:\nHarley-Davidson Motor Company -> base.motorcycle.motorcycle_part_manufacturer.part_numbers_of_manufactured_products -> 32124-04 -> base.motorcycle.motorcycle_part_number_manufacturer.motorcycle_part -> Ignition system\n# Answer:\n32124-04"], "ground_truth": ["Milwaukee"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-818", "prediction": ["# Reasoning Path:\nOttoman Empire -> location.location.containedby -> Near East -> location.location.containedby -> Western Asia\n# Answer:\nNear East", "# Reasoning Path:\nOttoman Empire -> location.location.containedby -> Near East -> location.location.events -> Sixth Crusade\n# Answer:\nNear East", "# Reasoning Path:\nOttoman Empire -> location.statistical_region.population -> m.04k6wf7\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nOttoman Empire -> location.location.containedby -> Near East -> location.location.events -> Third Crusade\n# Answer:\nNear East", "# Reasoning Path:\nOttoman Empire -> location.statistical_region.population -> m.04k6wff\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nOttoman Empire -> location.location.containedby -> Near East -> book.book_subject.works -> Voyage en Orient\n# Answer:\nNear East", "# Reasoning Path:\nOttoman Empire -> location.statistical_region.population -> m.04k6wfn\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nOttoman Empire -> location.location.containedby -> Near East -> location.location.events -> Arab\u2013Israeli conflict\n# Answer:\nNear East", "# Reasoning Path:\nOttoman Empire -> location.location.containedby -> Near East -> book.book_subject.works -> Arabic political memoirs and other studies\n# Answer:\nNear East", "# Reasoning Path:\nOttoman Empire -> location.location.containedby -> Near East -> book.book_subject.works -> Bad moon rising\n# Answer:\nNear East"], "ground_truth": ["Near East"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-819", "prediction": ["# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> Osama: The Making of a Terrorist -> book.written_work.subjects -> Al-Qaeda\n# Answer:\nOsama: The Making of a Terrorist", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Ali Azmat\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.events -> Death of Osama bin Laden\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Amrit Singh Thapar\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.contains -> Abbottabad Hockey Stadium\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> Osama: The Making of a Terrorist -> common.topic.notable_types -> Book\n# Answer:\nOsama: The Making of a Terrorist", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Faiza Khan\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> Osama: The Making of a Terrorist -> book.book.editions -> Osama : the making of a terrorist\n# Answer:\nOsama: The Making of a Terrorist", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.contains -> Abbottabad International Medical College\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.contains -> Abbottabad Public School\n# Answer:\nAbbottabad"], "ground_truth": ["Abbottabad"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> common.topic.notable_types -> Profession\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> people.profession.specialization_of -> Writer\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> music.group_member.instruments_played -> Piano\n# Answer:\nPiano", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> Music\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> common.topic.image -> Pietro Metastasio\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> base.descriptive_names.names.descriptive_name -> m.01260py_\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> common.topic.notable_for -> g.1259l_93p\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician -> people.profession.specializations -> Composer\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> Arranger\n# Answer:\nComposer"], "ground_truth": ["Librettist", "Musician", "Composer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-820", "prediction": ["# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Prisoner of Azkaban -> film.film.release_date_s -> m.0glbnz6\n# Answer:\nHarry Potter and the Prisoner of Azkaban", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Deathly Hallows -> fictional_universe.work_of_fiction.setting -> Wizarding world\n# Answer:\nHarry Potter and the Deathly Hallows", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Prisoner of Azkaban -> film.film.release_date_s -> m.0j1zm85\n# Answer:\nHarry Potter and the Prisoner of Azkaban", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Prisoner of Azkaban -> freebase.valuenotation.is_reviewed -> Directed by\n# Answer:\nHarry Potter and the Prisoner of Azkaban", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Chamber of Secrets -> book.written_work.subjects -> England\n# Answer:\nHarry Potter and the Chamber of Secrets", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Prisoner of Azkaban -> film.film.release_date_s -> m.0j56gsg\n# Answer:\nHarry Potter and the Prisoner of Azkaban", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Prisoner of Azkaban -> film.film.rating -> PG (USA)\n# Answer:\nHarry Potter and the Prisoner of Azkaban", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Deathly Hallows -> book.written_work.previous_in_series -> Harry Potter and the Half-Blood Prince\n# Answer:\nHarry Potter and the Deathly Hallows", "# Reasoning Path:\nHarry Potter -> film.film_series.films_in_series -> Harry Potter and the Prisoner of Azkaban -> film.film.release_date_s -> m.0glbnz6\n# Answer:\nHarry Potter and the Prisoner of Azkaban", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Prisoner of Azkaban -> freebase.valuenotation.is_reviewed -> Initial release date\n# Answer:\nHarry Potter and the Prisoner of Azkaban"], "ground_truth": ["Harry Potter and the Chamber of Secrets", "Harry Potter and the Goblet of Fire", "Harry Potter and the Philosopher's Stone", "Harry Potter and the Order of the Phoenix", "Harry Potter and the Deathly Hallows \u2013 Part 2", "Harry Potter and the Prisoner of Azkaban", "Harry Potter and the Half-Blood Prince", "Harry Potter and the Deathly Hallows - Part I"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.38095238095238093, "ans_precission": 0.8, "ans_recall": 0.25, "path_f1": 0.3684210526315789, "path_precision": 0.7, "path_recall": 0.25, "path_ans_f1": 0.5294117647058825, "path_ans_precision": 0.9, "path_ans_recall": 0.375}
{"id": "WebQTest-821", "prediction": ["# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Heroin overdose -> people.cause_of_death.people -> Mitch Hedberg\n# Answer:\nHeroin overdose", "# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Heroin overdose -> people.cause_of_death.people -> Alice Ormsby-Gore\n# Answer:\nHeroin overdose", "# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Heroin overdose -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nHeroin overdose", "# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Drug overdose -> people.cause_of_death.includes_causes_of_death -> Cocaine overdose\n# Answer:\nDrug overdose", "# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Heroin overdose -> people.cause_of_death.people -> Andrew Stewart\n# Answer:\nHeroin overdose", "# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Heroin overdose -> people.cause_of_death.parent_cause_of_death -> Drug overdose\n# Answer:\nHeroin overdose", "# Reasoning Path:\nPamela Courson -> common.topic.notable_for -> g.12552mkbz\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Drug overdose -> people.cause_of_death.includes_causes_of_death -> Heroin overdose\n# Answer:\nDrug overdose", "# Reasoning Path:\nPamela Courson -> common.topic.notable_for -> g.12599js9g\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Heroin overdose -> people.cause_of_death.parent_cause_of_death -> Assassination by 'Accident'\n# Answer:\nHeroin overdose"], "ground_truth": ["Drug overdose", "Heroin overdose"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-822", "prediction": ["# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Pathologist -> people.profession.specialization_of -> Physician\n# Answer:\nPathologist", "# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Pathologist -> common.topic.notable_types -> Profession\n# Answer:\nPathologist", "# Reasoning Path:\nJack Kevorkian -> common.topic.article -> m.01ctkp\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Pathologist -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Pathologists\n# Answer:\nPathologist", "# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Musician -> freebase.equivalent_topic.equivalent_type -> Musical Artist\n# Answer:\nMusician", "# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Musician -> common.topic.subjects -> MA$TADON\n# Answer:\nMusician", "# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Author -> fictional_universe.character_occupation.characters_with_this_occupation -> Auguste Gusteau\n# Answer:\nAuthor", "# Reasoning Path:\nJack Kevorkian -> common.topic.notable_types -> Film character -> freebase.type_profile.published -> Published\n# Answer:\nFilm character", "# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Musician -> award.award_category.winners -> Lisa Heller\n# Answer:\nMusician", "# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Author -> fictional_universe.character_occupation.characters_with_this_occupation -> Balph Eubank\n# Answer:\nAuthor"], "ground_truth": ["Right to die", "Assisted suicide"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-823", "prediction": ["# Reasoning Path:\nBolivia -> location.country.languages_spoken -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nBolivia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nBolivia -> location.country.languages_spoken -> Ayacucho Quechua -> language.human_language.countries_spoken_in -> Peru\n# Answer:\nAyacucho Quechua", "# Reasoning Path:\nBolivia -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nBolivia -> location.country.languages_spoken -> Ayacucho Quechua -> common.topic.notable_for -> g.125dp74jw\n# Answer:\nAyacucho Quechua", "# Reasoning Path:\nBolivia -> travel.travel_destination.tour_operators -> Bunnik Tours -> travel.tour_operator.travel_destinations -> Peru\n# Answer:\nBunnik Tours", "# Reasoning Path:\nBolivia -> travel.travel_destination.tour_operators -> FOX Vakanties -> travel.tour_operator.travel_destinations -> United States of America\n# Answer:\nFOX Vakanties", "# Reasoning Path:\nBolivia -> location.country.languages_spoken -> Ayacucho Quechua -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nAyacucho Quechua", "# Reasoning Path:\nBolivia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Argentina\n# Answer:\nSpanish Language", "# Reasoning Path:\nBolivia -> location.country.languages_spoken -> Ayacucho Quechua -> language.human_language.countries_spoken_in -> Ecuador\n# Answer:\nAyacucho Quechua"], "ground_truth": ["Argentina", "Paraguay", "Peru", "Brazil", "Chile"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.04580152671755726, "path_precision": 0.3, "path_recall": 0.024793388429752067, "path_ans_f1": 0.34285714285714286, "path_ans_precision": 0.3, "path_ans_recall": 0.4}
{"id": "WebQTest-825", "prediction": ["# Reasoning Path:\nNolan Ryan -> baseball.baseball_player.batting_stats -> m.06sdlmg -> baseball.batting_statistics.team -> Houston Astros\n# Answer:\nHouston Astros", "# Reasoning Path:\nNolan Ryan -> baseball.baseball_player.batting_stats -> m.06sdl5r -> baseball.batting_statistics.team -> Houston Astros\n# Answer:\nHouston Astros", "# Reasoning Path:\nNolan Ryan -> baseball.baseball_player.batting_stats -> m.06sdl8n -> baseball.batting_statistics.team -> Houston Astros\n# Answer:\nHouston Astros", "# Reasoning Path:\nNolan Ryan -> baseball.baseball_player.batting_stats -> m.06sdlmg -> baseball.batting_statistics.season -> 1988 Major League Baseball Season\n# Answer:\n1988 Major League Baseball Season", "# Reasoning Path:\nNolan Ryan -> baseball.baseball_player.batting_stats -> m.06sdl5r -> baseball.batting_statistics.season -> 1980 Major League Baseball Season\n# Answer:\n1980 Major League Baseball Season", "# Reasoning Path:\nNolan Ryan -> baseball.baseball_player.batting_stats -> m.06sdl8n -> baseball.batting_statistics.season -> 1982 Major League Baseball Season\n# Answer:\n1982 Major League Baseball Season", "# Reasoning Path:\nNolan Ryan -> sports.pro_athlete.teams -> m.0tl4ds3 -> sports.sports_team_roster.team -> New York Mets\n# Answer:\nNew York Mets", "# Reasoning Path:\nNolan Ryan -> sports.pro_athlete.teams -> m.0tl4ds3 -> sports.sports_team_roster.position -> Pitcher\n# Answer:\nPitcher", "# Reasoning Path:\nNolan Ryan -> people.person.places_lived -> m.0wlb4r8 -> people.place_lived.location -> Refugio\n# Answer:\nRefugio", "# Reasoning Path:\nNolan Ryan -> sports.pro_athlete.teams -> m.05yvwyc -> sports.sports_team_roster.team -> Houston Astros\n# Answer:\nHouston Astros"], "ground_truth": ["California Angels", "Houston Astros"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.4, "ans_recall": 0.5, "path_f1": 0.27586206896551724, "path_precision": 0.4, "path_recall": 0.21052631578947367, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.4, "path_ans_recall": 0.5}
{"id": "WebQTest-827", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.administrative_divisions -> Ontario\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.languages_spoken -> English Language\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Canada\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> common.topic.notable_types -> Country\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> common.topic.notable_types -> City/Town/Village\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.languages_spoken -> Spanish Language\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Ontario\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.people_born_here -> Pattie Mallette\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nJuno Awards of 2014", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.people_born_here -> Jeremy Bieber\n# Answer:\nStratford"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-828", "prediction": ["# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> people.appointed_role.appointment -> m.0hn35mm\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> people.appointed_role.appointment -> m.063dqnv\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> people.appointed_role.appointment -> m.0hmsrqy\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> common.topic.image -> Official roberts CJ\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> common.topic.image -> Seal of the Supreme Court\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> law.judicial_title.judges -> m.046x3bs\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> law.court.inferior_courts -> Vermont Supreme Court -> law.court.jurisdiction -> Vermont\n# Answer:\nVermont Supreme Court", "# Reasoning Path:\nSupreme Court of the United States -> law.court.inferior_courts -> Arkansas Supreme Court -> law.court.judges -> m.0wzgtvm\n# Answer:\nArkansas Supreme Court", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> law.judicial_title.judges -> m.05lwk8w\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> law.court.inferior_courts -> Vermont Supreme Court -> law.court.judges -> m.04flh4j\n# Answer:\nVermont Supreme Court"], "ground_truth": ["Chief Justice of the United States"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-829", "prediction": ["# Reasoning Path:\nFrida Kahlo -> influence.influence_node.influenced_by -> Diego Rivera -> influence.influence_node.influenced_by -> Georges Braque\n# Answer:\nDiego Rivera", "# Reasoning Path:\nFrida Kahlo -> influence.influence_node.influenced_by -> Diego Rivera -> influence.influence_node.influenced_by -> Pablo Picasso\n# Answer:\nDiego Rivera", "# Reasoning Path:\nFrida Kahlo -> influence.influence_node.influenced_by -> Diego Rivera -> people.person.religion -> Atheism\n# Answer:\nDiego Rivera", "# Reasoning Path:\nFrida Kahlo -> influence.influence_node.influenced_by -> Diego Rivera -> influence.influence_node.influenced_by -> Paul C\u00e9zanne\n# Answer:\nDiego Rivera", "# Reasoning Path:\nFrida Kahlo -> media_common.netflix_title.netflix_genres -> Art & Design\n# Answer:\nArt & Design", "# Reasoning Path:\nFrida Kahlo -> film.film.genre -> World cinema\n# Answer:\nWorld cinema", "# Reasoning Path:\nFrida Kahlo -> influence.influence_node.influenced_by -> Diego Rivera -> people.person.children -> Ruth Rivera\n# Answer:\nDiego Rivera", "# Reasoning Path:\nFrida Kahlo -> media_common.netflix_title.netflix_genres -> Art and Design\n# Answer:\nArt and Design", "# Reasoning Path:\nFrida Kahlo -> film.film.genre -> Biographical film\n# Answer:\nBiographical film", "# Reasoning Path:\nFrida Kahlo -> media_common.netflix_title.netflix_genres -> Documentary film\n# Answer:\nDocumentary film"], "ground_truth": ["Diego Rivera"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_used -> East German mark\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> West Germany -> location.country.currency_used -> Deutsche Mark\n# Answer:\nWest Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Vaduz\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Switzerland\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_used -> Deutsche Mark\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.people_born_here -> Andreas Bielau\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0102xvg7 -> education.education.student -> Sigurd Burckhardt\n# Answer:\nSigurd Burckhardt", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> West Germany -> location.location.containedby -> Western Europe\n# Answer:\nWest Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Balzers\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> olympics.olympic_participating_country.athletes -> m.04dq7vv\n# Answer:\nEast Germany"], "ground_truth": ["Austria", "Liechtenstein", "Germany", "Luxembourg", "Switzerland", "East Germany", "Belgium"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.5581395348837209, "ans_precission": 0.8, "ans_recall": 0.42857142857142855, "path_f1": 0.3870967741935483, "path_precision": 0.6, "path_recall": 0.2857142857142857, "path_ans_f1": 0.6990291262135921, "path_ans_precision": 0.9, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-830", "prediction": ["# Reasoning Path:\nTaylor Swift -> people.person.place_of_birth -> Reading -> location.location.containedby -> Berks County\n# Answer:\nReading", "# Reasoning Path:\nTaylor Swift -> people.person.place_of_birth -> Reading -> base.biblioness.bibs_location.state -> Pennsylvania\n# Answer:\nReading", "# Reasoning Path:\nTaylor Swift -> people.person.place_of_birth -> Reading -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nReading", "# Reasoning Path:\nTaylor Swift -> people.person.place_of_birth -> Reading -> location.location.containedby -> Pennsylvania\n# Answer:\nReading", "# Reasoning Path:\nTaylor Swift -> base.popstra.celebrity.friendship -> m.06461gz -> base.popstra.friendship.participant -> Selena Gomez\n# Answer:\nSelena Gomez", "# Reasoning Path:\nTaylor Swift -> base.popstra.celebrity.friendship -> m.06461h7 -> base.popstra.friendship.participant -> Miley Cyrus\n# Answer:\nMiley Cyrus", "# Reasoning Path:\nTaylor Swift -> base.popstra.celebrity.friendship -> m.06461k9 -> base.popstra.friendship.participant -> Demi Lovato\n# Answer:\nDemi Lovato", "# Reasoning Path:\nTaylor Swift -> music.artist.album -> Red -> music.recording.tracks -> m.0v3zlb9\n# Answer:\nRed", "# Reasoning Path:\nTaylor Swift -> music.artist.album -> Red -> music.album.release_type -> Single\n# Answer:\nRed", "# Reasoning Path:\nTaylor Swift -> music.artist.album -> 1989 -> common.topic.notable_types -> Musical Album\n# Answer:\n1989"], "ground_truth": ["Reading"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-831", "prediction": ["# Reasoning Path:\nArgentina -> location.statistical_region.religions -> m.05bp67g -> location.religion_percentage.religion -> Judaism\n# Answer:\nJudaism", "# Reasoning Path:\nArgentina -> location.statistical_region.religions -> m.05bp679 -> location.religion_percentage.religion -> Catholicism\n# Answer:\nCatholicism", "# Reasoning Path:\nArgentina -> location.statistical_region.religions -> m.05bp674 -> location.religion_percentage.religion -> Protestantism\n# Answer:\nProtestantism", "# Reasoning Path:\nArgentina -> location.location.partially_contains -> Alto San Juan -> location.location.partially_contained_by -> m.0wg8lvc\n# Answer:\nAlto San Juan", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Asiatic Boy -> base.thoroughbredracing.thoroughbred_racehorse.color -> Chestnut\n# Answer:\nAsiatic Boy", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Asiatic Boy -> biology.organism.sex -> Male\n# Answer:\nAsiatic Boy", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Candy Ride -> biology.organism.organism_type -> Horse\n# Answer:\nCandy Ride", "# Reasoning Path:\nArgentina -> location.location.partially_contains -> Alto San Juan -> location.location.partially_containedby -> Chile\n# Answer:\nAlto San Juan", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Bayakoa -> base.thoroughbredracing.thoroughbred_racehorse.color -> Bay\n# Answer:\nBayakoa", "# Reasoning Path:\nArgentina -> location.location.partially_contains -> Alto San Juan -> common.topic.notable_types -> Mountain\n# Answer:\nAlto San Juan"], "ground_truth": ["Protestantism", "Judaism", "Catholicism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-832", "prediction": ["# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.championships -> 2014 World Series -> sports.sports_championship_event.runner_up -> Kansas City Royals\n# Answer:\n2014 World Series", "# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.championships -> 2014 World Series -> sports.sports_championship_event.championship -> World Series\n# Answer:\n2014 World Series", "# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.championships -> 2014 World Series -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n2014 World Series", "# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.championships -> 2012 World Series -> time.event.locations -> Comerica Park\n# Answer:\n2012 World Series", "# Reasoning Path:\nSan Francisco Giants -> time.participant.event -> 2014 Major League Baseball season -> time.event.participant -> Kansas City Royals\n# Answer:\n2014 Major League Baseball season", "# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.championships -> 2010 World Series -> time.event.locations -> Rangers Ballpark in Arlington\n# Answer:\n2010 World Series", "# Reasoning Path:\nSan Francisco Giants -> time.participant.event -> 2014 Major League Baseball season -> time.event.next_in_series -> 2015 Major League Baseball season\n# Answer:\n2014 Major League Baseball season", "# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.championships -> 2012 World Series -> common.topic.notable_for -> g.125fqv3d3\n# Answer:\n2012 World Series", "# Reasoning Path:\nSan Francisco Giants -> time.participant.event -> 2014 Major League Baseball season -> common.topic.notable_types -> Sports League Season\n# Answer:\n2014 Major League Baseball season", "# Reasoning Path:\nSan Francisco Giants -> baseball.baseball_team.team_stats -> m.05n69_j -> baseball.baseball_team_stats.season -> 1965 Major League Baseball Season\n# Answer:\n1965 Major League Baseball Season"], "ground_truth": ["2014 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-833", "prediction": ["# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> language.human_language.countries_spoken_in -> New Zealand\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> common.topic.notable_types -> Human Language\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> language.human_language.countries_spoken_in -> Macau\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.location.partially_contains -> Teram Kangri -> base.schemastaging.disputed_location.claimed_by -> Pakistan\n# Answer:\nTeram Kangri", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Yili horse -> common.topic.notable_types -> Organism Classification\n# Answer:\nYili horse", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Black Telescope -> biology.animal_breed.breed_of -> Goldfish\n# Answer:\nBlack Telescope", "# Reasoning Path:\nChina -> location.location.partially_contains -> Amur River -> location.location.partially_containedby -> Russia\n# Answer:\nAmur River", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Balikun horse -> common.topic.notable_types -> Organism Classification\n# Answer:\nBalikun horse", "# Reasoning Path:\nChina -> location.location.partially_contains -> Teram Kangri -> location.location.partially_containedby -> Xinjiang\n# Answer:\nTeram Kangri"], "ground_truth": ["Standard Chinese"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-834", "prediction": ["# Reasoning Path:\nBattle of Waterloo -> time.event.locations -> Waterloo -> book.written_work.subjects -> Napoleonic era\n# Answer:\nWaterloo", "# Reasoning Path:\nBattle of Waterloo -> time.event.locations -> Waterloo -> book.written_work.subjects -> Napoleonic Wars\n# Answer:\nWaterloo", "# Reasoning Path:\nBattle of Waterloo -> time.event.locations -> Waterloo -> location.location.people_born_here -> Vic Lawrence\n# Answer:\nWaterloo", "# Reasoning Path:\nBattle of Waterloo -> base.ontologies.ontology_instance.equivalent_instances -> m.09bxfm0 -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nOpenCyc", "# Reasoning Path:\nBattle of Waterloo -> time.event.locations -> Waterloo -> book.written_work.subjects -> Belgium\n# Answer:\nWaterloo", "# Reasoning Path:\nBattle of Waterloo -> time.event.locations -> Waterloo -> location.location.geolocation -> m.0239s57\n# Answer:\nWaterloo", "# Reasoning Path:\nBattle of Waterloo -> time.event.locations -> Waterloo -> location.location.people_born_here -> Arthur Tauchert\n# Answer:\nWaterloo", "# Reasoning Path:\nBattle of Waterloo -> time.event.locations -> Waterloo -> location.location.people_born_here -> Clarrie Stevenson\n# Answer:\nWaterloo", "# Reasoning Path:\nBattle of Waterloo -> time.event.locations -> Waterloo -> location.location.geolocation -> m.0cpd0qs\n# Answer:\nWaterloo", "# Reasoning Path:\nBattle of Waterloo -> military.military_conflict.military_personnel_involved -> Andrew Barnard -> military.military_person.participated_in_conflicts -> Napoleonic Wars\n# Answer:\nAndrew Barnard"], "ground_truth": ["Waterloo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-836", "prediction": ["# Reasoning Path:\nVladimir Lenin -> people.person.parents -> Maria Alexandrovna Ulyanova -> people.person.children -> Alexandre Ilich Ulianov\n# Answer:\nMaria Alexandrovna Ulyanova", "# Reasoning Path:\nVladimir Lenin -> people.person.parents -> Maria Alexandrovna Ulyanova -> people.person.spouse_s -> m.0j81_b0\n# Answer:\nMaria Alexandrovna Ulyanova", "# Reasoning Path:\nVladimir Lenin -> base.activism.activist.area_of_activism -> Atheism -> base.activism.activism_issue.activists -> Joseph Stalin\n# Answer:\nAtheism", "# Reasoning Path:\nVladimir Lenin -> people.person.parents -> Maria Alexandrovna Ulyanova -> people.person.children -> Anna Ilichina Ulianova\n# Answer:\nMaria Alexandrovna Ulyanova", "# Reasoning Path:\nVladimir Lenin -> people.person.parents -> Maria Alexandrovna Ulyanova -> people.person.children -> Dmitry Ilyich Ulyanov\n# Answer:\nMaria Alexandrovna Ulyanova", "# Reasoning Path:\nVladimir Lenin -> people.person.parents -> Maria Alexandrovna Ulyanova -> common.topic.notable_for -> g.125f_5s6_\n# Answer:\nMaria Alexandrovna Ulyanova", "# Reasoning Path:\nVladimir Lenin -> people.person.parents -> Ilya Ulyanov -> people.person.children -> Alexandre Ilich Ulianov\n# Answer:\nIlya Ulyanov", "# Reasoning Path:\nVladimir Lenin -> people.person.parents -> Ilya Ulyanov -> people.person.spouse_s -> m.0j81_b0\n# Answer:\nIlya Ulyanov", "# Reasoning Path:\nVladimir Lenin -> base.activism.activist.area_of_activism -> Atheism -> base.activism.activism_issue.activists -> Karl Marx\n# Answer:\nAtheism", "# Reasoning Path:\nVladimir Lenin -> base.activism.activist.area_of_activism -> Atheism -> organization.organization_sector.organizations_in_this_sector -> Atheist Alliance International\n# Answer:\nAtheism"], "ground_truth": ["Nadezhda Krupskaya"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-837", "prediction": ["# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.article -> m.0642vqz\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> sports.sports_championship_event.season -> 2000 NFL season\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> film.film_subject.films -> The Band That Wouldn't Die -> film.film.music -> Marcello Zavros\n# Answer:\nThe Band That Wouldn't Die", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> 2001 AFC Championship Game -> sports.sports_championship_event.runner_up -> Oakland Raiders\n# Answer:\n2001 AFC Championship Game", "# Reasoning Path:\nBaltimore Ravens -> film.film_subject.films -> The Band That Wouldn't Die -> film.film.subjects -> Baltimore Colts\n# Answer:\nThe Band That Wouldn't Die", "# Reasoning Path:\nBaltimore Ravens -> american_football.football_team.away_games -> Baltimore Ravens at Cincinnati Bengals, 2009-11-08 -> american_football.football_game.season -> 2009 NFL season\n# Answer:\nBaltimore Ravens at Cincinnati Bengals, 2009-11-08", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> 2001 AFC Championship Game -> sports.sports_championship_event.season -> 2000 NFL season\n# Answer:\n2001 AFC Championship Game", "# Reasoning Path:\nBaltimore Ravens -> american_football.football_team.away_games -> Baltimore Ravens at Cincinnati Bengals, 2008-11-30 -> american_football.football_game.season -> 2008 NFL season\n# Answer:\nBaltimore Ravens at Cincinnati Bengals, 2008-11-30"], "ground_truth": ["Super Bowl XLVII", "Super Bowl XXXV"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-838", "prediction": ["# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w9gdcb -> soccer.football_player_stats.team -> Sporting-soc Desportiva De\n# Answer:\nSporting-soc Desportiva De", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.0k061tt -> soccer.football_goal.point_awarded_to -> Portugal national football team\n# Answer:\nPortugal national football team", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.0k061tt -> soccer.football_goal.match -> UEFA Euro 2012 Quarter-final I\n# Answer:\nUEFA Euro 2012 Quarter-final I", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzrlj -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w8w78v -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.match -> 2014 Real Madrid CF vs. CA Osasuna football match\n# Answer:\n2014 Real Madrid CF vs. CA Osasuna football match", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w8w78v -> soccer.football_player_stats.team -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w8w7f0 -> soccer.football_player_stats.team -> Portugal national football team\n# Answer:\nPortugal national football team", "# Reasoning Path:\nCristiano Ronaldo -> award.award_winner.awards_won -> m.010nx4bl -> award.award_honor.award -> UEFA Champions League Top Goalscorer\n# Answer:\nUEFA Champions League Top Goalscorer"], "ground_truth": ["Real Madrid C.F.", "Portugal national football team"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.3333333333333333, "path_precision": 0.5, "path_recall": 0.25, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Rock music\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> common.topic.notable_types -> Musical genre\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> common.topic.notable_types -> Musical genre\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.recordings -> Chasing a Feeling\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Pop rock\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> music.genre.subgenre -> Math rock\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music -> music.genre.parent_genre -> Rock music\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Rock and roll\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music -> common.topic.subject_of -> Stephen Melton\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Up Your Legs Forever -> film.film.genre -> Drama\n# Answer:\nUp Your Legs Forever"], "ground_truth": ["Rock music", "Psychedelic rock", "Soft rock", "Experimental music", "Pop music", "Blues rock", "Art rock", "Experimental rock", "Pop rock"], "ans_acc": 0.5555555555555556, "ans_hit": 1, "ans_f1": 0.48648648648648646, "ans_precission": 0.9, "ans_recall": 0.3333333333333333, "path_f1": 0.48648648648648646, "path_precision": 0.9, "path_recall": 0.3333333333333333, "path_ans_f1": 0.6870229007633587, "path_ans_precision": 0.9, "path_ans_recall": 0.5555555555555556}
{"id": "WebQTest-841", "prediction": ["# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Vatican City\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> common.topic.notable_types -> Currency\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Belgium\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37cxr\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Tenerani -> base.thoroughbredracing.thoroughbred_racehorse.foal_if_this_horse_is_sire -> Ribot\n# Answer:\nTenerani", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Tenerani -> biology.organism.sex -> Male\n# Answer:\nTenerani", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Nearco -> biology.organism.sex -> Male\n# Answer:\nNearco"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-843", "prediction": ["# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0n0tzlj -> education.education.institution -> Humboldt University of Berlin\n# Answer:\nHumboldt University of Berlin", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmxyk -> education.education.institution -> University of W\u00fcrzburg\n# Answer:\nUniversity of W\u00fcrzburg", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmy28 -> education.education.institution -> University of Bonn\n# Answer:\nUniversity of Bonn", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0n0tzlj -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nTheodor Schwann -> people.deceased_person.place_of_death -> Cologne -> location.location.contains -> University of Bonn\n# Answer:\nCologne", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmxyk -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nTheodor Schwann -> common.topic.image -> Theodore Schwann -> common.image.size -> m.0klz7g\n# Answer:\nTheodore Schwann", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmy28 -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nTheodor Schwann -> people.deceased_person.place_of_death -> Cologne -> location.location.contains -> Academy of Media Arts Cologne\n# Answer:\nCologne", "# Reasoning Path:\nTheodor Schwann -> people.deceased_person.place_of_death -> Cologne -> travel.travel_destination.tourist_attractions -> K\u00e4the Kollwitz Museum\n# Answer:\nCologne"], "ground_truth": ["University of Bonn", "University of W\u00fcrzburg", "Humboldt University of Berlin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-845", "prediction": ["# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> government.government_position_held.office_holder -> Tim Kaine\n# Answer:\nTim Kaine", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcw0 -> government.government_position_held.office_holder -> Jim Webb\n# Answer:\nJim Webb", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcm1 -> government.government_position_held.office_holder -> Mark Warner\n# Answer:\nMark Warner", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nUnited States Senate", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcm1 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcw0 -> government.government_position_held.legislative_sessions -> 111th United States Congress\n# Answer:\n111th United States Congress", "# Reasoning Path:\nVirginia -> location.location.containedby -> United States of America -> meteorology.cyclone_affected_area.cyclones -> Hurricane Isabel\n# Answer:\nUnited States of America", "# Reasoning Path:\nVirginia -> location.location.containedby -> Charles Irving Thornton Tombstone -> location.location.geolocation -> m.0zjvs72\n# Answer:\nCharles Irving Thornton Tombstone", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcm1 -> government.government_position_held.legislative_sessions -> 111th United States Congress\n# Answer:\n111th United States Congress"], "ground_truth": ["Tim Kaine", "Jim Webb", "Mark Warner"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-846", "prediction": ["# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Visual Artist -> fictional_universe.character_occupation.characters_with_this_occupation -> Eug\u00e8ne Delacroix\n# Answer:\nVisual Artist", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Visual Artist -> common.topic.subject_of -> Michael Godard\n# Answer:\nVisual Artist", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Visual Artist -> base.descriptive_names.names.descriptive_name -> m.0106_1y2\n# Answer:\nVisual Artist", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> common.topic.subject_of -> Dum-Doodles\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> theater.theater_production_staff_role.people_who_have_had_this_role -> m.0111n9h6\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Artist -> people.profession.specializations -> Visual Artist\n# Answer:\nArtist", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Visual Artist -> base.descriptive_names.names.descriptive_name -> m.0106_2f_\n# Answer:\nVisual Artist", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> theater.theater_production_staff_role.people_who_have_had_this_role -> m.0_grv_v\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> book.author.works_written -> ARE YOU MY MOTHER MINI PB -> common.topic.notable_for -> g.125fbm0pz\n# Answer:\nARE YOU MY MOTHER MINI PB", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Visual Artist -> base.descriptive_names.names.descriptive_name -> m.0106_3fj\n# Answer:\nVisual Artist"], "ground_truth": ["Theodor Seuss Geisel"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-847", "prediction": ["# Reasoning Path:\nUnited Nations Security Council -> base.unitednations.united_nations_body.members -> m.08__kq7 -> base.unitednations.united_nations_body_membership.member -> Costa Rica\n# Answer:\nCosta Rica", "# Reasoning Path:\nUnited Nations Security Council -> base.unitednations.united_nations_body.members -> m.08__knr -> base.unitednations.united_nations_body_membership.member -> Burkina Faso\n# Answer:\nBurkina Faso", "# Reasoning Path:\nUnited Nations Security Council -> base.unitednations.united_nations_body.members -> m.08__j0t -> base.unitednations.united_nations_body_membership.member -> France\n# Answer:\nFrance", "# Reasoning Path:\nUnited Nations Security Council -> common.topic.image -> UN security council 2005 -> common.image.appears_in_topic_gallery -> United Nations Security Council Resolution 1318\n# Answer:\nUN security council 2005", "# Reasoning Path:\nUnited Nations Security Council -> common.image.appears_in_topic_gallery -> Great power\n# Answer:\nGreat power", "# Reasoning Path:\nUnited Nations Security Council -> common.topic.image -> UN security council 2005 -> common.image.size -> m.0291vv2\n# Answer:\nUN security council 2005", "# Reasoning Path:\nUnited Nations Security Council -> common.image.appears_in_topic_gallery -> Reform of the United Nations Security Council\n# Answer:\nReform of the United Nations Security Council", "# Reasoning Path:\nUnited Nations Security Council -> common.topic.image -> UN security council 2005 -> common.image.appears_in_topic_gallery -> Arnstein Arneberg\n# Answer:\nUN security council 2005", "# Reasoning Path:\nUnited Nations Security Council -> common.image.appears_in_topic_gallery -> United Nations Security Council Resolution 1\n# Answer:\nUnited Nations Security Council Resolution 1", "# Reasoning Path:\nUnited Nations Security Council -> common.topic.image -> UN security council 2005 -> common.image.appears_in_topic_gallery -> Great power\n# Answer:\nUN security council 2005"], "ground_truth": ["Vietnam", "United States of America", "Burkina Faso", "France", "Russia", "Costa Rica", "United Kingdom", "Croatia", "China"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.3157894736842105, "ans_precission": 0.3, "ans_recall": 0.3333333333333333, "path_f1": 0.3157894736842105, "path_precision": 0.3, "path_recall": 0.3333333333333333, "path_ans_f1": 0.3157894736842105, "path_ans_precision": 0.3, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-848", "prediction": ["# Reasoning Path:\nNordic countries -> location.location.contains -> \u00c5land Islands -> location.location.contains -> Mariehamn sub-region\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nNordic countries -> base.schemastaging.context_name.pronunciation -> g.125_kxbsc\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nNordic countries -> location.location.contains -> \u00c5land Islands -> location.location.contains -> Archipelago\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nNordic countries -> location.location.partially_contained_by -> m.0_gjrmv -> location.partial_containment_relationship.partially_contained_by -> Northern Europe\n# Answer:\nNorthern Europe", "# Reasoning Path:\nNordic countries -> location.location.contains -> \u00c5land Islands -> location.location.contains -> Countryside\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nNordic countries -> location.location.contains -> \u00c5land Islands -> location.location.nearby_airports -> Mariehamn Airport\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nNordic countries -> location.location.contains -> \u00c5land Islands -> location.location.people_born_here -> Paul Gustavson\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nNordic countries -> location.location.contains -> \u00c5land Islands -> location.location.people_born_here -> Gabriel Fliflet\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nNordic countries -> location.location.contains -> Denmark -> location.country.second_level_divisions -> Middelfart Municipality\n# Answer:\nDenmark", "# Reasoning Path:\nNordic countries -> location.location.contains -> \u00c5land Islands -> location.location.people_born_here -> Georg August Wallin\n# Answer:\n\u00c5land Islands"], "ground_truth": ["Sweden", "Denmark", "Iceland", "Faroe Islands", "Norway", "\u00c5land Islands", "Finland", "Greenland"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.38095238095238093, "ans_precission": 0.8, "ans_recall": 0.25, "path_f1": 0.38095238095238093, "path_precision": 0.8, "path_recall": 0.25, "path_ans_f1": 0.38095238095238093, "path_ans_precision": 0.8, "path_ans_recall": 0.25}
{"id": "WebQTest-849", "prediction": ["# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.location.people_born_here -> Afonso Mendes de Melo\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> g.11btv6zfvr\n# Answer:\nprotected_sites.listed_site.designation_as_natural_or_cultural_site", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.location.people_born_here -> Afonso Pires Gato\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> government.governmental_jurisdiction.governing_officials -> m.04_9736\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.location.people_born_here -> Ant\u00f3nio Lu\u00eds de Sousa, 2nd Marquis of Minas\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.country.capital -> Lisbon\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> government.governmental_jurisdiction.governing_officials -> m.04_975f\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> symbols.name_source.namesakes -> Magellan -> common.topic.article -> m.01m0j_\n# Answer:\nMagellan", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> government.governmental_jurisdiction.governing_officials -> m.0c0mt_c\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.country.capital -> Coimbra\n# Answer:\nKingdom of Portugal"], "ground_truth": ["Kingdom of Portugal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> government.government_position_held.office_holder -> Michael Bennet\n# Answer:\nMichael Bennet", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.office_holder -> Mark Udall\n# Answer:\nMark Udall", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr -> government.government_position_held.office_holder -> Wayne Allard\n# Answer:\nWayne Allard", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nBasic title", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.legislative_sessions -> 111th United States Congress\n# Answer:\n111th United States Congress", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008 -> common.topic.image -> Mark Udall\n# Answer:\nUnited States Senate election in Colorado, 2008", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> freebase.valuenotation.is_reviewed -> District represented (if position is district-related)\n# Answer:\nDistrict represented (if position is district-related)", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008 -> government.election.general_election -> United States Senate elections, 2008\n# Answer:\nUnited States Senate election in Colorado, 2008"], "ground_truth": ["Michael Bennet", "Mark Udall"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-850", "prediction": ["# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.treatments -> Erlotinib\n# Answer:\nCancer", "# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Anthony Casso\n# Answer:\nCancer", "# Reasoning Path:\nVera-Ellen -> common.topic.article -> m.022smc\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Bryan Murray\n# Answer:\nCancer", "# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Deborah King\n# Answer:\nCancer", "# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nCancer", "# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04d7nvm\n# Answer:\nCancer", "# Reasoning Path:\nVera-Ellen -> film.actor.film -> m.0220nxg -> film.performance.film -> Love Happy\n# Answer:\nLove Happy", "# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.treatments -> Radiation therapy\n# Answer:\nCancer", "# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjf_c\n# Answer:\nCancer"], "ground_truth": ["Cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-851", "prediction": ["# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo -> location.location.containedby -> Cairo Governorate\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo -> periodicals.newspaper_circulation_area.newspapers -> Al-Ahram Weekly\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.capital -> Cairo -> periodicals.newspaper_circulation_area.newspapers -> Al-Ahram Weekly\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo Governorate -> location.location.containedby -> North Africa\n# Answer:\nCairo Governorate", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo Governorate -> common.topic.notable_types -> Administrative Division\n# Answer:\nCairo Governorate", "# Reasoning Path:\nEgypt -> location.country.capital -> Cairo -> common.topic.subject_of -> As Egypt Howls and History Tweets\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo -> periodicals.newspaper_circulation_area.newspapers -> Almesryoon\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Al Sharqia Governorate -> location.location.containedby -> North Africa\n# Answer:\nAl Sharqia Governorate", "# Reasoning Path:\nEgypt -> location.country.capital -> Cairo -> periodicals.newspaper_circulation_area.newspapers -> Almesryoon\n# Answer:\nCairo"], "ground_truth": ["Cairo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-852", "prediction": ["# Reasoning Path:\nToronto -> location.location.containedby -> Ontario -> location.location.containedby -> Canada\n# Answer:\nOntario", "# Reasoning Path:\nToronto -> base.biblioness.bibs_location.state -> Ontario -> location.location.containedby -> Canada\n# Answer:\nOntario", "# Reasoning Path:\nToronto -> location.location.containedby -> Rada of the Belarusian Democratic Republic -> common.topic.article -> m.0tkkqmp\n# Answer:\nRada of the Belarusian Democratic Republic", "# Reasoning Path:\nToronto -> location.location.containedby -> Ontario -> location.administrative_division.country -> Canada\n# Answer:\nOntario", "# Reasoning Path:\nToronto -> base.biblioness.bibs_location.state -> Ontario -> location.administrative_division.country -> Canada\n# Answer:\nOntario", "# Reasoning Path:\nToronto -> location.location.containedby -> Canada -> location.country.official_language -> English Language\n# Answer:\nCanada", "# Reasoning Path:\nToronto -> location.location.containedby -> Canada -> location.country.languages_spoken -> English Language\n# Answer:\nCanada", "# Reasoning Path:\nToronto -> common.topic.webpage -> m.02nc8t6 -> common.webpage.resource -> City of Toronto Web site\n# Answer:\nCity of Toronto Web site", "# Reasoning Path:\nToronto -> location.location.containedby -> Canada -> location.country.first_level_divisions -> Ontario\n# Answer:\nCanada", "# Reasoning Path:\nToronto -> location.location.containedby -> Canada -> location.country.official_language -> French\n# Answer:\nCanada"], "ground_truth": ["Ontario"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-855", "prediction": ["# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc39sjp\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fm01\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc385hl\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp"], "ground_truth": ["Brazilian real"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-856", "prediction": ["# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> visual_art.art_period_movement.associated_artworks -> Mickey\n# Answer:\nModern art", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.subjects -> Wil Emling\n# Answer:\nModern art", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.subject_of -> Wil Emling\n# Answer:\nModern art", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.art_forms -> Printmaking\n# Answer:\nPrintmaking", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Pop art -> business.product_theme.product_lines -> TAG Registries\n# Answer:\nPop art", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Pop art -> business.product_theme.products -> Andy Warhol Registry\n# Answer:\nPop art", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Pop art -> visual_art.art_period_movement.associated_artists -> Takashi Murakami\n# Answer:\nPop art", "# Reasoning Path:\nAndy Warhol -> influence.influence_node.influenced -> Bill Viola -> visual_art.visual_artist.associated_periods_or_movements -> Contemporary art\n# Answer:\nBill Viola", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Pop art -> visual_art.art_period_movement.associated_artists -> Antonia Gerstacker\n# Answer:\nPop art", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.art_forms -> Painting -> business.product_theme.products -> Andy Warhol Registry\n# Answer:\nPainting"], "ground_truth": ["Modern art", "Pop art"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-857", "prediction": ["# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> organization.organization.geographic_scope -> Alabama\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> CRHOME -> organization.organization.geographic_scope -> Alabama\n# Answer:\nCRHOME", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> organization.organization.geographic_scope -> Arkansas\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> location.location.partially_contains -> Blackwater River -> location.location.partially_containedby -> Alabama\n# Answer:\nBlackwater River", "# Reasoning Path:\nFlorida -> location.location.partially_contains -> Chattahoochee River -> location.location.partially_containedby -> Georgia\n# Answer:\nChattahoochee River", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> organization.organization.geographic_scope -> California\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> B'nai B'rith District Grand Lodge 7 -> organization.organization.geographic_scope -> Alabama\n# Answer:\nB'nai B'rith District Grand Lodge 7", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> common.topic.subjects -> Automation\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> CRHOME -> organization.organization.geographic_scope -> Arizona\n# Answer:\nCRHOME", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> CRHOME -> organization.organization.place_founded -> Los Angeles\n# Answer:\nCRHOME"], "ground_truth": ["Alabama", "Georgia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.25641025641025644, "path_precision": 0.5, "path_recall": 0.1724137931034483, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-859", "prediction": ["# Reasoning Path:\nRussia -> location.statistical_region.religions -> m.06jmmyk -> location.religion_percentage.religion -> Islam\n# Answer:\nIslam", "# Reasoning Path:\nRussia -> location.statistical_region.religions -> m.06jmmyb -> location.religion_percentage.religion -> Russian Orthodox Church\n# Answer:\nRussian Orthodox Church", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.11b60vv5th\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nRussia -> location.statistical_region.co2_emissions_per_capita -> g.1245_8036\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.12tb6gh0b\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.1hhc3f_fx\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nRussia -> location.statistical_region.co2_emissions_per_capita -> g.1245_9w0x\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nRussia -> location.statistical_region.co2_emissions_per_capita -> g.1245_chng\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita"], "ground_truth": ["Islam", "Russian Orthodox Church"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Capital Region of Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Central Denmark Region\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Northern Europe\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries -> location.location.contains -> Denmark\n# Answer:\nNordic countries", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> g.1hhc37psk\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> North Denmark Region\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Nordic countries\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.form_of_government -> Parliamentary system\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries -> location.location.contains -> \u00c5land Islands\n# Answer:\nNordic countries", "# Reasoning Path:\nGreenland -> location.location.containedby -> Kingdom of Denmark -> base.aareas.schema.administrative_area.administrative_children -> Denmark\n# Answer:\nKingdom of Denmark"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-861", "prediction": ["# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.0j81th4 -> american_football.football_historical_coach_position.coach -> John Fox\n# Answer:\nJohn Fox", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.0j81th4 -> american_football.football_historical_coach_position.position -> Head coach\n# Answer:\nHead coach", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.04xg_yc -> american_football.football_historical_coach_position.coach -> Dom Capers\n# Answer:\nDom Capers", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.coaches -> m.0_3r22s -> sports.sports_team_coach_tenure.coach -> John Fox\n# Answer:\nJohn Fox", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.04xg_yl -> american_football.football_historical_coach_position.coach -> George Seifert\n# Answer:\nGeorge Seifert", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.coaches -> m.0_3r22s -> sports.sports_team_coach_tenure.position -> Head coach\n# Answer:\nHead coach", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> american_football.football_coach.coaching_history -> m.0hpyjn8\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> award.award_winner.awards_won -> m.0_qw472\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> american_football.football_coach.coaching_history -> m.0hpyjm4\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> american_football.football_coach.coaching_history -> m.0hpyjm_\n# Answer:\nRon Rivera"], "ground_truth": ["Ron Rivera"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-862", "prediction": ["# Reasoning Path:\nLouis Sachar -> people.person.place_of_birth -> East Meadow -> location.location.people_born_here -> William Fichtner\n# Answer:\nEast Meadow", "# Reasoning Path:\nLouis Sachar -> people.person.place_of_birth -> East Meadow -> location.hud_foreclosure_area.estimated_number_of_mortgages -> m.07f2vhv\n# Answer:\nEast Meadow", "# Reasoning Path:\nLouis Sachar -> people.person.place_of_birth -> East Meadow -> location.location.people_born_here -> Adam Busch\n# Answer:\nEast Meadow", "# Reasoning Path:\nLouis Sachar -> people.person.place_of_birth -> East Meadow -> location.location.people_born_here -> Betsy Aidem\n# Answer:\nEast Meadow", "# Reasoning Path:\nLouis Sachar -> people.person.place_of_birth -> East Meadow -> common.topic.image -> East-meadow-ny-map\n# Answer:\nEast Meadow", "# Reasoning Path:\nLouis Sachar -> people.person.place_of_birth -> East Meadow -> common.topic.image -> Hempstead Town Hall on Front Street in Hempstead Village\n# Answer:\nEast Meadow", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> Hoyos/Holes -> common.topic.notable_types -> Book\n# Answer:\nHoyos/Holes", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> Hoyos/Holes -> common.topic.notable_for -> g.1256n6bzc\n# Answer:\nHoyos/Holes", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> A Flying Birthday Cake? -> common.topic.notable_for -> g.11bbqmptyt\n# Answer:\nA Flying Birthday Cake?", "# Reasoning Path:\nLouis Sachar -> film.writer.film -> Holes -> common.topic.article -> m.03tgtz\n# Answer:\nHoles"], "ground_truth": ["East Meadow"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-865", "prediction": ["# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Wet'n'Wild SplashTown -> amusement_parks.park.rides -> Tornado\n# Answer:\nWet'n'Wild SplashTown", "# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Wet'n'Wild SplashTown -> common.topic.article -> m.063g20\n# Answer:\nWet'n'Wild SplashTown", "# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Wet'n'Wild SplashTown -> common.topic.notable_for -> g.125dd3mwm\n# Answer:\nWet'n'Wild SplashTown", "# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Wet'n'Wild SplashTown -> amusement_parks.park.rides -> RipQurl\n# Answer:\nWet'n'Wild SplashTown", "# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Kemah Boardwalk -> location.location.containedby -> United States of America\n# Answer:\nKemah Boardwalk", "# Reasoning Path:\nHouston -> location.statistical_region.population -> g.11b66fk05m\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Wet'n'Wild SplashTown -> amusement_parks.park.rides -> Stingray Racer\n# Answer:\nWet'n'Wild SplashTown", "# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Kemah Boardwalk -> common.topic.notable_types -> Amusement Park\n# Answer:\nKemah Boardwalk", "# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Houston Zoo -> common.topic.notable_for -> g.125d_93ry\n# Answer:\nHouston Zoo", "# Reasoning Path:\nHouston -> location.place_with_neighborhoods.neighborhoods -> Addicks -> location.location.containedby -> Harris County\n# Answer:\nAddicks"], "ground_truth": ["Downtown Aquarium, Houston", "Space Center Houston", "Kemah Boardwalk", "Children's Museum of Houston", "Houston Arboretum and Nature Center", "Houston Museum of Natural Science", "Lyndon B. Johnson Space Center", "Rothko Chapel", "Gerald D. Hines Waterwall Park", "Museum of Fine Arts, Houston", "Wet'n'Wild SplashTown", "Houston Zoo", "USS Texas (BB-35)", "Houston Marathon", "Sam Houston Race Park", "George R. Brown Convention Center", "Hanna\u2013Barbera Land", "The Galleria", "Bayou Bend Collection and Gardens", "Toyota Center", "Lakewood Church"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0.24242424242424243, "ans_precission": 0.8, "ans_recall": 0.14285714285714285, "path_f1": 0.24242424242424243, "path_precision": 0.8, "path_recall": 0.14285714285714285, "path_ans_f1": 0.24242424242424243, "path_ans_precision": 0.8, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-866", "prediction": ["# Reasoning Path:\nOtto Frank -> people.person.education -> m.0gft51q -> education.education.institution -> Heidelberg University\n# Answer:\nHeidelberg University", "# Reasoning Path:\nOtto Frank -> common.topic.image -> 100_1380.jpg\n# Answer:\n100_1380.jpg", "# Reasoning Path:\nOtto Frank -> base.popstra.celebrity.friendship -> m.0gvp1lx -> base.popstra.friendship.participant -> Max Goldberg\n# Answer:\nMax Goldberg", "# Reasoning Path:\nOtto Frank -> common.topic.image -> 100_2022.jpg\n# Answer:\n100_2022.jpg", "# Reasoning Path:\nOtto Frank -> base.popstra.celebrity.friendship -> m.0gvp2cx -> base.popstra.friendship.participant -> Hilde Goldberg\n# Answer:\nHilde Goldberg", "# Reasoning Path:\nOtto Frank -> base.popstra.celebrity.friendship -> m.0gvp2g0 -> base.popstra.friendship.participant -> Cor Suijk\n# Answer:\nCor Suijk", "# Reasoning Path:\nOtto Frank -> common.topic.image -> 100_2237.jpg -> common.image.size -> m.01x3sy_\n# Answer:\n100_2237.jpg"], "ground_truth": ["Heidelberg University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-867", "prediction": ["# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0_vw5gg -> award.award_nomination.nominated_for -> The Hunger Games: Catching Fire\n# Answer:\nThe Hunger Games: Catching Fire", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0r9hngb -> award.award_nomination.nominated_for -> House at the End of the Street\n# Answer:\nHouse at the End of the Street", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0_vw7_t -> award.award_nomination.nominated_for -> American Hustle\n# Answer:\nAmerican Hustle", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0_vw5gg -> freebase.valuenotation.is_reviewed -> Award\n# Answer:\nAward", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0_vw5gg -> award.award_nomination.award_nominee -> Josh Hutchersonm\n# Answer:\nJosh Hutchersonm", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0r9hngb -> award.award_nomination.award -> MTV Movie Award for Best Scared-As-S**t Performance\n# Answer:\nMTV Movie Award for Best Scared-As-S**t Performance", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0r9hngb -> award.award_nomination.ceremony -> 2013 MTV Movie Awards\n# Answer:\n2013 MTV Movie Awards", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0_vw7_t -> award.award_nomination.award -> MTV Movie Award for Best Kiss\n# Answer:\nMTV Movie Award for Best Kiss", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0_vw5gg -> award.award_nomination.award_nominee -> Sam Claflin\n# Answer:\nSam Claflin", "# Reasoning Path:\nJennifer Lawrence -> award.award_winner.awards_won -> m.0101mk_y -> award.award_honor.ceremony -> 2014 Kids' Choice Awards\n# Answer:\n2014 Kids' Choice Awards"], "ground_truth": ["Garden Party", "American Hustle", "Winter\u2019s Bone", "Silver Linings Playbook", "X-Men: Days of Future Past", "The Poker House", "The Hunger Games: Mockingjay, Part 2", "The Hunger Games", "House at the End of the Street", "X-Men: Apocalypse", "Devil You Know", "The Burning Plain", "Serena", "Joy", "The Hunger Games: Mockingjay, Part 1", "X-Men: First Class", "Not Another High School Show", "The Glass Castle", "East of Eden", "The Beaver", "Burial Rites", "Like Crazy", "Company Town", "The Hunger Games: Catching Fire"], "ans_acc": 0.16666666666666666, "ans_hit": 1, "ans_f1": 0.21428571428571427, "ans_precission": 0.3, "ans_recall": 0.16666666666666666, "path_f1": 0.0379746835443038, "path_precision": 0.3, "path_recall": 0.02027027027027027, "path_ans_f1": 0.21428571428571427, "path_ans_precision": 0.3, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTest-868", "prediction": ["# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.010f42tf -> government.government_position_held.office_holder -> Jim Cawley\n# Answer:\nJim Cawley", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.011crdxd -> government.government_position_held.office_holder -> Rosemary Brown\n# Answer:\nRosemary Brown", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.048157l -> government.government_position_held.office_holder -> Catherine Baker Knoll\n# Answer:\nCatherine Baker Knoll", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.title -> Director, Office of Health Care Reform\n# Answer:\nDirector, Office of Health Care Reform", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.010f42tf -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nBasic title", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.011crdxd -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.person -> Rosemarie B. Greco\n# Answer:\nRosemarie B. Greco", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.010f42tf -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nFrom", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.048157l -> government.government_position_held.office_position_or_title -> Lieutenant Governor of Pennsylvania\n# Answer:\nLieutenant Governor of Pennsylvania", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.010f42tf -> freebase.valuenotation.is_reviewed -> Jurisdiction of office\n# Answer:\nJurisdiction of office"], "ground_truth": ["Tom Corbett"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-869", "prediction": ["# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.location.containedby -> Eurasia\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> location.location.geolocation -> m.0cmwfl8\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.location.containedby -> Arab world\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.location.containedby -> Asia\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.statistical_region.internet_users_percent_population -> g.11b60vk8mh\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.statistical_region.gdp_real -> g.11b61jk247\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.statistical_region.internet_users_percent_population -> g.1245_2gxy\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.statistical_region.internet_users_percent_population -> g.1245_rxhk\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.statistical_region.gdp_real -> g.12tb6fdpr\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> base.folklore.mythical_creature_location.mythical_creature_s -> Phoenix -> education.school_mascot.school -> Aquinas University\n# Answer:\nPhoenix"], "ground_truth": ["Lebanon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_for -> g.125h4fcxl\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> common.topic.notable_for -> g.125f2tsfn\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> g.11b66b70n7\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_types -> Postal Code\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98103 -> common.topic.notable_types -> Postal Code\n# Answer:\n98103", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> location.location.geometry -> m.055f4wk\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Arbor Heights -> location.location.containedby -> King County\n# Answer:\nArbor Heights", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> g.11btt54h7d\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.location.contains -> Washington Athletic Club\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> location.location.geolocation -> m.03dyr0d\n# Answer:\n98102"], "ground_truth": ["98133", "98104", "98136", "98124", "98108", "98103", "98107", "98119", "98115", "98155", "98199", "98154", "98119-4114", "98138", "98168", "98175", "98177", "98166", "98112", "98160", "98105", "98178", "98131", "98121", "98174", "98116", "98129", "98194", "98139", "98170", "98171", "98188", "98181", "98114", "98117", "98161", "98184", "98101", "98144", "98145", "98198", "98109", "98102", "98122", "98126", "98164", "98141", "98106", "98191", "98190", "98185", "98134", "98111", "98118", "98132", "98158", "98195", "98148", "98127", "98125", "98165", "98113", "98146"], "ans_acc": 0.047619047619047616, "ans_hit": 1, "ans_f1": 0.08917197452229297, "ans_precission": 0.7, "ans_recall": 0.047619047619047616, "path_f1": 0.08917197452229297, "path_precision": 0.7, "path_recall": 0.047619047619047616, "path_ans_f1": 0.08917197452229297, "path_ans_precision": 0.7, "path_ans_recall": 0.047619047619047616}
{"id": "WebQTest-870", "prediction": ["# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> TD Garden -> sports.sports_facility.teams -> Boston Bruins\n# Answer:\nTD Garden", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> TD Garden -> location.location.containedby -> Boston\n# Answer:\nTD Garden", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> TD Garden -> common.topic.notable_types -> Sports Facility\n# Answer:\nTD Garden", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> XL Center -> location.location.events -> WrestleMania XI\n# Answer:\nXL Center", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> XL Center -> location.location.geolocation -> m.0v1p8cd\n# Answer:\nXL Center", "# Reasoning Path:\nBoston Celtics -> sports.professional_sports_team.draft_picks -> m.0110p_t3 -> sports.sports_league_draft_pick.draft -> 2014 NBA draft\n# Answer:\n2014 NBA draft", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> XL Center -> sports.sports_facility.teams -> Hartford Wolf Pack\n# Answer:\nXL Center", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.roster -> m.0110pz2r -> sports.sports_team_roster.player -> Marcus Smart\n# Answer:\nMarcus Smart", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> XL Center -> location.location.events -> Charlotte-UIC  1998 NCAA Men's Division I Basketball Tournament Game\n# Answer:\nXL Center", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> XL Center -> location.location.events -> Michigan St-Eastern Michigan  1998 NCAA Men's Division I Basketball Tournament Game\n# Answer:\nXL Center"], "ground_truth": ["XL Center", "TD Garden"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-872", "prediction": ["# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> Osama: The Making of a Terrorist -> book.written_work.subjects -> Al-Qaeda\n# Answer:\nOsama: The Making of a Terrorist", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Ali Azmat\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.events -> Death of Osama bin Laden\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Amrit Singh Thapar\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.contains -> Abbottabad Hockey Stadium\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> Osama: The Making of a Terrorist -> common.topic.notable_types -> Book\n# Answer:\nOsama: The Making of a Terrorist", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Faiza Khan\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> Osama: The Making of a Terrorist -> book.book.editions -> Osama : the making of a terrorist\n# Answer:\nOsama: The Making of a Terrorist", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.contains -> Abbottabad International Medical College\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.contains -> Abbottabad Public School\n# Answer:\nAbbottabad"], "ground_truth": ["Abbottabad"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-873", "prediction": ["# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Centre for the Arts -> architecture.structure.architect -> Eberhard Zeidler\n# Answer:\nToronto Centre for the Arts", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Centre for the Arts -> travel.tourist_attraction.near_travel_destination -> Newmarket\n# Answer:\nToronto Centre for the Arts", "# Reasoning Path:\nToronto -> base.infrastructure.infrastructural_municipality.sewage_treatment_plant -> Ashbridges Bay Wastewater Treatment Plant -> common.topic.article -> m.02vnl80\n# Answer:\nAshbridges Bay Wastewater Treatment Plant", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Centre for the Arts -> common.topic.article -> m.05398q\n# Answer:\nToronto Centre for the Arts", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Art Gallery of Ontario -> common.topic.notable_for -> g.1254xtqpk\n# Answer:\nArt Gallery of Ontario", "# Reasoning Path:\nToronto -> base.infrastructure.infrastructural_municipality.sewage_treatment_plant -> Ashbridges Bay Wastewater Treatment Plant -> common.topic.notable_for -> g.1255zq9nb\n# Answer:\nAshbridges Bay Wastewater Treatment Plant", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Air Canada Centre -> location.location.events -> 1st Canadian Women's Hockey League All-Star Game\n# Answer:\nAir Canada Centre", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Air Canada Centre -> common.topic.notable_for -> g.125fsz2rg\n# Answer:\nAir Canada Centre", "# Reasoning Path:\nToronto -> common.topic.webpage -> m.02nc8t6 -> common.webpage.resource -> City of Toronto Web site\n# Answer:\nCity of Toronto Web site", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Air Canada Centre -> location.location.events -> 2016 NBA All-Star Game\n# Answer:\nAir Canada Centre"], "ground_truth": ["Ashbridges Bay Wastewater Treatment Plant"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-874", "prediction": ["# Reasoning Path:\nBosnia and Herzegovina -> location.country.languages_spoken -> Serbo-Croatian Language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nSerbo-Croatian Language", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.languages_spoken -> Serbo-Croatian Language -> common.topic.image -> Serbo croatian language2005\n# Answer:\nSerbo-Croatian Language", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.official_language -> Croatian language -> language.human_language.language_family -> South Slavic languages\n# Answer:\nCroatian language", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.official_language -> Croatian language -> language.human_language.countries_spoken_in -> Croatia\n# Answer:\nCroatian language", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.languages_spoken -> Serbo-Croatian Language -> language.human_language.language_family -> Slavic languages\n# Answer:\nSerbo-Croatian Language", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.languages_spoken -> Serbian language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nSerbian language", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.languages_spoken -> Serbian language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nSerbian language", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.official_language -> Croatian language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nCroatian language", "# Reasoning Path:\nBosnia and Herzegovina -> location.statistical_region.health_expenditure_as_percent_of_gdp -> g.12cp_j3km\n# Answer:\nlocation.statistical_region.health_expenditure_as_percent_of_gdp", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.languages_spoken -> Serbo-Croatian Language -> language.human_language.language_family -> South Slavic languages\n# Answer:\nSerbo-Croatian Language"], "ground_truth": ["Serbian language", "Croatian language", "Bosnian language", "Serbo-Croatian Language"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.5833333333333334, "path_precision": 0.7, "path_recall": 0.5, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-875", "prediction": ["# Reasoning Path:\nHugo Ch\u00e1vez -> people.person.education -> m.0467s1m -> education.education.institution -> Sim\u00f3n Bol\u00edvar University\n# Answer:\nSim\u00f3n Bol\u00edvar University", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.person.education -> m.0467r_k -> education.education.institution -> Julian Pino School\n# Answer:\nJulian Pino School", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.person.education -> m.0467s06 -> education.education.institution -> Daniel Florencio O'Leary School\n# Answer:\nDaniel Florencio O'Leary School", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.deceased_person.place_of_death -> Caracas -> location.location.contains -> Sim\u00f3n Bol\u00edvar University\n# Answer:\nCaracas", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.deceased_person.place_of_death -> Caracas -> location.location.contains -> Anauco Hilton\n# Answer:\nCaracas", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.person.sibling_s -> m.0467rzz -> people.sibling_relationship.sibling -> Ad\u00e1n Ch\u00e1vez\n# Answer:\nAd\u00e1n Ch\u00e1vez", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.deceased_person.place_of_death -> Caracas -> location.location.contains -> Andr\u00e9s Bello Catholic University\n# Answer:\nCaracas", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.deceased_person.place_of_death -> Caracas -> location.location.events -> Volleyball at the 1983 Pan American Games\n# Answer:\nCaracas", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.deceased_person.place_of_death -> Caracas -> periodicals.newspaper_circulation_area.newspapers -> Diario VEA\n# Answer:\nCaracas", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.deceased_person.place_of_death -> Caracas -> location.location.events -> 1969 Men's South American Volleyball Championship\n# Answer:\nCaracas"], "ground_truth": ["Sim\u00f3n Bol\u00edvar University", "Bolivarian Military University of Venezuela", "Daniel Florencio O'Leary School", "Julian Pino School"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.4285714285714285, "ans_precission": 0.3, "ans_recall": 0.75, "path_f1": 0.5333333333333333, "path_precision": 0.4, "path_recall": 0.8, "path_ans_f1": 0.5217391304347827, "path_ans_precision": 0.4, "path_ans_recall": 0.75}
{"id": "WebQTest-876", "prediction": ["# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n1952 Summer Olympics", "# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> Australia\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.participating_countries -> Australia\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.sports -> Handball\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> Canada\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.participating_countries -> Canada\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> Canada\n# Answer:\n1952 Summer Olympics"], "ground_truth": ["Kiribati", "Canada", "United States of America", "Sudan", "Tanzania", "New Zealand"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.17910447761194026, "path_precision": 0.6, "path_recall": 0.10526315789473684, "path_ans_f1": 0.42857142857142855, "path_ans_precision": 0.6, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-877", "prediction": ["# Reasoning Path:\nSonia Gandhi -> people.person.children -> Priyanka Gandhi -> people.person.children -> Raihan Vadra\n# Answer:\nPriyanka Gandhi", "# Reasoning Path:\nSonia Gandhi -> people.person.children -> Priyanka Gandhi -> people.person.children -> Miraya Vadra\n# Answer:\nPriyanka Gandhi", "# Reasoning Path:\nSonia Gandhi -> people.person.children -> Priyanka Gandhi -> freebase.valuenotation.has_value -> Height\n# Answer:\nPriyanka Gandhi", "# Reasoning Path:\nSonia Gandhi -> people.person.children -> Rahul Gandhi -> people.person.parents -> Rajiv Gandhi\n# Answer:\nRahul Gandhi", "# Reasoning Path:\nSonia Gandhi -> people.person.children -> Priyanka Gandhi -> common.topic.article -> m.02n20d\n# Answer:\nPriyanka Gandhi", "# Reasoning Path:\nSonia Gandhi -> people.person.children -> Rahul Gandhi -> people.person.religion -> Hinduism\n# Answer:\nRahul Gandhi", "# Reasoning Path:\nSonia Gandhi -> common.topic.notable_types -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdn_\n# Answer:\nPolitician", "# Reasoning Path:\nSonia Gandhi -> people.person.children -> Rahul Gandhi -> people.person.places_lived -> m.03pvw0h\n# Answer:\nRahul Gandhi", "# Reasoning Path:\nSonia Gandhi -> common.topic.notable_types -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nSonia Gandhi -> people.person.education -> m.0nby0w9 -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree"], "ground_truth": ["Priyanka Gandhi"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-878", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Hinduism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.deities -> Yahweh\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.article -> m.055v4j\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Jainism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.image -> Ramapir01\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> common.topic.article -> m.0jfq\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Sikhism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> God -> common.topic.notable_for -> g.125d2mw4b\n# Answer:\nGod", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.image -> Ramapir22\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> religion.deity.deity_of -> Sunni Islam\n# Answer:\nAllah"], "ground_truth": ["Allah", "God", "Ramdev Pir"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-879", "prediction": ["# Reasoning Path:\nMary Todd Lincoln -> people.person.place_of_birth -> Lexington -> film.film_location.featured_in_films -> Secretariat\n# Answer:\nLexington", "# Reasoning Path:\nMary Todd Lincoln -> people.person.place_of_birth -> Lexington -> location.location.containedby -> United States of America\n# Answer:\nLexington", "# Reasoning Path:\nMary Todd Lincoln -> government.politician.government_positions_held -> m.0l0qwng -> government.government_position_held.office_position_or_title -> First Lady of the United States\n# Answer:\nFirst Lady of the United States", "# Reasoning Path:\nMary Todd Lincoln -> people.person.place_of_birth -> Lexington -> location.location.contains -> 40536\n# Answer:\nLexington", "# Reasoning Path:\nMary Todd Lincoln -> people.person.place_of_birth -> Lexington -> location.location.containedby -> Area code 859\n# Answer:\nLexington", "# Reasoning Path:\nMary Todd Lincoln -> government.politician.government_positions_held -> m.0l0qwng -> government.government_position_held.basic_title -> First Lady\n# Answer:\nFirst Lady", "# Reasoning Path:\nMary Todd Lincoln -> people.person.place_of_birth -> Lexington -> location.location.containedby -> Fayette County\n# Answer:\nLexington", "# Reasoning Path:\nMary Todd Lincoln -> people.person.place_of_birth -> Lexington -> location.location.contains -> 40536-0596\n# Answer:\nLexington", "# Reasoning Path:\nMary Todd Lincoln -> people.person.place_of_birth -> Lexington -> location.location.contains -> African Cemetery No. 2\n# Answer:\nLexington"], "ground_truth": ["Lexington"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Robsol Pinkett, Jr.\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.children -> Jaden Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Adrienne Banfield-Jones\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> common.topic.image -> Pinkett-Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.parents -> Willard Christopher Smith, Sr.\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.parents -> Caroline Bright\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Pop music -> common.topic.subject_of -> Mamiboys\n# Answer:\nPop music", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Pop music -> base.schemastaging.music_genre_concept.artists -> Yves Bole\n# Answer:\nPop music", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> tv.tv_actor.guest_roles -> m.09nbq82\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Pop music -> common.topic.subject_of -> Felt Tip\n# Answer:\nPop music"], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-880", "prediction": ["# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_death -> Down House -> location.location.containedby -> United Kingdom\n# Answer:\nDown House", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_death -> Down House -> common.topic.notable_for -> g.12578zv1w\n# Answer:\nDown House", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_death -> Down House -> common.topic.article -> m.02_67b\n# Answer:\nDown House", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_burial -> Westminster Abbey -> travel.tourist_attraction.near_travel_destination -> City of Westminster\n# Answer:\nWestminster Abbey", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_burial -> Westminster Abbey -> location.location.containedby -> City of Westminster\n# Answer:\nWestminster Abbey", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_burial -> Westminster Abbey -> religion.place_of_worship.religion -> Anglicanism\n# Answer:\nWestminster Abbey", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_burial -> Westminster Abbey -> travel.tourist_attraction.near_travel_destination -> London\n# Answer:\nWestminster Abbey", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Natural history -> common.topic.notable_types -> Literature Subject\n# Answer:\nNatural history", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_burial -> Westminster Abbey -> location.location.containedby -> London\n# Answer:\nWestminster Abbey", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology"], "ground_truth": ["Down House"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-881", "prediction": ["# Reasoning Path:\nNick Cannon -> film.actor.film -> m.062wcnd -> film.performance.film -> Ball Don't Lie\n# Answer:\nBall Don't Lie", "# Reasoning Path:\nNick Cannon -> film.actor.film -> m.0ncf55j -> film.performance.film -> Men in Black II\n# Answer:\nMen in Black II", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0z9v3c6 -> award.award_nomination.nominated_for -> Love Don't Cost a Thing\n# Answer:\nLove Don't Cost a Thing", "# Reasoning Path:\nNick Cannon -> film.actor.film -> g.11bttggxl1\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nNick Cannon -> film.actor.film -> m.062wcnd -> film.performance.character -> Mico\n# Answer:\nMico", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0b4d586 -> award.award_nomination.nominated_for -> Bobby\n# Answer:\nBobby", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0z9v3c6 -> award.award_nomination.ceremony -> 2004 Teen Choice Awards\n# Answer:\n2004 Teen Choice Awards", "# Reasoning Path:\nNick Cannon -> film.actor.film -> m.0ncf55j -> film.performance.character -> MIB Autopsy Agent\n# Answer:\nMIB Autopsy Agent", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0z9v3c6 -> award.award_nomination.award -> Teen Choice Award for Choice Movie: Chemistry\n# Answer:\nTeen Choice Award for Choice Movie: Chemistry", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.010g2z6m -> award.award_nomination.nominated_for -> Disney Parks Christmas Day Parade\n# Answer:\nDisney Parks Christmas Day Parade"], "ground_truth": ["American Son", "The Adventures of Brer Rabbit", "Underclassman", "Drumline: A New Beat", "Goal II: Living the Dream", "Weapons", "Even Money", "Love Don't Cost a Thing", "Ball Don't Lie", "Drumline", "Day of the Dead", "Whatever It Takes", "Chiraq", "Shall We Dance?", "Roll Bounce", "The Killing Room", "Monster House", "A Very School Gyrls Holla-Day", "Men in Black II", "Garfield: The Movie", "Bobby"], "ans_acc": 0.19047619047619047, "ans_hit": 1, "ans_f1": 0.25806451612903225, "ans_precission": 0.4, "ans_recall": 0.19047619047619047, "path_f1": 0.1951219512195122, "path_precision": 0.4, "path_recall": 0.12903225806451613, "path_ans_f1": 0.25806451612903225, "path_ans_precision": 0.4, "path_ans_recall": 0.19047619047619047}
{"id": "WebQTest-882", "prediction": ["# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> travel.travel_destination.tourist_attractions -> Palace of the Legion of Honor\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> travel.travel_destination.tourist_attractions -> Golden Gate Park\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> location.location.containedby -> United States of America\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> base.schemastaging.context_name.pronunciation -> g.125_nh8xf\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> travel.travel_destination.tourist_attractions -> Alcatraz Island\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> book.book_subject.works -> 1906\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> location.location.containedby -> San Francisco Bay Area\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> common.topic.notable_types -> Disaster -> type.type.domain -> Event\n# Answer:\nDisaster", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> location.location.containedby -> Area codes 415 and 628\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> book.book_subject.works -> 1st to Die\n# Answer:\nSan Francisco"], "ground_truth": ["1906-04-18"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-883", "prediction": ["# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.family_member.family -> House of Windsor\n# Answer:\nElizabeth II", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> people.person.children -> Edward VIII of the United Kingdom\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> Mary of Teck -> people.person.children -> Prince John of the United Kingdom\n# Answer:\nMary of Teck", "# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.person.children -> Prince Edward, Earl of Wessex\n# Answer:\nElizabeth II", "# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.person.spouse_s -> m.028zms8\n# Answer:\nElizabeth II", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> people.person.parents -> Alexandra of Denmark\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.person.children -> Anne, Princess Royal\n# Answer:\nElizabeth II", "# Reasoning Path:\nGeorge VI -> people.person.children -> Princess Margaret, Countess of Snowdon -> people.person.children -> David Armstrong-Jones, Viscount Linley\n# Answer:\nPrincess Margaret, Countess of Snowdon", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> people.person.children -> Mary, Princess Royal and Countess of Harewood\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.person.children -> Charles, Prince of Wales\n# Answer:\nElizabeth II"], "ground_truth": ["Queen Elizabeth The Queen Mother"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-884", "prediction": ["# Reasoning Path:\nJesus Christ -> people.person.place_of_birth -> Judea -> location.location.containedby -> Middle East\n# Answer:\nJudea", "# Reasoning Path:\nJesus Christ -> people.person.place_of_birth -> Judea -> common.topic.article -> m.065shd\n# Answer:\nJudea", "# Reasoning Path:\nJesus Christ -> influence.influence_node.influenced -> Paul the Apostle -> people.person.place_of_birth -> Tarsus\n# Answer:\nPaul the Apostle", "# Reasoning Path:\nJesus Christ -> influence.influence_node.influenced -> Augustine of Hippo -> people.person.place_of_birth -> Roman Empire\n# Answer:\nAugustine of Hippo", "# Reasoning Path:\nJesus Christ -> common.topic.notable_for -> g.12551f1xr\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nJesus Christ -> influence.influence_node.influenced -> Augustine of Hippo -> influence.influence_node.influenced_by -> Aristotle\n# Answer:\nAugustine of Hippo", "# Reasoning Path:\nJesus Christ -> common.topic.notable_for -> g.1255sm1pr\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nJesus Christ -> influence.influence_node.influenced -> Paul the Apostle -> influence.influence_node.influenced -> Thomas Aquinas\n# Answer:\nPaul the Apostle", "# Reasoning Path:\nJesus Christ -> common.topic.notable_for -> g.12578_nd4\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nJesus Christ -> influence.influence_node.influenced -> Paul the Apostle -> people.person.gender -> Male\n# Answer:\nPaul the Apostle"], "ground_truth": ["Judea"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-885", "prediction": ["# Reasoning Path:\nJames Dyson -> people.person.profession -> Designer -> people.profession.specialization_of -> Artist\n# Answer:\nDesigner", "# Reasoning Path:\nJames Dyson -> people.person.profession -> Designer -> base.descriptive_names.names.descriptive_name -> m.01053pmf\n# Answer:\nDesigner", "# Reasoning Path:\nJames Dyson -> people.person.profession -> Industrial designer -> base.descriptive_names.names.descriptive_name -> m.0105wg7s\n# Answer:\nIndustrial designer", "# Reasoning Path:\nJames Dyson -> people.person.profession -> Industrial designer -> base.lightweight.profession.professions_similar -> Photographer\n# Answer:\nIndustrial designer", "# Reasoning Path:\nJames Dyson -> people.person.nationality -> United Kingdom -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nJames Dyson -> people.person.profession -> Industrial designer -> film.film_subject.films -> Objectified\n# Answer:\nIndustrial designer", "# Reasoning Path:\nJames Dyson -> people.person.profession -> Designer -> base.descriptive_names.names.descriptive_name -> m.01053pnt\n# Answer:\nDesigner", "# Reasoning Path:\nJames Dyson -> people.person.profession -> Industrial designer -> base.descriptive_names.names.descriptive_name -> m.0105wgvd\n# Answer:\nIndustrial designer", "# Reasoning Path:\nJames Dyson -> people.person.nationality -> United Kingdom -> base.aareas.schema.administrative_area.subdivides_place -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nJames Dyson -> people.person.profession -> Designer -> people.profession.specializations -> Industrial designer\n# Answer:\nDesigner"], "ground_truth": ["Engineer", "Inventor", "Industrial designer", "Designer"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6153846153846154, "ans_precission": 0.8, "ans_recall": 0.5, "path_f1": 0.6153846153846154, "path_precision": 0.8, "path_recall": 0.5, "path_ans_f1": 0.6153846153846154, "path_ans_precision": 0.8, "path_ans_recall": 0.5}
{"id": "WebQTest-886", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.film -> Star Wars\n# Answer:\nStar Wars", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Uncredited\n# Answer:\nUncredited", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.film -> Return of the Jedi\n# Answer:\nReturn of the Jedi", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nThe Making of Star Wars"], "ground_truth": ["James Earl Jones"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-887", "prediction": ["# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0n1mdbx -> education.education.institution -> George Washington High School\n# Answer:\nGeorge Washington High School", "# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0vv8k0g -> education.education.institution -> California Labor School\n# Answer:\nCalifornia Labor School", "# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0n1mdbx -> freebase.valuenotation.has_no_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0n1mdbx -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nMinor", "# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0vv8k0g -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0n1mdbx -> freebase.valuenotation.has_no_value -> Specialization\n# Answer:\nSpecialization", "# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0vv8k0g -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nMaya Angelou -> music.artist.track -> The Dreamer -> music.recording.artist -> Common\n# Answer:\nThe Dreamer", "# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0vv8k0g -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nMaya Angelou -> music.artist.track -> Donkey City -> common.topic.notable_types -> Musical Recording\n# Answer:\nDonkey City"], "ground_truth": ["George Washington High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-888", "prediction": ["# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Australia\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Austria\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.sports -> Ice Hockey\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n1896 Summer Olympics", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d0tc3g\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nSwitzerland -> location.location.partially_contains -> Alps -> location.location.partially_containedby -> Austria\n# Answer:\nAlps", "# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> freebase.valuenotation.is_reviewed -> Location(s)\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics -> olympics.olympic_games.sports -> Tennis\n# Answer:\n1896 Summer Olympics"], "ground_truth": ["Austria", "Liechtenstein", "Germany", "Italy", "France"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.0187793427230047, "path_precision": 0.2, "path_recall": 0.009852216748768473, "path_ans_f1": 0.20000000000000004, "path_ans_precision": 0.2, "path_ans_recall": 0.2}
{"id": "WebQTest-889", "prediction": ["# Reasoning Path:\nBenito Mussolini -> government.politician.government_positions_held -> m.0c9_j0x -> government.government_position_held.jurisdiction_of_office -> Italy\n# Answer:\nItaly", "# Reasoning Path:\nBenito Mussolini -> government.politician.government_positions_held -> m.0c9_j0x -> government.government_position_held.office_position_or_title -> Duce\n# Answer:\nDuce", "# Reasoning Path:\nBenito Mussolini -> people.person.nationality -> Italy -> government.governmental_jurisdiction.governing_officials -> m.0c9_j0x\n# Answer:\nItaly", "# Reasoning Path:\nBenito Mussolini -> people.person.nationality -> Italy -> base.locations.countries.continent -> Europe\n# Answer:\nItaly", "# Reasoning Path:\nBenito Mussolini -> organization.organization_founder.organizations_founded -> National Fascist Party -> government.political_party.politicians_in_this_party -> m.012qyfs6\n# Answer:\nNational Fascist Party", "# Reasoning Path:\nBenito Mussolini -> organization.organization_founder.organizations_founded -> National Fascist Party -> organization.organization.geographic_scope -> Italy\n# Answer:\nNational Fascist Party", "# Reasoning Path:\nBenito Mussolini -> organization.organization_founder.organizations_founded -> Cinecitt\u00e0 -> base.schemastaging.organization_extra.phone_number -> m.010h48jv\n# Answer:\nCinecitt\u00e0", "# Reasoning Path:\nBenito Mussolini -> people.person.nationality -> Italy -> location.country.languages_spoken -> Italian Language\n# Answer:\nItaly", "# Reasoning Path:\nBenito Mussolini -> organization.organization_founder.organizations_founded -> National Fascist Party -> organization.organization.sectors -> Anti-communism\n# Answer:\nNational Fascist Party", "# Reasoning Path:\nBenito Mussolini -> organization.organization_founder.organizations_founded -> Grand Council of Fascism -> common.topic.notable_types -> Organization\n# Answer:\nGrand Council of Fascism"], "ground_truth": ["Italy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_types -> Language Writing System\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> language.language_writing_system.languages -> Uyghur Language\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_for -> g.1258512fl\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> N\u00fcshu script -> common.topic.article -> m.014lbq\n# Answer:\nN\u00fcshu script", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> book.book_subject.works -> Flags of Our Fathers\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> location.location.contains -> China\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> language.language_writing_system.languages -> Vietnamese Language\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> common.topic.notable_for -> g.1259bftrt\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> base.schemastaging.context_name.pronunciation -> g.125_l82wv\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> education.field_of_study.students_majoring -> m.0104b7h1 -> education.education.degree -> PhD\n# Answer:\nPhD"], "ground_truth": ["'Phags-pa script", "N\u00fcshu script", "Simplified Chinese character", "Chinese characters", "Traditional Chinese characters"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.6, "ans_recall": 0.6, "path_f1": 0.6, "path_precision": 0.6, "path_recall": 0.6, "path_ans_f1": 0.6, "path_ans_precision": 0.6, "path_ans_recall": 0.6}
{"id": "WebQTest-891", "prediction": ["# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Mao Zedong -> people.person.children -> Li Min\n# Answer:\nMao Zedong", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Mao Zedong -> people.person.children -> Li Na\n# Answer:\nMao Zedong", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Mao Zedong -> people.person.quotations -> A revolution is not a dinner party, or writing an essay, or painting a picture, or doing embroidery; it cannot be so refined, so leisurely and gentle, so temperate, kind, courteous, restrained and magnanimous. A revolution is an insurrection, an act of violence by which one class overthrows another.\n# Answer:\nMao Zedong", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Mao Zedong -> people.person.children -> Mao Anhong\n# Answer:\nMao Zedong", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Mao Zedong -> influence.influence_node.influenced_by -> Thomas Robert Malthus\n# Answer:\nMao Zedong", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Zhou Enlai -> government.politician.party -> m.0lr13p4\n# Answer:\nZhou Enlai", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Mao Zedong -> people.person.quotations -> All reactionaries are paper tigers.\n# Answer:\nMao Zedong", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Zhou Enlai -> people.person.children -> Li Peng\n# Answer:\nZhou Enlai", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Zhou Enlai -> people.person.religion -> Atheism\n# Answer:\nZhou Enlai", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Mao Zedong -> people.person.quotations -> All the reputedly powerful reactionaries are merely paper tigers. The reason is that they are divorced from the people. Look! Was not Hitler a paper tiger? Was Hitler not overthrown? U.S. imperialism has not yet been overthrown and it has the atomic bomb. I believe it also will be overthrown. It, too, is a paper tiger.\n# Answer:\nMao Zedong"], "ground_truth": ["Mao Zedong", "Chen Duxiu", "Li Dazhao", "Zhou Enlai"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-892", "prediction": ["# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Pneumonia -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nPneumonia", "# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.includes_causes_of_death -> Suffocated\n# Answer:\nRespiratory failure", "# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nRespiratory failure", "# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Respiratory failure -> base.schemastaging.context_name.pronunciation -> g.125_ph2ct\n# Answer:\nRespiratory failure", "# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Stroke -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nStroke", "# Reasoning Path:\nMiles Davis -> music.composer.compositions -> A. Flute Song, B. Hotel Me -> music.composition.composer -> Gil Evans\n# Answer:\nA. Flute Song, B. Hotel Me", "# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.includes_causes_of_death -> Choking\n# Answer:\nRespiratory failure", "# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.includes_causes_of_death -> Erotic asphyxiation\n# Answer:\nRespiratory failure", "# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Stroke -> medicine.disease.risk_factors -> Male\n# Answer:\nStroke", "# Reasoning Path:\nMiles Davis -> music.featured_artist.recordings -> 'Round Midnight -> common.topic.notable_for -> g.1yg9jkzbt\n# Answer:\n'Round Midnight"], "ground_truth": ["Respiratory failure", "Stroke", "Pneumonia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-893", "prediction": ["# Reasoning Path:\nAzle -> location.citytown.postal_codes -> 76020 -> common.topic.notable_for -> g.1258t5sn5\n# Answer:\n76020", "# Reasoning Path:\nAzle -> location.citytown.postal_codes -> 76020 -> location.postal_code.country -> United States of America\n# Answer:\n76020", "# Reasoning Path:\nAzle -> location.citytown.postal_codes -> 76098 -> common.topic.notable_for -> g.125ccr2vm\n# Answer:\n76098", "# Reasoning Path:\nAzle -> location.citytown.postal_codes -> 76020 -> common.topic.notable_types -> Postal Code\n# Answer:\n76020", "# Reasoning Path:\nAzle -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Texas\n# Answer:\nUnited States of America", "# Reasoning Path:\nAzle -> location.statistical_region.population -> g.11b66gyw0f\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nAzle -> location.citytown.postal_codes -> 76098 -> common.topic.notable_types -> Postal Code\n# Answer:\n76098", "# Reasoning Path:\nAzle -> location.citytown.postal_codes -> 76098 -> location.postal_code.country -> United States of America\n# Answer:\n76098", "# Reasoning Path:\nAzle -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Texas\n# Answer:\nUnited States of America", "# Reasoning Path:\nAzle -> location.statistical_region.population -> g.11bymmps1m\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["76098", "76020"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-894", "prediction": ["# Reasoning Path:\nSally Pearson -> people.person.nationality -> Australia -> olympics.olympic_participating_country.olympics_participated_in -> The London 2012 Summer Olympics\n# Answer:\nAustralia", "# Reasoning Path:\nSally Pearson -> olympics.olympic_athlete.medals_won -> m.04hdw0b -> olympics.olympic_medal_honor.event -> Athletics at the 2008 Summer Olympics - Women's 100 metres hurdles\n# Answer:\nAthletics at the 2008 Summer Olympics - Women's 100 metres hurdles", "# Reasoning Path:\nSally Pearson -> people.person.nationality -> Australia -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\nAustralia", "# Reasoning Path:\nSally Pearson -> olympics.olympic_athlete.medals_won -> m.04hdw0b -> olympics.olympic_medal_honor.olympics -> 2008 Summer Olympics\n# Answer:\n2008 Summer Olympics", "# Reasoning Path:\nSally Pearson -> people.person.nationality -> Australia -> location.country.administrative_divisions -> New South Wales\n# Answer:\nAustralia", "# Reasoning Path:\nSally Pearson -> people.person.nationality -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Bunyip\n# Answer:\nAustralia", "# Reasoning Path:\nSally Pearson -> olympics.olympic_athlete.medals_won -> m.0kl1f80 -> olympics.olympic_medal_honor.olympics -> The London 2012 Summer Olympics\n# Answer:\nThe London 2012 Summer Olympics", "# Reasoning Path:\nSally Pearson -> people.person.nationality -> Australia -> location.country.administrative_divisions -> Australian Capital Territory\n# Answer:\nAustralia", "# Reasoning Path:\nSally Pearson -> common.topic.webpage -> m.0kdqgcl -> common.webpage.category -> About\n# Answer:\nAbout", "# Reasoning Path:\nSally Pearson -> people.person.nationality -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Yowie\n# Answer:\nAustralia"], "ground_truth": ["The London 2012 Summer Olympics"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-895", "prediction": ["# Reasoning Path:\nDetroit Pistons -> sports.sports_team.championships -> 2004 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n2004 NBA Finals", "# Reasoning Path:\nDetroit Pistons -> sports.sports_team.championships -> 2004 NBA Finals -> common.topic.notable_for -> g.1254zz_2p\n# Answer:\n2004 NBA Finals", "# Reasoning Path:\nDetroit Pistons -> sports.sports_team.championships -> 1989 NBA Finals -> common.topic.article -> m.08x9ts\n# Answer:\n1989 NBA Finals", "# Reasoning Path:\nDetroit Pistons -> award.award_nominee.award_nominations -> m.0z6d2v_ -> award.award_nomination.ceremony -> 2004 ESPY Awards\n# Answer:\n2004 ESPY Awards", "# Reasoning Path:\nDetroit Pistons -> sports.sports_team.championships -> 1989 NBA Finals -> common.topic.notable_for -> g.12550n6gn\n# Answer:\n1989 NBA Finals", "# Reasoning Path:\nDetroit Pistons -> sports.sports_team.roster -> m.010fy68k -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nTeam", "# Reasoning Path:\nDetroit Pistons -> sports.sports_team.championships -> 1990 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n1990 NBA Finals", "# Reasoning Path:\nDetroit Pistons -> sports.sports_team.championships -> 1989 NBA Finals -> sports.sports_championship_event.runner_up -> Los Angeles Lakers\n# Answer:\n1989 NBA Finals", "# Reasoning Path:\nDetroit Pistons -> sports.sports_team.championships -> 1990 NBA Finals -> sports.sports_championship_event.season -> 1989\u201390 NBA season\n# Answer:\n1990 NBA Finals", "# Reasoning Path:\nDetroit Pistons -> award.award_nominee.award_nominations -> m.0z6d2v_ -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work"], "ground_truth": ["2004 NBA Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-896", "prediction": ["# Reasoning Path:\nPhiladelphia Flyers -> sports.sports_team.championships -> 1974 Stanley Cup Finals -> sports.sports_championship_event.runner_up -> Boston Bruins\n# Answer:\n1974 Stanley Cup Finals", "# Reasoning Path:\nPhiladelphia Flyers -> common.topic.article -> m.0hn2_\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nPhiladelphia Flyers -> sports.sports_team.championships -> 1974 Stanley Cup Finals -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1974 Stanley Cup Finals", "# Reasoning Path:\nPhiladelphia Flyers -> sports.sports_team.championships -> 1975 Stanley Cup Finals -> common.topic.article -> m.03by14z\n# Answer:\n1975 Stanley Cup Finals", "# Reasoning Path:\nPhiladelphia Flyers -> common.topic.notable_for -> g.1256f_q8t\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nPhiladelphia Flyers -> sports.sports_team.championships -> 1974 Stanley Cup Finals -> sports.sports_championship_event.season -> 1973\u201374 NHL season\n# Answer:\n1974 Stanley Cup Finals", "# Reasoning Path:\nPhiladelphia Flyers -> sports.sports_team.championships -> 1975 Stanley Cup Finals -> common.topic.notable_for -> g.125dc4pr3\n# Answer:\n1975 Stanley Cup Finals"], "ground_truth": ["1975 Stanley Cup Finals", "1974 Stanley Cup Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8333333333333333, "ans_precission": 0.7142857142857143, "ans_recall": 1.0, "path_f1": 0.8333333333333333, "path_precision": 0.7142857142857143, "path_recall": 1.0, "path_ans_f1": 0.8333333333333333, "path_ans_precision": 0.7142857142857143, "path_ans_recall": 1.0}
{"id": "WebQTest-898", "prediction": ["# Reasoning Path:\nSt. Pauls -> location.location.containedby -> Robeson County -> location.location.containedby -> Eastern North Carolina\n# Answer:\nRobeson County", "# Reasoning Path:\nSt. Pauls -> location.location.containedby -> Robeson County -> location.location.containedby -> North Carolina\n# Answer:\nRobeson County", "# Reasoning Path:\nSt. Pauls -> location.location.containedby -> North Carolina -> location.location.containedby -> United States of America\n# Answer:\nNorth Carolina", "# Reasoning Path:\nSt. Pauls -> location.hud_county_place.county -> Robeson County -> location.location.containedby -> Eastern North Carolina\n# Answer:\nRobeson County", "# Reasoning Path:\nSt. Pauls -> location.hud_county_place.county -> Robeson County -> location.location.containedby -> North Carolina\n# Answer:\nRobeson County", "# Reasoning Path:\nSt. Pauls -> base.wikipedia_infobox.settlement.area_code -> Area code 910 -> location.location.containedby -> North Carolina\n# Answer:\nArea code 910", "# Reasoning Path:\nSt. Pauls -> location.location.containedby -> Robeson County -> location.statistical_region.population -> g.11b66dwnq2\n# Answer:\nRobeson County", "# Reasoning Path:\nSt. Pauls -> location.location.containedby -> North Carolina -> base.locations.states_and_provences.country -> United States of America\n# Answer:\nNorth Carolina", "# Reasoning Path:\nSt. Pauls -> location.location.containedby -> North Carolina -> base.aareas.schema.administrative_area.capital -> Raleigh\n# Answer:\nNorth Carolina", "# Reasoning Path:\nSt. Pauls -> location.location.containedby -> Robeson County -> location.statistical_region.population -> g.11x1cfnvv\n# Answer:\nRobeson County"], "ground_truth": ["Robeson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-899", "prediction": ["# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Jefferson County -> location.location.containedby -> Washington\n# Answer:\nJefferson County", "# Reasoning Path:\nOlympic National Park -> common.topic.notable_for -> g.1257hvh8r\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Washington -> location.location.containedby -> West Coast of the United States\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> base.usnationalparks.us_national_park.state -> Washington -> location.location.containedby -> West Coast of the United States\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Jefferson County -> location.statistical_region.population -> g.11b66j25ww\n# Answer:\nJefferson County", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Washington -> location.location.containedby -> Contiguous United States\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Jefferson County -> location.location.adjoin_s -> m.03jq636\n# Answer:\nJefferson County", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Washington -> location.location.containedby -> Northwestern United States\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Washington -> base.aareas.schema.administrative_area.administrative_children -> Adams County\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> base.usnationalparks.us_national_park.state -> Washington -> location.location.containedby -> Contiguous United States\n# Answer:\nWashington"], "ground_truth": ["Jefferson County", "Washington"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> people.person.children -> Christopher Nixon Cox\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> people.person.spouse_s -> m.0j4kt_s\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.spouse_s -> m.0j4k1q4\n# Answer:\nJulie Nixon Eisenhower", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> film.person_or_entity_appearing_in_film.films -> m.0vpghfz\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.spouse -> Pat Nixon\n# Answer:\nPat Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.location_of_ceremony -> The Mission Inn Hotel & Spa\n# Answer:\nThe Mission Inn Hotel & Spa", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.children -> Alexander Richard Eisenhower\n# Answer:\nJulie Nixon Eisenhower", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.employment_history -> m.0k0dcyp\n# Answer:\nJulie Nixon Eisenhower", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage"], "ground_truth": ["Pat Nixon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.11764705882352941, "path_precision": 0.1, "path_recall": 0.14285714285714285, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.ceremony -> 37th Primetime Emmy Awards\n# Answer:\n37th Primetime Emmy Awards", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Supporting Actress in a Comedy Series\n# Answer:\nPrimetime Emmy Award for Outstanding Supporting Actress in a Comedy Series", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Supporting Actress in a Comedy Series\n# Answer:\nPrimetime Emmy Award for Outstanding Supporting Actress in a Comedy Series", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.ceremony -> 35th Primetime Emmy Awards\n# Answer:\n35th Primetime Emmy Awards", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.award -> NAACP Image Award for Outstanding Actress in a Comedy Series\n# Answer:\nNAACP Image Award for Outstanding Actress in a Comedy Series", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0_yczwx -> award.award_honor.award_winner -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 0 -> common.topic.notable_for -> g.1yl5pbtsv\n# Answer:\nThe Jeffersons - Season 0"], "ground_truth": ["Isabel Sanford", "Roxie Roker", "Berlinda Tolbert", "Zara Cully", "Franklin Cover", "Damon Evans", "Paul Benedict", "Mike Evans", "Marla Gibbs", "Jay Hammer", "Sherman Hemsley"], "ans_acc": 0.09090909090909091, "ans_hit": 1, "ans_f1": 0.14814814814814814, "ans_precission": 0.4, "ans_recall": 0.09090909090909091, "path_f1": 0.11594202898550726, "path_precision": 0.4, "path_recall": 0.06779661016949153, "path_ans_f1": 0.14814814814814814, "path_ans_precision": 0.4, "path_ans_recall": 0.09090909090909091}
{"id": "WebQTest-900", "prediction": ["# Reasoning Path:\nNat Turner's slave rebellion -> time.event.locations -> Southampton County -> location.location.containedby -> United States of America\n# Answer:\nSouthampton County", "# Reasoning Path:\nNat Turner's slave rebellion -> time.event.locations -> Southampton County -> location.location.people_born_here -> William Y. Gholson\n# Answer:\nSouthampton County", "# Reasoning Path:\nNat Turner's slave rebellion -> symbols.namesake.named_after -> Nat Turner -> people.person.place_of_birth -> Southampton County\n# Answer:\nNat Turner", "# Reasoning Path:\nNat Turner's slave rebellion -> time.event.locations -> Southampton County -> location.location.containedby -> Virginia\n# Answer:\nSouthampton County", "# Reasoning Path:\nNat Turner's slave rebellion -> time.event.locations -> Southampton County -> location.location.people_born_here -> Anthony W. Gardiner\n# Answer:\nSouthampton County", "# Reasoning Path:\nNat Turner's slave rebellion -> time.event.locations -> Southampton County -> location.location.contains -> 23827\n# Answer:\nSouthampton County", "# Reasoning Path:\nNat Turner's slave rebellion -> common.topic.notable_types -> Event -> freebase.type_profile.published -> Published\n# Answer:\nEvent", "# Reasoning Path:\nNat Turner's slave rebellion -> symbols.namesake.named_after -> Nat Turner -> people.deceased_person.place_of_death -> Courtland\n# Answer:\nNat Turner", "# Reasoning Path:\nNat Turner's slave rebellion -> time.event.locations -> Southampton County -> location.location.people_born_here -> Dred Scott\n# Answer:\nSouthampton County", "# Reasoning Path:\nNat Turner's slave rebellion -> time.event.locations -> Southampton County -> location.location.contains -> 23828\n# Answer:\nSouthampton County"], "ground_truth": ["Southampton County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-901", "prediction": ["# Reasoning Path:\nLuke Skywalker -> film.film_character.portrayed_in_films -> m.010fs6tg -> film.performance.actor -> Mark Hamill\n# Answer:\nMark Hamill", "# Reasoning Path:\nLuke Skywalker -> film.film_character.portrayed_in_films -> m.0k3r3j -> film.performance.actor -> Mark Hamill\n# Answer:\nMark Hamill", "# Reasoning Path:\nLuke Skywalker -> film.film_character.portrayed_in_films -> m.0k6jzc -> film.performance.actor -> Mark Hamill\n# Answer:\nMark Hamill", "# Reasoning Path:\nLuke Skywalker -> tv.tv_character.appeared_in_tv_program -> m.0gy7h8w -> tv.regular_tv_appearance.actor -> Mark Hamill\n# Answer:\nMark Hamill", "# Reasoning Path:\nLuke Skywalker -> film.film_character.portrayed_in_films -> m.010fs6tg -> film.performance.film -> Star Wars: The Force Awakens\n# Answer:\nStar Wars: The Force Awakens", "# Reasoning Path:\nLuke Skywalker -> film.film_character.portrayed_in_films -> m.0k3r3j -> film.performance.film -> Return of the Jedi\n# Answer:\nReturn of the Jedi", "# Reasoning Path:\nLuke Skywalker -> tv.tv_character.appeared_in_tv_program -> m.0gy7h8w -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nLuke Skywalker -> film.film_character.portrayed_in_films -> m.0k6jzc -> film.performance.film -> Star Wars Holiday Special\n# Answer:\nStar Wars Holiday Special", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.powers_or_abilities -> Force -> fictional_universe.character_powers.characters_with_this_ability -> Darth Vader\n# Answer:\nForce", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.powers_or_abilities -> Force -> common.topic.notable_for -> g.125f342hs\n# Answer:\nForce"], "ground_truth": ["Mark Hamill"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.47058823529411764, "path_precision": 0.4, "path_recall": 0.5714285714285714, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-903", "prediction": ["# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> United States of America -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics\n# Answer:\nUnited States of America", "# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> United States of America -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics\n# Answer:\nUnited States of America", "# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> United States of America -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics\n# Answer:\nUnited States of America", "# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> United States of America -> sports.sport_country.multi_event_tournaments_participated_in -> 2012 World Mountain Running Championships\n# Answer:\nUnited States of America", "# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> Algeria -> location.country.languages_spoken -> Arabic Language\n# Answer:\nAlgeria", "# Reasoning Path:\nTurkish people -> people.ethnicity.includes_groups -> Iraqi Turkmen -> people.ethnicity.geographic_distribution -> Kirkuk\n# Answer:\nIraqi Turkmen", "# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> Austria -> location.location.containedby -> Eurasia\n# Answer:\nAustria", "# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> Algeria -> location.country.languages_spoken -> French\n# Answer:\nAlgeria", "# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> Algeria -> location.country.form_of_government -> Presidential system\n# Answer:\nAlgeria", "# Reasoning Path:\nTurkish people -> people.ethnicity.includes_groups -> Meskhetian Turks -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nMeskhetian Turks"], "ground_truth": ["Turkey", "Denmark", "Austria", "Germany", "France", "Iraq", "Kazakhstan", "Northern Cyprus", "Canada", "Azerbaijan", "Kyrgyzstan", "United Kingdom", "Republic of Kosovo", "Bulgaria", "Syria", "Sweden", "Netherlands", "United States of America", "Norway", "Algeria", "Bosnia and Herzegovina", "Belgium"], "ans_acc": 0.18181818181818182, "ans_hit": 1, "ans_f1": 0.3025210084033613, "ans_precission": 0.9, "ans_recall": 0.18181818181818182, "path_f1": 0.23300970873786406, "path_precision": 0.8, "path_recall": 0.13636363636363635, "path_ans_f1": 0.3076923076923077, "path_ans_precision": 1.0, "path_ans_recall": 0.18181818181818182}
{"id": "WebQTest-904", "prediction": ["# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> base.biblioness.bibs_location.state -> Hawaii\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> location.location.containedby -> United States of America\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Hawaii\n# Answer:\nUnited States of America", "# Reasoning Path:\nBarack Obama -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Hawaii\n# Answer:\nUnited States of America", "# Reasoning Path:\nBarack Obama -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Illinois\n# Answer:\nUnited States of America", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> Dreams from My Father -> book.written_work.subjects -> United States of America\n# Answer:\nDreams from My Father", "# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> location.location.containedby -> Hawaii\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nBarack Obama -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Illinois\n# Answer:\nUnited States of America"], "ground_truth": ["Hawaii"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-905", "prediction": ["# Reasoning Path:\nJoe Montana -> sports.sports_award_winner.awards -> m.04kcv46 -> sports.sports_award.season -> 1984 NFL season\n# Answer:\n1984 NFL season", "# Reasoning Path:\nJoe Montana -> sports.sports_award_winner.awards -> m.04kcv8k -> sports.sports_award.season -> 1981 NFL season\n# Answer:\n1981 NFL season", "# Reasoning Path:\nJoe Montana -> sports.sports_award_winner.awards -> m.04kcvcr -> sports.sports_award.season -> 1989 NFL season\n# Answer:\n1989 NFL season", "# Reasoning Path:\nJoe Montana -> sports.sports_award_winner.awards -> m.04kcv46 -> sports.sports_award.award -> Super Bowl Most Valuable Player Award\n# Answer:\nSuper Bowl Most Valuable Player Award", "# Reasoning Path:\nJoe Montana -> sports.sports_award_winner.awards -> m.04kcv8k -> sports.sports_award.award -> Super Bowl Most Valuable Player Award\n# Answer:\nSuper Bowl Most Valuable Player Award", "# Reasoning Path:\nJoe Montana -> film.actor.film -> m.0h0_0vg -> film.performance.film -> ESPN Honor Roll: The Best of College Football: Vol. 1\n# Answer:\nESPN Honor Roll: The Best of College Football: Vol. 1", "# Reasoning Path:\nJoe Montana -> sports.sports_award_winner.awards -> m.04kcvcr -> sports.sports_award.award -> Super Bowl Most Valuable Player Award\n# Answer:\nSuper Bowl Most Valuable Player Award", "# Reasoning Path:\nJoe Montana -> sports.pro_athlete.teams -> m.04nk3bx -> sports.sports_team_roster.team -> San Francisco 49ers\n# Answer:\nSan Francisco 49ers", "# Reasoning Path:\nJoe Montana -> film.actor.film -> m.0h10vn1 -> film.performance.film -> NFL: Greatest Super Bowl Moments\n# Answer:\nNFL: Greatest Super Bowl Moments", "# Reasoning Path:\nJoe Montana -> sports.pro_athlete.teams -> m.04nk3cl -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber"], "ground_truth": ["1982-01-24", "1985-01-20", "1990-01-28", "1989-01-22"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-908", "prediction": ["# Reasoning Path:\nGary Paulsen -> award.award_nominee.award_nominations -> m.09kbw63 -> award.award_nomination.award -> Dorothy Canfield Fisher Children's Book Award\n# Answer:\nDorothy Canfield Fisher Children's Book Award", "# Reasoning Path:\nGary Paulsen -> award.award_nominee.award_nominations -> m.09kby6s -> award.award_nomination.award -> Dorothy Canfield Fisher Children's Book Award\n# Answer:\nDorothy Canfield Fisher Children's Book Award", "# Reasoning Path:\nGary Paulsen -> award.award_winner.awards_won -> m.09kkr7w -> award.award_honor.award -> Anne V. Zarrow Award for Young Readers' Literature\n# Answer:\nAnne V. Zarrow Award for Young Readers' Literature", "# Reasoning Path:\nGary Paulsen -> award.award_nominee.award_nominations -> m.0zcvpyg -> award.award_nomination.award -> John Newbery Medal\n# Answer:\nJohn Newbery Medal", "# Reasoning Path:\nGary Paulsen -> award.award_nominee.award_nominations -> m.09kbw63 -> award.award_nomination.nominated_for -> How Angel Peterson Got His Name\n# Answer:\nHow Angel Peterson Got His Name", "# Reasoning Path:\nGary Paulsen -> award.award_nominee.award_nominations -> m.09kby6s -> award.award_nomination.nominated_for -> Guts: The True Stories Behind Hatchet and the Brian Books\n# Answer:\nGuts: The True Stories Behind Hatchet and the Brian Books", "# Reasoning Path:\nGary Paulsen -> award.award_nominee.award_nominations -> m.0zcvpyg -> freebase.valuenotation.is_reviewed -> Award\n# Answer:\nAward", "# Reasoning Path:\nGary Paulsen -> award.award_winner.awards_won -> m.046cq91 -> award.award_honor.award -> Regina Medal\n# Answer:\nRegina Medal", "# Reasoning Path:\nGary Paulsen -> award.award_winner.awards_won -> m.087149q -> award.award_honor.award -> Dorothy Canfield Fisher Children's Book Award\n# Answer:\nDorothy Canfield Fisher Children's Book Award", "# Reasoning Path:\nGary Paulsen -> award.award_nominee.award_nominations -> m.0zcvpyg -> freebase.valuenotation.is_reviewed -> Award Nominee\n# Answer:\nAward Nominee"], "ground_truth": ["Margaret Edwards Award", "Dorothy Canfield Fisher Children's Book Award", "Regina Medal", "John Newbery Medal", "Anne V. Zarrow Award for Young Readers' Literature", "Spur Award for Best Juvenile Fiction", "Spur Award for Best Juvenile Nonfiction"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.5853658536585366, "ans_precission": 0.6, "ans_recall": 0.5714285714285714, "path_f1": 0.5, "path_precision": 0.6, "path_recall": 0.42857142857142855, "path_ans_f1": 0.5853658536585366, "path_ans_precision": 0.6, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.periodical.first_issue_date -> m.02npn31\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.publication.contents -> m.0znkynv\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> The Golden Era -> common.topic.article -> m.04zg6d\n# Answer:\nThe Golden Era", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Street Sheet -> book.periodical.format -> m.02npbt7\n# Answer:\nStreet Sheet", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Street Sheet -> book.newspaper.owner -> Coalition on Homelessness\n# Answer:\nStreet Sheet", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> book.written_work.subjects -> California\n# Answer:\n1906", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> The Golden Era -> common.topic.notable_for -> g.1258tnwlj\n# Answer:\nThe Golden Era", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94101 -> location.postal_code.country -> United States of America\n# Answer:\n94101", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> common.topic.notable_for -> g.1258179ql\n# Answer:\n1906", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1st to Die -> book.written_work.subjects -> California\n# Answer:\n1st to Die"], "ground_truth": ["Bay Area Reporter", "Street Sheet", "AsianWeek", "Sing Tao Daily", "California Star", "San Francisco Chronicle", "Synapse", "Free Society", "San Francisco Bay Guardian", "San Francisco Bay View", "The San Francisco Examiner", "San Francisco Foghorn", "San Francisco Daily", "San Francisco News-Call Bulletin Newspaper", "The Daily Alta California", "San Francisco Business Times", "Dock of the Bay", "San Francisco Bay Times", "The Golden Era", "San Francisco Call"], "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0.24, "ans_precission": 0.6, "ans_recall": 0.15, "path_f1": 0.24, "path_precision": 0.6, "path_recall": 0.15, "path_ans_f1": 0.24, "path_ans_precision": 0.6, "path_ans_recall": 0.15}
{"id": "WebQTest-910", "prediction": ["# Reasoning Path:\nAlexander Graham Bell -> people.deceased_person.place_of_death -> Beinn Bhreagh, Nova Scotia -> location.location.containedby -> Nova Scotia\n# Answer:\nBeinn Bhreagh, Nova Scotia", "# Reasoning Path:\nAlexander Graham Bell -> people.deceased_person.place_of_death -> Beinn Bhreagh, Nova Scotia -> common.topic.article -> m.08npxf\n# Answer:\nBeinn Bhreagh, Nova Scotia", "# Reasoning Path:\nAlexander Graham Bell -> people.deceased_person.place_of_death -> Beinn Bhreagh, Nova Scotia -> common.topic.notable_for -> g.1258ng70q\n# Answer:\nBeinn Bhreagh, Nova Scotia", "# Reasoning Path:\nAlexander Graham Bell -> people.person.profession -> Professor -> fictional_universe.character_occupation.characters_with_this_occupation -> Advocat\n# Answer:\nProfessor", "# Reasoning Path:\nAlexander Graham Bell -> book.author.works_written -> Memoir upon the formation of a deaf variety of the human race -> common.topic.notable_for -> g.1259rkc_s\n# Answer:\nMemoir upon the formation of a deaf variety of the human race", "# Reasoning Path:\nAlexander Graham Bell -> people.person.profession -> Professor -> fictional_universe.character_occupation.characters_with_this_occupation -> Bandaran adepts\n# Answer:\nProfessor", "# Reasoning Path:\nAlexander Graham Bell -> people.person.profession -> Professor -> common.topic.webpage -> m.09w0lvt\n# Answer:\nProfessor", "# Reasoning Path:\nAlexander Graham Bell -> people.person.profession -> Businessperson -> common.topic.subject_of -> PointDrive\n# Answer:\nBusinessperson", "# Reasoning Path:\nAlexander Graham Bell -> book.author.works_written -> Memoir upon the formation of a deaf variety of the human race -> common.topic.notable_types -> Book\n# Answer:\nMemoir upon the formation of a deaf variety of the human race", "# Reasoning Path:\nAlexander Graham Bell -> people.person.profession -> Professor -> fictional_universe.character_occupation.characters_with_this_occupation -> Chartreuse Grande\n# Answer:\nProfessor"], "ground_truth": ["Beinn Bhreagh, Nova Scotia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-911", "prediction": ["# Reasoning Path:\nGolden State Warriors -> sports.sports_team.championships -> 1975 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n1975 NBA Finals", "# Reasoning Path:\nGolden State Warriors -> sports.sports_team.championships -> 1975 NBA Finals -> sports.sports_championship_event.season -> 1974\u201375 NBA season\n# Answer:\n1975 NBA Finals", "# Reasoning Path:\nGolden State Warriors -> tv.tv_actor.guest_roles -> m.0ghq5rf -> tv.tv_guest_role.episodes_appeared_in -> 1975 NBA Finals Game 1\n# Answer:\n1975 NBA Finals Game 1", "# Reasoning Path:\nGolden State Warriors -> tv.tv_actor.guest_roles -> m.0ghllvw -> tv.tv_guest_role.episodes_appeared_in -> 1975 NBA Finals Game 3\n# Answer:\n1975 NBA Finals Game 3", "# Reasoning Path:\nGolden State Warriors -> basketball.basketball_team.previous_coaches -> m.0j2m4h6 -> basketball.basketball_historical_coach_position.coach -> Keith Smart\n# Answer:\nKeith Smart", "# Reasoning Path:\nGolden State Warriors -> tv.tv_actor.guest_roles -> m.0ghn9j6 -> tv.tv_guest_role.episodes_appeared_in -> 1975 NBA Finals Game 2\n# Answer:\n1975 NBA Finals Game 2", "# Reasoning Path:\nGolden State Warriors -> basketball.basketball_team.previous_coaches -> m.010gy4q9 -> basketball.basketball_historical_coach_position.coach -> Mark Jackson\n# Answer:\nMark Jackson", "# Reasoning Path:\nGolden State Warriors -> basketball.basketball_team.previous_coaches -> m.04nscwm -> basketball.basketball_historical_coach_position.coach -> Don Nelson\n# Answer:\nDon Nelson"], "ground_truth": ["1975 NBA Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-912", "prediction": ["# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Revaluation of the Turkish Lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nRevaluation of the Turkish Lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.065sxsl -> military.military_combatant_group.conflict -> Korean War\n# Answer:\nKorean War", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.066402y -> military.military_combatant_group.conflict -> Iraqi no-fly zones\n# Answer:\nIraqi no-fly zones"], "ground_truth": ["Turkish lira"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-914", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Tyrone\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> base.aareas.schema.administrative_area.administrative_children -> East Midlands\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Scotland -> location.country.first_level_divisions -> Aberdeen\n# Answer:\nScotland", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nChild pornography", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> base.aareas.schema.administrative_area.administrative_children -> East of England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nNorthern Ireland"], "ground_truth": ["England", "Northern Ireland", "Scotland", "Wales"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8181818181818182, "ans_precission": 0.9, "ans_recall": 0.75, "path_f1": 0.6, "path_precision": 0.5, "path_recall": 0.75, "path_ans_f1": 0.8181818181818182, "path_ans_precision": 0.9, "path_ans_recall": 0.75}
{"id": "WebQTest-915", "prediction": ["# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.article -> m.0642vqz\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> sports.sports_championship_event.season -> 2000 NFL season\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> common.topic.notable_types -> Super bowl\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> film.film_subject.films -> The Band That Wouldn't Die -> film.film.music -> Marcello Zavros\n# Answer:\nThe Band That Wouldn't Die", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> 2001 AFC Championship Game -> sports.sports_championship_event.runner_up -> Oakland Raiders\n# Answer:\n2001 AFC Championship Game", "# Reasoning Path:\nBaltimore Ravens -> film.film_subject.films -> The Band That Wouldn't Die -> film.film.subjects -> Baltimore's Marching Ravens\n# Answer:\nThe Band That Wouldn't Die", "# Reasoning Path:\nBaltimore Ravens -> american_football.football_team.away_games -> Baltimore Ravens at Cincinnati Bengals, 2008-11-30 -> american_football.football_game.rushing -> m.079228c\n# Answer:\nBaltimore Ravens at Cincinnati Bengals, 2008-11-30", "# Reasoning Path:\nBaltimore Ravens -> american_football.football_team.away_games -> Baltimore Ravens at Cincinnati Bengals, 2009-11-08 -> american_football.football_game.season -> 2009 NFL season\n# Answer:\nBaltimore Ravens at Cincinnati Bengals, 2009-11-08", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> 2001 AFC Championship Game -> sports.sports_championship_event.season -> 2000 NFL season\n# Answer:\n2001 AFC Championship Game"], "ground_truth": ["Super Bowl XLVII", "2013 AFC Championship Game", "Super Bowl XXXV", "2001 AFC Championship Game"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.6666666666666665, "ans_precission": 0.6, "ans_recall": 0.75, "path_f1": 0.6666666666666665, "path_precision": 0.6, "path_recall": 0.75, "path_ans_f1": 0.6666666666666665, "path_ans_precision": 0.6, "path_ans_recall": 0.75}
{"id": "WebQTest-916", "prediction": ["# Reasoning Path:\nRussia -> location.country.languages_spoken -> Kumyk Language -> language.human_language.language_family -> Altaic languages\n# Answer:\nKumyk Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Yiddish Language -> common.topic.notable_types -> Human Language\n# Answer:\nYiddish Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> language.human_language.countries_spoken_in -> Ukraine\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Kumyk Language -> common.topic.notable_types -> Human Language\n# Answer:\nKumyk Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Kumyk Language -> language.human_language.language_family -> Turkic languages\n# Answer:\nKumyk Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Yiddish Language -> language.human_language.countries_spoken_in -> Sweden\n# Answer:\nYiddish Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> common.topic.notable_types -> Human Language\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.11b60vv5th\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Yiddish Language -> language.human_language.main_country -> United States of America\n# Answer:\nYiddish Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Abaza Language -> language.human_language.region -> Europe\n# Answer:\nAbaza Language"], "ground_truth": ["Altai language", "Abaza Language", "Russian Language", "Komi language", "Yakut Language", "Tuvin Language", "Yiddish Language", "Nogai Language", "Tabassaran Language", "Dargwa Language", "Aghul language", "Azerbaijani language", "Lak Language", "Bashkir Language", "Ukrainian Language", "Kalmyk-Oirat Language", "Rutul language", "Avar Language", "Khakas Language", "Erzya Language", "Adyghe Language", "Tsakhur Language", "Buryat language", "Osetin Language", "Crimean Turkish Language", "Lezgi Language", "Kumyk Language", "Udmurt Language", "Kabardian Language", "Moksha Language", "Chechen Language", "Karachay-Balkar Language", "Tatar Language", "Mari language", "Ingush Language"], "ans_acc": 0.11428571428571428, "ans_hit": 1, "ans_f1": 0.2028169014084507, "ans_precission": 0.9, "ans_recall": 0.11428571428571428, "path_f1": 0.2028169014084507, "path_precision": 0.9, "path_recall": 0.11428571428571428, "path_ans_f1": 0.2028169014084507, "path_ans_precision": 0.9, "path_ans_recall": 0.11428571428571428}
{"id": "WebQTest-918", "prediction": ["# Reasoning Path:\nCambodia -> government.governmental_jurisdiction.governing_officials -> m.010ggj4c -> government.government_position_held.office_holder -> Hun Sen\n# Answer:\nHun Sen", "# Reasoning Path:\nCambodia -> royalty.kingdom.rulers -> Norodom of Cambodia -> people.person.gender -> Male\n# Answer:\nNorodom of Cambodia", "# Reasoning Path:\nCambodia -> royalty.kingdom.rulers -> Ang Duong -> freebase.valuenotation.has_value -> Place of birth\n# Answer:\nAng Duong", "# Reasoning Path:\nCambodia -> royalty.kingdom.rulers -> Norodom of Cambodia -> people.deceased_person.place_of_death -> Phnom Penh\n# Answer:\nNorodom of Cambodia", "# Reasoning Path:\nCambodia -> government.governmental_jurisdiction.governing_officials -> m.010ggj4c -> government.government_position_held.basic_title -> Second Prime Minister\n# Answer:\nSecond Prime Minister", "# Reasoning Path:\nCambodia -> government.governmental_jurisdiction.governing_officials -> m.010ggj20 -> government.government_position_held.office_holder -> Norodom Ranariddh\n# Answer:\nNorodom Ranariddh", "# Reasoning Path:\nCambodia -> royalty.kingdom.rulers -> Ang Duong -> people.person.gender -> Male\n# Answer:\nAng Duong", "# Reasoning Path:\nCambodia -> royalty.kingdom.rulers -> Norodom Suramarit -> music.composer.compositions -> Nokor Reach\n# Answer:\nNorodom Suramarit", "# Reasoning Path:\nCambodia -> government.governmental_jurisdiction.governing_officials -> m.010ggj6j -> government.government_position_held.office_holder -> Ung Huot\n# Answer:\nUng Huot", "# Reasoning Path:\nCambodia -> travel.travel_destination.tour_operators -> Bunnik Tours -> travel.tour_operator.travel_destinations -> Africa\n# Answer:\nBunnik Tours"], "ground_truth": ["Norodom Sihamoni"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-919", "prediction": ["# Reasoning Path:\nSerena Williams -> people.person.parents -> Richard Williams -> people.person.parents -> Julia Mae Williams\n# Answer:\nRichard Williams", "# Reasoning Path:\nSerena Williams -> people.person.parents -> Richard Williams -> people.person.children -> Venus Williams\n# Answer:\nRichard Williams", "# Reasoning Path:\nSerena Williams -> people.person.parents -> Oracene Price -> people.person.education -> m.0n1kw8m\n# Answer:\nOracene Price", "# Reasoning Path:\nSerena Williams -> people.person.parents -> Oracene Price -> people.person.children -> Venus Williams\n# Answer:\nOracene Price", "# Reasoning Path:\nSerena Williams -> people.person.parents -> Richard Williams -> base.schemastaging.sports_coach.athletes_coached -> m.0h_k9kf\n# Answer:\nRichard Williams", "# Reasoning Path:\nSerena Williams -> people.person.parents -> Oracene Price -> common.topic.notable_for -> g.1256r8wrk\n# Answer:\nOracene Price", "# Reasoning Path:\nSerena Williams -> base.siswimsuitmodels.si_swimsuit_model.issues -> m.05dy_g1 -> base.siswimsuitmodels.si_swimsuit_appearance.year -> 2003 Sports Illustrated Swimsuit Issue\n# Answer:\n2003 Sports Illustrated Swimsuit Issue", "# Reasoning Path:\nSerena Williams -> award.award_nominee.award_nominations -> m.010_zz0q -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work", "# Reasoning Path:\nSerena Williams -> people.person.parents -> Richard Williams -> base.schemastaging.sports_coach.athletes_coached -> m.0h_k9mh\n# Answer:\nRichard Williams", "# Reasoning Path:\nSerena Williams -> award.award_nominee.award_nominations -> m.010_zz0q -> award.award_nomination.ceremony -> 2014 ESPY Awards\n# Answer:\n2014 ESPY Awards"], "ground_truth": ["Oracene Price", "Richard Williams"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-920", "prediction": ["# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncxj8d -> education.education.institution -> Trinity College, Cambridge\n# Answer:\nTrinity College, Cambridge", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.02kq1mm -> education.education.institution -> Trinity College, Cambridge\n# Answer:\nTrinity College, Cambridge", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncl4g6 -> education.education.institution -> The King's School, Grantham\n# Answer:\nThe King's School, Grantham", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncxj8d -> education.education.degree -> Master of Arts\n# Answer:\nMaster of Arts", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.02kq1mm -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncl4g6 -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> Newton -> astronomy.extraterrestrial_location.geolocation -> m.0w2sw5y\n# Answer:\nNewton", "# Reasoning Path:\nIsaac Newton -> base.kwebbase.kwtopic.connections_to -> abraham de moivre studied work of isaac newton -> base.kwebbase.kwconnection.subject -> Abraham de Moivre\n# Answer:\nabraham de moivre studied work of isaac newton", "# Reasoning Path:\nIsaac Newton -> base.kwebbase.kwtopic.connections_to -> anton van leeuwenhoek royal society member with isaac newton -> base.kwebbase.kwconnection.sentence -> In 1680 Leeuwenhoek was elected a full member of the Royal Society, joining Newton, Hooke, Oldenburg, Boyle, Wren,  Edmond Halley, and other scientific luminaries of his day, who exchanged letters with him, suggesting subjects for his research.\n# Answer:\nanton van leeuwenhoek royal society member with isaac newton", "# Reasoning Path:\nIsaac Newton -> base.kwebbase.kwtopic.connections_to -> abraham de moivre studied work of isaac newton -> base.kwebbase.kwconnection.sentence -> On his release in 1686, he fled to England, where he made a modest living tutoring in mathematics, and continued to study recent mathematical texts, the most notable being Newton's 'Principia Mathematica.' In 1692 he met the astronomer Halley who was so impressed by him that he read his first paper (on Newton's calculus), to the Royal Society in 1695.\n# Answer:\nabraham de moivre studied work of isaac newton"], "ground_truth": ["Trinity College, Cambridge", "The King's School, Grantham"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-921", "prediction": ["# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> medicine.disease.notable_people_with_this_condition -> Alan Joyce\n# Answer:\nProstate cancer", "# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> common.topic.webpage -> m.04r0btg\n# Answer:\nProstate cancer", "# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> medicine.disease.notable_people_with_this_condition -> Andrew Grove\n# Answer:\nProstate cancer", "# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjf8n\n# Answer:\nProstate cancer", "# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> common.topic.webpage -> m.09wv45d\n# Answer:\nProstate cancer", "# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> medicine.disease.notable_people_with_this_condition -> Billy Connolly\n# Answer:\nProstate cancer", "# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> common.topic.webpage -> m.09x3mvh\n# Answer:\nProstate cancer", "# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.05b7d26\n# Answer:\nProstate cancer", "# Reasoning Path:\nHuell Howser -> common.topic.image -> HuellandAaronProctor -> common.image.size -> m.0290xbz\n# Answer:\nHuellandAaronProctor", "# Reasoning Path:\nHuell Howser -> people.person.profession -> Television producer -> common.topic.subject_of -> Michael Palance\n# Answer:\nTelevision producer"], "ground_truth": ["Prostate cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-922", "prediction": ["# Reasoning Path:\nSuzanne Collins -> film.producer.films_executive_produced -> The Hunger Games: Catching Fire -> film.film.production_companies -> Color Force\n# Answer:\nThe Hunger Games: Catching Fire", "# Reasoning Path:\nSuzanne Collins -> book.author.book_editions_published -> Gregor and the Prophecy of Bane (Underland Chronicles (Paperback)) -> book.book_edition.book -> Gregor and the Prophecy of Bane\n# Answer:\nGregor and the Prophecy of Bane (Underland Chronicles (Paperback))", "# Reasoning Path:\nSuzanne Collins -> book.author.book_editions_published -> Gregor the Overlander (Underland Chronicles (Turtleback)) -> book.book_edition.book -> Gregor the Overlander\n# Answer:\nGregor the Overlander (Underland Chronicles (Turtleback))", "# Reasoning Path:\nSuzanne Collins -> film.producer.films_executive_produced -> The Hunger Games: Catching Fire -> film.film.written_by -> Michael Arndt\n# Answer:\nThe Hunger Games: Catching Fire", "# Reasoning Path:\nSuzanne Collins -> book.author.book_editions_published -> Gregor The Overlander (Underland Chronicles) -> book.book_edition.book -> Gregor the Overlander\n# Answer:\nGregor The Overlander (Underland Chronicles)", "# Reasoning Path:\nSuzanne Collins -> film.producer.films_executive_produced -> The Hunger Games: Catching Fire -> film.film.production_companies -> Lions Gate Entertainment\n# Answer:\nThe Hunger Games: Catching Fire", "# Reasoning Path:\nSuzanne Collins -> film.producer.films_executive_produced -> The Hunger Games: Catching Fire -> award.award_nominated_work.award_nominations -> m.0107lb8y\n# Answer:\nThe Hunger Games: Catching Fire", "# Reasoning Path:\nSuzanne Collins -> film.producer.films_executive_produced -> The Hunger Games -> award.award_nominated_work.award_nominations -> m.0_r6fh7\n# Answer:\nThe Hunger Games", "# Reasoning Path:\nSuzanne Collins -> film.producer.films_executive_produced -> The Hunger Games -> book.book_edition.place_of_publication -> New York City\n# Answer:\nThe Hunger Games", "# Reasoning Path:\nSuzanne Collins -> film.producer.films_executive_produced -> The Hunger Games -> film.film.songs -> m.0j6q1bx\n# Answer:\nThe Hunger Games"], "ground_truth": ["Gregor and the Curse of the Warmbloods", "The Hunger Games", "Gregor the Overlander (Underland Chronicles (Sagebrush))", "The Hunger Games Trilogy Boxed Set", "Gregor and the Code of Claw (Thorndike Press Large Print Literacy Bridge Series)", "Mockingjay", "Gregor and the marks of secret", "Gregor the Overlander (Underland Chronicles (Turtleback))", "Gregor and the curse of the warmbloods", "Catching Fire", "Gregor and the Code of Claw (Underland Chronicles, Book 5)", "Gregor and the Prophecy of Bane", "Gregor the Overlander", "FIRE PROOF", "Gregor and the Code of Claw", "Fire Proof", "Gregor The Overlander (Underland Chronicles)", "Gregor and the Prophecy of Bane (Underland Chronicles (Turtleback))", "Gregor And The Code Of Claw", "Gregor and the Prophecy of Bane (Underland Chronicles (Paperback))", "The Underland Chronicles Book Three", "Gregor and the Marks of Secret", "When Charlie McButton lost power", "Gregor the Overlander (Underland Chronicles)", "Gregor and the Prophecy of Bane (Underland Chronicles (Audio))"], "ans_acc": 0.32, "ans_hit": 1, "ans_f1": 0.48484848484848486, "ans_precission": 1.0, "ans_recall": 0.32, "path_f1": 0.391304347826087, "path_precision": 1.0, "path_recall": 0.24324324324324326, "path_ans_f1": 0.48484848484848486, "path_ans_precision": 1.0, "path_ans_recall": 0.32}
{"id": "WebQTest-923", "prediction": ["# Reasoning Path:\nEngland -> organization.organization_scope.organizations_with_this_scope -> Employment Appeal Tribunal -> organization.organization.geographic_scope -> Wales\n# Answer:\nEmployment Appeal Tribunal", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Border Collie -> biology.animal_breed.place_of_origin -> Scotland\n# Answer:\nBorder Collie", "# Reasoning Path:\nEngland -> organization.organization_scope.organizations_with_this_scope -> Employment Appeal Tribunal -> organization.organization.geographic_scope -> Scotland\n# Answer:\nEmployment Appeal Tribunal", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Border Collie -> biology.animal_breed.place_of_origin -> Wales\n# Answer:\nBorder Collie", "# Reasoning Path:\nEngland -> organization.organization_scope.organizations_with_this_scope -> Employment Appeal Tribunal -> common.topic.notable_for -> g.1yp3dy8kn\n# Answer:\nEmployment Appeal Tribunal", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Border Collie -> base.petbreeds.dog_breed.family_friendly -> Friendly\n# Answer:\nBorder Collie", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Airedale Terrier -> biology.animal_breed.place_of_origin -> United Kingdom\n# Answer:\nAiredale Terrier", "# Reasoning Path:\nEngland -> organization.organization_scope.organizations_with_this_scope -> Employment Appeal Tribunal -> common.topic.article -> m.07rjbn\n# Answer:\nEmployment Appeal Tribunal", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Border Collie -> biology.animal_breed.place_of_origin -> Ireland\n# Answer:\nBorder Collie", "# Reasoning Path:\nEngland -> organization.organization_scope.organizations_with_this_scope -> A BEE C -> common.topic.article -> m.0bm04k\n# Answer:\nA BEE C"], "ground_truth": ["Scotland", "Wales"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.0851063829787234, "path_precision": 0.4, "path_recall": 0.047619047619047616, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-924", "prediction": ["# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010r25mp -> soccer.football_goal.point_awarded_to -> Argentina national football team\n# Answer:\nArgentina national football team", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010h62gm -> soccer.football_goal.point_awarded_to -> FC Barcelona\n# Answer:\nFC Barcelona", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010r25mp -> soccer.football_goal.match -> 2014 Argentina vs Slovenia friendly match\n# Answer:\n2014 Argentina vs Slovenia friendly match", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010gkw62 -> soccer.football_goal.point_awarded_to -> FC Barcelona\n# Answer:\nFC Barcelona", "# Reasoning Path:\nLionel Messi -> base.schemastaging.athlete_extra.salary -> m.0qtkxlg -> base.schemastaging.athlete_salary.team -> FC Barcelona\n# Answer:\nFC Barcelona", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010h62gm -> soccer.football_goal.match -> 2014 Barcelona vs Athletic Bilbao\n# Answer:\n2014 Barcelona vs Athletic Bilbao", "# Reasoning Path:\nLionel Messi -> soccer.football_player.matches_played -> m.0g9n9gz -> soccer.football_player_match_participation.team -> Argentina national football team\n# Answer:\nArgentina national football team", "# Reasoning Path:\nLionel Messi -> base.schemastaging.athlete_extra.salary -> m.0qtkxlg -> base.schemastaging.athlete_salary.currency -> Euro\n# Answer:\nEuro", "# Reasoning Path:\nLionel Messi -> soccer.football_player.matches_played -> m.0g9n9gz -> soccer.football_player_match_participation.match -> 2010 FIFA World Cup Round of 16 - ARG ./. MEX\n# Answer:\n2010 FIFA World Cup Round of 16 - ARG ./. MEX", "# Reasoning Path:\nLionel Messi -> soccer.football_player.matches_played -> m.0c0ltpz -> soccer.football_player_match_participation.team -> Argentina national football team\n# Answer:\nArgentina national football team"], "ground_truth": ["FC Barcelona", "Argentina national football team"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.41379310344827586, "path_precision": 0.6, "path_recall": 0.3157894736842105, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-925", "prediction": ["# Reasoning Path:\nGreensburg -> location.location.time_zones -> Eastern Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nGreensburg -> location.location.time_zones -> Eastern Time Zone -> common.topic.notable_for -> g.125bn2wcf\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nGreensburg -> location.statistical_region.population -> g.11b66hhtj5\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nGreensburg -> location.location.time_zones -> Eastern Time Zone -> common.topic.image -> UnionSquareAtomicClock\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nGreensburg -> location.location.time_zones -> Eastern Time Zone -> common.topic.article -> m.02hcvg\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nGreensburg -> location.statistical_region.population -> g.11bymk01vg\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nGreensburg -> location.statistical_region.population -> g.11x1cdqvq\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nGreensburg -> location.location.time_zones -> Eastern Time Zone -> common.topic.article -> m.0j44rkj\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nGreensburg -> common.topic.webpage -> m.04lvlb7 -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nGreensburg -> location.location.time_zones -> Eastern Time Zone -> common.topic.article -> m.0j62c65\n# Answer:\nEastern Time Zone"], "ground_truth": ["Eastern Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-927", "prediction": ["# Reasoning Path:\nLord Voldemort -> film.film_character.portrayed_in_films -> m.0pc66_w -> film.performance.actor -> Christian Coulson\n# Answer:\nChristian Coulson", "# Reasoning Path:\nLord Voldemort -> film.film_character.portrayed_in_films -> m.0pc66_w -> film.performance.film -> Harry Potter and the Deathly Hallows \u2013 Part 2\n# Answer:\nHarry Potter and the Deathly Hallows \u2013 Part 2", "# Reasoning Path:\nLord Voldemort -> film.film_character.portrayed_in_films -> m.02t9_zv -> film.performance.actor -> Richard Bremmer\n# Answer:\nRichard Bremmer", "# Reasoning Path:\nLord Voldemort -> film.film_character.portrayed_in_films -> m.02vcdgy -> film.performance.actor -> Ralph Fiennes\n# Answer:\nRalph Fiennes", "# Reasoning Path:\nLord Voldemort -> film.film_character.portrayed_in_films -> m.0pc66_w -> film.performance.special_performance_type -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nLord Voldemort -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nLord Voldemort -> film.film_character.portrayed_in_films -> m.0pc66_w -> film.performance.special_performance_type -> Uncredited\n# Answer:\nUncredited", "# Reasoning Path:\nLord Voldemort -> film.film_character.portrayed_in_films -> m.02t9_zv -> film.performance.film -> Harry Potter and the Philosopher's Stone\n# Answer:\nHarry Potter and the Philosopher's Stone", "# Reasoning Path:\nLord Voldemort -> film.film_character.portrayed_in_films -> m.02vcdgy -> film.performance.film -> Harry Potter and the Order of the Phoenix\n# Answer:\nHarry Potter and the Order of the Phoenix", "# Reasoning Path:\nLord Voldemort -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Hogwarts School of Witchcraft and Wizardry -> fictional_universe.fictional_employer.employees -> m.02vhkr0\n# Answer:\nHogwarts School of Witchcraft and Wizardry"], "ground_truth": ["Hero Fiennes-Tiffin", "Ralph Fiennes", "Richard Bremmer", "Christian Coulson", "Frank Dillane"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.3, "ans_recall": 0.6, "path_f1": 0.2727272727272727, "path_precision": 0.3, "path_recall": 0.25, "path_ans_f1": 0.4, "path_ans_precision": 0.3, "path_ans_recall": 0.6}
{"id": "WebQTest-928", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> University of Alabama School of Law\n# Answer:\nUniversity of Alabama School of Law", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nHuntingdon College", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy9 -> education.education.institution -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.place_of_birth -> Los Angeles\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.gender -> Female\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_types -> Book Edition\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_for -> g.125920htw\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nHarper Seven Beckham"], "ground_truth": ["University of Oxford", "Huntingdon College", "University of Alabama", "University of Alabama School of Law"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-929", "prediction": ["# Reasoning Path:\nKu Klux Klan -> organization.organization.founders -> Richard R. Reed -> common.topic.notable_types -> Organization founder\n# Answer:\nRichard R. Reed", "# Reasoning Path:\nKu Klux Klan -> organization.organization.founders -> Richard R. Reed -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nRichard R. Reed", "# Reasoning Path:\nKu Klux Klan -> organization.organization.founders -> Frank O. McCord -> common.topic.notable_types -> Organization founder\n# Answer:\nFrank O. McCord", "# Reasoning Path:\nKu Klux Klan -> organization.organization.founders -> Richard R. Reed -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nRichard R. Reed", "# Reasoning Path:\nKu Klux Klan -> organization.organization.organization_type -> Secret society -> organization.organization_type.organizations_of_this_type -> ANAK Society\n# Answer:\nSecret society", "# Reasoning Path:\nKu Klux Klan -> organization.organization.organization_type -> Secret society -> common.topic.notable_for -> g.12556r00x\n# Answer:\nSecret society", "# Reasoning Path:\nKu Klux Klan -> organization.organization.founders -> Richard R. Reed -> freebase.valuenotation.has_value -> Parents\n# Answer:\nRichard R. Reed", "# Reasoning Path:\nKu Klux Klan -> organization.organization.founders -> Frank O. McCord -> people.person.gender -> Male\n# Answer:\nFrank O. McCord", "# Reasoning Path:\nKu Klux Klan -> organization.organization.founders -> J. Calvin Jones -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nJ. Calvin Jones", "# Reasoning Path:\nKu Klux Klan -> common.topic.image -> Klan-in-gainesville -> common.image.size -> m.029q1d5\n# Answer:\nKlan-in-gainesville"], "ground_truth": ["J. Calvin Jones", "John C. Lester", "Frank O. McCord", "James R. Crowe", "Richard R. Reed", "John B. Kennedy"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5833333333333334, "ans_precission": 0.7, "ans_recall": 0.5, "path_f1": 0.5833333333333334, "path_precision": 0.7, "path_recall": 0.5, "path_ans_f1": 0.5833333333333334, "path_ans_precision": 0.7, "path_ans_recall": 0.5}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.partially_contains -> Didi Abuli\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Denmark\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> base.locations.continents.countries_within -> Denmark\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> location.location.partially_contains -> Didi Abuli\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.containedby -> Eurasia -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurasia", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Georgia\n# Answer:\nEurope"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-931", "prediction": ["# Reasoning Path:\nJohn Adams -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> Ambassador\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.09ryhrl -> government.government_position_held.office_position_or_title -> Delegate to the Continental Congress\n# Answer:\nDelegate to the Continental Congress", "# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.04mm9px -> government.government_position_held.office_position_or_title -> Vice President of the United States\n# Answer:\nVice President of the United States", "# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.04466xb -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nJohn Adams -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> Diplomat\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Adams -> people.person.profession -> Politician -> freebase.type_profile.kind -> Title\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Adams -> people.person.profession -> Politician -> type.type.expected_by -> Officeholder\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Adams -> government.political_appointer.appointees -> m.04kq1pf -> government.government_position_held.office_position_or_title -> United States Secretary of the Treasury\n# Answer:\nUnited States Secretary of the Treasury", "# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.09ryhrl -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nFrom", "# Reasoning Path:\nJohn Adams -> government.political_appointer.appointees -> m.04kq1q1 -> government.government_position_held.office_position_or_title -> United States Secretary of the Treasury\n# Answer:\nUnited States Secretary of the Treasury"], "ground_truth": ["United States Ambassador to the United Kingdom", "Delegate to the Continental Congress", "Vice President of the United States", "United States Ambassador to the Netherlands"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.28571428571428575, "ans_precission": 0.2, "ans_recall": 0.5, "path_f1": 0.23529411764705882, "path_precision": 0.2, "path_recall": 0.2857142857142857, "path_ans_f1": 0.28571428571428575, "path_ans_precision": 0.2, "path_ans_recall": 0.5}
{"id": "WebQTest-932", "prediction": ["# Reasoning Path:\nSuper Mario Bros. 2 -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Super Mario Universe -> fictional_universe.fictional_universe.created_by -> Shigeru Miyamoto\n# Answer:\nSuper Mario Universe", "# Reasoning Path:\nSuper Mario Bros. 2 -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Super Mario Universe -> fictional_universe.fictional_universe.created_by -> Nintendo\n# Answer:\nSuper Mario Universe", "# Reasoning Path:\nSuper Mario Bros. 2 -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Super Mario Universe -> fictional_universe.fictional_universe.works_set_here -> Super Mario Bros.\n# Answer:\nSuper Mario Universe", "# Reasoning Path:\nSuper Mario Bros. 2 -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Super Mario Universe -> fictional_universe.fictional_universe.species -> Bob-omb\n# Answer:\nSuper Mario Universe", "# Reasoning Path:\nSuper Mario Bros. 2 -> book.book_subject.works -> For Super Players -> book.published_work.published_in_published_as -> m.065tqzx\n# Answer:\nFor Super Players", "# Reasoning Path:\nSuper Mario Bros. 2 -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Super Mario Universe -> fictional_universe.fictional_universe.works_set_here -> Dance Dance Revolution Mario Mix\n# Answer:\nSuper Mario Universe", "# Reasoning Path:\nSuper Mario Bros. 2 -> base.wikipedia_infobox.video_game.platforms -> Family Computer Disk System -> common.topic.notable_types -> Video Game Platform\n# Answer:\nFamily Computer Disk System", "# Reasoning Path:\nSuper Mario Bros. 2 -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Super Mario Universe -> fictional_universe.fictional_universe.works_set_here -> Donkey Kong\n# Answer:\nSuper Mario Universe", "# Reasoning Path:\nSuper Mario Bros. 2 -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Super Mario Universe -> fictional_universe.fictional_universe.species -> Boos\n# Answer:\nSuper Mario Universe", "# Reasoning Path:\nSuper Mario Bros. 2 -> book.book_subject.works -> For Super Players -> common.topic.notable_types -> Short Non-fiction\n# Answer:\nFor Super Players"], "ground_truth": ["1988-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-933", "prediction": ["# Reasoning Path:\nAndrew Johnson -> government.us_vice_president.to_president -> Abraham Lincoln -> organization.organization_founder.organizations_founded -> Republican Party\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAndrew Johnson -> government.us_vice_president.to_president -> Abraham Lincoln -> organization.organization_founder.organizations_founded -> Illinois Republican Party\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAndrew Johnson -> government.politician.party -> m.03gjg0m -> government.political_party_tenure.party -> Democratic Party\n# Answer:\nDemocratic Party", "# Reasoning Path:\nAndrew Johnson -> government.us_vice_president.to_president -> Abraham Lincoln -> government.politician.party -> m.03gjfyh\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAndrew Johnson -> government.politician.party -> m.0lr1qnz -> government.political_party_tenure.party -> Republican Party\n# Answer:\nRepublican Party", "# Reasoning Path:\nAndrew Johnson -> government.us_vice_president.to_president -> Abraham Lincoln -> organization.organization_founder.organizations_founded -> Freedmen's Bureau\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAndrew Johnson -> government.us_vice_president.to_president -> Abraham Lincoln -> government.politician.party -> m.03gjfyn\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAndrew Johnson -> government.us_vice_president.to_president -> Abraham Lincoln -> government.politician.party -> m.03ld2ph\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAndrew Johnson -> government.us_vice_president.to_president -> Abraham Lincoln -> people.person.places_lived -> m.03pgr_5\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAndrew Johnson -> government.politician.party -> m.03gjg0g -> government.political_party_tenure.party -> National Union Party\n# Answer:\nNational Union Party"], "ground_truth": ["Democratic Party", "National Union Party", "Republican Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-935", "prediction": ["# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> Osama: The Making of a Terrorist -> book.written_work.subjects -> Al-Qaeda\n# Answer:\nOsama: The Making of a Terrorist", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Ali Azmat\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.events -> Death of Osama bin Laden\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Amrit Singh Thapar\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.contains -> Abbottabad Hockey Stadium\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> Osama: The Making of a Terrorist -> common.topic.notable_types -> Book\n# Answer:\nOsama: The Making of a Terrorist", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Faiza Khan\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> Osama: The Making of a Terrorist -> book.book.editions -> Osama : the making of a terrorist\n# Answer:\nOsama: The Making of a Terrorist", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.contains -> Abbottabad International Medical College\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.contains -> Abbottabad Public School\n# Answer:\nAbbottabad"], "ground_truth": ["Abbottabad"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-936", "prediction": ["# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Temple of Aphaea -> location.location.containedby -> Aegina\n# Answer:\nTemple of Aphaea", "# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Temple of Zeus, Olympia -> location.location.containedby -> Ancient Greece\n# Answer:\nTemple of Zeus, Olympia", "# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Temple of Zeus, Olympia -> common.topic.notable_for -> g.125529h7s\n# Answer:\nTemple of Zeus, Olympia", "# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Temple of Aphaea -> architecture.structure.architectural_style -> Classical architecture\n# Answer:\nTemple of Aphaea", "# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Temple of Zeus, Olympia -> symbols.namesake.named_after -> Zeus\n# Answer:\nTemple of Zeus, Olympia", "# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Erechtheion -> location.location.containedby -> Athens\n# Answer:\nErechtheion", "# Reasoning Path:\nAncient Greek temple -> common.topic.image -> Athens Acropolis -> common.image.appears_in_topic_gallery -> Athens\n# Answer:\nAthens Acropolis", "# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Temple of Aphaea -> location.location.geolocation -> m.0cqr4qf\n# Answer:\nTemple of Aphaea", "# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Erechtheion -> location.location.containedby -> Greece\n# Answer:\nErechtheion", "# Reasoning Path:\nAncient Greek temple -> base.descriptive_names.names.descriptive_name -> m.010699js -> base.schemastaging.plural_form.language -> Russian\n# Answer:\nRussian"], "ground_truth": ["Shahhat", "Corfu", "Ephesus", "Ku\u015fadas\u0131", "Athens", "Olympia"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.4, "ans_recall": 0.3333333333333333, "path_f1": 0.1904761904761905, "path_precision": 0.2, "path_recall": 0.18181818181818182, "path_ans_f1": 0.4, "path_ans_precision": 0.5, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-937", "prediction": ["# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.location.containedby -> Near East\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.location.geolocation -> m.0zwv97z\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> South Africa -> location.location.containedby -> Africa\n# Answer:\nSouth Africa", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.country.capital -> Ray\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.location.containedby -> Central Asia\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.location.containedby -> Middle East\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> South Africa -> location.country.languages_spoken -> Urdu Language\n# Answer:\nSouth Africa", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> South Africa -> common.topic.notable_types -> Country\n# Answer:\nSouth Africa", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Israel -> location.location.containedby -> Middle East\n# Answer:\nIsrael", "# Reasoning Path:\nArabic Language -> language.human_language.writing_system -> Arabic alphabet -> language.language_writing_system.languages -> Algerian Arabic\n# Answer:\nArabic alphabet"], "ground_truth": ["Turkey", "Comoros", "Lebanon", "Jordan", "Djibouti", "Eritrea", "Morocco", "Mandatory Palestine", "Tanzania", "Iran", "South Africa", "Iraq", "South Yemen", "Canada", "Libya", "Kuwait", "Qatar", "Tunisia", "Egypt", "Syria", "Bahrain", "Yemen", "United Arab Emirates", "Sudan", "Saudi Arabia", "Oman", "Algeria", "Israel", "Mauritania", "Seljuk Empire"], "ans_acc": 0.13333333333333333, "ans_hit": 1, "ans_f1": 0.18000000000000002, "ans_precission": 0.9, "ans_recall": 0.1, "path_f1": 0.18000000000000002, "path_precision": 0.9, "path_recall": 0.1, "path_ans_f1": 0.23529411764705882, "path_ans_precision": 1.0, "path_ans_recall": 0.13333333333333333}
{"id": "WebQTest-938", "prediction": ["# Reasoning Path:\nElizabeth II -> people.person.parents -> Queen Elizabeth The Queen Mother -> people.person.parents -> Cecilia Bowes-Lyon, Countess of Strathmore and Kinghorne\n# Answer:\nQueen Elizabeth The Queen Mother", "# Reasoning Path:\nElizabeth II -> people.person.parents -> George VI -> people.person.parents -> Mary of Teck\n# Answer:\nGeorge VI", "# Reasoning Path:\nElizabeth II -> people.person.parents -> Queen Elizabeth The Queen Mother -> people.person.parents -> Claude Bowes-Lyon, 14th Earl of Strathmore and Kinghorne\n# Answer:\nQueen Elizabeth The Queen Mother", "# Reasoning Path:\nElizabeth II -> people.person.parents -> Queen Elizabeth The Queen Mother -> people.family_member.family -> House of Windsor\n# Answer:\nQueen Elizabeth The Queen Mother", "# Reasoning Path:\nElizabeth II -> people.person.parents -> Queen Elizabeth The Queen Mother -> people.person.sibling_s -> m.0b5lmgx\n# Answer:\nQueen Elizabeth The Queen Mother", "# Reasoning Path:\nElizabeth II -> people.person.parents -> George VI -> people.person.parents -> George V\n# Answer:\nGeorge VI", "# Reasoning Path:\nElizabeth II -> royalty.chivalric_order_officer.memberships_presented -> m.010hv442 -> royalty.chivalric_order_membership.title -> Officer of the Order of the British Empire\n# Answer:\nOfficer of the Order of the British Empire", "# Reasoning Path:\nElizabeth II -> people.person.parents -> Queen Elizabeth The Queen Mother -> people.person.sibling_s -> m.0b5lp62\n# Answer:\nQueen Elizabeth The Queen Mother", "# Reasoning Path:\nElizabeth II -> people.person.parents -> George VI -> royalty.noble_person.titles -> m.04_b_2d\n# Answer:\nGeorge VI", "# Reasoning Path:\nElizabeth II -> film.person_or_entity_appearing_in_film.films -> m.0j2sds_ -> film.personal_film_appearance.film -> A Queen's World Tour\n# Answer:\nA Queen's World Tour"], "ground_truth": ["Queen Elizabeth The Queen Mother"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-939", "prediction": ["# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Cubism -> visual_art.art_period_movement.associated_artworks -> Woman with a Guitar\n# Answer:\nCubism", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Cubism -> common.topic.image -> Braque\n# Answer:\nCubism", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Surrealism -> influence.influence_node.influenced -> J. G. Ballard\n# Answer:\nSurrealism", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Cubism -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Picasso and the Avant-Garde in Paris\n# Answer:\nCubism", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Picasso's Blue Period -> common.topic.image -> The Old Guitarist. Pablo Picasso. (1903) Oil on Panel. Art Institute of Chicago.\n# Answer:\nPicasso's Blue Period", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Cubism -> visual_art.art_period_movement.associated_artworks -> Chicago Picasso\n# Answer:\nCubism", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Cubism -> visual_art.art_period_movement.associated_artworks -> Las Meninas (after Vel\u00e1zquez)\n# Answer:\nCubism", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Surrealism -> influence.influence_node.influenced -> Arman\n# Answer:\nSurrealism", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Surrealism -> book.school_or_movement.associated_authors -> Aberjhani\n# Answer:\nSurrealism", "# Reasoning Path:\nPablo Picasso -> book.author.works_written -> Picasso -> award.award_nominated_work.award_nominations -> m.0zctbnr\n# Answer:\nPicasso"], "ground_truth": ["Cubism", "Analytic cubism", "Surrealism", "Picasso's African Period", "Picasso's Blue Period", "Picasso's Rose Period", "Modern art", "Synthetic cubism"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.5294117647058825, "ans_precission": 0.9, "ans_recall": 0.375, "path_f1": 0.5294117647058825, "path_precision": 0.9, "path_recall": 0.375, "path_ans_f1": 0.5294117647058825, "path_ans_precision": 0.9, "path_ans_recall": 0.375}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.symptom.symptom_of -> Systemic lupus erythematosus\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.symptom.symptom_of -> Coronary artery disease\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w0hpl\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> heart attack\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.symptom.symptom_of -> Myocardial Ischemia\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> Verapamil\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w4lwz\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09wjtbj\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> ACE inhibitor\n# Answer:\nheart attack"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-941", "prediction": ["# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> Temperance Fitzgerald -> freebase.valuenotation.has_value -> Parents\n# Answer:\nTemperance Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> Temperance Fitzgerald -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nTemperance Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> Temperance Fitzgerald -> people.person.spouse_s -> m.0ncm17l\n# Answer:\nTemperance Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> Temperance Fitzgerald -> people.deceased_person.cause_of_death -> heart attack\n# Answer:\nTemperance Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> William Fitzgerald -> freebase.valuenotation.has_value -> Parents\n# Answer:\nWilliam Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> Temperance Fitzgerald -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nTemperance Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> William Fitzgerald -> common.topic.notable_types -> Deceased Person\n# Answer:\nWilliam Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> music.featured_artist.recordings -> 'Tain't What You Do (It's the Way That Cha Do It) -> music.recording.canonical_version -> 'Tain't What You Do (It's the Way That You Do It)\n# Answer:\n'Tain't What You Do (It's the Way That Cha Do It)", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> William Fitzgerald -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nWilliam Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> William Fitzgerald -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nWilliam Fitzgerald"], "ground_truth": ["Temperance Fitzgerald", "William Fitzgerald"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-942", "prediction": ["# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Kate McCauley Hathaway -> people.person.parents -> Joe McCauley\n# Answer:\nKate McCauley Hathaway", "# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Gerald Hathaway -> people.person.children -> Thomas Hathaway\n# Answer:\nGerald Hathaway", "# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Gerald Hathaway -> freebase.valuenotation.has_value -> Parents\n# Answer:\nGerald Hathaway", "# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Gerald Hathaway -> people.person.children -> Michael Hathaway\n# Answer:\nGerald Hathaway", "# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Gerald Hathaway -> people.person.spouse_s -> m.0j4k1mh\n# Answer:\nGerald Hathaway", "# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Kate McCauley Hathaway -> people.person.children -> Thomas Hathaway\n# Answer:\nKate McCauley Hathaway", "# Reasoning Path:\nAnne Hathaway -> base.popstra.celebrity.canoodled -> m.0652shw -> base.popstra.canoodled.participant -> Francesco Coco\n# Answer:\nFrancesco Coco", "# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Kate McCauley Hathaway -> common.topic.notable_types -> Film actor\n# Answer:\nKate McCauley Hathaway", "# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Gerald Hathaway -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nGerald Hathaway", "# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Kate McCauley Hathaway -> people.person.children -> Michael Hathaway\n# Answer:\nKate McCauley Hathaway"], "ground_truth": ["Gerald Hathaway", "Kate McCauley Hathaway"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-943", "prediction": ["# Reasoning Path:\nDon Draper -> common.topic.notable_for -> g.1259lvfmp\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.children -> Sally Draper -> tv.tv_character.appeared_in_tv_program -> m.04ktj1q\n# Answer:\nSally Draper", "# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.children -> Sally Draper -> tv.tv_character.appeared_in_tv_episodes -> m.0ztq47n\n# Answer:\nSally Draper", "# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.children -> Bobby Draper -> common.topic.notable_types -> TV Character\n# Answer:\nBobby Draper", "# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.children -> Sally Draper -> tv.tv_character.appeared_in_tv_episodes -> m.0ztq4_9\n# Answer:\nSally Draper", "# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.children -> Bobby Draper -> tv.tv_character.appeared_in_tv_episodes -> m.0jx71n2\n# Answer:\nBobby Draper", "# Reasoning Path:\nDon Draper -> common.topic.webpage -> m.09y333s -> common.webpage.resource -> Watch the 'Mad Men' pilot episode... for free!\n# Answer:\nWatch the 'Mad Men' pilot episode... for free!", "# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.children -> Sally Draper -> tv.tv_character.appeared_in_tv_episodes -> m.0ztq4bx\n# Answer:\nSally Draper", "# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.children -> Bobby Draper -> tv.tv_character.appeared_in_tv_episodes -> m.0ztq90m\n# Answer:\nBobby Draper", "# Reasoning Path:\nDon Draper -> common.topic.webpage -> m.09y333s -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Betty Draper"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-944", "prediction": ["# Reasoning Path:\nVan Halen -> music.artist.album -> Ain't Talkin' 'Bout Love -> common.topic.notable_for -> g.12h2y4yvf\n# Answer:\nAin't Talkin' 'Bout Love", "# Reasoning Path:\nVan Halen -> music.artist.album -> Ain't Talkin' 'Bout Love -> common.topic.notable_types -> Musical Album\n# Answer:\nAin't Talkin' 'Bout Love", "# Reasoning Path:\nVan Halen -> music.artist.album -> (Oh) Pretty Woman -> music.album.compositions -> Oh, Pretty Woman\n# Answer:\n(Oh) Pretty Woman", "# Reasoning Path:\nVan Halen -> common.topic.webpage -> m.03l1y8s -> common.webpage.resource -> official website\n# Answer:\nofficial website", "# Reasoning Path:\nVan Halen -> music.artist.album -> (Oh) Pretty Woman -> music.album.release_type -> Single\n# Answer:\n(Oh) Pretty Woman", "# Reasoning Path:\nVan Halen -> music.artist.album -> 1984 -> music.album.releases -> g.11btqyy1wl\n# Answer:\n1984", "# Reasoning Path:\nVan Halen -> music.musical_group.member -> m.010kz9hr -> music.group_membership.member -> Wolfgang Van Halen\n# Answer:\nWolfgang Van Halen", "# Reasoning Path:\nVan Halen -> common.topic.webpage -> m.03l1y8s -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nVan Halen -> music.artist.album -> 1984 -> music.album.release_type -> Album\n# Answer:\n1984", "# Reasoning Path:\nVan Halen -> common.topic.webpage -> m.09wjqhm -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Ain't Talkin' 'Bout Love"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-945", "prediction": ["# Reasoning Path:\nSpain -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nSpain -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> United Kingdom\n# Answer:\nParliamentary system", "# Reasoning Path:\nSpain -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Andorra\n# Answer:\nUnitary state", "# Reasoning Path:\nSpain -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Andorra\n# Answer:\nParliamentary system", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60xs9d2\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nSpain -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nSpain -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Antigua and Barbuda\n# Answer:\nParliamentary system", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc38hhk\n# Answer:\nlocation.statistical_region.deposit_interest_rate"], "ground_truth": ["Parliamentary system", "Constitutional monarchy", "Unitary state", "Monarchy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.7241379310344827, "path_precision": 0.7, "path_recall": 0.75, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-948", "prediction": ["# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.written_work.author -> L. A. Scot Powe\n# Answer:\nThe Warren Court and American Politics", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Savannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal -> book.written_work.subjects -> Capital punishment\n# Answer:\nSavannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.written_work.author -> Lucas A. Powe\n# Answer:\nThe Warren Court and American Politics", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.written_work.subjects -> Politics of the United States\n# Answer:\nThe Warren Court and American Politics", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Savannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal -> book.written_work.previous_in_series -> Savannah Talks Troy Anthony Davis No. 11: Judge Moore says \\\"Not innocent\\\"\n# Answer:\nSavannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal", "# Reasoning Path:\nSupreme Court of the United States -> common.topic.notable_for -> g.125dtprfx\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.book.genre -> Non-fiction\n# Answer:\nThe Warren Court and American Politics", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Savannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal -> book.written_work.subjects -> Troy Anthony Davis\n# Answer:\nSavannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Courtiers of the Marble Palace: The Rise and Influence of the Supreme Court Law Clerk -> book.written_work.subjects -> Law clerk\n# Answer:\nCourtiers of the Marble Palace: The Rise and Influence of the Supreme Court Law Clerk", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.written_work.subjects -> Warren Court\n# Answer:\nThe Warren Court and American Politics"], "ground_truth": ["United States v. Oakland Cannabis Buyers' Cooperative", "Lucas v. South Carolina Coastal Council", "Duke Power Co. v. Carolina Environmental Study Group", "Domino's Pizza, Inc. v. McDonald", "Merrell Dow Pharmaceuticals Inc. v. Thompson", "McDonnell Douglas Corp. v. Green", "United States v. Gouveia", "Chicago, Milwaukee & St. Paul Railway Co. v. Minnesota", "Nix v. Hedden", "Landmark Communications, Inc. v. Virginia", "United States v. Virginia", "Edmonson v. Leesville Concrete Co.", "Jones v. Alfred H. Mayer Co.", "Jaffee v. Redmond", "Hudson v. McMillian", "College Savings Bank v. Florida Prepaid Postsecondary Education Expense Board", "Piper Aircraft Co. v. Reyno", "Haynes v. United States", "United States v. O'Brien", "Tison v. Arizona", "North Carolina v. Alford", "Burdick v. United States", "Florida Lime & Avocado Growers, Inc. v. Paul", "United States v. Dominguez Benitez", "Bellotti v. Baird", "Kassel v. Consolidated Freightways Corp.", "Boynton v. Virginia", "Kansas v. Hendricks", "McLaughlin v. Florida", "Newberry v. United States", "Rochin v. California", "Paul v. Virginia", "United States v. Ross", "City of Boerne v. Flores", "Parents Involved in Community Schools v. Seattle School District No. 1", "Cottage Savings Ass'n v. Commissioner", "Markman v. Westview Instruments, Inc.", "Witherspoon v. Illinois", "Washington v. Glucksberg", "United States v. Place", "Warner-Jenkinson Co. v. Hilton Davis Chemical Co.", "Bailey v. Drexel Furniture Co.", "New York City Transit Authority v. Beazer", "Tahoe-Sierra Preservation Council, Inc. v. Tahoe Regional Planning Agency", "Romer v. Evans", "Reed v. Reed", "Spector v. Norwegian Cruise Line Ltd.", "Houston East & West Texas Railway Co. v. United States", "Luther v. Borden", "Cox v. New Hampshire", "Rankin v. McPherson", "Douglas v. City of Jeannette", "Edelman v. Jordan", "Smith v. Maryland", "Roth v. United States", "Feist Publications, Inc., v. Rural Telephone Service Co.", "Mullane v. Central Hanover Bank & Trust Co.", "Sheldon v. Sill", "Mapp v. Ohio", "Chisholm v. Georgia", "Miller v. California", "Hill v. Wallace", "Commodity Futures Trading Commission v. Schor", "Old Colony Trust Co. v. Commissioner", "Sherbert v. Verner", "Kumho Tire Co. v. Carmichael", "Tinker v. Des Moines Independent Community School District", "Goldberg v. Kelly", "Schuette v. Coalition to Defend Affirmative Action", "Perry v. Sindermann", "Goss v. Lopez", "Stanford v. Kentucky", "Lisenba v. California", "Webster v. Reproductive Health Services", "Reynolds v. Sims", "McCullen v. Coakley", "Zablocki v. Redhail", "Kohl v. United States", "Niemotko v. Maryland", "Harris v. Balk", "Milliken v. Bradley", "Cherokee Nation v. Georgia", "Collins v. Yosemite Park & Curry Co.", "Tee-Hit-Ton Indians v. United States", "Rooker v. Fidelity Trust Co.", "West Coast Hotel Co. v. Parrish", "Octane Fitness, LLC v. ICON Health & Fitness, Inc.", "Craig v. Boren", "Riley v. California", "Trop v. Dulles", "Schlesinger v. Councilman", "Storer v. Brown", "Loretto v. Teleprompter Manhattan CATV Corp.", "Lockett v. Ohio", "District of Columbia v. Heller", "Santa Fe Independent School District v. Doe", "In re Gault", "Baze v. Rees", "Korematsu v. United States", "Parker v. Flook", "Holden v. Hardy", "Edwards v. Aguillard", "Village of Arlington Heights v. Metropolitan Housing Development Corp.", "United States v. Miller", "Leser v. Garnett", "Board of Trade of City of Chicago v. Olsen", "McGowan v. Maryland", "Texas v. White", "Martin v. Wilks", "Northern Securities Co. v. United States", "Ohio v. Robinette", "Wolf v. Colorado", "Coffin v. United States", "Tory v. Cochran", "Marsh v. Chambers", "Ewing v. California", "United States v. The Amistad", "United States v. Trans-Missouri Freight Ass'n", "Atwater v. City of Lago Vista", "Kennedy v. Louisiana", "Colorado River Water Conservation District v. United States", "Head Money Cases", "United States v. Lopez", "Marbury v. Madison", "Ray v. Blair", "Tennessee v. Lane", "Hollingsworth v. Virginia", "Guinn v. United States", "McCreary County v. American Civil Liberties Union", "United States v. Continental Can Co.", "Downes v. Bidwell", "Brown v. Entertainment Merchants Ass'n", "Saia v. New York", "Dred Scott v. Sandford", "Plessy v. Ferguson", "Wisconsin v. Illinois", "Crawford v. Washington", "Frank Lyon Co. v. United States", "Martin v. City of Struthers", "Gideon v. Wainwright", "United States v. Eichman", "Church of Lukumi Babalu Aye v. City of Hialeah", "Hansberry v. Lee", "United States v. Raines", "Vermont Yankee Nuclear Power Corp. v. Natural Resources Defense Council, Inc.", "Citizens to Preserve Overton Park v. Volpe", "Immigration and Naturalization Service v. Cardoza-Fonseca", "Asahi Metal Industry Co. v. Superior Court", "Miranda v. Arizona", "Clearfield Trust Co. v. United States", "BP America Production Co. v. Burton", "Ashcroft v. Free Speech Coalition", "Smith v. Allwright", "Dickerson v. United States", "Ohio v. Roberts", "United States v. Thompson-Center Arms Co.", "United States v. Hubbell", "Brigham City v. Stuart", "PGA Tour, Inc. v. Martin", "McConnell v. FEC", "Alaska v. Native Village of Venetie Tribal Government", "United Mine Workers of America v. Bagwell", "Escobedo v. Illinois", "Balzac v. Porto Rico", "Board of Regents of State Colleges v. Roth", "South Dakota v. Dole", "Lemon v. Kurtzman", "Cleveland Board of Education v. LaFleur", "South Carolina v. Katzenbach", "Community for Creative Non-Violence v. Reid", "Williams v. Mississippi", "Panetti v. Quarterman", "Marshall v. Marshall", "Nix v. Whiteside", "Exxon Mobil Corp. v. Saudi Basic Industries Corp.", "Cunningham v. California", "Watson v. Jones", "Little v. Barreme", "Berger v. New York", "Ferguson v. City of Charleston", "Daubert v. Merrell Dow Pharmaceuticals, Inc.", "National League of Cities v. Usery", "Sherman v. United States", "Breard v. Greene", "Regents of the University of California v. Bakke", "Milkovich v. Lorain Journal Co.", "Permanent Mission of India v. City of New York", "Follett v. Town of McCormick", "Florida v. Riley", "Intel Corp. v. Advanced Micro Devices, Inc.", "Elk Grove Unified School District v. Newdow", "Pickering v. Board of Education", "Adkins v. Children's Hospital", "Brandenburg v. Ohio", "Board of Education of Kiryas Joel Village School District v. Grumet", "National Federation of Independent Business v. Sebelius", "Civil Rights Cases", "Credit Suisse Securities (USA) LLC v. Billing", "Heckler v. Chaney", "Passenger Cases", "MGM Studios, Inc. v. Grokster, Ltd.", "Day v. McDonough", "Ex parte Milligan", "Edwards v. California", "Pryor v. United States", "United States v. Mead Corp.", "Lau v. Nichols", "New York v. Connecticut", "Nixon v. Condon", "United States v. Stanley", "Rust v. Sullivan", "Barker v. Wingo", "Central Hudson Gas & Electric Corp. v. Public Service Commission", "Baker v. Morton", "United States v. Seeger", "Sanchez-Llamas v. Oregon", "Bose Corp. v. Consumers Union of United States, Inc.", "Nixon v. Shrink Missouri Government PAC", "Jones v. Flowers", "Law v. Siegel", "International Salt Co. v. United States", "Torcaso v. Watkins", "Kelo v. City of New London", "Meritor Savings Bank v. Vinson", "Apprendi v. New Jersey", "Ex parte McCardle", "Pruneyard Shopping Center v. Robins", "Campbell v. Acuff-Rose Music, Inc.", "Garcia v. San Antonio Metropolitan Transit Authority", "Rumsfeld v. Forum for Academic & Institutional Rights, Inc.", "Ayotte v. Planned Parenthood of Northern New England", "Employment Division v. Smith", "United States v. Ballard", "California v. Byers", "Dolan v. United States Postal Service", "Lockyer v. Andrade", "Small v. United States", "Harper & Row v. Nation Enterprises", "Scheidler v. National Organization for Women", "Hope v. Pelzer", "Ring v. Arizona", "Miami Herald Publishing Co. v. Tornillo", "Friends of the Earth, Inc. v. Laidlaw Environmental Services, Inc.", "DeShaney v. Winnebago County", "Avegno v. Schmidt", "Simmons v. United States", "Mississippi v. Johnson", "Dent v. West Virginia", "Federal Power Commission v. Tuscarora Indian Nation", "Planned Parenthood of Central Missouri v. Danforth", "Bartnicki v. Vopper", "League of United Latin American Citizens v. Perry", "Penry v. Lynaugh", "Adair v. United States", "Diamond v. Diehr", "Cramer v. United States", "United States v. Salerno", "Marsh v. Alabama", "Speiser v. Randall", "Bates v. State Bar of Arizona", "Illinois v. Caballes", "Arkansas Department of Human Services v. Ahlborn", "Jacobson v. United States", "Hiibel v. Sixth Judicial District Court of Nevada", "Eisenstadt v. Baird", "Indiana v. Edwards", "Chaplinsky v. New Hampshire", "Bush v. Gore", "Nebraska Press Ass'n v. Stuart", "Tucker v. Texas", "Bragdon v. Abbott", "Garcetti v. Ceballos", "Everson v. Board of Education", "Irwin v. Gavit", "Georgia v. Randolph", "Perez v. Brownell", "Sony Corp. of America v. Universal City Studios, Inc.", "Ledbetter v. Goodyear Tire & Rubber Co.", "Cannon v. University of Chicago", "Sorrells v. United States", "Duro v. Reina", "Londoner v. City and County of Denver", "Bowsher v. Synar", "Schneider v. New Jersey", "Duncan v. Louisiana", "Hammer v. Dagenhart", "Schriro v. Summerlin", "Barnes v. Glen Theatre, Inc.", "Flast v. Cohen", "United States v. Playboy Entertainment Group", "Pennoyer v. Neff", "Village of Belle Terre v. Boraas", "County of Allegheny v. American Civil Liberties Union", "Troxel v. Granville", "Hickman v. Taylor", "Sereboff v. Mid Atlantic Medical Services, Inc.", "Wheaton v. Peters", "Palazzolo v. Rhode Island", "Youngstown Sheet & Tube Co. v. Sawyer", "Hawker v. New York", "Mistretta v. United States", "Dun & Bradstreet, Inc. v. Greenmoss Builders, Inc.", "United States v. Verdugo-Urquidez", "Utah v. Evans", "Twining v. New Jersey", "Board of Trustees of the University of Alabama v. Garrett", "City of Cleburne v. Cleburne Living Center, Inc.", "Hills v. Gautreaux", "Granholm v. Heald", "Clay v. United States", "eBay Inc. v. MercExchange, L.L.C.", "City of Elizabeth v. American Nicholson Pavement Co.", "Stenberg v. Carhart", "Joseph Burstyn, Inc. v. Wilson", "Harper v. Virginia State Board of Elections", "Locke v. Davey", "Heart of Atlanta Motel, Inc. v. United States", "Bauer & Cie. v. O'Donnell", "Katzenbach v. Morgan", "Tennessee v. Garner", "United States v. Morrison", "Banco Nacional de Cuba v. Sabbatino", "United States v. Constantine", "Shaw v. Reno", "Fletcher v. Peck", "Black and White Taxicab and Transfer Co. v. Brown and Yellow Taxicab and Transfer Co.", "Illinois v. Lidster", "Dillon v. Gloss", "Webster v. Doe", "City of Akron v. Akron Center for Reproductive Health", "Leary v. United States", "Olmstead v. United States", "Plyler v. Doe", "Lee v. Weisman", "TSC Industries, Inc. v. Northway, Inc.", "New York v. Ferber", "Jones v. City of Opelika", "Owen Equipment & Erection Co. v. Kroger", "Coker v. Georgia", "Witmer v. United States", "United States v. Forty Barrels & Twenty Kegs of Coca-Cola", "Goesaert v. Cleary", "Immigration and Naturalization Service v. Aguirre-Aguirre", "Wilkinson v. Austin", "United States v. Cruikshank", "Steward Machine Co. v. Davis", "Harmelin v. Michigan", "Bowen v. Roy", "Goldwater v. Carter", "Florida Prepaid Postsecondary Education Expense Board v. College Savings Bank", "Bell v. Wolfish", "Missouri v. Jenkins", "Burrow-Giles Lithographic Co. v. Sarony", "Pfaff v. Wells Electronics, Inc.", "Calder v. Bull", "Estelle v. Gamble", "Berea College v. Kentucky", "Sweatt v. Painter", "Rice v. Cayetano", "Schweiker v. Chilicky", "United Building & Construction Trades Council v. Mayor and Council of Camden", "Powell v. Alabama", "Near v. Minnesota", "Linder v. United States", "Hamdi v. Rumsfeld", "Joint Anti-Fascist Refugee Committee v. McGrath", "Minnesota v. Mille Lacs Band of Chippewa Indians", "Hurley v. Irish-American Gay, Lesbian, & Bisexual Group of Boston", "Lauro Lines v. Chasser", "Warth v. Seldin", "Quality King Distributors Inc., v. L'anza Research International Inc.", "Humphrey's Executor v. United States", "Cumming v. Richmond County Board of Education", "Hinderlider v. La Plata River & Cherry Creek Ditch Co.", "Doe v. Chao", "Benton v. Maryland", "Mitchell v. Forsyth", "Fogerty v. Fantasy, Inc.", "Swift v. Tyson", "United States v. Shoshone Tribe of Indians", "Colorado v. Connelly", "Illinois Central Railroad Co. v. Illinois", "Seminole Tribe of Florida v. Florida", "Rapanos v. United States", "Chevron U.S.A., Inc. v. Natural Resources Defense Council, Inc.", "Brown v. Board of Education", "Cox v. Louisiana", "Boumediene v. Bush", "United States v. Paramount Pictures, Inc.", "United States v. Russell", "County of Sacramento v. Lewis", "West Virginia State Board of Education v. Barnette", "Slaughter-House Cases", "Skinner v. Railway Labor Executives Ass'n", "Eastern Associated Coal Corp. v. United Mine Workers of America", "Cohens v. Virginia", "Motor Vehicles Manufacturers Ass'n v. State Farm Mutual Automobile Insurance Co.", "Gregg v. Georgia", "Illinois v. Rodriguez", "Pacific States Box & Basket Co. v. White", "Reynolds v. United States", "Leocal v. Ashcroft", "Hayburn's Case", "Charles River Bridge v. Warren Bridge", "Kansas v. Marsh", "Parratt v. Taylor", "National Gay Task Force v. Board of Education", "New York Times Co. v. Sullivan", "Hustler Magazine v. Falwell", "Minor v. Happersett", "Wooley v. Maynard", "Florida v. Bostick", "Greenholtz v. Inmates of the Nebraska Penal & Correctional Complex", "New York v. United States", "Thompson v. Oklahoma", "United States v. Kagama", "New England Mutual Life Insurance Co. v. Woodworth", "Grove City College v. Bell", "United States v. Butler", "Bailey v. United States", "Dennis v. United States", "Commissioner v. Wilcox", "Immigration and Naturalization Service v. Chadha", "Marquez v. Screen Actors Guild Inc.", "Gibson v. United States", "Bailey v. Alabama", "Champion v. Ames", "Chung Fook v. White", "Strawbridge v. Curtiss", "Lehnert v. Ferris Faculty Ass'n", "Pierce v. Society of Sisters", "Hartford Fire Insurance Co. v. California", "Graham v. John Deere Co.", "Doe v. Bolton", "Republican Party of Minnesota v. White", "Feiner v. New York", "Beauharnais v. Illinois", "Doyle v. Ohio", "Johnson v. Eisentrager", "Bolling v. Sharpe", "Board of Education v. Earls", "Arizona v. Hicks", "Cort v. Ash", "Virginia State Pharmacy Board v. Virginia Citizens Consumer Council", "Lovell v. City of Griffin", "Time, Inc. v. Firestone", "Prince v. Massachusetts", "Rummel v. Estelle", "Watchtower Bible & Tract Society of New York, Inc. v. Village of Stratton", "Sparf v. United States", "Shaffer v. Heitner", "United States v. Darby Lumber Co.", "McCulloch v. Maryland", "Westside School District v. Mergens", "Stanford v. Texas", "Rita v. United States", "Fowler v. Rhode Island", "NAACP v. Alabama", "Saenz v. Roe", "Osborne v. Ohio", "Rose v. Locke", "Federal Baseball Club v. National League", "NBC, Inc. v. United States", "Harris v. McRae", "Dowling v. United States", "Albertson v. Subversive Activities Control Board", "Erie Railroad Co. v. Tompkins", "New Mexico v. Texas", "Betts v. Brady", "Thomas v. Review Board of the Indiana Employment Security Division", "Lum v. Rice", "Illinois v. Gates", "Talbot v. Janson", "Rogers v. Tennessee", "Planned Parenthood v. Casey", "Lujan v. Defenders of Wildlife", "Texaco Inc. v. Dagher", "De Jonge v. Oregon", "Cantwell v. State of Connecticut", "Gertz v. Robert Welch, Inc.", "Brushaber v. Union Pacific Railroad Co.", "Crosby v. National Foreign Trade Council", "Jacobellis v. Ohio", "Allen v. Wright", "Correctional Services Corp. v. Malesko", "Johnson v. M'Intosh", "Hylton v. United States", "Blanton v. City of North Las Vegas", "Farrington v. Tokushige", "McCollum v. Board of Education", "Hunt v. Cromartie", "District of Columbia Court of Appeals v. Feldman", "Wickard v. Filburn", "United Steelworkers v. Weber", "New Jersey v. T. L. O.", "Roberts v. United States Jaycees", "Skinner v. Oklahoma", "Brown v. Louisiana", "R.A.V. v. City of St. Paul", "Henderson v. United States", "Hans v. Louisiana", "Morse v. Frederick", "Cooley v. Board of Wardens", "Microsoft Corp. v. AT&T Corp.", "Alden v. Maine", "Adams v. Texas", "Bowers v. Hardwick", "Kawakita v. United States", "James v. United States", "Prize Cases", "Turner v. Safley", "New York Times Co. v. Tasini", "Florida v. Royer", "United States v. United States District Court", "Church of the Holy Trinity v. United States", "NLRB v. J. Weingarten, Inc.", "Toolson v. New York Yankees", "DeLima v. Bidwell", "Tenet v. Doe", "Railroad Commission v. Pullman Co.", "Roe v. Wade", "Easley v. Cromartie", "Almendarez-Torres v. United States", "Schenck v. United States", "Bob Jones University v. United States", "BMW of North America, Inc. v. Gore", "Dastar Corp. v. Twentieth Century Fox Film Corp.", "United States v. Williams", "Pollock v. Farmers' Loan & Trust Co.", "Dames & Moore v. Regan", "Crandall v. Nevada", "Gray v. Sanders", "U.S. Term Limits, Inc. v. Thornton", "Minersville School District v. Gobitis", "Yick Wo v. Hopkins", "Santa Clara County v. Southern Pacific Railroad Co.", "South Dakota v. Opperman", "Hepburn v. Griswold", "Ex parte Endo", "Will v. Michigan Department of State Police", "Schenck v. Pro-Choice Network of Western New York", "Barron v. Baltimore", "United States v. Shabani", "Shelley v. Kraemer", "Fitzpatrick v. Bitzer", "Whitman v. American Trucking Ass'ns, Inc.", "Ball v. United States", "Cutter v. Wilkinson", "Enmund v. Florida", "Old Chief v. United States", "United States v. International Boxing Club of New York", "Flagg Bros., Inc. v. Brooks", "Gonzales v. Raich", "Herring v. United States", "Ex parte Young", "FEC v. Akins", "Pace v. Alabama", "Beacon Theatres, Inc. v. Westover", "Schechter Poultry Corp. v. United States", "Addington v. Texas", "Vernonia School District 47J v. Acton", "Radovich v. National Football League", "Flint v. Stone Tracy Co.", "Lopez v. Gonzales", "United States v. Nixon", "California v. Acevedo", "Oregon Waste Systems, Inc. v. Department of Environmental Quality of Oregon", "Mertens v. Hewitt Associates", "Epperson v. Arkansas", "Graver Tank & Manufacturing Co. v. Linde Air Products Co.", "Taylor v. Mississippi", "Arizona v. Evans", "Carey v. Musladin", "Flood v. Kuhn", "Wilkerson v. Utah", "Baird v. State Bar of Arizona", "Gade v. National Solid Wastes Management Ass'n", "Massiah v. United States", "Ake v. Oklahoma", "2003 term per curiam opinions of the Supreme Court of the United States", "Federal Election Commission v. Wisconsin Right to Life, Inc.", "Ex parte Garland", "Stump v. Sparkman", "Grutter v. Bollinger", "Monell v. Department of Social Services of the City of New York", "United States v. Curtiss-Wright Export Corp.", "Largent v. Texas", "Powell v. McCormack", "Immigration and Naturalization Service v. Stevic", "United States v. Grubbs", "Totten v. United States", "Egbert v. Lippmann", "Reid v. Covert", "Chambers v. Florida", "Wisconsin v. Yoder", "Edwards v. South Carolina", "Batson v. Kentucky", "Williamson v. Lee Optical Co.", "Stuart v. Laird", "Pennsylvania Coal Co. v. Mahon", "Reno v. American Civil Liberties Union", "Loving v. Virginia", "Godfrey v. Georgia", "Poulos v. New Hampshire", "Cooper Industries, Inc. v. Leatherman Tool Group, Inc.", "Gregory v. Helvering", "International News Service v. Associated Press", "Randall v. Sorrell", "Lechmere, Inc. v. NLRB", "Maynard v. Cartwright", "Gratz v. Bollinger", "Baker v. Carr", "Abington School District v. Schempp", "Saint Francis College v. al-Khazraji", "Hanna v. Plumer", "Illinois Tool Works Inc. v. Independent Ink, Inc.", "Pinkerton v. United States", "Cox v. United States", "Strader v. Graham", "United States v. Booker", "New Negro Alliance v. Sanitary Grocery Co.", "Palko v. Connecticut", "National Treasury Employees Union v. Von Raab", "New York Times Co. v. United States", "Virginia v. Moore", "United States v. Robel", "United States v. Bhagat Singh Thind", "Buchanan v. Warley", "Buckley v. Valeo", "O'Connor v. Donaldson", "Stromberg v. California", "Meredith v. Jefferson County Board of Education", "Stanley v. Georgia", "United States v. Moreland", "Central Virginia Community College v. Katz", "United States v. Hudson", "Taylor v. Taintor", "Davis v. Washington", "Hamdan v. Rumsfeld", "Moyer v. Peabody", "Department of Transportation v. Public Citizen", "Meyer v. Nebraska", "Murdock v. Pennsylvania", "American Insurance Co. v. 356 Bales of Cotton", "Continental Paper Bag Co. v. Eastern Paper Bag Co.", "Griggs v. Duke Power Co.", "Fullilove v. Klutznick", "Willson v. Black-Bird Creek Marsh Co.", "Sicurella v. United States", "United States v. Schwimmer", "Diamond v. Chakrabarty", "Cantwell v. Connecticut", "Kyllo v. United States", "American Broadcasting Cos. v. Aereo, Inc.", "Prigg v. Pennsylvania", "Commissioner v. Glenshaw Glass Co.", "Oregon v. Bradshaw", "United States v. Students Challenging Regulatory Agency Procedures", "United States v. E. C. Knight Co.", "Adamson v. California", "Dickinson v. United States", "Harris v. Quinn", "Standard Oil Co. of New Jersey v. United States", "Calder v. Jones", "Carnival Cruise Lines, Inc. v. Shute", "Scott v. Harris", "Dolan v. City of Tigard", "Knowles v. Iowa", "Verizon Communications Inc. v. Law Offices of Curtis V. Trinko, LLP", "Muskrat v. United States", "Hein v. Freedom From Religion Foundation", "Sibbach v. Wilson & Co.", "Lochner v. New York", "Interstate Commerce Commission v. Cincinnati, New Orleans & Texas Pacific Railway Co.", "Fuentes v. Shevin", "United States v. Fordice", "Coppage v. Kansas", "Immigration and Naturalization Service v. Elias-Zacarias", "Agostini v. Felton", "Gonzales v. O Centro Espirita Beneficente Uniao do Vegetal", "In re Debs", "Maryland v. Craig", "KSR International Co. v. Teleflex Inc.", "United States v. Flores-Montano", "Texas v. Johnson", "Caterpillar, Inc. v. Lewis", "Hawaii Housing Authority v. Midkiff", "Tellabs, Inc. v. Makor Issues & Rights, Ltd.", "Parisi v. Davidson", "Brown v. Mississippi", "Washington v. Davis", "Exxon Corp. v. Governor of Maryland", "City of Richmond v. J.A. Croson Co.", "Gonzales v. Oregon", "Universal Camera Corp. v. NLRB", "Afroyim v. Rusk", "Curtis Publishing Co. v. Butts", "Ex parte Quirin", "Louisville & Nashville Railroad Co. v. Mottley", "Florida Bar v. Went For It, Inc.", "Hazelwood v. Kuhlmeier", "Ex parte Madrazzo", "World-Wide Volkswagen Corp. v. Woodson", "Kidd v. Pearson", "Busey v. District of Columbia", "MedImmune, Inc. v. Genentech, Inc.", "Runyon v. McCrary", "Brendlin v. California", "Ford v. Wainwright", "Oregon v. Guzek", "Abrams v. United States", "Immigration and Naturalization Service v. Abudu", "Lawrence v. Texas", "Bunting v. Oregon", "Massachusetts v. Environmental Protection Agency", "Swidler & Berlin v. United States", "DeFunis v. Odegaard", "Osborn v. Bank of the United States", "Guaranty Trust Co. v. York", "Payton v. New York", "Wabash, St. Louis & Pacific Railway Co. v. Illinois", "The Paquete Habana", "Eldred v. Ashcroft", "Board of Airport Commissioners of Los Angeles v. Jews for Jesus, Inc.", "Northwestern National Life Insurance Co. v. Riggs", "Gibbons v. Ogden", "Nixon v. United States", "S. D. Warren Co. v. Maine Board of Environmental Protection", "Sipuel v. Board of Regents of the University of Oklahoma", "Central Laborers' Pension Fund v. Heinz", "Berman v. Parker", "Wyoming v. Colorado", "Hurtado v. California", "Virginia v. Black", "Braunfeld v. Brown", "Bivens v. Six Unknown Named Agents", "Island Trees School District v. Pico", "Morissette v. United States", "Rosenberger v. University of Virginia", "Furman v. Georgia", "Bernal v. Fainter", "Immigration and Naturalization Service v. Doherty", "Strauder v. West Virginia", "Wesberry v. Sanders", "Burlington Northern & Santa Fe Railway Co. v. White", "City of Mobile v. Bolden", "Coleman v. Miller", "United States v. Matlock", "Moore v. Dempsey", "Grosjean v. American Press Co.", "Peretz v. United States", "Walton v. Arizona", "Holmes v. South Carolina", "City of Philadelphia v. New Jersey", "Bates v. City of Little Rock", "Northern Pipeline Construction Co. v. Marathon Pipe Line Co.", "Philip Morris USA Inc. v. Williams", "United States v. Sioux Nation of Indians", "Estelle v. Smith", "Engel v. Vitale", "Sale v. Haitian Centers Council, Inc.", "Manual Enterprises, Inc. v. Day", "Frontiero v. Richardson", "Zelman v. Simmons-Harris", "Falbo v. United States", "TrafFix Devices, Inc. v. Marketing Displays, Inc.", "Egelhoff v. Egelhoff", "Qualitex Co. v. Jacobson Products Co.", "United States v. Wong Kim Ark", "Byrd v. Blue Ridge Rural Electric Cooperative, Inc.", "United States v. Southwestern Cable Co.", "United States v. Carolene Products Co.", "Burford v. Sun Oil Co.", "Silver v. New York Stock Exchange", "Taft v. Bowers", "Solem v. Helm", "Leegin Creative Leather Products, Inc. v. PSKS, Inc.", "United States v. Kirby Lumber Co.", "Estep v. United States", "Missouri ex rel. Gaines v. Canada", "Boy Scouts of America v. Dale", "Burwell v. Hobby Lobby", "Mississippi University for Women v. Hogan", "Good News Club v. Milford Central School", "Lynch v. Donnelly", "Oyama v. California", "United Public Workers v. Mitchell", "Hodgson v. Minnesota", "England v. Louisiana State Board of Medical Examiners", "J.E.B. v. Alabama ex rel. T.B.", "Panama Refining Co. v. Ryan", "Yates v. United States", "Munn v. Illinois", "Jones v. United States", "Swann v. Charlotte-Mecklenburg Board of Education", "Town of Castle Rock v. Gonzales", "Street v. New York", "Alexander v. Sandoval", "Arthur Andersen LLP v. United States", "Gomillion v. Lightfoot", "California v. Greenwood", "Morrison v. Olson", "Martin v. Hunter's Lessee", "Taylor v. United States", "Yasui v. United States", "Feres v. United States", "Savana Redding v. Safford Unified School District #1", "Muller v. Oregon", "Northern Insurance Co. of New York v. Chatham County", "Atkins v. Virginia", "Republic of Austria v. Altmann", "Gall v. United States", "Cohen v. Cowles Media Co.", "Chauffeurs, Teamsters, & Helpers Local No. 391 v. Terry", "Arbaugh v. Y & H Corp.", "Connecticut General Life Insurance Co. v. Johnson", "Hernandez v. Texas", "Merck KGaA v. Integra Lifesciences I, Ltd.", "Smith v. Doe", "Upjohn Co. v. United States", "Giles v. Harris", "Anderson v. Mt. Clemens Pottery Co.", "Crawford v. Marion County Election Board", "Wisconsin v. Mitchell", "Hague v. Committee for Industrial Organization", "Clinton v. City of New York", "Strickland v. Washington", "Zorach v. Clauson", "Bowers v. Kerbaugh-Empire Co.", "United States v. Shipp", "Rumsfeld v. Padilla", "United States v. Bajakajian", "United States v. Ninety-Five Barrels Alleged Apple Cider Vinegar", "United States v. Reynolds", "Griswold v. Connecticut", "Nixon v. Herndon", "United States v. Kirby", "United States v. Klein", "Griffith v. Kentucky", "Eli Lilly & Co. v. Medtronic, Inc.", "Jamison v. Texas", "Red Lion Broadcasting Co. v. FCC", "Wallace v. Jaffree", "Huddleston v. United States", "Whitney v. California", "Oncale v. Sundowner Offshore Services, Inc.", "Festo Corp. v. Shoketsu Kinzoku Kogyo Kabushiki Co.", "Burger King Corp. v. Rudzewicz", "Ex parte Bollman", "United States v. Leon", "Dartmouth College v. Woodward", "Sturges v. Crowninshield", "First National Bank of Boston v. Bellotti", "Beck v. Alabama", "Schneider v. State (of New Jersey)", "Cohen v. California", "Terry v. Ohio", "Cheek v. United States", "Hunt v. Washington State Apple Advertising Commission", "Pocket Veto Case", "Chimel v. California", "Ozawa v. United States", "United States v. X-Citement Video, Inc.", "Ogden v. Saunders", "Fox Film Corp. v. Muller", "Hirabayashi v. United States", "Hart v. United States", "Celotex Corp. v. Catrett", "Wheeling Steel Corp. v. Glander", "Puerto Rico v. Branstad", "Maine v. Taylor", "Brady v. Maryland", "Missouri v. Holland", "United States v. Sprague", "Cruzan v. Director, Missouri Department of Health", "Wallace v. Cutten", "Younger v. Harris", "United States v. Montoya De Hernandez", "Bi-Metallic Investment Co. v. State Board of Equalization", "Worcester v. Georgia", "NLRB v. Jones & Laughlin Steel Corp.", "Forsyth County v. Nationalist Movement", "Roper v. Simmons", "National Socialist Party of America v. Village of Skokie", "United States v. Harriss", "Van Orden v. Perry", "Shapiro v. Thompson", "McNeil v. Wisconsin", "United States v. Stewart", "Katz v. United States", "National Cable & Telecommunications Ass'n v. Brand X Internet Services", "Insular Cases", "Cooper v. Aaron", "Bouie v. City of Columbia", "Bethel School District v. Fraser", "SEC v. W. J. Howey Co.", "Helicopteros Nacionales de Colombia, S. A. v. Hall", "Memoirs v. Massachusetts", "Terminiello v. City of Chicago", "Kimbrough v. United States", "DaimlerChrysler Corp. v. Cuno", "Brown v. Hotel and Restaurant Employees", "Eisner v. Macomber", "Penn Central Transportation Co. v. New York City", "New State Ice Co. v. Liebmann", "Poe v. Ullman", "Gilbert v. California", "General Motors streetcar conspiracy", "Bradwell v. Illinois", "Rasul v. Bush", "Scott v. Illinois", "International Shoe Co. v. Washington", "Hudson v. Michigan", "Georgia v. Stanton", "Bronston v. United States", "American Well Works Co. v. Layne & Bowler Co.", "Consolidated Edison Co. v. Public Service Commission", "Myers v. United States", "Gitlow v. New York", "Miller v. Johnson", "Oregon v. Mitchell", "Keystone Bituminous Coal Ass'n v. DeBenedictis", "Buck v. Bell", "Nixon v. Fitzgerald", "Clinton v. Jones", "Debs v. United States", "Baker v. Selden", "Califano v. Yamasaki", "FCC v. Pacifica Foundation", "Nebbia v. New York", "Dean Milk Co. v. City of Madison", "California Democratic Party v. Jones", "McLaurin v. Oklahoma State Regents", "Ingraham v. Wright", "Late Corp. of the Church of Jesus Christ of Latter-Day Saints v. United States", "Tennessee Valley Authority v. Hill", "San Antonio Independent School District v. Rodriguez", "Oliphant v. Suquamish Indian Tribe", "City of Indianapolis v. Edmond", "United States v. Harris", "Samson v. California", "Bobbs-Merrill Co. v. Straus", "Michigan v. Long", "C&A Carbone, Inc. v. Town of Clarkstown", "Stewart v. Abend", "Rostker v. Goldberg", "McCleskey v. Kemp", "Village of Euclid v. Ambler Realty Co.", "Gonzales v. Carhart", "Branzburg v. Hayes", "Mathews v. Eldridge", "Blakely v. Washington"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-949", "prediction": ["# Reasoning Path:\nPennsylvania State University -> location.location.containedby -> Pennsylvania -> location.location.containedby -> United States of America\n# Answer:\nPennsylvania", "# Reasoning Path:\nPennsylvania State University -> location.location.containedby -> University Park -> location.location.containedby -> Pennsylvania\n# Answer:\nUniversity Park", "# Reasoning Path:\nPennsylvania State University -> location.location.containedby -> University Park -> location.location.containedby -> United States of America\n# Answer:\nUniversity Park", "# Reasoning Path:\nPennsylvania State University -> education.educational_institution.campuses -> Penn State University Creamery -> location.location.geolocation -> m.0z8b_gr\n# Answer:\nPenn State University Creamery", "# Reasoning Path:\nPennsylvania State University -> location.location.containedby -> Pennsylvania -> base.aareas.schema.administrative_area.administrative_children -> Harrisburg\n# Answer:\nPennsylvania", "# Reasoning Path:\nPennsylvania State University -> location.location.containedby -> Pennsylvania -> location.location.nearby_airports -> University Park Airport\n# Answer:\nPennsylvania", "# Reasoning Path:\nPennsylvania State University -> location.location.containedby -> State College -> location.location.containedby -> Pennsylvania\n# Answer:\nState College", "# Reasoning Path:\nPennsylvania State University -> education.educational_institution.campuses -> Penn State University Creamery -> education.educational_institution_campus.educational_institution -> Penn State College of Agricultural Sciences\n# Answer:\nPenn State University Creamery", "# Reasoning Path:\nPennsylvania State University -> location.location.containedby -> Pennsylvania -> base.aareas.schema.administrative_area.administrative_children -> Hazleton\n# Answer:\nPennsylvania", "# Reasoning Path:\nPennsylvania State University -> location.location.containedby -> University Park -> location.location.containedby -> Centre County\n# Answer:\nUniversity Park"], "ground_truth": ["University Park"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 3: 1844-1846\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 5: 1851-1855\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin -> book.author.works_written -> The Autobiography of Charles Darwin\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Anne Darwin -> people.person.parents -> Emma Darwin\n# Answer:\nAnne Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nEngland"], "ground_truth": ["Cartas de Darwin 18251859", "To the members of the Down Friendly Club", "Les moyens d'expression chez les animaux", "The action of carbonate of ammonia on the roots of certain plants", "The Life and Letters of Charles Darwin Volume 1", "The Correspondence of Charles Darwin, Volume 12: 1864", "Notebooks on transmutation of species", "Part I: Contributions to the Theory of Natural Selection / Part II", "Darwin Darwin", "Charles Darwin's marginalia", "The Correspondence of Charles Darwin, Volume 8: 1860", "Darwin's notebooks on transmutation of species", "The geology of the voyage of H.M.S. Beagle", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Evolution by natural selection", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "vari\u00eberen der huisdieren en cultuurplanten", "The Correspondence of Charles Darwin, Volume 11: 1863", "Charles Darwin", "The Different Forms of Flowers on Plants of the Same Species", "The Correspondence of Charles Darwin, Volume 16: 1868", "Tesakneri tsagume\u030c", "The\u0301orie de l'e\u0301volution", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The Correspondence of Charles Darwin, Volume 15: 1867", "Geological Observations on South America", "On evolution", "Diary of the voyage of H.M.S. Beagle", "Darwin Compendium", "The Voyage of the Beagle", "Leben und Briefe von Charles Darwin", "The Orgin of Species", "Darwin", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "La vie et la correspondance de Charles Darwin", "Questions about the breeding of animals", "The Life of Erasmus Darwin", "Darwinism stated by Darwin himself", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "Resa kring jorden", "Gesammelte kleinere Schriften", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "From Darwin's unpublished notebooks", "Del Plata a Tierra del Fuego", "Evolution and natural selection", "On the origin of species by means of natural selection", "On the tendency of species to form varieties", "Die fundamente zur entstehung der arten", "Darwin's journal", "The Correspondence of Charles Darwin, Volume 17: 1869", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The Darwin Reader First Edition", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "The portable Darwin", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "The Formation of Vegetable Mould through the Action of Worms", "genese\u014ds t\u014dn eid\u014dn", "The Correspondence of Charles Darwin, Volume 10: 1862", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "The Correspondence of Charles Darwin, Volume 14: 1866", "The Descent of Man, and Selection in Relation to Sex", "Reise um die Welt 1831 - 36", "The living thoughts of Darwin", "Notes on the fertilization of orchids", "Rejse om jorden", "Beagle letters", "A student's introduction to Charles Darwin", "Charles Darwin's natural selection", "Darwin and Henslow", "The Darwin Reader Second Edition", "Fertilisation of Orchids", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Diario del Viaje de Un Naturalista Alrededor", "Insectivorous Plants", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "Proiskhozhdenie vidov", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "The Correspondence of Charles Darwin, Volume 9: 1861", "Darwin for Today", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Darwin's insects", "La facult\u00e9 motrice dans les plantes", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Structure and Distribution of Coral Reefs", "The voyage of Charles Darwin", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "The Power of Movement in Plants", "The principal works", "Origins", "Works", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "The Autobiography of Charles Darwin", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "A Darwin Selection", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The foundations of the Origin of species", "Darwin on humus and the earthworm", "Volcanic Islands", "Motsa ha-minim", "Darwin's Ornithological notes", "The Correspondence of Charles Darwin, Volume 13: 1865", "Kleinere geologische Abhandlungen", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "Charles Darwin's letters", "Metaphysics, Materialism, & the evolution of mind", "Geological Observations on the Volcanic Islands", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Reise eines Naturforschers um die Welt", "Darwin from Insectivorous Plants to Worms", "From so simple a beginning", "ontstaan der soorten door natuurlijke teeltkeus", "More Letters of Charles Darwin", "South American Geology", "Die geschlechtliche Zuchtwahl", "The Life and Letters of Charles Darwin Volume 2", "Darwin en Patagonia", "On Natural Selection", "On a remarkable bar of sandstone off Pernambuco", "H.M.S. Beagle in South America", "monograph on the sub-class Cirripedia", "Darwin-Wallace", "El Origin De Las Especies", "Evolutionary Writings: Including the Autobiographies", "Wu zhong qi yuan", "On the Movements and Habits of Climbing Plants", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "Evolution", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "The collected papers of Charles Darwin", "The Correspondence of Charles Darwin, Volume 18: 1870", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "red notebook of Charles Darwin", "The Expression of the Emotions in Man and Animals", "Voyage d'un naturaliste autour du monde", "Charles Darwin on the routes of male humble bees", "Memorias y epistolario i\u0301ntimo", "The Essential Darwin", "Human nature, Darwin's view", "The education of Darwin", "Les mouvements et les habitudes des plantes grimpantes", "The Variation of Animals and Plants under Domestication", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Het uitdrukken van emoties bij mens en dier", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Opsht\u0323amung fun menshen", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general"], "ans_acc": 0.05228758169934641, "ans_hit": 1, "ans_f1": 0.025586353944562903, "ans_precission": 0.6, "ans_recall": 0.013071895424836602, "path_f1": 0.3636363636363636, "path_precision": 1.0, "path_recall": 0.2222222222222222, "path_ans_f1": 0.09937888198757763, "path_ans_precision": 1.0, "path_ans_recall": 0.05228758169934641}
{"id": "WebQTest-952", "prediction": ["# Reasoning Path:\nAlaska -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> location.administrative_division.country -> United States of America -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> location.administrative_division.country -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.location.containedby -> Americas\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.location.partially_contains -> Alsek River\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.location.partially_contains -> Fortymile River\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> location.administrative_division.country -> United States of America -> location.location.containedby -> Americas\n# Answer:\nUnited States of America"], "ground_truth": ["United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-953", "prediction": ["# Reasoning Path:\nKansas City -> location.location.containedby -> Missouri -> location.location.containedby -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nKansas City -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nKansas City -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Platte County -> location.location.containedby -> Missouri\n# Answer:\nPlatte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Kansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nKansas City -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> location.location.containedby -> United States of America\n# Answer:\nKansas", "# Reasoning Path:\nKansas City -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nKansas City -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Kansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nKansas City -> location.location.containedby -> Missouri -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nMissouri"], "ground_truth": ["Platte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-954", "prediction": ["# Reasoning Path:\nPink Floyd -> music.musical_group.member -> m.02hz8bl -> music.group_membership.member -> Syd Barrett\n# Answer:\nSyd Barrett", "# Reasoning Path:\nPink Floyd -> music.musical_group.member -> m.0z0nj30 -> music.group_membership.member -> Richard Wright\n# Answer:\nRichard Wright", "# Reasoning Path:\nPink Floyd -> music.musical_group.member -> m.02hz8bl -> freebase.valuenotation.is_reviewed -> Member\n# Answer:\nMember", "# Reasoning Path:\nPink Floyd -> music.musical_group.member -> g.11b6bd_cl2\n# Answer:\nmusic.musical_group.member", "# Reasoning Path:\nPink Floyd -> music.musical_group.member -> m.0z0nj30 -> freebase.valuenotation.is_reviewed -> Member\n# Answer:\nMember", "# Reasoning Path:\nPink Floyd -> music.artist.label -> Tower Records -> business.employer.employees -> m.02xhhld\n# Answer:\nTower Records", "# Reasoning Path:\nPink Floyd -> music.artist.label -> Tower Records -> music.record_label.artist -> Tom Jones\n# Answer:\nTower Records", "# Reasoning Path:\nPink Floyd -> music.artist.label -> Tower Records -> common.topic.notable_for -> g.1256dg21d\n# Answer:\nTower Records", "# Reasoning Path:\nPink Floyd -> music.artist.label -> Columbia -> freebase.valuenotation.is_reviewed -> Headquarters\n# Answer:\nColumbia", "# Reasoning Path:\nPink Floyd -> music.musical_group.member -> m.02hz8bl -> freebase.valuenotation.is_reviewed -> Group\n# Answer:\nGroup"], "ground_truth": ["Psychedelic Session-compilation album"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-955", "prediction": ["# Reasoning Path:\nGranville Woods -> people.person.spouse_s -> m.0j6f6k_ -> people.marriage.spouse -> Loretta Woods\n# Answer:\nLoretta Woods", "# Reasoning Path:\nGranville Woods -> people.person.spouse_s -> m.0j6f6k_ -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nGranville Woods -> people.person.spouse_s -> m.0j6f6k_ -> freebase.valuenotation.has_value -> From\n# Answer:\nFrom", "# Reasoning Path:\nGranville Woods -> people.person.spouse_s -> m.0j6f6k_ -> freebase.valuenotation.has_value -> To\n# Answer:\nTo", "# Reasoning Path:\nGranville Woods -> people.deceased_person.place_of_death -> New York City -> travel.travel_destination.tourist_attractions -> A.I.R. Gallery\n# Answer:\nNew York City", "# Reasoning Path:\nGranville Woods -> law.inventor.inventions -> Third rail -> base.argumentmaps.original_idea.innovator -> Bion J. Arnold\n# Answer:\nThird rail", "# Reasoning Path:\nGranville Woods -> people.deceased_person.place_of_death -> New York City -> travel.travel_destination.tourist_attractions -> American Folk Art Museum\n# Answer:\nNew York City", "# Reasoning Path:\nGranville Woods -> people.deceased_person.place_of_death -> New York City -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Songs of My People\n# Answer:\nNew York City", "# Reasoning Path:\nGranville Woods -> law.inventor.inventions -> Third rail -> common.topic.image -> WMATA third rail at West Falls Church\n# Answer:\nThird rail", "# Reasoning Path:\nGranville Woods -> people.deceased_person.place_of_death -> New York City -> travel.travel_destination.tourist_attractions -> American Museum of Natural History\n# Answer:\nNew York City"], "ground_truth": ["Loretta Woods"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-957", "prediction": ["# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> Sioux language -> language.human_language.main_country -> United States of America\n# Answer:\nSioux language", "# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> Sioux language -> language.human_language.language_family -> Siouan languages\n# Answer:\nSioux language", "# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> Sioux language -> common.topic.article -> m.0565zd\n# Answer:\nSioux language", "# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> Sioux language -> language.human_language.language_family -> Western Siouan languages\n# Answer:\nSioux language", "# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> English Language -> fictional_universe.fictional_language.where_spoken -> Heroes in Hell fictional universe\n# Answer:\nEnglish Language", "# Reasoning Path:\nSioux -> people.ethnicity.includes_groups -> Brul\u00e9 -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nBrul\u00e9", "# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> Sioux language -> language.human_language.language_family -> g.120v12vq\n# Answer:\nSioux language", "# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> English Language -> organization.organization_sector.organizations_in_this_sector -> dragon-class.com\n# Answer:\nEnglish Language", "# Reasoning Path:\nSioux -> people.ethnicity.included_in_group -> Native Americans in the United States -> people.ethnicity.languages_spoken -> French\n# Answer:\nNative Americans in the United States", "# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language"], "ground_truth": ["Sioux language", "French", "English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7272727272727272, "ans_precission": 0.8, "ans_recall": 0.6666666666666666, "path_f1": 0.7272727272727272, "path_precision": 0.8, "path_recall": 0.6666666666666666, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-958", "prediction": ["# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> base.culturalevent.event.entity_involved -> Maximiliano Hern\u00e1ndez Mart\u00ednez\n# Answer:\n1932 Salvadoran peasant massacre", "# Reasoning Path:\nEl Salvador -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nEl Salvador -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> United States of America\n# Answer:\nPresidential system", "# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> base.culturalevent.event.entity_involved -> Osm\u00edn Aguirre y Salinas\n# Answer:\n1932 Salvadoran peasant massacre", "# Reasoning Path:\nEl Salvador -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> common.topic.notable_for -> g.125btm68w\n# Answer:\n1932 Salvadoran peasant massacre", "# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> base.culturalevent.event.entity_involved -> Armed Forces of El Salvador\n# Answer:\n1932 Salvadoran peasant massacre", "# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> common.topic.article -> m.0bjpf4\n# Answer:\n1932 Salvadoran peasant massacre", "# Reasoning Path:\nEl Salvador -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nEl Salvador -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Colombia\n# Answer:\nConstitutional republic"], "ground_truth": ["Norman Quijano", "Mario Montoya", "Mauricio Alonso Rodr\u00edguez", "Saturnino Osorio", "Rub\u00e9n Zamora", "Julio Adalberto Rivera Carballo", "Pedro Chavarria", "Ernesto Aparicio", "Edwin Ramos", "Robert Renderos", "Doroteo Vasconcelos", "Miguel Angel Deras", "Elena Diaz", "Jose B. Gonzalez", "Malin Arvidsson", "Prudencia Ayala", "Jos\u00e9 Castellanos Contreras", "Jos\u00e9 Inocencio Alas", "Roberto Carlos Martinez", "Damaris Qu\u00e9les", "Xenia Estrada", "Erwin McManus", "Jos\u00e9 Manfredi Portillo", "F\u00e9lix Pineda", "Laura Molina", "Patricia Chica", "Francisco Due\u00f1as", "Am\u00e9rico Gonz\u00e1lez", "Santiago \\\"Jimmy\\\" Mellado", "Ra\u00fal Cicero", "Wilfredo Iraheta", "Jose Solis", "Francisco Gavidia", "\u00c1ngel Orellana", "Eva Dimas", "Milton Palacios", "Guillermo Garc\u00eda", "William L\u00f3pez", "Steve Montenegro", "Emilio Guardado", "DJ Quest", "Rafael Campo", "Sarah Ramos", "William Armando", "Tom\u00e1s Medina", "Enrique \u00c1lvarez C\u00f3rdova", "Isa\u00edas Choto", "Gerardo Barrios", "Arturo Rivera y Damas", "Armando Chac\u00f3n", "Francisco Funes", "Arturo Armando Molina", "Mario Wilfredo Contreras", "Salvador Castaneda Castro", "Jaime Portillo", "Diego Vel\u00e1zquez", "Mauricio Alvarenga", "Alexander M\u00e9ndoza", "Eduardo Hern\u00e1ndez", "Gualberto Fern\u00e1ndez", "Papa A.P.", "Jorge Rivera", "Rutilio Grande", "Ruben Cedillos", "Juan Ram\u00f3n S\u00e1nchez", "Rafael Menj\u00edvar Ochoa", "Pedro Jos\u00e9 Escal\u00f3n", "Carlos Linares", "\u00d3scar Antonio Ulloa", "Ana Sol Gutierrez", "Rene Moran", "Victor Lopez", "Bobby Rivas", "Nicolas F. Shi", "Bernard Lewinsky", "Claudia Lars", "g.11b8058v7j", "Elmer Acevedo", "Ricardo L\u00f3pez Tenorio", "Francisco Men\u00e9ndez", "Victor Manuel Ochoa", "Paula Heredia", "Jorge Mel\u00e9ndez", "Johnny Lopez", "Juan Rafael Bustillo", "Mauricio Alfaro", "William Renderos Iraheta", "Marlon Menj\u00edvar", "Carlos Barrios", "V\u00edctor Ram\u00edrez", "Genaro Serme\u00f1o", "Jorge B\u00facaro", "Consuelo de Saint Exup\u00e9ry", "Andr\u00e9s Eduardo Men\u00e9ndez", "Selvin Gonz\u00e1lez", "Keoki", "Roberto Rivas", "Manuel Enrique Araujo", "Miguel Cruz", "Alexander Campos", "Joel Aguilar", "Richard Oriani", "Fausto Omar V\u00e1squez", "Jos\u00e9 Mar\u00eda Ca\u00f1as", "Jos\u00e9 Francisco Valiente", "Takeshi Fujiwara", "Alfredo Ruano", "Jose Orlando Martinez", "Eduardo \\\"Volkswagen\\\" Hern\u00e1ndez", "Melvin Barrera", "Pedro Geoffroy Rivas", "Camilo Minero", "Ana Maria de Martinez", "Francisca Gonz\u00e1lez", "Jos\u00e9 Luis Rugamas", "Miguel Ca\u00f1izalez"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-959", "prediction": ["# Reasoning Path:\nBrett Gardner -> baseball.baseball_player.batting_stats -> m.06qf764 -> baseball.batting_statistics.team -> New York Yankees\n# Answer:\nNew York Yankees", "# Reasoning Path:\nBrett Gardner -> baseball.baseball_player.batting_stats -> m.06qf764 -> baseball.batting_statistics.season -> 2008 Major League Baseball season\n# Answer:\n2008 Major League Baseball season", "# Reasoning Path:\nBrett Gardner -> baseball.baseball_player.lifetime_batting_statistics -> m.09kt6v9 -> baseball.lifetime_batting_statistics.starting_season -> 2008 Major League Baseball season\n# Answer:\n2008 Major League Baseball season", "# Reasoning Path:\nBrett Gardner -> baseball.baseball_player.lifetime_batting_statistics -> m.09kt6v9 -> baseball.lifetime_batting_statistics.last_statistics_season -> 2009 Major League Baseball season\n# Answer:\n2009 Major League Baseball season", "# Reasoning Path:\nBrett Gardner -> people.person.places_lived -> m.03pk8j5 -> people.place_lived.location -> South Carolina\n# Answer:\nSouth Carolina", "# Reasoning Path:\nBrett Gardner -> people.person.places_lived -> m.0wk30xl -> people.place_lived.location -> Holly Hill\n# Answer:\nHolly Hill"], "ground_truth": ["New York Yankees"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 0.16666666666666666, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.16666666666666666, "path_recall": 0.3333333333333333, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> location.administrative_division.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.0gmjl4l -> tv.tv_guest_role.episodes_appeared_in -> Day of the Moon (2)\n# Answer:\nDay of the Moon (2)", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.film -> End of the Road: How Money Became Worthless\n# Answer:\nEnd of the Road: How Money Became Worthless", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15 -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15 -> film.personal_film_appearance.film -> Sicko\n# Answer:\nSicko", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglb -> tv.tv_guest_role.episodes_appeared_in -> The American Film Institute Salute to James Cagney\n# Answer:\nThe American Film Institute Salute to James Cagney", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.0_0cs2_ -> film.personal_film_appearance.film -> Our Nixon\n# Answer:\nOur Nixon", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsgl5 -> tv.tv_guest_role.episodes_appeared_in -> The Future of the GOP\n# Answer:\nThe Future of the GOP"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-960", "prediction": ["# Reasoning Path:\nThe Office -> award.award_winning_work.awards_won -> m.0z86k10 -> award.award_honor.award_winner -> Rainn Wilson\n# Answer:\nRainn Wilson", "# Reasoning Path:\nThe Office -> tv.tv_series_season.regular_cast -> m.02l00g_ -> tv.regular_tv_appearance.actor -> Rainn Wilson\n# Answer:\nRainn Wilson", "# Reasoning Path:\nThe Office -> award.award_winning_work.awards_won -> m.0nbwfjm -> award.award_honor.award_winner -> Rainn Wilson\n# Answer:\nRainn Wilson", "# Reasoning Path:\nThe Office -> tv.tv_series_season.regular_cast -> m.02l00g_ -> tv.regular_tv_appearance.character -> Dwight Schrute\n# Answer:\nDwight Schrute", "# Reasoning Path:\nThe Office -> tv.tv_series_season.regular_cast -> m.021y6jp -> tv.regular_tv_appearance.actor -> Steve Carell\n# Answer:\nSteve Carell", "# Reasoning Path:\nThe Office -> award.award_winning_work.awards_won -> m.0z86k10 -> award.award_honor.award_winner -> Angela Kinsey\n# Answer:\nAngela Kinsey", "# Reasoning Path:\nThe Office -> award.award_winning_work.awards_won -> m.0z86k10 -> award.award_honor.award -> TV Land Future Classic Award\n# Answer:\nTV Land Future Classic Award", "# Reasoning Path:\nThe Office -> tv.tv_series_season.episodes -> A Benihana Christmas\n# Answer:\nA Benihana Christmas", "# Reasoning Path:\nThe Office -> tv.tv_series_season.regular_cast -> m.02l00g_ -> tv.regular_tv_appearance.seasons -> The Office - Season 9\n# Answer:\nThe Office - Season 9", "# Reasoning Path:\nThe Office -> tv.tv_series_season.regular_cast -> m.02l00hg -> tv.regular_tv_appearance.actor -> Jenna Fischer\n# Answer:\nJenna Fischer"], "ground_truth": ["Rainn Wilson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-962", "prediction": ["# Reasoning Path:\nSan Diego -> location.location.containedby -> San Diego County -> location.location.containedby -> California\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego -> location.location.containedby -> San Diego County -> location.location.containedby -> United States of America\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego -> location.hud_county_place.county -> San Diego County -> location.location.containedby -> California\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego -> location.hud_county_place.county -> San Diego County -> location.location.containedby -> United States of America\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego -> location.location.containedby -> San Diego County -> location.us_county.hud_county_place -> San Diego County / County Place 0607399999\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego -> location.location.containedby -> United States of America -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nSan Diego -> location.location.containedby -> San Diego County -> location.us_county.hud_county_place -> Alpine\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego -> location.location.containedby -> San Diego County -> location.location.people_born_here ->  Jamie Bock\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego -> location.location.containedby -> 92101 -> location.location.containedby -> San Diego County\n# Answer:\n92101", "# Reasoning Path:\nSan Diego -> location.hud_county_place.county -> San Diego County -> location.location.people_born_here ->  Jamie Bock\n# Answer:\nSan Diego County"], "ground_truth": ["San Diego County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-963", "prediction": ["# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.010hdmz0 -> sports.sports_league_draft_pick.player -> Jarvis Landry\n# Answer:\nJarvis Landry", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.010q80zg -> sports.sports_league_draft_pick.player -> Dan Marino\n# Answer:\nDan Marino", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.05bj8vr -> sports.sports_league_draft_pick.player -> Jake Long\n# Answer:\nJake Long", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.010hdmz0 -> sports.sports_league_draft_pick.school -> Louisiana State University\n# Answer:\nLouisiana State University", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.010hdmz0 -> sports.sports_league_draft_pick.draft -> 2014 NFL draft\n# Answer:\n2014 NFL draft", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.010q80zg -> sports.sports_league_draft_pick.school -> University of Pittsburgh\n# Answer:\nUniversity of Pittsburgh", "# Reasoning Path:\nMiami Dolphins -> base.schemastaging.organization_extra.contact_webpages -> m.010g8ggp -> freebase.valuenotation.is_reviewed -> Language\n# Answer:\nLanguage", "# Reasoning Path:\nMiami Dolphins -> base.schemastaging.organization_extra.contact_webpages -> m.010g8ggp -> internet.localized_uri.language -> English\n# Answer:\nEnglish", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.010q80zg -> sports.sports_league_draft_pick.draft -> 1983 NFL draft\n# Answer:\n1983 NFL draft", "# Reasoning Path:\nMiami Dolphins -> base.schemastaging.organization_extra.contact_webpages -> m.010g8ggp -> internet.localized_uri.location -> United States of America\n# Answer:\nUnited States of America"], "ground_truth": ["Ryan Tannehill", "Matt Moore"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-965", "prediction": ["# Reasoning Path:\nAnne Frank -> symbols.name_source.namesakes -> Anne Frank House -> architecture.museum.address -> m.0w4zznj\n# Answer:\nAnne Frank House", "# Reasoning Path:\nAnne Frank -> symbols.name_source.namesakes -> Anne Frank House -> travel.tourist_attraction.near_travel_destination -> Amsterdam\n# Answer:\nAnne Frank House", "# Reasoning Path:\nAnne Frank -> symbols.name_source.namesakes -> 5535 Annefrank -> astronomy.astronomical_discovery.discovery_site -> Landessternwarte Heidelberg-K\u00f6nigstuhl\n# Answer:\n5535 Annefrank", "# Reasoning Path:\nAnne Frank -> people.person.education -> m.0bdw7f8 -> education.education.institution -> Montessori Lyceum Amsterdam\n# Answer:\nMontessori Lyceum Amsterdam", "# Reasoning Path:\nAnne Frank -> symbols.name_source.namesakes -> 5535 Annefrank -> common.topic.image -> Asteroid 5535 Annefrank\n# Answer:\n5535 Annefrank", "# Reasoning Path:\nAnne Frank -> symbols.name_source.namesakes -> Anne Frank Shoah Library -> base.jewlib.research_collection.online_catalogs -> AGGB - Union Catalog\n# Answer:\nAnne Frank Shoah Library", "# Reasoning Path:\nAnne Frank -> people.person.education -> m.0t5tz6k -> education.education.institution -> Jewish Lyceum\n# Answer:\nJewish Lyceum", "# Reasoning Path:\nAnne Frank -> people.person.places_lived -> m.0djzwlm -> people.place_lived.location -> Amsterdam\n# Answer:\nAmsterdam", "# Reasoning Path:\nAnne Frank -> symbols.name_source.namesakes -> 5535 Annefrank -> astronomy.celestial_object.category -> Asteroid\n# Answer:\n5535 Annefrank", "# Reasoning Path:\nAnne Frank -> people.person.education -> m.0bdw7f8 -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree"], "ground_truth": ["Montessori Lyceum Amsterdam", "Jewish Lyceum"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.28571428571428575, "path_precision": 0.2, "path_recall": 0.5, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-966", "prediction": ["# Reasoning Path:\nMonica -> celebrities.celebrity.sexual_relationships -> m.05nlg42 -> celebrities.romantic_relationship.relationship_type -> Engaged to\n# Answer:\nEngaged to", "# Reasoning Path:\nMonica -> celebrities.celebrity.sexual_relationships -> m.05nlg42 -> celebrities.romantic_relationship.celebrity -> Rocko\n# Answer:\nRocko", "# Reasoning Path:\nMonica -> people.person.spouse_s -> m.0h3gsqb -> people.marriage.spouse -> Shannon Brown\n# Answer:\nShannon Brown", "# Reasoning Path:\nMonica -> music.artist.origin -> Atlanta -> base.wikipedia_infobox.settlement.area_code -> Area code 678\n# Answer:\nAtlanta", "# Reasoning Path:\nMonica -> people.person.spouse_s -> m.0h3gsqb -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nMonica -> people.person.spouse_s -> m.0h3gsqb -> people.marriage.location_of_ceremony -> Los Angeles\n# Answer:\nLos Angeles", "# Reasoning Path:\nMonica -> music.artist.origin -> Atlanta -> common.topic.notable_types -> City/Town/Village\n# Answer:\nAtlanta", "# Reasoning Path:\nMonica -> music.artist.origin -> Atlanta -> base.wikipedia_infobox.settlement.area_code -> Area code 404\n# Answer:\nAtlanta", "# Reasoning Path:\nMonica -> music.artist.origin -> Decatur -> location.location.containedby -> United States of America\n# Answer:\nDecatur", "# Reasoning Path:\nMonica -> music.artist.origin -> Decatur -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nDecatur"], "ground_truth": ["Shannon Brown"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-967", "prediction": ["# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Suicide -> common.topic.notable_types -> Cause Of Death\n# Answer:\nSuicide", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.article -> m.02qnd1f\n# Answer:\nBallistic trauma", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nBallistic trauma", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.notable_for -> g.12558k_cd\n# Answer:\nBallistic trauma", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.resource -> Another Megan Fox letter -- and this one sings her praises!\n# Answer:\nAnother Megan Fox letter -- and this one sings her praises!", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wgn1j -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wgn1j -> common.webpage.resource -> Charles Manson: 40 years later, the movie about him you have to see\n# Answer:\nCharles Manson: 40 years later, the movie about him you have to see", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wqfb3 -> common.webpage.resource -> Erykah Badu teases 'New Amerykah Part Two' with trippy, funky clip\n# Answer:\nErykah Badu teases 'New Amerykah Part Two' with trippy, funky clip"], "ground_truth": ["Ballistic trauma", "Suicide"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-968", "prediction": ["# Reasoning Path:\nGermany -> location.country.languages_spoken -> Bavarian Language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nBavarian Language", "# Reasoning Path:\nGermany -> location.country.languages_spoken -> Bavarian Language -> language.language_dialect.language -> German Language\n# Answer:\nBavarian Language", "# Reasoning Path:\nGermany -> location.country.languages_spoken -> Bavarian Language -> language.human_language.main_country -> Austria\n# Answer:\nBavarian Language", "# Reasoning Path:\nGermany -> location.country.official_language -> German Language -> language.human_language.dialects -> Saxon, Upper Language\n# Answer:\nGerman Language", "# Reasoning Path:\nGermany -> location.country.official_language -> German Language -> language.human_language.countries_spoken_in -> Czech Republic\n# Answer:\nGerman Language", "# Reasoning Path:\nGermany -> location.country.official_language -> German Language -> language.human_language.dialects -> Bavarian Language\n# Answer:\nGerman Language", "# Reasoning Path:\nGermany -> location.country.official_language -> German Language -> language.human_language.region -> Europe\n# Answer:\nGerman Language", "# Reasoning Path:\nGermany -> location.country.languages_spoken -> German Language -> language.human_language.dialects -> Saxon, Upper Language\n# Answer:\nGerman Language", "# Reasoning Path:\nGermany -> location.country.languages_spoken -> Frisian, Northern Language -> language.human_language.region -> Europe\n# Answer:\nFrisian, Northern Language", "# Reasoning Path:\nGermany -> location.country.languages_spoken -> German Language -> language.human_language.countries_spoken_in -> Czech Republic\n# Answer:\nGerman Language"], "ground_truth": ["Low German", "German Language", "Bavarian Language", "Saxon, Upper Language", "Frisian, Northern Language", "Turoyo Language"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6153846153846154, "path_precision": 0.8, "path_recall": 0.5, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-969", "prediction": ["# Reasoning Path:\nDenver Broncos -> sports.sports_team.championships -> Super Bowl XXXIII -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XXXIII", "# Reasoning Path:\nDenver Broncos -> sports.sports_team.championships -> Super Bowl XXXIII -> base.schemastaging.context_name.pronunciation -> g.125_r6xhn\n# Answer:\nSuper Bowl XXXIII", "# Reasoning Path:\nDenver Broncos -> sports.sports_team.championships -> 1990 AFC Championship Game -> sports.sports_championship_event.championship -> AFC Championship Game\n# Answer:\n1990 AFC Championship Game", "# Reasoning Path:\nDenver Broncos -> sports.sports_team.championships -> 1998 AFC Championship Game -> sports.sports_championship_event.season -> 1997 NFL season\n# Answer:\n1998 AFC Championship Game", "# Reasoning Path:\nDenver Broncos -> sports.sports_team.championships -> 1990 AFC Championship Game -> common.topic.notable_for -> g.1q3sfq3cn\n# Answer:\n1990 AFC Championship Game", "# Reasoning Path:\nDenver Broncos -> american_football.football_team.home_games -> The Fumble -> time.event.people_involved -> Earnest Byner\n# Answer:\nThe Fumble", "# Reasoning Path:\nDenver Broncos -> sports.sports_team.venue -> m.0tm17sx -> sports.team_venue_relationship.venue -> Mile High Stadium\n# Answer:\nMile High Stadium", "# Reasoning Path:\nDenver Broncos -> sports.sports_team.championships -> 1998 AFC Championship Game -> time.event.instance_of_recurring_event -> AFC Championship Game\n# Answer:\n1998 AFC Championship Game", "# Reasoning Path:\nDenver Broncos -> american_football.football_team.home_games -> The Fumble -> common.topic.notable_types -> American football game\n# Answer:\nThe Fumble", "# Reasoning Path:\nDenver Broncos -> sports.sports_team.venue -> m.0tm17p3 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["Super Bowl XXXIII"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.appears_in_topic_gallery -> Grammy Legend Award\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.appears_in_topic_gallery -> Zubin Mehta\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.subjects -> Bleona\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.size -> m.0kjrkq\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.size -> m.02cljr8\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Music executive -> common.topic.article -> m.047rgq1\n# Answer:\nMusic executive", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Musician -> freebase.equivalent_topic.equivalent_type -> Musical Artist\n# Answer:\nMusician", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> Little Miss Sweetness -> music.recording.releases -> Gettin' Ready\n# Answer:\nLittle Miss Sweetness", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> I've Been Good to You -> music.composition.recordings -> I've Been Good to You (extended mix)\n# Answer:\nI've Been Good to You"], "ground_truth": ["It's Her Turn to Live", "Girlfriend", "If You Wanna Make Love", "You Go to My Head", "Going to a Go-Go", "I Hear The Children Singing", "The Track of My Tears", "Love Brought Us Here", "Tracks of My Tears", "The Family Song", "She's Only a Baby Herself", "I Praise & Worship You Father", "Let Me Be The Clock", "Jasmin", "The Tears of a Clown", "Time After Time", "You Made Me Feel Love", "Be Kind to the Growing Mind", "I Am I Am", "Away in the Manger / Coventry Carol", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "Please Don't Take Your Love (feat. Carlos Santana)", "Be Kind To The Growing Mind (with The Temptations)", "Can't Fight Love", "Quiet Storm (Groove Boutique remix)", "Love So Fine", "Sweet Harmony", "You Don't Know What It's Like", "Happy (Love Theme From Lady Sings the Blues)", "Sleepless Nights", "Winter Wonderland", "I Can\u2019t Stand to See You Cry (Commercial version)", "Tears of a Sweet Free Clown", "A Tattoo", "Our Love Is Here to Stay", "More Than You Know", "It's Fantastic", "Will You Still Love Me Tomorrow", "Why Do Happy Memories Hurt So Bad", "Little Girl, Little Girl", "A Child Is Waiting", "Just Another Kiss", "Gone Forever", "Just a Touch Away", "So Bad", "Same Old Love", "(It's The) Same Old Love", "Did You Know (Berry's Theme)", "Speak Low", "Let Your Light Shine On Me", "Theme From the Big Time", "Whatcha Gonna Do", "Tell Me Tomorrow, Part 1", "Ebony Eyes (Duet with Rick James)", "Girl I'm Standing There", "I've Made Love To You A Thousand Times", "Tracks of my Tears", "Food For Thought", "I'm in the Mood for Love", "Wedding Song", "Close Encounters of the First Kind", "The Road to Damascus", "What's Too Much", "Keep Me", "Come to Me Soon", "Going to a Gogo", "Time Flies", "He Can Fix Anything", "Why", "And I Love Her", "We Are The Warriors", "There Will Come a Day (I'm Gonna Happen to You)", "Some People Will Do Anything for Love", "I Can't Find", "Why Are You Running From My Love", "If You Wanna Make Love (Come 'round Here)", "Love Letters", "Tracks Of My Tears (Live)", "I've Got You Under My Skin", "Tell Me Tomorrow", "Photograph in My Mind", "I Care About Detroit", "Crusin", "You're the One for Me (feat. Joss Stone)", "Love Is The Light", "You've Really Got a Hold on Me", "You Are So Beautiful (feat. Dave Koz)", "Ooo Baby Baby (live)", "Don't Wanna Be Just Physical", "Open", "I'll Keep My Light In My Window", "I\u2019ve Got You Under My Skin", "Mickey's Monkey", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "Ooh Baby Baby", "The Tears Of A Clown", "I Love Your Face", "Unless You Do It Again", "Who's Sad", "Share It", "When Smokey Sings Tears Of A Clown", "One Time", "Everything You Touch", "Bad Girl", "Just My Soul Responding", "Virgin Man", "Standing On Jesus", "I Like Your Face", "One Heartbeat", "It's a Good Feeling", "Just To See Her Again", "Christmas Every Day", "Quiet Storm", "The Christmas Song", "If You Want My Love", "Little Girl Little Girl", "Gang Bangin'", "God Rest Ye Merry Gentlemen", "The Tracks of My Tears", "Blame It on Love", "Deck the Halls", "Be Who You Are", "Ooo Baby Baby", "Will You Love Me Tomorrow?", "My World", "I Am, I Am", "Satisfy You", "Mother's Son", "The Hurt's On You", "As You Do", "Night and Day", "Being With You", "Just to See Her", "Get Ready", "Season's Greetings from Smokey Robinson", "Hanging on by a Thread", "Just Like You", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "The Agony and the Ecstasy", "Train of Thought", "I've Made Love to You a Thousand Times", "Love Don't Give No Reason", "Cruisin", "Jingle Bells", "You're Just My Life (feat. India.Arie)", "The Tracks of My Tears (live)", "A Silent Partner in a Three-Way Love Affair", "Te Quiero Como Si No Hubiera Un Manana", "And I Don't Love You", "Quiet Storm (Groove Boutique Chill Jazz mix)", "Please Come Home for Christmas", "The Way You Do (The Things You Do)", "Quiet Storm (single version)", "Walk on By", "You've Really Go a Hold on Me", "Nearness of You", "In My Corner", "I Can't Get Enough", "I Love The Nearness Of You", "I Want You Back", "More Love", "Christmas Greeting", "Tears of a Clown", "Don't Know Why", "Come by Here (Kum Ba Ya)", "Going to a Go Go", "It's A Good Night", "Ebony Eyes", "If You Can Want", "The Love Between Me and My Kids", "Shoe Soul", "Be Careful What You Wish For (instrumental)", "Cruisin'", "That Place", "Noel", "Love Don' Give No Reason (12 Inch Club Mix)", "Double Good Everything", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "Driving Thru Life in the Fast Lane", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "It's Time to Stop Shoppin' Around", "Wishful Thinking", "Love Bath", "My Girl", "You Really Got a Hold on Me", "Be Careful What You Wish For", "The Tracks of My Heart", "Ain't That Peculiar", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "Will You Love Me Tomorrow", "You Are Forever", "Tell Me Tomorrow (12\\\" extended mix)", "Heavy On Pride (Light On Love)", "The Agony And The Ecstasy", "My Guy", "Skid Row", "Don't Play Another Love Song", "Fallin'", "Pops, We Love You", "Rewind", "Let Me Be the Clock", "Really Gonna Miss You", "Blame It On Love (Duet with Barbara Mitchell)", "Holly", "Rack Me Back", "I Can't Give You Anything but Love", "Tears Of A Clown", "I Know You by Heart", "Medley: Never My Love / Never Can Say Goodbye", "Vitamin U", "Yes It's You Lady", "Because of You It's the Best It's Ever Been", "Crusin'", "With Your Love Came", "There Will Come A Day ( I'm Gonna Happen To You )", "Never My Love / Never Can Say Goodbye", "You Take Me Away", "Coincidentally", "Baby That's Backatcha", "Ever Had A Dream", "We\u2019ve Come Too Far to End It Now", "No Time to Stop Believing", "And I Don't Love You (Larry Levan instrumental dub)", "Jesus Told Me To Love You", "Santa Claus is Coming to Town", "Everything for Christmas", "Love' n Life", "No\u00ebl", "Fulfill Your Need", "I Second That Emotion", "Melody Man", "I Have Prayed On It", "Baby Come Close", "Wanna Know My Mind", "When A Woman Cries", "Yester Love", "You Cannot Laugh Alone", "I'm Glad There Is You", "Aqui Con Tigo (Being With You)", "We've Saved the Best for Last", "Shop Around", "We've Saved The Best For Last (Kenny G with Smokey Robinson)", "Take Me Through The Night", "Hold on to Your Love", "Easy", "Christmas Everyday", "Fly Me to the Moon (In Other Words)", "It's Christmas Time", "Asleep on My Love", "Pops, We Love You (disco)", "Daylight & Darkness", "The Tracks Of My Tears", "I Second That Emotions", "Save Me", "Tea for Two", "Just Passing Through"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-970", "prediction": ["# Reasoning Path:\nJulius Caesar -> people.deceased_person.place_of_death -> Theatre of Pompey -> location.location.containedby -> Campus Martius\n# Answer:\nTheatre of Pompey", "# Reasoning Path:\nJulius Caesar -> people.deceased_person.place_of_death -> Theatre of Pompey -> common.topic.notable_for -> g.1256qj4g5\n# Answer:\nTheatre of Pompey", "# Reasoning Path:\nJulius Caesar -> people.deceased_person.place_of_death -> Theatre of Pompey -> common.topic.image -> Artist rendition of the front exterior of the Theatre of Pompey\n# Answer:\nTheatre of Pompey", "# Reasoning Path:\nJulius Caesar -> common.topic.article -> m.0lr8678\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nJulius Caesar -> film.film.starring -> m.010gs_d2 -> film.performance.character -> Cleopatra\n# Answer:\nCleopatra", "# Reasoning Path:\nJulius Caesar -> common.topic.article -> m.02667zw\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nJulius Caesar -> common.topic.article -> m.03y1_p\n# Answer:\ncommon.topic.article"], "ground_truth": ["Theatre of Pompey"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.6, "path_precision": 0.42857142857142855, "path_recall": 1.0, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-971", "prediction": ["# Reasoning Path:\nBessie Smith -> music.artist.genre -> Blues -> music.genre.subgenre -> Zydeco\n# Answer:\nBlues", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Blues -> book.book_subject.works -> Jazz\n# Answer:\nBlues", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Blues -> music.genre.subgenre -> Americana\n# Answer:\nBlues", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Blues -> book.book_subject.works -> The jazz life\n# Answer:\nBlues", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Jazz -> book.book_subject.works -> The jazz life\n# Answer:\nJazz", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Blues -> music.genre.subgenre -> Bluegrass\n# Answer:\nBlues", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Jazz -> book.book_subject.works -> 52nd Street\n# Answer:\nJazz", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Blues -> broadcast.genre.content -> WGBH Boston\n# Answer:\nBlues", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Blues -> book.book_subject.works -> 52nd Street\n# Answer:\nBlues", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Blues -> broadcast.genre.content -> Classic Soul Network\n# Answer:\nBlues"], "ground_truth": ["Jazz", "Blues"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.28571428571428575, "path_precision": 0.2, "path_recall": 0.5, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-972", "prediction": ["# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.season -> 1953 Major League Baseball season\n# Answer:\n1953 Major League Baseball season", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.season -> 1956 Major League Baseball Season\n# Answer:\n1956 Major League Baseball Season", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.season -> 1954 Major League Baseball season\n# Answer:\n1954 Major League Baseball season", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> common.topic.notable_types -> Cemetery\n# Answer:\nCypress Hills Cemetery", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> people.place_of_interment.interred_here -> Abraham Cohn\n# Answer:\nCypress Hills Cemetery", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> common.topic.article -> m.03vbmy\n# Answer:\nCypress Hills Cemetery", "# Reasoning Path:\nJackie Robinson -> influence.influence_node.influenced -> Steve Hofstetter -> influence.influence_node.influenced_by -> Jerry Seinfeld\n# Answer:\nSteve Hofstetter"], "ground_truth": ["UCLA Bruins football", "Kansas City Monarchs", "Los Angeles Bulldogs", "Brooklyn Dodgers", "Montreal Royals"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.24, "ans_precission": 0.3, "ans_recall": 0.2, "path_f1": 0.24, "path_precision": 0.3, "path_recall": 0.2, "path_ans_f1": 0.24, "path_ans_precision": 0.3, "path_ans_recall": 0.2}
{"id": "WebQTest-973", "prediction": ["# Reasoning Path:\nChicago -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nChicago -> location.statistical_region.population -> g.11b66kxd4l\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nChicago -> location.statistical_region.population -> g.11b7vbyfz4\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nChicago -> location.statistical_region.population -> g.11btt77639\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> Windy City Times -> common.topic.notable_for -> g.125fmxhqr\n# Answer:\nWindy City Times", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> Windy City Times -> book.periodical.first_issue_date -> m.0h_jkgx\n# Answer:\nWindy City Times", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> American Medical News -> common.topic.article -> m.03mf90t\n# Answer:\nAmerican Medical News", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> American Medical News -> common.topic.notable_for -> g.1254zz3kn\n# Answer:\nAmerican Medical News", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> Arbeiter-Zeitung -> common.topic.notable_types -> Newspaper\n# Answer:\nArbeiter-Zeitung", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> Arbeiter-Zeitung -> book.newspaper.headquarters -> m.0j0pp60\n# Answer:\nArbeiter-Zeitung"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-975", "prediction": ["# Reasoning Path:\nRonald Reagan -> government.us_president.vice_president -> George H. W. Bush -> base.activism.activist.area_of_activism -> Anti-communism\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nRonald Reagan -> government.us_president.vice_president -> George H. W. Bush -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nRonald Reagan -> government.us_president.vice_president -> George H. W. Bush -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nRonald Reagan -> architecture.building_occupant.buildings_occupied -> m.0dfzl8d -> architecture.occupancy.building -> White House\n# Answer:\nWhite House", "# Reasoning Path:\nRonald Reagan -> government.us_president.vice_president -> George H. W. Bush -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nRonald Reagan -> people.person.quotations -> Abortion is advocated only by persons who have themselves been born. -> common.topic.notable_for -> g.1259jvdzj\n# Answer:\nAbortion is advocated only by persons who have themselves been born.", "# Reasoning Path:\nRonald Reagan -> people.person.quotations -> Above all, we must realize that no arsenal, or no weapon in the arsenals of the world, is so formidable as the will and moral courage of free men and women. It is a weapon our adversaries in today's world do not have.  It is a weapon that we as Americans do have. Let that be understood by those who practice terrorism and prey upon their neighbours. -> media_common.quotation.source -> First Inaugural address of Ronald Reagan\n# Answer:\nAbove all, we must realize that no arsenal, or no weapon in the arsenals of the world, is so formidable as the will and moral courage of free men and women. It is a weapon our adversaries in today's world do not have.  It is a weapon that we as Americans do have. Let that be understood by those who practice terrorism and prey upon their neighbours.", "# Reasoning Path:\nRonald Reagan -> people.person.quotations -> Abortion is advocated only by persons who have themselves been born. -> common.topic.notable_types -> Quotation\n# Answer:\nAbortion is advocated only by persons who have themselves been born.", "# Reasoning Path:\nRonald Reagan -> people.person.quotations -> Approximately 80 % of our air pollution stems from hydrocarbons released by vegetation, so let's not go overboard in setting and enforcing tough emission standards from man-made sources. -> media_common.quotation.subjects -> Pollution\n# Answer:\nApproximately 80 % of our air pollution stems from hydrocarbons released by vegetation, so let's not go overboard in setting and enforcing tough emission standards from man-made sources.", "# Reasoning Path:\nRonald Reagan -> people.person.quotations -> Above all, we must realize that no arsenal, or no weapon in the arsenals of the world, is so formidable as the will and moral courage of free men and women. It is a weapon our adversaries in today's world do not have.  It is a weapon that we as Americans do have. Let that be understood by those who practice terrorism and prey upon their neighbours. -> common.topic.notable_types -> Quotation\n# Answer:\nAbove all, we must realize that no arsenal, or no weapon in the arsenals of the world, is so formidable as the will and moral courage of free men and women. It is a weapon our adversaries in today's world do not have.  It is a weapon that we as Americans do have. Let that be understood by those who practice terrorism and prey upon their neighbours."], "ground_truth": ["George H. W. Bush"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-976", "prediction": ["# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> U.S. Cellular Field -> business.sponsored_recipient.sponsorship -> m.05_g4w7\n# Answer:\nU.S. Cellular Field", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> U.S. Cellular Field -> location.location.events -> 2003 Major League Baseball All-Star Game\n# Answer:\nU.S. Cellular Field", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> Comiskey Park -> location.location.geolocation -> m.02_tzz7\n# Answer:\nComiskey Park", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> U.S. Cellular Field -> location.location.events -> 2005 World Series\n# Answer:\nU.S. Cellular Field", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> Comiskey Park -> location.location.events -> Soccer Bowl '84\n# Answer:\nComiskey Park", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> Comiskey Park -> sports.sports_facility.teams -> Chicago Mustangs\n# Answer:\nComiskey Park", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> Comiskey Park -> location.location.events -> 1917 World Series\n# Answer:\nComiskey Park", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> Comiskey Park -> location.location.events -> 1918 World Series\n# Answer:\nComiskey Park", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> Comiskey Park -> sports.sports_facility.teams -> Chicago Sting\n# Answer:\nComiskey Park", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.league -> m.0crtd51 -> sports.sports_league_participation.league -> American League Central\n# Answer:\nAmerican League Central"], "ground_truth": ["Comiskey Park", "U.S. Cellular Field"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-977", "prediction": ["# Reasoning Path:\nJeff Conaway -> film.actor.film -> m.0jy_bs -> film.performance.character -> Kenickie Murdoch\n# Answer:\nKenickie Murdoch", "# Reasoning Path:\nGrease -> film.film.starring -> m.0y5cjv1 -> film.performance.character -> Girl Fixing Hair in Mirror at Drive-in\n# Answer:\nGirl Fixing Hair in Mirror at Drive-in", "# Reasoning Path:\nJeff Conaway -> film.actor.film -> m.0jy_bs -> film.performance.film -> Grease\n# Answer:\nfilm.performance.film", "# Reasoning Path:\nGrease -> film.film.starring -> m.0jy_bs -> film.performance.character -> Kenickie Murdoch\n# Answer:\nKenickie Murdoch", "# Reasoning Path:\nJeff Conaway -> people.person.profession -> Singer -> common.topic.notable_types -> Profession\n# Answer:\nSinger", "# Reasoning Path:\nGrease -> film.film.starring -> m.0y5cjv1 -> film.performance.special_performance_type -> Uncredited\n# Answer:\nUncredited", "# Reasoning Path:\nJeff Conaway -> film.actor.film -> m.03lhv6t -> film.performance.film -> Elvira: Mistress of the Dark\n# Answer:\nElvira: Mistress of the Dark", "# Reasoning Path:\nJeff Conaway -> people.person.profession -> Singer -> people.profession.specialization_of -> Musician\n# Answer:\nSinger", "# Reasoning Path:\nGrease -> film.film.featured_song -> You're the One That I Want -> music.composition.recordings -> Grease: The Grease Mega-Mix\n# Answer:\nYou're the One That I Want", "# Reasoning Path:\nGrease -> film.film.featured_song -> You're the One That I Want -> music.composition.language -> English Language\n# Answer:\nYou're the One That I Want"], "ground_truth": ["Kenickie Murdoch"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-978", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> The Papers of Benjamin Franklin -> book.literary_series.works_in_this_series -> The Papers of Benjamin Franklin, Vol. 28: Volume 28: November 1, 1778 through February 28, 1779\n# Answer:\nThe Papers of Benjamin Franklin", "# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> The Papers of Benjamin Franklin -> book.literary_series.works_in_this_series -> The Papers of Benjamin Franklin, Volume 1: January 1, 1706 through December 31, 1734\n# Answer:\nThe Papers of Benjamin Franklin", "# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> The Papers of Benjamin Franklin -> common.topic.image -> papers of ben franklin vol 1 cover\n# Answer:\nThe Papers of Benjamin Franklin", "# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> Letters of Silence Dogood -> book.literary_series.works_in_this_series -> Silence Dogood, No. 1\n# Answer:\nLetters of Silence Dogood", "# Reasoning Path:\nBenjamin Franklin -> book.author.book_editions_published -> Autobiography of Benjamin Franklin 2e & Narrative of the Life of Frederick Douglass 2e -> book.book_edition.book -> The Autobiography of Benjamin Franklin\n# Answer:\nAutobiography of Benjamin Franklin 2e & Narrative of the Life of Frederick Douglass 2e", "# Reasoning Path:\nBenjamin Franklin -> book.author.book_editions_published -> A Dissertation On Liberty, Necessity, Pleasure, And Pain (Notable American Authors) -> book.book_edition.book -> A Dissertation on Liberty and Necessity, Pleasure and Pain\n# Answer:\nA Dissertation On Liberty, Necessity, Pleasure, And Pain (Notable American Authors)", "# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> Letters of Silence Dogood -> common.topic.notable_types -> Literary Series\n# Answer:\nLetters of Silence Dogood", "# Reasoning Path:\nBenjamin Franklin -> book.author.book_editions_published -> A Dissertation On Liberty, Necessity, Pleasure, And Pain (Notable American Authors) -> common.topic.notable_for -> g.125fb3zvq\n# Answer:\nA Dissertation On Liberty, Necessity, Pleasure, And Pain (Notable American Authors)", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent", "# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> Letters of Silence Dogood -> book.literary_series.works_in_this_series -> Silence Dogood, No. 10\n# Answer:\nLetters of Silence Dogood"], "ground_truth": ["The works of Benjamin Franklin", "Essays And Letters V1", "Franklin's boyhood in Boston", "The life of the late Dr. Benjamin Franklin", "Faceti\u00e6 Frankliana.   [sic]", "Franklin's Way to wealth, or, \\\"Poor Richard improved\\\"", "The sayings of Benjamin Franklin", "The Drinker's Dictionary", "The autobiography, with an introd", "The works of Dr. Benjamin Franklin, in philosophy, politics, and morals", "Early to bed, and early to rise, makes a man healthy, wealthy, and wise, or, Early rising, a natural, social, and religious duty", "Articles of belief", "The life of Benjamin Franklin, written chiefly by himself", "The Papers of Benjamin Franklin, Volume 1: January 1, 1706 through December 31, 1734", "Some account of the Pennsylvania Hospital", "Collected Works Of Benjamin Franklin", "The life and letters", "Letters and papers of Benjamin Franklin and Richard Jackson, 1753-1785", "\\\"The sayings of Poor Richard\\\"", "The autobiography, Poor Richard's almanac and other papers", "The Works Of Benjamin Franklin V1", "The ingenious Dr. Franklin", "America's Big Ben", "The life of the late Doctor Benjamin Franklin", "Benjamin Franklin's own story", "Benjamin Franklin's autobiographical writings", "Silence Dogood, No. 10", "Select works, including his autobiography", "My Dear Girl Ii", "Conseils pour s'enrichir", "Benjamin Franklin's the art of virtue", "The essays, humorous, moral and literary of the late Dr. Benjamin Franklin", "The Life of Benjamin Franklin", "Silence Dogood, No. 8", "Franklin's wit & folly", "The works of Dr. Benjamin Franklin", "The Autobiography of Benjamin Franklin", "The works of the late Dr. Benjamin Franklin", "Experiments and observations on electricity, made at Philadelphia in America", "Selected Works of Benjamin Franklin", "Free silver, and some other things", "Private correspondence of Benjamin Franklin", "The Papers of Benjamin Franklin, Vol. 28: Volume 28: November 1, 1778 through February 28, 1779", "Benjamin Franklin's Experiments", "The Morals of Chess", "The way to wealth, or, Poor Richard improved", "Founding Fathers Benjamin Franklin Volume 2", "Works of the late Doctor Benjamin Franklin", "What good is a newborn baby?", "The autobiography and other writings", "The Writings Of Benjamin Franklin, Vol. 1", "The life of Dr. Benjamin Franklin", "Apology for printers", "Silence Dogood, No. 2", "The complete works in philosophy, politics, and morals, of the late Dr. Benjamin Franklin, now first collected and arranged: with memoirs of his early life, written by himself", "B. Franklin, innovator", "The Way to Wealth", "Silence Dogood, No. 1", "The autobiography of Benjamin Franklin, and a sketch of Franklin's life from the point where the autobiography ends, drawn chiefly from his letters", "The Means and Manner of Obtaining Virtue", "Observations Concerning the Increase of Mankind, Peopling of Countries, etc.", "Benjamin Franklin's Proposals for the education of youth in Pennsylvania, 1749", "Father Abraham's speech to a great number of people, at a vendue of merchant-goods", "Silence Dogood, No. 3", "On war and peace", "Memoirs of the life and writings of Benjamin Franklin", "Silence Dogood, No. 13", "Poor Richard's Horse Keeper", "The life and essays, of Dr. Franklin", "Poor Richard's Almanack", "Political, Miscellaneous And Philosophical Pieces", "Silence Dogood, No. 6", "The glory of eternity", "A parable", "The bagatelles from Passy", "Silence Dogood, No. 14", "Silence Dogood, No. 7", "A letter of advice to a young man concerning marriage", "How to Attain Moral Perfection", "Autobiography, Poor Richard, and later writings", "The autobiography of Benjamin Franklin and selections from his other writings", "Memoirs of Benjamin Franklin", "A letter from B. Franklin to a young man", "Observations on the causes and cure of smoky chimneys", "A letter from Mr. Franklin to Mr. Peter Collinson, F.R.S. concerning the effect of lightning ; A letter of Benjamin Franklin, Esq. to Mr. Peter Collinson, F.R.S. concerning an electrical kite", "The writings of Benjamin Franklin", "Poor Richard day by day", "Some Fruits of Solitude in Reflections and Maxims", "Bite-size Ben Franklin", "A Dissertation on Liberty and Necessity, Pleasure and Pain", "Representative selections", "Letters to the press, 1758-1775", "Autobiography", "Benjamin Franklin on balloons", "Silence Dogood, No. 11", "Sheep will never make insurrections", "The political thought of Benjamin Franklin", "The Immortal Mentor", "Address", "Reflection On Courtship And Marriage", "Not Your Usual Founding Father", "The select works of Benjamin Franklin", "Poor Richard, 1733", "Silence Dogood, No. 4", "Silence Dogood, No. 12", "The autobiography of Benjamin Franklin; a restoration of a \\\"fair copy\\\"", "The life and essays of the late Doctor Benjamin Franklin", "The works of Dr. Benjn. Franklin;", "Benjamin Franklin and Jonathan Edwards", "Avis n\u00e9cessaire \u00e0 ceux qui veulent devenir riche", "Compleated Autobiography by Benjamin Franklin", "Silence Dogood, No. 5", "New experiments and observations on electricity", "The life of Doctor Benjamin Franklin", "Poor Richard, or, The way to wealth", "The wisdom of Benjamin Franklin", "Satires & Bagatelles", "The letters of Benjamin Franklin & Jane Mecom", "Authobiography Of Benjamin Franklin", "Articles Of Belief And Acts Of Religion Vol.2", "The art of making money plenty", "Franklin was there", "Silence Dogood, No. 9"], "ans_acc": 0.056910569105691054, "ans_hit": 1, "ans_f1": 0.027972027972027972, "ans_precission": 0.1, "ans_recall": 0.016260162601626018, "path_f1": 0.19672131147540983, "path_precision": 0.6, "path_recall": 0.11764705882352941, "path_ans_f1": 0.10396039603960396, "path_ans_precision": 0.6, "path_ans_recall": 0.056910569105691054}
{"id": "WebQTest-979", "prediction": ["# Reasoning Path:\nBarbados -> location.country.currency_used -> Barbadian dollar -> common.topic.notable_for -> g.125b468qw\n# Answer:\nBarbadian dollar", "# Reasoning Path:\nBarbados -> location.country.currency_used -> Barbadian dollar -> common.topic.article -> m.05hy7y\n# Answer:\nBarbadian dollar", "# Reasoning Path:\nBarbados -> location.statistical_region.official_development_assistance -> g.1hhc37pj3\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBarbados -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc37ppn\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nBarbados -> location.statistical_region.official_development_assistance -> g.1hhc382cm\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBarbados -> location.statistical_region.official_development_assistance -> g.1hhc38m3z\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBarbados -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc37x3k\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nBarbados -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc3b5td\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp"], "ground_truth": ["Barbadian dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> White people -> medicine.risk_factor.diseases -> Breast cancer\n# Answer:\nWhite people", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Yad Kennedy -> common.topic.notable_types -> Structure\n# Answer:\nYad Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of death\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Yad Kennedy -> common.topic.article -> m.0262gn5\n# Answer:\nYad Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nIrish American", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School -> education.educational_institution.students_graduates -> m.0n0tr1k\n# Answer:\nBloomington Kennedy High School", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> common.topic.notable_types -> Event\n# Answer:\nAssassination of John F. Kennedy"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-980", "prediction": ["# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1964 Stanley Cup Finals -> sports.sports_championship_event.champion -> Toronto Maple Leafs\n# Answer:\n1964 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1967 Stanley Cup Finals -> sports.sports_championship_event.champion -> Toronto Maple Leafs\n# Answer:\n1967 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1954 Stanley Cup Finals -> sports.sports_championship_event.champion -> Detroit Red Wings\n# Answer:\n1954 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1967 Stanley Cup Finals -> sports.sports_championship_event.runner_up -> Montreal Canadiens\n# Answer:\n1967 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1964 Stanley Cup Finals -> sports.sports_championship_event.season -> 1963\u201364 NHL season\n# Answer:\n1964 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> time.recurring_event.current_frequency -> Yearly -> base.events.festival_event.part_of_series -> National Poetry Month\n# Answer:\nYearly", "# Reasoning Path:\nStanley Cup -> time.recurring_event.current_frequency -> Yearly -> common.topic.notable_types -> Unit of frequency\n# Answer:\nYearly", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1954 Stanley Cup Finals -> common.topic.article -> m.03cj1q6\n# Answer:\n1954 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> time.recurring_event.current_frequency -> Yearly -> common.topic.article -> m.02nbmwg\n# Answer:\nYearly", "# Reasoning Path:\nStanley Cup -> freebase.valuenotation.is_reviewed -> Current frequency -> type.property.expected_type -> Unit of frequency\n# Answer:\nCurrent frequency"], "ground_truth": ["Frederick Stanley, 16th Earl of Derby"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-982", "prediction": ["# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13290 -> common.topic.notable_for -> g.125c79vqr\n# Answer:\n13290", "# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13201 -> location.postal_code.country -> United States of America\n# Answer:\n13201", "# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13290 -> location.location.containedby -> Onondaga County\n# Answer:\n13290", "# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13202 -> common.topic.notable_types -> Postal Code\n# Answer:\n13202", "# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13201 -> common.topic.notable_types -> Postal Code\n# Answer:\n13201", "# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13290 -> location.location.contains -> Destiny USA\n# Answer:\n13290", "# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13201 -> common.topic.notable_for -> g.125h5bq4l\n# Answer:\n13201", "# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13202 -> location.location.geolocation -> m.03dv6m2\n# Answer:\n13202", "# Reasoning Path:\nSyracuse -> location.place_with_neighborhoods.neighborhoods -> Armory Square -> location.location.containedby -> Syracuse, NY Metropolitan Statistical Area\n# Answer:\nArmory Square", "# Reasoning Path:\nSyracuse -> location.location.events -> 1983 NCAA Men's Division I Basketball Tournament -> base.marchmadness.ncaa_basketball_tournament.stages -> 1983 NCAA Men's Division I Basketball Tournament - East Regional Finals\n# Answer:\n1983 NCAA Men's Division I Basketball Tournament"], "ground_truth": ["13235", "13252", "13244-1020", "13219", "13217", "13244", "13209", "13290", "13212", "13204", "13202", "13208", "13211", "13206", "13221", "13220", "13215", "13261", "13210", "13205", "13207", "13224", "13214", "13218", "13203", "13225", "13201", "13250", "13251"], "ans_acc": 0.10344827586206896, "ans_hit": 1, "ans_f1": 0.183206106870229, "ans_precission": 0.8, "ans_recall": 0.10344827586206896, "path_f1": 0.183206106870229, "path_precision": 0.8, "path_recall": 0.10344827586206896, "path_ans_f1": 0.183206106870229, "path_ans_precision": 0.8, "path_ans_recall": 0.10344827586206896}
{"id": "WebQTest-983", "prediction": ["# Reasoning Path:\nTennessee Williams -> people.person.education -> m.02wpn3_ -> education.education.institution -> University of Missouri\n# Answer:\nUniversity of Missouri", "# Reasoning Path:\nTennessee Williams -> people.person.education -> m.0n0mldl -> education.education.institution -> The New School\n# Answer:\nThe New School", "# Reasoning Path:\nTennessee Williams -> people.person.education -> m.0n1m266 -> education.education.institution -> Washington University in St. Louis\n# Answer:\nWashington University in St. Louis", "# Reasoning Path:\nTennessee Williams -> people.person.quotations -> A high station in life is earned by the gallantry with which appalling experiences are survived with grace. -> common.topic.notable_for -> g.1258jbcmd\n# Answer:\nA high station in life is earned by the gallantry with which appalling experiences are survived with grace.", "# Reasoning Path:\nTennessee Williams -> people.person.quotations -> All cruel people describe themselves as paragons of frankness. -> common.topic.notable_for -> g.1258jtb7f\n# Answer:\nAll cruel people describe themselves as paragons of frankness.", "# Reasoning Path:\nTennessee Williams -> people.person.quotations -> A high station in life is earned by the gallantry with which appalling experiences are survived with grace. -> media_common.quotation.subjects -> Elegance\n# Answer:\nA high station in life is earned by the gallantry with which appalling experiences are survived with grace.", "# Reasoning Path:\nTennessee Williams -> book.author.works_written -> 27 Wagons Full of Cotton -> common.topic.notable_types -> Play\n# Answer:\n27 Wagons Full of Cotton", "# Reasoning Path:\nTennessee Williams -> people.person.quotations -> All cruel people describe themselves as paragons of frankness. -> media_common.quotation.subjects -> Frankness\n# Answer:\nAll cruel people describe themselves as paragons of frankness.", "# Reasoning Path:\nTennessee Williams -> book.author.works_written -> 27 Wagons Full of Cotton -> media_common.adapted_work.adaptations -> Baby Doll\n# Answer:\n27 Wagons Full of Cotton", "# Reasoning Path:\nTennessee Williams -> book.author.works_written -> A Lovely Sunday for Creve Coeur -> book.book.editions -> A lovely Sunday for Creve Coeur\n# Answer:\nA Lovely Sunday for Creve Coeur"], "ground_truth": ["University of Missouri", "Washington University in St. Louis", "The New School", "University of Iowa"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.4285714285714285, "ans_precission": 0.3, "ans_recall": 0.75, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4285714285714285, "path_ans_precision": 0.3, "path_ans_recall": 0.75}
{"id": "WebQTest-984", "prediction": ["# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> aviation.airport.serves -> Viceroy New York\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> location.location.containedby -> United States of America\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> LaGuardia Airport -> common.topic.notable_types -> Airport\n# Answer:\nLaGuardia Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> travel.transport_terminus.travel_destinations_served -> m.0519ccq\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> Flushing Airport -> common.topic.notable_types -> Airport\n# Answer:\nFlushing Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> aviation.airport.serves -> MBSF - Private Jets\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> LaGuardia Airport -> aviation.airport.serves -> New York metropolitan area\n# Answer:\nLaGuardia Airport", "# Reasoning Path:\nNew York City -> book.book_subject.works -> A Christmas Caroline -> book.written_work.subjects -> Christmas\n# Answer:\nA Christmas Caroline", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> aviation.airport.serves -> Queens\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> LaGuardia Airport -> travel.transport_terminus.travel_destinations_served -> m.0519ccq\n# Answer:\nLaGuardia Airport"], "ground_truth": ["John F. Kennedy International Airport"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-985", "prediction": ["# Reasoning Path:\nQueen Victoria -> people.person.spouse_s -> m.02kp9lp -> people.marriage.spouse -> Albert, Prince Consort\n# Answer:\nAlbert, Prince Consort", "# Reasoning Path:\nQueen Victoria -> people.person.children -> Victoria, Princess Royal -> people.person.children -> Sophia of Prussia\n# Answer:\nVictoria, Princess Royal", "# Reasoning Path:\nQueen Victoria -> people.person.children -> Edward VII -> people.person.children -> Princess Victoria of the United Kingdom\n# Answer:\nEdward VII", "# Reasoning Path:\nQueen Victoria -> people.person.children -> Edward VII -> people.person.parents -> Albert, Prince Consort\n# Answer:\nEdward VII", "# Reasoning Path:\nQueen Victoria -> people.person.children -> Alfred, Duke of Saxe-Coburg and Gotha -> people.person.children -> Alfred, Hereditary Prince of Saxe-Coburg and Gotha\n# Answer:\nAlfred, Duke of Saxe-Coburg and Gotha", "# Reasoning Path:\nQueen Victoria -> people.person.children -> Edward VII -> people.person.children -> George V\n# Answer:\nEdward VII", "# Reasoning Path:\nQueen Victoria -> people.person.spouse_s -> m.02kp9lp -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nQueen Victoria -> people.person.children -> Victoria, Princess Royal -> people.person.spouse_s -> m.0j4k6l9\n# Answer:\nVictoria, Princess Royal", "# Reasoning Path:\nQueen Victoria -> people.person.children -> Edward VII -> people.person.children -> Louise, Princess Royal\n# Answer:\nEdward VII", "# Reasoning Path:\nQueen Victoria -> people.person.children -> Edward VII -> people.deceased_person.place_of_death -> Buckingham Palace\n# Answer:\nEdward VII"], "ground_truth": ["Albert, Prince Consort"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.1904761904761905, "path_precision": 0.2, "path_recall": 0.18181818181818182, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-986", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> people.person.places_lived -> m.04lpf2x\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Josiah Franklin Jr.\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Francis Folger Franklin -> people.person.parents -> Deborah Read\n# Answer:\nFrancis Folger Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> common.topic.image -> WilliamFranklin\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Sarah Franklin Bache -> people.person.children -> Benjamin Franklin Bache\n# Answer:\nSarah Franklin Bache", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.parents -> Jane White Franklin\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Anne Harris\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Sarah Franklin Bache -> people.person.parents -> Deborah Read\n# Answer:\nSarah Franklin Bache", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Ebenezer Franklin\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.place_of_birth -> Ecton\n# Answer:\nJosiah Franklin"], "ground_truth": ["James Franklin Printing Shop", "United States Postal Service", "Josiah Franklin"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.5, "ans_recall": 0.3333333333333333, "path_f1": 0.4, "path_precision": 0.5, "path_recall": 0.3333333333333333, "path_ans_f1": 0.4, "path_ans_precision": 0.5, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-987", "prediction": ["# Reasoning Path:\nBarbara Bush -> people.person.profession -> Homemaking -> fictional_universe.character_occupation.characters_with_this_occupation -> Betty Draper\n# Answer:\nHomemaking", "# Reasoning Path:\nBarbara Bush -> people.person.profession -> Homemaking -> book.magazine_genre.magazines_in_this_genre -> Better Homes and Gardens\n# Answer:\nHomemaking", "# Reasoning Path:\nBarbara Bush -> people.person.profession -> Homemaking -> base.ontologies.ontology_instance.equivalent_instances -> m.07ngjqb\n# Answer:\nHomemaking", "# Reasoning Path:\nBarbara Bush -> people.person.profession -> Homemaking -> fictional_universe.character_occupation.characters_with_this_occupation -> Carmela Soprano\n# Answer:\nHomemaking", "# Reasoning Path:\nBarbara Bush -> people.person.profession -> Homemaking -> fictional_universe.character_occupation.characters_with_this_occupation -> Celia Hodes\n# Answer:\nHomemaking", "# Reasoning Path:\nBarbara Bush -> government.politician.party -> m.0lr18t_ -> government.political_party_tenure.party -> Republican Party\n# Answer:\nRepublican Party", "# Reasoning Path:\nBarbara Bush -> base.famouspets.pet_owner.pets_owned -> m.05hnbrv -> base.famouspets.pet_ownership.pet -> Millie\n# Answer:\nMillie", "# Reasoning Path:\nBarbara Bush -> base.famouspets.pet_owner.pets_owned -> m.05hnbrv -> base.famouspets.pet_ownership.owner -> George H. W. Bush\n# Answer:\nGeorge H. W. Bush"], "ground_truth": ["First Lady"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-988", "prediction": ["# Reasoning Path:\nJacob Latimore -> people.person.profession -> Dancer -> people.profession.specialization_of -> Artist\n# Answer:\nDancer", "# Reasoning Path:\nJacob Latimore -> people.person.profession -> Dancer -> common.topic.notable_for -> g.125h67_9m\n# Answer:\nDancer", "# Reasoning Path:\nJacob Latimore -> people.person.profession -> Dancer -> base.descriptive_names.names.descriptive_name -> m.0102lxg3\n# Answer:\nDancer", "# Reasoning Path:\nJacob Latimore -> people.person.profession -> Dancer -> base.descriptive_names.names.descriptive_name -> m.0102lxkn\n# Answer:\nDancer", "# Reasoning Path:\nJacob Latimore -> people.person.profession -> Actor -> people.profession.specializations -> Voice Actor\n# Answer:\nActor", "# Reasoning Path:\nJacob Latimore -> music.artist.album -> Heartbreak Heard Around the World -> music.recording.artist -> T-Pain\n# Answer:\nHeartbreak Heard Around the World", "# Reasoning Path:\nJacob Latimore -> people.person.profession -> Dancer -> base.descriptive_names.names.descriptive_name -> m.0102lxys\n# Answer:\nDancer", "# Reasoning Path:\nJacob Latimore -> people.person.profession -> Singer -> common.topic.subject_of -> L\u00e9o Ferr\u00e9\n# Answer:\nSinger", "# Reasoning Path:\nJacob Latimore -> people.person.profession -> Singer -> base.lightweight.profession.specialization_of -> Musicians and Singers\n# Answer:\nSinger", "# Reasoning Path:\nJacob Latimore -> people.person.parents -> Jacob Latimore, Sr. -> common.topic.notable_for -> g.126s_ym9k\n# Answer:\nJacob Latimore, Sr."], "ground_truth": ["Actor", "Singer", "Dancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-989", "prediction": ["# Reasoning Path:\nDunkirk -> common.topic.article -> m.0md6r\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nDunkirk -> location.location.events -> Battle of Dunkirk -> common.topic.notable_types -> Military Conflict\n# Answer:\nBattle of Dunkirk", "# Reasoning Path:\nDunkirk -> location.location.events -> Battle of the Dunes -> military.military_conflict.military_personnel_involved -> Bernard de Gomme\n# Answer:\nBattle of the Dunes", "# Reasoning Path:\nDunkirk -> location.location.events -> Battle of the Dunes -> common.topic.image -> LariviereBatailleDunes\n# Answer:\nBattle of the Dunes", "# Reasoning Path:\nDunkirk -> location.location.events -> Battle of Dunkirk -> military.military_conflict.casualties -> m.04lkt8p\n# Answer:\nBattle of Dunkirk", "# Reasoning Path:\nDunkirk -> location.location.events -> Battle of the Dunes -> base.culturalevent.event.entity_involved -> Louis, Grand Cond\u00e9\n# Answer:\nBattle of the Dunes", "# Reasoning Path:\nDunkirk -> location.location.events -> Action of 18 February 1639 -> base.culturalevent.event.entity_involved -> Spain\n# Answer:\nAction of 18 February 1639", "# Reasoning Path:\nDunkirk -> location.location.events -> Battle of Dunkirk -> military.military_conflict.casualties -> m.04lkt8z\n# Answer:\nBattle of Dunkirk", "# Reasoning Path:\nDunkirk -> location.location.events -> Battle of Dunkirk -> military.military_conflict.casualties -> m.04lkt9b\n# Answer:\nBattle of Dunkirk", "# Reasoning Path:\nDunkirk -> location.location.events -> Action of 18 February 1639 -> common.topic.notable_types -> Battle\n# Answer:\nAction of 18 February 1639"], "ground_truth": ["Battle of Dunkirk", "Siege of Dunkirk"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.4, "ans_recall": 0.5, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.4, "path_ans_recall": 0.5}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> South Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Serbia and Montenegro\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> common.topic.notable_types -> Human Language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Bosnia and Herzegovina\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> time.event.locations -> Czech Republic\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Torlakian dialect -> common.topic.image -> Torlak\n# Answer:\nTorlakian dialect", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Serbia and Montenegro\n# Answer:\nAlbanian language"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-990", "prediction": ["# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.position_s -> Small forward -> sports.sports_position.sport -> Basketball\n# Answer:\nSmall forward", "# Reasoning Path:\nScottie Pippen -> sports.pro_athlete.sports_played_professionally -> m.0d5jh3n -> sports.pro_sports_played.sport -> Basketball\n# Answer:\nBasketball", "# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.position_s -> Small forward -> common.topic.image -> John Salmons\n# Answer:\nSmall forward", "# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.position_s -> Small forward -> common.topic.notable_types -> Basketball Position\n# Answer:\nSmall forward", "# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.position_s -> Small forward -> common.topic.image -> Basketball half-court\n# Answer:\nSmall forward", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> base.descriptive_names.names.descriptive_name -> m.0105y97v\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Basketball players\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> base.descriptive_names.names.descriptive_name -> m.01069wj6\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> fictional_universe.character_occupation.characters_with_this_occupation -> Cam Calloway\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> base.descriptive_names.names.descriptive_name -> m.01069wkk\n# Answer:\nBasketball player"], "ground_truth": ["Basketball"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-991", "prediction": ["# Reasoning Path:\nDonnatella Moss -> tv.tv_character.appeared_in_tv_program -> m.02kvknt -> tv.regular_tv_appearance.actor -> Janel Moloney\n# Answer:\nJanel Moloney", "# Reasoning Path:\nDonnatella Moss -> tv.tv_character.appeared_in_tv_program -> m.02kvknt -> tv.regular_tv_appearance.seasons -> The West Wing Season 6\n# Answer:\nThe West Wing Season 6", "# Reasoning Path:\nDonnatella Moss -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The West Wing Universe -> fictional_universe.fictional_universe.characters -> Abbey Bartlet\n# Answer:\nThe West Wing Universe", "# Reasoning Path:\nDonnatella Moss -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The West Wing Universe -> fictional_universe.fictional_universe.works_set_here -> The West Wing\n# Answer:\nThe West Wing Universe", "# Reasoning Path:\nDonnatella Moss -> tv.tv_character.appeared_in_tv_program -> m.02kvknt -> tv.regular_tv_appearance.seasons -> The West Wing Season 7\n# Answer:\nThe West Wing Season 7", "# Reasoning Path:\nDonnatella Moss -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The West Wing Universe -> fictional_universe.fictional_universe.languages -> American English\n# Answer:\nThe West Wing Universe", "# Reasoning Path:\nDonnatella Moss -> fictional_universe.fictional_character.ethnicity -> Irish American -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Archbishop Gilday\n# Answer:\nIrish American", "# Reasoning Path:\nDonnatella Moss -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The West Wing Universe -> fictional_universe.fictional_universe.characters -> Abdul ibn Shareef\n# Answer:\nThe West Wing Universe", "# Reasoning Path:\nDonnatella Moss -> fictional_universe.fictional_character.ethnicity -> Irish American -> people.ethnicity.languages_spoken -> American English\n# Answer:\nIrish American", "# Reasoning Path:\nDonnatella Moss -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The West Wing Universe -> fictional_universe.fictional_universe.characters -> Ainsley Hayes\n# Answer:\nThe West Wing Universe"], "ground_truth": ["Janel Moloney"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-993", "prediction": ["# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.0k6pxpv -> tv.regular_tv_appearance.actor -> Lacey Chabert\n# Answer:\nLacey Chabert", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.0k6pxpv -> tv.regular_tv_appearance.series -> Family Guy\n# Answer:\nFamily Guy", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.0w0v0l2 -> tv.regular_tv_appearance.series -> The Cleveland Show\n# Answer:\nThe Cleveland Show", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntq_4 -> tv.regular_tv_appearance.actor -> Mila Kunis\n# Answer:\nMila Kunis", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.0w0v0l2 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nMeg Griffin -> cvg.game_character.games -> m.0ypsv3n -> cvg.game_performance.voice_actor -> Mila Kunis\n# Answer:\nMila Kunis", "# Reasoning Path:\nMeg Griffin -> film.film_character.portrayed_in_films -> m.0k3ssw -> film.performance.actor -> Mila Kunis\n# Answer:\nMila Kunis", "# Reasoning Path:\nMeg Griffin -> cvg.game_character.games -> m.0ypsv3n -> cvg.game_performance.performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntq_4 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nMeg Griffin -> film.film_character.portrayed_in_films -> m.010x342y -> film.performance.actor -> Mila Kunis\n# Answer:\nMila Kunis"], "ground_truth": ["Lacey Chabert"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-994", "prediction": ["# Reasoning Path:\nDallas Cowboys -> sports.sports_team.championships -> Super Bowl XXX -> time.event.instance_of_recurring_event -> Super Bowl\n# Answer:\nSuper Bowl XXX", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.championships -> Super Bowl XXX -> common.topic.notable_types -> Super bowl\n# Answer:\nSuper Bowl XXX", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.championships -> Super Bowl VI -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl VI", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.championships -> Super Bowl VI -> time.event.instance_of_recurring_event -> Super Bowl\n# Answer:\nSuper Bowl VI", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.championships -> Super Bowl XII -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XII", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.championships -> Super Bowl VI -> common.topic.notable_for -> g.125b2cvcd\n# Answer:\nSuper Bowl VI", "# Reasoning Path:\nDallas Cowboys -> american_football.football_team.away_games -> Dallas Cowboys at Washington Redskins, 2009-12-27 -> american_football.football_game.season -> 2009 NFL season\n# Answer:\nDallas Cowboys at Washington Redskins, 2009-12-27", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.championships -> Super Bowl XII -> time.event.locations -> Mercedes-Benz Superdome\n# Answer:\nSuper Bowl XII", "# Reasoning Path:\nDallas Cowboys -> american_football.football_team.away_games -> Dallas Cowboys at Arizona Cardinals, 2008-10-12 -> american_football.football_game.season -> 2008 NFL season\n# Answer:\nDallas Cowboys at Arizona Cardinals, 2008-10-12", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.season_record -> m.075fk0q -> sports.sports_team_season_record.season -> 1978 NFL season\n# Answer:\n1978 NFL season"], "ground_truth": ["Super Bowl XXX"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-996", "prediction": ["# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> soccer.football_pitch.matches -> Shamrock Rovers vs Liverpool F.C.\n# Answer:\nAviva Stadium", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> location.location.events -> 2011\u201312 Pro12\n# Answer:\nAviva Stadium", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> sports.sports_facility.teams -> Republic of Ireland national football team\n# Answer:\nAviva Stadium", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> soccer.football_pitch.matches -> 2010 FAI Cup Final\n# Answer:\nAviva Stadium", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> soccer.football_pitch.matches -> 2011 FAI Cup Final\n# Answer:\nAviva Stadium", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.fight_song -> Ireland's Call -> common.topic.image -> The lyrics of Ireland's Call displayed on big screen at Croke Park, Dublin, February 11th, 2007\n# Answer:\nIreland's Call", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> sports.sports_facility.teams -> League of Ireland XI\n# Answer:\nAviva Stadium", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.fight_song -> Ireland's Call -> common.topic.article -> m.01fy1q\n# Answer:\nIreland's Call", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.venue -> m.0wz2kl3 -> sports.team_venue_relationship.venue -> Aviva Stadium\n# Answer:\nAviva Stadium", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> sports.sports_facility.teams -> Leinster Rugby\n# Answer:\nAviva Stadium"], "ground_truth": ["Aviva Stadium"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-997", "prediction": ["# Reasoning Path:\nLee Harvey Oswald -> base.kwebbase.kwtopic.connections_to -> john fitzgerald kennedy allegedly assassinated by lee harvey oswald -> base.kwebbase.kwconnection.subject -> John F. Kennedy\n# Answer:\njohn fitzgerald kennedy allegedly assassinated by lee harvey oswald", "# Reasoning Path:\nLee Harvey Oswald -> base.kwebbase.kwtopic.connections_to -> fidel castro self-proclaimed follower was lee harvey oswald -> base.kwebbase.kwconnection.subject -> Fidel Castro\n# Answer:\nfidel castro self-proclaimed follower was lee harvey oswald", "# Reasoning Path:\nLee Harvey Oswald -> base.kwebbase.kwtopic.connections_to -> john fitzgerald kennedy allegedly assassinated by lee harvey oswald -> base.kwebbase.kwconnection.relation -> allegedly assassinated by\n# Answer:\njohn fitzgerald kennedy allegedly assassinated by lee harvey oswald", "# Reasoning Path:\nLee Harvey Oswald -> base.kwebbase.kwtopic.connections_to -> karl marx influenced lee harvey oswald -> base.kwebbase.kwconnection.subject -> Karl Marx\n# Answer:\nkarl marx influenced lee harvey oswald", "# Reasoning Path:\nLee Harvey Oswald -> base.kwebbase.kwtopic.connections_from -> lee harvey oswald allegedly assassinated john fitzgerald kennedy -> base.kwebbase.kwconnection.other -> John F. Kennedy\n# Answer:\nlee harvey oswald allegedly assassinated john fitzgerald kennedy", "# Reasoning Path:\nLee Harvey Oswald -> people.deceased_person.cause_of_death -> Bleeding -> people.cause_of_death.people -> Anthony Minghella\n# Answer:\nBleeding", "# Reasoning Path:\nLee Harvey Oswald -> people.deceased_person.cause_of_death -> Bleeding -> base.pethealth.symptom.symptom_of -> Head Trauma in Animals\n# Answer:\nBleeding", "# Reasoning Path:\nLee Harvey Oswald -> people.deceased_person.cause_of_death -> Ballistic trauma -> base.pethealth.cause.pet_diseases_or_conditions_caused -> Head Trauma in Animals\n# Answer:\nBallistic trauma", "# Reasoning Path:\nLee Harvey Oswald -> base.kwebbase.kwtopic.connections_from -> lee harvey oswald allegedly assassinated john fitzgerald kennedy -> base.kwebbase.kwconnection.sentence -> He was also seen on the first floor of the depository about 10 minutes before President Kennedy was shot as he drove in an open car at 12.30 pm.\n# Answer:\nlee harvey oswald allegedly assassinated john fitzgerald kennedy", "# Reasoning Path:\nLee Harvey Oswald -> base.kwebbase.kwtopic.connections_to -> fidel castro self-proclaimed follower was lee harvey oswald -> base.kwebbase.kwconnection.sentence -> Knew ^knew^ ^Hemingway^ Oswald claimed to be a follower.\n# Answer:\nfidel castro self-proclaimed follower was lee harvey oswald"], "ground_truth": ["Jack Ruby"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-998", "prediction": ["# Reasoning Path:\nNew York Knicks -> sports.sports_team.championships -> 1970 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n1970 NBA Finals", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.championships -> 1970 NBA Finals -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1970 NBA Finals", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.championships -> 1973 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n1973 NBA Finals", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.championships -> 1970 NBA Finals -> common.topic.article -> m.0cs1x9\n# Answer:\n1970 NBA Finals", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.championships -> 1973 NBA Finals -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1973 NBA Finals", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.coaches -> m.0qt2p64 -> sports.sports_team_coach_tenure.coach -> Mike Woodson\n# Answer:\nMike Woodson", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.coaches -> m.0qt2p64 -> sports.sports_team_coach_tenure.position -> Head coach\n# Answer:\nHead coach", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.venue -> m.0wz1zjx -> sports.team_venue_relationship.venue -> Madison Square Garden\n# Answer:\nMadison Square Garden", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.coaches -> m.010r7q18 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.venue -> m.0wz1zjx -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["1973 NBA Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-999", "prediction": ["# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1986 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n1986 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1986 NBA Finals -> sports.sports_championship_event.runner_up -> Houston Rockets\n# Answer:\n1986 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1986 NBA Finals -> sports.sports_championship_event.season -> 1985\u201386 NBA season\n# Answer:\n1986 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1959 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n1959 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1957 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n1957 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1959 NBA Finals -> common.topic.article -> m.0cs2j1\n# Answer:\n1959 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1957 NBA Finals -> common.topic.notable_for -> g.125906rkn\n# Answer:\n1957 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.professional_sports_team.draft_picks -> m.0110p_t3 -> sports.sports_league_draft_pick.draft -> 2014 NBA draft\n# Answer:\n2014 NBA draft", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1957 NBA Finals -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1957 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.roster -> m.0110pz2r -> sports.sports_team_roster.player -> Marcus Smart\n# Answer:\nMarcus Smart"], "ground_truth": ["1981 NBA Finals", "1957 NBA Finals", "1986 NBA Finals", "1961 NBA Finals", "1960 NBA Finals", "1974 NBA Finals", "1962 NBA Finals", "1966 NBA Finals", "1963 NBA Finals", "1976 NBA Finals", "1984 NBA Finals", "1964 NBA Finals", "2008 NBA Finals", "1969 NBA Finals", "1959 NBA Finals", "1968 NBA Finals", "1965 NBA Finals"], "ans_acc": 0.17647058823529413, "ans_hit": 1, "ans_f1": 0.28915662650602414, "ans_precission": 0.8, "ans_recall": 0.17647058823529413, "path_f1": 0.28915662650602414, "path_precision": 0.8, "path_recall": 0.17647058823529413, "path_ans_f1": 0.28915662650602414, "path_ans_precision": 0.8, "path_ans_recall": 0.17647058823529413}
